<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133870944006250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201907009%26RESULT%3d1%26SIGN%3dqZUFP19j9Iy3X6ovo40c8P3Jw9U%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201907009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201907009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201907009&amp;v=MDY1MzBWdUZ5amtWcnZNTWpYU1pMRzRIOWpNcUk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#1" data-title="1 引言 ">1 引言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#3" data-title="2 基于分组残差结构的轻量级卷积神经网络设计 ">2 基于分组残差结构的轻量级卷积神经网络设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#4" data-title="2.1 分组瓶颈结构">2.1 分组瓶颈结构</a></li>
                                                <li><a href="#7" data-title="2.2 残差结构">2.2 残差结构</a></li>
                                                <li><a href="#15" data-title="2.3 分组残差结构">2.3 分组残差结构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#22" data-title="3 实验 ">3 实验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#23" data-title="3.1 实验设置">3.1 实验设置</a></li>
                                                <li><a href="#29" data-title="3.2 实验结果及分析">3.2 实验结果及分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#6" data-title="图1 标准卷积与瓶颈结构">图1 标准卷积与瓶颈结构</a></li>
                                                <li><a href="#9" data-title="图2 分组瓶颈结构">图2 分组瓶颈结构</a></li>
                                                <li><a href="#10" data-title="图3 残差结构">图3 残差结构</a></li>
                                                <li><a href="#17" data-title="图4 GResNet1模块示意图">图4 GResNet1模块示意图</a></li>
                                                <li><a href="#18" data-title="图5 GResNet2模块示意图">图5 GResNet2模块示意图</a></li>
                                                <li><a href="#19" data-title="图6 GResNet3网络整体架构">图6 GResNet3网络整体架构</a></li>
                                                <li><a href="#21" data-title="表1 GResNets网络的基本参数设置">表1 GResNets网络的基本参数设置</a></li>
                                                <li><a href="#31" data-title="表2 各模型在Caltech-256上的性能">表2 各模型在Caltech-256上的性能</a></li>
                                                <li><a href="#32" data-title="表3 各模型在Food-101上的性能">表3 各模型在Food-101上的性能</a></li>
                                                <li><a href="#33" data-title="表4 各模型在GTSRB上的性能">表4 各模型在GTSRB上的性能</a></li>
                                                <li><a href="#36" data-title="图7 各模型在Caltech-256上的准确率曲线">图7 各模型在Caltech-256上的准确率曲线</a></li>
                                                <li><a href="#37" data-title="图8 各模型在Food-101上的准确率曲线">图8 各模型在Food-101上的准确率曲线</a></li>
                                                <li><a href="#38" data-title="图9 各模型在GTSRB上的准确率曲线">图9 各模型在GTSRB上的准确率曲线</a></li>
                                                <li><a href="#41" data-title="表5 不同方法在各数据集上的性能对比">表5 不同方法在各数据集上的性能对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="44">


                                    <a id="bibliography_1" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems.[s.l.]2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">
                                        <b>[1]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems.[s.l.]2012:1097-1105.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_2" title="杨远飞, 曾上游, 冯燕燕, 等.基于并行和切片的深度卷积网络设计研究[J].微电子学与计算机, 2018, 35 (3) :120-124." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201803025&amp;v=MDg0NjBNalhTWkxHNEg5bk1ySTlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlqa1ZydlA=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        杨远飞, 曾上游, 冯燕燕, 等.基于并行和切片的深度卷积网络设计研究[J].微电子学与计算机, 2018, 35 (3) :120-124.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_3" title="IANDOLA F N, HAN S, MOSKEWICZ M W, et al.Squeezenet:Alexnet-level accuracy with 50xfewer parameters and&amp;lt;0.5mb model size[J].arXiv preprint arXiv:1602.07360, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Squeezenet:Alexnet-level accuracy with 50xfewer parameters and&amp;lt;0.5mb model size">
                                        <b>[3]</b>
                                        IANDOLA F N, HAN S, MOSKEWICZ M W, et al.Squeezenet:Alexnet-level accuracy with 50xfewer parameters and&amp;lt;0.5mb model size[J].arXiv preprint arXiv:1602.07360, 2016.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_4" title="HOWARD A G, ZHU M, CHEN B, et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[J].arXiv preprint arXiv:1704.04861, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenets:Efficient convolutional neural networks for mobile vision applications">
                                        <b>[4]</b>
                                        HOWARD A G, ZHU M, CHEN B, et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[J].arXiv preprint arXiv:1704.04861, 2017.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_5" title="CHOLLET F.Xception:Deep learning with depthwise separable convolutions[J].arXiv preprint arXiv:1610.02357, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">
                                        <b>[5]</b>
                                        CHOLLET F.Xception:Deep learning with depthwise separable convolutions[J].arXiv preprint arXiv:1610.02357, 2017.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_6" title="XIE S, GIRSHICK R, DOLLAR P, et al.Aggregated residual transformations for deep neural networks[C]//2017IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .USA, Honolulu, IEEE, 2017:5987-5995." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">
                                        <b>[6]</b>
                                        XIE S, GIRSHICK R, DOLLAR P, et al.Aggregated residual transformations for deep neural networks[C]//2017IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .USA, Honolulu, IEEE, 2017:5987-5995.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_7" title="JIA Y, SHELHAMER E, DONAHUE J, et al.Caffe:Convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia.ACM, 2014:675-678." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[7]</b>
                                        JIA Y, SHELHAMER E, DONAHUE J, et al.Caffe:Convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia.ACM, 2014:675-678.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_8" title="HERAVI E J, AGHDAM H H, PUIG D.An optimized convolutional neural network with bottleneck and spatial pyramid pooling layers for classification of foods[J].Pattern Recognition Letters, 2018 (105) :50-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF46265A5E1979150217BD4938E598E8E&amp;v=MDAzMjJMcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhMbTZ4Szg9TmlmT2ZjVzhHTlBLcXY1QUVlb0dDM1U0eWhZUjZ6Z1BQSHZycnhwQWZMdWNNTA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        HERAVI E J, AGHDAM H H, PUIG D.An optimized convolutional neural network with bottleneck and spatial pyramid pooling layers for classification of foods[J].Pattern Recognition Letters, 2018 (105) :50-58.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_9" title="PANDEY P, DEEPTHI A, MANDAL B, et al.FoodNet:recognizing foods using ensemble of deep networks[J].IEEE Signal Processing Letters, 2017, 24 (12) :1758-1762." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FoodNet:Recognizing foods using ensemble of deep networks">
                                        <b>[9]</b>
                                        PANDEY P, DEEPTHI A, MANDAL B, et al.FoodNet:recognizing foods using ensemble of deep networks[J].IEEE Signal Processing Letters, 2017, 24 (12) :1758-1762.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_10" title="GONZALEZ-REYNA S E, AVINA-CERVANTES JG, LEDESMA-OROZCO S E, et al.Traffic sign recognition based on linear discriminant analysis[C]//Mexican International Conference on Artificial Intelligence.Springer, Berlin, Heidelberg, 2013:185-193." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Traffic sign recognition based on linear discriminant analysis">
                                        <b>[10]</b>
                                        GONZALEZ-REYNA S E, AVINA-CERVANTES JG, LEDESMA-OROZCO S E, et al.Traffic sign recognition based on linear discriminant analysis[C]//Mexican International Conference on Artificial Intelligence.Springer, Berlin, Heidelberg, 2013:185-193.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_11" title="SERMANET P, LECUN Y.Traffic sign recognition with multi-scale convolutional networks[C]//Neural Networks (IJCNN) , The 2011 International Joint Conference on.USA, San Jose, IEEE, 2011:2809-2813." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Traffic sign recognition with multi-scale Convolutional Networks">
                                        <b>[11]</b>
                                        SERMANET P, LECUN Y.Traffic sign recognition with multi-scale convolutional networks[C]//Neural Networks (IJCNN) , The 2011 International Joint Conference on.USA, San Jose, IEEE, 2011:2809-2813.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(07),43-47             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于分组残差结构的轻量级卷积神经网络设计</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%B9%8F&amp;code=14019474&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E5%93%81%E7%BE%A4&amp;code=06885716&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋品群</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%BE%E4%B8%8A%E6%B8%B8&amp;code=24446464&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曾上游</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E6%B5%B7%E8%8B%B1&amp;code=26853921&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏海英</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BB%96%E5%BF%97%E8%B4%A4&amp;code=22123470&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">廖志贤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8C%83%E7%91%9E&amp;code=42157108&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">范瑞</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E8%A5%BF%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0019441&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广西师范大学电子工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统深度卷积神经网络参数数量过多, 很难在移动设备上应用的问题, 提出基于分组残差结构的轻量级卷积神经网络架构GResNets.利用三个卷积层的瓶颈结构将上层输出特征图分为数量相等的四组, 根据组内的瓶颈模块加入恒等映射的方式和组外相邻模块是否加入残差学习, 设计了三种轻量级卷积神经网络架构.试验阶段, 在Caltech-256, Food-101和GTSRB图像分类数据集上评测了三种网络架构的性能.实验结果表明, 与传统深度卷积神经网络相比, GResNets能在网络参数较少的情况下, 具有同样、甚至更优越的分类性能, 适合在移动设备上应用.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%BB%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分组;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E6%80%A7%E8%83%BD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类性能;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BD%BB%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">轻量;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李鹏 男, (1992-) , 硕士研究生.研究方向为图像处理.;
                                </span>
                                <span>
                                    *蒋品群 (通讯作者) 男, (1970-) , 博士, 副教授.研究方向为复杂网络理论及其应用.E-mail:pqjiang@mailbox.gxnu.edu.cn.;
                                </span>
                                <span>
                                    曾上游 男, (1974-) , 博士, 教授.研究方向为生物神经网络理论及其应用.;
                                </span>
                                <span>
                                    夏海英 女, (1983-) , 博士, 副教授.研究方向为图像处理.;
                                </span>
                                <span>
                                    廖志贤 男, (1986-) , 硕士, 讲师.研究方向为复杂网络理论及其应用.;
                                </span>
                                <span>
                                    范瑞 女, (1993-) , 硕士研究生.研究方向为图像处理.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (11465004、61762014);</span>
                                <span>桂林市科学研究与技术开发计划项目 (20170113-4);</span>
                    </p>
            </div>
                    <h1>Design of lightweight convolution neural network based on group residual structure</h1>
                    <h2>
                    <span>LI Peng</span>
                    <span>JIANG Pin-qun</span>
                    <span>ZENG Shang-you</span>
                    <span>XIA Hai-Ying</span>
                    <span>LIAO Zhi-xian</span>
                    <span>FAN Rui</span>
            </h2>
                    <h2>
                    <span>College of Electronic Engineering, Guangxi Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that the parameters of traditional deep convolution neural network are too large to be used in mobile devices, a lightweight convolution neural network architecture GResNets based on group residual structure is proposed.Using the bottleneck structure with three convolution layers, the output feature maps of the previous layer are divided into four equal groups.Three lightweight convolution neural network architectures are designed according to the way of adding identical mapping to the bottleneck module in the group and whether or not the adjacent module outside the group joins residual learning.In the experimental stage, the performance of the three network architectures was evaluated on three image recognition datasets, Caltech-256, Food-101 and GTSRB.The experimental results show that GResNets has the same or even better classification performance than traditional deep convolution neural networks under the condition of fewer network parameters, and is suitable for mobile devices.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=group&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">group;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20performance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification performance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=lightweight&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">lightweight;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="1" name="1" class="anchor-tag">1 引言</h3>
                <div class="p1">
                    <p id="2">自2012年AlexNet<citation id="66" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>在LSVRC比赛中崭露头角以来, 卷积神经网络 (Convolutional Neural Networks, CNN) 在图像分类<citation id="67" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、目标检测、语义分割等领域取得了巨大的成功.为了提高网络模型的准确率, 研究人员不断地加深卷积网络的层数, 相继提出了各种各样性能更加优越的卷积神经网络.虽然这些网络的准确率都比较高, 但是大多都是建立在层次较深、模型复杂、参数数量庞大的情况下, 其运行所需要的计算资源和存储资源也比较多, 不利于网络移植到计算资源和存储资源都十分有限的移动设备.因此, 一些研究人员提出了一些轻量级卷积神经网络<citation id="68" type="reference"><link href="48" rel="bibliography" /><link href="50" rel="bibliography" /><link href="52" rel="bibliography" /><link href="54" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>.受以上工作的启发, 本文提出基于分组残差结构的轻量级卷积神经网络架构GResNets.</p>
                </div>
                <h3 id="3" name="3" class="anchor-tag">2 基于分组残差结构的轻量级卷积神经网络设计</h3>
                <h4 class="anchor-tag" id="4" name="4">2.1 分组瓶颈结构</h4>
                <div class="p1">
                    <p id="5">瓶颈结构 (Bottleneck) 的思想来源于1×1的卷积, 目的是减少网络参数.图1给出了标准卷积和改进后的瓶颈结构示意图.从图中可以看出, 假设输入输出特征图数量N=128, 标准卷积层参数量P=128×3×3×128=147 456, 瓶颈结构卷积层参数量P=128×1×1×32+32×3×3×32+32×1×1×128=17 408.通过瓶颈结构可以将标准卷积层参数量减少到原来的近九分之一.由于瓶颈结构加深了网络深度, 使得网络非线性表达能力更强, 进一步提升网络准确率.文献<citation id="69" type="reference">[<a class="sup">6</a>]</citation>指出要提高网络准确率, 增加基数比增加深度和宽度更有效, 所谓基数就是平行堆叠某个模块的个数.受此启发, 本节通过瓶颈结构的第一个1×1卷积核将上层输出分为四组, 如图2所示.图2中四组的特征图数量均保持一致, 每组的第一个1×1卷积用来降维, 第三个1×1卷积用来升维, 并行输出通过级联的方式整合在一起.当多个分组瓶颈结构进行堆叠, 每组的第一个1×1卷积核对各分组信息进行跨通道融合.</p>
                </div>
                <div class="area_img" id="6">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_00600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 标准卷积与瓶颈结构" src="Detail/GetImg?filename=images/WXYJ201907009_00600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 标准卷积与瓶颈结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_00600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="7" name="7">2.2 残差结构</h4>
                <div class="p1">
                    <p id="8">卷积神经网络通过不断学习来更新网络的参数, 从而得到输入与输出之间的映射关系, 但是当网络层次较深时, 这个映射关系不容易优化, 网络会出现退化的问题.若在输入与输出之间添加一个恒等映射 (Identity Mapping) 来学习输入数据与输出数据之间的残差, 可以解决深层网络训练困难的问题.具体做法分两种, 一种是当输入与输出维度相等时, 在输入与输出之间添加一个短接线;一种是输入输出维度不相等时, 使用线性投影方式来匹配维度.由此, 图1的瓶颈结构加入残差连接后如图3所示.</p>
                </div>
                <div class="area_img" id="9">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_00900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 分组瓶颈结构" src="Detail/GetImg?filename=images/WXYJ201907009_00900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 分组瓶颈结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_00900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="10">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_01000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 残差结构" src="Detail/GetImg?filename=images/WXYJ201907009_01000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 残差结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_01000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="11">图3中若x为输入, 经三个卷积层运算后输出为F (x, W<sub>i</sub>) , 卷积层后加入归一化BN层, 激活函数采用ReLU, 残差结构的输出y定义为式 (1) :</p>
                </div>
                <div class="area_img" id="71">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/WXYJ201907009_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="14">式中, W<sub>i</sub>为第i个卷积层学习的权重参数, m为输入特征图数量, n为输出特征图数量, W<sub>s</sub>x是利用1×1卷积核实现的线性投影.</p>
                </div>
                <h4 class="anchor-tag" id="15" name="15">2.3 分组残差结构</h4>
                <div class="p1">
                    <p id="16">基于分组瓶颈结构和残差结构, 在给定的准确率水平下, 设计一种轻量级的网络架构是可能的.本节在保证网络参数较少的条件下, 根据2.1节和2.2节的描述设计了三种分组残差结构.第一种网络模块示意图如图4所示.主要是对图2中每两组添加经线性变换后的恒等映射, 即G1和G2、G3和G4分别共用一个线性投影, 称为GResNet1.第二种是在每组瓶颈结构中均加入线性投影, 如图5所示.目的是与GResNet1进行对比, 网络参数也较前者有所增加, 称为GResNet2.第三种是在GResNet2基础上改进, 在组外输入输出维度相等的相邻模块加入残差连接, 如图6所示, 称为GResNet3.这样做本质上等价于组内的瓶颈结构通过残差学习与组外模块继续进行残差学习, 方便网络的信息传输与优化.</p>
                </div>
                <div class="area_img" id="17">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_01700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 GResNet1模块示意图" src="Detail/GetImg?filename=images/WXYJ201907009_01700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 GResNet1模块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_01700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="18">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_01800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 GResNet2模块示意图" src="Detail/GetImg?filename=images/WXYJ201907009_01800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 GResNet2模块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_01800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="19">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_01900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 GResNet3网络整体架构" src="Detail/GetImg?filename=images/WXYJ201907009_01900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 GResNet3网络整体架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_01900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="20">借鉴SqueezeNet网络的设计思想, 模块化的方式使得网络的扩展性更好, 用图4和图5的模块替换图6的Block可完成GResNet1和GResNet2的设计.GResNets的基本网络参数设置如表1所示.通过堆叠Block模块以及在模块中间穿插多个池化层而形成, 池化层的操作采用Max-Ave-Ave-Ave (Max指最大值池化, Ave指平均池化.) 架构, 括号中是瓶颈结构的三个卷积层参数, G=4表示输入分为四组, 网络架构清晰明了, 易于扩展.</p>
                </div>
                <div class="area_img" id="21">
                                            <p class="img_tit">
                                                表1 GResNets网络的基本参数设置
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_02100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/WXYJ201907009_02100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_02100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 GResNets网络的基本参数设置" src="Detail/GetImg?filename=images/WXYJ201907009_02100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="22" name="22" class="anchor-tag">3 实验</h3>
                <h4 class="anchor-tag" id="23" name="23">3.1 实验设置</h4>
                <div class="p1">
                    <p id="24">数据集:实验在Caltech-256, Food-101和GTSRB三个数据集上进行.Caltech-256包含256个类别共29 781张图片, 原数据集未划分训练集和测试集, 其识别难度也比较大.本文按4:1的比例将其分为训练集23 919张图片和测试集5 862张图片.Food-101包含101种食物共101 000张图片, 每类食物有1 000张图片, 750张作为训练图片, 250张作为测试图片, 共75 750张训练图片和25 250张测试图片.GTSRB包含43类德国交通标志共51 839张图片, 分为训练集39 209张图片, 测试集12 630张图片.在图像识别方面的研究, 这三个数据集的使用比较广泛.</p>
                </div>
                <div class="p1">
                    <p id="25">图像预处理:主要包括尺度归一化、数据增强和去均值三个操作.尺度归一化操作是将三个数据集的所有图片尺寸缩放到256×256大小.为了提高网络泛化能力和防止过拟合, 训练时对图片使用数据增强操作, 具体做法是从每张图片的5个方位 (左上角, 左下角, 右上角, 右下角以及中间方位) 随机裁剪得到227×227大小像素的图像块, 再加上水平翻转可以将数据扩大为原来的10倍.值得注意的是, GTSRB交通标志数据中个别交通标志翻转后不符合实际, 这里不进行水平翻转操作.</p>
                </div>
                <div class="p1">
                    <p id="26">实验环境:实验所依赖的深度学习框架为Caffe<citation id="70" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 计算机配置为i7-6700K四核CPU、Ubuntun14.04 Linux操作系统、32GB内存以及NVIDIA-GTX 1070GPU.</p>
                </div>
                <div class="p1">
                    <p id="28">网络训练参数设置:实验在Caltech-256数据集上训练时, solver文件配置为:初始学习率为0.01, 学习率变化方式为multistep, gamma为0.1, 学习率每24 000次迭代衰减一次, 共衰减两次, 最大迭代70 000次, 每训练500次测试一次网络性能.同样的在Food-101和GTSRB数据上实验时, 训练参数基本保持一致, 只是初始学习率为0.005, 在Food-101上每40 000次迭代衰减一次, 共衰减三次, 最大迭代150 000次.在GTSRB上学习率每24 000次迭代衰减一次, 共衰减两次, 最大迭代60 000次.</p>
                </div>
                <h4 class="anchor-tag" id="29" name="29">3.2 实验结果及分析</h4>
                <div class="p1">
                    <p id="30">实验在上述三个数据集上对所设计的三种网络模型进行评测.表2、表3和表4分别给出了在Caltech-256、Food-101和GTSRB上三种网络性能对比.</p>
                </div>
                <div class="area_img" id="31">
                                            <p class="img_tit">
                                                表2 各模型在Caltech-256上的性能
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_03100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/WXYJ201907009_03100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_03100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各模型在Caltech-256上的性能" src="Detail/GetImg?filename=images/WXYJ201907009_03100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="32">
                                            <p class="img_tit">
                                                表3 各模型在Food-101上的性能
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_03200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/WXYJ201907009_03200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_03200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 各模型在Food-101上的性能" src="Detail/GetImg?filename=images/WXYJ201907009_03200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="33">
                                            <p class="img_tit">
                                                表4 各模型在GTSRB上的性能
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_03300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/WXYJ201907009_03300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_03300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 各模型在GTSRB上的性能" src="Detail/GetImg?filename=images/WXYJ201907009_03300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="34">从表2、表3和表4可以看出, GResNet2的准确率均高于GResNet1, 说明在每组内的瓶颈结构中添加恒等映射对提高模型准确率是有贡献的.虽然参数量有所增加, 进而使得生成的模型变大, 但是仍在同一数量级之内.对比GResNet2和GResNet3的性能可以看出组外模块加入残差连接, 准确率可以进一步提高, 但是不会增加参数数量和模型大小, 只是训练时间稍有增加.另外, 随着识别种类的减少, 生成的Caffe模型变小, 主要因为网络中全连接层占据了网络的主要参数, 识别种类越少, 模型越小.总体来看, GResNet3的性能最优, 用比较少的参数在Caltech-256上达到59.28%, Food-101上达到72.69%, GTSRB上达到98.78%的识别率.实验结果表明, GResNets具有参数少、分类性能优越以及模型较小的优点, 实现了网络轻量化.</p>
                </div>
                <div class="p1">
                    <p id="35">图7、图8和图9分别是在三种数据集上各模型的测试准确率曲线对比.</p>
                </div>
                <div class="area_img" id="36">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_03600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 各模型在Caltech-256上的准确率曲线" src="Detail/GetImg?filename=images/WXYJ201907009_03600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 各模型在Caltech-256上的准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_03600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="37">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_03700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 各模型在Food-101上的准确率曲线" src="Detail/GetImg?filename=images/WXYJ201907009_03700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 各模型在Food-101上的准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_03700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="38">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_03800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 各模型在GTSRB上的准确率曲线" src="Detail/GetImg?filename=images/WXYJ201907009_03800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 各模型在GTSRB上的准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_03800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="39">另外, 在GResNet1组外加入残差连接, 分类准确率会进一步提升, 参数量保持不变, 使得网络的可控性和扩展性很好.</p>
                </div>
                <div class="p1">
                    <p id="40">表5给出了在三种数据集上GResNet3与其它方法的模型大小和准确率对比.在Caltech-256上, 与传统AlexNet网络相比, 模型大小仅是其1/21.在Food-101和GTSRB上, 与轻量级SqueezeNet网络相比, 虽然模型略大, 但仍在同一级别, 其准确率却分别超过SqueezeNet方法16.39%和4.56%.与其它的一些方法相比, GResNet3取得了较好地分类效果, 证明了该网络的有效性.</p>
                </div>
                <div class="area_img" id="41">
                                            <p class="img_tit">
                                                表5 不同方法在各数据集上的性能对比
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201907009_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/WXYJ201907009_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201907009_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表5 不同方法在各数据集上的性能对比" src="Detail/GetImg?filename=images/WXYJ201907009_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="42" name="42" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="43">本文提出的轻量级网络是通过瓶颈结构和恒等映射使网络能够提取更本质、更抽象、更具有辨识度的特征, 进而提升网络的分类性能.同时结合了一些轻量化方法和训练技巧, 主要包括瓶颈结构、分组思想、残差学习、以及批量归一化等, 使得网络可以克服训练上的困难并得到更优的性能.网络采用模块化设计, 使得网络需要改进或扩展时更加容易.实验结果表明, 与传统深度卷积神经网络相比, 基于分组残差结构的轻量级网络能在参数较少的情况下, 具有同样、甚至更优越的分类性能, 适合在移动设备上应用.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="44">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">

                                <b>[1]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems.[s.l.]2012:1097-1105.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201803025&amp;v=MjgxODBGckNVUkxPZVplVnVGeWprVnJ2UE1qWFNaTEc0SDluTXJJOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>杨远飞, 曾上游, 冯燕燕, 等.基于并行和切片的深度卷积网络设计研究[J].微电子学与计算机, 2018, 35 (3) :120-124.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Squeezenet:Alexnet-level accuracy with 50xfewer parameters and&amp;lt;0.5mb model size">

                                <b>[3]</b>IANDOLA F N, HAN S, MOSKEWICZ M W, et al.Squeezenet:Alexnet-level accuracy with 50xfewer parameters and&lt;0.5mb model size[J].arXiv preprint arXiv:1602.07360, 2016.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenets:Efficient convolutional neural networks for mobile vision applications">

                                <b>[4]</b>HOWARD A G, ZHU M, CHEN B, et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[J].arXiv preprint arXiv:1704.04861, 2017.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">

                                <b>[5]</b>CHOLLET F.Xception:Deep learning with depthwise separable convolutions[J].arXiv preprint arXiv:1610.02357, 2017.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">

                                <b>[6]</b>XIE S, GIRSHICK R, DOLLAR P, et al.Aggregated residual transformations for deep neural networks[C]//2017IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .USA, Honolulu, IEEE, 2017:5987-5995.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[7]</b>JIA Y, SHELHAMER E, DONAHUE J, et al.Caffe:Convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia.ACM, 2014:675-678.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF46265A5E1979150217BD4938E598E8E&amp;v=MjY5OTJyeHBBZkx1Y01MTHFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4TG02eEs4PU5pZk9mY1c4R05QS3F2NUFFZW9HQzNVNHloWVI2emdQUEh2cg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>HERAVI E J, AGHDAM H H, PUIG D.An optimized convolutional neural network with bottleneck and spatial pyramid pooling layers for classification of foods[J].Pattern Recognition Letters, 2018 (105) :50-58.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FoodNet:Recognizing foods using ensemble of deep networks">

                                <b>[9]</b>PANDEY P, DEEPTHI A, MANDAL B, et al.FoodNet:recognizing foods using ensemble of deep networks[J].IEEE Signal Processing Letters, 2017, 24 (12) :1758-1762.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Traffic sign recognition based on linear discriminant analysis">

                                <b>[10]</b>GONZALEZ-REYNA S E, AVINA-CERVANTES JG, LEDESMA-OROZCO S E, et al.Traffic sign recognition based on linear discriminant analysis[C]//Mexican International Conference on Artificial Intelligence.Springer, Berlin, Heidelberg, 2013:185-193.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Traffic sign recognition with multi-scale Convolutional Networks">

                                <b>[11]</b>SERMANET P, LECUN Y.Traffic sign recognition with multi-scale convolutional networks[C]//Neural Networks (IJCNN) , The 2011 International Joint Conference on.USA, San Jose, IEEE, 2011:2809-2813.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201907009" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201907009&amp;v=MDY1MzBWdUZ5amtWcnZNTWpYU1pMRzRIOWpNcUk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
