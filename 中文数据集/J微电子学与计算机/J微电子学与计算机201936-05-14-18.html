<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133885099162500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201905004%26RESULT%3d1%26SIGN%3d4q8G%252fkIGVELA%252fuAYqcR3F1Ik1fc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201905004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201905004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201905004&amp;v=MDIzNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1XcjNJTWpYU1pMRzRIOWpNcW85RllJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#21" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#23" data-title="2 &lt;b&gt;数据库&lt;/b&gt; ">2 <b>数据库</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#25" data-title="2.1 EMO-DB&lt;b&gt;数据库结构&lt;/b&gt;">2.1 EMO-DB<b>数据库结构</b></a></li>
                                                <li><a href="#28" data-title="2.2 CASIA&lt;b&gt;数据库结构&lt;/b&gt;">2.2 CASIA<b>数据库结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#31" data-title="3 &lt;b&gt;特征提取&lt;/b&gt; ">3 <b>特征提取</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#33" data-title="4 &lt;b&gt;特征选择&lt;/b&gt; ">4 <b>特征选择</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#35" data-title="4.1 BP&lt;b&gt;神经网络训练步骤&lt;/b&gt;">4.1 BP<b>神经网络训练步骤</b></a></li>
                                                <li><a href="#51" data-title="4.2 &lt;b&gt;隐层节点数的选取&lt;/b&gt;">4.2 <b>隐层节点数的选取</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="5 &lt;b&gt;实验及结果分析&lt;/b&gt; ">5 <b>实验及结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="5.1 &lt;b&gt;一次特征排序实验结果&lt;/b&gt;">5.1 <b>一次特征排序实验结果</b></a></li>
                                                <li><a href="#61" data-title="5.2 &lt;b&gt;二次特征排序实验结果&lt;/b&gt;">5.2 <b>二次特征排序实验结果</b></a></li>
                                                <li><a href="#66" data-title="5.3 &lt;b&gt;混库实验结果&lt;/b&gt;">5.3 <b>混库实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="6 &lt;b&gt;结束语&lt;/b&gt; ">6 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#27" data-title="&lt;b&gt;表&lt;/b&gt;1 EMO-DB&lt;b&gt;情感数据库结构&lt;/b&gt;"><b>表</b>1 EMO-DB<b>情感数据库结构</b></a></li>
                                                <li><a href="#30" data-title="&lt;b&gt;表&lt;/b&gt;2 CASIA&lt;b&gt;情感数据库结构&lt;/b&gt;"><b>表</b>2 CASIA<b>情感数据库结构</b></a></li>
                                                <li><a href="#53" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;隐层节点数为&lt;/b&gt;1～50&lt;b&gt;的平均识别率&lt;/b&gt;"><b>图</b>1 <b>隐层节点数为</b>1～50<b>的平均识别率</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;特征重要性排序结果&lt;/b&gt;"><b>表</b>3 <b>特征重要性排序结果</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;两个数据库特征排序的结果&lt;/b&gt;"><b>表</b>4 <b>两个数据库特征排序的结果</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;混库识别率的分类矩阵&lt;/b&gt;"><b>图</b>2 <b>混库识别率的分类矩阵</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="91">


                                    <a id="bibliography_1" title=" 宋鹏, 郑文明, 赵力.基于子空间学习和特征选择融合的语音情感识别[J].清华大学学报 (自然科学版) , 2018, 58 (4) :347-354." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=QHXB201804003&amp;v=MTc2MTMzenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVdyM0lOQ1hUYkxHNEg5bk1xNDlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         宋鹏, 郑文明, 赵力.基于子空间学习和特征选择融合的语音情感识别[J].清华大学学报 (自然科学版) , 2018, 58 (4) :347-354.
                                    </a>
                                </li>
                                <li id="93">


                                    <a id="bibliography_2" title=" LOTFIAN R, BUSSO C.Emotion recognition using synthetic speech as neutral reference[C]//2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Brisbane, QLD, 2015:4759-4763.doi:10.1109/ICASSP.2015.7178874." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Emotion recognition using synthetic speech as neutral reference">
                                        <b>[2]</b>
                                         LOTFIAN R, BUSSO C.Emotion recognition using synthetic speech as neutral reference[C]//2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Brisbane, QLD, 2015:4759-4763.doi:10.1109/ICASSP.2015.7178874.
                                    </a>
                                </li>
                                <li id="95">


                                    <a id="bibliography_3" title=" 陈俊, 王爱国, 王坤侠, 等.基于类依赖的语音情感特征选择[J].微电子学与计算机, 2016, 33 (8) :92-96." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201608020&amp;v=MDIyMzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1XcjNJTWpYU1pMRzRIOWZNcDQ5SFo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         陈俊, 王爱国, 王坤侠, 等.基于类依赖的语音情感特征选择[J].微电子学与计算机, 2016, 33 (8) :92-96.
                                    </a>
                                </li>
                                <li id="97">


                                    <a id="bibliography_4" title=" 何淑琳, 张雪英, 孙颖, 等.基于极限学习机的语音情感识别[J].微电子学与计算机, 2015, 32 (7) :50-54." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201507012&amp;v=MDcyMDNTWkxHNEg5VE1xSTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVdyM0lNalg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         何淑琳, 张雪英, 孙颖, 等.基于极限学习机的语音情感识别[J].微电子学与计算机, 2015, 32 (7) :50-54.
                                    </a>
                                </li>
                                <li id="99">


                                    <a id="bibliography_5" title=" WANG Y.Speech emotion recognition based on improved MFCC[EB/OL]. (2018-08-06) .http://dl.acm.org.citation.cfm&amp;gt;id=327837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition based on improved MFCC">
                                        <b>[5]</b>
                                         WANG Y.Speech emotion recognition based on improved MFCC[EB/OL]. (2018-08-06) .http://dl.acm.org.citation.cfm&amp;gt;id=327837.
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_6" title=" 郝梓岚一种基于倒谱分离信号得非特定人语音情感识别方法[p].中国专利:201711434048.3." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN108154879A&amp;v=MTUyMzZVYjg9SmlPNkhydTVHOVhFcUlZMEMrNFBEMzFMeHhZVDZ6b09TM2ZtcFdGYWU3S1ZUTHVkWk9kdUZ5dnM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         郝梓岚一种基于倒谱分离信号得非特定人语音情感识别方法[p].中国专利:201711434048.3.
                                    </a>
                                </li>
                                <li id="103">


                                    <a id="bibliography_7" title=" ZENG Y.Research on feature extraction of pathological voice [D].Guangxi :Guangxi Normal University, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on feature extraction of pathological voice">
                                        <b>[7]</b>
                                         ZENG Y.Research on feature extraction of pathological voice [D].Guangxi :Guangxi Normal University, 2015.
                                    </a>
                                </li>
                                <li id="105">


                                    <a id="bibliography_8" title=" 李翔, 李昕, 胡晨, 等.面向智能机器人的Teager语音情感交互系统设计与实现[J].仪器仪表学报, 2016, 34 (8) :1826-1833." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201308020&amp;v=MTEwMjllVnVGeWptV3IzSVBEelRiTEc0SDlMTXA0OUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         李翔, 李昕, 胡晨, 等.面向智能机器人的Teager语音情感交互系统设计与实现[J].仪器仪表学报, 2016, 34 (8) :1826-1833.
                                    </a>
                                </li>
                                <li id="107">


                                    <a id="bibliography_9" title=" ZHU B.End-to-end speech emotion recognition based on neutral network[C]//Proceedings of 2017 17th IEEE International Conference on Communication Technology (ICCT 2017) .IEEE, Beijing, 2017:5-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end speech emotion recognition based on neutral network">
                                        <b>[9]</b>
                                         ZHU B.End-to-end speech emotion recognition based on neutral network[C]//Proceedings of 2017 17th IEEE International Conference on Communication Technology (ICCT 2017) .IEEE, Beijing, 2017:5-7.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(05),14-18             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于BP特征选择的语音情感识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%89%B3&amp;code=06888814&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王艳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E7%BB%B4%E5%B9%B3&amp;code=06879686&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡维平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E8%A5%BF%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0019441&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广西师范大学电子工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目前语音情感识别主要面临着的难题在关于语音声学特征与情感之间关系的研究成果缺乏一致性, 同样的特征运用不同的库, 识别结果会相差很大.使用支持向量机SVM作为识别机, 通过BP神经网络进行特征选择, 得到EMO-DB库特征组合的最高识别率为85.59%, 得到CASIA库特征组合的最高识别率为74.75%.本文包含2个语音库, 其中一个中文, 一个德文.通过BP神经网络特征选择后, 最优特征子集包含8个特征, 将特征子集应用于EMO-DB库和CASIA库的混库实验的识别率为72.34%, 并与近三年的文章进行了对比分析, 本文的实验结果处在较高的水平.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音情感识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SVM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SVM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征子集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=BP%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">BP特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B7%E5%BA%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">混库;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E7%9F%A9%E9%98%B5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类矩阵;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王艳, 女, (1993-) , 硕士研究生.研究方向为语音情感识别.E-mail:1174542882@qq.com.;
                                </span>
                                <span>
                                    胡维平, 男, (1963-) , 博士, 教授.研究方向为信号处理、图像处理.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61362003);</span>
                    </p>
            </div>
                    <h1><b>Speech emotion recognition based on BP feature selection</b></h1>
                    <h2>
                    <span>WANG Yan</span>
                    <span>HU Wei-ping</span>
            </h2>
                    <h2>
                    <span>College of Electronic Engineering Institute, Guangxi Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>At present, the main problems faced by speech emotion recognition are the lack of consistency in the research results on the relationship between speech acoustic features and emotions, the same characteristics use different databases, the recognition results will vary greatly. Using support vector machine as the recognition machine, feature selection is performed through BP neural network, and the highest recognition rate of EMO-DB database feature combination is 85.59%, the highest recognition rate of the CASIA database feature combination is 74.75%, which improves the speed of the operation. This paper contains two speech databases, one of which is Chinese and one German. After BP neural network feature selection, the recognition rate of the mixed databases experiment of the EMO-DB databases and the CASIA databases was 72.34%.And compared with the articles of the past three years, the experimental results of this paper are at a relatively high level.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20emotion%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech emotion recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SVM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SVM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20subset&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature subset;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=BP%20feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">BP feature selection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mixed%20database&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mixed database;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20matrix&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification matrix;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="21" name="21" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="22">目前, 在语音情感识别中特征选择的研究只局限于给出声学特征对情感区别度的重要程度排名, 缺乏在特征选择的基础上进行特征个数、特征组合对准确率影响的研究, 并且大部分研究都是针对英语等西方语种, 普通话的相关研究较少<citation id="109" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.本文在德文和中文数据库上做实验, 并且进行了二次特征选择, 即在初次选择特征的统计量使用哪种, 二次确定组合特征的个数, 以达到最优识别率.神经网络具有很强的聚类能力和静态分类能力, 在语音处理方向得到了广泛的应用<citation id="110" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.陈俊等人提出了基于类依赖的语音情感特征选择, 维数低, 识别率高<citation id="111" type="reference"><link href="95" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.何淑琳等人提出了基于极限学习机的语音情感识别, 其在EMO-DB情感语音数据库上3种情感标签的识别效果高达92.50%<citation id="112" type="reference"><link href="97" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.本文采用BP神经网络对特征集进行特征排序, 选择一组简单、容易实现并能尽全面表征语音情感信息的特征子集.由于目前单通道的声学特征用于交叉库的语音情感识别率在45%左右, 所以本实验进行混库的语音识别.</p>
                </div>
                <h3 id="23" name="23" class="anchor-tag">2 <b>数据库</b></h3>
                <div class="p1">
                    <p id="24">语音数据库是分析识别语音情感的基础.现今还没有一个包括多种语音多个不同年龄不同性别说话者的大规模情感语音库.本文用到的语料库有以下两个.</p>
                </div>
                <h4 class="anchor-tag" id="25" name="25">2.1 EMO-DB<b>数据库结构</b></h4>
                <div class="p1">
                    <p id="26">EMO-DB数据库是由德国柏林工业大学记录的德国情感语音数据库.它由10名演员组成, 共10个句子, 表现出7种情感.采样率为48 kHz (压缩至16 kHz后) , 16位量化.听辨实验后, 共筛选535个句子.该数据库广泛应用于语音情感识别领域, 所有7个类别535个句子应用于本文.</p>
                </div>
                <div class="area_img" id="27">
                    <p class="img_tit"><b>表</b>1 EMO-DB<b>情感数据库结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="27" border="1"><tr><td><br />情感类别</td><td>快乐</td><td>中性</td><td>愤怒</td><td>悲伤</td><td>恐惧</td><td>无聊</td><td>厌恶</td></tr><tr><td><br />语料数量</td><td>71</td><td>79</td><td>127</td><td>62</td><td>69</td><td>81</td><td>46</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="28" name="28">2.2 CASIA<b>数据库结构</b></h4>
                <div class="p1">
                    <p id="29">CASIA (汉语情感语料库) 该数据库由中国科学院自动化研究所录制, 由4位录音人在纯净录音环境下 (信噪比约为35 db) 分别在6类不同情感下对50句与情感无关的文本进行的演绎得到, 16 kHz采样, 16 bit量化.经过听辨筛选, 最终保留其中1 200句<citation id="113" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="30">
                    <p class="img_tit"><b>表</b>2 CASIA<b>情感数据库结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="30" border="1"><tr><td><br />情感类别</td><td>快乐</td><td>中性</td><td>愤怒</td><td>悲伤</td><td>害怕</td><td>惊讶</td></tr><tr><td><br />语料数量</td><td>200</td><td>200</td><td>200</td><td>200</td><td>200</td><td>200</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="31" name="31" class="anchor-tag">3 <b>特征提取</b></h3>
                <div class="p1">
                    <p id="32">本文从情感语音中提取了全局统计特征:基音频率F0、能量EE、过零率ZCR、频率微扰FF、Mel频域倒谱系数 (Mel Frequency Cepstrum Coefficients, MFCC) 、hurst参数、线性预测编码 (Linear Predictive Coding, LPC) 、感知线性预测系数 (Perceptual linear predictive coefficient, PLP) 共8类原始特征<citation id="114" type="reference"><link href="99" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.本文还包括本实验室提取的改进的MFCC特征<citation id="115" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>:基频MFCC, 即把基频曲线代替原始语音作MFCC变换得到F0MFCC;同理得到能量MFCC EEMFCC<citation id="116" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>;倒谱MFCC, 对声道声带分离后的谱信号重构回时域信号, 再对其提取MFCC参数, 得到CSS_MFCC;基于频域Teager MFCC, 在提取MFCC的过程中加入Teager能量算子得到NFD_Mel共4类新特征<citation id="117" type="reference"><link href="105" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.本文共使用12类特征, 分别计算了它们的统计学特征, 包括平均值、中位值、标准差、最小值、最大值、方差、变化率、一阶差分等.</p>
                </div>
                <h3 id="33" name="33" class="anchor-tag">4 <b>特征选择</b></h3>
                <div class="p1">
                    <p id="34">传统声学特征对语音情感识别的效果大不相同, 随意组合所得来的特征向量常常得不到满意的效果, 因此, 特征选择便是很有意义的一项工作.而BP (Back Propagation) 神经网络具有较强的学习能力, 在特征选择方面表现了良好的检测冗余能力, 因此实验采用BP神经网络进行特征选择, 基于BP神经网络的特征排序方法根据每个输入特征显著性度量值, 得出对识别贡献的情况, 从而得到排序结果.</p>
                </div>
                <h4 class="anchor-tag" id="35" name="35">4.1 BP<b>神经网络训练步骤</b></h4>
                <div class="p1">
                    <p id="36">BP神经网络实质是通过梯度算法和迭代算法, 将样本输入、输出问题转化为非线性优化问题的一种学习<citation id="118" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.BP神经网络的训练的具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="37"> (1) 设置初始权值及阈值, 一般将权值和阈值设为较小的随机数.</p>
                </div>
                <div class="p1">
                    <p id="38"> (2) 设置新的输入值x<sub>1</sub>, x<sub>2</sub>, …, x<sub>N</sub>及相应理想输入信号d<sub>1</sub>, d<sub>2</sub>, …, d<sub>M</sub></p>
                </div>
                <div class="area_img" id="39">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/WXYJ201905004_03900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="40"> (3) 计算输入矢量<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>N</i></sub>通过网络时, 网络的实际输出值<i>z</i><sub>1</sub>, <i>z</i><sub>2</sub>, …, <i>z</i><sub><i>M</i></sub>.对于网络中任意节点<i>j</i>, 其输出计算步骤为</p>
                </div>
                <div class="p1">
                    <p id="41" class="code-formula">
                        <mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="42">式中, <i>u</i><sub><i>j</i></sub>是节点<i>j</i>的阈值与节点输入值的加权求和;<i>θ</i><sub><i>j</i></sub>是节点<i>j</i>的阈值.</p>
                </div>
                <div class="p1">
                    <p id="43"> (4) 修正网络节点权值和阈值, 由输出误差, 网络从输出节点想输入层节点传播</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd columnalign="left"><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>=</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mi>η</mi><mspace width="0.25em" /><mi>σ</mi><msub><mrow></mrow><mi>j</mi></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mi>η</mi><mspace width="0.25em" /><mi>σ</mi><msub><mrow></mrow><mi>j</mi></msub></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">式中, <i>w</i><sub><i>ij</i></sub> (<i>t</i>) 为<i>t</i>时刻从节点<i>i</i> (输出节点或隐层节点) 到节点<i>j</i> (隐层节点或输入节点) 的权值;<i>x</i><sub><i>i</i></sub>为第<i>i</i>个节点的输入;<i>η</i>代表网络学习的速率, 称为增益因子或收敛因子, 当节点<i>j</i>是输出节点时, 理想输出明确, 所以<i>σ</i><sub><i>j</i></sub>可以由下式求得:</p>
                </div>
                <div class="p1">
                    <p id="46"><i>σ</i><sub><i>j</i></sub>=<i>y</i><sub><i>j</i></sub> (1-<i>y</i><sub><i>j</i></sub>) (<i>d</i><sub><i>j</i></sub>-<i>y</i><sub><i>j</i></sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="47">当节点<i>j</i>是隐层节点时, 理想输出不明确, <i>σ</i><sub><i>j</i></sub>定义为</p>
                </div>
                <div class="p1">
                    <p id="48"><i>σ</i><sub><i>j</i></sub>=<i>x</i><sub><i>i</i></sub> (1-<i>x</i><sub><i>i</i></sub>) ∑<i>σ</i><sub>k</sub><i>W</i><sub><i>jk</i></sub>      (6) </p>
                </div>
                <div class="p1">
                    <p id="49">式中, <i>d</i><sub><i>j</i></sub>和<i>y</i><sub><i>j</i></sub>分别是输出节点<i>j</i>的理想输出和实际输出;<i>k</i>是隐层节点<i>j</i>上一层的全部节点数.</p>
                </div>
                <div class="p1">
                    <p id="50"> (5) 转移到第 (2) 步重复进行, 直到<i>w</i><sub><i>ij</i></sub>, <i>θ</i><sub><i>j</i></sub>稳定为止.</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">4.2 <b>隐层节点数的选取</b></h4>
                <div class="p1">
                    <p id="52">在BP神经网络中, 输入层和输出层的节点个数都是确定的, 而隐含层节点个数不确定<citation id="119" type="reference"><link href="107" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>.本实验在两个数据库的混合数据上进行, 共1 735句语音, 选用单隐含层的BP神经网络, 隐含层激活函数使用Sigmoid函数, 输出层激活函数为线性函数.为了得到较好的神经网络结构, 实验中分别对当隐节点数为1～50时, 网络的分类情况好坏来判断哪个隐节点数较好.将每个隐层节点数下的网络训练10次, 最后得到这10次实验中的测试集识别率, 并求和作平均.得到如图1所示的结果.</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201905004_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 隐层节点数为1～50的平均识别率" src="Detail/GetImg?filename=images/WXYJ201905004_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>隐层节点数为</b>1～50<b>的平均识别率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201905004_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">由图1可知, 当隐层单元节点为20时, 其测试集平均最高识别率为94.39%, 此时得网络结构分类能力最强.所以本实验最终选择隐层节点数为20的BP神经网络进行实验.</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">5 <b>实验及结果分析</b></h3>
                <div class="p1">
                    <p id="56">为了挑选出对网络贡献较大的一部分特征, 通过对输入节点信号变化的敏感度来度量特征的重要性.将贡献较大的特征通过组合特征实验来进一步验证特征选择的实验结果.</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">5.1 <b>一次特征排序实验结果</b></h4>
                <div class="p1">
                    <p id="58">实验中用到的特征参数, <i>F</i><sub>0</sub>均值用<i>f</i><sub>1</sub>代表, MFCC均值<i>f</i><sub>2</sub>, LPC均值<i>f</i><sub>3</sub>, FF标准差<i>f</i><sub>4</sub>, ZCR一阶差分<i>f</i><sub>5</sub>, F0MFCC均值<i>f</i><sub>6</sub>, EE一阶差分<i>f</i><sub>7</sub>, EEMFCC均值<i>f</i><sub>8</sub>, CSS-MFCC均值<i>f</i><sub>9</sub>, NFD_Mel均值<i>f</i><sub>10</sub>, Hurst标准差<i>f</i><sub>11</sub>, plp均值<i>f</i><sub>12</sub>, EE最大值<i>f</i><sub>13</sub>, EE最小值<i>f</i><sub>14</sub>, EE中位值<i>f</i><sub>15</sub>, EE标准差<i>f</i><sub>16</sub>, EE均值<i>f</i><sub>17</sub>, EE方差<i>f</i><sub>18</sub>, ZCR最大值<i>f</i><sub>19</sub>, ZCR最小值<i>f</i><sub>20</sub>, ZCR中位值<i>f</i><sub>21</sub>, ZCR标准差<i>f</i><sub>22</sub>, ZCR均值<i>f</i><sub>23</sub>, ZCR方差<i>f</i><sub>24</sub>, <i>F</i><sub>0</sub>最大值<i>f</i><sub>25</sub>, <i>F</i><sub>0</sub>最小值f<sub>26</sub>, <i>F</i><sub>0</sub>中位值<i>f</i><sub>27</sub>, <i>F</i><sub>0</sub>标准差<i>f</i><sub>28</sub>, <i>F</i><sub>0</sub>方差<i>f</i><sub>29</sub>, <i>F</i><sub>0</sub>一阶差分<i>f</i><sub>30</sub>, <i>F</i><sub>0</sub>变化率<i>f</i><sub>31</sub>, MFCC最大值<i>f</i><sub>32</sub>, MFCC最小值<i>f</i><sub>33</sub>, MFCC中位值<i>f</i><sub>34</sub>, MFCC标准差<i>f</i><sub>35</sub>, MFCC方差<i>f</i><sub>36</sub>, MFCC一阶差分<i>f</i><sub>37</sub>, LPC最大值<i>f</i><sub>38</sub>, LPC最小值<i>f</i><sub>39</sub>, LPC中位值<i>f</i><sub>40</sub>, LPC标准差<i>f</i><sub>41</sub>, LPC方差<i>f</i><sub>42</sub>, LPC一阶差分<i>f</i><sub>43</sub>, PLP最大值<i>f</i><sub>44</sub>, PLP最小值<i>f</i><sub>45</sub>, PLP中位值<i>f</i><sub>46</sub>, PLP标准差<i>f</i><sub>47</sub>, PLP方差<i>f</i><sub>48</sub>, PLP一阶差分<i>f</i><sub>49</sub>, FF均值<i>f</i><sub>50</sub>, NFD_Mel方差<i>f</i><sub>51</sub>, Hurst均值<i>f</i><sub>52</sub>, NFD_Mel一阶差分<i>f</i><sub>53</sub>, <i>F</i><sub>0</sub>MFCC最大值<i>f</i><sub>54</sub>, <i>F</i><sub>0</sub>MFCC最小值<i>f</i><sub>55</sub>, <i>F</i><sub>0</sub>MFCC中位值<i>f</i><sub>56</sub>, <i>F</i><sub>0</sub>MFCC标准差<i>f</i><sub>57</sub>, <i>F</i><sub>0</sub>MFCC方差<i>f</i><sub>58</sub>, <i>F</i><sub>0</sub>MFCC一阶差分<i>f</i><sub>59</sub>, EEMFCC最大值<i>f</i><sub>60</sub>, EEMFCC最小值<i>f</i><sub>61</sub>, EEMFCC中位值<i>f</i><sub>62</sub>, EEMFCC标准差<i>f</i><sub>63</sub>, EEMFCC方差<i>f</i><sub>64</sub>, EEMFCC一阶差分<i>f</i><sub>65</sub>, CSS_MFCC最大值<i>f</i><sub>66</sub>, CSS_MFCC最小值<i>f</i><sub>67</sub>, CSS_MFCC中位值<i>f</i><sub>68</sub>, CSS_MFCC标准差<i>f</i><sub>69</sub>, CSS_MFCC方差<i>f</i><sub>70</sub>, CSS_MFCC一阶差分<i>f</i><sub>71</sub>, NFD_Mel最大值<i>f</i><sub>72</sub>, NFD_Mel最小值<i>f</i><sub>73</sub>, NFD_Mel中位值<i>f</i><sub>74</sub>, NFD_Mel标准差<i>f</i><sub>75</sub>.本实验在两个数据库的混合数据上进行, 共1 735句语音.将75个特征共527维的特征矢量集作为神经网络的输入, 隐层神经元个数为20, 训练好神经网络, 确定网络的各连接权值, 然后计算各输入特征的显著性度量值.由于神经网络的初值是随机设置的, 为了实验的正确性, 本文对527维的特征矢量进行10次训练出来的每个特征的显著性值进行排序.经过10次试验, 得到的排序结果如表3所示.</p>
                </div>
                <div class="p1">
                    <p id="59">根据表3, 原始的75个特征参数527维特征矢量, 根据次序的大小选择特征的统计量, 次序的数字越小, 表明该参数的重要性越高.选取前12个特征进行以下实验.从文献<citation id="120" type="reference">[<a class="sup">1</a>,<a class="sup">7</a>]</citation>中, 研究结果与本文相符, 谱特征的均值要比其他统计量更能表达语音情感信息.但实验中发现, 若使用12个特征的111维特征矢量识别率EMO-DB库为77.92%, CASIA库为67.42%, 反而没有部分特征组合的识别率高, 所以接下来对这12个特征111维特征矢量再进行一次特征选择, 以确定特征个数为多少时达到更高的识别率.</p>
                </div>
                <div class="area_img" id="60">
                    <p class="img_tit"><b>表</b>3 <b>特征重要性排序结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="60" border="1"><tr><td><br />次序</td><td>特征</td><td>次序</td><td>特征</td><td>次序</td><td>特征</td><td>次序</td><td>特征</td><td>次序</td><td>特征</td></tr><tr><td><br />1</td><td><i>f</i><sub>7</sub></td><td>16</td><td><i>f</i><sub>50</sub></td><td>31</td><td><i>f</i><sub>33</sub></td><td>46</td><td><i>f</i><sub>36</sub></td><td>61</td><td><i>f</i><sub>72</sub></td></tr><tr><td><br />2</td><td><i>f</i><sub>1</sub></td><td>17</td><td><i>f</i><sub>23</sub></td><td>32</td><td><i>f</i><sub>22</sub></td><td>47</td><td><i>f</i><sub>67</sub></td><td>62</td><td><i>f</i><sub>69</sub></td></tr><tr><td><br />3</td><td><i>f</i><sub>4</sub></td><td>18</td><td><i>f</i><sub>24</sub></td><td>33</td><td><i>f</i><sub>35</sub></td><td>48</td><td><i>f</i><sub>48</sub></td><td>63</td><td><i>f</i><sub>45</sub></td></tr><tr><td><br />4</td><td><i>f</i><sub>5</sub></td><td>19</td><td><i>f</i><sub>52</sub></td><td>34</td><td><i>f</i><sub>32</sub></td><td>49</td><td><i>f</i><sub>73</sub></td><td>64</td><td><i>f</i><sub>51</sub></td></tr><tr><td><br />5</td><td><i>f</i><sub>6</sub></td><td>20</td><td><i>f</i><sub>56</sub></td><td>35</td><td><i>f</i><sub>29</sub></td><td>50</td><td><i>f</i><sub>74</sub></td><td>65</td><td><i>f</i><sub>25</sub></td></tr><tr><td><br />6</td><td><i>f</i><sub>11</sub></td><td>21</td><td><i>f</i><sub>58</sub></td><td>36</td><td><i>f</i><sub>31</sub></td><td>51</td><td><i>f</i><sub>18</sub></td><td>66</td><td><i>f</i><sub>75</sub></td></tr><tr><td><br />7</td><td><i>f</i><sub>2</sub></td><td>22</td><td><i>f</i><sub>55</sub></td><td>37</td><td><i>F</i><sub>43</sub></td><td>52</td><td><i>f</i><sub>28</sub></td><td>67</td><td><i>f</i><sub>14</sub></td></tr><tr><td><br />8</td><td><i>f</i><sub>3</sub></td><td>23</td><td><i>f</i><sub>15</sub></td><td>38</td><td><i>f</i><sub>62</sub></td><td>53</td><td><i>f</i><sub>21</sub></td><td>68</td><td><i>f</i><sub>66</sub></td></tr><tr><td><br />9</td><td><i>f</i><sub>12</sub></td><td>24</td><td><i>f</i><sub>57</sub></td><td>39</td><td><i>f</i><sub>46</sub></td><td>54</td><td><i>f</i><sub>26</sub></td><td>69</td><td><i>f</i><sub>44</sub></td></tr><tr><td><br />10</td><td><i>f</i><sub>8</sub></td><td>25</td><td><i>f</i><sub>13</sub></td><td>40</td><td><i>f</i><sub>42</sub></td><td>55</td><td><i>f</i><sub>53</sub></td><td>70</td><td><i>f</i><sub>47</sub></td></tr><tr><td><br />11</td><td><i>f</i><sub>9</sub></td><td>26</td><td><i>f</i><sub>27</sub></td><td>41</td><td><i>f</i><sub>68</sub></td><td>56</td><td><i>f</i><sub>54</sub></td><td>71</td><td><i>f</i><sub>59</sub></td></tr><tr><td><br />12</td><td><i>f</i><sub>10</sub></td><td>27</td><td><i>f</i><sub>41</sub></td><td>42</td><td><i>f</i><sub>64</sub></td><td>57</td><td><i>f</i><sub>20</sub></td><td>72</td><td><i>f</i><sub>37</sub></td></tr><tr><td><br />13</td><td><i>f</i><sub>17</sub></td><td>28</td><td><i>f</i><sub>40</sub></td><td>43</td><td><i>f</i><sub>61</sub></td><td>58</td><td><i>f</i><sub>49</sub></td><td>73</td><td><i>f</i><sub>39</sub></td></tr><tr><td><br />14</td><td><i>f</i><sub>16</sub></td><td>29</td><td><i>f</i><sub>19</sub></td><td>44</td><td><i>f</i><sub>38</sub></td><td>59</td><td><i>f</i><sub>60</sub></td><td>74</td><td><i>f</i><sub>65</sub></td></tr><tr><td><br />15</td><td><i>f</i><sub>30</sub></td><td>30</td><td><i>f</i><sub>34</sub></td><td>45</td><td><i>f</i><sub>70</sub></td><td>60</td><td><i>f</i><sub>63</sub></td><td>75</td><td><i>f</i><sub>71</sub></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="61" name="61">5.2 <b>二次特征排序实验结果</b></h4>
                <div class="p1">
                    <p id="62">本实验使用的输入数据为12个特征111维的特征矢量<citation id="121" type="reference"><link href="99" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.实验中用到的特征参数, <i>f</i><sub>1</sub>、<i>f</i><sub>2</sub>、<i>f</i><sub>3</sub>、<i>f</i><sub>4</sub>、<i>f</i><sub>5</sub>、<i>f</i><sub>6</sub>、<i>f</i><sub>7</sub>、<i>f</i><sub>8</sub>、<i>f</i><sub>9</sub>、<i>f</i><sub>10</sub>、<i>f</i><sub>11</sub>、<i>f</i><sub>12</sub>.得到的排序结果如下表所示, 次序的数字越小, 表明该参数的重要性越高<citation id="122" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="63">
                    <p class="img_tit"><b>表</b>4 <b>两个数据库特征排序的结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="63" border="1"><tr><td colspan="4"><br />EMO-DB数据库</td><td colspan="4">CASIA数据库</td></tr><tr><td>次序</td><td>特征</td><td>次序</td><td>特征</td><td>次序</td><td>特征</td><td>次序</td><td>特征</td></tr><tr><td><br />1</td><td><i>f</i><sub>1</sub> (8) </td><td>7</td><td><i>f</i><sub>3</sub> (7) </td><td>1</td><td><i>f</i><sub>7</sub> (10) </td><td>7</td><td><i>f</i><sub>12</sub> (10) </td></tr><tr><td><br />2</td><td><i>f</i><sub>4</sub> (5) </td><td>8</td><td><i>f</i><sub>8</sub> (5) </td><td>2</td><td><i>f</i><sub>11</sub> (9) </td><td>8</td><td><i>f</i><sub>5</sub> (6) </td></tr><tr><td><br />3</td><td><i>f</i><sub>5</sub> (5) </td><td>9</td><td><i>f</i><sub>9</sub> (7) </td><td>3</td><td><i>f</i><sub>1</sub> (9) </td><td>9</td><td><i>f</i><sub>3</sub> (5) </td></tr><tr><td><br />4</td><td><i>f</i><sub>6</sub> (9) </td><td>10</td><td><i>f</i><sub>11</sub> (4) </td><td>4</td><td><i>f</i><sub>4</sub> (10) </td><td>10</td><td><i>f</i><sub>8</sub> (10) </td></tr><tr><td><br />5</td><td><i>f</i><sub>7</sub> (7) </td><td>11</td><td><i>f</i><sub>10</sub> (7) </td><td>5</td><td><i>f</i><sub>6</sub> (9) </td><td>11</td><td><i>f</i><sub>9</sub> (10) </td></tr><tr><td><br />6</td><td><i>f</i><sub>2</sub> (7) </td><td>12</td><td><i>f</i><sub>12</sub> (10) </td><td>6</td><td><i>f</i><sub>2</sub> (9) </td><td>12</td><td><i>f</i><sub>10</sub> (10) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="64">根据以上的实验结果, 虽然训练时, BP神经网络中的初始值都不一样, 但实验中进行10次排序, 其结果“<i>f</i><sub>1</sub> (8) ”表示10次实验中有8次都是<i>f</i><sub>1</sub>, 而且排序混淆也只是在附近几个之间, 不会出现跳变, 所以, 即使BP神经网络中的初值不同, 得到的结果相对稳定, 说明此方法的鲁棒性较好.两个数据库特征排序得优先级虽然差别较大, 但排列在前9个的特征一致性还是很高的, 前9个特征两个数据库的交集含有8个相同的特征, 这8个共有特征为<i>f</i><sub>1</sub>、<i>f</i><sub>2</sub>、<i>f</i><sub>3</sub>、<i>f</i><sub>4</sub>、<i>f</i><sub>5</sub>、<i>f</i><sub>6</sub>、<i>f</i><sub>7</sub>、<i>f</i><sub>11</sub>, 而不同的是EMO-DB库包含EEMFCC特征, CASIA库包含PLP特征.且特征组合实验中, 特征的排序前后并不会影响识别率的高低.</p>
                </div>
                <div class="p1">
                    <p id="65">实验中根据特征排序的优先级, 设计组合特征实验, 排在后面的特征对我们的数据会产生冗余, 浪费运算时间, 使用SVM对其进行组合特征的识别, 得到识别结果为:在EMO-DB库中, 最高识别率85.59%, 在CASIA库中, 最高识别率74.75%.在EMO-DB库中, 两个数据库相同的8个特征得识别率为84.06%, 而其最优特征组合为9个特征.在8特征基础上加入EEMFCC特征, 识别率为85.59%, 在8特征基础上加入PLP特征, 识别率为81.51%.在CASIA库中, 两个数据库相同的8个特征得识别率为71.72%, 而其最优特征组合为9个特征.在8特征基础上加入PLP特征, 识别率为74.75%, 在8特征基础上加入EEMFCC特征, 识别率为70.20%.由以上分析可知, 两个库相同的8个特征的识别率都要高于其他组合, 而且特征组合实验中, 特征的排序前后并不会影响识别率得高低.所以混库实验中使用两个库交集的8个特征.该实验得出的特征排序结果与文献<citation id="123" type="reference">[<a class="sup">1</a>,<a class="sup">7</a>]</citation>中的结相一致, 说明了此方法的有效性.</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">5.3 <b>混库实验结果</b></h4>
                <div class="p1">
                    <p id="67">EMO-DB和CASIA数据库的情感标签有5个相同, 所以混库实验中用EMO-DB和CASIA两个数据库<citation id="125" type="reference"><link href="93" rel="bibliography" /><link href="105" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">8</a>]</sup></citation>.所以实验情感标签取5个相同的情感, 即1、快乐, 2、中性, 3、愤怒, 4、悲伤, 5、恐惧\\害怕.最终用于实验的样本即1 000句CASIA数据库的中文语料加上EMO-DB数据库的408句德语语料, 共1 408句语料<citation id="124" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.在实验中, 我们使用65%的语料库作为训练集, 35%的语料库作为测试集.识别机采用SVM.实验中使用两个库交集的8个特征子集, 特征子集包含<i>f</i><sub>1</sub>、<i>f</i><sub>2</sub>、<i>f</i><sub>3</sub>、<i>f</i><sub>4</sub>、<i>f</i><sub>5</sub>、<i>f</i><sub>6</sub>、<i>f</i><sub>7</sub>、<i>f</i><sub>11</sub>.</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201905004_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 混库识别率的分类矩阵" src="Detail/GetImg?filename=images/WXYJ201905004_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>混库识别率的分类矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201905004_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="69">从图2中看出, 混库实验中测试集的平均最高识别率为72.34%, 其中“快乐”和“恐惧\\害怕”情感的识别率较低, 分别为58.89%和55.06%, “快乐”情感识别率低, 是由于大量样本错误识别为“愤怒”和“恐惧”, 而“恐惧”情感识别率低是因为混库过程中, 我们把“scared”和“fear”的标签当作一类样本, 但情感是有细微差别, 所以第5类情感分类比较乱.</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">6 <b>结束语</b></h3>
                <div class="p1">
                    <p id="71">在本文中, 经过特征选择, 得到EMO-DB库最高识别率为85.59%, 得到CASIA库最高识别率为74.75%.李翔等人提出面向智能机器人的Teager语音情感系统设计, 其在EMO-DB情感语音数据库上7种情感的识别率为85.00%<citation id="126" type="reference"><link href="105" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.R. Lotfian 等人提出使用合成语音作为中立参考的情感识别, 其在CASIA情感语音数据库上6种情感标签的识别率为73.60%<citation id="127" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.特征选择使得实验不再是粗糙的根据韵律学特征、基于谱的相关特征、音质特征等类别进行随机组合.实验结果和引文的文章相比, 体现实验设计得优越性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="91">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=QHXB201804003&amp;v=MDkyMDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeWptV3IzSU5DWFRiTEc0SDluTXE0OUZaNFFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 宋鹏, 郑文明, 赵力.基于子空间学习和特征选择融合的语音情感识别[J].清华大学学报 (自然科学版) , 2018, 58 (4) :347-354.
                            </a>
                        </p>
                        <p id="93">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Emotion recognition using synthetic speech as neutral reference">

                                <b>[2]</b> LOTFIAN R, BUSSO C.Emotion recognition using synthetic speech as neutral reference[C]//2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Brisbane, QLD, 2015:4759-4763.doi:10.1109/ICASSP.2015.7178874.
                            </a>
                        </p>
                        <p id="95">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201608020&amp;v=MDQyMzdCdEdGckNVUkxPZVplVnVGeWptV3IzSU1qWFNaTEc0SDlmTXA0OUhaSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 陈俊, 王爱国, 王坤侠, 等.基于类依赖的语音情感特征选择[J].微电子学与计算机, 2016, 33 (8) :92-96.
                            </a>
                        </p>
                        <p id="97">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201507012&amp;v=MzE1NjZHNEg5VE1xSTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVdyM0lNalhTWkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 何淑琳, 张雪英, 孙颖, 等.基于极限学习机的语音情感识别[J].微电子学与计算机, 2015, 32 (7) :50-54.
                            </a>
                        </p>
                        <p id="99">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition based on improved MFCC">

                                <b>[5]</b> WANG Y.Speech emotion recognition based on improved MFCC[EB/OL]. (2018-08-06) .http://dl.acm.org.citation.cfm&gt;id=327837.
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN108154879A&amp;v=MTgxNDV4WVQ2em9PUzNmbXBXRmFlN0tWVEx1ZFpPZHVGeXZzVWI4PUppTzZIcnU1RzlYRXFJWTBDKzRQRDMxTHg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 郝梓岚一种基于倒谱分离信号得非特定人语音情感识别方法[p].中国专利:201711434048.3.
                            </a>
                        </p>
                        <p id="103">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on feature extraction of pathological voice">

                                <b>[7]</b> ZENG Y.Research on feature extraction of pathological voice [D].Guangxi :Guangxi Normal University, 2015.
                            </a>
                        </p>
                        <p id="105">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201308020&amp;v=MTAzMjNUYkxHNEg5TE1wNDlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlqbVdyM0lQRHo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 李翔, 李昕, 胡晨, 等.面向智能机器人的Teager语音情感交互系统设计与实现[J].仪器仪表学报, 2016, 34 (8) :1826-1833.
                            </a>
                        </p>
                        <p id="107">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end speech emotion recognition based on neutral network">

                                <b>[9]</b> ZHU B.End-to-end speech emotion recognition based on neutral network[C]//Proceedings of 2017 17th IEEE International Conference on Communication Technology (ICCT 2017) .IEEE, Beijing, 2017:5-7.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201905004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201905004&amp;v=MDIzNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5am1XcjNJTWpYU1pMRzRIOWpNcW85RllJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
