<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134026679318750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201904002%26RESULT%3d1%26SIGN%3d0wtib1FVoEO7v24V9O5XhcFx4Ds%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201904002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201904002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201904002&amp;v=MDAwNDFyQ1VSTE9lWmVWdUZ5em1WcnpPTWpYU1pMRzRIOWpNcTQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#25" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#27" data-title="2 &lt;b&gt;相关工作&lt;/b&gt; ">2 <b>相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#28" data-title="2.1 &lt;b&gt;卷积神经网络的工作原理&lt;/b&gt;">2.1 <b>卷积神经网络的工作原理</b></a></li>
                                                <li><a href="#47" data-title="2.2 &lt;b&gt;并行卷积神经网络&lt;/b&gt;">2.2 <b>并行卷积神经网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="3 &lt;b&gt;本文网络&lt;/b&gt; ">3 <b>本文网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="3.1 Crop&lt;b&gt;层简要介绍&lt;/b&gt;">3.1 Crop<b>层简要介绍</b></a></li>
                                                <li><a href="#56" data-title="3.2 &lt;b&gt;网络架构&lt;/b&gt;">3.2 <b>网络架构</b></a></li>
                                                <li><a href="#60" data-title="3.3 &lt;b&gt;模型设置&lt;/b&gt;">3.3 <b>模型设置</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="4 &lt;b&gt;实验&lt;/b&gt; ">4 <b>实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="4.1 &lt;b&gt;实验设置&lt;/b&gt;">4.1 <b>实验设置</b></a></li>
                                                <li><a href="#73" data-title="4.2 &lt;b&gt;实验结果及分析&lt;/b&gt;">4.2 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="5 &lt;b&gt;结束语&lt;/b&gt; ">5 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="&lt;b&gt;表&lt;/b&gt;1 cifar-10&lt;b&gt;上的分类错误率&lt;/b&gt;"><b>表</b>1 cifar-10<b>上的分类错误率</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;图&lt;/b&gt;1 PSC-CNN&lt;b&gt;结构&lt;/b&gt;"><b>图</b>1 PSC-CNN<b>结构</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;表&lt;/b&gt;2 PSC-AlexNet&lt;b&gt;的网络结构和参数设置&lt;/b&gt;"><b>表</b>2 PSC-AlexNet<b>的网络结构和参数设置</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;各模型在&lt;/b&gt;Caltech101&lt;b&gt;上的性能&lt;/b&gt;"><b>表</b>5 <b>各模型在</b>Caltech101<b>上的性能</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表&lt;/b&gt;6 &lt;b&gt;各模型在&lt;/b&gt;Caltech256&lt;b&gt;上的性能&lt;/b&gt;"><b>表</b>6 <b>各模型在</b>Caltech256<b>上的性能</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;不同模型在&lt;/b&gt;Caltech101&lt;b&gt;上准确率曲线&lt;/b&gt;"><b>图</b>2 <b>不同模型在</b>Caltech101<b>上准确率曲线</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;图&lt;/b&gt;3 &lt;b&gt;不同模型在&lt;/b&gt;Caltech256&lt;b&gt;上准确率曲线&lt;/b&gt;"><b>图</b>3 <b>不同模型在</b>Caltech256<b>上准确率曲线</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" HE K, ZHANG X, REN S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving Deep into Rectifiers:Surpassing Human-Level Performance on Imagenet Classification">
                                        <b>[1]</b>
                                         HE K, ZHANG X, REN S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" GLOROT X, BENGIO Y. Understanding the difficulty of training deep feedforward neural networks[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010: 249-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">
                                        <b>[2]</b>
                                         GLOROT X, BENGIO Y. Understanding the difficulty of training deep feedforward neural networks[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010: 249-256.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 汤鹏杰, 王瀚漓, 左凌轩.并行交叉的深度卷积神经网络模型[J].中国图象图形学报, 2016, 21 (3) :339-347." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201603008&amp;v=MTg0NjdlWmVWdUZ5em1WcnpCUHlyZmJMRzRIOWZNckk5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         汤鹏杰, 王瀚漓, 左凌轩.并行交叉的深度卷积神经网络模型[J].中国图象图形学报, 2016, 21 (3) :339-347.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" GUO L, LI F, LIEW A W C.Image esthetic evaluation using parallel deep convolution neural network[C]//Digital image computing:techniques and applications (DICTA) , 2016 international conference on.IEEE, 2016:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image esthetic evaluation using parallel deep convolution neural network">
                                        <b>[4]</b>
                                         GUO L, LI F, LIEW A W C.Image esthetic evaluation using parallel deep convolution neural network[C]//Digital image computing:techniques and applications (DICTA) , 2016 international conference on.IEEE, 2016:1-5.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 杨远飞, 曾上游, 冯燕燕, 等.基于并行和切片的深度卷积网络设计研究[J].微电子学与计算机, 2018, 35 (3) :120-124." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201803025&amp;v=MDYzMzR6cXFCdEdGckNVUkxPZVplVnVGeXptVnJ6Qk1qWFNaTEc0SDluTXJJOUhZWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         杨远飞, 曾上游, 冯燕燕, 等.基于并行和切片的深度卷积网络设计研究[J].微电子学与计算机, 2018, 35 (3) :120-124.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LONG J, SHELHAMER E, DARRELL T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">
                                        <b>[6]</b>
                                         LONG J, SHELHAMER E, DARRELL T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" KRIZEVSKY A, SUTSKEVER I, HINTON G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[7]</b>
                                         KRIZEVSKY A, SUTSKEVER I, HINTON G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HINTON G E, SALAKHUTDINOV R R. Reducing the dimensionality of data with neural networks[J]. science, 2006, 313 (5786) : 504-507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reducing the dimensionality of data with neural networks">
                                        <b>[8]</b>
                                         HINTON G E, SALAKHUTDINOV R R. Reducing the dimensionality of data with neural networks[J]. science, 2006, 313 (5786) : 504-507.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" RUMELHART D E, HINTON G E, WILLIAMS R J. Learning representations by back-propagating errors[J]. nature, 1986, 323 (6088) : 533." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning representations by back propagating errors">
                                        <b>[9]</b>
                                         RUMELHART D E, HINTON G E, WILLIAMS R J. Learning representations by back-propagating errors[J]. nature, 1986, 323 (6088) : 533.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[10]</b>
                                         HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" JIA Y, SHELHAMER E, DONAHUE J, et al. Caffe: Convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014: 675-678." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[11]</b>
                                         JIA Y, SHELHAMER E, DONAHUE J, et al. Caffe: Convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014: 675-678.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(04),6-11             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>并行尺度裁切的深度卷积神经网络模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E5%8F%8B%E6%96%87&amp;code=23595472&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄友文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%96%B9%E6%B0%B8%E5%B9%B3&amp;code=41465483&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">方永平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%A5%BF%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0217990&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江西理工大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对并行卷积神经网络 (PCNN) 的参数过多, 模型训练时间成本高的问题, 本文提出了并行尺度裁切卷积神经网络 (PSC-CNN) .PSC-CNN算法是将并行卷积神经网络其中一路 (Path A) 的输入及该通路的特征提取层的输出通过Crop层裁切得到新的尺寸的图像供给另一路 (Path B) 网络作为输入.这样, Path A的输入图像在数据层经过一次随机裁剪, Path B则经过了两次裁剪操作, 增加了输入数据的多样性, 提升了模型的学习能力.算法以AlexNet为基础网络, 分别设计了对应的PCNN及PSC-CNN模型, 在数据集Caltech101、Caltech256上进行实验.实验结果表明, 相较原始的PCNN, 本文提出的改进算法有效的提升了分类准确度同时缩短了训练时间.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B6%E8%A1%8C%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">并行卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E8%A3%81%E5%88%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度裁切;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=AlexNet&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">AlexNet;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    黄友文 男, (1982-) , 博士, 副教授.研究方向为计算机视觉, 图像处理与模式识别、嵌入式系统及应用.;
                                </span>
                                <span>
                                    *方永平 (通讯作者) 男, (1994-) , 硕士研究生.研究方向为深度学习.E-mail:2754172445@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>江西省教育厅科技资助项目 (GJJ150683);</span>
                    </p>
            </div>
                    <h1><b>Parallel Scale Cropping Deep Convolutional Neural Network Model</b></h1>
                    <h2>
                    <span>HUANG You-wen</span>
                    <span>FANG Yong-ping</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, Jiangxi University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>As the excessive parameters of parallel convolutional neural network (PCNN) and the high cost of model training time, this paper proposes a parallel scale cropping convolutional neural network (PSC-CNN) . The PSC-CNN algorithm is that obtains an input for one path (Path A) of the parallel convolutional neural network and an output of the feature extraction layer of the path, and then to get a new size image as input of the other path (Path B) network through the Crop layer. In this way, the input image of Path A undergoes a random cropping in the data layer, and Path B undergoes two cropping operations, which increases the data diversity and improves the learning ability. The algorithm is based on AlexNet, and the corresponding PCNN, PSC-CNN models are designed respectively. Experiments are carried out on the datasets Caltech101 and Caltech256. Experimental results show that compared with the original PCNN, the improved algorithm proposed in this paper effectively improves the classification accuracy and shortens the training time.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=parallel%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">parallel convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scale%20cropping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scale cropping;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=alexnet&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">alexnet;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-06</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="25" name="25" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="26">深度卷积神经网络 (DCNN) 是由卷积、池化等特征提取层堆叠起来的端到端的网络, 由低到高逐层提取特征, 最后通过分类器完成分类任务.与浅层网络相比, DCNN的权值共享策略使得网络参数减少了数个量级, 网络层数可以继续向下拓展.文献<citation id="87" type="reference">[<a class="sup">1</a>]</citation>认为虽然随着网络层数的增加, 模型识别性能会更好, 但是由于梯度弥散、梯度爆炸问题<citation id="88" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>的存在, 网络不能依靠简单地堆叠更深的层数以达到优化网络模型性能的目的.随着科研人员对神经网络结构的探索, 不仅纵向加深网络深度可以提高网络的识别性能, 横向拓展网络的宽度<citation id="92" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>也可以起到提高识别率的效果, 并且一定程度上减轻了梯度弥散带来的影响.虽然网络的识别性能得到提升, 但这是以成倍增加的网络参数为代价的.本文针对这个问题, 结合文献<citation id="89" type="reference">[<a class="sup">6</a>]</citation>中对特征图进行crop处理达到修正特征图大小的目的, 提出了并行尺度裁切的思想, 在AlexNet<citation id="90" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>网络的基础上, 设计了PSC-AleNet网络.在数据集Caltech101、Caltech256上进行分类实验, 本文设计的网络, 与对应的并行网络PCNN-AlexNet及文献<citation id="91" type="reference">[<a class="sup">3</a>]</citation>相比, 不仅缩短了训练时间, 还提升了识别准确度.</p>
                </div>
                <h3 id="27" name="27" class="anchor-tag">2 <b>相关工作</b></h3>
                <h4 class="anchor-tag" id="28" name="28">2.1 <b>卷积神经网络的工作原理</b></h4>
                <div class="p1">
                    <p id="29">卷积神经网络是一种多层前馈神经网络:由输入层、隐藏层及输出层构成, 其中隐藏层通常由卷积层及池化层 (降采样) 交替组成, 每层由多个相互独立的神经元构成的二维平面组成, 最后通过全连接层输出, 最终输出的维度对应网络识别的类别数.</p>
                </div>
                <div class="p1">
                    <p id="30">网络的训练是一种端到端的模式, 通过卷积运算逐层抽象获取不同层次语义信息, 并对数据集逐批次地训练从而调整各层卷积核的内部参数, 从而能够无监督地对图像特征进行学习<citation id="93" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.网络的整个训练过程在前向传播及反向传播算法下完成的.在前向传播中, 输入图像通过卷积、池化操作不断获取新的特征向量, 前一层的输出被当作当前层的输入.</p>
                </div>
                <div class="p1">
                    <p id="31" class="code-formula">
                        <mathml id="31"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>×</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="32">式中, <i>l</i>表示的是网络层数;<i>x</i><mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>表示的是第<i>l</i>-1层神经元<i>i</i>的输出;<i>n</i>个输出信号同时输入进神经元<i>j</i>, <i>w</i><mathml id="34"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示的是第<i>l</i>层的第<i>i</i>个神经元连接到第<i>l</i>-1个神经元的权重, <i>b</i><mathml id="35"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>表示第<i>l</i>层的第<i>i</i>个神经元的偏置;<i>y</i><sub><i>j</i></sub>为神经元<i>j</i>的输出.每个神经元在参与完卷积运算之后都会经过激活函数<i>f</i> (常用的非线性激活函数有sigmoid、tanh、ReLu函数等) , 得到非线性输出, 实现特征信息的非线性转换, 能够有效地解决线性分类难以完成的分类任务.经过多个特征提取 (卷积、池化等) 层, 然后光栅化 , 将进入全连接层前一层的像素点依次展开, 排成一列.最后进入分类层 , 采用的是Softmax函数, 函数表达式为</p>
                </div>
                <div class="p1">
                    <p id="36" class="code-formula">
                        <mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>e</mi><msup><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></msubsup><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="37">式中, <mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math></mathml>对概率分布归一化, 分类结果之和为1, <i>T</i>是总类别数, <i>j</i>是判定的类别.当一个样本经过Softmax层并输出一个<i>T</i>*1的向量时, 取最大的值时的<i>j</i>作为这个样本的预测标签.</p>
                </div>
                <div class="p1">
                    <p id="39">网络中各个神经元参数<i>w</i><sub><i>ij</i></sub>是随机的, 所以只进行一次前向运算不能很好地完成分类任务, 随机性强.Rumelhart等人<citation id="94" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出由误差反向传播算法训练多层前馈网络即反向传播网络 (Back Propagation Network) .反向传播算法是一种无监督算法, 当网络输出与期望值之间存在差异, 误差通过梯度下降算法逐层向前传播, 逐层更新网络参数的值, 最终缩小实际输出与期望值之间的差异.公式如下:</p>
                </div>
                <div class="p1">
                    <p id="40"><i>L</i> (<i>w</i>, <i>b</i>;<mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>w</mi><mo>, </mo><mi>b</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="42">式 (3) 表示的是损失函数.式中, <i>x</i><sub><i>i</i></sub>是第<i>i</i>类样本特征;<i>h</i><sub><i>w</i>, <i>b</i></sub> (<i>x</i><sub><i>i</i></sub>) 是第<i>i</i>类样本经过网络的输出;<i>y</i><sub><i>i</i></sub>是第<i>i</i>类样本的期望输出.目的是让损失函数尽可能的小, 采用梯度下降算法来求出每个参数的偏导值, 并更新每一层的权重<i>w</i>和偏置项<i>b</i>.</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false"> (</mo><mi>w</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>b</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo>=</mo><mi>b</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false"> (</mo><mi>w</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mo stretchy="false"> (</mo><mi>b</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">式中, <i>η</i>表示的是学习率, 其值越大参数更新的幅度越大.网络通过前向传播、误差反向传播及梯度下降算法, 不断调整网络权重、偏置等参数, 直至识别准确率曲线收敛.但根据实验对象的不同, 可能存在过拟合的风险, 解决办法是在SGD算法中加入一个正则项对这个损失函数进行约束:</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>E</mi></mrow><mrow><mo>∂</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></mfrac><mo>-</mo><mi>η</mi><mspace width="0.25em" /><mi>λ</mi><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">式中, <i>λ</i>是正则化系数 (weight decay) ;<i>η λw</i><sub><i>i</i></sub>是损失函数的正则项, 起到约束的作用, 使得需要更新的权重值与其大小成比例衰减.</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">2.2 <b>并行卷积神经网络</b></h4>
                <div class="p1">
                    <p id="48">卷积神经网络的网络结构对识别性能起到决定性的影响, 一般情况下, 深层网络相较于浅层网络, 识别效果更好, 增加网络的层数可以起到提升模型识别性能的作用, 但是网络的层数过多, 由于梯度弥散的问题存在, 导致靠近输入层的隐藏层的梯度小, 参数更新慢.虽然文献<citation id="95" type="reference">[<a class="sup">10</a>]</citation>在一定程度上克服了这种问题带来的影响, 但是如果网络层数过深还是会导致准确度下降.部分实验数据如下<citation id="96" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="49">
                    <p class="img_tit"><b>表</b>1 cifar-10<b>上的分类错误率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="49" border="1"><tr><td><br />方法</td><td>层数</td><td>错误率 (%) </td></tr><tr><td rowspan="2"><br />Highway</td><td><br />19</td><td>7.54</td></tr><tr><td><br />32</td><td>8.80</td></tr><tr><td rowspan="4"><br />ResNet</td><td><br />32</td><td>7.51</td></tr><tr><td><br />56</td><td>6.97</td></tr><tr><td><br />110</td><td>6.43</td></tr><tr><td><br />1202</td><td>7.93</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="50">Highway Network在32层时候的错误率比在19层更高, ResNet虽然能加深到110层, 并且保持错误率下降的趋势, 但是当网络层数继续加深, 仍然存在识别性能下降的可能.相关科研人员提出并行卷积神经网络, 为优化神经网络模型的性能提供一个研究方向.</p>
                </div>
                <div class="p1">
                    <p id="51">并行卷积神经网络是分别通过两条通道对输入图像提取特征, 在全连接层把两条通路的输出特征图融合在一起, 将其作为最后的特征向量, 通过Softmax函数完成分类任务.</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">3 <b>本文网络</b></h3>
                <h4 class="anchor-tag" id="53" name="53">3.1 Crop<b>层简要介绍</b></h4>
                <div class="p1">
                    <p id="54">在深度学习框架Caffe (Convolutional architecture forfast feature embedding) 中<citation id="97" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 数据是以Blobs的形式在各层之间输入输出的.Blobs是四维连续数组, 即 (<i>n</i>, <i>k</i>, <i>w</i>, <i>h</i>) , 分别表示的是图像个数, 通道个数, 图像宽度及图像高度.</p>
                </div>
                <div class="p1">
                    <p id="55">Crop层的输入有两个, 分别记作Blob<sub>A</sub>, Blob<sub>B</sub>, 一个输出记作Blob<sub>C</sub>.其中Blob<sub>A</sub>是需要裁切的输入, Blob<sub>B</sub>是裁切的参考输入, Blob<sub>C</sub>是最后裁切的新的数据.即以Blob<sub>B</sub>为模板在Blob<sub>A</sub>上裁切一个尺寸与Blob<sub>B</sub>相同的输出Blob<sub>C</sub>.Crop层有两个重要的参数axis, offsets.其中axis决定从Blobs的哪个轴开始裁剪, offsets决定裁剪的位置.设Blob<sub>A</sub>= (<i>n</i><sub><i>a</i></sub>, <i>k</i><sub><i>a</i></sub>, <i>w</i><sub><i>a</i></sub>, <i>h</i><sub><i>a</i></sub>) , Blob<sub>B</sub>= (<i>n</i><sub><i>b</i></sub>, <i>k</i><sub><i>b</i></sub>, <i>w</i><sub><i>b</i></sub>, <i>h</i><sub><i>b</i></sub>) , Blob<sub>C</sub>= (<i>n</i><sub><i>c</i></sub>, <i>k</i><sub><i>c</i></sub>, <i>w</i><sub><i>c</i></sub>, <i>h</i><sub><i>c</i></sub>) , 若axis=1, 保留轴0, 则<i>n</i><sub><i>c</i></sub>=<i>n</i><sub><i>b</i></sub>=<i>n</i><sub><i>a</i></sub>;offsets= (<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, <i>s</i><sub>3</sub>) , 则<i>k</i><sub><i>c</i></sub>=<i>k</i><sub><i>b</i></sub>, <i>w</i><sub><i>c</i></sub>=<i>w</i><sub><i>b</i></sub>, <i>h</i><sub><i>c</i></sub>=<i>h</i><sub><i>b</i></sub>, 裁切的位置及尺寸分别为[<i>s</i><sub>1</sub>, <i>s</i><sub>1</sub>+<i>k</i><sub><i>b</i></sub>], [<i>s</i><sub>2</sub>, <i>s</i><sub>2</sub>+<i>w</i><sub><i>b</i></sub>], [<i>s</i><sub>3</sub>, <i>s</i><sub>3</sub>+<i>h</i><sub><i>b</i></sub>].</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56">3.2 <b>网络架构</b></h4>
                <div class="p1">
                    <p id="57">较单路卷积神经网络, 并行卷积神经网络的参数量成倍增加, 虽然模型识别性能得到提升, 但增加了训练时间成本.本文依此设计了并行尺度裁切网络模型, 在减少参数的同时, 还能提供不同的输入数据给网络, 提升了模型的学习能力和泛化性.网络结构如图1.</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904002_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 PSC-CNN结构" src="Detail/GetImg?filename=images/WXYJ201904002_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 PSC-CNN<b>结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904002_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">网络存在两条网络通路, Path A和Path B.输入图像像素是256×256的, 首先在数据层进行随机Crop处理, 截取成224×224大小.Path A的输入像素为224×224保持不变, 在隐藏层经过一系列的卷积、池化等操作, 当特征图大小为112×112时, 将其与数据层共同通过Crop层<citation id="98" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 将224×224的数据层图像像素裁切成112×112, 并供给Path B作为B通路的输入.即利用第一个通路的某一层的输出, 在数据层裁切得到另一路通路的输入, 这样Path B通路的输入图像经历了两次裁切操作 , 不仅整个模型输入的数据更具多样性, 提升了模型的学习能力, 且另一路网络的神经元个数也将减少, 加速模型的收敛.</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">3.3 <b>模型设置</b></h4>
                <div class="p1">
                    <p id="61">本文选取AlexNet网络作为Path A通路的网络, 然后做出适当改动, 构建Path B通路的网络.改造后的网络 (PSC-AlexNet) 参数设置如表2.</p>
                </div>
                <div class="p1">
                    <p id="62">Path A的Conv1层输入是Data层输出, 大小为224 ×224;Path B的Conv1层输入是Crop输出, 以Conv1_A层的输出尺寸为模板, 将Data层的输出裁切成大小为112×112的图像供给Path B网络训练.目的是让Path B的输入特征图矩阵大小为Path A通路的一半.由于AlexNet原网络中的第一个卷积层卷积步长设置为4, 网络在第一次卷积之后, 特征图矩阵直接缩小到55×55, 需要对网络进行改动, 本文将原来Conv1拆成两个卷积模块Conv1和Conv1’进行级联, 参数都设置为7×7/2.然后根据Conv1_A层的形状信息对Data层的输出进行裁切, 在Data层裁切一个112×112的图像供给Path B通路.这样Path B的输入图像经历了两次随机裁切, 相较原网络, 输入数据更具多样性, 并且由于输入图像尺寸变小, 整个双路网络的计算量将降低.</p>
                </div>
                <div class="area_img" id="63">
                    <p class="img_tit"><b>表</b>2 PSC-AlexNet<b>的网络结构和参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="63" border="1"><tr><td rowspan="2">Layer name</td><td colspan="2"><br />Path A</td><td rowspan="2">Layer name</td><td colspan="2"><br />Path B</td></tr><tr><td><br />K/S</td><td>F/N<sub>out</sub></td><td><br />K/S</td><td>F/N<sub>out</sub></td></tr><tr><td>Data</td><td>--</td><td>224×224/3</td><td>Data</td><td>--</td><td>224×224/3</td></tr><tr><td><br />--</td><td>--</td><td>--</td><td>Crop</td><td>--</td><td>112×112/96</td></tr><tr><td><br />Conv1_A</td><td>7×7/2</td><td>112×112/64</td><td>Conv1_B</td><td>7×7/2</td><td>56×56/64</td></tr><tr><td><br />Conv1_A’</td><td>7×7/2</td><td>55×55/96</td><td>Conv1_B’</td><td>7×7/2</td><td>27×27/96</td></tr><tr><td><br />Max pool1/LRN</td><td>3×3/2</td><td>27×27/96</td><td>Max pool1/LRN</td><td>3×3/2</td><td>13×13/96</td></tr><tr><td><br />Conv2_A</td><td>5×5/1</td><td>27×27/256</td><td>Conv2_B</td><td>5×5/1</td><td>13×13/256</td></tr><tr><td><br />Max pool2/LRN</td><td>3×3/2</td><td>13×13/256</td><td>Max pool2/LRN</td><td>3×3/2</td><td>6×6/256</td></tr><tr><td><br />Conv3_A</td><td>3×3/1</td><td>13×13/384</td><td>Conv3_B</td><td>3×3/1</td><td>6×6/384</td></tr><tr><td><br />Conv4_A</td><td>3×3/1</td><td>13×13/384</td><td>Conv4_B</td><td>3×3/1</td><td>6×6/384</td></tr><tr><td><br />Conv5_A</td><td>3×3/1</td><td>13×13/256</td><td>Conv5_B</td><td>3×3/1</td><td>6×6/256</td></tr><tr><td><br />Max pool3/LRN</td><td>3×3/2</td><td>6×6/256</td><td>Max pool3/LRN</td><td>3×3/2</td><td>3×3/256</td></tr><tr><td><br />FC1_A</td><td>--</td><td>256×6×6/2 048</td><td>FC1_B</td><td>--</td><td>256×3×3/2 048</td></tr><tr><td><br />FC2_A</td><td>--</td><td>4 096×2 048</td><td>FC2_B</td><td>--</td><td>4 096×2 048</td></tr><tr><td><br />FC3_A</td><td>--</td><td>4 096×1 024</td><td>FC3_B</td><td>--</td><td>4 096×1 024</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">4 <b>实验</b></h3>
                <h4 class="anchor-tag" id="65" name="65">4.1 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="66">实验在深度学习框架caffe<citation id="99" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>环境下进行.实验环境如下:</p>
                </div>
                <div class="p1">
                    <p id="67">①Intel (R) Core (TM) 3.91 GHz;</p>
                </div>
                <div class="p1">
                    <p id="68">②内存8 GB;</p>
                </div>
                <div class="p1">
                    <p id="69">③显卡NVIDIAGeForce GTX1070;</p>
                </div>
                <div class="p1">
                    <p id="70">④Ubuntu 16.04×64.</p>
                </div>
                <div class="p1">
                    <p id="71">本文实验是在计算机视觉领域较为经典的数据库Caltech101和Caltech256中进行的.Caltech101包含101个实物类和一个背景干扰类共9144幅图像, 其中最少的包含31幅, 最多包含800多幅图像, 将其中7 254幅图像作为训练集, 1 890幅图像构作为测试集.Caltech256是Caltech101数据库的扩展, 共30 607幅图像, 包含256个实物类和一个背景干扰类, 将其中24 323幅图像作为训练集, 剩下的6284幅图像作为测试集, 相较Caltech101数据库, 该数据库不仅类别和图像更多, 它在物体的位置、大小和形态上差异更明显, 识别难度更大.</p>
                </div>
                <div class="p1">
                    <p id="72">在训练之前, 首先将数据库中所有图像的尺寸统一为256×256像素.在训练和测试时, 在数据层对每幅图像随机裁切成224×224像素 (测试时只裁切图像中间部分) , 并对其进行水平翻转.本文实验中所有网络的学习率变化方式选择‘fixed’模式, 设置为0.001, 迭代次数为20万次, gamma为0.1, weight decay为0.001, Batch size设置为20.</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">4.2 <b>实验结果及分析</b></h4>
                <div class="p1">
                    <p id="74">文献<citation id="100" type="reference">[<a class="sup">6</a>]</citation>设计了一个并行卷积神经网络, 并在该数据库Caltech101和Caltech256上做了图像分类的研究工作.本文将并行尺度裁切卷积神经网络算法应用在AlexNet的并行网络上, 通过与文献<citation id="101" type="reference">[<a class="sup">6</a>]</citation>、单路AlexNet及并行AlexNet网络作对比, 验证本文算法的可行性.</p>
                </div>
                <div class="area_img" id="75">
                    <p class="img_tit"><b>表</b>5 <b>各模型在</b>Caltech101<b>上的性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="75" border="1"><tr><td><br />网络模型</td><td>准确率 (Top1) /%</td><td>训练时间/s</td></tr><tr><td><br />文献6</td><td>63</td><td>--</td></tr><tr><td><br />AlexNet</td><td>70.26</td><td>7 697</td></tr><tr><td><br />PCNN-AlexNet</td><td>72.15</td><td>33 616</td></tr><tr><td><br />PSC-AlexNet</td><td>73.42</td><td>23 059</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表</b>6 <b>各模型在</b>Caltech256<b>上的性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td><br />网络模型</td><td>准确率 (Top1) /%</td><td>训练时间/s</td></tr><tr><td><br />文献6</td><td>46.4</td><td>--</td></tr><tr><td><br />AlexNet</td><td>45.91</td><td>8 229</td></tr><tr><td><br />PCNN-AlexNet</td><td>48.91</td><td>34 873</td></tr><tr><td><br />PSC-AlexNet</td><td>50.37</td><td>24 013</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="77">表5, 表6给出了不同卷积神经网络模型在Caltech101和Caltech256上的识别准确率, 以及模型的训练时间.从中可以看出, 基于AlexNet的并行卷积神经网络 (PCNN-AlexNet) 和并行尺度裁切卷积神经网络 (PSC-AlexNet) 的识别准确度要高于传统网络AlexNet, 且PSC-AlexNet的训练时间比PCNN-AlexNet更少.可以看出, 在Caltech101上, PSC-AlexNet算法的Top1分类准确度达到了73.42%, 其值相对AlexNet, PCNN-AlexNet分别提升了4.58%, 1.76%, 且训练时间相对PCNN-AlexNet缩短了45.78%.在Caltech256上, PSC-AlexNet的Top1达到了50.37%, 比AlexNet, PCNN-AlexNet分别提高了近9.71%, 2.98%, 且训练时间较PCNN-AlexNet缩短了45.22%.</p>
                </div>
                <div class="p1">
                    <p id="78">另外, 文献<citation id="102" type="reference">[<a class="sup">6</a>]</citation>的分类准确度均比本文设计的PSC-AlexNet算法低, 甚至在数据集Caltech101上, 其Top1识别率不及本文的单路AlexNet网络.原因可能有:</p>
                </div>
                <div class="p1">
                    <p id="79"> (1) 文献<citation id="103" type="reference">[<a class="sup">6</a>]</citation>为防止过拟合发生将Dropout ratio设为0.9 (本文设为0.5) .通过实验发现过拟合的预防, 可以通过调节Weight decay来缓解, 不需要舍弃更多的神经元输出;</p>
                </div>
                <div class="p1">
                    <p id="80"> (3) 训练集和测试集的分配方式不同 ;</p>
                </div>
                <div class="p1">
                    <p id="81"> (3) Weight decay等一些超参设置不同.</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904002_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同模型在Caltech101上准确率曲线" src="Detail/GetImg?filename=images/WXYJ201904002_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>不同模型在</b>Caltech101<b>上准确率曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904002_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904002_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同模型在Caltech256上准确率曲线" src="Detail/GetImg?filename=images/WXYJ201904002_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 <b>不同模型在</b>Caltech256<b>上准确率曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904002_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="84">图2, 图3中横坐标是网络迭代次数, 每迭代10000次进行一次测试, 共20个点, 纵坐标是测试分类准确度.利用Caffe自带的绘图工具, 绘制准确度曲线.从中可以看出, 本文网络算法在刚开始训练时, 识别准确度并不高, 甚至在Caltech256上出现过分类准确度低于单路网络的情况, 模型迭代到3万次左右开始超过AlexNet, 迭代到9万次开始超过并行网络PCNN-AlexNet.综合表4、表5、图2和图3可以得出, 本文设计的网络算法比并行网络的性能更优, 不仅识别准确度更高, 且模型训练的时间更短.</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag">5 <b>结束语</b></h3>
                <div class="p1">
                    <p id="86">本文针对并行卷积神经网络参数过多, 实验复杂度高的问题提出了并行尺度裁切思想, 设计了并行尺度裁切卷积神经网络 (PSC-CNN) .在数据集Caltech101和Caltech256上进行实验, 选择AlexNet作为基础网络, 设计了并行网络PCNN-AlexNet和并行尺度裁切网络PSC-AlexNet.实验发现, 本文在并行网络上加入本文提出的算法之后, 在数据集Caltech101和Caltech256上性能均得到提升.分类准确度分别提升了1.76%、2.98%, 训练时间分别缩短了45.78%、45.22%.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving Deep into Rectifiers:Surpassing Human-Level Performance on Imagenet Classification">

                                <b>[1]</b> HE K, ZHANG X, REN S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the difficulty of training deep feedforward neural networks">

                                <b>[2]</b> GLOROT X, BENGIO Y. Understanding the difficulty of training deep feedforward neural networks[C]//Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010: 249-256.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201603008&amp;v=MjY1MDBGckNVUkxPZVplVnVGeXptVnJ6QlB5cmZiTEc0SDlmTXJJOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 汤鹏杰, 王瀚漓, 左凌轩.并行交叉的深度卷积神经网络模型[J].中国图象图形学报, 2016, 21 (3) :339-347.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image esthetic evaluation using parallel deep convolution neural network">

                                <b>[4]</b> GUO L, LI F, LIEW A W C.Image esthetic evaluation using parallel deep convolution neural network[C]//Digital image computing:techniques and applications (DICTA) , 2016 international conference on.IEEE, 2016:1-5.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201803025&amp;v=MTkxNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5em1WcnpCTWpYU1pMRzRIOW5Nckk5SFlZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 杨远飞, 曾上游, 冯燕燕, 等.基于并行和切片的深度卷积网络设计研究[J].微电子学与计算机, 2018, 35 (3) :120-124.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">

                                <b>[6]</b> LONG J, SHELHAMER E, DARRELL T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[7]</b> KRIZEVSKY A, SUTSKEVER I, HINTON G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reducing the dimensionality of data with neural networks">

                                <b>[8]</b> HINTON G E, SALAKHUTDINOV R R. Reducing the dimensionality of data with neural networks[J]. science, 2006, 313 (5786) : 504-507.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning representations by back propagating errors">

                                <b>[9]</b> RUMELHART D E, HINTON G E, WILLIAMS R J. Learning representations by back-propagating errors[J]. nature, 1986, 323 (6088) : 533.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[10]</b> HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[11]</b> JIA Y, SHELHAMER E, DONAHUE J, et al. Caffe: Convolutional architecture for fast feature embedding[C]//Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014: 675-678.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201904002" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201904002&amp;v=MDAwNDFyQ1VSTE9lWmVWdUZ5em1WcnpPTWpYU1pMRzRIOWpNcTQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
