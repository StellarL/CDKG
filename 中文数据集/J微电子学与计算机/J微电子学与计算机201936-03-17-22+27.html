<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134042326818750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201903004%26RESULT%3d1%26SIGN%3dUN0TG8JRETyrpw8nn5xgoCUGH54%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201903004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201903004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201903004&amp;v=MDAwNDFyQ1VSTE9lWmVWdUZ5emhVcjNMTWpYU1pMRzRIOWpNckk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="2 &lt;b&gt;自然场景统计模型&lt;/b&gt; (NSS) &lt;b&gt;与高斯尺度混合模型&lt;/b&gt; (GSM)  ">2 <b>自然场景统计模型</b> (NSS) <b>与高斯尺度混合模型</b> (GSM) </a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="3 &lt;b&gt;互补色小波变换&lt;/b&gt; (&lt;i&gt;Complementary Color&lt;/i&gt;&lt;i&gt;Wavelet Transform&lt;/i&gt;, &lt;i&gt;CCWT&lt;/i&gt;)  ">3 <b>互补色小波变换</b> (<i>Complementary Color</i><i>Wavelet Transform</i>, <i>CCWT</i>) </a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="4 &lt;b&gt;基于互补色小波&lt;/b&gt;DNT&lt;b&gt;域的显著图模型&lt;/b&gt; ">4 <b>基于互补色小波</b>DNT<b>域的显著图模型</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="5 &lt;b&gt;互补色自然场景统计显著图模型性能评估与分析&lt;/b&gt; ">5 <b>互补色自然场景统计显著图模型性能评估与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#146" data-title="6 &lt;b&gt;结束语&lt;/b&gt; ">6 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;互补色关系图&lt;/b&gt;"><b>图</b>1 <b>互补色关系图</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;互补色小波&lt;/b&gt;CCWTs&lt;b&gt;对应的经典小波&lt;/b&gt;, &lt;b&gt;每列对应&lt;/b&gt;&lt;i&gt;n&lt;/i&gt;=&lt;i&gt;kπ&lt;/i&gt;/8, &lt;i&gt;k&lt;/i&gt;=1, 2…, 8&lt;b&gt;中的一个方向&lt;/b&gt;, &lt;b&gt;每行对应&lt;/b&gt;&lt;i&gt;θ&lt;/i&gt;=0, 2&lt;i&gt;π&lt;/i&gt;/3, 4&lt;i&gt;π&lt;/i&gt;/3&lt;b&gt;中的一种相位&lt;/b&gt;"><b>图</b>2 <b>互补色小波</b>CCWTs<b>对应的经典小波</b>, <b>每列对应</b><i>n</i>=<i>kπ</i>/8, <i>k</i>=1, 2…, 8<b>中的一个方向</b>, <b>每行对应</b><i>θ</i>=0, 2<i>π</i>/3, 4<i>π</i>/3<b>中的一种相位</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;图&lt;/b&gt;3 &lt;b&gt;不同显著图模型所得&lt;/b&gt;&lt;i&gt;ROC&lt;/i&gt;&lt;b&gt;曲线对比&lt;/b&gt;"><b>图</b>3 <b>不同显著图模型所得</b><i>ROC</i><b>曲线对比</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;i&gt;Bruce&lt;/i&gt;&lt;b&gt;和&lt;/b&gt;&lt;i&gt;ImgSal&lt;/i&gt;&lt;b&gt;数据库上不同方法的&lt;/b&gt;&lt;i&gt;AUC&lt;/i&gt;&lt;b&gt;值&lt;/b&gt;"><b>表</b>1 <i>Bruce</i><b>和</b><i>ImgSal</i><b>数据库上不同方法的</b><i>AUC</i><b>值</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;图&lt;/b&gt;4 5&lt;b&gt;幅彩色图像 (左) ;用&lt;/b&gt;&lt;i&gt;cNSSSal&lt;/i&gt;&lt;b&gt;所得的显著图 (中) ;用&lt;/b&gt;&lt;i&gt;CCNSSal&lt;/i&gt;&lt;b&gt;所得的显著图 (右&lt;/b&gt;) "><b>图</b>4 5<b>幅彩色图像 (左) ;用</b><i>cNSSSal</i><b>所得的显著图 (中) ;用</b><i>CCNSSal</i><b>所得的显著图 (右</b>) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" TREISMAN A M, GELADE G. A feature-integration theory of attention[J].Cognitive psychology, 1980, 12 (1) : 97-136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A feature integration theory of attention">
                                        <b>[1]</b>
                                         TREISMAN A M, GELADE G. A feature-integration theory of attention[J].Cognitive psychology, 1980, 12 (1) : 97-136.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" KOCH C, ULLMAN S. Shifts in selective visual attention: towards the underlying neural circuitry[M]. Springer, Dordrecht, Matters of intelligence, 1987: 115-141." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shifts in selective visual-attention:Towards the Underlying Neural Circuitry">
                                        <b>[2]</b>
                                         KOCH C, ULLMAN S. Shifts in selective visual attention: towards the underlying neural circuitry[M]. Springer, Dordrecht, Matters of intelligence, 1987: 115-141.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" ITTI L, KOCH C, NIEBUR E. A model of saliency-based visual attention for rapid scene analysis[J]. IEEE Transactions on pattern analysis and machine intelligence, 1998, 20 (11) : 1254-1259." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A model of saliency-based visual attention for rapid scene analysis">
                                        <b>[3]</b>
                                         ITTI L, KOCH C, NIEBUR E. A model of saliency-based visual attention for rapid scene analysis[J]. IEEE Transactions on pattern analysis and machine intelligence, 1998, 20 (11) : 1254-1259.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" WALTHER D, KOCH C. Modeling attention to salient proto-objects[J]. Neural networks, 2006, 19 (9) : 1395-1407." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300069646&amp;v=MTg0NjdHZXJxUVRNbndaZVp1SHlqbVViL0lKRm9WYUJFPU5pZk9mYks3SHRET3JJOUZaTzBHQ25nL29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         WALTHER D, KOCH C. Modeling attention to salient proto-objects[J]. Neural networks, 2006, 19 (9) : 1395-1407.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" HAREL J, KOCH C, PERONA P. Graph-based visual saliency[C]//Advances in neural information processing systems. New York, 2006: 545-552." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph-based visual saliency">
                                        <b>[5]</b>
                                         HAREL J, KOCH C, PERONA P. Graph-based visual saliency[C]//Advances in neural information processing systems. New York, 2006: 545-552.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" BRUCE N, TSOTSOS J. Saliency based on information maximization[C]//Advances in neural information processing systems. New York, 2006: 155-162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency Based on Information Maximization">
                                        <b>[6]</b>
                                         BRUCE N, TSOTSOS J. Saliency based on information maximization[C]//Advances in neural information processing systems. New York, 2006: 155-162.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" ZHANG L, TONG M H, MARKS T K, et al. SUN: A Bayesian framework for saliency using natural statistics[J]. Journal of vision, 2008, 8 (7) : 32-32." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SUN: A Bayesian framework for saliency using natural statistics">
                                        <b>[7]</b>
                                         ZHANG L, TONG M H, MARKS T K, et al. SUN: A Bayesian framework for saliency using natural statistics[J]. Journal of vision, 2008, 8 (7) : 32-32.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HOU X, ZHANG L. Saliency detection: A spectral residual approach[C]//Computer Vision and Pattern Recognition, CVPR′07. IEEE Conference on. Minneapolis, Minnesota, USA. IEEE, 2007: 1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency Detection: A Spectral Residual Ap-proach">
                                        <b>[8]</b>
                                         HOU X, ZHANG L. Saliency detection: A spectral residual approach[C]//Computer Vision and Pattern Recognition, CVPR′07. IEEE Conference on. Minneapolis, Minnesota, USA. IEEE, 2007: 1-8.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" YU Y, WANG B, ZHANG L. Pulse discrete cosine transform for saliency-based visual attention[C]//Development and Learning, ICDL 2009. IEEE 8th International Conference on. Shanghai, China. IEEE, 2009: 1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pulse discrete cosine transform for saliency-based visual attention">
                                        <b>[9]</b>
                                         YU Y, WANG B, ZHANG L. Pulse discrete cosine transform for saliency-based visual attention[C]//Development and Learning, ICDL 2009. IEEE 8th International Conference on. Shanghai, China. IEEE, 2009: 1-6.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" HOU X, HAREL J, KOCH C. Image signature: Highlighting sparse salient regions[J]. IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (1) : 194-201." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Signature: Highlighting Sparse Salient Regions">
                                        <b>[10]</b>
                                         HOU X, HAREL J, KOCH C. Image signature: Highlighting sparse salient regions[J]. IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (1) : 194-201.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" WAINWRIGHT M J, SIMONCELLI E P. Scale mixtures of Gaussians and the statistics of natural images[C]//Advances in neural information processing systems. United States, Colorado, 2000: 855-861." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale Mixtures of Gaussians and the Statistics of Natural Images">
                                        <b>[11]</b>
                                         WAINWRIGHT M J, SIMONCELLI E P. Scale mixtures of Gaussians and the statistics of natural images[C]//Advances in neural information processing systems. United States, Colorado, 2000: 855-861.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" LI Q, WANG Z. Reduced-reference image quality assessment using divisive normalization-based image representation[J]. IEEE journal of selected topics in signal processing, 2009, 3 (2) : 202-211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reduced-reference image quality assessment using divisive normalization-based image representation">
                                        <b>[12]</b>
                                         LI Q, WANG Z. Reduced-reference image quality assessment using divisive normalization-based image representation[J]. IEEE journal of selected topics in signal processing, 2009, 3 (2) : 202-211.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" PORTILLA J, STRELA V, WAINWRIGHT M J, et al. Image denoising using scale mixtures of Gaussians in the wavelet domain[J]. IEEE Transactions on Image processing, 2003, 12 (11) : 1338-1351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image denoising using scale mixtures of Gaussians in the wavelet domain">
                                        <b>[13]</b>
                                         PORTILLA J, STRELA V, WAINWRIGHT M J, et al. Image denoising using scale mixtures of Gaussians in the wavelet domain[J]. IEEE Transactions on Image processing, 2003, 12 (11) : 1338-1351.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 黄虹, 张建秋. 彩色自然场景统计显著图模型[J]. 复旦学报: 自然科学版, 2014, 59 (1) : 51-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FDXB201401007&amp;v=MDYzMzR6cXFCdEdGckNVUkxPZVplVnVGeXpoVXIzTEl5blRiTEc0SDlYTXJvOUZZNFFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         黄虹, 张建秋. 彩色自然场景统计显著图模型[J]. 复旦学报: 自然科学版, 2014, 59 (1) : 51-58.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" CHEN Y, LI D, ZHANG J Q. Complementary color wavelet: a novel tool for the color image/video analysis and processing[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2017, 29 (3) :1-3. DOI: 10.1109/TCSVT.2017.2776239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Complementary color wavelet: a novel tool for the color image/video analysis and processing">
                                        <b>[15]</b>
                                         CHEN Y, LI D, ZHANG J Q. Complementary color wavelet: a novel tool for the color image/video analysis and processing[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2017, 29 (3) :1-3. DOI: 10.1109/TCSVT.2017.2776239.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" MA Q, ZHANG L. Saliency-based image quality assessment criterion[C]//International Conference on Intelligent Computing. Springer, Berlin, Heidelberg, 2008: 1124-1133." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency-based image quality assessment criterion">
                                        <b>[16]</b>
                                         MA Q, ZHANG L. Saliency-based image quality assessment criterion[C]//International Conference on Intelligent Computing. Springer, Berlin, Heidelberg, 2008: 1124-1133.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" PRIDMORE R W. Complementary colors theory of color vision: Physiology, color mixture, color constancy and color perception[J]. Color Research &amp;amp; Application, 2011, 36 (6) : 394-412." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Complementary colors theory of color vision: Physiology, color mixture, color constancy and color perception">
                                        <b>[17]</b>
                                         PRIDMORE R W. Complementary colors theory of color vision: Physiology, color mixture, color constancy and color perception[J]. Color Research &amp;amp; Application, 2011, 36 (6) : 394-412.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" PRIDMORE R W. Complementary colors: the structure of wavelength discrimination, uniform hue, spectral sensitivity, saturation, chromatic adaptation, and chromatic induction[J]. Color Research &amp;amp; Application, 2009, 34 (3) : 233-252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Complementary colors: The structure of wavelength discrimination, uniform hue spectral sensitivity, saturation, chromaticadaptation, and chromatic induction">
                                        <b>[18]</b>
                                         PRIDMORE R W. Complementary colors: the structure of wavelength discrimination, uniform hue, spectral sensitivity, saturation, chromatic adaptation, and chromatic induction[J]. Color Research &amp;amp; Application, 2009, 34 (3) : 233-252.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(03),17-22+27             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>互补色小波域自然场景统计显著图模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%89%AC&amp;code=06702993&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈扬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%BB%BA%E7%A7%8B&amp;code=06708035&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张建秋</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0075855&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复旦大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统显著图 (Saliency map) 通常基于灰度图像+彩色拮抗辅助通道的模型。其未能整体充分考虑颜色通道之间、颜色与方向等显著性要素之间关系.为了克服这样的缺点, 本文将人眼视觉有重要作用的互补色理论引入小波设计, 提出一种基于自然场景的高斯尺度混合模型 (Gaussian scale mixture, GSM) 及其分区归一化变换 (Divisive normalization transformation, DNT) 的彩色整体显著图模型.实验结果表明, 该模型较其他同类模型有显著优越性, 特别在处理色彩丰富的场景时能大幅提高与人眼视觉机制一致性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E8%91%97%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显著图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E7%BB%9F%E8%AE%A1%E5%88%86%E5%B8%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然场景统计分布;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小波变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%92%E8%A1%A5%E8%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">互补色;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈扬, 男, (1982-) , 博士研究生.研究方向为多尺度滤波与图像处理.E-mail:lB110720040@fudan.edu.cn.;
                                </span>
                                <span>
                                    张建秋, 男, (1962-) , 教授、博士生导师.研究方向为包括信息处理应用、图像处理等.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61571131);</span>
                    </p>
            </div>
                    <h1><b>A natural scene statistical saliency map model in complementary color wavelet domain</b></h1>
                    <h2>
                    <span>CHEN Yang</span>
                    <span>ZHANG Jian-qiu</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Technology, Fudan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional saliency map models are usually based on grayscale images and auxiliary color antagonism channels, not carefully considering the relations of the key saliency elements such as relations among color channels and orientations. To overcome this disadvantage, in this paper, we introduce the complementary color theory into the wavelet designs and propose a holistic color saliency map model based on the Gaussian scale mixture (GSM) of natural scene statistics and its Divisive normalization transformation (DNT) in the wavelet domain. The experimental results show that our model achieves better consistency with the visual attention mechanism of the human eyes, especially in processing colorful scenes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=saliency%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">saliency map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=natural%20scene%20statistics&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">natural scene statistics;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=wavelet%20transform&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">wavelet transform;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=complementary%20colors&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">complementary colors;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="40">智能地预测提取图像视频中人眼感兴趣的区域是数字图像处理的一个研究热点, 这个问题称为人眼视觉注意力机制的显著性检测 (Saliency detection) 问题.人眼视觉有自底向上 (bottom-up) 和自顶向下 (top-down) 两种注意方式.绝大多数的自底向上模型来自于特征整合理论 (Feature-integration theory, FIT) <citation id="148" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.该理论认为视觉注意力系统对各种刺激如颜色、方向、亮度、纹理等做出单独反应, 而后将各个层次的度量整合成统一的显著性度量.Koch在此基础上提出了适合计算的视觉注意力框架<citation id="149" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.Itti根据该方法考虑图像的颜色、方向、亮度在不同尺度下的特征, 首次提出了有深远影响力的视觉注意力计算模型<citation id="150" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.基于Itti经典的方法整理改进后形成基于空域对比度的STB (Saliency Tool Box) 算法<citation id="151" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>;基于带权重的马尔科夫链产生局部活跃图产生GBVS<citation id="152" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>方法;基于信息熵理论与独立成分分析 (ICA) 的AIM方法<citation id="153" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>;基于贝叶斯推理的SUN方法<citation id="154" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等.Hou通过频域滤波去除低频变换提出了基于频谱的残差模型<citation id="155" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.Yu利用脉冲余弦变换PCT (Pulse Discrete Cosine Transform) 的系数的符号来全局地计算显著性<citation id="156" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 也称为signature的图像描述子 (signature-based Saliency map, signatureSal) , 有着比其他方法更高的一致性<citation id="157" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="41">自然场景统计模型 (Natural Scene Statistics, NSS) 是人眼视觉机制的一个高效而广泛应用的模型.NSS的统计模型可以用自然场景高斯尺度混合模型 (Gaussian scale mixture, GSM) 来描述<citation id="158" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>.GSM采用方向性的多分辨率金字塔算法将图像分解为多尺度不同方向的子带并做区分归一化变换 (Divisive normalization transformation, DNT) <citation id="159" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.DNT被证实与人眼视觉系统对能量分配的原则一致, 对偶于人眼进化过程中受能量消耗限制所进行的自身进化优化, 能很好地解释视神经反应对视觉环境变化的机理, 并很好解释视觉掩蔽效应<citation id="160" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.GSM模型采用了统一的一个计算框架 (多方向多尺度统计建模) 来描述人眼视觉机制并得到与FIT相同的刺激与抑制等机理, 可以在统一框架下更好描述Itti等传统方法中的多方向、全局-局部对比、中心-周围抑制的模型.正因GSM对人眼视觉系统的精确建模, 使其成为图像去噪<citation id="161" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、图像质量评估<citation id="162" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>方面经典的算法, 并在显著图领域取得了较以往同类方法更好的效果<citation id="163" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="42">然而, 目前GSM等所采用的多尺度小波图像处理方法都是基于灰度图像的.当其应用于彩色图像处理时, 不同的彩色通道往往采用完全相同的小波滤波器, 而后将各个通道的处理结果直接相加作为最终结果.在这种情况下, FIT框架中的颜色对比分量难以全面实现.本文提出采用文献<citation id="164" type="reference">[<a class="sup">15</a>]</citation>的互补色小波来直接建模NSS的彩色图像显著图.通过实验结果证明, 通过采用互补色小波来改善传统小波在彩色拮抗对通道的表现, 并基于模拟视觉显著性的DNT系数的模型, 可以有效提高显著图模型与HVS认知的一致性.公开数据库上计算显著图模型ROC (Receiver Operating Characteristics) 与ROC曲线下面积 (Area Under the Curve, AUC) 定量地显示我们方法较以往其他同类方法的优越性.</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">2 <b>自然场景统计模型</b> (NSS) <b>与高斯尺度混合模型</b> (GSM) </h3>
                <div class="p1">
                    <p id="44">不同的自然图像, 由于场景、光线、内容等的不同有着千差万别的表现.但与人工生成的图像如卡通、文字等, 仍有着人眼能瞬间分辨的区别.对自然图像建模并寻找区别于其他类型图像的不变性特征, 正是自然场景统计模型 (NSS) 所要实现的目标.</p>
                </div>
                <div class="p1">
                    <p id="45">根据NSS模型统计特性, 自然图像小波变换系数的概率分布可以通过高斯尺度混合模型 (GSM) 来描述<citation id="165" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">15</a>]</sup></citation>.自然图像的小波系数可以表示为<mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mi>d</mi></mover><mi>z</mi><mi>u</mi><mo>‚</mo><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mi>d</mi></mover></mrow></math></mathml>表示等分布.其中<i>Y</i>为小波系数矢量, <i>z</i>为系数方差乘数, <i>u</i>是0均值、协方差<i>C</i><sub><i>u</i></sub>的高斯随机变量.即将<i>Y</i>表示为无限个高斯矢量<i>u</i>以密度<i>p</i><sub><i>z</i></sub> (<i>z</i>) 混合的结果</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mi>Y</mi></msub><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow></mrow></mrow></mstyle></mrow><msubsup><mrow></mrow><mn>0</mn><mi>∞</mi></msubsup><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>π</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mi>Ν</mi><mo>/</mo><mn>2</mn></mrow></msup><mrow><mo>|</mo><mrow><mi>z</mi><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold">Y</mi><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mo> (</mo><mrow><mi>z</mi><mi>C</mi><msub><mrow></mrow><mi>U</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">Y</mi></mrow><mo>) </mo></mrow><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mi>d</mi><mi>z</mi><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="48">小波系数<i>Y</i>关于<i>z</i>服从协方差<i>zC</i><sub><i>u</i></sub>的高斯分布, 也就是在每一个邻域内符合高斯分布<citation id="166" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">15</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">|</mo><mi>z</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>π</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mi>Ν</mi><mo>/</mo><mn>2</mn></mrow></msup><mrow><mo>|</mo><mrow><mi>z</mi><mi>C</mi><msub><mrow></mrow><mi>U</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>Y</mi><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mrow><mo> (</mo><mrow><mi>z</mi><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mi>Y</mi></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">由此对自然图像小波系数进行的GSM建模, 可以由小波系数邻域矢量和协方差矩阵估计出混合尺度<i>z</i>.在每一个邻域内混合尺度<i>z</i>的最大似然估计为</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>z</mi><mo>^</mo></mover><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mi>z</mi></munder><mrow><mo>{</mo><mrow><mtext>l</mtext><mtext>o</mtext><mtext>g</mtext><mi>p</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">|</mo><mi>z</mi><mo stretchy="false">) </mo></mrow><mo>}</mo></mrow></mtd></mtr><mtr><mtd><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>z</mi></munder><mrow><mo>{</mo><mrow><mi>Ν</mi><mi>log</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Y</mi><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mrow><mo> (</mo><mrow><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow><mi>Y</mi><mo>/</mo><mn>2</mn><mi>z</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>}</mo></mrow></mtd></mtr><mtr><mtd><mo>=</mo><msqrt><mrow><mi>Y</mi><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mo> (</mo><mrow><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>Y</mi><mo>/</mo><mi>Ν</mi></mrow></msqrt><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">式中, <i>C</i><sub><i>u</i></sub>是子带内所有领域矢量所估算得到的协方差, 已预先求得估计值.要求得一个邻域内的混合尺度系数<mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>z</mi><mo>^</mo></mover></math></mathml>, 只需由该邻域的系数矢量<i>Y</i>按式计算即可.</p>
                </div>
                <div class="p1">
                    <p id="54">将每一个邻域内的系数<i>Y</i>对乘数<i>z</i>做归一化, 可以得到归一化的单位协方差高斯分布</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo>/</mo><mover accent="true"><mi>z</mi><mo>^</mo></mover><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mn>2</mn><mi>π</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mi>Ν</mi><mo>/</mo><mn>2</mn></mrow></msup><mrow><mo>|</mo><mrow><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></mfrac></mtd></mtr><mtr><mtd><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>Y</mi><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mo> (</mo><mrow><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>Y</mi></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">将每一个邻域矢量<i>Y</i>的中心点<i>y</i><sub><i>c</i></sub>对其乘数<mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>z</mi><mo>^</mo></mover></math></mathml>做归一化, 定义为分区归一化变换 (Divisive normalization transformation, DNT) , <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>z</mi><mo>^</mo></mover></math></mathml>为归一化因子, 系数<mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo>=</mo><mi>y</mi><msub><mrow></mrow><mi>c</mi></msub><mo>/</mo><mover accent="true"><mi>z</mi><mo>^</mo></mover></mrow></math></mathml>为归一化后的系数, 该系数符合单位方差的高斯分布.</p>
                </div>
                <div class="p1">
                    <p id="60">图像小波GSM模型的DNT在一个统一框架下描述与FIT相同的刺激与抑制等机理.根据DNT与视觉生理的相关性, 能引起人类视觉注意的场景与物体即是能引起视觉能量变化最显著的区域, 对偶于统计模型中偏离视觉通常统计分布的点.而DNT模型中<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>z</mi><mo>^</mo></mover></math></mathml>表征了邻域矢量<i>Y</i>与视觉通常统计模型<i>d</i> (符合单位高斯分布) 之间的偏差程度, 代表了该邻域的显著性特征.<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>z</mi><mo>^</mo></mover></math></mathml>的值越大, 则该邻域矢量偏离中心分布越远, 其引起视觉能量变化越显著, 视觉显著性越高<citation id="167" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.由于<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>z</mi><mo>^</mo></mover></math></mathml>表征的是特定领域统计与通常统计协方差之间的距离, 也被称为<i>Mahalanobis</i>距离, 在传统小波的灰度图像显著图中取得了良好的效果<citation id="168" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">3 <b>互补色小波变换</b> (<i>Complementary Color</i><i>Wavelet Transform</i>, <i>CCWT</i>) </h3>
                <div class="p1">
                    <p id="65">为了克服传统小波等图像工具缺乏颜色信息、割裂颜色通道联系所带来的问题, 根据视觉理论中重要的互补色理论<citation id="170" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>在文献<citation id="169" type="reference">[<a class="sup">15</a>]</citation>提出了一种新型的彩色图像多尺度分析工具—互补色小波.</p>
                </div>
                <div class="p1">
                    <p id="66">互补色关系可以用RGB三色系统及其色环方便表示, 如图1 (a) 所示.可以看出RGB三轴分别位于色环上的0, 2π/3, 4π/3方向.任意π相位差的两种颜色形成一对互补色.在其中, 沿轴的正是在人眼视觉和颜色感知方面具有重要作用的四对互补色, 红-青, 绿-品红, 蓝-黄和黑-白<citation id="171" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="67">为了将RGB色环上位于0, 2<i>π</i>/3和4<i>π</i>/3方向的颜色轴拓展到小波域以得到彩色图像处理的便利, 我们在文献<citation id="172" type="reference">[<a class="sup">15</a>]</citation>设计了一族相对相位分别为0, 2<i>π</i>/3和4<i>π</i>/3的一维互补色小波<i>ψ</i><sup>0</sup>, <i>ψ</i><sup>2<i>π</i>/3</sup>和<i>ψ</i><sup>4<i>π</i>/3</sup>.如图1 (<i>b</i>) 所示, 我们可以看出这族小波基结合了色环对应的2<i>π</i>/3相位关系, 以及小波框架下良好的能量集中性 (由小波的紧支撑特性而来) , 并可以很容易提取互补色相关的颜色变化信息.</p>
                </div>
                <div class="p1">
                    <p id="68">在水平与垂直二个维度上分别进行一维互补色小波分解并将所得的各个相位分量排列组合成二维分量, 筛选非0二维分量得到近似八个方向<i>n</i>=<i>kπ</i>/8, <i>k</i>=1, 2…, 8和三种相位<i>θ</i>=0, 2<i>π</i>/3, 4<i>π</i>/3的二维小波组, 如图2所示.从图中我们可以看出, 二维互补色小波的八个子带分别近似于指向<i>nπ</i>/8, <i>n</i>=1, 2, …, 8方向, 可以比传统小波更好地提取二维图像的方向信息.</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903004_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 互补色关系图" src="Detail/GetImg?filename=images/WXYJ201903004_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>互补色关系图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903004_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903004_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 互补色小波CCWTs对应的经典小波, 每列对应n=kπ/8, k=1, 2…, 8中的一个方向, 每行对应θ=0, 2π/3, 4π/3中的一种相位" src="Detail/GetImg?filename=images/WXYJ201903004_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>互补色小波</b>CCWTs<b>对应的经典小波</b>, <b>每列对应</b><i>n</i>=<i>kπ</i>/8, <i>k</i>=1, 2…, 8<b>中的一个方向</b>, <b>每行对应</b><i>θ</i>=0, 2<i>π</i>/3, 4<i>π</i>/3<b>中的一种相位</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903004_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="72">将互补色小波层级<i>j</i>方向<i>n</i>的2<i>π</i>/3相位差小波<i>ψ</i><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>0</mn><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>ψ</i><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>2</mn><mi>π</mi><mo>/</mo><mn>3</mn><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>和<i>ψ</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>4</mn><mi>π</mi><mo>/</mo><mn>3</mn><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>分别表示为<i>ψ</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>ψ</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>和<i>ψ</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, 对彩色图像各个彩色通道进行小波分解得到层级<i>j</i>方向<i>n</i>的互补色小波系数矢量<i>d</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>=<i>r</i>*<i>ψ</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>d</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>=<i>g</i>*<i>ψ</i><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>和<i>d</i><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>=<i>b</i>*<i>ψ</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, 其中<i>r</i>, <i>g</i>和<i>b</i>是彩色图像的通道矢量而*代表卷积;而后通过定义的强度算子<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msup><mrow></mrow><mi>Ι</mi></msup><mo stretchy="false"> (</mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>d</mi><msup><mrow></mrow><mi>R</mi></msup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>d</mi><msup><mrow></mrow><mi>G</mi></msup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>d</mi><msup><mrow></mrow><mi>B</mi></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>, 黑-白互补色算子<i>O</i><sup><i>C</i></sup> (<i>d</i>) =<i>d</i><sup><i>R</i></sup>+<i>d</i><sup><i>G</i></sup>+<i>d</i><sup><i>B</i></sup>, 红-青互补色算子<i>O</i><sup><i>R</i></sup> (<i>d</i>) =<i>d</i><sup><i>R</i></sup>-<i>d</i><sup><i>G</i></sup>-<i>d</i><sup><i>B</i></sup>, 绿-品红互补色算子<i>O</i><sup><i>G</i></sup> (<i>d</i>) =<i>d</i><sup><i>G</i></sup>-<i>d</i><sup><i>R</i></sup>-<i>d</i><sup><i>B</i></sup>, 蓝-黄互补色算子<i>O</i><sup><i>B</i></sup> (<i>d</i>) =<i>d</i><sup><i>B</i></sup>-<i>d</i><sup><i>R</i></sup>-<i>d</i><sup><i>G</i></sup>, 可求得各个方向的红-青, 绿-品红, 蓝-黄、黑-白特征系数<citation id="173" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.上述的算子运算可以写为:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>C</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="left"><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="left"><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="left"><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>d</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="left"><mi>d</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr><mtr><mtd columnalign="left"><mi>d</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">关于互补色特征系数详细的特性及相比传统小波的优点, 可以参考我们的文献<citation id="174" type="reference">[<a class="sup">15</a>]</citation>.</p>
                </div>
                <div class="p1">
                    <p id="88">将颜色和方向的变化对人眼造成的刺激性统一在同一个算子框架下, 直接计算得出特定方向的颜色变化强度这样的信息, 最为符合互补色和FIT理论<citation id="175" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>所做关于人眼视觉的研究结论.传统小波基于灰度图像, 难以直接实现统一框架.而我们的互补色小波依据人眼视觉互补色理论中互补色关系对于色彩感知的重要地位<citation id="176" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>, 将颜色、亮度、方向、尺度等变化统一成多尺度方向性的互补色统一度量, 很好地反映了各种刺激源互相之间的对立统一关系及人眼对于客观视觉变化的感知.</p>
                </div>
                <div class="p1">
                    <p id="89">基于互补色理论中各组互补色在人眼视觉产生等量刺激<citation id="177" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>, 而显著性检测等应用需要求取各组互补色特征的综合描述.因此求得各个互补色特征系数后, 我们可以进一步用来求取层级<i>j</i>方向<i>n</i>的颜色变化的统一表征.</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mtext>Τ</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext><mo>, </mo><mi>n</mi></mrow></msubsup><mo>=</mo><mrow><mo>|</mo><mrow><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>C</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>Ο</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow><mo>|</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">4 <b>基于互补色小波</b>DNT<b>域的显著图模型</b></h3>
                <div class="p1">
                    <p id="92">DNT模型在传统小波的灰度图像显著图中取得了良好的效果<citation id="178" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.然而用于彩色图像显著图时, 只是通过添加彩色辅助通道来进行扩展, 未能整体考虑、理论联系各通道之间关系.为克服这个缺点, 本文将互补色理论引入小波DNT模型, 从而提出整体的彩色图像显著性检测方法.</p>
                </div>
                <div class="p1">
                    <p id="93">自然图像的互补色小波系数很好符合DNT模型<citation id="179" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.四对互补色红-青、绿-品红、蓝-黄与黑-白在人眼视觉显著性中具有同样重要的地位<citation id="180" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>.因此在对各个互补色小波子带及各个方向采取等权重融合的方式后, 可以将基于灰度图像的DNT显著图模型拓展为彩色模型.在小波尺度层级方面, 采用三个尺度可以接近提取近、中、远及不同大小的目标<citation id="181" type="reference"><link href="7" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">14</a>]</sup></citation>.我们同样采用三个尺度 (4～6) 来做整体融合.</p>
                </div>
                <div class="p1">
                    <p id="94">具体的算法的具体实施细节如下.</p>
                </div>
                <div class="p1">
                    <p id="95">将一幅彩色图像<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><msub><mrow></mrow><mi>m</mi></msub><mrow><mo> (</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>m</mi></msub><mo>∈</mo><mi>R</mi><msup><mrow></mrow><mrow><mi>R</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><mo>) </mo></mrow></mrow></math></mathml>进行4层级的二维互补色小波分解并提取互补色特征子带<citation id="182" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 可以得到4个层级, 每个层级8个方向的黑-白、红-青、绿-品红与蓝-黄互补色特征子带<i>O</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>C</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>O</i><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>O</i><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>O</i><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml> (<i>j</i>=1, 2, 3, 4, <i>n</i>=<i>kπ</i>/8, <i>k</i>=1, 2, …, 8) .层级<i>j</i>方向<i>n</i>的任意互补色子带<i>O</i><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>大小均为<i>R</i><sub><i>j</i></sub>×<i>C</i><sub><i>j</i></sub>, 其中<i>R</i><sub><i>j</i></sub>=<i>R</i>/2<sup><i>j</i></sup>, <i>C</i><sub><i>j</i></sub>=<i>C</i>/2<sup><i>j</i></sup>.</p>
                </div>
                <div class="p1">
                    <p id="102">要对每一个子带进行GSM建模并求其DNT归一化因子.首考虑任意子带内邻域.记子带<i>O</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>中坐标<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>的系数点为<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>, 选取以该点为中心的一个3×3邻域的九个点<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mover accent="true"><mi>r</mi><mo>˜</mo></mover><mo>, </mo><mover accent="true"><mi>c</mi><mo>˜</mo></mover></mrow><mo>) </mo></mrow></mrow><mo>}</mo></mrow></mrow></math></mathml>, 以及该坐标其他7个方向子带的系数点<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mover accent="true"><mi>n</mi><mo>˜</mo></mover></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mrow><mover accent="true"><mi>n</mi><mo>˜</mo></mover><mo>=</mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>8</mn><mo>, </mo><mover accent="true"><mi>n</mi><mo>˜</mo></mover><mo>≠</mo><mi>n</mi></mrow></msub></mrow></math></mathml>组成一个相对应于<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>的16×1邻域矢量<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>.依据GSM模型, 该邻域矢量的这些点是服从方差<i>zC</i><sub><i>u</i></sub>的高斯分布的, 其中<i>z</i>是对应于该邻域矢量方差大小的变量, <i>C</i><sub><i>u</i></sub>是整个子带整体的协方差矩阵, 可以通过子带内所有系数估算得出, 是预先求得的常量.</p>
                </div>
                <div class="p1">
                    <p id="110">完成子带邻域矢量的GSM建模后, 即可使用式 (3) 最大似然估计每个邻域的DNT归一化因子.即层级<i>j</i>方向<i>n</i>的某一互补色子带<i>O</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>中坐标<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>系数点所对应DNT归一化因子为</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>z</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><msqrt><mrow><mrow><mrow><mo> (</mo><mrow><mi>Y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mo> (</mo><mrow><mi>C</mi><msub><mrow></mrow><mi>u</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>Y</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup><mrow><mo> (</mo><mrow><mi>r</mi><mo>, </mo><mi>c</mi></mrow><mo>) </mo></mrow><mo>/</mo><mi>Ν</mi></mrow></msqrt></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>Ν</mi><mo>=</mo><mn>1</mn><mn>6</mn><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">将整个子带所有点的DNT归一化因子写为矢量形式, 记为<i>Z</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>g</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, 代表了层级<i>j</i>方向<i>n</i>相应互补色变化对人眼视觉所产生的显著性特征.根据之前所述的融合规则, 我们将各个尺度层级、各个方向的四对互补色特征系数所得<i>DNT</i>归一化因子融合得到互补色自然场景统计显著图模型 (Complementary Color Natural Scene Statistical Saliency map, CCNSSal) </p>
                </div>
                <div class="area_img" id="117">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/WXYJ201903004_11700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="118">式中, <i>Z</i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>C</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>Z</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>R</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>Z</i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>G</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>, <i>Z</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mi>B</mi><mo>, </mo><mi>n</mi></mrow></msubsup></mrow></math></mathml>分别是黑-白、红-青、绿-品红与蓝-黄互补色特征子带DNT归一化因子矢量, ∑<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mn>8</mn></msubsup></mrow></math></mathml><i>g</i>是求取这些矢量在8个方向上的和;<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>⊕</mo><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>4</mn></mrow><mn>6</mn></msubsup><mrow><mo> (</mo><mi>g</mi><mo>) </mo></mrow></mrow></math></mathml>是将各个尺度层级所求得的这些和进行融合, 由于尺度不同, 各个层级矢量大小不一, 因此需对各个层级其做相应比例插值扩展到与原图尺寸<i>R</i>×<i>C</i>大小相同后相加;<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mrow><mo>{</mo><mi>g</mi><mo>}</mo></mrow></mrow></math></mathml>为高斯模糊核, 在大部分显著图模型中用于对最终显著图进行平滑<citation id="183" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="126">CCNSSal模型依据人眼视觉生理的互补色理论的基于GSM的DNT模型解释了颜色、亮度、方向对人类视觉感知显著性的统一根源及掩蔽效应等视觉机制, 符合人眼视觉生理基础的重要研究结论, 综合颜色、亮度、方向之间不可分割的相互关系, 以统一的框架描述图像对人眼视觉刺激的显著性分布.</p>
                </div>
                <div class="p1">
                    <p id="127">下面的实验可以看出我们方法在检测人眼视觉显著性区域的优越性.</p>
                </div>
                <h3 id="128" name="128" class="anchor-tag">5 <b>互补色自然场景统计显著图模型性能评估与分析</b></h3>
                <div class="p1">
                    <p id="129">我们选取近期主流的与CCNSSal同为空频域变换/多尺度分析的同类方法进行比较, 它们是Itti&amp;Koch<citation id="184" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, PFTSal<citation id="185" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, signatureSal<citation id="186" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, NSSSal<citation id="187" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.Itti&amp;Koch在FIT理论和Koch&amp;Ullman框架下, 将对人类视觉刺激显著性的图像颜色、亮度、方向变化特征建模并采用多尺度相减的方法来模拟中心-周围抑制以实现模拟视觉生理机制.PFTSal将图像进行二维傅里叶变换, 提取其相位谱图特征得到显著图模型, signatureSal则是基于离散余弦变换PCT得到图像描述子得到的显著图模型.NSSSal是基于灰度图像自然场景统计的显著图模型, 采用过完备的方向性金字塔分解小波系数的统计模型.这些显著图模型涵盖了图像空频域/多尺度分析中最主流的傅里叶变换FFT、离散余弦变换DCT、小波变换WT等算法, 代表了该类算法的前沿表现.</p>
                </div>
                <div class="p1">
                    <p id="130">将通过公开的Bruce数据库与ImgSal数据库测试比较, 对它们进行显著图ROC (Receiver Operating Characteristics) 曲线与AUC (Area Under the Curve, ROC曲线下的面积) 定量评估.</p>
                </div>
                <div class="p1">
                    <p id="131">一方面, 归一化后的显著图<i>S</i>是与原图大小相同, 每个像素点值介于[0, 1]的标注吸引人眼注意程度的热度图.某个像素点值越大标注了该像素点越吸引人眼注意.另一方面, 在Bruce和ImgSal标准数据库中都提供了多位人类测试者实际圈出的认为吸引他们注视的区域, 将这些区域{0, 1}二值化后所得到的标定真实数据 (Ground Truth) <i>S</i><sub><i>G</i></sub>.为了将显著图像素点的连续数值与标准数据库的离散二值做比较得出近似程度, 需要ROC曲线做阈值评价标准.即以某个阈值<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>∈</mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>, </mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>对显著图S进行二值化得到二值化后的显著图S<sub>B</sub>, 大于该值化为正类 (<i>True Positive</i>) , 小于该值为负类 (<i>Negative Positive</i>) .阈值越小, 固然能识别出更多正类 (预测正确<i>Ground Truth</i>中那些正类点) , 但同时将更多的负类当做正类 (预测错误<i>Ground Truth</i>中那些负类点) .前者称为正类率 (<i>TPR</i>, <i>True Positive Rate</i>) :</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Τ</mtext><mtext>Ρ</mtext><mtext>R</mtext><mo>=</mo><mfrac><mrow><mrow><mo stretchy="false">∥</mo><mi>S</mi><msub><mrow></mrow><mi>B</mi></msub><mo>*</mo><mi>S</mi><msub><mrow></mrow><mi>G</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mrow><mo stretchy="false">∥</mo><mi>S</mi><msub><mrow></mrow><mi>G</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">式中, *表示像素点对位相乘;下标1表示1范数也就是所有元素绝对值之和 (在此等效于1元素的个数) .</p>
                </div>
                <div class="p1">
                    <p id="135">后者称为虚警率 (<i>FPR</i>, <i>False Positive Rate</i>) :</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>F</mtext><mtext>Ρ</mtext><mtext>R</mtext><mo>=</mo><mfrac><mrow><mrow><mo stretchy="false">∥</mo><mi>S</mi><msub><mrow></mrow><mi>B</mi></msub><mo>*</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi>S</mi><msub><mrow></mrow><mi>G</mi></msub></mrow><mo>) </mo></mrow><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mrow><mo stretchy="false">∥</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi>S</mi><msub><mrow></mrow><mi>G</mi></msub></mrow><mo>) </mo></mrow><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">选取不同的阈值取得不同的 (<i>FPR</i>, <i>TPR</i>) 坐标点, 将<i>FPR</i>作为横轴<i>TPR</i>作为纵轴所画出的曲线称为<i>ROC</i>曲线, 通过计算<i>ROC</i>曲线下面积<i>AUC</i>得到显著图模型与人眼实际注视分布的相符程度.<i>AUC</i>值越接近1, 显著图模型与人眼视觉实际注意力的一致性越高.</p>
                </div>
                <div class="p1">
                    <p id="138">图3为<i>ImgSal</i>和<i>Bruce</i>数据库上<i>Itti</i>&amp;<i>Koch</i>、<i>PFTSal</i>、<i>signatureSal</i>、<i>cNSSSal</i>, 以及我们的<i>CCNSSal</i>模型运行得出的<i>ROC</i>曲线, 表1为对应的<i>AUC</i>.从图和表中我们可以看出基于<i>FFT</i>的<i>PFTSal</i>与<i>DCT</i>的<i>signatureSal</i>结果接近, 都较<i>Itti</i>&amp;<i>Koch</i>方法好.<i>cNSSSal</i>基于小波系数自然统计模型并扩展至彩色图像处理, 结果领先于这三种方法.而我们的<i>CCNSSal</i>综合考虑各尺度颜色、方向等方面与视觉生理的显著性的关系, 结果则领先于其他方法, 显示出我们模型与人类视觉生理一致性更高.</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903004_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同显著图模型所得ROC曲线对比" src="Detail/GetImg?filename=images/WXYJ201903004_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 <b>不同显著图模型所得</b><i>ROC</i><b>曲线对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903004_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="141">在<i>Bruce</i>数据库测试中我们的模型领先较多, 这与<i>Bruce</i>数据库色彩更为丰富, 色彩对比提供更大显著性, 引入互补色小波提升色彩识别度有关.这也印证了互补色理论描述人眼视觉机制的有效性.</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表</b>1 <i>Bruce</i><b>和</b><i>ImgSal</i><b>数据库上不同方法的</b><i>AUC</i><b>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td><br /><i>AUC</i></td><td><i>Bruce</i></td><td><i>ImgSal</i></td></tr><tr><td><br /><i>Itti</i>&amp;<i>Koch</i></td><td>0.722 1</td><td>0.736 1</td></tr><tr><td><br /><i>PFTSal</i></td><td>0.712 6</td><td>0.748 0</td></tr><tr><td><br /><i>signat ureSal</i></td><td>0.715 3</td><td>0.721 0</td></tr><tr><td><br /><i>cNSSSal</i></td><td>0.743 6</td><td>0.795 1</td></tr><tr><td><br /><i>CCNSSal</i></td><td><b>0.802 1</b></td><td><b>0.819 4</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="143">基于互补色理论的模型不仅引入了颜色关系, 且将其融入整体框架中与方向、尺度等共同建模从而提高了模型准确性, 而非大多已有的框架那样只是单独添加颜色、明暗对比等单独通道而未考虑与方向尺度等的关系.</p>
                </div>
                <div class="p1">
                    <p id="144">通过数据库中经典的几幅测试图来比较我们的<i>CCNSSal</i>模型与其他模型得到的显著图.为公平起见, 我们选取其他模型中结果最好的<i>cNSSSal</i>模型<citation id="188" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>所示例的全部彩色图片来进行比较.图4显示了彩色图像用<i>CCNSSal</i>与<i>cNSSSal</i>所得的显著图.从图中我们可以看出, 我们的<i>CCNSSal</i>模型可以更精确提取图中显著的场景与物体, 将不受注意的背景部分的值降低, 与人眼视觉生理注意力有更高的一致性.</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903004_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 5幅彩色图像 (左) ;用cNSSSal所得的显著图 (中) ;用CCNSSal所得的显著图 (右)" src="Detail/GetImg?filename=images/WXYJ201903004_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 5<b>幅彩色图像 (左) ;用</b><i>cNSSSal</i><b>所得的显著图 (中) ;用</b><i>CCNSSal</i><b>所得的显著图 (右</b>)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903004_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="146" name="146" class="anchor-tag">6 <b>结束语</b></h3>
                <div class="p1">
                    <p id="147">本文提出了一种基于互补色小波自然场景统计模型的显著图方法.将视觉生理感知中两种重要的理论—色彩感知方面的互补色理论与人眼视觉皮层细胞应激性与能量分配的GSM-DNT统计模型结合提出统一的显著图框架, 解释了颜色、亮度、方向对人类视觉感知显著性的统一根源及掩蔽效应等视觉机制.实验结果表明我们的模型较其他同类模型有显著优越性, 特别在处理色彩丰富的场景时能大幅提高与人眼视觉机制一致性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A feature integration theory of attention">

                                <b>[1]</b> TREISMAN A M, GELADE G. A feature-integration theory of attention[J].Cognitive psychology, 1980, 12 (1) : 97-136.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shifts in selective visual-attention:Towards the Underlying Neural Circuitry">

                                <b>[2]</b> KOCH C, ULLMAN S. Shifts in selective visual attention: towards the underlying neural circuitry[M]. Springer, Dordrecht, Matters of intelligence, 1987: 115-141.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A model of saliency-based visual attention for rapid scene analysis">

                                <b>[3]</b> ITTI L, KOCH C, NIEBUR E. A model of saliency-based visual attention for rapid scene analysis[J]. IEEE Transactions on pattern analysis and machine intelligence, 1998, 20 (11) : 1254-1259.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300069646&amp;v=MjY1MDBaZVp1SHlqbVViL0lKRm9WYUJFPU5pZk9mYks3SHRET3JJOUZaTzBHQ25nL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> WALTHER D, KOCH C. Modeling attention to salient proto-objects[J]. Neural networks, 2006, 19 (9) : 1395-1407.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph-based visual saliency">

                                <b>[5]</b> HAREL J, KOCH C, PERONA P. Graph-based visual saliency[C]//Advances in neural information processing systems. New York, 2006: 545-552.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency Based on Information Maximization">

                                <b>[6]</b> BRUCE N, TSOTSOS J. Saliency based on information maximization[C]//Advances in neural information processing systems. New York, 2006: 155-162.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SUN: A Bayesian framework for saliency using natural statistics">

                                <b>[7]</b> ZHANG L, TONG M H, MARKS T K, et al. SUN: A Bayesian framework for saliency using natural statistics[J]. Journal of vision, 2008, 8 (7) : 32-32.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency Detection: A Spectral Residual Ap-proach">

                                <b>[8]</b> HOU X, ZHANG L. Saliency detection: A spectral residual approach[C]//Computer Vision and Pattern Recognition, CVPR′07. IEEE Conference on. Minneapolis, Minnesota, USA. IEEE, 2007: 1-8.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pulse discrete cosine transform for saliency-based visual attention">

                                <b>[9]</b> YU Y, WANG B, ZHANG L. Pulse discrete cosine transform for saliency-based visual attention[C]//Development and Learning, ICDL 2009. IEEE 8th International Conference on. Shanghai, China. IEEE, 2009: 1-6.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Signature: Highlighting Sparse Salient Regions">

                                <b>[10]</b> HOU X, HAREL J, KOCH C. Image signature: Highlighting sparse salient regions[J]. IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (1) : 194-201.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale Mixtures of Gaussians and the Statistics of Natural Images">

                                <b>[11]</b> WAINWRIGHT M J, SIMONCELLI E P. Scale mixtures of Gaussians and the statistics of natural images[C]//Advances in neural information processing systems. United States, Colorado, 2000: 855-861.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reduced-reference image quality assessment using divisive normalization-based image representation">

                                <b>[12]</b> LI Q, WANG Z. Reduced-reference image quality assessment using divisive normalization-based image representation[J]. IEEE journal of selected topics in signal processing, 2009, 3 (2) : 202-211.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image denoising using scale mixtures of Gaussians in the wavelet domain">

                                <b>[13]</b> PORTILLA J, STRELA V, WAINWRIGHT M J, et al. Image denoising using scale mixtures of Gaussians in the wavelet domain[J]. IEEE Transactions on Image processing, 2003, 12 (11) : 1338-1351.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FDXB201401007&amp;v=MTkxNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5emhVcjNMSXluVGJMRzRIOVhNcm85Rlk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 黄虹, 张建秋. 彩色自然场景统计显著图模型[J]. 复旦学报: 自然科学版, 2014, 59 (1) : 51-58.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Complementary color wavelet: a novel tool for the color image/video analysis and processing">

                                <b>[15]</b> CHEN Y, LI D, ZHANG J Q. Complementary color wavelet: a novel tool for the color image/video analysis and processing[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2017, 29 (3) :1-3. DOI: 10.1109/TCSVT.2017.2776239.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency-based image quality assessment criterion">

                                <b>[16]</b> MA Q, ZHANG L. Saliency-based image quality assessment criterion[C]//International Conference on Intelligent Computing. Springer, Berlin, Heidelberg, 2008: 1124-1133.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Complementary colors theory of color vision: Physiology, color mixture, color constancy and color perception">

                                <b>[17]</b> PRIDMORE R W. Complementary colors theory of color vision: Physiology, color mixture, color constancy and color perception[J]. Color Research &amp; Application, 2011, 36 (6) : 394-412.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Complementary colors: The structure of wavelength discrimination, uniform hue spectral sensitivity, saturation, chromaticadaptation, and chromatic induction">

                                <b>[18]</b> PRIDMORE R W. Complementary colors: the structure of wavelength discrimination, uniform hue, spectral sensitivity, saturation, chromatic adaptation, and chromatic induction[J]. Color Research &amp; Application, 2009, 34 (3) : 233-252.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201903004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201903004&amp;v=MDAwNDFyQ1VSTE9lWmVWdUZ5emhVcjNMTWpYU1pMRzRIOWpNckk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
