<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133885304475000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201905010%26RESULT%3d1%26SIGN%3dtQ1junfUwAsL%252fgjcyjbnTkaV%252bu4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201905010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201905010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201905010&amp;v=MTU4MjNVUkxPZVplVnVGeWptV3J2SU1qWFNaTEc0SDlqTXFvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#23" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#27" data-title="2 &lt;b&gt;基于行为上下文的视频情感识别方法&lt;/b&gt; ">2 <b>基于行为上下文的视频情感识别方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#30" data-title="2.1 &lt;b&gt;基于&lt;/b&gt;k-means&lt;b&gt;聚类的特征选取&lt;/b&gt;">2.1 <b>基于</b>k-means<b>聚类的特征选取</b></a></li>
                                                <li><a href="#36" data-title="2.2 &lt;b&gt;基于动态权重注意力机制的特征融合&lt;/b&gt;">2.2 <b>基于动态权重注意力机制的特征融合</b></a></li>
                                                <li><a href="#51" data-title="2.3 &lt;b&gt;多模态特征融合&lt;/b&gt;">2.3 <b>多模态特征融合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="3 &lt;b&gt;性能评估&lt;/b&gt; ">3 <b>性能评估</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="3.1 &lt;b&gt;实验设置&lt;/b&gt;">3.1 <b>实验设置</b></a></li>
                                                <li><a href="#60" data-title="3.2 &lt;b&gt;实验结果与分析&lt;/b&gt;">3.2 <b>实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="4 &lt;b&gt;结束语&lt;/b&gt; ">4 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#29" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;基于行为上下文的视频情感识别方法&lt;/b&gt;"><b>图</b>1 <b>基于行为上下文的视频情感识别方法</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;融合性能比较&lt;/b&gt;"><b>表</b>1 <b>融合性能比较</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同模态性能比较&lt;/b&gt;"><b>表</b>2 <b>不同模态性能比较</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;性能验证&lt;/b&gt;"><b>表</b>3 <b>性能验证</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="78">


                                    <a id="bibliography_1" title=" CHU J F, DE F LA TORRE.Selective transfer machine for personalized facial expression analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (3) :529-545." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Selective transfer machine for personalized facial expression analysis">
                                        <b>[1]</b>
                                         CHU J F, DE F LA TORRE.Selective transfer machine for personalized facial expression analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (3) :529-545.
                                    </a>
                                </li>
                                <li id="80">


                                    <a id="bibliography_2" title=" BENITEZ QUIROZ C F, SRINIVASAN R, MARTINEZ A M.Emotionet:An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild[C]//Proceedings of IEEE International Conference on Computer Vision &amp;amp; Pattern Recognition.Portland, Oregon, USA, 2016:5562-5570." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EmotioNet:An Accurate,Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild">
                                        <b>[2]</b>
                                         BENITEZ QUIROZ C F, SRINIVASAN R, MARTINEZ A M.Emotionet:An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild[C]//Proceedings of IEEE International Conference on Computer Vision &amp;amp; Pattern Recognition.Portland, Oregon, USA, 2016:5562-5570.
                                    </a>
                                </li>
                                <li id="82">


                                    <a id="bibliography_3" title=" XU M, XU C S, HE X J, et al.Hierarchical affective content analysis in arousal and valence dimensions[J].Signal Processing, 2014, 93 (8) :2140-2150." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300649948&amp;v=MzA0NDN3WmVadUh5am1VYi9JSUYwZGJoRT1OaWZPZmJLN0h0RE9ySTlGWXU4R0JYZ3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         XU M, XU C S, HE X J, et al.Hierarchical affective content analysis in arousal and valence dimensions[J].Signal Processing, 2014, 93 (8) :2140-2150.
                                    </a>
                                </li>
                                <li id="84">


                                    <a id="bibliography_4" title=" RENE MARCELINO, ABRITTA TEIXEIRA, TOSHIHIKO YAMASAKI, et al.Determination of emotional content of video clips by low-level audiovisual features[J].Multimedia Tools and Applications, 2012, 61 (1) :21-49." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD120717001953&amp;v=MTYzNDR6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNybFU3L0xLVm9YTmo3QmFySzZIdGJOcUk5RlplSUtEeE04&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         RENE MARCELINO, ABRITTA TEIXEIRA, TOSHIHIKO YAMASAKI, et al.Determination of emotional content of video clips by low-level audiovisual features[J].Multimedia Tools and Applications, 2012, 61 (1) :21-49.
                                    </a>
                                </li>
                                <li id="86">


                                    <a id="bibliography_5" title=" NICOLAOU M A, GUNES H, PANTIC M.Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space[J].IEEE Transactions on Affective Computing, 2011, 2 (2) :92-105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space">
                                        <b>[5]</b>
                                         NICOLAOU M A, GUNES H, PANTIC M.Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space[J].IEEE Transactions on Affective Computing, 2011, 2 (2) :92-105.
                                    </a>
                                </li>
                                <li id="88">


                                    <a id="bibliography_6" title=" VALENTIN VIELZEUF, ST&#201;PHANE PATEUX , FR&#201;D&#201;RIC JURIE.Temporal multimodal fusion for video emotion classification in the wild[C]//Proceedings of the 19th ACM International Conference on Multimodal Interaction.Jiangsu, China, 2017:569-576." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal multimodal fusion for video emotion classification in the wild">
                                        <b>[6]</b>
                                         VALENTIN VIELZEUF, ST&#201;PHANE PATEUX , FR&#201;D&#201;RIC JURIE.Temporal multimodal fusion for video emotion classification in the wild[C]//Proceedings of the 19th ACM International Conference on Multimodal Interaction.Jiangsu, China, 2017:569-576.
                                    </a>
                                </li>
                                <li id="90">


                                    <a id="bibliography_7" title=" CHEN C, WU Z X, JIANG Y G.Emotion in context:deep semantic feature fusion for video emotion recognition[C]//ACM International Conference on Multimedia.Amsterdam, The Netherlands, 2016:127-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Emotion in context deep semantic feature fusion for video emotion recognition">
                                        <b>[7]</b>
                                         CHEN C, WU Z X, JIANG Y G.Emotion in context:deep semantic feature fusion for video emotion recognition[C]//ACM International Conference on Multimedia.Amsterdam, The Netherlands, 2016:127-131.
                                    </a>
                                </li>
                                <li id="92">


                                    <a id="bibliography_8" title=" FAN Y, LU X J, LI D, et al.Video-based emotion recognition using CNN-RNN and C3D hybrid networks[C]// ACM International Conference on Multimodal Interaction.Tokyo, Japan , 2016:445-450." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Video-based emotion recognition using CNN-RNN and C3D hybrid networks">
                                        <b>[8]</b>
                                         FAN Y, LU X J, LI D, et al.Video-based emotion recognition using CNN-RNN and C3D hybrid networks[C]// ACM International Conference on Multimodal Interaction.Tokyo, Japan , 2016:445-450.
                                    </a>
                                </li>
                                <li id="94">


                                    <a id="bibliography_9" title=" HEYSEM KAYA, FURKAN G&#220;URPINAR, ALBERT ALI SALAH.Video-based emotion recognition in the wild using deep transfer learning and score fusion [J].Image and Vision Computing, 2017 (65) :66-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES90147125B96074B3F737AB43B4177186&amp;v=MDYxNDZmQ3BiUTM1ZGhoeEx1MnhLcz1OaWZPZmJxNEg5WExybzFBRnVJSkRIczl2UlZsN1R4Nk9RM21yMkF4ZUxXVFJMS1pDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         HEYSEM KAYA, FURKAN G&#220;URPINAR, ALBERT ALI SALAH.Video-based emotion recognition in the wild using deep transfer learning and score fusion [J].Image and Vision Computing, 2017 (65) :66-75.
                                    </a>
                                </li>
                                <li id="96">


                                    <a id="bibliography_10" title=" FAN L J, KE Y J.Spatiotemporal networks for video emotion recognition[J].Computer Science - Computer Vision and Pattern Recognition.arxiv.org, 2018:1-8.DOI:10.1109/CVPR.2017.787" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatiotemporal networks for video emotion recognition">
                                        <b>[10]</b>
                                         FAN L J, KE Y J.Spatiotemporal networks for video emotion recognition[J].Computer Science - Computer Vision and Pattern Recognition.arxiv.org, 2018:1-8.DOI:10.1109/CVPR.2017.787
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(05),43-46             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于行为上下文的视频情感识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%99%93%E4%B8%9C&amp;code=33642293&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘晓东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B7%BC&amp;code=27622185&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王淼</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%9D%BE%E9%98%B3&amp;code=28946382&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李松阳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8D%97%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0446416&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河南工程学院计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>视频中的场景与对象等含有丰富的情感线索, 传统的视频情感识别方法主要关注视频中人体行为, 往往忽略了场景和对象等上下文线索.本文提出一种基于行为上下文的视频情感识别方法.该方法首先基于卷积神经网络提取视频场景、对象、行为等多个模态特征;然后根据各个模态特征信息确定各模态视频帧情感分数;在此基础上基于金字塔架构, 建立多模态特征信息融合模型, 对视频情感进行识别.我们基于caffe框架实现了该方法, 实验结果表明该方法在性能上优于已有方法.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘晓东, 男, (1981-) , 博士 (博士后) , 副教授.研究方向为网络信息与多媒体.E-mail:liuxiaodongxht@qq.com.;
                                </span>
                                <span>
                                    王淼, 男, (1981-) , 博士, 副教授.研究方向为空间查询、空间推理.;
                                </span>
                                <span>
                                    李松阳, 男, (1985-) , 博士, 副教授.研究方向为计算机图形学.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61501174);</span>
                                <span>河南省科技厅科技攻关项目 (182102310919);</span>
                                <span>河南工程学院博士基金项目 (D2015022);</span>
                                <span>河南省教育厅科学技术研究重点项目 (16A520041);</span>
                    </p>
            </div>
                    <h1><b>A behavior context based video emotion recognition method</b></h1>
                    <h2>
                    <span>LIU Xiao-dong</span>
                    <span>WANG miao</span>
                    <span>LI Song-yang</span>
            </h2>
                    <h2>
                    <span>School of Computer, Henan Institute of Engineering</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Scenes and objects in videos include abundant emotional clues. Traditional video emotion recognition mainly focuses on human behavior in video, and context clues such as scenes and objects are often ignored. This paper proposed a behavior context based video emotion recognition method. Firstly, multiple modal features of video scene, object and behavior are extracted based on convolution neural network (CNN) . Secondly, emotional scores of video frames are determined according to its feature information. Thirdly, multiple modal feature fusion model is built based on Pyramid framework to recognize video emotion. The method is implemented based on Caffe. The experiment results show that our method is superior to the existing methods in performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=emotion%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">emotion recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="23" name="23" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="24">近年来, 视频情感识别作为一个新兴的研究领域受到越来越多的关注, 对视频情感的理解有很广阔的应用场景.比如, 在智能安防上情感识别平台可用于识别潜在可疑危险人员;视频推荐服务可以将用户兴趣与视频情感相匹配, 政府部门可以更好地了解人们对热点事件或新政策的反应.</p>
                </div>
                <div class="p1">
                    <p id="25">情感计算的概念最早是在1997年由MIT媒体实验室Picard教授提出, 20世纪末, 情感作为认知过程重要组成部分得到了学术界的普遍认同.美国卡耐基梅隆大学人工智能实验室、日本广岛大学、意大利特伦托大学等高校均在情感计算这一领域开展了研究工作.国内高校和科研院所近年来也在该领域开展了研究.目前, 在视频情感识别领域主要基于视频外观和声音特征.Wen-Sheng Chu等人<citation id="98" type="reference"><link href="78" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>基于脸部运动编码系统来识别人员情感, 该系统采用一系列脸部特定位置运动 (行为单元) 来编码脸部表达.这种行为单元可以通过从人脸图像中提取的几何特征和外观特征来识别.C. F. Benitez-Quiroz等人<citation id="99" type="reference"><link href="80" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>采用卷积神经网络 (convolutional neural network, CNN) 来识别行为单元和脸部情感.这些研究仅仅考虑了视频中的视觉信息, 在多数情况下, 音频信息对情感识别也非常有益.文献<citation id="103" type="reference">[<a class="sup">3</a>,<a class="sup">4</a>]</citation>融合外观和声音特征进行视频情感分类, 但是他们仅仅利用了简单的多模态数据融合方法, 没有考虑进一步多模态数据类型的潜在联系, 且采用的外观特征大部分是低层次特征.M. A. Nicolaou等人<citation id="100" type="reference"><link href="86" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>融合脸部表达、肩的姿势和声音等线索进行情感识别.Valentin Vielzeuf等人<citation id="101" type="reference"><link href="88" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出一种分层方法允许在不同的层融合分数和特征, 该方法可以在利用不同模块信息时保留不同模块信息, 但尚未考虑视频中多模态数据之间潜在的联系.人类的情感表现出很强的相关性, 情感通常由特定场景下多个对象执行特定事件而触发, 因此情感可以通过事件、对象和场景等语境信息识别.Yu-Gang Jiang等人<citation id="102" type="reference"><link href="90" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>首先利用CNN获取事件、对象和场景语义特征, 提取的高层次特征作为有效的上下文信息, 然后利用一个情景融合网络将这些信息进行融合, 形成一个统一的情感表示, 用于视频情感识别.然而, Yu-Gang Jiang等人对事件、对象和场景等特征采用平均聚合方法, 并没有考虑不同模态之间的权重.</p>
                </div>
                <div class="p1">
                    <p id="26">本文提出一种基于行为上下文的视频情感识别方法.该方法首先分别提取场景、对象和行为等多个模态特征;然后依据各模态特征信息, 确定各模态情感分数, 并以情感分数为权重建立各模态情感特征表示;最后基于特征金字塔架构, 建立多模态特征信息融合模型.基于Caffe架构实现了该方法, 利用公共视频情感识别数据集进行性能验证, 实验结果表明该方法在识别性能上优于已有研究方法.</p>
                </div>
                <h3 id="27" name="27" class="anchor-tag">2 <b>基于行为上下文的视频情感识别方法</b></h3>
                <div class="p1">
                    <p id="28">基于行为上下文的视频情感识别方法 (如图1所示) 包含基于k-means的特征选取、注意力动态权重融合和基于双线性的动态特征融合三个阶段.</p>
                </div>
                <div class="area_img" id="29">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201905010_029.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于行为上下文的视频情感识别方法" src="Detail/GetImg?filename=images/WXYJ201905010_029.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>基于行为上下文的视频情感识别方法</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201905010_029.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="30" name="30">2.1 <b>基于</b>k-means<b>聚类的特征选取</b></h4>
                <div class="p1">
                    <p id="31">由于视频含有许多相似帧, 为了避免特征冗余, 我们首先对视频<i>V</i>的特征向量进行k-menas聚类, 划分为<i>k</i>个集合<i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>, …, <i>C</i><sub><i>k</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="32">用<i>V</i>={<i>f</i><sub><i>i</i></sub>|<i>i</i>=1, 2, …, <i>n</i>}来表示视频, 其中<i>f</i><sub><i>i</i></sub>为第<i>i</i>个视频帧, <i>n</i>为视频帧个数, 用<i>X</i>={<i>x</i><sub><i>i</i></sub>|<i>i</i>=1, 2, …, <i>n</i>}表示视频<i>V</i>对应的CNN特征向量, 其中, <i>x</i><sub><i>i</i></sub>为帧<i>f</i><sub><i>i</i></sub>对应的CNN特征向量.对<i>X</i>={<i>x</i><sub><i>i</i></sub>|<i>i</i>=1, 2, …, <i>n</i>}进行k-means聚类可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="33" class="code-formula">
                        <mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mo>-</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>d</mi></mrow></msub><mrow><mi>cos</mi></mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="34">式中, cos (<i>x</i><sub><i>i</i></sub>, <i>c</i><sub><i>d</i></sub>) 表示<i>x</i><sub><i>i</i></sub>和<i>c</i><sub><i>d</i></sub>之间余弦相似性;<i>c</i><sub><i>d</i></sub>为第<i>d</i>个类别聚类中心;当<i>x</i><sub><i>i</i></sub>属于<i>C</i><sub><i>i</i></sub>时, <i>γ</i><sub><i>id</i></sub>=1, 否则<i>γ</i><sub><i>id</i></sub>=0.</p>
                </div>
                <div class="p1">
                    <p id="35">然后分别从<i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>, …, <i>C</i><sub><i>k</i></sub>中选取一个特征向量 (比如选取各个集合中心点) , 我们用<i>X</i>′={<i>x</i>′<sub><i>i</i></sub>|<i>i</i>=1, 2, …, <i>k</i>}来表示选取的<i>k</i>个特征向量.</p>
                </div>
                <h4 class="anchor-tag" id="36" name="36">2.2 <b>基于动态权重注意力机制的特征融合</b></h4>
                <div class="p1">
                    <p id="37">由于X′中k个不同类型的特征向量与情感状态的相关性不一定相同, 我们采用基于注意力机制的动态权重融合方法.</p>
                </div>
                <div class="p1">
                    <p id="38">假设情感状态个数为E, 利用<i>CNN</i>学习一个线性转换矩阵<b><i>M</i></b>, 将特征向量<i>x</i>′<sub><i>i</i></sub>转换为一个<i>E</i>维的向量<i>v</i><sub><i>i</i></sub>, 然后将<i>v</i><sub><i>i</i></sub>通过一个softmax层</p>
                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>v</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>e</mi><mo>=</mo><mn>1</mn></mrow><mi>E</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>e</mi><mo stretchy="false">]</mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="40">式中, <i>v</i><sub><i>i</i></sub>[<i>k</i>]表示<i>v</i><sub><i>i</i></sub>第<i>k</i>个维度的值;<i>v</i>′<sub><i>i</i></sub>表示softmax分类分数.</p>
                </div>
                <div class="p1">
                    <p id="41"><i>v</i><sub><i>i</i></sub>的值反映了特征向量的信息量大小, <i>v</i>′<sub><i>i</i></sub>中的最大值反映了判别力, 不同特征向量的注意力权重值可由公式 (3) 得到</p>
                </div>
                <div class="p1">
                    <p id="42"><mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>λ</mi><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>E</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>E</mi></munderover><mi>v</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>λ</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mrow><mi>max</mi></mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mo>-</mo><mtext>a</mtext><mtext>v</mtext><mtext>g</mtext></mrow></math></mathml>v<sub>i</sub>[n])      (3) </p>
                </div>
                <div class="p1">
                    <p id="44">式中, 参数λ用来平衡特征信息量大小所占比重和判别力所占比重.</p>
                </div>
                <div class="p1">
                    <p id="45">在获得各个特征向量的注意力权重后, 对这k个特征向量进行加权融合.首先将这k个特征向量进行加权链接为一个向量</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mo stretchy="false">[</mo><msup><mi>x</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><msup><mi>x</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><msup><mi>x</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">然后将向量划分为k块, 用[x<sub>i1</sub>, x<sub>i (k-1) </sub>]表示第i块, 我们可以通过公式 (5) </p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">得到一个池特征μ<sub>i</sub>, 把基于注意力机制的融合方法得到的特征μ<sub>R</sub>=[μ<sub>1</sub>, μ<sub>2</sub>, …, μ<sub>F</sub>] (F为特征向量长度) 作为视频的外观特征.</p>
                </div>
                <div class="p1">
                    <p id="50">同理, 可以获取视频的场景特征μ<sub>S</sub>和对象特征μ<sub>O</sub>, 并通过将每个<i>CNN</i>的输入为L个连续的光流得到视频的运动特征μ<sub>M</sub>.</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">2.3 <b>多模态特征融合</b></h4>
                <div class="p1">
                    <p id="52">针对一段视频, 在得到四个高层次语义特征<i>μ</i><sub><i>R</i></sub>、<i>μ</i><sub><i>M</i></sub>、<i>μ</i><sub><i>S</i></sub>和<i>μ</i><sub><i>O</i></sub>后, 采用基于多层次多模态金字塔架构进行融合, 得到最终的情感预测结果.首先将特征<i>μ</i><sub><i>R</i></sub>、<i>μ</i><sub><i>S</i></sub>、<i>μ</i><sub><i>O</i></sub>和<i>μ</i><sub><i>M</i></sub>进行连接得到特征向量[<i>μ</i><sub><i>R</i></sub>, <i>μ</i><sub><i>M</i></sub>, <i>μ</i><sub><i>S</i></sub>, <i>μ</i><sub><i>O</i></sub>], 将该特征向量经过一个三层的全连接网络得到预测分数score<sub>1</sub>.然后将特征<i>μ</i><sub><i>R</i></sub>、<i>μ</i><sub><i>M</i></sub>、<i>μ</i><sub><i>S</i></sub>和<i>μ</i><sub><i>O</i></sub>进行双线性融合, 得到两个新的特征<i>f</i> (<i>μ</i><sub><i>R</i></sub>, <i>μ</i><sub><i>M</i></sub>) 和<i>f</i> (<i>μ</i><sub><i>s</i></sub>, <i>μ</i><sub><i>o</i></sub>) , 其中<i>f</i>为双线性融合函数.将<i>f</i> (<i>μ</i><sub><i>R</i></sub>, <i>μ</i><sub><i>M</i></sub>) 和<i>f</i> (<i>μ</i><sub><i>s</i></sub>, <i>μ</i><sub><i>o</i></sub>) 经过一个三层的全连接网络得到预测分数score<sub>2</sub>, 同时对<i>f</i> (<i>μ</i><sub><i>R</i></sub>, <i>μ</i><sub><i>M</i></sub>) 和<i>f</i> (<i>μ</i><sub><i>s</i></sub>, <i>μ</i><sub><i>o</i></sub>) 进行融合得到特征<i>f</i> (<i>f</i> (<i>μ</i><sub><i>R</i></sub>, <i>μ</i><sub><i>M</i></sub>) , <i>f</i> (<i>μ</i><sub><i>s</i></sub>, <i>μ</i><sub><i>o</i></sub>) ) .</p>
                </div>
                <div class="p1">
                    <p id="53">将<i>f</i> (<i>f</i> (<i>μ</i><sub><i>R</i></sub>, <i>μ</i><sub><i>M</i></sub>) , <i>f</i> (<i>μ</i><sub><i>s</i></sub>, <i>μ</i><sub><i>o</i></sub>) ) 经过一个三层的全连接网络得到预测分数score<sub>3</sub>, 则视频情感分数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="54">score=<i>ω</i><sub>1</sub>×score<sub>1</sub>+<i>ω</i><sub>2</sub>×score<sub>2</sub>+<i>ω</i><sub>3</sub>×score<sub>3</sub>      (6) </p>
                </div>
                <div class="p1">
                    <p id="55">式中, <i>ω</i><sub><i>i</i></sub>为第<i>i</i>层分数对应的权重, 可以通过训练学习得到.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">3 <b>性能评估</b></h3>
                <div class="p1">
                    <p id="57">为了评估基于行为上下文的视频情感识别方法的有效性, 本节给出实验结果并对实验结果进行分析.</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58">3.1 <b>实验设置</b></h4>
                <div class="p1">
                    <p id="59">我们在公开的情感识别数据集<i>AFEW</i>上对本文所提方法进行实验验证.<i>AFEW</i>数据集从好莱坞电影中提取短的视频片段, 将视频情感划分为生气、厌恶、害怕、高兴、伤心、吃惊和中性7种, 其中774个视频片段用于训练数据集, 383个视频片段用于验证数据集.</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">3.2 <b>实验结果与分析</b></h4>
                <div class="p1">
                    <p id="61">在该实验中, 我们首先对动态权重注意力机制进行验证, 将基于动态权重的融合方法与最大值聚合、平均值聚合进行性能比较.结果如表1所示.</p>
                </div>
                <div class="area_img" id="62">
                    <p class="img_tit"><b>表</b>1 <b>融合性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="62" border="1"><tr><td><br />融合方法</td><td>实验结果</td></tr><tr><td><br />平均值</td><td>50.9%</td></tr><tr><td><br />最大值</td><td>51.8%</td></tr><tr><td><br />注意力机制</td><td>56.3%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="63">从表1可以看出, 基于动态权重注意力机制的融合方法要比平均值融合和最大值融合分别高5.4%和4.5%.这是由于平均值融合和最大值融合没有考虑不同视频帧之间的情感表达差异, 基于动态权重注意力机制的融合方法根据视频帧信息量和情感判别力来分配视频帧权重, 考虑了不同视频帧之间的情感表达差异.</p>
                </div>
                <div class="p1">
                    <p id="64">对不同模态视频情感识别能力进行验证, 表2给出了不同模态及其融合性能.</p>
                </div>
                <div class="area_img" id="65">
                    <p class="img_tit"><b>表</b>2 <b>不同模态性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="65" border="1"><tr><td><br />采用模态</td><td>实验结果</td></tr><tr><td><br />场景</td><td>27.5%</td></tr><tr><td><br />对象</td><td>35.1%</td></tr><tr><td><br />视频帧行为</td><td>41.2%</td></tr><tr><td><br />光流行为</td><td>42.4%</td></tr><tr><td><br />视频帧+光流行为</td><td>45.1%</td></tr><tr><td><br />多模态特征融合</td><td>56.3%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="66">从表2可以看出, 行为特征情感识别能力最强, 基于视频帧和光流行为特征融合达到了45.1%, 场景、对象等都具备一定的情感识别能力, 对这些特征进行融合能充分利用不同模态的情感线索.</p>
                </div>
                <div class="p1">
                    <p id="67">最后, 将本文方法与最近流行的几种方法性能性能比较, 结果如表3所示.</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表</b>3 <b>性能验证</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td><br />情感识别方法</td><td>实验结果</td></tr><tr><td><br /><i>Yin Fan et al</i>.<sup>[8]</sup></td><td>48.3 %</td></tr><tr><td><br /><i>Heysem Kaya et al</i>.<sup>[9]</sup></td><td>51.77%</td></tr><tr><td><br /><i>Lijie Fan et al</i>.<sup>[10]</sup></td><td>55.3%</td></tr><tr><td><br />本文</td><td>56.3%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">从表3可以看出, 本文提出的基于行为上下文的视频情感识别方法在性能上要优于已有方法.本文采用了动态权重注意力机制, 考虑了不同视频帧情感表达的差异, 并且考虑了场景和对象所含有的情感线索.</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">4 <b>结束语</b></h3>
                <div class="p1">
                    <p id="71">本文提出了一种基于行为上下文的视频情感识别方法.该方法首先提取视频场景、对象和行为等多模态特征信息;基于多模态特征信息计算该模态情感分数;基于金字塔架构对多模态特征信息进行融合.在<i>AFEW</i>数据集上情感识别率达到56.3%, 在性能上要优于已有方法.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="78">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Selective transfer machine for personalized facial expression analysis">

                                <b>[1]</b> CHU J F, DE F LA TORRE.Selective transfer machine for personalized facial expression analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (3) :529-545.
                            </a>
                        </p>
                        <p id="80">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EmotioNet:An Accurate,Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild">

                                <b>[2]</b> BENITEZ QUIROZ C F, SRINIVASAN R, MARTINEZ A M.Emotionet:An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild[C]//Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition.Portland, Oregon, USA, 2016:5562-5570.
                            </a>
                        </p>
                        <p id="82">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300649948&amp;v=MDE5NThqbVViL0lJRjBkYmhFPU5pZk9mYks3SHRET3JJOUZZdThHQlhneG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> XU M, XU C S, HE X J, et al.Hierarchical affective content analysis in arousal and valence dimensions[J].Signal Processing, 2014, 93 (8) :2140-2150.
                            </a>
                        </p>
                        <p id="84">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD120717001953&amp;v=MjM5NDBhcks2SHRiTnFJOUZaZUlLRHhNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3JsVTcvTEtWb1hOajdC&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> RENE MARCELINO, ABRITTA TEIXEIRA, TOSHIHIKO YAMASAKI, et al.Determination of emotional content of video clips by low-level audiovisual features[J].Multimedia Tools and Applications, 2012, 61 (1) :21-49.
                            </a>
                        </p>
                        <p id="86">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space">

                                <b>[5]</b> NICOLAOU M A, GUNES H, PANTIC M.Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space[J].IEEE Transactions on Affective Computing, 2011, 2 (2) :92-105.
                            </a>
                        </p>
                        <p id="88">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal multimodal fusion for video emotion classification in the wild">

                                <b>[6]</b> VALENTIN VIELZEUF, STÉPHANE PATEUX , FRÉDÉRIC JURIE.Temporal multimodal fusion for video emotion classification in the wild[C]//Proceedings of the 19th ACM International Conference on Multimodal Interaction.Jiangsu, China, 2017:569-576.
                            </a>
                        </p>
                        <p id="90">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Emotion in context deep semantic feature fusion for video emotion recognition">

                                <b>[7]</b> CHEN C, WU Z X, JIANG Y G.Emotion in context:deep semantic feature fusion for video emotion recognition[C]//ACM International Conference on Multimedia.Amsterdam, The Netherlands, 2016:127-131.
                            </a>
                        </p>
                        <p id="92">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Video-based emotion recognition using CNN-RNN and C3D hybrid networks">

                                <b>[8]</b> FAN Y, LU X J, LI D, et al.Video-based emotion recognition using CNN-RNN and C3D hybrid networks[C]// ACM International Conference on Multimodal Interaction.Tokyo, Japan , 2016:445-450.
                            </a>
                        </p>
                        <p id="94">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES90147125B96074B3F737AB43B4177186&amp;v=MDE1MjlyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh4THUyeEtzPU5pZk9mYnE0SDlYTHJvMUFGdUlKREhzOXZSVmw3VHg2T1EzbXIyQXhlTFdUUkxLWkNPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> HEYSEM KAYA, FURKAN GÜURPINAR, ALBERT ALI SALAH.Video-based emotion recognition in the wild using deep transfer learning and score fusion [J].Image and Vision Computing, 2017 (65) :66-75.
                            </a>
                        </p>
                        <p id="96">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatiotemporal networks for video emotion recognition">

                                <b>[10]</b> FAN L J, KE Y J.Spatiotemporal networks for video emotion recognition[J].Computer Science - Computer Vision and Pattern Recognition.arxiv.org, 2018:1-8.DOI:10.1109/CVPR.2017.787
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201905010" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201905010&amp;v=MTU4MjNVUkxPZVplVnVGeWptV3J2SU1qWFNaTEc0SDlqTXFvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY3Zm5LZEo2VXlXemZUQTF2RT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
