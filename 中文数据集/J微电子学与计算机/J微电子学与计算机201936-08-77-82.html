<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133853707131250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201908017%26RESULT%3d1%26SIGN%3dPpbtzcBWtxF2%252fdO41bHBVEpVcrk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201908017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201908017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201908017&amp;v=MDcyMDRaTEc0SDlqTXA0OUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5uVkwvTE1qWFM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="2 &lt;b&gt;本文的算法设计&lt;/b&gt; ">2 <b>本文的算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="2.1 &lt;b&gt;基于增强的&lt;/b&gt;MSER&lt;b&gt;的连通区域提取与剪枝&lt;/b&gt;">2.1 <b>基于增强的</b>MSER<b>的连通区域提取与剪枝</b></a></li>
                                                <li><a href="#44" data-title="2.2 &lt;b&gt;基于改进型&lt;/b&gt;CNN&lt;b&gt;的连通区域分类&lt;/b&gt;">2.2 <b>基于改进型</b>CNN<b>的连通区域分类</b></a></li>
                                                <li><a href="#49" data-title="2.3 &lt;b&gt;多方向的候选文本行的形成&lt;/b&gt;">2.3 <b>多方向的候选文本行的形成</b></a></li>
                                                <li><a href="#61" data-title="2.4 &lt;b&gt;基于&lt;/b&gt;C4.5&lt;b&gt;决策树的候选文本行的分类&lt;/b&gt;">2.4 <b>基于</b>C4.5<b>决策树的候选文本行的分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="3 &lt;b&gt;实验结果与分析&lt;/b&gt; ">3 <b>实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="4 &lt;b&gt;结束语&lt;/b&gt; ">4 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;文本定位流程图&lt;/b&gt;"><b>图</b>1 <b>文本定位流程图</b></a></li>
                                                <li><a href="#46" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;基于改进型&lt;/b&gt;CNN&lt;b&gt;的连通区域分类流程图&lt;/b&gt;"><b>图</b>2 <b>基于改进型</b>CNN<b>的连通区域分类流程图</b></a></li>
                                                <li><a href="#135" data-title="图3 C4．5决策树算法流程">图3 C4．5决策树算法流程</a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;中间结果&lt;/b&gt;"><b>图</b>4 <b>中间结果</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;用&lt;/b&gt;ICDAR2013&lt;b&gt;数据集评估&lt;/b&gt;"><b>表</b>1 <b>用</b>ICDAR2013<b>数据集评估</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;用&lt;/b&gt;MSRA-TD500&lt;b&gt;数据集评估&lt;/b&gt;"><b>表</b>2 <b>用</b>MSRA-TD500<b>数据集评估</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;用&lt;/b&gt;ICDAR2015&lt;b&gt;数据集评估&lt;/b&gt;"><b>表</b>3 <b>用</b>ICDAR2015<b>数据集评估</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="136">


                                    <a id="bibliography_1" title=" CHEN X, YUILLE A L.Detecting and reading text in natural scenes[C]// IEEE Computer Society Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2004:366-373." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting and Reading Text in Natural Scenes">
                                        <b>[1]</b>
                                         CHEN X, YUILLE A L.Detecting and reading text in natural scenes[C]// IEEE Computer Society Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2004:366-373.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_2" title=" LEE J J, LEE P H, LEE S W, et al.AdaBoost for Text Detection in Natural Scene[C]// International Conference on Document Analysis and Recognition.IEEE, 2011:429-434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaboost for text detection in natural scene">
                                        <b>[2]</b>
                                         LEE J J, LEE P H, LEE S W, et al.AdaBoost for Text Detection in Natural Scene[C]// International Conference on Document Analysis and Recognition.IEEE, 2011:429-434.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_3" title=" WANG K, BELONGIE S.Word Spotting in the Wild[C]// European Conference on Computer Vision.Springer-Verlag, 2010:591-604." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Word Spotting in the Wild">
                                        <b>[3]</b>
                                         WANG K, BELONGIE S.Word Spotting in the Wild[C]// European Conference on Computer Vision.Springer-Verlag, 2010:591-604.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_4" title=" BAI B, YIN F, LIU C L.A Fast Stroke-Based Method for Text Detection in Video[C]// Iapr International Workshop on Document Analysis Systems.IEEE, 2012:69-73." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Fast Stroke-Based Method for Text Detection in Video">
                                        <b>[4]</b>
                                         BAI B, YIN F, LIU C L.A Fast Stroke-Based Method for Text Detection in Video[C]// Iapr International Workshop on Document Analysis Systems.IEEE, 2012:69-73.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_5" title=" LIU X, WANG W.Robustly Extracting Captions in Videos Based on Stroke-Like Edges and Spatio-Temporal Analysis[M].IEEE Press, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robustly Extracting Captions in Videos Based on Stroke-Like Edges and Spatio-Temporal Analysis">
                                        <b>[5]</b>
                                         LIU X, WANG W.Robustly Extracting Captions in Videos Based on Stroke-Like Edges and Spatio-Temporal Analysis[M].IEEE Press, 2012.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_6" title=" ZHAO Y, LU T, LIAO W.A Robust Color-Independent Text Detection Method from Complex Videos[C]// International Conference on Document Analysis and Recognition.IEEE Computer Society, 2011:374-378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Robust Color-independent Text DetectionMethod from Complex Videos">
                                        <b>[6]</b>
                                         ZHAO Y, LU T, LIAO W.A Robust Color-Independent Text Detection Method from Complex Videos[C]// International Conference on Document Analysis and Recognition.IEEE Computer Society, 2011:374-378.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_7" title=" MATAS J, CHUM O, URBAN M, et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image &amp;amp; Vision Computing, 2004, 22 (10) :761-767." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MDEwMjBOaWZPZmJLN0h0RE9yWTlFWis4R0MzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVndUYWhBPQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         MATAS J, CHUM O, URBAN M, et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image &amp;amp; Vision Computing, 2004, 22 (10) :761-767.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_8" title=" 范一华, 邓德祥, 颜佳.基于色彩空间的最大稳定极值区域的自然场景文本检测[J].计算机应用, 2018, 38 (1) :264-269." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201801047&amp;v=MzIwMTE0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5uVkwvS0x6N0JkN0c0SDluTXJvOUJZNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         范一华, 邓德祥, 颜佳.基于色彩空间的最大稳定极值区域的自然场景文本检测[J].计算机应用, 2018, 38 (1) :264-269.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_9" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems.Curran Associates Inc.2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[9]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems.Curran Associates Inc.2012:1097-1105.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_10" title=" REN S, HE K, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[10]</b>
                                         REN S, HE K, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_11" title=" 方承志, 黄梅玲.自然场景中多方向文本的检测[J].计算机工程与设计, 2018, 39 (5) :1377-1381." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201805030&amp;v=MjM5NDJmWVpMRzRIOW5NcW85R1pJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bm5WTC9LTmk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         方承志, 黄梅玲.自然场景中多方向文本的检测[J].计算机工程与设计, 2018, 39 (5) :1377-1381.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_12" title=" 方承志, 田彪.结合连通分量规则度与Adaboost的文本定位算法[J].电视技术, 2015, 39 (21) :1-4." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DSSS201521002&amp;v=MDAyMTE0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5uVkwvS0lUN1lmYkc0SDlUT3JvOUZab1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         方承志, 田彪.结合连通分量规则度与Adaboost的文本定位算法[J].电视技术, 2015, 39 (21) :1-4.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_13" title=" REN X, CHEN K, YANG X, et al.A novel scene text detection algorithm based on convolutional neural network[C]// Visual Communications and Image Processing (VCIP) , 2016.IEEE, 2016:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel scene text detection algorithm based on convolutional neural network">
                                        <b>[13]</b>
                                         REN X, CHEN K, YANG X, et al.A novel scene text detection algorithm based on convolutional neural network[C]// Visual Communications and Image Processing (VCIP) , 2016.IEEE, 2016:1-4.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_14" title=" EPSHTEIN B, OFEK E, WEXLER Y.Detecting text in natural scenes with stroke width transform[C]// Computer Vision and Pattern Recognition.IEEE, 2010:2963-2970." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Detecting Text in Natural Scenes with Stroke Width Transform,&amp;quot;">
                                        <b>[14]</b>
                                         EPSHTEIN B, OFEK E, WEXLER Y.Detecting text in natural scenes with stroke width transform[C]// Computer Vision and Pattern Recognition.IEEE, 2010:2963-2970.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_15" title=" 田彪.自然场景图像中的文字检测关键算法研究[D].南京:南京邮电大学, 2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016303209.nh&amp;v=MDQ1MTViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlublZML0tWRjI2R0xDNEhkUE1wcEU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         田彪.自然场景图像中的文字检测关键算法研究[D].南京:南京邮电大学, 2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(08),77-82             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进型卷积神经网络和行特征的文本检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%96%B9%E6%89%BF%E5%BF%97&amp;code=23890541&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">方承志</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A8%8A%E6%A2%A6%E9%9B%85&amp;code=39539254&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">樊梦雅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E5%85%89%E5%AD%A6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0101257&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京邮电大学电子与光学工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有的自然场景文本检测算法准确率尚未理想的问题, 提出了一种基于改进型卷积神经网络和行特征的文本检测方法.首先, 采用增强的最大稳定极值区域 (MSER) 提取图像的连通分量, 并应用剪枝方法来获取孤立的连通区域;其次, 应用改进型卷积神经网络 (CNN) 对非字符区域进行消除, 获得候选字符区域;然后, 提出基于行特征构建多方向候选文本行的算法, 用于检测任意定向和弯曲的场景文本;最后, 应用C4.5决策树算法对候选文本行进行分类.该算法在ICDAR2013、ICDAR2015和MSER-TD500数据集上进行实验, 实验结果表明, 该算法能显著提高自然场景文本检测的准确率和召回率, 且适用于任意方向、语言和字体的文本.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7%E7%A8%B3%E5%AE%9A%E6%9E%81%E5%80%BC%E5%8C%BA%E5%9F%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大稳定极值区域;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=C4.5%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">C4.5决策树算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    方承志, 男, (1976-) , 博士, 副教授.研究方向为模式识别、嵌入式系统和信号处理.;
                                </span>
                                <span>
                                    *樊梦雅, (通讯作者) , 女, (1994-) , 硕士研究生.研究方向为图像处理与模式识别.E-mail:956594122@qq.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上资助项目 (61271334, 61073115);</span>
                    </p>
            </div>
                    <h1><b>Text detection based on improved convolutional neural network and the feature of text lines</b></h1>
                    <h2>
                    <span>FANG Cheng-zhi</span>
                    <span>FAN Meng-ya</span>
            </h2>
                    <h2>
                    <span>College of Electronic and Optical Engineering, Nanjing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem of low precision in existing natural scene text detection algorithm, a text detection algorithm based on improved convolutional neural network and the feature of text lines was proposed. Firstly, the enhanced maximally stable extreme region (MSER) was used to extract connected components of the image and isolated connected regions were obtained by pruning method. Secondly, an improved convolutional neural network (CNN) was used to eliminate non-character regions and obtain candidate character regions. Thirdly, an algorithm based on the feature of text lines for constructing multi-oriented candidate text lines was proposed to detect arbitrary oriented and curved scene text. Finally, C4.5 decision tree algorithm was applied to classify candidate text lines. Experiments were carried out on ICDAR2013, ICDAR2015 and MSRA-TD500 datasets. Experimental results show that the algorithm can significantly improve the precision and recall of text detection in natural scenes, and is suitable for text in any direction, language and font.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=maximally%20stable%20extremal%20region%20(MSER)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">maximally stable extremal region (MSER) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=the%20feature%20of%20text%20lines&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">the feature of text lines;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=C4.5%20decision%20tree%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">C4.5 decision tree algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="34">尽管在自然场景中的文本检测方面进行了大量的研究工作, 但依然面临着诸多挑战, 主要有以下三个方面: (1) 文本的多样性; (2) 背景的复杂性; (3) 光照不均、扭曲、部分遮挡等其他外界因素的干扰.</p>
                </div>
                <div class="p1">
                    <p id="35">目前的自然场景文本检测算法主要分为两类:基于滑动窗口的方法和基于连通区域的方法.基于滑动窗口的方法<citation id="168" type="reference"><link href="136" rel="bibliography" /><link href="138" rel="bibliography" /><link href="140" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>是用多尺度窗口扫描图像中的所有可能的位置, 提取窗口内的特征, 使用一个分类器对其进行分类.目前主流的基于连通区域的方法<citation id="169" type="reference"><link href="142" rel="bibliography" /><link href="144" rel="bibliography" /><link href="146" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>在速度上展现了很大的优势, 基本原理是采用一个底层的滤波器快速分割文本像素和非文本像素, 然后将具有相似属性的文本像素连通起来构成连通区域, 最后再使用一个分类器对其分类.最近几年, MSER (Maximally Stable Extreme Regions) <sup></sup><citation id="166" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>由于尺度旋转不变性、仿射不变性的优势被广泛地应用于文本检测, 成为最常用的连通区域方法, 其复杂度为O (2NloglogN) <citation id="167" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 其中N为原图像的像素总数.</p>
                </div>
                <div class="p1">
                    <p id="36">上述两种方法都需要一个强大的文本分类器对滑动窗口或者连通分量进行文本和非文本的判定.人工设计的特征的低层属性很难准确地辨别有挑战的分量, 比如模糊的文本模式和复杂的背景异常值.CNN (Convolutional Neural Network) <citation id="170" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>可以避免显式地提取低层特征, 隐式地从训练数据中学习文本的高层特征, 极大地减少了复杂背景的干扰.但是如果用CNN生成候选区域, 却有两个不可忽视的缺点: (1) 产生太多的冗余窗口, 增加了算法的时间复杂度; (2) 忽略了文字本身的低层特征, 而且滑动窗口的长宽比一般都是固定的设置几个, 对于长宽比浮动较大的文本, 无法得到形状很吻合文本的区域.因此, 不同于文献<citation id="171" type="reference">[<a class="sup">10</a>]</citation>中直接采用CNN得到候选区域的做法, 本文结合 MSER和CNN各自的优点, 采用基于CNN的分类器过滤MSER检测出来的连通分量, 得到候选字符区域.受到文献<citation id="172" type="reference">[<a class="sup">11</a>]</citation>中有相似笔画宽度的单个字符可以形成一串的启发, 本文从文本行的角度进一步提取特征, 采用基于行特征的候选字符合并算法, 构建多方向的候选文本行.考虑到复杂背景所带来的干扰, 为了进一步减少虚警率, 本文采用C4.5决策树算法对候选文本行进行文本行和非文本行的判定.</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag">2 <b>本文的算法设计</b></h3>
                <div class="p1">
                    <p id="38">本文提出的基于改进型卷积神经网络和行特征的文本检测方法具体流程图如图1所示.</p>
                </div>
                <div class="area_img" id="39">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908017_039.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 文本定位流程图" src="Detail/GetImg?filename=images/WXYJ201908017_039.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>文本定位流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908017_039.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="40" name="40">2.1 <b>基于增强的</b>MSER<b>的连通区域提取与剪枝</b></h4>
                <div class="p1">
                    <p id="41">MSER一直被认为是最佳的区域检测算子.但是, 传统的MSER算法对噪声、模糊比较敏感.主要存在以下问题:由于图像模糊, 提取的字符是相连的;字符中间的孔洞没有被剔除;自然场景中包含有字符阴影的文本.为了增强图像前景和背景的对比度, 本文将Canny边缘检测算子与MSER相结合, 得到边缘增强的MSER区域<citation id="173" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>.首先利用Canny算子得到边缘图像并向梯度方向膨胀, 得到增强的二值图像, 然后将这个二值图像取反, 与MSER二值图像相与, 得到增强后的连通区域.增强后的MSER可以分割因模糊而相连的字符像素, 以及字符连通域的孔洞, 删除了部分不含文本的极值区域, 提高候选字符区域的准确率.</p>
                </div>
                <div class="p1">
                    <p id="42">经过MSER后, 得到的区域会出现嵌套的情况, 即最大区域可能包含文本区域, 也可能包含非文本区域, 甚至可能会导致不同的连通分量相交形成一个大的连通分量, 这无疑会增加后续检测过程的计算量.因为文本区域与背景区域具有显着差异, 孤立的对象在文本检测中具有良好的性能, 所以在实际操作中, 字符不能被其他字符包含或者被包含.针对这种情况, 本文对此进行剪枝操作.该剪枝算法可以用文献<citation id="174" type="reference">[<a class="sup">12</a>]</citation>中的结合平滑度的剪枝操作来修剪MSER树, 从而可以得到孤立的MSER区域.</p>
                </div>
                <div class="p1">
                    <p id="43">经过剪枝之后, 每一个像素仅属于一个连通区域, 这不仅保证了得到的连通区域的独立性, 而且不删除一些较小的区域, 最大限度的保证了文本的完整性, 为后续文本检测提供更加有效的文本连通区域, 提高了连通区域分类的准确率, 减少了计算复杂度.</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44">2.2 <b>基于改进型</b>CNN<b>的连通区域分类</b></h4>
                <div class="p1">
                    <p id="45">对于一幅给定的自然场景图像, 在使用增强的MSER和剪枝算法进行区域检测以后, 会得到大量的连通体区域, 其中包括字符区域和背景区域, 因此, 需要过滤背景区域, 保留文本区域.CNN检测器表现着良好的性能, 本文应用改进型CNN过滤大量的背景区域, 得到候选字符区域.主要的流程如图2所示.</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908017_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于改进型CNN的连通区域分类流程图" src="Detail/GetImg?filename=images/WXYJ201908017_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>基于改进型</b>CNN<b>的连通区域分类流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908017_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="47">本文的CNN模型共由6层构成, 其中, 前4层为卷积层, 最后2层为全连接层.输入为32*32像素的灰度图像.首先, 输入图像经过四个特征提取阶段逐层提取得到高层特征表示.在第一个卷积层C1中, 一共使用了4个不同的卷积核, 其大小为5*5.第二个卷积层C2采用了16个大小为3*3的卷积核.第三个卷积层C3和第四个卷积层C4的卷积核个数分别为32和64, 其大小分别为3*3和2*2.第五层是个全连接层, 神经单元数设置为600.网络的前5层后面都跟随一个修正线性单元 (ReLU) 作为激活函数, 目的是给网络引入非线性特征.第六层是网络的输出层, 该层也是一个全连接层, 采用Softmax作为最后输出层的激活函数.Softmax函数也称为归一化函数, 其输出表征了不同类别之间的相对概率, 而不是分类标签, 其优势主要有两点: (1) 衡量分类问题不再是单一的0或1, 而是属于[0, 1]区间的概率值.概率本质上是置信度的反映, 丰富了输出结果, 从而更有利于理解输入之间彼此的不同. (2) 基于不同的场景, 可以设置不同的概率输出阈值来改变预测的标签, 这样更有利于满足不同的实际应用需求.一般情况下, 50%常作为阈值点.但是, 实际上会有一些受复杂背景、光照、遮挡等因素影响的字符, 其字符概率可能只有0.49、0.48等, 十分接近分界线0.5, 如果将其判为背景区域, 漏检的情况将会较多.因此, 为了提高字符检测的召回率, 本文将字符概率大于等于0.4的连通区域判决为候选字符区域, 反之为背景区域.虽然准确率略有下降, 但是后续基于行特征的强大的候选字符合并能力, 会显著提高文本检测的准确率, 足以弥补此时准确率下降的不足.</p>
                </div>
                <div class="p1">
                    <p id="48">传统的卷积层<citation id="175" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>只有两层, 这种比较浅的网络只能学习到图像的一些边、角等基本特征, 且检测的准确率不高, 具有很大的改进空间.网络的深度逐渐加深, 能够获得更高的分类精度, 但也增加了网络的整体复杂程度, 训练复杂度会随着深度增加而急剧增加, 使得网络变得难以优化, 很容易过拟合.为了平衡分类准确率和计算复杂度之间的关系, 本文进行了大量的实验对比, 最终确定了四层卷积层, 其足以提取字符和背景的深层特征, 同时不会增加太多计算量.而且四层卷积层的设计拥有更多的非线性变换, 对特征的学习能力更强.</p>
                </div>
                <h4 class="anchor-tag" id="49" name="49">2.3 <b>多方向的候选文本行的形成</b></h4>
                <div class="p1">
                    <p id="50">在对连通区域进行分类后, 可获得大量的候选字符区域.由于上述过程只考虑了单个字符区域与背景区域的差异, 因此, 除了得到真正的字符外, 还可能会存在一部分和字符很相似的背景区域 (如:砖块、窗户等) .但是, 真正的字符区域通常是按照一定的规律出现的, 即同一文本行的字符具有相似性, 如相似的颜色, 相似的笔画宽度<citation id="176" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等.本文将根据上述规律, 在得到的候选字符区域图上进行扫描, 对每一个候选字符区域进行特征提取, 在其周围寻找任意方向的候选字符区域.如果这两个候选字符具有相似的特征, 则可以将其分成一组, 进而形成一串.这样就可以有效地将候选字符整合成候选文本行, 同时可以进一步滤除大量孤立的类似字符的背景区域, 大大地提高了文本检测的准确率.从文本行的角度, 而不是单个字符的角度提取特征, 另一个好处是, 判为背景的字符一样会被周围判为候选字符区域的字符拉入候选文本行中, 提高了文本检测的召回率.</p>
                </div>
                <div class="p1">
                    <p id="51">如果设置的特征太少、过于简单, 那么在某一受限场景下可以达到很好的检测效果, 但是鲁棒性较差, 因此, 本文采用大量特征来进行综合判定.具体特征设置如下:</p>
                </div>
                <div class="p1">
                    <p id="52"> (1) 宽度比Rw=max (W<sub>A</sub>, W<sub>B</sub>) / min (W<sub>A</sub>, W<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="53"> (2) 高度比Rh=max (H<sub>A</sub>, H<sub>B</sub>) / min (H<sub>A</sub>, H<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="54"> (3) 面积比Rs=max (S<sub>A</sub>, S<sub>B</sub>) / min (S<sub>A</sub>, S<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="55"> (4) 宽高比比值Rq=max (Q<sub>A</sub>, Q<sub>B</sub>) / min (Q<sub>A</sub>, Q<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="56"> (5) 占空比比值Rocc=max (OC<sub>A</sub>, OC<sub>B</sub>) / min (OC<sub>A</sub>, OC<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="57"> (6) 紧致度比Red=max (ED<sub>A</sub>, ED<sub>B</sub>) / min (ED<sub>A</sub>, ED<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="58"> (7) 颜色比Ry=max (Y<sub>A</sub>, Y<sub>B</sub>) / min (Y<sub>A</sub>, Y<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="59"> (8) 笔画宽度比R<sub>SW</sub>=max (SW<sub>A</sub>, SW<sub>B</sub>) / min (SW<sub>A</sub>, SW<sub>B</sub>) </p>
                </div>
                <div class="p1">
                    <p id="60">候选字符整合成候选文本行以后, 图像中可能会有少量孤立存在的字符被滤除, 但是字符概率极大, 为了避免这种情况, 本文也将其保留下来.</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61">2.4 <b>基于</b>C4.5<b>决策树的候选文本行的分类</b></h4>
                <div class="p1">
                    <p id="62">至此, 已经得到图像中的候选文本行, 但是得到的候选文本行中有可能还有非文本行的存在, 为了得到更加精确的文本行区域, 要对非文本行进行剔除, 根据机器学习算法分类具有更强的鲁棒性, 本文采用C4.5决策树<citation id="177" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>算法进一步过滤候选文本行.但是, 机器学习算法对训练集有较高的依赖度, 因此, 本文通过借助不同类特征提取, 可以克服机器学习算法对训练集的依赖度, 降低漏检率和误检率, 增强场景鲁棒性.图3给出了C4.5决策树算法的基本流程.</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908017_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 C4．5决策树算法流程" src="Detail/GetImg?filename=images/WXYJ201908017_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 C4．5决策树算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908017_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="81">根据同一文本行中的字符具有相似性, 比如相似的颜色、相似的笔画宽度等, 本文从文本行的角度进一步提取特征, 计算同一文本行相邻候选字符相同类别特征的平均值, 利用C4.5决策树算法对候选文本行进行文本行和非文本行区域分类, 能够精确地将非文本行滤除, 确定出最终的文本行.本文选用的特征与上一步多方向的候选文本行的形成所选用的特征类别相同, 具体特征设置如下:</p>
                </div>
                <div class="p1">
                    <p id="82"> (1) 宽度比的平均值;</p>
                </div>
                <div class="p1">
                    <p id="83"> (2) 高度比的平均值;</p>
                </div>
                <div class="p1">
                    <p id="84"> (3) 面积比的平均值;</p>
                </div>
                <div class="p1">
                    <p id="85"> (4) 宽高比比值的平均值;</p>
                </div>
                <div class="p1">
                    <p id="86"> (5) 占空比比值的平均值;</p>
                </div>
                <div class="p1">
                    <p id="87"> (6) 紧致度比的平均值;</p>
                </div>
                <div class="p1">
                    <p id="88"> (7) 颜色比的平均值 ;</p>
                </div>
                <div class="p1">
                    <p id="89"> (8) 笔画宽度比的平均值。</p>
                </div>
                <h3 id="90" name="90" class="anchor-tag">3 <b>实验结果与分析</b></h3>
                <div class="p1">
                    <p id="91">本文使用了ICDAR2013、ICDAR2015 和MSRA-TD500数据集, 分别进行三组实验.为了将该算法与文献<citation id="178" type="reference">[<a class="sup">13</a>]</citation>的算法进行比较, 第一组实验使用了ICDAR2013数据集进行评估.ICDAR2013数据集是最基本的文本检测数据集, 其中包含了229幅训练图像和233幅测试图像, 图像中的文本呈水平方向排列, 分辨率高, 文字清晰可见.为了与文献<citation id="179" type="reference">[<a class="sup">11</a>]</citation>的算法进行比较, 第二组实验使用了MSRA-TD500数据集进行评估.这是一个针对多方向、中英文混合的数据集, 该数据集包含了300幅训练图像和200幅测试图像.为了更好地证明本算法的性能, 第三组实验采用了ICDAR2015-偶然场景文本数据集, 其包含了1000幅训练图像和500幅测试图像, 该数据集是用穿戴设备随意拍摄的生活场景, 不是刻意针对文字拍摄的, 图像会出现模糊、遮挡等情况, 文字通常是多方向的, 因此, 检测难度最大.在这组实验中, 同样与文献<citation id="180" type="reference">[<a class="sup">11</a>]</citation>的算法进行比较.本文遵循了数据集各自的标准性能评估准则, 来测量所提出方法的性能.所有数据集的最终衡量参数包括准确率P、召回率R和F综合指标, 定义为:</p>
                </div>
                <div class="p1">
                    <p id="92">P=C/ (C+E)      (1) </p>
                </div>
                <div class="p1">
                    <p id="93">R=C/ (C+T)      (2) </p>
                </div>
                <div class="p1">
                    <p id="94">F=1/ (α/P+ (1-α) /R)      (3) </p>
                </div>
                <div class="p1">
                    <p id="95">式中, C为正确检测到的文本区域的个数;E为错误检测到的文本区域个数;T为未检测到的文本区域个数;F是准确率和召回率之间的调和平均数;α一般设置成0.5.</p>
                </div>
                <div class="p1">
                    <p id="96">算法的中间结果显示在图4上.图4 (a) 展现了增强的MSER和剪枝算法的输出.在图4 (b) 上可以看到, 许多背景区域都被滤除了, 如树叶.所有的文本区域都保留下来了, 还有像窗户这样类似字符的区域保留下来了, 这表明了本文改进型CNN模型是高度可信的.图4 (c) 展现了形成的候选文本行.孤立的背景区域没有形成文本行, 因为其特征和周围候选字符区域的特征是不相似的.最终的结果显示在图4 (d) 中.经过C4.5决策树算法, 所有的非文本区域被过滤, 所有的文本行都保留下来了.</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908017_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 中间结果" src="Detail/GetImg?filename=images/WXYJ201908017_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>中间结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908017_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="99">表1所示为本文提出的方法与文献<citation id="181" type="reference">[<a class="sup">13</a>]</citation>的算法在ICDAR2013数据集上的实验对比.</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表</b>1 <b>用</b>ICDAR2013<b>数据集评估</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td><br /></td><td>准确率P</td><td>召回率R</td><td>F值</td></tr><tr><td><br />本文算法</td><td>0.91</td><td>0.76</td><td>0.83</td></tr><tr><td><br />[13]的算法</td><td>0.83</td><td>0.71</td><td>0.77</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="101">从表1可以看出, 本文的算法在处理水平方向的文本的P, R和F值分别为0.91, 0.76, 0.83, 明显优于文献<citation id="182" type="reference">[<a class="sup">13</a>]</citation>, 准确率比文献<citation id="183" type="reference">[<a class="sup">13</a>]</citation>提高了0.08, 主要原因有以下三点: (1) 文献<citation id="184" type="reference">[<a class="sup">13</a>]</citation>进行连通区域分类时, 卷积层只有两层, 因此, 只能提取一些低层次特征, 字符的准确率欠佳;而本文采用了四层卷积层, 可以提取更加抽象的特征, 从而获得更为精确的候选字符. (2) 文献<citation id="185" type="reference">[<a class="sup">13</a>]</citation>进行候选字符合并时, 选取的特征太少, 过于简单, 合并效果不是很理想;而本文提取了八个不同类型的特征, 大大地减少了那些类似字符的背景串成文本行的可能性, 从而增强了算法的鲁棒性. (3) 文献<citation id="186" type="reference">[<a class="sup">13</a>]</citation>没有用机器学习算法进行候选文本行分类, 因此, 文本行的准确率欠佳;而本文应用了对于分类问题更加鲁棒的C4.5决策树算法, 能够有效地将类似文本行的背景区域过滤掉, 从而得到准确率更高的文本行.综上所述, 本文两级分类器出色的分类性能, 以及基于行特征的强大的候选字符合并能力, 将非文本区域几乎全部过滤掉, 极大地提高了文本检测的准确率, 证明了本文所提算法的有效性.</p>
                </div>
                <div class="p1">
                    <p id="102">为了验证本文算法的鲁棒性和泛化能力, 本文在MSRA-TD500数据集上做了进一步实验.表2所示为本文提出的自然场景文本检测方法与文献<citation id="187" type="reference">[<a class="sup">11</a>]</citation>的算法在MSRA-TD500数据集上的实验对比.</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表</b>2 <b>用</b>MSRA-TD500<b>数据集评估</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br /></td><td>准确率P</td><td>召回率R</td><td>F值</td></tr><tr><td><br />本文算法</td><td>0.86</td><td>0.74</td><td>0.80</td></tr><tr><td><br />[11]的算法</td><td>0.79</td><td>0.72</td><td>0.75</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="104">从表2可以看出, 在执行多方向文本时与文献<citation id="188" type="reference">[<a class="sup">11</a>]</citation>相比也有较好的效果, 实验结果说明了本文的方法适用于任意方向的文本, 且准确率非常高, 召回率也有一些提高, 验证了所提算法的有效性.</p>
                </div>
                <div class="p1">
                    <p id="105">为了进一步验证算法的有效性, 本文在ICDAR2015数据集上进行实验, 并与文献<citation id="189" type="reference">[<a class="sup">11</a>]</citation>的算法做对比, 结果显示在表3中.</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表</b>3 <b>用</b>ICDAR2015<b>数据集评估</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="106" border="1"><tr><td><br /></td><td>准确率P</td><td>召回率R</td><td>F值</td></tr><tr><td><br />本文算法</td><td>0.81</td><td>0.63</td><td>0.71</td></tr><tr><td><br />[11]的算法</td><td>0.72</td><td>0.58</td><td>0.64</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="107">从表3可以看出, 本文的方法相比文献<citation id="190" type="reference">[<a class="sup">11</a>]</citation>的算法, 准确率提高了9%, 具有明显的优势.因此, 算法在处理模糊、透视变形、极端光照、文本字体太小等不利因素影响的多方向文本图像, 也有较好的效果, 进一步验证了本文算法的有效性.</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">4 <b>结束语</b></h3>
                <div class="p1">
                    <p id="109">本文提出了一种基于改进型卷积神经网络和行特征的文本检测算法.应用了增强的MSER和剪枝算法得到连通区域.相比以往采用人工设计的特征或浅层网络提取特征, 本文使用基于改进型CNN的深层特征对连通区域分类, 本文从文本行的角度进一步提取特征, 采用基于行特征的候选字符合并算法, 构建准确率和召回率更高的多方向候选文本行.最后根据机器学习算法分类具有更强的鲁棒性, 本文采用C4.5决策树算法进行候选文本行的分类和定位, 从而提高了最终文本行的准确率.实验结果表明, 本文提出的算法准确率非常高, 并且可以有效地检测到图像中任意方向的文本, 对文本的语言、字体等也有较强的鲁棒性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="136">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting and Reading Text in Natural Scenes">

                                <b>[1]</b> CHEN X, YUILLE A L.Detecting and reading text in natural scenes[C]// IEEE Computer Society Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2004:366-373.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaboost for text detection in natural scene">

                                <b>[2]</b> LEE J J, LEE P H, LEE S W, et al.AdaBoost for Text Detection in Natural Scene[C]// International Conference on Document Analysis and Recognition.IEEE, 2011:429-434.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Word Spotting in the Wild">

                                <b>[3]</b> WANG K, BELONGIE S.Word Spotting in the Wild[C]// European Conference on Computer Vision.Springer-Verlag, 2010:591-604.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Fast Stroke-Based Method for Text Detection in Video">

                                <b>[4]</b> BAI B, YIN F, LIU C L.A Fast Stroke-Based Method for Text Detection in Video[C]// Iapr International Workshop on Document Analysis Systems.IEEE, 2012:69-73.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robustly Extracting Captions in Videos Based on Stroke-Like Edges and Spatio-Temporal Analysis">

                                <b>[5]</b> LIU X, WANG W.Robustly Extracting Captions in Videos Based on Stroke-Like Edges and Spatio-Temporal Analysis[M].IEEE Press, 2012.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Robust Color-independent Text DetectionMethod from Complex Videos">

                                <b>[6]</b> ZHAO Y, LU T, LIAO W.A Robust Color-Independent Text Detection Method from Complex Videos[C]// International Conference on Document Analysis and Recognition.IEEE Computer Society, 2011:374-378.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MjE1NjV0RE9yWTlFWis4R0MzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lJVndUYWhBPU5pZk9mYks3SA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> MATAS J, CHUM O, URBAN M, et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image &amp; Vision Computing, 2004, 22 (10) :761-767.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201801047&amp;v=MTU4MDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bm5WTC9LTHo3QmQ3RzRIOW5Ncm85Qlk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 范一华, 邓德祥, 颜佳.基于色彩空间的最大稳定极值区域的自然场景文本检测[J].计算机应用, 2018, 38 (1) :264-269.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[9]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems.Curran Associates Inc.2012:1097-1105.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[10]</b> REN S, HE K, GIRSHICK R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201805030&amp;v=MzE3MjFyQ1VSTE9lWmVWdUZ5bm5WTC9LTmlmWVpMRzRIOW5NcW85R1pJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 方承志, 黄梅玲.自然场景中多方向文本的检测[J].计算机工程与设计, 2018, 39 (5) :1377-1381.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DSSS201521002&amp;v=MjU5MjZHNEg5VE9ybzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlublZML0tJVDdZZmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 方承志, 田彪.结合连通分量规则度与Adaboost的文本定位算法[J].电视技术, 2015, 39 (21) :1-4.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel scene text detection algorithm based on convolutional neural network">

                                <b>[13]</b> REN X, CHEN K, YANG X, et al.A novel scene text detection algorithm based on convolutional neural network[C]// Visual Communications and Image Processing (VCIP) , 2016.IEEE, 2016:1-4.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Detecting Text in Natural Scenes with Stroke Width Transform,&amp;quot;">

                                <b>[14]</b> EPSHTEIN B, OFEK E, WEXLER Y.Detecting text in natural scenes with stroke width transform[C]// Computer Vision and Pattern Recognition.IEEE, 2010:2963-2970.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016303209.nh&amp;v=MTY4NjlkUE1wcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnlublZML0tWRjI2R0xDNEg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 田彪.自然场景图像中的文字检测关键算法研究[D].南京:南京邮电大学, 2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201908017" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201908017&amp;v=MDcyMDRaTEc0SDlqTXA0OUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5uVkwvTE1qWFM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
