<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134027058537500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201904012%26RESULT%3d1%26SIGN%3daMOeXycTGF9h3S4G8uwhAjCBB9I%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201904012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201904012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201904012&amp;v=MTY4ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXptVjc3TU1qWFNaTEc0SDlqTXE0OUVab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#21" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#25" data-title="2 &lt;b&gt;卷积神经网络算法分析&lt;/b&gt; ">2 <b>卷积神经网络算法分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#26" data-title="2.1 &lt;b&gt;卷积神经网络拓扑结构&lt;/b&gt;">2.1 <b>卷积神经网络拓扑结构</b></a></li>
                                                <li><a href="#28" data-title="2.2 &lt;b&gt;卷积神经网络前向传播模型&lt;/b&gt;">2.2 <b>卷积神经网络前向传播模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="3 FPGA&lt;b&gt;的设计实现&lt;/b&gt; ">3 FPGA<b>的设计实现</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="3.1 CNN&lt;b&gt;的并行结构分析&lt;/b&gt;">3.1 CNN<b>的并行结构分析</b></a></li>
                                                <li><a href="#53" data-title="3.2 FPGA&lt;b&gt;整体架构设计&lt;/b&gt;">3.2 FPGA<b>整体架构设计</b></a></li>
                                                <li><a href="#56" data-title="3.3 &lt;b&gt;卷积运算模块设计&lt;/b&gt;">3.3 <b>卷积运算模块设计</b></a></li>
                                                <li><a href="#65" data-title="3.4 sigmoid&lt;b&gt;函数运算模块设计&lt;/b&gt;">3.4 sigmoid<b>函数运算模块设计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="4 &lt;b&gt;结果分析与讨论&lt;/b&gt; ">4 <b>结果分析与讨论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="5 &lt;b&gt;结束语&lt;/b&gt; ">5 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#30" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;卷积神经网络结构拓扑图&lt;/b&gt;"><b>图</b>1 <b>卷积神经网络结构拓扑图</b></a></li>
                                                <li><a href="#31" data-title="&lt;b&gt;表&lt;/b&gt;1 sigmoid&lt;b&gt;函数正区间内分段逼近函数表达式&lt;/b&gt;"><b>表</b>1 sigmoid<b>函数正区间内分段逼近函数表达式</b></a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;图&lt;/b&gt;2 CNN&lt;b&gt;卷积神经网络硬件架构图&lt;/b&gt;"><b>图</b>2 CNN<b>卷积神经网络硬件架构图</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;图&lt;/b&gt;3 &lt;b&gt;带行缓存的并行流水线卷积计算模块&lt;/b&gt;"><b>图</b>3 <b>带行缓存的并行流水线卷积计算模块</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;第二个卷积层的部分并行电路设计及缓存设计&lt;/b&gt;"><b>图</b>4 <b>第二个卷积层的部分并行电路设计及缓存设计</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表&lt;/b&gt;2 sigmoid&lt;b&gt;函数正区间内分段逼近函数表达式&lt;/b&gt;"><b>表</b>2 sigmoid<b>函数正区间内分段逼近函数表达式</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;图&lt;/b&gt;5 sigmoid&lt;b&gt;函数的硬件电路模块&lt;/b&gt;"><b>图</b>5 sigmoid<b>函数的硬件电路模块</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;图&lt;/b&gt;6 &lt;b&gt;卷积神经网络算法的仿真结果&lt;/b&gt;"><b>图</b>6 <b>卷积神经网络算法的仿真结果</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;卷积神经网络&lt;/b&gt;FPGA&lt;b&gt;实现的性能比较&lt;/b&gt;"><b>表</b>3 <b>卷积神经网络</b>FPGA<b>实现的性能比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[1]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" GHAFFARI S, SHARIFIAN S. FPGA-based convolutional neural network accelerator design using high level synthesize[C]// International Conference of Signal Processing and Intelligent Systems. IEEE, 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FPGA-based convolutional neural network accelerator design using high level synthesize">
                                        <b>[2]</b>
                                         GHAFFARI S, SHARIFIAN S. FPGA-based convolutional neural network accelerator design using high level synthesize[C]// International Conference of Signal Processing and Intelligent Systems. IEEE, 2017:1-6.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" GOKHALE V, JIN J, DUNDAR A, et al. A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks[C]// Computer Vision and Pattern Recognition Workshops. IEEE, 2014:696-701." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A 240G-ops/s mobile coprocessor for deep neural networks">
                                        <b>[3]</b>
                                         GOKHALE V, JIN J, DUNDAR A, et al. A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks[C]// Computer Vision and Pattern Recognition Workshops. IEEE, 2014:696-701.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" CHEN Y H, KRISHNA T, EMER J, et al. 14.5 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks[C]// IEEE International Solid-State Circuits Conference. IEEE, 2016:262-263." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=14.5 Eyeriss:An energy-efficient reconfigurable accelerator for deep convolutional neural networks">
                                        <b>[4]</b>
                                         CHEN Y H, KRISHNA T, EMER J, et al. 14.5 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks[C]// IEEE International Solid-State Circuits Conference. IEEE, 2016:262-263.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" FARABET C, POULET C, HAN J Y, et al. CNP: An FPGA-based processor for Convolutional Networks[C]// International Conference on Field Programmable Logic and Applications. IEEE, 2009:32-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An fpga-based processor for convolutional networks">
                                        <b>[5]</b>
                                         FARABET C, POULET C, HAN J Y, et al. CNP: An FPGA-based processor for Convolutional Networks[C]// International Conference on Field Programmable Logic and Applications. IEEE, 2009:32-37.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" FENG G, HU Z, CHEN S, et al. Energy-efficient and high-throughput FPGA-based accelerator for Convolutional Neural Networks[C]// IEEE International Conference on Solid-State and Integrated Circuit Technology. IEEE, 2017:624-626." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-efficient and high-throughput FPGA-based accelerator for Convolutional Neural Networks">
                                        <b>[6]</b>
                                         FENG G, HU Z, CHEN S, et al. Energy-efficient and high-throughput FPGA-based accelerator for Convolutional Neural Networks[C]// IEEE International Conference on Solid-State and Integrated Circuit Technology. IEEE, 2017:624-626.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" CHEN ZHANG, PENG LI, GUANGYU SUN, et al. Optimizing fpga-based accelerator design for deep convolutional neural networks[C]// in Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2015:161-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks">
                                        <b>[7]</b>
                                         CHEN ZHANG, PENG LI, GUANGYU SUN, et al. Optimizing fpga-based accelerator design for deep convolutional neural networks[C]// in Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2015:161-170.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" ZHOU Y, JIANG J. An FPGA-based accelerator implementation for deep convolutional neural networks[C]// International Conference on Computer Science and Network Technology. IEEE, 2016:829-832." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An FPGA-based accelerator implementation for deep convolutional neural networks">
                                        <b>[8]</b>
                                         ZHOU Y, JIANG J. An FPGA-based accelerator implementation for deep convolutional neural networks[C]// International Conference on Computer Science and Network Technology. IEEE, 2016:829-832.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" JIANG J, HU R, LUJAN M. A Flexible Memory Controller Supporting Deep Belief Networks with Fixed-Point Arithmetic[C]// Parallel and Distributed Processing Symposium Workshops &amp;amp; Phd Forum. IEEE, 2013:144-152." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Flexible Memory Controller Supporting Deep Belief Networks with Fixed-Point Arithmetic">
                                        <b>[9]</b>
                                         JIANG J, HU R, LUJAN M. A Flexible Memory Controller Supporting Deep Belief Networks with Fixed-Point Arithmetic[C]// Parallel and Distributed Processing Symposium Workshops &amp;amp; Phd Forum. IEEE, 2013:144-152.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(04),57-62+66             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>卷积神经网络 (CNN) 算法的FPGA并行结构设计</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%B7%8D&amp;code=14189641&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王巍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E5%87%AF%E5%88%A9&amp;code=38228067&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周凯利</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%BC%8A%E6%98%8C&amp;code=38705100&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王伊昌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%B9%BF&amp;code=37603627&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王广</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%AD%A3%E7%90%B3&amp;code=34102286&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨正琳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E5%86%9B&amp;code=30248120&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E5%85%89%E7%94%B5%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%2F%E5%9B%BD%E9%99%85%E5%8D%8A%E5%AF%BC%E4%BD%93%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学光电工程学院/国际半导体学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>本文进行了CNN算法的FPGA并行结构设计.该设计首先利用CNN的并行计算特征以及循环变换方法, 实现了可高效进行并行流水线的卷积计算电路, 然后利用能够减少存储器访存时间的双缓存技术, 在输入输出部分实现了缓存阵列, 用于提高电路的计算性能 (GOPS, 每秒十亿次运算数) .同时本文还对激活函数进行了优化设计, 利用查找表和多项式结合的分段拟合方法设计了激活函数 (sigmoid) 的硬件电路, 以保证近似的激活函数的硬件电路不会使精度下降.实验结果表明:输入时钟为150 MHz时, 整体电路在计算性能上由15.87 GOPS提高到了20.62 GOPS, 并在MNIST数据集上的识别率达到了98.81%.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%8E%B0%E5%9C%BA%E5%8F%AF%E7%BC%96%E7%A8%8B%E9%97%A8%E9%98%B5%E5%88%97%20(FPGA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">现场可编程门阵列 (FPGA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B6%E8%A1%8C%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">并行结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%81%E6%B0%B4%E7%BA%BF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">流水线;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王巍 男, (1967-) , 博士后, 教授.研究方向为集成电路设计.;
                                </span>
                                <span>
                                    *周凯利 (通讯作者) 女, (1991-) , 硕士研究生.研究方向为数字集成电路设计.E-mail:2508005354@qq.com.;
                                </span>
                                <span>
                                    王伊昌 男, (1996-) , 硕士研究生.研究方向为模拟集成电路设计.;
                                </span>
                                <span>
                                    王广 男, (1994-) , 硕士研究生.研究方向为半导体光电器件设计.;
                                </span>
                                <span>
                                    杨正琳 男, (1980-) , 博士, 副教授.研究方向为模拟集成电路设计.;
                                </span>
                                <span>
                                    袁军 男, (1984-) , 博士, 副教授.研究方向为数模混合集成电路设计.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61404019);</span>
                                <span>重庆市基础与前沿研究计划项目 (cstc2016jcyjA0272);</span>
                    </p>
            </div>
                    <h1><b>FPGA Parallel Structure Design of Convolutional Neural Network (CNN) Algorithm</b></h1>
                    <h2>
                    <span>WANG Wei</span>
                    <span>ZHOU Kai-li</span>
                    <span>WANG Yi-chang</span>
                    <span>WANG Guang</span>
                    <span>YANG Zheng-lin</span>
                    <span>YUAN Jun</span>
            </h2>
                    <h2>
                    <span>College of Electronics Engineering/International Semiconductor College, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In this paper, the FPGA parallel structure design of CNN algorithm is carried out. The design first uses the parallel computing features of CNN and the cyclic transformation method to realize a convolution calculation circuit that can efficiently perform parallel pipelines. Then, using the double-buffer technology that can reduce the memory access time, a cache array is implemented in the input and output sections to improve the computational performance of the circuit (GOPS, one billion operations per second) . At the same time, the activation function is optimized. The hardware circuit of the activation function (sigmoid) is designed by using the segmentation fitting method of lookup table and polynomial to ensure that the hardware circuit of the approximate activation function will not reduce the accuracy. The experimental results show that when the input clock is 150 MHz, the overall performance of the circuit is improved from 15.87 GOPS to 20.62 GOPS, and the recognition rate on the MNIST data set reaches 98.81%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=field%20programmable%20gate%20array%20(FPGA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">field programmable gate array (FPGA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=parallel%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">parallel structure;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pipeline&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pipeline;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="21" name="21" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="22">近年来, 神经网络领域依靠深度学习技术使其算法性能方面的优异表现被广泛关注, 其中卷积神经网络表现突出, 它的一个典型应用场景是图像中手写数字的识别.但是卷积神经网络中存在大量独立、重复的乘法和加法运算, 并且这些计算过程涉及大量的数据访问以及中间数据的存储, 这使得研究人员非常关注计算的性能, 并行度和功耗.而现有的通用计算平台 (CPU、GPU<citation id="81" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>) 具有一些不足, 所以研究人员在算法实现上提出了许多基于FPGA<citation id="82" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, SOC<citation id="83" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和ASIC<citation id="84" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的硬件加速实现方案.</p>
                </div>
                <div class="p1">
                    <p id="23">为了提高CNN算法的硬件电路性能, 采用具有很多专用算术功能单元、大量通用逻辑资源、片上存储资源、外围I/O接口和高速网络接口的FPGA器件是一个非常好的选择<citation id="85" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.目前, 相关方面的研究也得到了越来越多的关注<citation id="89" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>.文献<citation id="86" type="reference">[<a class="sup">6</a>]</citation>提出了CNN卷积核的硬件电路, 并利用了流水线和循环展开来优化不同的循环迭代中重叠循环体的执行, 使得数据共享的使用程度大大影响吞吐量, 但是在嵌套循环重构方面的设计并不合理, 虽然节省了硬件资源, 但是在计算性能 (GOPS, 每秒十亿次运算数) 方面没有太大提升.文献<citation id="87" type="reference">[<a class="sup">8</a>]</citation>提出增加足够的计算资源来并行执行卷积计算从而产生输出特征图的一行值, 并且在不影响识别精度的情况下探究了合理的定点表示<citation id="88" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 但是这种计算模式会造成计算单元的利用率不高, 虽然提高了计算性能 (GOPS) , 但是消耗了太多的硬件资源.尽管这些FPGA的实现报告良好的计算性能, 但是还存在改进的空间.</p>
                </div>
                <div class="p1">
                    <p id="24">本文介绍一种卷积神经网络 (CNN) 算法的FPGA并行架构设计, 其首先利用CNN的并行计算特征以及循环变换方法, 实现了可高效进行并行流水线的卷积计算电路, 用于提高电路的整体计算性能 (GOPS) , 并减少硬件资源的使用;然后在输入输出部分还加入了双缓冲用于减少存储器的访存时间.同时还对激活函数进行了优化设计, 以保证近似的激活函数的硬件电路不会使精度下降.</p>
                </div>
                <h3 id="25" name="25" class="anchor-tag">2 <b>卷积神经网络算法分析</b></h3>
                <h4 class="anchor-tag" id="26" name="26">2.1 <b>卷积神经网络拓扑结构</b></h4>
                <div class="p1">
                    <p id="27">图1所示为本文采用的应用于手写数字识别的卷积神经网络模型结构拓扑图.卷积神经网络利用了图像的空间关系, 使得在网络结构上有三个特性:局部连接, 权重共享以及空间或时间上的次采样.这使得它能够对大量有标签的数据进行自动学习并从中提取复杂特征.CNN模型中包括一个输入层 (input) 、两个卷积层 (C1和C2) 、两个池化层 (S1和S2) 和一个全连接层 (FC) .其中每层由多个二维平面组成, 二维平面里包含多个独立的神经元.</p>
                </div>
                <h4 class="anchor-tag" id="28" name="28">2.2 <b>卷积神经网络前向传播模型</b></h4>
                <div class="p1">
                    <p id="29">本文在LeNet卷积神经网络的基础上优化了激活函数和池化计算方式, 使得算法能够更有利于硬件设计.同时在训练阶段, 优化了学习率, 以便算法能够更好地寻找最优值, 从而提高手写数字识别的识别率.在训练开始后, 随着循环迭代次数的增加, 学习率的设置不再采用LeNet卷积神经网络中固定值的方式而是以变化的方式逐渐越小.MATLAB仿真该优化模型的实验结果显示对于手写数字MNIST测试集的识别率达到了98.81%.在该模型中涉及的权重系数 (weights) 以及偏置系数 (biases) 个数如表1.</p>
                </div>
                <div class="area_img" id="30">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904012_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 卷积神经网络结构拓扑图" src="Detail/GetImg?filename=images/WXYJ201904012_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>卷积神经网络结构拓扑图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904012_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="31">
                    <p class="img_tit"><b>表</b>1 sigmoid<b>函数正区间内分段逼近函数表达式</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="31" border="1"><tr><td><br />层类型</td><td>特征图尺寸</td><td>参数个数 (weights+biases) </td></tr><tr><td><br />卷积层1</td><td>28×28×6</td><td>5×5×6+6=156</td></tr><tr><td><br />池化层1</td><td>14×14×6</td><td>--</td></tr><tr><td><br />卷积层2</td><td>8×8×12</td><td>5×5×12+12=312</td></tr><tr><td><br />池化层2</td><td>4×4×12</td><td>--</td></tr><tr><td><br />全连接层</td><td>10</td><td>4×4×12+10=202</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="32">卷积层用来将卷积核与前一层输出特征图的局部感受野相连, 从而提取该局部特征.卷积运算如公式 (1) 表示.同时优化后的激活函数, 选用了更有利于硬件实现的修正线性单元 (relu) 函数, 如公式 (2) .</p>
                </div>
                <div class="p1">
                    <p id="33" class="code-formula">
                        <mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>Μ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>h</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>*</mo><mi>k</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><mo>, </mo><mi>x</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mi>x</mi><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="34">式中, <i>h</i><mathml id="35"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为第<i>l</i>隐含层的第<i>j</i>个输出特征图;<i>k</i><mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>为第<i>l</i>层的第<i>j</i>个卷积核的第<i>i</i>个系数;<i>b</i><mathml id="37"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为第<i>l</i>卷积层的第<i>j</i>个输出特征对应的偏置系数;<i>M</i><sub><i>i</i></sub>为第<i>l</i>层卷积运算对输入特征图的选择;<i>f</i> (<i>x</i>) 为激活函数.</p>
                </div>
                <div class="p1">
                    <p id="38">池化层用来控制过拟合, 通过对特征进行二次提取, 使模型有较高的畸变容错能力, 但不改变特征图的数目.池化操作把输入特征图分割为不重叠的2×2像素小块, 每块对应的输出结果为这四个像素的均值或最大值, 因此也被称为“平均池化”或“最大池化”.其中, 最大池化操作不需要算术计算单元, 有利于FPGA实现, 所以本层被设置为最大池化而不再是平均池化, 公式如 (3) .</p>
                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>p</mtext><mtext>o</mtext><mtext>o</mtext><mtext>l</mtext><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><msubsup><mrow></mrow><mrow><mi>λ</mi><mo>, </mo><mi>τ</mi></mrow><mi>A</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>λ</mi><mo>, </mo><mi>τ</mi></mrow></munder><mo stretchy="false"> (</mo><mrow><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>t</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msub><mrow></mrow><mrow><mi>λ</mi><mo>×</mo><mi>τ</mi></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="40">式中, 特征图<i>A</i>的不重叠块大小为<i>λ</i>×<i>τ</i>;第<i>ij</i>个块为<i>G</i><mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>λ</mi><mo>, </mo><mi>τ</mi></mrow><mi>A</mi></msubsup></mrow></math></mathml> (<i>i</i>, <i>j</i>) ;<i>s</i>和<i>t</i>为特征图<i>A</i>中像素<i>a</i>的位置.</p>
                </div>
                <div class="p1">
                    <p id="42">全连接层主要用来分类, 它可以把所有局部特征结合起来抽象成全局特征.该层的激活函数采用sigmoid函数, 其输出公式如式 (4) , 其中的激活函数如公式 (5) .</p>
                </div>
                <div class="p1">
                    <p id="43"><i>o</i><mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>=<i>f</i> (∑<i>w</i><sup><i>l</i></sup>·<i>h</i><sup> (<i>l</i>-1) </sup>+<i>b</i><mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">式中, <i>o</i><mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>表示第<i>l</i>输出层的第<i>j</i>个输出神经元;<i>w</i><sup><i>l</i></sup>表示第<i>l</i>输出层的连接权值;<i>h</i><sup> (<i>l</i>-1) </sup>表示第<i>l</i>-1层的第<i>i</i>个输入特征图;<i>f</i> (<i>x</i>) 为激活函数;<i>σ</i> (<i>x</i>) 为sigmoid函数的表示形式.</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag">3 FPGA<b>的设计实现</b></h3>
                <h4 class="anchor-tag" id="50" name="50">3.1 CNN<b>的并行结构分析</b></h4>
                <div class="p1">
                    <p id="51">论文<citation id="90" type="reference">[<a class="sup">3</a>]</citation>指出在卷积神经网络的计算中, 卷积层的输出是由多个嵌套循环卷积计算组成, 这使得卷积层的计算消耗大量的乘加运算单元并且耗时最多.而这种情况很适合将算法映射到FPGA的体系结构中实现硬件加速, 从而在最大程度上挖掘网络模型的并行性特征.其中的并行特征可以概括4种:层间并行性、输出间并行性、卷积核间并行性和卷积核内并行性.由于FPGA片上资源是有限的, 无法设计完全并行的计算结构, 因此只能对部分卷积核进行并行结构计算.首先, 我们需要利用输出间并行性对多个循环嵌套卷积计算进行计算划分, 其中使用了循环变换方法, 主要包括采用循环分块优化全局多个循环迭代的分块划分, 以及采用循环展开实现分块循环结构到局部并行结构的设计, 来构建直接用于卷积神经网络卷积层的计算电路.其中, 我们只对输入特征图数量N和输出特征图数量M这两个维度进行循环分块, 循环分块尺寸用&lt;输出特征图T<i>m</i>, 输入特征图T<i>n</i>&gt;表示.我们采用均匀分块对多个卷积运算循环进行处理, 第一个卷积层的循环分块尺寸为&lt;T<i>m</i>, T<i>n</i>&gt;=&lt;3, 1&gt;, 第二个卷积层的循环分块尺寸为&lt;T<i>m</i>, T<i>n</i>&gt;=&lt;6, 3&gt;.这种循环分块方法能够很自然的组成多输入多输出内部并行结构, 从而达到充分开发网络模型的并行性特征.</p>
                </div>
                <div class="p1">
                    <p id="52">而循环展开则利用了卷积核间的并行性, 对块内循环的多输入单输出的特征提取过程, 进行多个卷积运算的并行处理.这种多输入单输出并行更有利于减少提取一张输出特征图的执行时间.对于层间并行性, 主要体现在系统初始化时参数的装载阶段, 即网络中各层的权值参数可以同时并行加载.这是因为卷积神经网络是一种多层的网络结构, 其中整个网络的数据是层层递进传播和处理的, 前后层之间的运算有很强的相关性, 使得层间计算的并行性很低.对于卷积核内并行性, 主要是利用输入数据之间不存在数据相关的特性, 把大小为K×K的卷积核中全部的K<sup>2</sup>次乘累加运算都并行实现.</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">3.2 FPGA<b>整体架构设计</b></h4>
                <div class="p1">
                    <p id="54">本文在只考虑前向传播的情况下, 利用卷积神经网络的并行计算特征及循环变换方法来确定整体架构中的设计划分, 并在FPGA平台上实现了由若干并行或串行子系统构成的层级结构.图2所示为CNN卷积神经网络硬件架构图, 其中, 各子系统层对应了卷积神经网络中的特征映射图层.整体架构的子系统主要包括输入数据缓存模块、权值数据的存储模块、卷积计算模块、池化计算模块、全连接层内积运算模块、激活函数 (sigmoid) 模块以及控制模块.其中控制模块控制数据流在架构内各模块之间传递的先后关系以及模块之间互联通信的控制、握手信号.首先, 控制逻辑产生数据请求信号, 在该信号的控制下输入的像素数据流与系统时钟同步, 并在每个周期后更新数据.然后输入的像素数据被送入到系统中各个并行或串行子系统的功能模块, 完成若干个特征映射图的运算.最后, 在经过逐层特征提取后, 需要进行特征分类, 它把多路局部特征数据流组合成全局特征送入到全连接层内积运算模块进行分类.</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904012_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN卷积神经网络硬件架构图" src="Detail/GetImg?filename=images/WXYJ201904012_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 CNN<b>卷积神经网络硬件架构图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904012_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="56" name="56">3.3 <b>卷积运算模块设计</b></h4>
                <div class="p1">
                    <p id="57">卷积运算在数学形式上可以看作是由若干个乘累加运算部件组成, 但在硬件设计上还需要考虑不同的数据缓存结构对卷积计算模块的性能影响, 以及不同的硬件结构需要合适的存储方式来配合计算过程中的流水线设计.为了满足性能要求, 我们不但要考虑卷积层中通用卷积运算的设计, 还要对循环分块后每个卷积层中多输入多输出并行结构进行合理的设计.在卷积核内及层间可以采用流水线式数据通路来提高数据运算的计算性能 (GOPS) .</p>
                </div>
                <div class="p1">
                    <p id="58">首先, 本文的卷积计算模块是关于5×5卷积核的卷积运算.这里为了更好的显示电路结构, 我们简化地给出3×3卷积运算电路, 如图3所示.图3的设计通过行缓存结构搭配转置后的流水线卷积结构组成了一种新的能高效进行并行流水线操作的卷积计算电路.其中行缓存结构实现图像的行列对齐, 并保证流水线的通畅, 提高数据运算的计算性能.而转置后的流水线结构使得我们不用再给电路的输入数据提供额外的移位寄存器, 也不用为达了获得高通过量给乘积的加法器添加额外的流水线级.同时该电路的存储结构能够充分开发卷积窗口内的数据重用.</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904012_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 带行缓存的并行流水线卷积计算模块" src="Detail/GetImg?filename=images/WXYJ201904012_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 <b>带行缓存的并行流水线卷积计算模块</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904012_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="60">在图3的电路中, 为了不破坏流水线的流通性, 电路单元需要一定的时间将数据装满整个行缓存结构, 然后转置电路结构的流水线开始流通.具体的电路组成主要包括K (卷积核大小) 个长度为N (输入图像的宽度) 的FIFO缓存阵列作为输入的行缓存;以及K<sup>2</sup>个乘法器和 K<sup>2</sup>个加法器用作卷积计算.其中K<sup>2</sup>-1个加法器用作二维卷积基本计算单元自身的卷积运算, 另外一个加法器作为多个二维卷积计算单元的累加单元.在开始的时候, 输入图像的数据首先按照逐行扫描的顺序进入FIFO阵列进行缓存, 在保证数据装满整个行缓存后, 每行电路开始同时读取FIFO阵列上的输入数据, 并将FIFO阵列上的每行输入数据分别广播到对应的K个乘法器上, 用于与存储器中的权值相乘.接着将得到的乘积结果通过加法器链逐级累加, 其中两个临近加法器之间的寄存器用于流水线的时钟同步, 并且在计算下一个输入像素时, 为后面的加法器提供前一个时钟周期的部分累加结果.直到所有的卷积窗口内的像素数据移动完毕.</p>
                </div>
                <div class="p1">
                    <p id="61">然后, 对于循环分块后卷积层的多输入多输出并行结构, 我们也采用了流水线式数据通路和输入输出缓存结构来提高数据运算的计算性能 (GOPS) .图4所示为第二个卷积层的部分并行电路设计及缓存设计.其中, 设计为了使存储器的访问不影响计算性能, 保证计算单元持续工作, 在每层循环分块后的输入和输出部分都进行了乒乓缓存设计用于实现数据存储, 即当前的缓存在工作完成时, 另一块缓存已经存储好下一次需要的数据.</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904012_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 第二个卷积层的部分并行电路设计及缓存设计" src="Detail/GetImg?filename=images/WXYJ201904012_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>第二个卷积层的部分并行电路设计及缓存设计</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904012_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">在图4的电路中, 我们使用图3中的卷积计算模块组成了图4中对应的3个并行卷积电路, 也就是同时对3张输入特征图进行卷积计算.这种多输入单输出结构是因为, 对于第二个卷积层, 实际的输入特征图的数量有6个, 输出特征图的数量有12个, 但是在利用了循环分块进行硬件设计后, 输入特征图的数量变成3个.所以图4是结合卷积间并行特征, 利用卷积计算模块中的多个二维卷积累加单元, 累加输出1张输出特征图的中间结果.其中输出的乒乓缓存BRAM-1用于缓存循环分块中的第一组循环结果, BRAM-2用于缓存循环分块中的第二组循环结果.又因为输出特征图的数量变成了6个, 而图4只能用于生成1张特征图, 所以本文使用6个图4的电路结构组成了第二个卷积层的多输入多输出并行运算电路, 即需要18个卷积计算模块.而对于第一个卷积层的并行设计, 其设计思路与第二层类似, 只是输入特征图的数量为1张, 但有3个并行卷积电路, 同时对同一张输入特征图做卷积运算, 并且在卷积运算结束后直接把卷积结果存入上缓存中, 而不需要累加多个二维卷积结果.</p>
                </div>
                <div class="p1">
                    <p id="64">综上可知, 带行缓存的并行流水线卷积电路, 在流水线填充完毕后能够保证每个时钟周期内, 可以输出相关特征图中某一个像素点上的值.而每一层的多输入多输出并行电路, 在块循环全部结束后, 每一层可以输出相关特征图中全部的像素点上的值.这有利于提高电路的整体计算性能 (GOPS) .</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">3.4 sigmoid<b>函数运算模块设计</b></h4>
                <div class="p1">
                    <p id="66">激活函数是实现神经网络计算的关键环节, 它主要用来加入非线性因素, 通过线性变化解决线性不可分问题.其中, relu函数可以直接在FPGA上实现.而S型函数sigmoid, 则不能在PFGA上直接实现, 它需要使用一些函数逼近的方法来近似.常用的逼近方法有:查表法、泰勒级数展开法、分段线性函数逼近法、多项式拟合法和CORDIC算法.其中, 查表法属于最简单的方法, 在精度要求高时, 会消耗大量的存储资源.泰勒级数展开法需要很多阶数, 涉及大量乘法操作.多项式拟合法也存在消耗大量乘法器的问题.CORDIC算法的运算过程消耗的时间长, 并且精度不高, 但是占用的资源少.</p>
                </div>
                <div class="p1">
                    <p id="67">为了更好的实现sigmoid函数, 本文结合sigmoid函数关于点 (0, 0.5) 对称的性质, 以及函数曲线在y轴附近弧度明显, 而在远离y轴时, 曲线比较平坦的性质, 采用了查表法和多项式拟合相结合的方法对sigmoid函数进行分段逼近, 而且只计算[0, +∞) 区间的函数值.这样做是为了更好地平衡近似的激活函数的硬件电路在计算精度与资源消耗之间的矛盾.首先, 借助MATLAB软件分段逼近sigmoid函数, 确定分段区间、分段数目及分段区间内的拟合多项式和近似常数值, 并保证误差的数量级在0.001以下.在MATLAB中, 使用函数polyfit来求拟合多项式中每一阶的系数.然后, 对分段拟合后的sigmoid函数的函数值与理论值之间的误差绝对值进行分析, 其中最大的误差绝对值为8.62×10<sup>-4</sup>, 误差的数量级满足0.001以下.表2显示了sigmoid函数正区间内分段逼近函数的表达式.正区间被分为了12个子区间.</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表</b>2 sigmoid<b>函数正区间内分段逼近函数表达式</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td><br />区间</td><td>逼近函数表达式</td></tr><tr><td><br /><i>x</i>13∈[0, 1 ) </td><td><i>y</i>=-0.0279<sup>*</sup><i>x</i>13.<sup>∧</sup>2+0.2605<sup>*</sup><i>x</i>13+0.4992</td></tr><tr><td><br /><i>x</i>14∈[1, 2 ) </td><td><i>y</i>=-0.0467<sup>*</sup><i>x</i>14.<sup>∧</sup>2+0.2896<sup>*</sup><i>x</i>14+0.4882</td></tr><tr><td><br /><i>x</i>15∈[2, 3 ) </td><td><i>y</i>=-0.0298<sup>*</sup><i>x</i>15.<sup>∧</sup>2+0.2202<sup>*</sup><i>x</i>15+0.5600</td></tr><tr><td><br /><i>x</i>16∈[3, 4 ) </td><td><i>y</i>=-0.0135<sup>*</sup><i>x</i>16.<sup>∧</sup>2+0.1239<sup>*</sup><i>x</i>16+0.7031</td></tr><tr><td><br /><i>x</i>17∈[4, 5 ) </td><td><i>y</i>=-0.0054<sup>*</sup><i>x</i>17.<sup>∧</sup>2+0.0597<sup>*</sup><i>x</i>17+0.8297</td></tr><tr><td><br /><i>x</i>18∈[5, 5.02 ) </td><td><i>y</i>=0.9930;</td></tr><tr><td><br /><i>x</i>19∈[5.02, 5.19) </td><td><i>y</i>=0.9940;</td></tr><tr><td><br /><i>x</i>20∈[5.19, 5.39) </td><td><i>y</i>=0.9950;</td></tr><tr><td><br /><i>x</i>21∈[5.39, 5.64) </td><td><i>y</i>=0.9960;</td></tr><tr><td><br /><i>x</i>22∈[5.64, 5.97) </td><td><i>y</i>=0.9970;</td></tr><tr><td><br /><i>x</i>23∈[5.97, 6.47) </td><td><i>y</i>=0.9980;</td></tr><tr><td><br /><i>x</i>24∈[6.47, 7.55) </td><td><i>y</i>=0.9990;</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">在FPGA中, 卷积神经网络识别的准确性与数据精度高度相关.为了不影响识别结果, 本文把输入输出数据的定点位宽设置为16位, 其中第一位为符号位<citation id="91" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>.由表2可知, 本文首先需要对二次函数<i>y</i>=<i>a</i><sup>*</sup><i>x</i>^2+<i>b</i><sup>*</sup><i>x</i>+<i>c</i>进行硬件设计.为了优化硬件设计, 对二次函数的运算表达式进行重新组合, 即<i>y</i>= (<i>a</i><sup>*</sup><i>x</i>+<i>b</i>) <sup>*</sup><i>x</i>+<i>c</i>, 其中多项式的系数<i>a</i>, <i>b</i>, <i>c</i>经过量化存入寄存器.然后, 对用于拟合曲线的常数经过量化后放入查找表, 硬件电路如图5所示.图5中的多路器 (MUX) 用于判定量化后的输入数据所在的区间.</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904012_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 sigmoid函数的硬件电路模块" src="Detail/GetImg?filename=images/WXYJ201904012_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>5 sigmoid<b>函数的硬件电路模块</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904012_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="71" name="71" class="anchor-tag">4 <b>结果分析与讨论</b></h3>
                <div class="p1">
                    <p id="72">在实验中, 首先将上节得到的卷积神经网络算法的FPGA设计进行硬件仿真分析, 然后使用Xilinx Virtex-7平台进行硬件实现.实验中将MNIST测试集 (10000张图片) 中的图像像素作为卷积神经网络算法的输入特征图, 将最后每一类的得分作为输出.如图6所示的仿真图, 它显示了处理一张图片后会输出每一类的得分.</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201904012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 卷积神经网络算法的仿真结果" src="Detail/GetImg?filename=images/WXYJ201904012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>6 <b>卷积神经网络算法的仿真结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201904012_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="74">在结果分析中, 本文主要以卷积神经网络消耗的MAC算术运算单元的数量来衡量FPGA的硬件开销.同时, 我们通常用 GOPS (每秒十亿次运算数) 的单位来描述性能, 它是一个非常好地用于衡量计算性能的指标.其中GOPS以文献<citation id="92" type="reference">[<a class="sup">3</a>]</citation>给出的公式为参考:</p>
                </div>
                <div class="p1">
                    <p id="75">计算性能 = 操作总数 / 执行周期数</p>
                </div>
                <div class="p1">
                    <p id="76">操作总数=2×R×C×M×N×K×K;执行周期数为完成计算所需的时钟周期, 一般取实际测量值.其中N和M分别为输入和输出特征图的数量, R和C分别为输出特征图的行列数, K为卷积核的尺寸.</p>
                </div>
                <div class="p1">
                    <p id="77">表3给出了卷积神经网络FPGA实现的性能比较, 其中比较了在MNIST测试集 (10000张图片) 中预测一张图片的时间, 识别率, FPGA资源占用情况和计算性能 (GOPS) .由于本文利用了循环分块方法重新设计了循环操作以及为了较少数据存取延迟使用了双缓存技术, 使得本文的实验性能较好.通过表3的性能比较发现与文献<citation id="93" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>相比, 虽然文献<citation id="94" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>由于采用片外存储器节省了BRAM的使用, 并且FF的使用也比本文占用的资源少, 但是本文由于使用了新的循环计算节省了主要的DSP资源73个, 同时本文的计算性能GOPS与文献<citation id="95" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>相比也提高了4.75GOPS, 因此有利于整体性能.而与文献<citation id="96" type="reference"><link href="5" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">6</a>]</sup></citation>相比, 在计算性能上得到明显提升, 其GOPS提升了7.5倍, 不过在主要的硬件资源消耗上相对较多.</p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表</b>3 <b>卷积神经网络</b>FPGA<b>实现的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td><br />参数</td><td>文献[2]</td><td>文献[6]</td><td>文献[8]</td><td>本文</td></tr><tr><td><br />FPGA</td><td>Virtex-7<br />xc7vx485t</td><td>Zynq-<br />ZC702</td><td>Virttex-7<br />xc7vx485t</td><td>Virtex-7<br />xc7vx485t</td></tr><tr><td><br />频率 (M) </td><td>100</td><td>166</td><td>150</td><td>150</td></tr><tr><td><br />时间/ms</td><td>2.636 8</td><td>0.151</td><td>0.025 4</td><td>0.019 7</td></tr><tr><td><br />BRAM</td><td>27</td><td>96</td><td>0</td><td>89</td></tr><tr><td><br />DSP</td><td>20</td><td>95</td><td>638</td><td>565</td></tr><tr><td><br />FF</td><td>54 075</td><td>27 664</td><td>66 346</td><td>66 747</td></tr><tr><td><br />LUT</td><td>14 832</td><td>38 836</td><td>51 125</td><td>35 295</td></tr><tr><td><br />识别率 (%) </td><td>98.62</td><td>99.01</td><td>96.8</td><td>98.81</td></tr><tr><td><br />GOPS</td><td>1.58</td><td>2.70</td><td>15.87</td><td>20.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">5 <b>结束语</b></h3>
                <div class="p1">
                    <p id="80">本文在对卷积神经网络 (CNN) 算法深入研究的基础上进行了CNN算法的FPGA并行结构设计.该设计实现了卷积计算电路和激活函数 (sigmoid) 的硬件电路.并行结构的设计实现使电路的整体计算性能得到提高, 并减少了硬件资源的使用.实现结果在MNIST数据集上获得了98.81%的识别率以及在输入时钟为150 MHz时, 整体电路在计算性能 (GOPS) 上由15.87 GOPS提高到了20.62 GOPS.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[1]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FPGA-based convolutional neural network accelerator design using high level synthesize">

                                <b>[2]</b> GHAFFARI S, SHARIFIAN S. FPGA-based convolutional neural network accelerator design using high level synthesize[C]// International Conference of Signal Processing and Intelligent Systems. IEEE, 2017:1-6.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A 240G-ops/s mobile coprocessor for deep neural networks">

                                <b>[3]</b> GOKHALE V, JIN J, DUNDAR A, et al. A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks[C]// Computer Vision and Pattern Recognition Workshops. IEEE, 2014:696-701.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=14.5 Eyeriss:An energy-efficient reconfigurable accelerator for deep convolutional neural networks">

                                <b>[4]</b> CHEN Y H, KRISHNA T, EMER J, et al. 14.5 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks[C]// IEEE International Solid-State Circuits Conference. IEEE, 2016:262-263.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An fpga-based processor for convolutional networks">

                                <b>[5]</b> FARABET C, POULET C, HAN J Y, et al. CNP: An FPGA-based processor for Convolutional Networks[C]// International Conference on Field Programmable Logic and Applications. IEEE, 2009:32-37.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-efficient and high-throughput FPGA-based accelerator for Convolutional Neural Networks">

                                <b>[6]</b> FENG G, HU Z, CHEN S, et al. Energy-efficient and high-throughput FPGA-based accelerator for Convolutional Neural Networks[C]// IEEE International Conference on Solid-State and Integrated Circuit Technology. IEEE, 2017:624-626.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks">

                                <b>[7]</b> CHEN ZHANG, PENG LI, GUANGYU SUN, et al. Optimizing fpga-based accelerator design for deep convolutional neural networks[C]// in Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2015:161-170.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An FPGA-based accelerator implementation for deep convolutional neural networks">

                                <b>[8]</b> ZHOU Y, JIANG J. An FPGA-based accelerator implementation for deep convolutional neural networks[C]// International Conference on Computer Science and Network Technology. IEEE, 2016:829-832.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Flexible Memory Controller Supporting Deep Belief Networks with Fixed-Point Arithmetic">

                                <b>[9]</b> JIANG J, HU R, LUJAN M. A Flexible Memory Controller Supporting Deep Belief Networks with Fixed-Point Arithmetic[C]// Parallel and Distributed Processing Symposium Workshops &amp; Phd Forum. IEEE, 2013:144-152.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201904012" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201904012&amp;v=MTY4ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXptVjc3TU1qWFNaTEc0SDlqTXE0OUVab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNJUGsvbURSNjZHaHV0MCsxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
