

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127885466056250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908015%26RESULT%3d1%26SIGN%3dhOW8uVKNvSKHgSca3%252b2%252b1gBMHnQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908015&amp;v=MjIwMDZPZVplUnJGeXZnVzc3TUx5dlNkTEc0SDlqTXA0OUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#71" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="&lt;b&gt;2 RSSQ方法&lt;/b&gt; "><b>2 RSSQ方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;2.1 目标解码&lt;/b&gt;"><b>2.1 目标解码</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;2.2 残差置乱&lt;/b&gt;"><b>2.2 残差置乱</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;2.3 位置嵌入&lt;/b&gt;"><b>2.3 位置嵌入</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#142" data-title="&lt;b&gt;3 实验与结果分析&lt;/b&gt; "><b>3 实验与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#144" data-title="&lt;b&gt;3.1 数据集及评价指标&lt;/b&gt;"><b>3.1 数据集及评价指标</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;3.2 RSSQ方法整体定量分析&lt;/b&gt;"><b>3.2 RSSQ方法整体定量分析</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;3.3 残差置乱模块评价&lt;/b&gt;"><b>3.3 残差置乱模块评价</b></a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;3.4 消融实验&lt;/b&gt;"><b>3.4 消融实验</b></a></li>
                                                <li><a href="#169" data-title="&lt;b&gt;3.5 部分场景图可视化结果&lt;/b&gt;"><b>3.5 部分场景图可视化结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#172" data-title="&lt;b&gt;4 总  结&lt;/b&gt; "><b>4 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="图1 场景图示意图">图1 场景图示意图</a></li>
                                                <li><a href="#77" data-title="图2 残差置乱上下文信息场景图生成方法框架">图2 残差置乱上下文信息场景图生成方法框架</a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表1 RSSQ方法与现有方法对比实验结果&lt;/b&gt;"><b>表1 RSSQ方法与现有方法对比实验结果</b></a></li>
                                                <li><a href="#155" data-title="图3 关系分类逐类分析">图3 关系分类逐类分析</a></li>
                                                <li><a href="#161" data-title="图4 残差置乱模块示意图">图4 残差置乱模块示意图</a></li>
                                                <li><a href="#163" data-title="&lt;b&gt;表2 残差置乱模块分析&lt;/b&gt;"><b>表2 残差置乱模块分析</b></a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表3 消融实验结果&lt;/b&gt;"><b>表3 消融实验结果</b></a></li>
                                                <li><a href="#167" data-title="图5 场景图分类结果可视化结果">图5 场景图分类结果可视化结果</a></li>
                                                <li><a href="#168" data-title="图6 时态不一致引起的错误示例">图6 时态不一致引起的错误示例</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="225">


                                    <a id="bibliography_1" title="Xu Danfei, Zhu Yuke, Choy C B, et al.Scene graph generation by iterative message passing[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3097- 3106" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene graph generation by iterative message passing">
                                        <b>[1]</b>
                                        Xu Danfei, Zhu Yuke, Choy C B, et al.Scene graph generation by iterative message passing[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3097- 3106
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_2" title="Johnson J, Krishna R, Stark M, et al.Image retrieval using scene graphs[C] //Proc of the 2015 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3668- 3678" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image retrieval using scene graphs">
                                        <b>[2]</b>
                                        Johnson J, Krishna R, Stark M, et al.Image retrieval using scene graphs[C] //Proc of the 2015 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3668- 3678
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_3" title="Sadeghi M A, Farhadi A.Recognition using visual phrases[C] //Proc of the 2011 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:1745- 1752" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recognition using visual phrases">
                                        <b>[3]</b>
                                        Sadeghi M A, Farhadi A.Recognition using visual phrases[C] //Proc of the 2011 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:1745- 1752
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_4" title="Hu Han, Gu Jianyuan, Zhang Zheng, et al.Relation Networks for Object Detection[C] //Proc of the 2018 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:3588- 3597" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relation Networks for Object Detection">
                                        <b>[4]</b>
                                        Hu Han, Gu Jianyuan, Zhang Zheng, et al.Relation Networks for Object Detection[C] //Proc of the 2018 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:3588- 3597
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_5" title="Teney D, Liu Lingqiao, Den Hengel A V, et al.Graph-structured representations for visual question answering[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3233- 3241" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph-structured representations for visual question answering">
                                        <b>[5]</b>
                                        Teney D, Liu Lingqiao, Den Hengel A V, et al.Graph-structured representations for visual question answering[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3233- 3241
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_6" title="Tang Kaihua, Zhang Hanwang, Wu Baoyuan, et al.Learning to compose dynamic tree structures for visual contexts[J].arXiv preprint, arXiv:1812.01880v1, 2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to compose dynamic tree structures for visual contexts">
                                        <b>[6]</b>
                                        Tang Kaihua, Zhang Hanwang, Wu Baoyuan, et al.Learning to compose dynamic tree structures for visual contexts[J].arXiv preprint, arXiv:1812.01880v1, 2018
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_7" title="Yu Jun, Wang Liang, Yu Zhou.Research on visual question answering techniques[J].Journal of Computer Research and Development, 2018, 55 (9) :1946- 1958 (in Chinese) (俞俊, 汪亮, 余宙.视觉问答技术研究[J].计算机研究与发展, 2018, 55 (9) :1946- 1958) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201809010&amp;v=MjQ0ODhIOW5NcG85RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5dmdXNzdNTHl2U2RMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        Yu Jun, Wang Liang, Yu Zhou.Research on visual question answering techniques[J].Journal of Computer Research and Development, 2018, 55 (9) :1946- 1958 (in Chinese) (俞俊, 汪亮, 余宙.视觉问答技术研究[J].计算机研究与发展, 2018, 55 (9) :1946- 1958) 
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_8" title="Li Yikang, Ouyang Wanli, Zhou Bolei, et al.Scene graph generation from objects, phrases and region captions[C]//Proc of the 2017 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:1270- 1279" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene graph generation from objects,phrases and region captions">
                                        <b>[8]</b>
                                        Li Yikang, Ouyang Wanli, Zhou Bolei, et al.Scene graph generation from objects, phrases and region captions[C]//Proc of the 2017 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:1270- 1279
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_9" title="Li Yikang, Ouyang Wanli, Wang Xiaogang, et al.ViP-CNN:Visual phrase guided convolutional neural network[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:7244- 7253" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ViP-CNN:Visual phrase guided convolutional neural network">
                                        <b>[9]</b>
                                        Li Yikang, Ouyang Wanli, Wang Xiaogang, et al.ViP-CNN:Visual phrase guided convolutional neural network[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:7244- 7253
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_10" title="Li Yikang, Ouyang Wanli, Zhou Bolei, et al.Factorizable Net:An efficient subgraph-based framework for scene graph generation[C] //Proc of the 2018 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2018:346- 363" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Factorizable Net An efficient subgraph-based framework for scene graph generation">
                                        <b>[10]</b>
                                        Li Yikang, Ouyang Wanli, Zhou Bolei, et al.Factorizable Net:An efficient subgraph-based framework for scene graph generation[C] //Proc of the 2018 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2018:346- 363
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_11" title="Zellers R, Yatskar M, Thomson S, et al.Neural motifs:Scene graph parsing with global context[C] //Proc of the 2018 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:5831- 5840" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural motifs:Scene graph parsing with global context">
                                        <b>[11]</b>
                                        Zellers R, Yatskar M, Thomson S, et al.Neural motifs:Scene graph parsing with global context[C] //Proc of the 2018 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:5831- 5840
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_12" title="Yang Jianwei, Lu Jiasen, Lee S, et al.Graph R-CNN for scene graph generation[C] //Proc of the 2018 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2018:690- 706" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph R-CNN for scene graph generation">
                                        <b>[12]</b>
                                        Yang Jianwei, Lu Jiasen, Lee S, et al.Graph R-CNN for scene graph generation[C] //Proc of the 2018 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2018:690- 706
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_13" title="Girshick R B.Fast R-CNN[C] //Proc of the 2015 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1440- 1448" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[13]</b>
                                        Girshick R B.Fast R-CNN[C] //Proc of the 2015 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1440- 1448
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_14" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[C] //Proc of the 2015 IEEE Int Conf on Learning Representations.Piscataway, NJ:IEEE, 2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[14]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[C] //Proc of the 2015 IEEE Int Conf on Learning Representations.Piscataway, NJ:IEEE, 2015
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_15" title="Chung J, Gulcehre C, Cho K, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].arXiv preprint, arXiv:1412.3555, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of gated recurrent neural networks on sequence modeling">
                                        <b>[15]</b>
                                        Chung J, Gulcehre C, Cho K, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].arXiv preprint, arXiv:1412.3555, 2014
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_16" title="Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTY4MzJyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNjYXhZPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_17" title="He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Deep residual learning for image recognition[C] //Proc of the 2016 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770- 778" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[17]</b>
                                        He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Deep residual learning for image recognition[C] //Proc of the 2016 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770- 778
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_18" title="Kim J, Elkhamy M, Lee J, et al.Residual LSTM:Design of a deep recurrent architecture for distant speech recognition[C] //Proc of the 2017 Conf of the Int Speech Communication Association, 2017:1591- 1595.https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0477.PDF" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Residual LSTM:Design of a deep recurrent architecture for distant speech recognition">
                                        <b>[18]</b>
                                        Kim J, Elkhamy M, Lee J, et al.Residual LSTM:Design of a deep recurrent architecture for distant speech recognition[C] //Proc of the 2017 Conf of the Int Speech Communication Association, 2017:1591- 1595.https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0477.PDF
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_19" title="Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[C] //Proc of the 2014 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2014:818- 833" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[19]</b>
                                        Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[C] //Proc of the 2014 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2014:818- 833
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_20" title="Zhang Yu, Chen Guoguo, Yu Dong, et al.Highway long short-term memory RNNS for distant speech recognition[C] //Proc of the 41st Int Conf on Acoustics, Speech, and Signal Processing.Piscataway, NJ:IEEE, 2016:5755- 5759" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Highway long short-term memory RNNS for distant speech recognition">
                                        <b>[20]</b>
                                        Zhang Yu, Chen Guoguo, Yu Dong, et al.Highway long short-term memory RNNS for distant speech recognition[C] //Proc of the 41st Int Conf on Acoustics, Speech, and Signal Processing.Piscataway, NJ:IEEE, 2016:5755- 5759
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                    Ren S, He Kaiming, Girshick R B, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137- 1149</a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_22" title="Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C] //Proc of the Conf and Workshop on Neural Information Processing Systems.New York:Curran Associates, 2017:5998- 6008" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention is all you need">
                                        <b>[22]</b>
                                        Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C] //Proc of the Conf and Workshop on Neural Information Processing Systems.New York:Curran Associates, 2017:5998- 6008
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_23" title="Zhu Yaohui, Jiang Shuqiang, Li Xiangyang, et al.Visual relationship detection with object spatial distribution[C] //Proc of the 2017 IEEE Int Conf on Multimedia and Expo, Piscataway, NJ:IEEE, 2017:379- 384" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual relationship detection with object spatial distribution">
                                        <b>[23]</b>
                                        Zhu Yaohui, Jiang Shuqiang, Li Xiangyang, et al.Visual relationship detection with object spatial distribution[C] //Proc of the 2017 IEEE Int Conf on Multimedia and Expo, Piscataway, NJ:IEEE, 2017:379- 384
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_24" title="Krishna R, Zhu Yuke, Groth O, et al.Visual genome:Connecting language and vision using crowdsourced dense image annotations[J].International Journal of Computer Vision, 2017, 123 (1) :32- 73" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDBAF74762D4CC9493EB715E691F405699&amp;v=MTE5OTVrcFJORGZiS1JRN09XQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjFoeDcyM3dhdz1OajdCYXNISmFOYklxSWxIRU85OGYzVTl4aFZtbURoOFRRcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        Krishna R, Zhu Yuke, Groth O, et al.Visual genome:Connecting language and vision using crowdsourced dense image annotations[J].International Journal of Computer Vision, 2017, 123 (1) :32- 73
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_25" title="Dai Bo, Zhang Yuqi, Lin Dahua, et al.Detecting visual relationships with deep relational networks[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3298- 3308" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting visual relationships with deep relational networks">
                                        <b>[25]</b>
                                        Dai Bo, Zhang Yuqi, Lin Dahua, et al.Detecting visual relationships with deep relational networks[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3298- 3308
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_26" title="Lu Cewu, Krishna R, Bernstein M S, et al.Visual relationship detection with language priors[C] //Proc of the 2016 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2016:852- 869" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual relationship detection with language priors">
                                        <b>[26]</b>
                                        Lu Cewu, Krishna R, Bernstein M S, et al.Visual relationship detection with language priors[C] //Proc of the 2016 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2016:852- 869
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_27" title="Li Zhenguo, Zhou Fengwei, Chen Fei, et al.Meta-SGD:Learning to learn quickly for few shot learning[J].arXiv preprint, arXiv:1707.09835, 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Meta-SGD:Learning to learn quickly for few shot learning">
                                        <b>[27]</b>
                                        Li Zhenguo, Zhou Fengwei, Chen Fei, et al.Meta-SGD:Learning to learn quickly for few shot learning[J].arXiv preprint, arXiv:1707.09835, 2017
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1721-1730 DOI:10.7544/issn1000-1239.2019.20190329            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种残差置乱上下文信息的场景图生成方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E6%AC%A3&amp;code=38079526&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林欣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B0%E9%91%AB&amp;code=42588560&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">田鑫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%A3%E6%80%A1&amp;code=27672977&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">季怡</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E4%BA%91%E9%BE%99&amp;code=43074854&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐云龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%BA%AF%E5%B9%B3&amp;code=09886919&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘纯平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0240077&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=1750700&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州大学应用技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%AC%A6%E5%8F%B7%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%90%89%E6%9E%97%E5%A4%A7%E5%AD%A6)&amp;code=1046463&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">符号计算与知识工程教育部重点实验室(吉林大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>场景图在视觉理解中有着很重要的作用.现有的场景图生成方法对于主语、宾语以及主宾语间的视觉关系进行研究.但是, 人类通过空间关系上下文、语义上下文和目标之间的互动信息来进行关系的理解和推理.为了获得更好的全局上下文表示, 同时减少数据集偏差的影响, 提出了一个新的场景图生成框架RSSQ (residual shuffle sequence model) .该框架由目标解码、残差置乱和位置嵌入3部分构成.残差置乱模块由随机置乱和残差连接的双向LSTM的基本结构叠加而成, 利用迭代方式实现随机打乱双向LSTM的隐藏状态以减少数据集偏差影响, 利用残差连接提取共享的全局上下文信息.在位置嵌入模块中, 通过对目标的相对位置和面积比例的编码则可以增强目标对之间的空间关系.在数据集Visual Genome的3个不同层次子任务的实验中, 证明了提出的RSSQ方法因全局上下文改善和空间关系增强, 在Recall@50和Recall@100指标评价下, 相对于现有方法能生成更好的场景图.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%BA%E6%99%AF%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">场景图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%85%B3%E7%B3%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉关系;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8A%E4%B8%8B%E6%96%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上下文;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E5%8F%8C%E5%90%91LSTM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差双向LSTM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘纯平, cpliu@suda.edu.cn;
                                </span>
                                <span>
                                    林欣, xlin2017@stu.suda.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61773272, 61272258, 61301299);</span>
                                <span>吉林大学符号计算与知识工程教育部重点实验室项目 (93K172016K08);</span>
                                <span>江苏高校优势学科建设工程资助项目;</span>
                    </p>
            </div>
                    <h1><b>Scene Graph Generation Based on Shuffle Residual Context Information</b></h1>
                    <h2>
                    <span>Lin Xin</span>
                    <span>Tian Xin</span>
                    <span>Ji Yi</span>
                    <span>Xu Yunlong</span>
                    <span>Liu Chunping</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Soochow University</span>
                    <span>Applied Technology College of Soochow University</span>
                    <span>Key Laboratory of Symbol Computation and Knowledge Engineering (Jilin University) , Ministry of Education</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Scene graphs play an important role in visual understanding. Existing scene graph generation methods focus on the research of the subjects, the objects as well as the predicates between them. However, human being abstracts the relationships using spatial relation context, semantic context and interaction between scene objects for better understanding and reasoning as whole. In order to obtain the better global context representation and reduce the impact of dataset bias, we propose a new framework of scene graph generation, called as residual shuffle sequence model (RSSQ) . Our method is made up of object decoding, residual shuffle and position embedding modules. Residual shuffle module is stacked with two basic structures including the random shuffle operation and the residual bidirectional LSTM. We implement the random shuffle on the hidden state of bidirectional LSTM by the process of iterative operation to reduce the impact of dataset bias, and extract the shared global context information by the residual connection structure. To strengthen the spatial relationship between pair-wise objects, the encoding is achieved using the relative position and area ratio of objects in position embedding module. The experimental results of three sub-tasks of different difficulty performed on Visual Genome dataset, demonstrate that the poposed method can generate better scene graphs under Recall@50 and Recall@100 settings due to better global context and spatial information.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scene%20graph&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scene graph;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20relationship&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual relationship;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=context&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">context;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20bidirectional%20LSTM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual bidirectional LSTM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Lin Xin, born in 1995.MSc candidate. Student member of CCF. Her main research interests include computer vision and scene understanding.<image id="331" type="" href="images/JFYZ201908015_33100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Tian Xin, born in 1996.MSc candidate. His main research interests include computer vision and image processing. <image id="333" type="" href="images/JFYZ201908015_33300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Ji Yi, born in 1973.PhD, associate professor.Member of CCF.Her main research interests include 3D action recognition and complex scene understanding. <image id="335" type="" href="images/JFYZ201908015_33500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Xu Yunlong, born in 1964.BSc, associate professor. His main research interests include reinforcement learning, natural language processing, operating system and big data. <image id="337" type="" href="images/JFYZ201908015_33700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Chunping, born in 1971.PhD, professor, PhD supervisor.Her main research interests include computer vision, image analysis and recognition, in particular in domains of visual saliency detection, object detection and scene understanding.<image id="339" type="" href="images/JFYZ201908015_33900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-06-03</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61773272, 61272258, 61301299);</span>
                                <span>the Program of the Key Laboratory of Symbolic Computation and Knowledge Engineering (Jilin University) , Ministry of Education (93K172016K08);</span>
                                <span>the Priority Academic Program Development of Jiangsu Higher Education Institutions;</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908015_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 场景图示意图" src="Detail/GetImg?filename=images/JFYZ201908015_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 场景图示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908015_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 A sample of a ground truth scene graph</p>

                </div>
                <div class="p1">
                    <p id="63">场景图<citation id="279" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是真实图像中目标和目标间关系的精细化语义抽取, 通过对预定义的目标实例、目标属性和目标对间关系进行预测来构建, 常用三元组的结构化语言表示场景中目标间的交互.图1给出了一幅图像三元组关系表示的场景图实例, 如&lt;boy-wearing-shirt&gt;.在场景图中, 节点描述类别信息连同边界盒表示的目标实体, 有向边则表示主、宾语间的关系类别.借助场景图对一幅图像可解释结构化表示的描述, 图像被重构为连接图结构而不是孤立的目标实体, 可以支持高层视觉智能任务, 如图像检索<citation id="280" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、目标检测<citation id="281" type="reference"><link href="229" rel="bibliography" /><link href="231" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>以及视觉问答<citation id="282" type="reference"><link href="233" rel="bibliography" /><link href="235" rel="bibliography" /><link href="237" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>等视觉任务.由于手工标注海量图像的三元组关系描述格外昂贵, 因此训练一个模型来自动生成高质量的场景图是近年来视觉理解的一种重要方向, 再加上场景图表示需要推理复杂的依赖关系, 高效准确地提取场景图也是一个极具挑战性的任务.</p>
                </div>
                <div class="p1">
                    <p id="64">作为连接视觉与语言的桥梁, 场景图生成任务是尽可能生成一个精确映射真实视觉场景的图表示.现有大多数基于目标的场景图方法, 主要有基于目标检测和关系分类两阶段生成方法、基于目标和关系联合推理两大类.基于推理的场景图生成方法又可细分为基于消息传递<citation id="283" type="reference"><link href="225" rel="bibliography" /><link href="239" rel="bibliography" /><link href="241" rel="bibliography" /><link href="243" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>和全局上下文<citation id="284" type="reference"><link href="245" rel="bibliography" /><link href="247" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>2类.为得到更精准的目标标签, 这类方法在候选场景图上进行消息传递与推理.</p>
                </div>
                <div class="p1">
                    <p id="65">基于消息传递的方法中, 首先提取目标区域的局部特征输入循环神经网络学习, 其次使用相邻节点和边的表示来生成消息, 并在图的拓扑结构中进行传递, 最终获得主语、宾语和关系的最终表示结果.常见的消息传递策略包括迭代消息传递<citation id="285" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、并行和串行消息传递<citation id="286" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、空间加权消息传递<citation id="287" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等.Xu等人<citation id="288" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>最早提出基于迭代消息传递的场景图生成方法IMP (iterative message passing) .该方法首先通过ROI-pooling<citation id="289" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>从VGG-16卷积层<citation id="290" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>中提取目标和关系的特征, 然后将视觉特征分别输入节点和边GRU (gated recurrent unit) <citation id="291" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>中, 在之后的迭代过程中根据拓扑结构, 利用相邻节点或边的隐藏状态生成消息, 获取最终目标和关系表示.此外, 还有一些改进的消息传递方法被提出, 如并行和串行消息传递策略<citation id="292" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>可以更好地在目标和关系间传递信息;空间加权消息传递结构和空间敏感关系推理模块机制下的基于子图连接图<citation id="293" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>可有效加速推理过程和提高场景图生成效率.但是由于不完全的数据集标注, 此类模型生成的消息受到局部上下文偏差的影响以及缺乏全局的视野.</p>
                </div>
                <div class="p1">
                    <p id="66">基于视觉和语义特征候选场景图中节点间上下文传递下更新节点和关系表示能更加有效地学习到可靠边的位置, 减少不可能边的影响.NM (neural motifs) 模型<citation id="294" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是最具代表性的全局上下文方法, 此外还有注意力图卷积网络<citation id="295" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的场景图生成方法.相对于局部上下文方法局限于关系三元组进行消息传递, 全局上下文方法在全图范围内进行上下文更新, 从而获取更加全面的特征表示.在NM模型中, 目标候选框的特征以一个固定的顺序被输入到双向LSTM (long short-term memory) 网络<citation id="296" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>中, 从而获得图像的全局上下文, 并通过连接主、宾语的全局上下文表示, 实现对关系的分类.由于该类方法将原始图像中呈二维空间分布的目标排列成一个固定的从左至右的线性顺序, 全局上下文信息受到破坏, 使模型更倾向于学习到数据集的偏差, 而不是真正的视觉关系表示, 同时损失了空间信息, 无法获得全面的全局上下文.</p>
                </div>
                <div class="p1">
                    <p id="67">鉴于上述问题, 本文以NM模型<citation id="297" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>为基础, 提出了残差置乱上下文信息的场景图生成模型 (residual shuffle sequence model, RSSQ) , 其主要贡献有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="68">1) 提出随机置乱策略, 将固定顺序的隐藏状态迭代打乱重组.该策略就像纸牌游戏中的洗牌操作, 可以加强目标节点和其他所有相邻节点的信息交换, 提高模型的泛化能力, 降低数据集偏差对场景图生成的影响.</p>
                </div>
                <div class="p1">
                    <p id="69">2) 构建不同双向LSTM层之间的残差连接, 获得不同层次的全局上下文信息, 以形成更好的全局共享上下文表达, 同时因残差的引入解决梯度消失问题.</p>
                </div>
                <div class="p1">
                    <p id="70">3) 提出显式编码目标对间的位置信息嵌入, 以增强场景图生成中的空间上下文, 改善目标关系描述.</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="72">场景图生成是近几年才发展起来的计算机视觉高级任务之一.与本文提出场景图生成方法密切相关联的工作主要有NM模型和残差连接.下面分别介绍这2个方面.</p>
                </div>
                <div class="p1">
                    <p id="73">NM模型<citation id="298" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是一种代表性的全局上下文方法.该模型将场景图生成分为候选目标边界盒、区域标签和关系预测3个阶段.在候选目标边界盒预测阶段, 计算边界盒区域内的上下文信息并进行传递;然后将全局上下文用于预测边界盒的标签, 并基于全局上下文进行边预测;最后在融合上下文边界盒区域信息的基础上给边分配标签.具体实现中首先提取候选目标的局部特征, 并以候选区域中心点在原图上的位置从左至右的线性顺序将局部特征输入双向LSTM;然后用一个单向LSTM来解码目标类别, 连同目标上下文输入到边上下文双向LSTM网络中;最后组合主、宾语特征, 获取关系的最终表示.通过序列学习, NM模型能够学到视觉场景的强规则化信息, 但是具有复杂空间分布和丰富语义信息的图像被抽象为一个固定次序线性序列的简单操作造成了重要信息损失, 如场景中的空间位置信息丢失;再加上双向LSTM的强记忆能力使得NM模型更容易学习到数据集的偏差.</p>
                </div>
                <div class="p1">
                    <p id="74">与本文提出场景图生成方法相关的另一个工作是残差连接.残差连接的关键思想是在网络层之间增加短路连接, 提供额外的梯度路径<citation id="299" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.通过残差连接, 非常深的卷积网络<citation id="300" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>被应用与图像分类和检测.残差连接在深层卷积神经网络中的应用, 提高了模型的泛化能力, 解决了模型的“退化”问题.最近, Kim等人<citation id="301" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了在LSTM模型中增加残差连接的方法, 并将该方法应用于远场语音识别, 证明了残差连接可以提供短路, 解决梯度消失问题.鉴于深度学习中, 不同的网络层可以表示低/中/高不同层次的特征<citation id="302" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 因此, 在不同层次的LSTM中建立残差连接能够更好地学习抽象视觉关系, 减少梯度消失问题.NM模型在双向LSTM中使用高速连接的设计, 在时间维度上解决了梯度消失问题, 但是随着层数的增加, 建立了高速连接的LSTM仍然存在退化问题<citation id="303" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 同时在空间维度上高速连接使得训练过程更加困难, 残差连接解决了这个问题<citation id="304" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag"><b>2 RSSQ方法</b></h3>
                <div class="p1">
                    <p id="76">为了获取更优的关系表示以生成更精确的场景图, 提出了RSSQ方法.该方法主要由目标解码模块、残差置乱模块以及位置嵌入模块3个部分组成, 其整体框架如图2所示.为了简洁和方便, 下文双向LSTM隐藏状态均表述为上下文信息.</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 残差置乱上下文信息场景图生成方法框架" src="Detail/GetImg?filename=images/JFYZ201908015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 残差置乱上下文信息场景图生成方法框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908015_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The framework of our Residual Shuffle Sequence Model (RSSQ) </p>

                </div>
                <div class="p1">
                    <p id="78">场景图中的视觉关系包括目标和谓词.对于目标的提取, 利用Faster RCNN模型<citation id="305" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>给出初始目标分类预测<b><i>o</i></b><sub><i>i</i></sub>, 然后在目标解码模块中, 利用初始目标<b><i>o</i></b><sub><i>i</i></sub>解码上下文信息<b><i>h</i></b><sub><i>i</i>, <i>d</i></sub>以进一步分类目标获得场景图中目标<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">o</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>的表示:</p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">o</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mi>f</mi><mi>c</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mtext>d</mtext></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="82">其中, <i>fc</i> (·) 表示全连接, d表示目标解码模块.主语目标<i>i</i>和宾语目标<i>j</i>之间的谓词表示由置乱残差边上下文表示<b><i>pr</i></b><sub><i>i</i>, <i>j</i></sub>以及位置嵌入向量<b><i>ps</i></b><sub><i>i</i>, <i>j</i></sub>的最大全连接获得.谓词表示为</p>
                </div>
                <div class="p1">
                    <p id="83"><i>rel</i><sub><i>i</i>, <i>j</i></sub>=arg max (<i>fc</i> (<b><i>pr</i></b><sub><i>i</i>, <i>j</i></sub>, <b><i>ps</i></b><sub><i>i</i>, <i>j</i></sub>) ) . (2) </p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>2.1 目标解码</b></h4>
                <div class="p1">
                    <p id="85">目标解码阶段的主要目的是实现目标分类.该模块首先使用Faster RCNN<citation id="306" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>来进行目标的预分类以及目标边界盒的回归.由于Faster RCNN中, 目标分类是不考虑上下文信息的.为了引入上下文信息, 采用NM模型<citation id="307" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>中的目标上下文模块构建目标预测的上下文表示.</p>
                </div>
                <div class="p1">
                    <p id="86">目标上下文信息<b><i>h</i></b><sub><i>i</i>, o</sub>提取是利用中心点偏移从左至右将其目标特征向量<i>f</i><sub><i>i</i></sub>输入到高速双向LSTM<citation id="308" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>中获得, 即:</p>
                </div>
                <div class="p1">
                    <p id="87"><b><i>h</i></b><sub><i>i</i>, o</sub>=biLSTM (<b><i>f</i></b><sub><i>i</i></sub>) . (3) </p>
                </div>
                <div class="p1">
                    <p id="88">目标的分类向量由目标上下文信息<b><i>h</i></b><sub><i>i</i>, o</sub>输入目标解码LSTM获得, 即:</p>
                </div>
                <div class="p1">
                    <p id="89"><b><i>h</i></b><sub><i>i</i>, d</sub>=LSTM (<b><i>h</i></b><sub><i>i</i>, o</sub>) . (4) </p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.2 残差置乱</b></h4>
                <div class="p1">
                    <p id="91">置乱操作被定义成一个将固定有序序列转换成随机序列的映射函数.映射函数采用随机数函数.假设目标序列初始排列顺序为<b><i>I</i></b>= (1, 2, …, <i>i</i>, …, <i>N</i>) , <i>N</i>为图像中目标总数, <i>i</i>为第<i>i</i>个目标.经过<i>t</i>轮置乱, 新的顺序被更新为序列<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Ι</mi><mo>^</mo></mover></math></mathml><sup><i>t</i>-1</sup>, 其表达式为</p>
                </div>
                <div class="p1">
                    <p id="93"><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Ι</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mtext>s</mtext><mtext>h</mtext><mtext>u</mtext><mtext>f</mtext><mtext>f</mtext><mtext>l</mtext><mtext>e</mtext></mrow><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo></mrow></math></mathml>, (5) </p>
                </div>
                <div class="p1">
                    <p id="95">其中, <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>i</mi><mo>^</mo></mover></math></mathml><sup><i>t</i>-1</sup>表示第<i>t</i>-1次第<i>i</i>个目标, <i>t</i>∈{1, 2, …, <i>M</i>}.</p>
                </div>
                <div class="p1">
                    <p id="97">残差置乱模块的输入由目标上下文编码的隐藏状态和词向量编码2部分拼接而成:</p>
                </div>
                <div class="p1">
                    <p id="98"><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>0</mn></msup></mrow><mn>0</mn></msubsup><mo>=</mo><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>c</mtext><mtext>a</mtext><mtext>t</mtext></mrow></math></mathml><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>0</mn></msup><mo>, </mo><mtext>o</mtext></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>0</mn></msup></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (6) </p>
                </div>
                <div class="p1">
                    <p id="101">其中, <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">m</mi><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>0</mn></msup></mrow></msub></mrow></math></mathml>是目标<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>i</mi><mo>^</mo></mover></math></mathml><sup>0</sup>的语义词向量, 经过<b>t</b>+1次置乱, 边上下文输入双向<b><i>LSTM</i></b>的隐藏状态为</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup><mo>=</mo><mtext>b</mtext><mtext>i</mtext><mtext>L</mtext><mtext>S</mtext><mtext>Τ</mtext><mtext>Μ</mtext></mrow></math></mathml><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>. (7) </p>
                </div>
                <div class="p1">
                    <p id="107">隐藏状态<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup></mrow></math></mathml>经置乱之后, 新的隐藏状态序列则为<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><mi>t</mi></msubsup></mrow></math></mathml>.由于双向LSTM层间采用残差连接方式, 所以<i>t</i>+1轮置乱迭代之后, 残差的边上下文信息<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup></mrow></math></mathml>更新为</p>
                </div>
                <div class="p1">
                    <p id="111"><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup><mo>=</mo><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>c</mtext><mtext>a</mtext><mtext>t</mtext></mrow></math></mathml><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msubsup><mo stretchy="false">) </mo><mo>, </mo><mi>t</mi><mo>≥</mo><mn>2</mn></mrow></math></mathml>. (8) </p>
                </div>
                <div class="p1">
                    <p id="114">特别地, 当<b>t</b>=1时, <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>1</mn></msup></mrow><mn>1</mn></msubsup><mo>=</mo><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>c</mtext><mtext>a</mtext><mtext>t</mtext></mrow></math></mathml><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>1</mn></msup></mrow><mn>0</mn></msubsup><mo>, </mo><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mn>1</mn></msup></mrow><mn>0</mn></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>.经过<b><i>t</i></b>+1轮置乱后, 利用全连接层将提取的边上下文信息<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup></mrow></math></mathml>分解成主宾成分:</p>
                </div>
                <div class="p1">
                    <p id="118"><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>, </mo><mtext>s</mtext></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>, </mo><mtext>o</mtext></mrow></msub><mo>=</mo><mi>f</mi><mi>c</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup></mrow><mi>t</mi></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>. (9) </p>
                </div>
                <div class="p1">
                    <p id="120">其中, <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>, </mo><mtext>s</mtext></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>, </mo><mtext>o</mtext></mrow></msub></mrow></math></mathml>分别表示目标对的主语和宾语成分, 每对可能的目标对主语<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>i</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>和宾语<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>j</mi><mo>^</mo></mover></math></mathml><sup><i>t</i></sup>, 对应原始编号<i>i</i>, <i>j</i>.</p>
                </div>
                <div class="p1">
                    <p id="124">最终残差边上下文表示<b><i>pr</i></b><sub><i>i</i>, <i>j</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="125"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>i</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>, </mo><mtext>s</mtext></mrow></msub><mo>⊙</mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mrow><mover accent="true"><mi>j</mi><mo>^</mo></mover><msup><mrow></mrow><mi>t</mi></msup><mo>, </mo><mtext>o</mtext></mrow></msub></mrow></math></mathml>. (10) </p>
                </div>
                <div class="p1">
                    <p id="127">其中, ⊙表示点乘运算.</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>2.3 位置嵌入</b></h4>
                <div class="p1">
                    <p id="129">给定主语包围盒<b><i>box</i></b><sub><i>i</i></sub>= (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>, <i>w</i><sub><i>i</i></sub>, <i>h</i><sub><i>i</i></sub>) , 宾语包围盒<b><i>box</i></b><sub><i>j</i></sub>= (<i>x</i><sub><i>j</i></sub>, <i>y</i><sub><i>j</i></sub>, <i>w</i><sub><i>j</i></sub>, <i>h</i><sub><i>j</i></sub>) , 主宾语间的相对几何特征<b><i>PE</i></b>和区域比特征<b><i>A</i></b><sub>up</sub>, 位置嵌入特征<b><i>ps</i></b><sub><i>i</i>, <i>j</i></sub>则可通过一个全连接层的融合得到:</p>
                </div>
                <div class="p1">
                    <p id="130"><b><i>ps</i></b><sub><i>i</i>, <i>j</i></sub>=<i>fc</i> (<b><i>PE</i></b>, <b><i>A</i></b><sub>up</sub>) . (11) </p>
                </div>
                <div class="p1">
                    <p id="131">主、宾语间的相对几何特征<b><i>PE</i></b>是一个高维嵌入表示.为了获取平移和尺度不变的相对几何特征, 对主宾语间的4维相对几何特征进行对数转换, 转换后的相对几何特征为</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">s</mi><mo>=</mo><mrow><mo> (</mo><mrow><mi>log</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mi>log</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi>log</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mi>log</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">在本文实验中, 根据文献<citation id="309" type="reference">[<a class="sup">22</a>]</citation>的方法, 通过正弦和余弦函数分别计算主、宾语间的相对几何特征<b><i>PE</i></b>的奇数 (2<i>m</i>+1) 和偶数 (2<i>m</i>) 维度的变换特征, 将4维相对几何特征<b><i>pos</i></b>换为64维表示.变换公式分别为</p>
                </div>
                <div class="p1">
                    <p id="134"><b><i>PE</i></b><sub> (<i>pos</i>, 2<i>m</i>) </sub>=sin (<b><i>pos</i></b>/1000<sup>2<i>m</i>/<i>d</i><sub>mod <i>el</i></sub></sup>) , (13) </p>
                </div>
                <div class="p1">
                    <p id="135"><b><i>PE</i></b><sub> (<i>pos</i>, 2<i>m</i>+1) </sub>=cos (<b><i>pos</i></b>/1000<sup>2<i>m</i>+1/<i>d</i><sub>mod <i>el</i></sub></sup>) . (14) </p>
                </div>
                <div class="p1">
                    <p id="136">除了相对几何位置关系, 目标对间的空间关系通过目标对之间面积关系和重叠关系来进一步增强<citation id="310" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>.文献<citation id="311" type="reference">[<a class="sup">23</a>]</citation>中, 通过相对位置、面积、形状等描述空间分布.受到该文献启发, 本文引入4维区域比特征<b><i>A</i></b><sub><i>i</i>, <i>j</i></sub>, 并利用一个ReLu函数激活的全连接层将其转换至64维:</p>
                </div>
                <div class="p1">
                    <p id="137"><b><i>A</i></b><sub>up</sub>=ReLu (<i>fc</i> (<b><i>A</i></b><sub><i>i</i>, <i>j</i></sub>) ) . (15) </p>
                </div>
                <div class="p1">
                    <p id="138">区域比特征<b><i>A</i></b><sub><i>i</i>, <i>j</i></sub>= (<i>V</i><sub><i>i</i>, <i>j</i></sub>, <i>V</i><sub><i>o</i>, <i>i</i></sub>, <i>V</i><sub><i>o</i>, <i>j</i></sub>, <i>V</i><sub><i>o</i>.<i>u</i></sub>) 由1个面积比<i>V</i><sub><i>i</i>, <i>j</i></sub>和3个重叠比<i>V</i><sub><i>o</i>, <i>i</i></sub>, <i>V</i><sub><i>o</i>, <i>j</i></sub>, <i>V</i><sub><i>o</i>.<i>u</i></sub>构成:</p>
                </div>
                <div class="area_img" id="139">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908015_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="141">其中, <i>A</i> (<i>b</i><sub><i>i</i></sub>) 表示包围盒<i>box</i><sub><i>i</i></sub>的面积, <i>A</i> (<i>o</i><sub><i>i</i>, <i>j</i></sub>) 表示包围盒的重叠面积, <i>A</i> (<i>u</i><sub><i>i</i>, <i>j</i></sub>) 表示主宾语的外包围盒面积.</p>
                </div>
                <h3 id="142" name="142" class="anchor-tag"><b>3 实验与结果分析</b></h3>
                <div class="p1">
                    <p id="143">实验在公开数据集Visual Genome (VG) <citation id="312" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>上展开.为了验证提出RSSQ方法场景图生成性能, 进行了模型本身的消融分析, 同时进一步在关系分类、场景图分类和场景图生成3个不同层次子任务上进行方法性能的评价.</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>3.1 数据集及评价指标</b></h4>
                <div class="p1">
                    <p id="145">Visual Genome数据集是一个人工标注的视觉关系数据集.根据不同的数据预处理方式和数据划分方法, 存在多种不同的版本<citation id="314" type="reference"><link href="239" rel="bibliography" /><link href="245" rel="bibliography" /><link href="247" rel="bibliography" /><link href="273" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">25</a>]</sup></citation>.在实验中, 使用最普遍使用的数据预处理和数据集划分方法<citation id="313" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 其中训练集和测试集分别有75 651图像和32 422图像.保留了最常见的150类目标以及50类关系, 每张图像平均有11.5个目标和6.2个关系.</p>
                </div>
                <div class="p1">
                    <p id="146">场景图生成任务的目的是定位预定义的目标以及预测目标对间的关系.整个任务被分成3个子任务:</p>
                </div>
                <div class="p1">
                    <p id="147">1) 关系分类任务 (predicate classification, PredCls) .给定真实目标框以及真实标签, 需要预测目标对间关系;</p>
                </div>
                <div class="p1">
                    <p id="148">2) 场景图分类任务 (scene graph classification, SGCls) .给定真实的目标边界盒, 需要预测目标标签和目标对间关系;</p>
                </div>
                <div class="p1">
                    <p id="149">3) 场景图生成任务 (scene graph generation, SGGen) .给定一张图像, 需要检测其中的目标和关系.</p>
                </div>
                <div class="p1">
                    <p id="150">实验评价指标采用Recall@K, 缩写为R@K, 是置信度最高的<i>K</i>个分类结果在关系真值中所占比例.本文根据在Visual Genome数据集中证明结论:随机生成一个三元组关系Recall@100约为0.000 089<citation id="315" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 在实验中将<i>K</i>取值为50和100.</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>3.2 RSSQ方法整体定量分析</b></h4>
                <div class="p1">
                    <p id="152">实验中, 以场景图中3个子任务为目标, 将RSSQ方法与一些现存模型进行对比, 包括Language Priors (LP) 模型<citation id="316" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、IMP模型<citation id="317" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、Graph R-CNN (GR) 模型<citation id="318" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>以及NM模型<citation id="319" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>.实验结果如表1所示:</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表1 RSSQ方法与现有方法对比实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison with Some Existing Works</b></p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td rowspan="2"><br /> Methods</td><td colspan="2"><br />PredCls</td><td colspan="2">SGCls</td><td colspan="2">SGGen</td><td colspan="2">SGGen<sup>*</sup></td></tr><tr><td><br />R@50</td><td>R@100</td><td>R@50</td><td>R@100</td><td>R@50</td><td>R@100</td><td>R@50</td><td>R@100</td></tr><tr><td><br />PR<sup>[26]</sup></td><td>27.9</td><td>35.0</td><td>11.8</td><td>11.4</td><td>0.3</td><td>0.4</td><td></td><td></td></tr><tr><td><br />IMP<sup>[1]</sup></td><td>44.8</td><td>53.0</td><td>21.7</td><td>24.4</td><td>3.4</td><td>4.2</td><td></td><td></td></tr><tr><td><br />GR<sup>[12]</sup></td><td>54.2</td><td>59.1</td><td>29.6</td><td>31.6</td><td>11.4</td><td>13.7</td><td></td><td></td></tr><tr><td><br />NM<sup>[11]</sup></td><td>65.2</td><td>67.1</td><td>35.8</td><td>36.5</td><td></td><td></td><td>27.2</td><td>30.3</td></tr><tr><td><br />RSSQ (ours) </td><td><b>65.7</b></td><td><b>67.5</b></td><td><b>36.7</b></td><td><b>37.4</b></td><td><b>20.7</b></td><td><b>25.7</b></td><td><b>27.4</b></td><td><b>30.6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="154">IMP模型<citation id="320" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>主要针对局部关系上下文进行建模, 丢失了全局上下文的视野.GR模型<citation id="321" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>使用特定线性变换方法根据相邻节点进行节点表示更新, 但是更新的策略相对简单.NM模型<citation id="322" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>通过双向LSTM网络生成边上下文, 丢失了结构化信息.从表1中可以看出, 提出的RSSQ方法在3个子任务中都超过了现有方法.相对于2018年CVPR的NM模型, 在子任务SGCls上超过0.9%, 在PredCls子任务上超过0.5%.在SGGen子任务上, 提出方法超过GR模型12%.这表明提出RSSQ方法可以更加有效地生成场景图.</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908015_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 关系分类逐类分析" src="Detail/GetImg?filename=images/JFYZ201908015_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 关系分类逐类分析  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908015_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 The accuracy of each relationship categories of SGCls of R@20 setting</p>

                </div>
                <div class="p1">
                    <p id="156">为了更进一步精确地对比提出地RSSQ方法和NM模型在分类性能上的改进.图3给出了在SGCls子任务中Recall@20设置上进行的关系分类准确率统计分析.横坐标上关系类别以出现频率的降序排列, 只有在关系三元组全部被预测正确, 包括主宾语和关系, 才会被统计.图3给出了根据频率将关系分为高频 (a) 、中低频 (b) 2个部分区段的实验对比.在高频段 (图3 (a) ) , NM模型和RSSQ方法对关系频率高的分类均表现良好, 在部分关系类别中, 提出的RSSQ方法相对于NM模型有微弱提升.</p>
                </div>
                <div class="p1">
                    <p id="157">在中频区域 (如图3 (b) 所示) , NM模型的分类准确率较低, 这是因为NM模型学到更多的数据集偏差而并非真正理解关系.提出的RSSQ方法在这个区间的关系分类精度有相对大的提升, 比如of, holding, behind, above, riding, at, carrying, using以及covered in关系类别.受益于更好的全局上下文特征, 提出的RSSQ方法在抽象关系分类精度方面有较明显提升, 如holding (+2.36%) 、riding (+4.76%) 、carrying (+9.75%) 以及using (+6.79) .基于位置嵌入对位置信息的增强, 提出的RSSQ方法对位置关系分类精度也有较大提升, 如of (+2.43%) 、behind (+1.12%) 、above (+1.55%) 、at (+2.14%) 以及covered in (+2.55%) .在低频段的分类识别, 2个模型均没什么表现, 这就需要更多研究, 比如少量学习<citation id="323" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="158">总之, 由于Visual Genome是一个严重不均衡的数据集, 使大多模型更容易学习数据集偏差.提出的RSSQ方法在中等频率区间性能的明显提升, 表明提出的RSSQ方法更少地受数据集偏差的影响, 在一定程度上较好地改善了数据偏差对关系分类的影响.</p>
                </div>
                <h4 class="anchor-tag" id="159" name="159"><b>3.3 残差置乱模块评价</b></h4>
                <div class="p1">
                    <p id="160">基于NM模型<citation id="324" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>中4层LSTM层组成的边上下文模块 (如图4 (a) 所示) , 本文通过置乱模块和残差连接基本架构单元来构成残差置乱模块.通过对图4 (a) 分别插入1, 2, 4次置乱层和残差连接构成3种残差置乱模块结构e1, e2和e4, 如图4 (b) ～4 (d) 所示.</p>
                </div>
                <div class="area_img" id="161">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908015_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 残差置乱模块示意图" src="Detail/GetImg?filename=images/JFYZ201908015_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 残差置乱模块示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908015_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 The initial edge context module in NM<citation id="325" type="reference"><link href="243" rel="bibliography" /><sup>[10]</sup></citation> and structures of residual shuffle module insertion</p>

                </div>
                <div class="p1">
                    <p id="162">由于NM模型<citation id="326" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>没有给出未经微调的SGGen子任务的实验结果, 残差置换模块的实验分析在PredCls和SGCls两个子任务上进行.此外, 也进行了LSTM层之间的原始设置以及残差连接2种不同连接方式的实验.如表2所示, 通过置乱操作, 在SGCls任务中有0.3%相对提升;通过残差连接, 在PredCls子任务和SGCls子任务分别有0.5%和0.7%的相对提升.在单纯加入置乱操作的设置中, PredCls子任务中有些许性能下降, 这是由于PredCls使用目标标签真值, 置乱破坏了关系的固定模式.从实验结果来看, 置乱操作不断地打乱目标序列输入次序, 在训练迭代过程中, 即使是同一条训练数据也会有不同的输入次序, 增加了模型的鲁棒性, 提高了模型的泛化能力.残差连接融合了不同层次的边上下文, 在不同LSTM层间建立短路, 从而减少梯度消失问题, 获取了更丰富语义的边上下文.</p>
                </div>
                <div class="area_img" id="163">
                    <p class="img_tit"><b>表2 残差置乱模块分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Evaluation of the Residual Shuffle Module</b></p>
                    <p class="img_note"></p>
                    <table id="163" border="1"><tr><td><br />Sub-tasks</td><td>Metrics</td><td>NM<sup>[10]</sup></td><td>e1</td><td>e2</td><td>e4</td><td>e2r</td><td>e4r</td></tr><tr><td><br />Connections</td><td></td><td></td><td>raw</td><td>raw</td><td>raw</td><td>res</td><td>res</td></tr><tr><td rowspan="2"><br />PredCls</td><td><br />R@50</td><td>65.2</td><td>64.96</td><td>65.07</td><td>64.87</td><td>65.38</td><td><b>65.67</b></td></tr><tr><td><br />R@100</td><td>67.1</td><td>66.79</td><td>66.94</td><td>66.79</td><td>67.18</td><td><b>67.47</b></td></tr><tr><td rowspan="2"><br />SGCls</td><td><br />R@50</td><td>35.8</td><td>36.04</td><td>36.11</td><td>35.98</td><td>36.39</td><td><b>36.47</b></td></tr><tr><td><br />R@100</td><td>36.5</td><td>36.76</td><td>36.83</td><td>36.74</td><td>37.12</td><td><b>37.17</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: “raw” means regular connection of LSTM layers, and “res” means residual connection.</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164"><b>3.4 消融实验</b></h4>
                <div class="p1">
                    <p id="165">为进一步分析提出的RSSQ方法中残差置乱和位置嵌入2个模块对场景图生成的性能影响, 表3给出了在3个子任务上的消融学习结果.这部分实验以NM模型为基准模型, 单纯用残差置乱模块替换NM模型中的边上下文提取模块, 在PredCls子任务和SGCls子任务中分别有0.5%和0.7% 的提升.单纯将位置嵌入模块添加到NM模型的边上下文模块中, 在PredCls子任务和SGCls子任务中有些许提升.在SGGen子任务的实验中, 位置嵌入模块与NM模型的结合是残差置换与NM模型结合, 是提出RSSQ方法中性能表现最好的组合.提出的RSSQ方法在2个子任务PredCls和SGCls是表现最好的.综上分析, 残差置乱和位置嵌入2个模块部分缓解了数据集偏差和全局上下文共享问题, 完整的RSSQ方法在3个子任务中的综合表现良好.</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表3 消融实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Ablation Study</b></p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td rowspan="2"><br />Methods</td><td rowspan="2">Residual Shuffle<br />Module</td><td rowspan="2">Spatial<br />Embedding</td><td colspan="2"><br />PredCls</td><td colspan="2">SGCls</td><td colspan="2">SGGen</td></tr><tr><td><br />R@50</td><td>R@100</td><td>R@50</td><td>R@100</td><td>R@50</td><td>R@100</td></tr><tr><td><br />NM<sup>[11]</sup></td><td>Exclude</td><td>Exclude</td><td>65.2</td><td>67.1</td><td>35.8</td><td>36.5</td><td></td><td></td></tr><tr><td><br />1</td><td>Include</td><td>Exclude</td><td>65.67</td><td>67.47</td><td>36.47</td><td>37.16</td><td>20.50</td><td>25.50</td></tr><tr><td><br />2</td><td>Exclude</td><td>Include</td><td>65.18</td><td>66.98</td><td>35.92</td><td>36.61</td><td><b>20.74</b></td><td><b>25.91</b></td></tr><tr><td><br />3 (RSSQ) </td><td>Include</td><td>Include</td><td><b>65.72</b></td><td><b>67.48</b></td><td><b>36.67</b></td><td><b>37.38</b></td><td>20.64</td><td>25.66</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="167">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908015_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 场景图分类结果可视化结果" src="Detail/GetImg?filename=images/JFYZ201908015_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 场景图分类结果可视化结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908015_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Qualitative results of SGCls</p>

                </div>
                <div class="area_img" id="168">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908015_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 时态不一致引起的错误示例" src="Detail/GetImg?filename=images/JFYZ201908015_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 时态不一致引起的错误示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908015_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Errors caused by tense disagreements</p>

                </div>
                <h4 class="anchor-tag" id="169" name="169"><b>3.5 部分场景图可视化结果</b></h4>
                <div class="p1">
                    <p id="170">为了更直观展示提出的RSSQ方法在场景图生成的效果, 图5、图6给出了场景图可视化结果.其中图像中给出的是真值标签的边界盒, 场景图给出了SGCls子任务中生成场景图和真值场景图的对比, 方框表示目标实体, 有向箭头从主语指向宾语, 椭圆形表示关系.每个给出的具体样例中的完整场景图是真值描述的场景图, 其中深色底纹表示正确预测, 浅色底纹表示错误预测.图5 (a) 是原始带有目标真值标签的原始图像, 图5 (b) 给出的是RSSQ方法生成的场景图, 图5 (c) 是NM模型<citation id="327" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>生成的场景图.</p>
                </div>
                <div class="p1">
                    <p id="171">图6给出了由于谓词的时态不一致性带来的关系分类错误, 如图6 (a) 中wears和图6 (b) 中的wearing.从图5第1行样例可以看出, RSSQ方法和NM模型<citation id="328" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>均能比较吻合地生成比较简单的场景图.从图5第3行与第5行样例可以看出, RSSQ方法相对于NM模型<citation id="329" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>改进了相对位置关系 (near, under, in front of) 的分类.从图5第2行与第5行样例可以看出, RSSQ方法在中频区间的关系类别 (carrying, in front of) 有一定改进, 缓解了数据集偏差问题.图5第4行样例说明, RSSQ方法对于高频区间的关系分类 (如of) 也有改进.</p>
                </div>
                <h3 id="172" name="172" class="anchor-tag"><b>4 总  结</b></h3>
                <div class="p1">
                    <p id="173">鉴于场景图生成方法更多的学习数据集偏差, 本文从残差置乱和位置嵌入角度改进NM模型, 提出了一个新的基于残差置乱上下文信息的场景图生成方法 (RSSQ) .置乱策略有效地改善了数据集偏差对场景图生成的影响, 尤其是在中频段的关系分类性能的提升比较明显;残差连接在不同LSTM层之间建立短路连接, 完成不同层次的信息交换, 较好解决了全局上下文信息共享, 此外, 残差连接还解决了梯度消失问题.位置嵌入从面积比和重叠比角度整合目标位置信息, 也有效地提升了提出的RSSQ方法对位置关系分类的性能.在Visual Genome数据集的实验中验证了提出的RSSQ方法可行且高效, 可以更少地受到数据集偏差的影响.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="225">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene graph generation by iterative message passing">

                                <b>[1]</b>Xu Danfei, Zhu Yuke, Choy C B, et al.Scene graph generation by iterative message passing[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3097- 3106
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image retrieval using scene graphs">

                                <b>[2]</b>Johnson J, Krishna R, Stark M, et al.Image retrieval using scene graphs[C] //Proc of the 2015 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3668- 3678
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recognition using visual phrases">

                                <b>[3]</b>Sadeghi M A, Farhadi A.Recognition using visual phrases[C] //Proc of the 2011 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:1745- 1752
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relation Networks for Object Detection">

                                <b>[4]</b>Hu Han, Gu Jianyuan, Zhang Zheng, et al.Relation Networks for Object Detection[C] //Proc of the 2018 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:3588- 3597
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph-structured representations for visual question answering">

                                <b>[5]</b>Teney D, Liu Lingqiao, Den Hengel A V, et al.Graph-structured representations for visual question answering[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3233- 3241
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to compose dynamic tree structures for visual contexts">

                                <b>[6]</b>Tang Kaihua, Zhang Hanwang, Wu Baoyuan, et al.Learning to compose dynamic tree structures for visual contexts[J].arXiv preprint, arXiv:1812.01880v1, 2018
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201809010&amp;v=MTQ5OTc3N01MeXZTZExHNEg5bk1wbzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1c=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>Yu Jun, Wang Liang, Yu Zhou.Research on visual question answering techniques[J].Journal of Computer Research and Development, 2018, 55 (9) :1946- 1958 (in Chinese) (俞俊, 汪亮, 余宙.视觉问答技术研究[J].计算机研究与发展, 2018, 55 (9) :1946- 1958) 
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene graph generation from objects,phrases and region captions">

                                <b>[8]</b>Li Yikang, Ouyang Wanli, Zhou Bolei, et al.Scene graph generation from objects, phrases and region captions[C]//Proc of the 2017 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:1270- 1279
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ViP-CNN:Visual phrase guided convolutional neural network">

                                <b>[9]</b>Li Yikang, Ouyang Wanli, Wang Xiaogang, et al.ViP-CNN:Visual phrase guided convolutional neural network[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:7244- 7253
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Factorizable Net An efficient subgraph-based framework for scene graph generation">

                                <b>[10]</b>Li Yikang, Ouyang Wanli, Zhou Bolei, et al.Factorizable Net:An efficient subgraph-based framework for scene graph generation[C] //Proc of the 2018 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2018:346- 363
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural motifs:Scene graph parsing with global context">

                                <b>[11]</b>Zellers R, Yatskar M, Thomson S, et al.Neural motifs:Scene graph parsing with global context[C] //Proc of the 2018 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:5831- 5840
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph R-CNN for scene graph generation">

                                <b>[12]</b>Yang Jianwei, Lu Jiasen, Lee S, et al.Graph R-CNN for scene graph generation[C] //Proc of the 2018 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2018:690- 706
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[13]</b>Girshick R B.Fast R-CNN[C] //Proc of the 2015 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1440- 1448
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[14]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[C] //Proc of the 2015 IEEE Int Conf on Learning Representations.Piscataway, NJ:IEEE, 2015
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of gated recurrent neural networks on sequence modeling">

                                <b>[15]</b>Chung J, Gulcehre C, Cho K, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].arXiv preprint, arXiv:1412.3555, 2014
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTQwMDZZPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2NheA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[17]</b>He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Deep residual learning for image recognition[C] //Proc of the 2016 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770- 778
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Residual LSTM:Design of a deep recurrent architecture for distant speech recognition">

                                <b>[18]</b>Kim J, Elkhamy M, Lee J, et al.Residual LSTM:Design of a deep recurrent architecture for distant speech recognition[C] //Proc of the 2017 Conf of the Int Speech Communication Association, 2017:1591- 1595.https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0477.PDF
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[19]</b>Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[C] //Proc of the 2014 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2014:818- 833
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Highway long short-term memory RNNS for distant speech recognition">

                                <b>[20]</b>Zhang Yu, Chen Guoguo, Yu Dong, et al.Highway long short-term memory RNNS for distant speech recognition[C] //Proc of the 41st Int Conf on Acoustics, Speech, and Signal Processing.Piscataway, NJ:IEEE, 2016:5755- 5759
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                Ren S, He Kaiming, Girshick R B, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137- 1149
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention is all you need">

                                <b>[22]</b>Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C] //Proc of the Conf and Workshop on Neural Information Processing Systems.New York:Curran Associates, 2017:5998- 6008
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual relationship detection with object spatial distribution">

                                <b>[23]</b>Zhu Yaohui, Jiang Shuqiang, Li Xiangyang, et al.Visual relationship detection with object spatial distribution[C] //Proc of the 2017 IEEE Int Conf on Multimedia and Expo, Piscataway, NJ:IEEE, 2017:379- 384
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDBAF74762D4CC9493EB715E691F405699&amp;v=MjMzMjJPV0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjN3YXc9Tmo3QmFzSEphTmJJcUlsSEVPOThmM1U5eGhWbW1EaDhUUXJrcFJORGZiS1JRNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>Krishna R, Zhu Yuke, Groth O, et al.Visual genome:Connecting language and vision using crowdsourced dense image annotations[J].International Journal of Computer Vision, 2017, 123 (1) :32- 73
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting visual relationships with deep relational networks">

                                <b>[25]</b>Dai Bo, Zhang Yuqi, Lin Dahua, et al.Detecting visual relationships with deep relational networks[C] //Proc of the 2017 IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3298- 3308
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual relationship detection with language priors">

                                <b>[26]</b>Lu Cewu, Krishna R, Bernstein M S, et al.Visual relationship detection with language priors[C] //Proc of the 2016 IEEE European Conf on Computer Vision.Piscataway, NJ:IEEE, 2016:852- 869
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Meta-SGD:Learning to learn quickly for few shot learning">

                                <b>[27]</b>Li Zhenguo, Zhou Fengwei, Chen Fei, et al.Meta-SGD:Learning to learn quickly for few shot learning[J].arXiv preprint, arXiv:1707.09835, 2017
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908015&amp;v=MjIwMDZPZVplUnJGeXZnVzc3TUx5dlNkTEc0SDlqTXA0OUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ2WUZNWW5YWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

