

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127884680900000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908012%26RESULT%3d1%26SIGN%3doLFVjts0wnOmdT%252bbEH7rWt38cdk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908012&amp;v=Mjg2MDllUnJGeXZnVmJyUEx5dlNkTEc0SDlqTXA0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#79" data-title="&lt;b&gt;1 典型动量方法的收敛性介绍&lt;/b&gt; "><b>1 典型动量方法的收敛性介绍</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="&lt;b&gt;2 个体最优收敛性分析&lt;/b&gt; "><b>2 个体最优收敛性分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#188" data-title="&lt;b&gt;3 实  验&lt;/b&gt; "><b>3 实  验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#190" data-title="&lt;b&gt;3.1 实验数据集和比较算法&lt;/b&gt;"><b>3.1 实验数据集和比较算法</b></a></li>
                                                <li><a href="#194" data-title="&lt;b&gt;3.2 实验方法及结论&lt;/b&gt;"><b>3.2 实验方法及结论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#206" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#270" data-title="附录1．正文引理1证明． ">附录1．正文引理1证明．</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#192" data-title="&lt;b&gt;表1 标准数据集描述&lt;/b&gt;"><b>表1 标准数据集描述</b></a></li>
                                                <li><a href="#204" data-title="图1 收敛速率比较图">图1 收敛速率比较图</a></li>
                                                <li><a href="#205" data-title="图2 稀疏度比较图">图2 稀疏度比较图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="286">


                                    <a id="bibliography_1" title="Bottou L, Curtis F E, Nocedal J.Optimization methods for large-scale machine learning[J].SIAM Review, 2018, 60 (2) :223- 311" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimization methods for large-scale machine learning">
                                        <b>[1]</b>
                                        Bottou L, Curtis F E, Nocedal J.Optimization methods for large-scale machine learning[J].SIAM Review, 2018, 60 (2) :223- 311
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_2" title="Polyak B T.Some methods of speeding up the convergence of iteration methods[J].USSR Computational Mathematics and Mathematical Physics, 1964, 4 (5) :1- 17" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Some methods of speeding up the convergence of iteration methods">
                                        <b>[2]</b>
                                        Polyak B T.Some methods of speeding up the convergence of iteration methods[J].USSR Computational Mathematics and Mathematical Physics, 1964, 4 (5) :1- 17
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_3" title="Nesterov Y.A method of solving a convex programming problem with convergence rate O (1/k2) [J].Soviet Mathematics Doklady, 1983, 27 (2) :372- 376" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A method of solving a convex programming problem with convergence rate O (1/k2)">
                                        <b>[3]</b>
                                        Nesterov Y.A method of solving a convex programming problem with convergence rate O (1/k2) [J].Soviet Mathematics Doklady, 1983, 27 (2) :372- 376
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_4" title="Sutskever I, Martens J, Dahl G E, et al.On the importance of initialization and momentum in deep learning[C] //Proc of the Int Conf on Machine Learning (ICML) .New York:ACM, 2013:1139- 1147" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the importance of initialization and momentum in deep learning">
                                        <b>[4]</b>
                                        Sutskever I, Martens J, Dahl G E, et al.On the importance of initialization and momentum in deep learning[C] //Proc of the Int Conf on Machine Learning (ICML) .New York:ACM, 2013:1139- 1147
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_5" title="Nemirovsky A S, Yudin D B.Problem Complexity and Method Efficiency in optimization[M].New York:Wiley-Interscience, 1983" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Problem Complexity and Method Efficiency in optimization">
                                        <b>[5]</b>
                                        Nemirovsky A S, Yudin D B.Problem Complexity and Method Efficiency in optimization[M].New York:Wiley-Interscience, 1983
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_6" title="Tseng P.Approximation accuracy, gradient methods, and error bound for structured convex optimization[J].Mathematical Programming, 2010, 125 (2) :263- 295" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003749915&amp;v=MDUyNDRKVms9Tmo3QmFyTzRIdEhQcUl0TWJlb0tZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDM2xVTHJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Tseng P.Approximation accuracy, gradient methods, and error bound for structured convex optimization[J].Mathematical Programming, 2010, 125 (2) :263- 295
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_7" title="Beck A, Teboulle M.A fast iterative shrinkage-thresholding algorithm for linear inverse problems[J].SIAM Journal on Imaging Sciences, 2009, 2 (1) :183- 202" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems">
                                        <b>[7]</b>
                                        Beck A, Teboulle M.A fast iterative shrinkage-thresholding algorithm for linear inverse problems[J].SIAM Journal on Imaging Sciences, 2009, 2 (1) :183- 202
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_8" title="Hu C, Kwok J T, Pan W.Accelerated gradient methods for stochastic optimization and online learning[C] //Proc of the 23rd Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2009:781- 789" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerated gradient methods for stochastic optimization and online learning">
                                        <b>[8]</b>
                                        Hu C, Kwok J T, Pan W.Accelerated gradient methods for stochastic optimization and online learning[C] //Proc of the 23rd Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2009:781- 789
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_9" title="Lin H, Mairal J, Harchaoui Z.A universal catalyst for first-order optimization[C] //Proc of the 29th Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2015:3384- 3392" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A universal catalyst for first-order optimization">
                                        <b>[9]</b>
                                        Lin H, Mairal J, Harchaoui Z.A universal catalyst for first-order optimization[C] //Proc of the 29th Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2015:3384- 3392
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_10" title="Allen-Zhu Z.Katyusha:The first direct acceleration of stochastic gradient methods[J].The Journal of Machine Learning Research, 2017, 18 (1) :8194- 8244" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Katyusha:the first direct acceleration of stochastic gradient methods">
                                        <b>[10]</b>
                                        Allen-Zhu Z.Katyusha:The first direct acceleration of stochastic gradient methods[J].The Journal of Machine Learning Research, 2017, 18 (1) :8194- 8244
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_11" title="Mahdavi M, Zhang L, Jin R.Mixed optimization for smooth functions[C] //Proc of the 27th Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2013:674- 682" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mixed optimization for smooth functions">
                                        <b>[11]</b>
                                        Mahdavi M, Zhang L, Jin R.Mixed optimization for smooth functions[C] //Proc of the 27th Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2013:674- 682
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_12" title="Shang Fanhua, Jiao Licheng, Zhou Kaiwen, et al.ASVRG:Accelerated proximal SVRG[C] //Proc of the 10th Asian Conf on Machine Learning.Cambridge:JMLR, 2018:815- 830" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ASVRG:Accelerated proximal SVRG">
                                        <b>[12]</b>
                                        Shang Fanhua, Jiao Licheng, Zhou Kaiwen, et al.ASVRG:Accelerated proximal SVRG[C] //Proc of the 10th Asian Conf on Machine Learning.Cambridge:JMLR, 2018:815- 830
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_13" title="Ghadimi E, Feyzmahdavian H R, Johansson M.Global convergence of the heavy-ball method for convex optimization[C] //proc of the 14th European Control Conf.Piscataway, NJ:IEEE, 2015:310- 315" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global convergence of the heavy-ball method for convex optimization">
                                        <b>[13]</b>
                                        Ghadimi E, Feyzmahdavian H R, Johansson M.Global convergence of the heavy-ball method for convex optimization[C] //proc of the 14th European Control Conf.Piscataway, NJ:IEEE, 2015:310- 315
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_14" title="Yang Tianbao, Lin Qihang, Li Zhe.Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization[EB/OL]. (2016-05-04) [2018-12-13].https://arxiv.org/abs/1604.03257" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization">
                                        <b>[14]</b>
                                        Yang Tianbao, Lin Qihang, Li Zhe.Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization[EB/OL]. (2016-05-04) [2018-12-13].https://arxiv.org/abs/1604.03257
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_15" title="Xiao Lin.Dual averaging methods for regularized stochastic learning and online optimization[J].Journal of Machine Learning Research, 2010, 11 (1) :2543- 2596" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization">
                                        <b>[15]</b>
                                        Xiao Lin.Dual averaging methods for regularized stochastic learning and online optimization[J].Journal of Machine Learning Research, 2010, 11 (1) :2543- 2596
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_16" title="Xiao Lin, Zhang Tong.A proximal stochastic gradient method with progressive variance reduction[J].SIAM Journal on Optimization, 2014, 24 (4) :2057- 2075" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Proximal Stochastic Gradient Method with Progressive Variance Reduction">
                                        <b>[16]</b>
                                        Xiao Lin, Zhang Tong.A proximal stochastic gradient method with progressive variance reduction[J].SIAM Journal on Optimization, 2014, 24 (4) :2057- 2075
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_17" title="Shamir O.Open problem:Is averaging needed for strongly convex stochastic gradient descent?[C] //Proc of the 25th Conf on Learning Theory.New York:ACM, 2012:471- 473" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Open problem:Is averaging needed for strongly convex stochastic gradient descent?">
                                        <b>[17]</b>
                                        Shamir O.Open problem:Is averaging needed for strongly convex stochastic gradient descent?[C] //Proc of the 25th Conf on Learning Theory.New York:ACM, 2012:471- 473
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_18" title="Shamir O, Zhang Tong.Stochastic gradient descent for non-smooth optimization:Convergence results and optimal averaging schemes[C] //Proc of the 29th Int Conf on Machine Learning.New York:ACM, 2013:71- 79" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent for nonsmooth optimization:convergence results and optimal averaging schemes">
                                        <b>[18]</b>
                                        Shamir O, Zhang Tong.Stochastic gradient descent for non-smooth optimization:Convergence results and optimal averaging schemes[C] //Proc of the 29th Int Conf on Machine Learning.New York:ACM, 2013:71- 79
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_19" title="Tao Wei, Pan Zhisong, Zhu Xiaohui, et al.The Optimal individual convergence rate for the projected subgradient method with linear interpolation operation[J].Journal of Computer Research and Development, 2017, 54 (3) :529- 536 (in Chinese) (陶蔚, 潘志松, 朱小辉, 等.线性插值投影次梯度方法的最优个体收敛速率[J].计算机研究与发展, 2017, 54 (3) :529- 536) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201703007&amp;v=MTMwOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnVmJyUEx5dlNkTEc0SDliTXJJOUZZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        Tao Wei, Pan Zhisong, Zhu Xiaohui, et al.The Optimal individual convergence rate for the projected subgradient method with linear interpolation operation[J].Journal of Computer Research and Development, 2017, 54 (3) :529- 536 (in Chinese) (陶蔚, 潘志松, 朱小辉, 等.线性插值投影次梯度方法的最优个体收敛速率[J].计算机研究与发展, 2017, 54 (3) :529- 536) 
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_20" title="Tao Wei, Pan Zhisong, Chu Dejun, et al.The individual convergence of projected subgradient methods using the Nesterov’s step-size strategy[J].Chinese Journal of Computers, 2018, 41 (1) :164- 176 (in Chinese) (陶蔚, 潘志松, 储德军, 等.使用 Nesterov步长策略投影次梯度方法的个体收敛性[J].计算机学报, 2018, 41 (1) :164- 176) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801010&amp;v=MjY0NzZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnVmJyUEx6N0Jkckc0SDluTXJvOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        Tao Wei, Pan Zhisong, Chu Dejun, et al.The individual convergence of projected subgradient methods using the Nesterov’s step-size strategy[J].Chinese Journal of Computers, 2018, 41 (1) :164- 176 (in Chinese) (陶蔚, 潘志松, 储德军, 等.使用 Nesterov步长策略投影次梯度方法的个体收敛性[J].计算机学报, 2018, 41 (1) :164- 176) 
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_21" title="Tao Wei, Pan Zhisong, Wu Gaowei, et al.Primal averaging:A new gradient evaluation step to attain the optimal individual convergence[J].IEEE Transactions on Cybernetics, 2018.DOI:10.1109/TCYB.2018.2874332" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Primal averaging:A new gradient evaluation step to attain the optimal individual convergence">
                                        <b>[21]</b>
                                        Tao Wei, Pan Zhisong, Wu Gaowei, et al.Primal averaging:A new gradient evaluation step to attain the optimal individual convergence[J].IEEE Transactions on Cybernetics, 2018.DOI:10.1109/TCYB.2018.2874332
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_22" title="Zinkevich M.Online convex programming and generalized infinitesimal gradient ascent[C] //Proc of the 20th Int Conf on Machine Learning.New York:ACM, 2003:928- 936" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online convex programming and generalized infinitesimal gradient ascent">
                                        <b>[22]</b>
                                        Zinkevich M.Online convex programming and generalized infinitesimal gradient ascent[C] //Proc of the 20th Int Conf on Machine Learning.New York:ACM, 2003:928- 936
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_23" title="Duchi J, Shalev-Shwartz S, Singer Y, et al.Efficient projections onto the l1-ball for learning in high dimensions[C] //Proc of the 25th Int Conf on Machine learning.New York:ACM, 2008:272- 279" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient projections onto the L1-ball for learning in high dimensions">
                                        <b>[23]</b>
                                        Duchi J, Shalev-Shwartz S, Singer Y, et al.Efficient projections onto the l1-ball for learning in high dimensions[C] //Proc of the 25th Int Conf on Machine learning.New York:ACM, 2008:272- 279
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_24" title="Liu Jun, Ye Jieping.Efficient Euclidean projections in linear time[C] //Proc of the 26th Annual Int Conf on Machine Learning.New York:ACM, 2009:657- 664" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient euclidean projections in linear time">
                                        <b>[24]</b>
                                        Liu Jun, Ye Jieping.Efficient Euclidean projections in linear time[C] //Proc of the 26th Annual Int Conf on Machine Learning.New York:ACM, 2009:657- 664
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_25" title="Agarwal A, Bartlett P L, Ravikumar P, et al.Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization[J].IEEE Transactions on Information Theory, 2012, 58 (5) :3235- 3249" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization">
                                        <b>[25]</b>
                                        Agarwal A, Bartlett P L, Ravikumar P, et al.Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization[J].IEEE Transactions on Information Theory, 2012, 58 (5) :3235- 3249
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_26" title="Shalev-Shwartz S, Singer Y, Srebro N, et al.Pegasos:Primal estimated sub-gradient solver for svm[J].Mathematical Programming, 2011, 127 (1) :3- 30" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pegasos: Primal estimated sub-gradient solver for SVM">
                                        <b>[26]</b>
                                        Shalev-Shwartz S, Singer Y, Srebro N, et al.Pegasos:Primal estimated sub-gradient solver for svm[J].Mathematical Programming, 2011, 127 (1) :3- 30
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_27" title="Duchi J C.Introductory lectures on stochastic optimization[EB/OL].2010 [2018-12-18].http://web.standford.edu/～jduchi" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introductory lectures on stochastic optimization">
                                        <b>[27]</b>
                                        Duchi J C.Introductory lectures on stochastic optimization[EB/OL].2010 [2018-12-18].http://web.standford.edu/～jduchi
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_28" title="Liu Jun, Ji Shuiwang, Ye Jieping.SLEP:Sparse learning with efficient projections[EB/OL]. (2010-10-08) [2018-12-31].http://www.public.asu.edu/～jye02/Software/SLEP" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLEP:Sparse learning with efficient projections">
                                        <b>[28]</b>
                                        Liu Jun, Ji Shuiwang, Ye Jieping.SLEP:Sparse learning with efficient projections[EB/OL]. (2010-10-08) [2018-12-31].http://www.public.asu.edu/～jye02/Software/SLEP
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_29" title="Rakhlin A, Shamir O, Sridharan K.Making gradient descent optimal for strongly convex stochastic optimization[C] //Proc of the 29th Int Conf on Machine Learning.New York:ACM, 2012:449- 456" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making gradient descent optimal for strongly convex stochastic optimization">
                                        <b>[29]</b>
                                        Rakhlin A, Shamir O, Sridharan K.Making gradient descent optimal for strongly convex stochastic optimization[C] //Proc of the 29th Int Conf on Machine Learning.New York:ACM, 2012:449- 456
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1686-1694 DOI:10.7544/issn1000-1239.2019.20190167            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>Heavy-Ball型动量方法的最优个体收敛速率</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E7%A6%B9%E5%98%89&amp;code=42307079&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程禹嘉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%B6%E8%94%9A&amp;code=38464913&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陶蔚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%AE%87%E7%BF%94&amp;code=42376376&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘宇翔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%B6%E5%8D%BF&amp;code=38615181&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陶卿</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E9%99%86%E5%86%9B%E7%82%AE%E5%85%B5%E9%98%B2%E7%A9%BA%E5%85%B5%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=1702679&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军陆军炮兵防空兵学院信息工程系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E9%99%86%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E6%8C%87%E6%8C%A5%E6%8E%A7%E5%88%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1701801&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军陆军工程大学指挥控制工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>动量方法作为一种加速技巧被广泛用于提高一阶梯度优化算法的收敛速率.目前, 大多数文献所讨论的动量方法仅限于Nesterov提出的加速方法, 而对Polyak提出的Heavy-ball型动量方法的研究却较少.特别, 在目标函数非光滑的情形下, Nesterov加速方法具有最优的个体收敛性, 并在稀疏优化问题的求解中具有很好的效果.但对于Heavy-ball型动量方法, 目前仅仅获得了平均输出形式的最优收敛速率, 个体收敛是否具有最优性仍然未知.对于非光滑优化问题, 通过巧妙地设置步长, 证明了Heavy-ball型动量方法具有最优的个体收敛速率, 从而说明了Heavy-ball型动量方法可以将投影次梯度方法的个体收敛速率加速至最优.作为应用, 考虑了<i>l</i><sub>1</sub>范数约束的hinge损失函数优化问题.通过与同类的优化算法相比, 实验验证了该理论分析的正确性以及所提算法在保持稀疏性方面的良好性能.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%80%E9%98%B6%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">一阶梯度方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E9%87%8F%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动量方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AA%E4%BD%93%E6%94%B6%E6%95%9B%E9%80%9F%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">个体收敛速率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Heavy-ball%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Heavy-ball方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *陶卿, qing.tao@ia.ac.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61673394);</span>
                    </p>
            </div>
                    <h1><b>Optimal Individual Convergence Rate of the Heavy-Ball-Based Momentum Methods</b></h1>
                    <h2>
                    <span>Cheng Yujia</span>
                    <span>Tao Wei</span>
                    <span>Liu Yuxiang</span>
                    <span>Tao Qing</span>
            </h2>
                    <h2>
                    <span>Department of Information Engineering, Army Academy of Artillery and Air Defense of PLA</span>
                    <span>College of Command and Control Engineering, Army Engineering University of PLA</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The momentum method is widely used as an acceleration technique to improve the convergence of the first-order gradient algorithms. So far, the momentum methods discussed in most literatures are only limited to the accelerated method proposed by Nesterov, but the Heavy-ball momentum method proposed by Polyak is seldom studied. In particular, in the case of non-smooth objective functions, the individual optimal convergence of Nesterov accelerated methods has been derived, and it has high performance in solving sparse optimization problems. In contrast, while it has been proved that the Heavy-ball momentum method has an optimal convergence rate, it is only in terms of the averaged outputs. To our best knowledge, whether it has optimal individual convergence or not still remains unknown. In this paper, we focus on the non-smooth optimizations. We prove that the Heavy-ball momentum method achieves the optimal individual convergence by skillfully selecting the time-varying step-size, which indicates that Heavy-ball momentum is an efficient acceleration strategy for the individual convergence of the projected subgradient methods. As an application, the constrained hinge loss function optimization problems within an <i>l</i><sub>1</sub>-norm ball are considered. In comparison with other optimization algorithms, the experiments demonstrate the correctness of our theoretical analysis and performance of the proposed algorithms in keeping the sparsity.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=first-order%20gradient%20methods&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">first-order gradient methods;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=momentum%20methods&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">momentum methods;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=individual%20convergence%20rate&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">individual convergence rate;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=heavy-ball%20methods&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">heavy-ball methods;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparsity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparsity;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Cheng Yujia, born in 1996. Master candidate.Her main research interests include convex optimization algorithms and its application in machine learning. <image id="254" type="" href="images/JFYZ201908012_25400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Tao Wei, born in 1991.PhD candidate.His main research interests include convex optimization algorithms and its application in machine learning.<image id="256" type="" href="images/JFYZ201908012_25600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Yuxiang, born in 1992. Master candidate. His main research interests include convex optimization algorithms and its application in machine learning.<image id="260" type="" href="images/JFYZ201908012_26000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Tao Qing, born in 1965.Professor and PhD supervisor.Senior member of CCF. His main research interests include pattern recognition, machine learning and applied mathematic<image id="261" type="" href="images/JFYZ201908012_26100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61673394);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="69">机器学习问题普遍可以转化为求解目标函数最小值的优化问题, 一阶梯度优化方法由于具有算法简单、迭代成本小等特点, 成为处理大规模数据问题的首要选择.在此基础上发展起来的随机优化方法由于避免了每一次迭代都需要遍历整个样本集, 充分利用训练样本集合的冗余性, 从而具有计算代价低和实际收敛速率快等优点, 已经成为处理大规模机器学习问题的实用方法<citation id="344" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="70">动量方法是在经典梯度下降方法的基础上通过添加动量而获得的一种特殊的一阶优化方法.研究者将动量算法分为2类:一类是Polyak于1964年提出的Heavy-ball型动量方法<citation id="345" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 另一类是1983年Nesterov提出的NAG (Nesterov accelerated gradient) 型动量方法<citation id="346" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.这2类算法的主要差别在于所使用动量项的不同, 前者只是使用了前一步的迭代信息而后者引入了当前步迭代算法的二阶信息.对于不同条件下的优化问题, 这2类算法的收敛性也有不同的表现.当目标函数光滑时, NAG具有最优的<i>O</i> (1/<i>t</i><sup>2</sup>) 收敛速率<citation id="347" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 其中<i>t</i>为算法的迭代步数.当目标函数强凸且二次可微时, 尽管Heavy-ball型动量方法、梯度下降法和NAG方法都具有相同的线性收敛速率, 但Heavy-ball型动量方法具有最小的收敛因子<citation id="348" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.随机动量方法被广泛应用于神经网络的训练, 并显著的提高了其收敛性能<citation id="349" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="71">NAG是优化领域具有里程碑意义的算法, 它填补了Nemirovski与Yudin所证明的“任何一阶优化算法都不可能得到比<i>O</i> (1/<i>t</i><sup>2</sup>) 更快的收敛速率<citation id="350" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>”的间隙, 也吸引了众多机器学习研究者的关注.特别是针对大规模具有特定含义的正则化损失函数优化问题, 研究工作层出不穷.早期重要的工作主要包括只要损失函数满足光滑性条件就可得到整个目标函数光滑时的最优收敛速率<citation id="354" type="reference"><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>, 以及NAG随机形式的最优收敛速率等等<citation id="351" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.近期NAG的研究主要集中在与其他优化方法的结合上.如2015年Lin等人基于NAG提出了一种通用的加速策略Catalyst<citation id="352" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 在目标函数强凸的条件下将批处理优化方法、坐标优化方法和增量优化算法进行了加速.最近, Allen-Zhu引入带有动量参数的方差项, 提出了著名的Katyusha算法<citation id="353" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 成功地将方差减少方法与NAG相结合.2018年Shang等人将NAG算法与Mixed Optimization算法相结合, 仅使用了一个动量项就取得了与Katyusha算法相同的收敛速率<citation id="355" type="reference"><link href="306" rel="bibliography" /><link href="308" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="72">与标准的梯度下降法相较, Heavy-ball型动量方法在目标函数在某些方向变化较弱而在另一些方向变化很强的时候, 可以取得好的加速效果, 复杂度却几乎没有增加.但与NAG方法相比, Heavy-ball型动量方法的研究却屈指可数.2014年Ghadimi等人对Heavy-ball方法的收敛性进行了深入的研究, 给出了目标函数光滑条件下的平均和个体收敛速率<citation id="356" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 但均未达到最优.2016年Yang等人建立了一种含有多种参数的算法框架, 统一处理了梯度下降法、Heavy-ball方法以及NAG方法<citation id="357" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 在该框架中设置不同的参数即可得到不同的优化算法.这种统一的算法对于非光滑优化问题在平均输出方式下具有最优的收敛速率.</p>
                </div>
                <div class="p1">
                    <p id="73">对于非光滑问题, 目前大多数优化算法所获得的最优速率都基于加权平均和的输出形式, 这种形式较易获得稳定的收敛性, 但在结构优化特别是稀疏学习问题中<citation id="361" type="reference"><link href="314" rel="bibliography" /><link href="316" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>, 使用迭代过程中的个体输出能够获得比平均输出更好的稀疏效果.但个体解能否获得最优的收敛速率却显得困难重重, 强凸条件下的个体收敛问题也成为了open问题<citation id="358" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.2013年Shamir和Zhang提出了一种将SGD问题由平均输出方式得到个体收敛速率的技巧<citation id="359" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 成功得到了<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>log</mi><mtext> </mtext><mi>t</mi><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">) </mo><mtext>的</mtext></mrow></math></mathml>收敛速率, 这是第一个关于SGD个体收敛速率的结果, 但却与最优值相差一个对数因子.2015年文献<citation id="360" type="reference">[<a class="sup">19</a>]</citation>采用线性差值技巧虽然保证了个体解最优的收敛性和稳定性, 却由于插值的累积而失去了稀疏性.2018年文献<citation id="362" type="reference">[<a class="sup">20</a>,<a class="sup">21</a>]</citation>将NAG步长策略引入到投影次梯度中, 得到了最优个体收敛性并同时保证了良好的稀疏性.由于Heavy-ball方法与NAG在动量方法中具有同等重要的地位, Heavy-ball方法是否也具有最优个体收敛性这一问题显然值得研究.</p>
                </div>
                <div class="p1">
                    <p id="75">本文的主要工作有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="76">1) 对于非光滑优化问题, 证明了Heavy-ball型动量方法具有最优的个体收敛速率.据我们所知, 这一结果填补了Heavy-ball型动量方法在非光滑情形下个体最优收敛性研究的缺失, 有助于更全面地理解Heavy-ball型动量方法, 也说明了在处理非光滑问题时Heavy-ball型动量方法和NAG具有同样的重要地位.</p>
                </div>
                <div class="p1">
                    <p id="77">2) 本文证明基于光滑情形下Heavy-ball型动量方法的收敛性分析<citation id="363" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 但不同的是, 为得到非光滑情形下的个体最优收敛速率, 我们巧妙构造了步长和动量权重的迭代方式, 同时利用Zinkevich在处理在线优化方法收敛性使用的技巧<citation id="364" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, 避免了变步长和权重导致的递归问题.</p>
                </div>
                <div class="p1">
                    <p id="78">3) 通过典型的稀疏优化问题实验, 验证了理论分析的正确性以及所提算法在保持稀疏性方面的良好性能.</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag"><b>1 典型动量方法的收敛性介绍</b></h3>
                <div class="p1">
                    <p id="80">本节我们对2种动量方法的收敛性以及它们之间的联系和区别进行必要的介绍.</p>
                </div>
                <div class="p1">
                    <p id="81">考虑有约束优化问题:</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>∈</mo><mi>Q</mi></mrow></munder><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <i>f</i> (<b><i>w</i></b>) 为凸函数, <i>Q</i>⊆R<sup><i>n</i></sup>是有界闭凸集合, 记<b><i>w</i></b><sup>*</sup>为式 (1) 的一个最优解.批处理形式投影次梯度方法的迭代步骤为</p>
                </div>
                <div class="area_img" id="263">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="87"><b><i>P</i></b><sub><i>Q</i></sub>是<i>Q</i>上的投影算子<citation id="365" type="reference"><link href="330" rel="bibliography" /><link href="332" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>, <b><i>w</i></b><sub><i>t</i></sub>为<b><i>w</i></b>在第<i>t</i>步的输出, <i>α</i><sub><i>t</i></sub>为迭代步长, <image id="88" type="" href="images/JFYZ201908012_08800.jpg" display="inline" placement="inline"><alt></alt></image><i>f</i> (<b><i>w</i></b><sub><i>t</i></sub>) 是<i>f</i> (<b><i>w</i></b>) 在<b><i>w</i></b><sub><i>t</i></sub>处的次梯度.</p>
                </div>
                <div class="p1">
                    <p id="89">对于形如式 (2) 的算法, 所谓的平均收敛速率指的是<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">w</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>的收敛速率, 其中<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">w</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>.而个体收敛速率指的是<i>f</i> (<b><i>w</i></b><sub><i>t</i></sub>) -<i>f</i> (<b><i>w</i></b><sup>*</sup>) 的收敛速率.一般来说, 特别是对非光滑优化问题, 个体收敛更难获得最优速率<citation id="366" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="92">Agarwal等人给出非光滑条件下式 (2) 的平均收敛速率为<citation id="367" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="93"><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mrow><mo>[</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">w</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>≤</mo><mi>Ο</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">) </mo></mrow></math></mathml>. (3) </p>
                </div>
                <div class="p1">
                    <p id="95">式 (2) 的个体收敛速率由Shamir和Zhang Tong证得<citation id="368" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="96"><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mrow><mo>[</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>≤</mo><mi>Ο</mi><mo stretchy="false"> (</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">) </mo></mrow></math></mathml>, (4) </p>
                </div>
                <div class="p1">
                    <p id="98">这与最优速率之间还是存在着数量级上的差距.</p>
                </div>
                <div class="p1">
                    <p id="99">Yang等人建立了随机梯度下降法与随机动量方法的统一框架<citation id="369" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>:</p>
                </div>
                <div class="area_img" id="100">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="102">其中, <i>β</i>为动量参数, <i>s</i>≥0.随着<i>s</i>由大至小, 式 (5) 依次变为</p>
                </div>
                <div class="p1">
                    <p id="103">1) 当<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>-</mo><mi>β</mi></mrow></mfrac></mrow></math></mathml>时, 为梯度下降法:</p>
                </div>
                <div class="area_img" id="262">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="108">2) 当<i>s</i>=1时, 即为NAG方法:</p>
                </div>
                <div class="area_img" id="264">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="111"><b><i>w</i></b><sub><i>t</i>+1</sub>=<b><i>y</i></b><sub><i>t</i>+1</sub>+<i>β</i> (<b><i>y</i></b><sub><i>t</i>+1</sub>-<b><i>y</i></b><sub><i>t</i></sub>) ; (7) </p>
                </div>
                <div class="p1">
                    <p id="112">3) 当<i>s</i>=0时, 即为Heavy-ball方法:</p>
                </div>
                <div class="area_img" id="266">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="115">通过选取适当的步长, 文献<citation id="370" type="reference">[<a class="sup">14</a>]</citation>给出了平均收敛速率:</p>
                </div>
                <div class="p1">
                    <p id="116"><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mrow><mo>[</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">w</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>≤</mo><mi>Ο</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">) </mo></mrow></math></mathml>. (9) </p>
                </div>
                <div class="p1">
                    <p id="118">在光滑目标函数条件下, 由于NAG方法进行每一步迭代时都会使用之前迭代的部分甚至全部信息, 所以通常可以取得较Heavy-ball方法更快的收敛速率.当目标函数光滑且强凸时, 梯度下降方法、Heavy-ball方法与NAG均可以达到线性收敛, 即:</p>
                </div>
                <div class="p1">
                    <p id="119"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>≤</mo><mi>q</mi><msup><mrow></mrow><mi>t</mi></msup><mrow><mo>[</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="121">但Heavy-ball方法获得的收敛因子<i>q</i>是最小的<citation id="371" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="122">文献<citation id="372" type="reference">[<a class="sup">19</a>]</citation>通过在投影次梯度上进行了线性插值的操作:</p>
                </div>
                <div class="area_img" id="123">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="125">其中, <i>Q</i>={<b><i>w</i></b>:<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>≤</mo><mi>z</mi><mo stretchy="false">}</mo><mo>, </mo><mi>z</mi><mo>≻</mo><mn>0</mn></mrow></math></mathml>.该方法在每次迭代时虽然获得了个体解的最优性, 但由于在投影操作之后又对所有<b><i>w</i></b><sub><i>t</i></sub>进行了一次加权求和的运算, 导致了稀疏性的缺失.文献<citation id="373" type="reference">[<a class="sup">20</a>]</citation>采用NAG步长策略同样得到了个体收敛的最优性:</p>
                </div>
                <div class="area_img" id="267">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="131">其中, <i>θ</i><sub><i>t</i></sub>与<i>η</i><sub><i>t</i></sub>为步长参数, 与线性插值技巧不同的是该方法每一步的解都是通过投影直接得到, 因此可以得到良好的稀疏效果.与之类似, Heavy-ball方法的个体解也应当具备稀疏性.</p>
                </div>
                <h3 id="132" name="132" class="anchor-tag"><b>2 个体最优收敛性分析</b></h3>
                <div class="p1">
                    <p id="133">本节给出Heavy-ball方法在目标函数非光滑条件下的个体收敛性证明.</p>
                </div>
                <div class="p1">
                    <p id="134">对于光滑的优化问题, 文献<citation id="374" type="reference">[<a class="sup">13</a>]</citation>引进加权的动量项<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mi>β</mi><mrow><mn>1</mn><mo>-</mo><mi>β</mi></mrow></mfrac><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 此时Heavy-ball方法的迭代方式可以转化为</p>
                </div>
                <div class="area_img" id="268">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="139">从式 (13) 可以看出, Heavy-ball型动量方法是在梯度下降法基础上添加了动量项<b><i>p</i></b><sub><i>t</i></sub>.正是由于与梯度下降法的这种相似性, 使得梯度下降法的收敛分析思路也可以用于Heavy-ball方法.</p>
                </div>
                <div class="p1">
                    <p id="140">值得注意的是, 文献<citation id="375" type="reference">[<a class="sup">13</a>]</citation>将<i>α</i>和<i>β</i>均设定为常数, 但对于非光滑优化问题, 这样的选取办法无法获得个体收敛速率.因此我们选取了时变的<i>α</i>与<i>β</i>, 但此时又会导致式 (13) 的迭代关系不成立.为了解决这个问题, 我们设置<b><i>p</i></b><sub><i>t</i></sub>=<i>t</i> (<b><i>w</i></b><sub><i>t</i></sub>-<b><i>w</i></b><sub><i>t</i>-1</sub>) , 通过巧妙地选取<i>α</i><sub><i>t</i></sub>和<i>β</i><sub><i>t</i></sub> (见定理1) , 我们得到:</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mfrac><mrow><mn>2</mn><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>β</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">基于这个关系式, 我们可以证明定理1.为了解决变步长和权重导致的递归问题, 我们先证明引理1.</p>
                </div>
                <div class="p1">
                    <p id="143"><b>引理1</b>. 令<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi mathvariant="bold-italic">u</mi><mo>∈</mo><mi>Q</mi></mrow></munder><mspace width="0.25em" /><mi>d</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">) </mo><mo>, </mo><mi>G</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>∈</mo><mi>Q</mi></mrow></munder><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 有:</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mi>η</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac></mrow></mstyle><mrow><mo>[</mo><mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mrow><mo>|</mo><mi>G</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mi>η</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>≤</mo><mrow><mo>|</mo><mi>F</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mfrac><mn>1</mn><mrow><mn>2</mn><mi>η</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mrow><mo>|</mo><mi>G</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mn>2</mn><msqrt><mi>t</mi></msqrt><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">具体证明见附录1.</p>
                </div>
                <div class="p1">
                    <p id="147"><b>定理1</b>. 设<i>f</i> (<b><i>w</i></b>) 为一般凸函数, 取<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mi>t</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo>, </mo><mrow><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo></mrow><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>2</mn><mo stretchy="false">) </mo><msqrt><mi>t</mi></msqrt></mrow></mfrac><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>由式 (8) 产生, 则:</p>
                </div>
                <div class="p1">
                    <p id="149" class="code-formula">
                        <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>≤</mo><mfrac><mrow><msqrt><mi>t</mi></msqrt></mrow><mrow><mn>2</mn><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mo>|</mo><mi>F</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mfrac><mrow><mn>2</mn><msqrt><mi>t</mi></msqrt><mo>-</mo><mn>1</mn></mrow><mrow><mn>2</mn><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mo>|</mo><mi>G</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>t</mi></mrow></mfrac><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="150">具体证明见附录2.</p>
                </div>
                <div class="p1">
                    <p id="151">综上, 我们成功得到了Heavy-ball方法在非光滑情况下的最优个体收敛速率.然而批处理形式的Heavy-ball方法在计算<image id="152" type="" href="images/JFYZ201908012_15200.jpg" display="inline" placement="inline"><alt></alt></image><i>f</i> (<b><i>w</i></b>) 时需要遍历整个样本集合, 这种操作不适合处理大规模数据.为此, 我们将上述算法推广至随机形式以求解机器学习问题.</p>
                </div>
                <div class="p1">
                    <p id="153">仅考虑二分类问题, 假设训练样本集</p>
                </div>
                <div class="p1">
                    <p id="154"><i>S</i>={ (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) |<i>i</i>=1, 2, …, <i>m</i>}⊆R<sup><i>n</i></sup>×{+1, -1}, </p>
                </div>
                <div class="p1">
                    <p id="155">其中 (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 是独立同分布的.</p>
                </div>
                <div class="p1">
                    <p id="156">考虑非光滑稀疏学习问题的损失函数为“hinge损失”, 即<i>f</i><sub><i>i</i></sub> (<b><i>w</i></b>) =max{0, 1-<i>y</i><sub><i>i</i></sub>&lt;<b><i>w</i></b>, <b><i>x</i></b><sub><i>i</i></sub>&gt;}的优化目标函数为</p>
                </div>
                <div class="p1">
                    <p id="157"><mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>∈</mo><mi>Q</mi></mrow></munder><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>f</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo></mrow></math></mathml>. (14) </p>
                </div>
                <div class="p1">
                    <p id="159">约束情况下随机形式的Heavy-ball算法的迭代步骤自然可以表示为</p>
                </div>
                <div class="area_img" id="269">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_26900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="162">其中<i>i</i>为迭代到第<i>t</i>步时随机抽取的样本序号.</p>
                </div>
                <div class="p1">
                    <p id="163">与批处理形式不同的是, 随机优化方法的迭代步骤中的<image id="164" type="" href="images/JFYZ201908012_16400.jpg" display="inline" placement="inline"><alt></alt></image><i>f</i><sub><i>i</i></sub> (<b><i>w</i></b><sub><i>t</i></sub>) 是<i>f</i><sub><i>i</i></sub> (<b><i>w</i></b><sub><i>t</i></sub>) 在<i>t</i>处的次梯度.由于hinge损失函数的次梯度有多种计算方式, 这里我们采用文献<citation id="376" type="reference">[<a class="sup">26</a>]</citation>的方式进行计算, 即:</p>
                </div>
                <div class="p1">
                    <p id="165"><mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>A</mi><msubsup><mrow></mrow><mi>t</mi><mo>+</mo></msubsup></mrow></munder><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, (16) </p>
                </div>
                <div class="p1">
                    <p id="167">其中, <i>A</i><sub><i>t</i></sub>⊆<i>S</i>, <i>A</i><sup>+</sup><sub><i>t</i></sub>={ (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) ∈<i>A</i><sub><i>t</i></sub>:<i>y</i><sub><i>i</i></sub>&lt;<b><i>w</i></b>, <b><i>x</i></b><sub><i>i</i></sub>&gt;&lt;1}, 在实验中设置<mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>|</mo></mrow><mo>=</mo><mn>1</mn><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="169"><b>算法1</b>. 随机Heavy-ball算法.</p>
                </div>
                <div class="p1">
                    <p id="170">输入: 循环次数<i>t</i>;</p>
                </div>
                <div class="p1">
                    <p id="171">输出:<b><i>w</i></b><sub><i>t</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="172">① 初始化向量<b><i>w</i></b><sub>1</sub>=<b>0</b>;</p>
                </div>
                <div class="p1">
                    <p id="173">② For <i>k</i>=1 to <i>t</i></p>
                </div>
                <div class="p1">
                    <p id="174">③  Update<mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mi>k</mi><mrow><mi>k</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo>, </mo><mi>α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>2</mn><mo stretchy="false">) </mo><msqrt><mi>k</mi></msqrt></mrow></mfrac><mo>;</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="176">④  随机选取<i>i</i>∈{1, 2, …, <i>m</i>};</p>
                </div>
                <div class="p1">
                    <p id="177">⑤  由式 (16) 计算<image id="178" type="" href="images/JFYZ201908012_17800.jpg" display="inline" placement="inline"><alt></alt></image><i>f</i><sub><i>i</i></sub> (<b><i>w</i></b><sub><i>k</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="179">⑥  由式 (15) 计算<b><i>w</i></b><sub><i>k</i>+1</sub>;</p>
                </div>
                <div class="p1">
                    <p id="180">⑦ End For</p>
                </div>
                <div class="p1">
                    <p id="181">当样本点独立同分布时, 经过随机抽取样本方式计算得到的<image id="182" type="" href="images/JFYZ201908012_18200.jpg" display="inline" placement="inline"><alt></alt></image><i>f</i><sub><i>i</i></sub> (<b><i>w</i></b><sub><i>t</i></sub>) 就是<i>f</i> (<b><i>w</i></b>) 在<b><i>w</i></b><sub><i>t</i></sub>处次梯度的无偏估计.从算法1中可以看出, 随机形式的算法就是将批处理形式的目标函数梯度替换为其无偏估计.文献<citation id="377" type="reference">[<a class="sup">29</a>]</citation>给出了将批处理算法的收敛界转换为随机算法收敛界的技巧, 该技巧对定理1同样成立.与文献<citation id="378" type="reference">[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">29</a>]</citation>完全类似, 我们可以将定理1推广至随机形式得到定理2.</p>
                </div>
                <div class="p1">
                    <p id="183"><b>定理2</b>. 设<i>f</i> (<b><i>w</i></b>) 为一般凸函数, 取<mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mi>t</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></mfrac><mo>, </mo><mrow><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo></mrow><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo>+</mo><mn>2</mn><mo stretchy="false">) </mo><msqrt><mi>t</mi></msqrt></mrow></mfrac><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>由式 (15) 产生, 则:</p>
                </div>
                <div class="p1">
                    <p id="185" class="code-formula">
                        <mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mrow><mo>[</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>≤</mo><mfrac><mrow><msqrt><mi>t</mi></msqrt></mrow><mrow><mn>2</mn><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mo>|</mo><mi>F</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mn>2</mn><msqrt><mi>t</mi></msqrt><mo>-</mo><mn>1</mn></mrow><mrow><mn>2</mn><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mo>|</mo><mi>G</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>t</mi></mrow></mfrac><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="186">根据定理2, 随机Heavy-ball方法具有最优的个体收敛速率<mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml></p>
                </div>
                <h3 id="188" name="188" class="anchor-tag"><b>3 实  验</b></h3>
                <div class="p1">
                    <p id="189">本节对算法1的个体收敛速率及其稀疏性的理论分析进行实验验证.</p>
                </div>
                <h4 class="anchor-tag" id="190" name="190"><b>3.1 实验数据集和比较算法</b></h4>
                <div class="p1">
                    <p id="191">实验所采用的6个常用标准数据集, 分别为ijcnn1, covtype, a9a, CCAT, RCV1, astro-physic.数据集来源于LIBSVM网站<citation id="252" type="note"><link href="5" rel="footnote" /><sup>①</sup></citation>.表1给出了这6个数据集的详细描述.</p>
                </div>
                <div class="area_img" id="192">
                    <p class="img_tit"><b>表1 标准数据集描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Introduction of Standard Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="192" border="1"><tr><td><br />Datasets</td><td>Training<br />Samples</td><td>Test<br />Samples</td><td>Dimensions</td><td>Sparsity/%</td></tr><tr><td>ijcnn1</td><td>49 990</td><td>91 701</td><td>22</td><td>59.09</td></tr><tr><td><br />covtype</td><td>522 911</td><td>58 101</td><td>54</td><td>22.12</td></tr><tr><td><br />a9a</td><td>24 703</td><td>7 858</td><td>123</td><td>11.27</td></tr><tr><td><br />CCAT</td><td>23 149</td><td>781 265</td><td>47 236</td><td>0.16</td></tr><tr><td><br />RCV1</td><td>20 242</td><td>677 399</td><td>47 236</td><td>0.16</td></tr><tr><td><br />astro-physic</td><td>29 882</td><td>32 487</td><td>99 757</td><td>0.08</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="193">实验采用5种随机优化方法进行比较, 这些方法分别为平均形式输出的标准投影次梯度方法<citation id="382" type="reference"><link href="334" rel="bibliography" /><link href="338" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">27</a>]</sup></citation>、线性插值投影次梯度方法<citation id="379" type="reference"><link href="322" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、NAG方法<citation id="380" type="reference"><link href="324" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、平均形式输出的Heavy-ball方法<citation id="381" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>以及个体形式输出的Heavy-ball方法.从理论分析的角度来说, 这5种随机优化方法的收敛速率均达到了最优.但在稀疏性方面, 个体形式输出的Heavy-ball方法与NAG方法应该具有较好的表现, 而平均形式输出的Heavy-ball方法、线性插值投影次梯度方法与标准的投影次梯度方法的稀疏性应该较差.</p>
                </div>
                <h4 class="anchor-tag" id="194" name="194"><b>3.2 实验方法及结论</b></h4>
                <div class="p1">
                    <p id="195">为公平起见, 各算法在每个数据集上均运行10次并取平均值作为最后输出.投影次梯度算法及以平均形式输出的Heavy-ball算法步长为常数, 计算方法分别取自文献<citation id="384" type="reference">[<a class="sup">14</a>,<a class="sup">27</a>]</citation>, 即<mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow></math></mathml>、<mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><msqrt><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msqrt></mrow><mo>, </mo><mi>β</mi><mo>=</mo><mn>0</mn><mo>.</mo><mn>8</mn></mrow></math></mathml>, 迭代次数<i>t</i>=10 000.线性插值投影次梯度算法与NAG方法的步长均与迭代次数有关, 根据文献<citation id="385" type="reference">[<a class="sup">19</a>,<a class="sup">20</a>]</citation>分别取<mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>η</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><msqrt><mi>k</mi></msqrt></mrow></math></mathml>与<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mi>k</mi><msqrt><mi>k</mi></msqrt></mrow></mrow></math></mathml>.在本实验中, 我们调用SLEP工具箱的函数来实现投影的计算<citation id="383" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, 其中<b><i>P</i></b><sub><i>Q</i></sub>为<i>l</i><sub>1</sub>范数球{<b><i>w</i></b>:<mathml id="200"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>≤</mo><mi>z</mi><mo stretchy="false">}</mo></mrow></math></mathml>上的投影算子, 根据数据集的不同, <i>z</i>对应选取不同的值, 并且各算法均取相同的约束参数.</p>
                </div>
                <div class="p1">
                    <p id="201">图1为5种算法的收敛速率对比图, 其中纵坐标表示当前目标函数值与目标函数最优值之差.粉色实线与蓝色实线分别表示标准的投影次梯度方法与平均形式输出的Heavy-ball方法的收敛趋势, 青绿色虚线与红色虚线表示线性插值投影次梯度方法与NAG方法的收敛趋势, 绿色虚线则表示本文提出的以个体形式输出的Heavy-ball方法的收敛趋势.从图1可以看出, 5种算法在6个标准数据集上运行了约5 000步之后, 基本都达到10<sup>-2</sup>的精度, 可以说均表现出基本相同的收敛趋势, 这与理论分析的结果是吻合的.</p>
                </div>
                <div class="p1">
                    <p id="202">图2给出了5种算法在6个标准数据集上的稀疏性对比, 纵坐标表示各算法对应输出方式的稀疏度.稀疏性通过稀疏度来衡量, 稀疏度是指变量中非零向量所占的百分比, 所以稀疏度越高则稀疏性越差.从图2可以看出, 线性插值投影次梯度方法虽然以个体形式输出, 但稀疏性较差.而Heavy-ball方法与NAG方法个体解的稀疏度近乎相同, 且都明显低于以平均形式输出的投影次梯度方法及Heavy-ball方法.由此可知, Heavy-ball方法的个体输出较好的保留了个体收敛在稀疏性上独具的优势.</p>
                </div>
                <div class="p1">
                    <p id="203">另外, 从图2中还可以看出, 对于维数较低的前3个数据库, 个体解的稀疏性明显优于平均解, 基本接近数据集的稀疏度 (见表1所示) , 这充分说明个体解比平均解能更好地描述样本集的稀疏性.但个体解的稀疏度却存在着震荡现象, 这主要是由于算法的随机性和稀疏度的分母较小导致的.对维数较高的后3个数据集, 个体解同样可以描述数据集的稀疏度, 但稀疏度已经不再震荡, 与平均解一样平稳.</p>
                </div>
                <div class="area_img" id="204">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908012_204.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 收敛速率比较图" src="Detail/GetImg?filename=images/JFYZ201908012_204.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 收敛速率比较图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908012_204.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Comparison of convergence rate</p>

                </div>
                <div class="area_img" id="205">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908012_205.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 稀疏度比较图" src="Detail/GetImg?filename=images/JFYZ201908012_205.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 稀疏度比较图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908012_205.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Comparison of sparsity</p>

                </div>
                <h3 id="206" name="206" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="207">与其他优化方法相比, Heavy-ball型动量优化方法目前所知的主要优势是在目标函数强凸且二次可微的条件下获得的收敛速率是最快的.本文对非光滑条件下Heavy-ball型动量优化方法的收敛性进行了初步的研究, 证明了这种方法可以获得最优的个体收敛速率.众所周知, 在不改变算法的情况下, 梯度下降方法目前最好的个体收敛速率是Shamir和Zhang得到的与最优收敛速率差一个log因子的个体收敛速率<citation id="386" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.显然, 本文的结论表明Heavy-ball型动量技巧是对梯度下降法个体收敛速率的一种加速策略, 并且与NAG方法具有相同的性能.下一步我们将考虑Heavy-ball型动量优化方法在正则化和强凸条件下的最优个体收敛速率问题, 我们还会考虑在随机Heavy-ball型动量优化方法中引进方差减少技巧进一步提升实际收敛效果.</p>
                </div>
                <h3 id="270" name="270" class="anchor-tag">附录1．正文引理1证明．</h3>
                <div class="p1">
                    <p id="271">我们使用Zinkevich证明在线优化时采用的迭代技巧<citation id="387" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>进行整理：</p>
                </div>
                <div class="area_img" id="272">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_27200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="273">引理1得证．</p>
                </div>
                <div class="p1">
                    <p id="274">附录2．正文定理1证明．</p>
                </div>
                <div class="p1">
                    <p id="275">根据式（13）有：</p>
                </div>
                <div class="area_img" id="276">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_27600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="276">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_27601.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="277">将<image id="278" type="formula" href="images/JFYZ201908012_27800.jpg" display="inline" placement="inline"><alt></alt></image>代入：</p>
                </div>
                <div class="area_img" id="280">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_28000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="279">不等式左右两边同乘<image id="281" type="" href="images/JFYZ201908012_28100.jpg" display="inline" placement="inline"><alt></alt></image>:</p>
                </div>
                <div class="area_img" id="282">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_28200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="283">根据引理1得：</p>
                </div>
                <div class="area_img" id="284">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908012_28400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="285">定理1得证．</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="286">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimization methods for large-scale machine learning">

                                <b>[1]</b>Bottou L, Curtis F E, Nocedal J.Optimization methods for large-scale machine learning[J].SIAM Review, 2018, 60 (2) :223- 311
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Some methods of speeding up the convergence of iteration methods">

                                <b>[2]</b>Polyak B T.Some methods of speeding up the convergence of iteration methods[J].USSR Computational Mathematics and Mathematical Physics, 1964, 4 (5) :1- 17
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A method of solving a convex programming problem with convergence rate O (1/k2)">

                                <b>[3]</b>Nesterov Y.A method of solving a convex programming problem with convergence rate O (1/k2) [J].Soviet Mathematics Doklady, 1983, 27 (2) :372- 376
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the importance of initialization and momentum in deep learning">

                                <b>[4]</b>Sutskever I, Martens J, Dahl G E, et al.On the importance of initialization and momentum in deep learning[C] //Proc of the Int Conf on Machine Learning (ICML) .New York:ACM, 2013:1139- 1147
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Problem Complexity and Method Efficiency in optimization">

                                <b>[5]</b>Nemirovsky A S, Yudin D B.Problem Complexity and Method Efficiency in optimization[M].New York:Wiley-Interscience, 1983
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003749915&amp;v=MTM1NTRkdEZDM2xVTHJPSlZrPU5qN0Jhck80SHRIUHFJdE1iZW9LWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Tseng P.Approximation accuracy, gradient methods, and error bound for structured convex optimization[J].Mathematical Programming, 2010, 125 (2) :263- 295
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems">

                                <b>[7]</b>Beck A, Teboulle M.A fast iterative shrinkage-thresholding algorithm for linear inverse problems[J].SIAM Journal on Imaging Sciences, 2009, 2 (1) :183- 202
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerated gradient methods for stochastic optimization and online learning">

                                <b>[8]</b>Hu C, Kwok J T, Pan W.Accelerated gradient methods for stochastic optimization and online learning[C] //Proc of the 23rd Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2009:781- 789
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A universal catalyst for first-order optimization">

                                <b>[9]</b>Lin H, Mairal J, Harchaoui Z.A universal catalyst for first-order optimization[C] //Proc of the 29th Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2015:3384- 3392
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Katyusha:the first direct acceleration of stochastic gradient methods">

                                <b>[10]</b>Allen-Zhu Z.Katyusha:The first direct acceleration of stochastic gradient methods[J].The Journal of Machine Learning Research, 2017, 18 (1) :8194- 8244
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mixed optimization for smooth functions">

                                <b>[11]</b>Mahdavi M, Zhang L, Jin R.Mixed optimization for smooth functions[C] //Proc of the 27th Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2013:674- 682
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ASVRG:Accelerated proximal SVRG">

                                <b>[12]</b>Shang Fanhua, Jiao Licheng, Zhou Kaiwen, et al.ASVRG:Accelerated proximal SVRG[C] //Proc of the 10th Asian Conf on Machine Learning.Cambridge:JMLR, 2018:815- 830
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global convergence of the heavy-ball method for convex optimization">

                                <b>[13]</b>Ghadimi E, Feyzmahdavian H R, Johansson M.Global convergence of the heavy-ball method for convex optimization[C] //proc of the 14th European Control Conf.Piscataway, NJ:IEEE, 2015:310- 315
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization">

                                <b>[14]</b>Yang Tianbao, Lin Qihang, Li Zhe.Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization[EB/OL]. (2016-05-04) [2018-12-13].https://arxiv.org/abs/1604.03257
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization">

                                <b>[15]</b>Xiao Lin.Dual averaging methods for regularized stochastic learning and online optimization[J].Journal of Machine Learning Research, 2010, 11 (1) :2543- 2596
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Proximal Stochastic Gradient Method with Progressive Variance Reduction">

                                <b>[16]</b>Xiao Lin, Zhang Tong.A proximal stochastic gradient method with progressive variance reduction[J].SIAM Journal on Optimization, 2014, 24 (4) :2057- 2075
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Open problem:Is averaging needed for strongly convex stochastic gradient descent?">

                                <b>[17]</b>Shamir O.Open problem:Is averaging needed for strongly convex stochastic gradient descent?[C] //Proc of the 25th Conf on Learning Theory.New York:ACM, 2012:471- 473
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent for nonsmooth optimization:convergence results and optimal averaging schemes">

                                <b>[18]</b>Shamir O, Zhang Tong.Stochastic gradient descent for non-smooth optimization:Convergence results and optimal averaging schemes[C] //Proc of the 29th Int Conf on Machine Learning.New York:ACM, 2013:71- 79
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201703007&amp;v=MzE3MzN5dmdWYnJQTHl2U2RMRzRIOWJNckk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>Tao Wei, Pan Zhisong, Zhu Xiaohui, et al.The Optimal individual convergence rate for the projected subgradient method with linear interpolation operation[J].Journal of Computer Research and Development, 2017, 54 (3) :529- 536 (in Chinese) (陶蔚, 潘志松, 朱小辉, 等.线性插值投影次梯度方法的最优个体收敛速率[J].计算机研究与发展, 2017, 54 (3) :529- 536) 
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801010&amp;v=MDMyOTJGeXZnVmJyUEx6N0Jkckc0SDluTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>Tao Wei, Pan Zhisong, Chu Dejun, et al.The individual convergence of projected subgradient methods using the Nesterov’s step-size strategy[J].Chinese Journal of Computers, 2018, 41 (1) :164- 176 (in Chinese) (陶蔚, 潘志松, 储德军, 等.使用 Nesterov步长策略投影次梯度方法的个体收敛性[J].计算机学报, 2018, 41 (1) :164- 176) 
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Primal averaging:A new gradient evaluation step to attain the optimal individual convergence">

                                <b>[21]</b>Tao Wei, Pan Zhisong, Wu Gaowei, et al.Primal averaging:A new gradient evaluation step to attain the optimal individual convergence[J].IEEE Transactions on Cybernetics, 2018.DOI:10.1109/TCYB.2018.2874332
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online convex programming and generalized infinitesimal gradient ascent">

                                <b>[22]</b>Zinkevich M.Online convex programming and generalized infinitesimal gradient ascent[C] //Proc of the 20th Int Conf on Machine Learning.New York:ACM, 2003:928- 936
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient projections onto the L1-ball for learning in high dimensions">

                                <b>[23]</b>Duchi J, Shalev-Shwartz S, Singer Y, et al.Efficient projections onto the l1-ball for learning in high dimensions[C] //Proc of the 25th Int Conf on Machine learning.New York:ACM, 2008:272- 279
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient euclidean projections in linear time">

                                <b>[24]</b>Liu Jun, Ye Jieping.Efficient Euclidean projections in linear time[C] //Proc of the 26th Annual Int Conf on Machine Learning.New York:ACM, 2009:657- 664
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization">

                                <b>[25]</b>Agarwal A, Bartlett P L, Ravikumar P, et al.Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization[J].IEEE Transactions on Information Theory, 2012, 58 (5) :3235- 3249
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pegasos: Primal estimated sub-gradient solver for SVM">

                                <b>[26]</b>Shalev-Shwartz S, Singer Y, Srebro N, et al.Pegasos:Primal estimated sub-gradient solver for svm[J].Mathematical Programming, 2011, 127 (1) :3- 30
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introductory lectures on stochastic optimization">

                                <b>[27]</b>Duchi J C.Introductory lectures on stochastic optimization[EB/OL].2010 [2018-12-18].http://web.standford.edu/～jduchi
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLEP:Sparse learning with efficient projections">

                                <b>[28]</b>Liu Jun, Ji Shuiwang, Ye Jieping.SLEP:Sparse learning with efficient projections[EB/OL]. (2010-10-08) [2018-12-31].http://www.public.asu.edu/～jye02/Software/SLEP
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making gradient descent optimal for strongly convex stochastic optimization">

                                <b>[29]</b>Rakhlin A, Shamir O, Sridharan K.Making gradient descent optimal for strongly convex stochastic optimization[C] //Proc of the 29th Int Conf on Machine Learning.New York:ACM, 2012:449- 456
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="5" href="javascript:void(0)">
                            <b>1</b> https://www.csie.ntu.edu.tw/～cjlin/libsvm/
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908012&amp;v=Mjg2MDllUnJGeXZnVmJyUEx5dlNkTEc0SDlqTXA0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUwQ05iaTY2QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

