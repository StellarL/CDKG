<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243712002500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902016%26RESULT%3d1%26SIGN%3dABG4RUlgkeL90r8RJ37zZxGnblE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902016&amp;v=MDQ1ODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzNKTHl2U2RMRzRIOWpNclk5RVlvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#288" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;线性阵列计算结构及大规模矩阵乘算法&lt;/b&gt; "><b>1</b><b>线性阵列计算结构及大规模矩阵乘算法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#318" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;协处理器的设计&lt;/b&gt; "><b>2</b><b>协处理器的设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#319" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;线性阵列计算结构的优化&lt;/b&gt;"><b>2.1</b><b>线性阵列计算结构的优化</b></a></li>
                                                <li><a href="#324" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;存储访问调度&lt;/b&gt;"><b>2.2</b><b>存储访问调度</b></a></li>
                                                <li><a href="#350" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;系统结构&lt;/b&gt;"><b>2.3</b><b>系统结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#363" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;性能模型&lt;/b&gt; "><b>3</b><b>性能模型</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#381" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;实验与结果&lt;/b&gt; "><b>4</b><b>实验与结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#383" data-title="&lt;b&gt;4.1&lt;/b&gt;&lt;b&gt;双缓冲优化技术对矩阵乘协处理器性能的影响&lt;/b&gt;"><b>4.1</b><b>双缓冲优化技术对矩阵乘协处理器性能的影响</b></a></li>
                                                <li><a href="#388" data-title="&lt;b&gt;4.2&lt;/b&gt;&lt;b&gt;局部存储器深度对矩阵乘协处理器性能的影响&lt;/b&gt;"><b>4.2</b><b>局部存储器深度对矩阵乘协处理器性能的影响</b></a></li>
                                                <li><a href="#392" data-title="&lt;b&gt;4.3&lt;/b&gt;&lt;b&gt;矩阵规模对矩阵乘协处理器计算效率的影响&lt;/b&gt;"><b>4.3</b><b>矩阵规模对矩阵乘协处理器计算效率的影响</b></a></li>
                                                <li><a href="#397" data-title="&lt;b&gt;4.4&lt;/b&gt;&lt;b&gt;功能正确性验证与硬件实现开销的评估&lt;/b&gt;"><b>4.4</b><b>功能正确性验证与硬件实现开销的评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#405" data-title="&lt;b&gt;5&lt;/b&gt;&lt;b&gt;结束语&lt;/b&gt; "><b>5</b><b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#290" data-title="图1 线性阵列计算结构">图1 线性阵列计算结构</a></li>
                                                <li><a href="#293" data-title="图2 矩阵分块数据到阵列计算结构的映射">图2 矩阵分块数据到阵列计算结构的映射</a></li>
                                                <li><a href="#315" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;针对矩阵乘运算过程中数据传输的分析&lt;/b&gt;"><b>表1</b><b>针对矩阵乘运算过程中数据传输的分析</b></a></li>
                                                <li><a href="#321" data-title="图3 采用双缓冲优化技术的PE结构">图3 采用双缓冲优化技术的PE结构</a></li>
                                                <li><a href="#352" data-title="图4 基于矩阵乘协处理器计算系统的架构">图4 基于矩阵乘协处理器计算系统的架构</a></li>
                                                <li><a href="#361" data-title="图5 基于矩阵乘协处理器的计算过程">图5 基于矩阵乘协处理器的计算过程</a></li>
                                                <li><a href="#387" data-title="图6 双缓冲优化技术对矩阵乘协处理器性能的提升">图6 双缓冲优化技术对矩阵乘协处理器性能的提升</a></li>
                                                <li><a href="#389" data-title="图7 局部存储器深度对矩阵乘协处理器性能的影响">图7 局部存储器深度对矩阵乘协处理器性能的影响</a></li>
                                                <li><a href="#394" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;矩阵规模对矩阵乘协处理器性能和效率的影响&lt;/b&gt;"><b>表2</b><b>矩阵规模对矩阵乘协处理器性能和效率的影响</b></a></li>
                                                <li><a href="#401" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;逻辑综合的参数设置&lt;/b&gt;"><b>表3</b><b>逻辑综合的参数设置</b></a></li>
                                                <li><a href="#404" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;b&gt;协处理器与K40 GPGPU计算性能和硬件开销的对比&lt;/b&gt;"><b>表4</b><b>协处理器与K40 GPGPU计算性能和硬件开销的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="445">


                                    <a id="bibliography_1" title="Dongarra J J, Duff I S, Sorensen D C, et al.Numerical Linear Algebra on High-Performance Computers[M].Philadelphia, PA:SIAM, 1998" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Numerical Linear Algebra on High-Performance Computers">
                                        <b>[1]</b>
                                        Dongarra J J, Duff I S, Sorensen D C, et al.Numerical Linear Algebra on High-Performance Computers[M].Philadelphia, PA:SIAM, 1998
                                    </a>
                                </li>
                                <li id="447">


                                    <a id="bibliography_2" title="Dongarra J J, Luszczek P, Petitet A.The LINPACKbenchmark:Past, present and future[J].Concurrency and Computation:Practice and Experience, 2003, 15 (9) :803-820" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000150164&amp;v=MTk5ODJwRlplMExZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxWYjNBSWw4PU5pZmNhck80SHRITXJv&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Dongarra J J, Luszczek P, Petitet A.The LINPACKbenchmark:Past, present and future[J].Concurrency and Computation:Practice and Experience, 2003, 15 (9) :803-820
                                    </a>
                                </li>
                                <li id="449">


                                    <a id="bibliography_3" title="Wang Endong, Zhang Qing, Shen Bo, et al.HighPerformance Computing on the Intel/Xeon Phi TM[M].Berlin:Springer, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HighPerformance Computing on the Intel/Xeon Phi TM">
                                        <b>[3]</b>
                                        Wang Endong, Zhang Qing, Shen Bo, et al.HighPerformance Computing on the Intel/Xeon Phi TM[M].Berlin:Springer, 2014
                                    </a>
                                </li>
                                <li id="451">


                                    <a id="bibliography_4" title="IBM Corporation.Engineering and scientific subroutine library (ESSL) and parallel ESSL[EB/OL].[2017-08-04].https:www-03.ibm.com/systems/power/software/essl/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Engineering and scientific subroutine library (ESSL)and parallel ESSL">
                                        <b>[4]</b>
                                        IBM Corporation.Engineering and scientific subroutine library (ESSL) and parallel ESSL[EB/OL].[2017-08-04].https:www-03.ibm.com/systems/power/software/essl/
                                    </a>
                                </li>
                                <li id="453">


                                    <a id="bibliography_5" title="Barrachina S, Castillo M, Igual F, et al.Evaluation and tuning of the level 3CUBLAS for graphics processors[C]Proc of the 22nd IEEE Int Parallel&amp;amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2008:3103-3111" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluation and tuning of the level 3CUBLAS for graphics processors">
                                        <b>[5]</b>
                                        Barrachina S, Castillo M, Igual F, et al.Evaluation and tuning of the level 3CUBLAS for graphics processors[C]Proc of the 22nd IEEE Int Parallel&amp;amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2008:3103-3111
                                    </a>
                                </li>
                                <li id="455">


                                    <a id="bibliography_6" title="Heinecke A, Vaidyanathan K, Smelyanskiy M, et al.Design and implementation of the Linpack benchmark for single and multi-node systems based on Intel Xeon Phi coprocessor[C]Proc of the 27th IEEE Int Parallel&amp;amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2013:126-137" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design and implementation of the Linpack benchmark for single and multi-node systems based on Intel Xeon Phi coprocessor">
                                        <b>[6]</b>
                                        Heinecke A, Vaidyanathan K, Smelyanskiy M, et al.Design and implementation of the Linpack benchmark for single and multi-node systems based on Intel Xeon Phi coprocessor[C]Proc of the 27th IEEE Int Parallel&amp;amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2013:126-137
                                    </a>
                                </li>
                                <li id="457">


                                    <a id="bibliography_7" title="Oak Ridge National Laboratory.IBM Power8CPU overview[EB/OL].[2017-07-27].https:www.olcf.ornl.gov/wpcontent/uploads/2017/01/Summit/Dev_IBM-Power8-CPUs_Walkup.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=IBM Power8CPU overview">
                                        <b>[7]</b>
                                        Oak Ridge National Laboratory.IBM Power8CPU overview[EB/OL].[2017-07-27].https:www.olcf.ornl.gov/wpcontent/uploads/2017/01/Summit/Dev_IBM-Power8-CPUs_Walkup.pdf
                                    </a>
                                </li>
                                <li id="459">


                                    <a id="bibliography_8" title="Wang Shen, Qi Fengbing, Gu Hongfeng, et al.Linpack parallel performance model and its prediction[J].Computer Engineering, 2012, 38 (16) :81-84 (in Chinese) (王申, 漆锋滨, 谷洪峰, 等.Linpack并行性能模型及其预测[J].计算机工程, 2012, 38 (16) :81-84) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201216020&amp;v=MDY0MzFOcVk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzNKTHo3QmJiRzRIOVA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Wang Shen, Qi Fengbing, Gu Hongfeng, et al.Linpack parallel performance model and its prediction[J].Computer Engineering, 2012, 38 (16) :81-84 (in Chinese) (王申, 漆锋滨, 谷洪峰, 等.Linpack并行性能模型及其预测[J].计算机工程, 2012, 38 (16) :81-84) 
                                    </a>
                                </li>
                                <li id="461">


                                    <a id="bibliography_9" title="Nowatzki T, Gangadhan V, Sankaralingam K, et al.Pushing the limits of accelerator efficiency while retaining programmability[C]Proc of the 22nd Int Symp on High Performance Computer Architecture.Piscataway, NJ:IEEE, 2016:27-39" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pushing the limits of accelerator efficiency while retaining programmability">
                                        <b>[9]</b>
                                        Nowatzki T, Gangadhan V, Sankaralingam K, et al.Pushing the limits of accelerator efficiency while retaining programmability[C]Proc of the 22nd Int Symp on High Performance Computer Architecture.Piscataway, NJ:IEEE, 2016:27-39
                                    </a>
                                </li>
                                <li id="463">


                                    <a id="bibliography_10" title="Singapura S G, Panangadan A, Prasanna V K.Performance modeling of matrix multiplication on 3D memory integrated FPGA[C]Proc of the 29th IEEE Int Parallel&amp;amp;Distributed Processing Symp Workshops.Piscataway, NJ:IEEE, 2015:154-162" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance modeling of matrix multiplication on 3D memory integrated FPGA">
                                        <b>[10]</b>
                                        Singapura S G, Panangadan A, Prasanna V K.Performance modeling of matrix multiplication on 3D memory integrated FPGA[C]Proc of the 29th IEEE Int Parallel&amp;amp;Distributed Processing Symp Workshops.Piscataway, NJ:IEEE, 2015:154-162
                                    </a>
                                </li>
                                <li id="465">


                                    <a id="bibliography_11" title="Kumar V B Y, Joshi S, Patkar S B, et al.FPGA based high performance double-precision matrix multiplication[J].International Journal of Parallel Programming, 2010, 38 (3) :322-338" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110200827487&amp;v=MDk2NDRVYjdJSmx3Y2FCTT1OajdCYXJLOUg5RE1yWTlGYk9rSUNIUStvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Kumar V B Y, Joshi S, Patkar S B, et al.FPGA based high performance double-precision matrix multiplication[J].International Journal of Parallel Programming, 2010, 38 (3) :322-338
                                    </a>
                                </li>
                                <li id="467">


                                    <a id="bibliography_12" title="Wu Guiming.Parallel algorithms and architectures for matrix computations on FPGA[D].Changsha:National University of Defense and Technology, 2011 (in Chinese) (邬贵明.FPGA矩阵计算并行算法与结构[D].长沙:国防科学技术大学, 2011) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1011303506.nh&amp;v=MTI2NjRIN0M0SGRUTXFaRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVzczSlZGMjY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Wu Guiming.Parallel algorithms and architectures for matrix computations on FPGA[D].Changsha:National University of Defense and Technology, 2011 (in Chinese) (邬贵明.FPGA矩阵计算并行算法与结构[D].长沙:国防科学技术大学, 2011) 
                                    </a>
                                </li>
                                <li id="469">


                                    <a id="bibliography_13" title="Wu Guiming, Dou Yong, Wang Miao.High performance and memory efficient implementation of matrix multiplication on FPGAs[C]Proc of the 9th IEEE Int Conf on Field Programmable Technology.Piscataway, NJ:IEEE, 2010:134-137" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High performance and memory efficient implementation of matrix multiplication on FPGAs">
                                        <b>[13]</b>
                                        Wu Guiming, Dou Yong, Wang Miao.High performance and memory efficient implementation of matrix multiplication on FPGAs[C]Proc of the 9th IEEE Int Conf on Field Programmable Technology.Piscataway, NJ:IEEE, 2010:134-137
                                    </a>
                                </li>
                                <li id="471">


                                    <a id="bibliography_14" title="Zhou Leitao, Tao Yaodong, Liu Sheng, et al.Research on systolic multiplication and technology based on FPGA[J].Journal of Computer Science and Engineering, 2015, 37 (9) :1632-1636 (in Chinese) (周磊涛, 陶耀东, 刘生, 等.基于FPGA的Systolic乘法技术研究[J].计算机工程与科学, 2015, 37 (9) :1632-1636) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201509005&amp;v=MTg4NTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzNKTHo3QlpiRzRIOVRNcG85Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Zhou Leitao, Tao Yaodong, Liu Sheng, et al.Research on systolic multiplication and technology based on FPGA[J].Journal of Computer Science and Engineering, 2015, 37 (9) :1632-1636 (in Chinese) (周磊涛, 陶耀东, 刘生, 等.基于FPGA的Systolic乘法技术研究[J].计算机工程与科学, 2015, 37 (9) :1632-1636) 
                                    </a>
                                </li>
                                <li id="473">


                                    <a id="bibliography_15" title="Jovanovi, Milutinovi V.FPGA accelerator for floatingpoint matrix multiplication[J].IET Computers&amp;amp;Digital Techniques, 2012, 6 (4) :249-256" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FPGA accelerator for floating-point matrix multiplication">
                                        <b>[15]</b>
                                        Jovanovi, Milutinovi V.FPGA accelerator for floatingpoint matrix multiplication[J].IET Computers&amp;amp;Digital Techniques, 2012, 6 (4) :249-256
                                    </a>
                                </li>
                                <li id="475">


                                    <a id="bibliography_16" title="Lei Yuanwu, Chen Xiaowen, Peng Yuanxi.A high energy efficiency FFT accelerator on DSP chip[J].Journal of Computer Research and Development, 2016, 53 (7) :1438-1446 (in Chinese) (雷元武, 陈小文, 彭元喜.DSP芯片中的高能效FFT加速器[J].计算机研究与发展, 2016, 53 (7) :1438-1446) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201607002&amp;v=MDc4MTJxSTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blc3M0pMeXZTZExHNEg5Zk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Lei Yuanwu, Chen Xiaowen, Peng Yuanxi.A high energy efficiency FFT accelerator on DSP chip[J].Journal of Computer Research and Development, 2016, 53 (7) :1438-1446 (in Chinese) (雷元武, 陈小文, 彭元喜.DSP芯片中的高能效FFT加速器[J].计算机研究与发展, 2016, 53 (7) :1438-1446) 
                                    </a>
                                </li>
                                <li id="477">


                                    <a id="bibliography_17" title="Qian Lei, Zhao Jinming, Peng Dajia, et al.Energy-efficient fingerprint matching based on reconfigurable micro server[J].Journal of Computer Research and Development, 2016, 53 (7) :1425-1437 (in Chinese) (钱磊, 赵锦明, 彭达佳, 等.基于可重构微服务器的高能效指纹比对方法[J].计算机研究与发展, 2016, 53 (7) :1425-1437) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201607001&amp;v=MDU5MTdCdEdGckNVUkxPZVplVnZGeTduVzczSkx5dlNkTEc0SDlmTXFJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Qian Lei, Zhao Jinming, Peng Dajia, et al.Energy-efficient fingerprint matching based on reconfigurable micro server[J].Journal of Computer Research and Development, 2016, 53 (7) :1425-1437 (in Chinese) (钱磊, 赵锦明, 彭达佳, 等.基于可重构微服务器的高能效指纹比对方法[J].计算机研究与发展, 2016, 53 (7) :1425-1437) 
                                    </a>
                                </li>
                                <li id="479">


                                    <a id="bibliography_18" title="Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C]Proc of the 44th IEEE Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2017:1-12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In-datacenter performance analysis of a tensor processing unit">
                                        <b>[18]</b>
                                        Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C]Proc of the 44th IEEE Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2017:1-12
                                    </a>
                                </li>
                                <li id="481">


                                    <a id="bibliography_19" title="NVIDIA Corporation.Inside volta:The world’s most advanced data-center GPU[EB/OL].[2017-06-17].https:devblogs.nvidia.com/parallelforall/inside-volta/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inside volta:The world&amp;#39;&amp;#39;s most advanced data-center GPU">
                                        <b>[19]</b>
                                        NVIDIA Corporation.Inside volta:The world’s most advanced data-center GPU[EB/OL].[2017-06-17].https:devblogs.nvidia.com/parallelforall/inside-volta/
                                    </a>
                                </li>
                                <li id="483">


                                    <a id="bibliography_20" title="Sze V, Chen Y H, Suleiman, A, et al.Hardware for machine learning:Challenges and opportunities[C]Proc of the 30th IEEE Custom Integrated Circuits Conf.Piscataway, NJ:IEEE, 2017:299-306" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hardware for machine learning:Challenges and opportunities">
                                        <b>[20]</b>
                                        Sze V, Chen Y H, Suleiman, A, et al.Hardware for machine learning:Challenges and opportunities[C]Proc of the 30th IEEE Custom Integrated Circuits Conf.Piscataway, NJ:IEEE, 2017:299-306
                                    </a>
                                </li>
                                <li id="485">


                                    <a id="bibliography_21" title="Gupta S, Agrawal A, Gopalakrishnan K, et al.Deep learning with limited numerical precision[C]Proc of the32nd Int Conf on Machine Learning.New York:ACM, 2015:1737-1746" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning with limited numerical precision">
                                        <b>[21]</b>
                                        Gupta S, Agrawal A, Gopalakrishnan K, et al.Deep learning with limited numerical precision[C]Proc of the32nd Int Conf on Machine Learning.New York:ACM, 2015:1737-1746
                                    </a>
                                </li>
                                <li id="487">


                                    <a id="bibliography_22" title="JDJEC Organization.DDR3 SDRAM standard[EB/OL].[2017-06-18].https:www.jedec.org/standards-documents/docs/jesd-79-3d" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DDR3 SDRAM standard">
                                        <b>[22]</b>
                                        JDJEC Organization.DDR3 SDRAM standard[EB/OL].[2017-06-18].https:www.jedec.org/standards-documents/docs/jesd-79-3d
                                    </a>
                                </li>
                                <li id="489">


                                    <a id="bibliography_23" title="Lee Y, Waterman A, Avizienis R, et al.A 45nm 1.3GHz16.7double-precision GFLOPS/W RISC-V processor with vector accelerators[C]Proc of the 40th IEEE European Solid-State Circuit Conf.Piscataway, NJ:IEEE, 2014:199-202" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A 45nm 1.3GHz 16.7Double-Precision GFLOPS/W RISC-V Processor with Vector Accelerators">
                                        <b>[23]</b>
                                        Lee Y, Waterman A, Avizienis R, et al.A 45nm 1.3GHz16.7double-precision GFLOPS/W RISC-V processor with vector accelerators[C]Proc of the 40th IEEE European Solid-State Circuit Conf.Piscataway, NJ:IEEE, 2014:199-202
                                    </a>
                                </li>
                                <li id="491">


                                    <a id="bibliography_24" title="Overton M L.Numerical Computing with IEEE Floating Point Arithmetic[M].Philadelphia, PA:SIAM, 2001" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Numerical Computing with IEEE Floating Point Arithmetic">
                                        <b>[24]</b>
                                        Overton M L.Numerical Computing with IEEE Floating Point Arithmetic[M].Philadelphia, PA:SIAM, 2001
                                    </a>
                                </li>
                                <li id="493">


                                    <a id="bibliography_25" title="Barrett R F, Chan T, D’Azevedo E F, et al.Complex version of high performance computing LINPACK benchmark (HPL) [J].Concurrency&amp;amp;Computation Practice&amp;amp;Experience, 2010, 22 (5) :573-587" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000150606&amp;v=MDY1MzU0SHRITXJvcEZZdXNKWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU25sVmIzQUlsOD1OaWZjYXJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        Barrett R F, Chan T, D’Azevedo E F, et al.Complex version of high performance computing LINPACK benchmark (HPL) [J].Concurrency&amp;amp;Computation Practice&amp;amp;Experience, 2010, 22 (5) :573-587
                                    </a>
                                </li>
                                <li id="495">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                    Zhang Xianyi, Wang Qian, Zhang Yunquan.OpenBLAS:Ahigh-performance BLAS library on Loongson 3A CPU[J].Journal of Software, 2011, 22 (2) :208-216 (in Chinese) (张先轶, 王茜, 张云泉.OpenBLAS:龙芯3A CPU的高性能BLAS库[J].软件学报, 2011, 22 (2) :208-216) </a>
                                </li>
                                <li id="497">


                                    <a id="bibliography_27" title="GitHub.OpenBLAS:An optimized BLAS library[EB/OL].[2017-08-04].https:github.com/xianyi/OpenBLAS" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OpenBLAS:An optimized BLAS library">
                                        <b>[27]</b>
                                        GitHub.OpenBLAS:An optimized BLAS library[EB/OL].[2017-08-04].https:github.com/xianyi/OpenBLAS
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),410-420 DOI:10.7544/issn1000-1239.2019.20170908            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>双精度浮点矩阵乘协处理器研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BE%E8%BF%85&amp;code=35723455&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贾迅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%AC%E8%B4%B5%E6%98%8E&amp;code=35723454&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邬贵明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E5%90%91%E8%BE%89&amp;code=35170847&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢向辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E4%B8%9C&amp;code=35352347&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴东</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%95%B0%E5%AD%A6%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%85%88%E8%BF%9B%E8%AE%A1%E7%AE%97%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数学工程与先进计算国家重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>矩阵乘运算在多个应用领域特别是数值计算领域被广泛使用, 但双精度浮点矩阵乘在CPU, GPGPU, FPGA等现有计算平台上的性能和效率受限, 其往往成为大规模数值计算应用的性能瓶颈.针对该问题, 以线性阵列计算结构为基础, 研究了双精度浮点矩阵乘的定制加速.首先, 对线性阵列计算结构进行了双缓冲优化并设计了针对双缓冲的存储访问调度, 以提高结构的计算效率.其次, 提出了矩阵乘协处理器和加速计算系统的结构, 构建了协处理器的性能模型并对其结构设计空间进行了探索.最后, 验证了协处理器的功能正确性并在某主流工艺下评估了其硬件开销.实验结果表明, 设计的双精度浮点矩阵乘协处理器可以达到3 TFLOPS的计算性能和99%的计算效率.与NVIDIA K40 GPGPU相比, 协处理器执行双精度浮点矩阵乘的性能是K40的1.95倍, 而面积开销仅为K40的21.05%.探索了定制加速结构设计在高性能计算中的应用, 对现有计算系统的性能提升具有一定的参考价值.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9F%A9%E9%98%B5%E4%B9%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">矩阵乘;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">协处理器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E9%80%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加速;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%AE%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浮点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A1%AC%E4%BB%B6%E5%AE%9A%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">硬件定制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    贾迅 jia.xun@meac-skl.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-11-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (91430214, 61732018);</span>
                    </p>
            </div>
                    <h1><b>A Coprocessor for Double-Precision Floating-Point Matrix Multiplication</b></h1>
                    <h2>
                    <span>Jia Xun</span>
                    <span>Wu Guiming</span>
                    <span>Xie Xianghui</span>
                    <span>Wu Dong</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory of Mathematical Engineering and Advanced Computing</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Matrix multiplication has been widely used in various application fields, especially the field of numerical computation. However, double-precision floating-point matrix multiplication suffers from non-optimal performance or efficiency on contemporary computing platforms, including CPU, GPGPU and FPGA. To address this problem, acceleration of double-precision floating-point matrix multiplication with a customized coprocessor is proposed in this paper, which adopts linear array as the basic building block. Firstly, double-buffering technique and optimized memory scheduling are applied to the basic linear array for better computation efficiency. Then, architecture of the matrix multiplication coprocessor and coprocessor-based accelerated computing system are formulated. Furthermore, a performance model tailored for the coprocessor is developed and the design space of coprocessor is explored in detail. Finally, functional correctness of the coprocessor is verified and its hardware implementation cost under mainstream technology node is evaluated. Experimental results show that the proposed coprocessor can achieve the performance of 3 TFLOPS and the efficiency of 99%. Compared with NVIDIA K40 GPGPU for executing double-precision floating-point matrix multiplication, the coprocessor proposed in this paper achieves 1.95× performance with hardware overheads of only 21.05% in area. This work explores the application of customized acceleration in high-performance computing and has certain guidance for improving performance of existing computing systems.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=matrix%20multiplication&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">matrix multiplication;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=coprocessor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">coprocessor;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=acceleration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">acceleration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=floating-point&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">floating-point;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hardware%20customization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hardware customization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Jia Xun, born in 1989.PhD candidate.Student member of CCF.His main research interests include high performance computing and processor micro-architecture.<image id="438" type="formula" href="images/JFYZ201902016_43800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wu Guiming, born in 1981.PhD, engineer.His main research interests include high performance computing and processor micro-architecture. (wu.guiming@meac-skl.cn) <image id="440" type="formula" href="images/JFYZ201902016_44000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Xie Xianghui, born in 1958.PhDsupervisor and professor.Senior member of CCF.His main research interests include computer architecture and parallel computing. (xie.xianghui@meac-skl.cn) <image id="443" type="formula" href="images/JFYZ201902016_44300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wu Dong, born in 1972.PhD and senior engineer.His main research interests include high performance computer architecture and reconfigurable computing. (wu.dong@meac-skl.cn) <image id="444" type="formula" href="images/JFYZ201902016_44400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-11-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (91430214, 61732018);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="281">矩阵乘运算核心广泛应用于数值计算、数字信号分析、图像处理、人工智能等领域, 其计算形式为</p>
                </div>
                <div class="p1">
                    <p id="282" class="code-formula">
                        <mathml id="282"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Κ</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>C</mi></mstyle></mrow></mstyle></mrow></mstyle><mo stretchy="false">[</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">]</mo><mo>+</mo><mo>=</mo><mi>A</mi><mo stretchy="false">[</mo><mi>i</mi><mo>, </mo><mi>k</mi><mo stretchy="false">]</mo><mo>×</mo><mi>B</mi><mo stretchy="false">[</mo><mi>k</mi><mo>, </mo><mi>j</mi><mo stretchy="false">]</mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="283">其中, <b><i>A</i></b>∈R<sup><i>M</i>×<i>K</i></sup>, <b><i>B</i></b>∈R<sup><i>K</i>×<i>N</i></sup>, <b><i>C</i></b>∈R<sup><i>M</i>×<i>N</i></sup>.循环<i>k</i>的计算存在写后读数据相关, 无法并行;循环<i>i</i>和循环<i>j</i>的计算不存在真相关, 可以并行.另外, 计算过程中的存储访问均比较规整.由于计算复杂度为<i>O</i> (<i>n</i><sup>3</sup>) , 双精度浮点矩阵乘往往是大规模数值计算应用的性能瓶颈<sup><a class="sup">[1]</a></sup>, 其性能直接反映了系统的计算能力.以性能测试程序Linpack<sup><a class="sup">[2]</a></sup>为例, 程序实现了基于双精度浮点矩阵乘的线性方程求解, 并以单位时间内系统执行浮点操作的次数 (FLOPS) 来衡量系统的计算能力.因此, 为双精度浮点矩阵乘运算提供高性能且高效的计算平台对大规模数值计算应用的性能发挥至关重要.</p>
                </div>
                <div class="p1">
                    <p id="284">目前, 矩阵乘的计算平台主要包括CPU, GPGPU, FPGA.其中, CPU和GPGPU平台上矩阵乘的实现和应用较为成熟.处理器厂商会提供针对各自产品架构进行性能优化的计算函数接口库, 如Intel MKL<sup><a class="sup">[3]</a></sup>, IBM ESSL<sup><a class="sup">[4]</a></sup>, NVIDIA cuBLAS<sup><a class="sup">[5]</a></sup>等.由于处理器架构面向通用计算, CPU和GPGPU上矩阵乘的计算效率无法达到最优, 如Intel Knights Corner众核处理器矩阵乘的计算效率仅为89%<sup><a class="sup">[6]</a></sup>, NVIDIA P100的计算效率约为93%<sup><a class="sup">[7]</a></sup>.矩阵乘效率对系统计算效率的影响几乎是线性的, 矩阵乘效率的损失必然导致系统计算效率的降低<sup><a class="sup">[8]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="285">FPGA计算平台具有灵活的可编程性、细粒度的并行能力和丰富的逻辑资源.同时, 矩阵乘运算数据并行性好和存储访问规整的算法特点适合并行计算结构的设计<sup><a class="sup">[9]</a></sup>, 因此基于FPGA平台实现针对矩阵乘的并行计算结构成为学术研究的热点, 并且已经取得一些研究成果<sup><a class="sup">[10]</a></sup>.以浮点乘加运算部件作为计算单元 (processing elements, PE) 的核心, 多个计算单元组成广播<sup><a class="sup">[11]</a></sup>或阵列结构<sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup>进行加速计算是典型的实现方式.其中文献[12]提出的线性阵列计算结构可以处理任意规模的矩阵乘运算, 且数据传输和存储的效率较高.由于矩阵乘的性能受限于FPGA的可用的逻辑资源和时钟频率, 即使以资源利用和时钟频率为优化目标的结构设计, 其计算性能也只能达到203.1 GFLOPS<sup><a class="sup">[15]</a></sup> (252个PE, 时钟频率为403 MHz) .显然, 目前基于FPGA的矩阵乘无法满足大规模数值计算应用对计算能力的需求.</p>
                </div>
                <div class="p1">
                    <p id="286">近年来, 面向特定应用的硬件定制加速成为高性能、高效计算结构设计的一个重要趋势<sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup>.Google公开了TPU (tensor processing unit) 芯片的设计<sup><a class="sup">[18]</a></sup>, 其核心是整数矩阵乘脉动阵列计算结构.NVIDIA最新发布的V100 GPGPU<sup><a class="sup">[19]</a></sup>芯片中引入了称为“Tensor Core”的16 b, 32 b混合精度矩阵乘阵列计算结构, 单芯片集成了672个TensorCore, 可提供120 TFLOPS的计算能力.由于TPU和V100芯片中矩阵乘定制加速结构的设计面向深度学习类应用, 结构支持的计算精度较低<sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup>, 因而无法直接应用于对计算精度要求较高的数值计算领域.</p>
                </div>
                <div class="p1">
                    <p id="287">本文以线性阵列计算结构为基础, 对其进行结构优化, 并采用硬件定制的方法设计实现了支持双精度浮点运算的矩阵乘协处理器.同时, 本文建立了性能模型并深入分析了各结构设计参数对协处理器实际计算性能和效率的影响.此外, 本文还验证了矩阵乘协处理器的功能正确性并评估了其硬件实现的开销.本文探索了硬件定制结构设计在双精度浮点矩阵乘加速计算中的应用, 研究成果对提升现有计算系统的性能和效率有一定的借鉴意义.</p>
                </div>
                <h3 id="288" name="288" class="anchor-tag"><b>1</b><b>线性阵列计算结构及大规模矩阵乘算法</b></h3>
                <div class="p1">
                    <p id="289">线性阵列计算结构<sup><a class="sup">[12]</a></sup>如图1所示, 多个计算单元线性互连, 每个计算单元包含局部存储器<i>c</i>以存储矩阵<b><i>C</i></b>的分块数据, 寄存器<i>a</i>和<i>b</i>分别存储矩阵<b><i>A</i></b>和矩阵<b><i>B</i></b>的数据元素, 一个双精度浮点乘加运算部件和一个计数器<i>CNT</i>.</p>
                </div>
                <div class="area_img" id="290">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_290.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 线性阵列计算结构" src="Detail/GetImg?filename=images/JFYZ201902016_290.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 线性阵列计算结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_290.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Linear array computing architecture</p>

                </div>
                <div class="p1">
                    <p id="291">数据<i>b</i>在PE之间进行传输, 其有效时驱动PE执行一次乘加计算:<i>c</i>[<i>CNT</i>]=<i>c</i>[<i>CNT</i>]+<i>a</i>×<i>b</i>.</p>
                </div>
                <div class="p1">
                    <p id="292">算法1给出了基于上述线性阵列计算结构的大规模分块矩阵乘算法<sup><a class="sup">[12]</a></sup>.参数<i>S</i><sub><i>p</i></sub>, <i>S</i><sub><i>t</i></sub>分别是矩阵<b><i>C</i></b>行数<i>M</i>和列数<i>N</i>的分块粒度, 同时也表示线性阵列计算结构中PE的数目和各PE集成的局部存储器<i>c</i>的深度.算法中索引为<i>p</i>和<i>t</i>的最内层循环对应线性阵列计算结构执行的浮点矩乘迭代计算, 即<i>S</i><sub><i>p</i></sub>个PE实现矩阵<b><i>A</i></b>中规模为<i>S</i><sub><i>p</i></sub>×1的矩阵子块与矩阵<b><i>B</i></b>中规模为1×<i>S</i><sub><i>t</i></sub>的矩阵子块乘计算, 并将计算结果与矩阵<b><i>C</i></b>中规模为<i>S</i><sub><i>p</i></sub>×<i>S</i><sub><i>t</i></sub>的子块相加.阵列计算结构在单次迭代计算中共执行2<i>S</i><sub><i>p</i></sub><i>S</i><sub><i>t</i></sub>次双精度浮点运算.</p>
                </div>
                <div class="area_img" id="293">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_293.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 矩阵分块数据到阵列计算结构的映射" src="Detail/GetImg?filename=images/JFYZ201902016_293.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 矩阵分块数据到阵列计算结构的映射  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_293.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Data block mapping for the linear array</p>

                </div>
                <div class="p1">
                    <p id="294"><b>算法1</b>. 基于线性阵列结构的大规模矩阵乘算法.</p>
                </div>
                <div class="p1">
                    <p id="295">输入:双精度浮点矩阵<b><i>A</i></b>、矩阵<b><i>B</i></b>、矩阵<b><i>C</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="296">输出:双精度浮点矩阵<b><i>C</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="297">① For <i>T</i><sub><i>p</i></sub>=0 to <i>M</i>-1, <i>S</i><sub><i>p</i></sub></p>
                </div>
                <div class="p1">
                    <p id="298">② For <i>T</i><sub><i>t</i></sub>=0 to <i>N</i>-1, <i>S</i><sub><i>t</i></sub></p>
                </div>
                <div class="p1">
                    <p id="299">③ Load <i>C</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>T</i><sub><i>t</i></sub>:<i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1];</p>
                </div>
                <div class="p1">
                    <p id="300">④ For <i>k</i>=0 to <i>K</i>-1</p>
                </div>
                <div class="p1">
                    <p id="301">⑤ Load <i>A</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>k</i>];</p>
                </div>
                <div class="p1">
                    <p id="302">⑥ Load <i>B</i>[<i>k</i>, <i>T</i><sub><i>t</i></sub>:<i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1];</p>
                </div>
                <div class="p1">
                    <p id="303">⑦ For <i>p</i>=<i>T</i><sub><i>p</i></sub> to <i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1</p>
                </div>
                <div class="p1">
                    <p id="304">⑧ For <i>t</i>=<i>T</i><sub><i>t</i></sub> to <i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1</p>
                </div>
                <div class="p1">
                    <p id="305">⑨ <i>C</i>[<i>p</i>, <i>t</i>]+=<i>A</i>[<i>p</i>, <i>k</i>]×<i>B</i>[<i>k</i>, <i>t</i>];</p>
                </div>
                <div class="p1">
                    <p id="306">⑩ End For</p>
                </div>
                <div class="p1">
                    <p id="307"> (11) End For</p>
                </div>
                <div class="p1">
                    <p id="308"> (12) End For</p>
                </div>
                <div class="p1">
                    <p id="309"> (13) Store <i>C</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>T</i><sub><i>t</i></sub>:<i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1];</p>
                </div>
                <div class="p1">
                    <p id="310"> (14) End For</p>
                </div>
                <div class="p1">
                    <p id="311"> (15) End For</p>
                </div>
                <div class="p1">
                    <p id="312">算法1中索引为<i>T</i><sub><i>p</i></sub>, <i>T</i><sub><i>t</i></sub>, <i>k</i>的3层循环在线性阵列计算结构进行矩阵乘迭代计算的同时, 处理矩阵分块数据的加载与写回, 其功能在硬件上由数据传输控制部件DTC (data transfer controller) 实现.各矩阵分块数据到线性阵列计算结构的映射如图2所示:</p>
                </div>
                <div class="p1">
                    <p id="313">线性阵列结构经过<i>K</i>轮迭代计算才能得到矩阵<b><i>C</i></b>中一个分块数据的计算结果.因此, 在<i>K</i>轮迭代计算开始前, 数据传输控制部件需要先将矩阵<b><i>C</i></b>中规模为<i>S</i><sub><i>p</i></sub>×<i>S</i><sub><i>t</i></sub>的分块数据<i>C</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>T</i><sub><i>t</i></sub>:<i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1]分别加载到<i>S</i><sub><i>p</i></sub>个计算单元的深度为<i>S</i><sub><i>t</i></sub>的局部存储器<i>c</i>中, 并在<i>K</i>轮迭代计算完成后将对应的分块计算结果写回主存.另外, 每次迭代计算中各PE需要访问矩阵<b><i>A</i></b>中相同的数据元素.因此, 每次迭代计算开始前, 数据传输控制部件还需要将矩阵<b><i>A</i></b>中规模为<i>S</i><sub><i>p</i></sub>×1的分块数据<i>A</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>k</i>]加载到<i>S</i><sub><i>p</i></sub>个计算单元的寄存器<i>a</i>中.</p>
                </div>
                <div class="p1">
                    <p id="314">矩阵<b><i>A</i></b>和矩阵<b><i>C</i></b>的分块数据全部加载完成后, 数据传输控制部件再加载矩阵<b><i>B</i></b>的数据元素以驱动阵列计算结构进行迭代计算.矩阵乘运算执行完成共需<i>K</i>× (<i>M</i>/<i>S</i><sub><i>p</i></sub>) × (<i>N</i>/<i>S</i><sub><i>t</i></sub>) 次迭代计算.对计算过程中数据传输控制部件传输矩阵<b><i>A</i></b>, <b><i>B</i></b>, <b><i>C</i></b>分块数据的数据量、时间间隔以及次数的量化分析如表1所示:</p>
                </div>
                <div class="area_img" id="315">
                    <p class="img_tit"><b>表1</b><b>针对矩阵乘运算过程中数据传输的分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1</b><b>Analysis on Data Transfer for Matrix Multiplication</b></p>
                    <p class="img_note"></p>
                    <table id="315" border="1"><tr><td><br />Matrix</td><td>Amount of Data</td><td>Interval</td><td>Number of Transfer</td></tr><tr><td><br /><b><i>A</i></b></td><td><i>S</i><sub><i>p</i></sub></td><td><i>S</i><sub><i>t</i></sub></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo>×</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac></mrow></math></td></tr><tr><td><br /><b><i>B</i></b></td><td>1</td><td>1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo>×</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>×</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac></mrow></math></td></tr><tr><td><br /><b><i>C</i></b></td><td><i>S</i><sub><i>p</i></sub>×<i>S</i><sub><i>t</i></sub></td><td><i>K</i>×<i>S</i><sub><i>t</i></sub></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mo>×</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="316">文献[12]基于Xilinx公司Virtex-5系列的FPGA实现了上述线性阵列计算结构.PE个数<i>S</i><sub><i>p</i></sub>设置为96、局部存储器深度<i>S</i><sub><i>t</i></sub>设置为1 024的情况下, FPGA的最大运行频率为158.08 MHz.输入矩阵的规模<i>M</i>, <i>N</i>, <i>K</i>均为8 192时, 阵列计算结构完成双精度浮点矩阵乘运算所需的时间为40.48 s, 即结构的计算性能为27.16 GFLOPS, 效率为89.49%.</p>
                </div>
                <div class="p1">
                    <p id="317">线性阵列计算结构虽然存储效率较高, 但每次迭代计算中各PE需要等待计算所需的矩阵分块数据全部加载完成后才能开始计算, 计算与存储访问在时间上是串行的, 计算资源的闲置必然导致计算效率的损失, 高效计算结构的设计必须对此进行改进.由于结构设计面向FPGA计算平台, 文献[12]仅以FPGA可用的逻辑资源作为设计约束, 并未考虑局部存储器的深度、输入矩阵的规模等参数对线性阵列计算结构实际计算性能和效率的影响, 而这些参数在定制协处理器的设计中尤为重要.另外, 本文协处理器的设计除了考虑核心的计算结构外, 还需要考虑其与主机端的接口, 以实现控制信息和计算数据的传输, 从而构成完整的加速计算系统.</p>
                </div>
                <h3 id="318" name="318" class="anchor-tag"><b>2</b><b>协处理器的设计</b></h3>
                <h4 class="anchor-tag" id="319" name="319"><b>2.1</b><b>线性阵列计算结构的优化</b></h4>
                <div class="p1">
                    <p id="320">根据算法1给出的大规模分块矩阵乘运算算法, 线性阵列计算结构在进行矩阵乘迭代计算前, 需要先从主存读入矩阵<b><i>A</i></b>和<b><i>C</i></b>的分块数据;<i>K</i>轮迭代计算完成后, 需要先将矩阵<b><i>C</i></b>的分块计算结果写回主存, 才能开始下一轮的迭代计算.矩阵分块数据在主存和线性阵列计算结构之间进行传输的过程中, 结构中各计算单元无法进行计算, 计算资源被闲置, 从而导致线性阵列计算结构整体的效率较低.针对这个问题, 本文采用双缓冲技术对阵列计算结构中各计算单元的设计进行了优化.优化后, 计算单元的结构如图3所示:</p>
                </div>
                <div class="area_img" id="321">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_321.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 采用双缓冲优化技术的PE结构" src="Detail/GetImg?filename=images/JFYZ201902016_321.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 采用双缓冲优化技术的PE结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_321.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structure of PE optimized with double buffering</p>

                </div>
                <div class="p1">
                    <p id="322">图3所示的PE结构中除了已有的寄存器<i>a</i>, <i>b</i>和局部存储器<i>c</i>之外, 还包括双缓冲优化引入的存储矩阵<b><i>A</i></b>, <b><i>B</i></b>数据元素的寄存器<i>a</i>_<i>buf</i>, <i>b</i>_<i>buf</i>以及存储矩阵<b><i>C</i></b>分块数据的局部存储器<i>c</i>_<i>buf</i>.</p>
                </div>
                <div class="p1">
                    <p id="323">应用双缓冲优化技术后, 阵列计算结构可以在使用当前缓冲中的数据进行迭代计算的同时, 提前将下一轮迭代计算所需的矩阵分块数据装入到另一缓冲, 实现计算与访存的重叠, 隐藏存储访问的开销.此时, 高效的存储访问调度成为应用双缓冲优化提升计算结构资源利用率的关键.</p>
                </div>
                <h4 class="anchor-tag" id="324" name="324"><b>2.2</b><b>存储访问调度</b></h4>
                <div class="p1">
                    <p id="325">由表1列出的矩阵乘运算过程中矩阵分块数据的传输可知, 线性阵列计算结构在进行矩阵乘运算时, 每个时钟周期仅需访问矩阵<b><i>B</i></b>的一个数据元素, 其对存储带宽的需求较低;而矩阵<b><i>A</i></b>, <b><i>C</i></b>的访问时间间隔分别为<i>S</i><sub><i>t</i></sub>和<i>K</i>×<i>S</i><sub><i>t</i></sub>个时钟周期, 因此可以利用读取矩阵<b><i>B</i></b>数据元素剩余的存储访问带宽提前加载矩阵<b><i>A</i></b>, <b><i>C</i></b>的分块数据至PE的缓冲中, 或将矩阵<b><i>C</i></b>的分块计算结果从缓冲写回主存.</p>
                </div>
                <div class="p1">
                    <p id="326">算法2给出了基于双缓冲的存储访问调度算法.数据传输控制部件在硬件上实现了该算法, 并支持双缓冲读写访问的控制.</p>
                </div>
                <div class="p1">
                    <p id="327"><b>算法2</b>. 基于双缓冲的存储访问调度算法.</p>
                </div>
                <div class="p1">
                    <p id="328">输入:双精度浮点矩阵<b><i>A</i></b>、矩阵<b><i>B</i></b>、矩阵<b><i>C</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="329">输出:双精度浮点矩阵<b><i>C</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="330">① Load <i>C</i>[0:<i>S</i><sub><i>p</i></sub>-1, 0:<i>S</i><sub><i>t</i></sub>-1];</p>
                </div>
                <div class="p1">
                    <p id="331">② Load <i>A</i>[0:<i>S</i><sub><i>p</i></sub>-1, 0];</p>
                </div>
                <div class="p1">
                    <p id="332">③ For <i>T</i><sub><i>p</i></sub>=0 to <i>M</i>-1, <i>S</i><sub><i>p</i></sub></p>
                </div>
                <div class="p1">
                    <p id="333">④ For <i>T</i><sub><i>t</i></sub>=0 to <i>N</i>-1, <i>S</i><sub><i>t</i></sub></p>
                </div>
                <div class="p1">
                    <p id="334">⑤ For <i>k</i>=0 to <i>K</i>-1</p>
                </div>
                <div class="p1">
                    <p id="335">⑥ Load <i>B</i>[<i>k</i>, <i>T</i><sub><i>t</i></sub>:<i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1];</p>
                </div>
                <div class="p1">
                    <p id="336">Load <i>A</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>k</i>+1] or</p>
                </div>
                <div class="p1">
                    <p id="337">Store <i>C</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>T</i><sub><i>t</i></sub>-<i>S</i><sub><i>t</i></sub>:<i>T</i><sub><i>t</i></sub>-1] or</p>
                </div>
                <div class="p1">
                    <p id="338">Load <i>C</i>[<i>T</i><sub><i>p</i></sub>:<i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1, <i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1:<i>T</i><sub><i>t</i></sub>+2<i>S</i><sub><i>t</i></sub>-1];</p>
                </div>
                <div class="p1">
                    <p id="339">⑦ For <i>p</i>=<i>T</i><sub><i>p</i></sub> to <i>T</i><sub><i>p</i></sub>+<i>S</i><sub><i>p</i></sub>-1</p>
                </div>
                <div class="p1">
                    <p id="340">⑧ For <i>t</i>=<i>T</i><sub><i>t</i></sub> to <i>T</i><sub><i>t</i></sub>+<i>S</i><sub><i>t</i></sub>-1</p>
                </div>
                <div class="p1">
                    <p id="341">⑨ <i>C</i>[<i>p</i>, <i>t</i>]+=<i>A</i>[<i>p</i>, <i>k</i>]×<i>B</i>[<i>k</i>, <i>t</i>];</p>
                </div>
                <div class="p1">
                    <p id="342">⑩ End For</p>
                </div>
                <div class="p1">
                    <p id="343"> (11) End For</p>
                </div>
                <div class="p1">
                    <p id="344"> (12) End For</p>
                </div>
                <div class="p1">
                    <p id="345"> (13) End For</p>
                </div>
                <div class="p1">
                    <p id="346"> (14) End For</p>
                </div>
                <div class="p1">
                    <p id="347"> (15) Store <i>C</i>[<i>M</i>-<i>S</i><sub><i>p</i></sub>:<i>M</i>-1, <i>N</i>-<i>T</i><sub><i>p</i></sub>:<i>N</i>-1].</p>
                </div>
                <div class="p1">
                    <p id="348">算法2中, 阵列计算结构进行第一次迭代计算所需的矩阵<b><i>A</i></b>, <b><i>C</i></b>的分块数据先被读入.读取矩阵<b><i>B</i></b>的数据元素进行迭代计算时, 优先利用剩余的存储访问带宽提前读取下一次迭代计算所需的矩阵<b><i>A</i></b>的分块数据;待矩阵<b><i>A</i></b>的分块数据读入完成, 再利用剩余带宽写回前<i>K</i>轮迭代计算得到的矩阵<b><i>C</i></b>的分块计算结果;待计算结果写回完成, 最后利用剩余带宽装入后<i>K</i>轮迭代计算所需的矩阵<b><i>C</i></b>的分块数据.所有迭代计算完成后, 矩阵<b><i>C</i></b>中最后一个分块计算结果被写回.</p>
                </div>
                <div class="p1">
                    <p id="349">理想情况下, 线性阵列计算结构在开始新一次的迭代计算时, 各PE所需的矩阵数据均已在缓冲中, 即存储访问的延迟可以有效地被计算隐藏, 结构中的计算资源可以得到充分利用.而实际的存储访问调度受限于存储带宽、计算单元数、局部存储器的深度以及矩阵规模等参数, 其对矩阵乘协处理器实际性能的影响将在2.3节进行分析.</p>
                </div>
                <h4 class="anchor-tag" id="350" name="350"><b>2.3</b><b>系统结构</b></h4>
                <div class="p1">
                    <p id="351">基于矩阵乘协处理器的加速计算系统的整体结构如图4所示.系统由主机端处理器芯片和双精度浮点矩阵乘协处理器芯片2部分组成.其中, 矩阵乘协处理器通过I/O总线与主机端处理器相连.系统采用PCIe-3.0×16作为芯片间的互连接口, 双向通信带宽最高为31.5 GBps.</p>
                </div>
                <div class="area_img" id="352">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_352.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于矩阵乘协处理器计算系统的架构" src="Detail/GetImg?filename=images/JFYZ201902016_352.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于矩阵乘协处理器计算系统的架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_352.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 System architecture based on matrix multiplication coprocessor</p>

                </div>
                <div class="p1">
                    <p id="353">主机端和协处理器端存储空间相互独立, 待计算的矩阵数据存储在协处理器端的内存.本文协处理器的设计面向大规模矩阵乘, 支持的矩阵规模至少为16 384×16 384.因此, 主机端与协处理器端的内存容量配置为8 GB, 并采用DDR3-2133<sup><a class="sup">[22]</a></sup>规格的存储芯片, 接口的数据宽度为128 b, 带宽为34.13 GBps.</p>
                </div>
                <div class="p1">
                    <p id="354">系统对主机端处理器的类型没有限制, 只要支持PCIe传输协议即可, 其指令集系统架构可以是x86, Power, ARM, RISC-V<sup><a class="sup">[23]</a></sup>等.矩阵乘协处理器由应用双缓冲优化的线性阵列计算结构、存储访问控制器、协处理器与主机端接口 (coprocessor-host interface, CHI) 3部分构成.乘加运算部件是计算单元的核心, 本文采用全流水且兼容IEEE-754浮点数标准<sup><a class="sup">[24]</a></sup>的乘加运算部件设计, 以保证协处理器的计算性能.CHI接口实现主机端与协处理器之间控制和数据的传输.主机端向矩阵乘协处理器发送的主要命令如下:</p>
                </div>
                <div class="p1">
                    <p id="355">1) <i>Initialization</i>对协处理器的控制与状态寄存器进行初始化;</p>
                </div>
                <div class="p1">
                    <p id="356">2) <i>DataIn</i>将待计算矩阵数据从主机端内存加载到协处理器端内存;</p>
                </div>
                <div class="p1">
                    <p id="357">3) <i>MatrixMultiply</i>启动协处理器上的双精度浮点矩阵乘运算;</p>
                </div>
                <div class="p1">
                    <p id="358">4) <i>DataOut</i>将矩阵乘运算的结果从协处理器端内存读出到主机端内存.</p>
                </div>
                <div class="p1">
                    <p id="359">接收到来自主机端发送的命令, CHI接口对命令进行分析并生成线性阵列计算结构相关的控制信号.协处理器在进行矩阵乘运算的过程中, 主机端与协处理器之间不需要进行数据传输, 从而简化了协处理器的设计与调试.</p>
                </div>
                <div class="p1">
                    <p id="360">应用程序在计算系统上加速矩阵乘运算的过程如图5所示.当矩阵乘协处理器连接至主机端时, 主机端发送<i>Initialization</i>命令对协处理器进行初始化, 如设置浮点计算的舍入模式、溢出控制等.应用程序运行在主机端, 当其调用双精度浮点矩阵乘运算核心, 如基础线性代数子程序库 (basic linear algebra subroutines, BLAS) 中的函数<i>blas</i>_<i>dgemm</i> (<i>M</i>, <i>N</i>, <i>K</i>, <b><i>A</i></b>, <b><i>B</i></b>, <b><i>C</i></b>, …) 时, 主机端通过CHI接口向协处理器发送<i>DataIn</i>命令和函数调用传入的矩阵规模、矩阵数据在内存中的起始地址等参数.协处理器的数据传输控制部件将待计算的矩阵数据从主机端内存以直接内存访问 (direct memory access, DMA) 方式传输至协处理器端内存.数据传输完成后, 主机端发送<i>MatrixMultiply</i>命令以启动协处理器上的矩阵乘运算.运算过程中, 数据传输控制部件按算法2设计的存储访问调度实现矩阵分块数据在协处理器端内存和线性阵列计算结构之间的传输.</p>
                </div>
                <div class="area_img" id="361">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_361.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于矩阵乘协处理器的计算过程" src="Detail/GetImg?filename=images/JFYZ201902016_361.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于矩阵乘协处理器的计算过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_361.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Computation based on matrix multiplication coprocessor</p>

                </div>
                <div class="p1">
                    <p id="362">协处理器完成矩阵乘运算后, CHI接口向主机端发出中断信号.主机端收到中断后, 向CHI接口发送<i>DataOut</i>命令, 协处理器的数据传输控制部件以DMA的方式将矩阵乘运算的结果从协处理器端内存传输回主机端内存.运算结果传输完成后, 矩阵乘运算调用结束, 应用程序继续执行.</p>
                </div>
                <h3 id="363" name="363" class="anchor-tag"><b>3</b><b>性能模型</b></h3>
                <div class="p1">
                    <p id="364">为了量化分析双精度浮点矩阵乘协处理器的各结构设计参数对其实际计算性能和效率的影响, 本文基于算法2设计的存储访问调度算法构建了协处理器的性能模型.模型中的基本参数包括线性阵列计算结构中PE的个数<i>S</i><sub><i>p</i></sub>、PE集成的局部存储器的深度<i>S</i><sub><i>t</i></sub>以及输入矩阵的规模<i>M</i>, <i>N</i>, <i>K</i>.此外, 本文将模型中协处理器的时钟频率记为<i>Freq</i> (单位为GHz) , 协处理器端内存的访问带宽记为<i>Band</i> (单位为GBps) , 矩阵乘运算核心在协处理器上执行完成所需的时间记为<i>T</i> (单位为s) , 协处理器的实际计算性能和效率记为<i>Perf</i> (单位为GFLOPS) 和<i>Eff</i>.</p>
                </div>
                <div class="p1">
                    <p id="365">当计算过程中的存储访问延迟完全被计算隐藏, 协处理器的计算性能可以充分发挥, 此时的计算效率最高.就本文提出的存储访问调度算法, 每次迭代计算提前读入矩阵<b><i>A</i></b>中规模为<i>S</i><sub><i>p</i></sub>的分块数据的延迟与读入矩阵<b><i>B</i></b>中规模为<i>S</i><sub><i>t</i></sub>的分块数据的延迟之和应当小于等于对应的计算延迟, 即<i>S</i><sub><i>t</i></sub>个时钟周期.同理, <i>K</i>轮迭代计算的总存储访问延迟应当小于等于对应的总计算延迟, 即<i>K</i>×<i>S</i><sub><i>t</i></sub>个时钟周期.存储访问的数据粒度为8 B的双精度浮点数, 上述2个约束条件可表示为</p>
                </div>
                <div class="p1">
                    <p id="366" class="code-formula">
                        <mathml id="366"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>8</mn><mo>×</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>B</mi><mi>a</mi><mi>n</mi><mi>d</mi></mrow></mfrac><mo>≤</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mn>8</mn><mo>×</mo><mfrac><mrow><mi>Κ</mi><mo>×</mo><mrow><mo> (</mo><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>+</mo><mn>2</mn><mo>×</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mo>×</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>B</mi><mi>a</mi><mi>n</mi><mi>d</mi></mrow></mfrac><mo>≤</mo><mfrac><mrow><mi>Κ</mi><mo>×</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow></mfrac></mtd></mtr></mtable></mrow><mo>⇒</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mn>2</mn><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mi>Κ</mi></mfrac><mo>≤</mo><mfrac><mrow><mi>B</mi><mi>a</mi><mi>n</mi><mi>d</mi></mrow><mrow><mn>8</mn><mo>×</mo><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow></mfrac><mo>-</mo><mn>1</mn><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="367">协处理器端DDR3-2133存储芯片的数据传输率为2.133 GT/s, 单次传输的数据宽度为128 b.若协处理器的工作时钟频率<i>Freq</i>=1.5 GHz, 则其每个时钟周期可访问2个双精度浮点数.此时, 式 (1) 的约束条件可进一步改写为</p>
                </div>
                <div class="p1">
                    <p id="368"><mathml id="369"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mn>2</mn><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mi>Κ</mi></mfrac><mo>≤</mo><mn>1</mn></mrow></math></mathml>. (2) </p>
                </div>
                <div class="p1">
                    <p id="370">若参数<i>S</i><sub><i>p</i></sub>, <i>S</i><sub><i>t</i></sub>, <i>K</i>可以满足式 (2) 中的约束条件, 则计算过程中的存储访问延迟可以完全被计算隐藏.此时, 矩阵乘运算核心在协处理器上的执行完成所需的时间<i>T</i>最小.其由3部分时间构成, 包括读入矩阵<b><i>A</i></b>和<b><i>C</i></b>第1个子块数据的时间、迭代计算的时间以及将矩阵<b><i>C</i></b>中最后1个子块的计算结果写回内存的时间.计算时间<i>T</i>可表示为</p>
                </div>
                <div class="p1">
                    <p id="371"><mathml id="372"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>×</mo><mi>Κ</mi><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></mfrac></mrow></math></mathml>. (3) </p>
                </div>
                <div class="p1">
                    <p id="373">若参数<i>S</i><sub><i>p</i></sub>, <i>S</i><sub><i>t</i></sub>, <i>K</i>不满足式 (2) 的约束条件, 存在<i>S</i><sub><i>t</i></sub>&lt;<i>S</i><sub><i>p</i></sub>和<i>S</i><sub><i>p</i></sub>&lt;<i>S</i><sub><i>t</i></sub>, 2<i>S</i><sub><i>p</i></sub>/<i>K</i>&gt;1-<i>S</i><sub><i>p</i></sub>/<i>S</i><sub><i>t</i></sub>这2种可能.下文将分别分析这2种情况下矩阵乘运算核心在协处理器上的执行时间.</p>
                </div>
                <div class="p1">
                    <p id="374">对于第1种情况, 提前读入矩阵<b><i>A</i></b>子块数据的存储访问延迟无法完全被计算时间隐藏, 每次迭代计算的时间增加 (<i>S</i><sub><i>p</i></sub>-<i>S</i><sub><i>t</i></sub>) /2个时钟周期.根据算法2, 当前迭代计算执行过程中, 写回前<i>K</i>轮迭代计算中矩阵<b><i>C</i></b>子块计算结果和提前读入后<i>K</i>轮迭代计算中矩阵<b><i>C</i></b>子块计算数据的存储访问延迟均无法被计算隐藏, 每<i>K</i>轮迭代计算的时间增加<i>S</i><sub><i>p</i></sub>×<i>S</i><sub><i>t</i></sub>个时钟周期.此时, 矩阵乘运算核心在协处理器上执行完成所需的时间<i>T</i>可表示为</p>
                </div>
                <div class="p1">
                    <p id="375" class="code-formula">
                        <mathml id="375"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mrow><mo> (</mo><mrow><mi>Κ</mi><mrow><mo> (</mo><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="376">对于第2种情况, 当前迭代计算的执行时间可以完全隐藏提前读入矩阵<b><i>A</i></b>子块数据的存储访问延迟, 但只能部分隐藏写回前<i>K</i>轮迭代计算中矩阵<b><i>C</i></b>子块计算结果和提前读入后<i>K</i>轮迭代计算中矩阵<b><i>C</i></b>子块计算数据的存储访问延迟, 其导致每<i>K</i>轮迭代计算的时间增加<i>S</i><sub><i>p</i></sub>×<i>S</i><sub><i>t</i></sub>-<i>K</i>× (<i>S</i><sub><i>t</i></sub>-<i>S</i><sub><i>p</i></sub>) /2个时钟周期.此时, 矩阵乘运算核心在协处理器上执行完成所需的时间<i>T</i>可表示为</p>
                </div>
                <div class="p1">
                    <p id="377" class="code-formula">
                        <mathml id="377"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mrow><mo> (</mo><mrow><mi>Κ</mi><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mfrac><mi>Κ</mi><mn>2</mn></mfrac><mrow><mo> (</mo><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="378">协处理器完成矩阵乘核心的加速计算共需执行2<i>MNK</i>次双精度浮点操作.基于对3种情况下矩阵乘运算核心在协处理器上执行时间<i>T</i>的分析结果, 可以构建式 (6) 所示的协处理器的性能模型.此时, 协处理器的计算效率可表示为实际计算性能与峰值计算性能的比值, 即<i>Eff</i>=<i>Perf</i>/ (2×<i>S</i><sub><i>p</i></sub>×<i>Freq</i>) .</p>
                </div>
                <div class="p1">
                    <p id="379" class="code-formula">
                        <mathml id="379"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mn>2</mn><mi>Μ</mi><mi>Ν</mi><mi>Κ</mi><mo>×</mo><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow><mrow><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>×</mo><mi>Κ</mi><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></mfrac></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mn>2</mn><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mi>Κ</mi></mfrac><mo>≤</mo><mn>1</mn><mo>;</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mn>2</mn><mi>Μ</mi><mi>Ν</mi><mi>Κ</mi><mo>×</mo><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow><mrow><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>×</mo><mrow><mo> (</mo><mrow><mi>Κ</mi><mrow><mo> (</mo><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>&lt;</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mo>;</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mn>2</mn><mi>Μ</mi><mi>Ν</mi><mi>Κ</mi><mo>×</mo><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow><mrow><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mi>Μ</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mo>×</mo><mfrac><mi>Ν</mi><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>×</mo><mrow><mo> (</mo><mrow><mi>Κ</mi><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mfrac><mi>Κ</mi><mn>2</mn></mfrac><mrow><mo> (</mo><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow></mrow></mfrac><mo>, </mo><mspace width="0.25em" /><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>&gt;</mo><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub><mtext>且</mtext><mfrac><mrow><mn>2</mn><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mi>Κ</mi></mfrac><mo>&gt;</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo>.</mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="380"> (6) </p>
                </div>
                <h3 id="381" name="381" class="anchor-tag"><b>4</b><b>实验与结果</b></h3>
                <div class="p1">
                    <p id="382">本节以第3节提出的性能模型为基础, 分析矩阵乘协处理器结构设计中双缓冲优化技术、局部存储器的深度和输入矩阵规模等参数对协处理器实际计算性能和效率的影响.实验假设线性阵列计算结构的PE数为1 024, 协处理器的时钟频率为1.5 GHz, 待计算的矩阵均为方阵, 即<i>M</i>=<i>N</i>=<i>K</i>=<i>n</i>.此外, 本节还验证了矩阵乘协处理器功能实现的正确性;在主流工艺下评估了其硬件实现的开销, 并与同等工艺的GPGPU芯片进行了对比, 以探索其硬件可实现性和在实际应用中的优势.</p>
                </div>
                <h4 class="anchor-tag" id="383" name="383"><b>4.1</b><b>双缓冲优化技术对矩阵乘协处理器性能的影响</b></h4>
                <div class="p1">
                    <p id="384">实验在不同的矩阵规模下, 分别统计了应用双缓冲优化技术前后矩阵乘协处理器的性能, 以分析应用双缓冲优化技术对协处理器性能的影响.实验中, 各PE局部存储器的深度均设置为2 048, 实验的结果如图6所示.</p>
                </div>
                <div class="p1">
                    <p id="385">从图6可以看出, 不同矩阵规模下, 双缓冲优化技术的应用均可以提升矩阵乘协处理器的计算性能, 协处理器的性能平均提升30.06%.矩阵规模为4 096时, 性能提升最为明显, 达到了45.45%.可见, 本文设计的存储访问调度算法可以在计算过程中利用双缓冲优化有效隐藏存储访问的延迟.</p>
                </div>
                <div class="p1">
                    <p id="386">同时, 双缓冲优化对矩阵乘协处理器的性能提升受矩阵规模的影响.对此分析如下:当矩阵规模较小时, 计算无法有效隐藏存储访问的延迟, 应用双缓冲优化对性能的提升作用有限;随着矩阵规模逐渐增加, 计算可以隐藏更多的访存延迟, 此时应用双缓冲优化可以显著提升协处理器的计算性能;当矩阵规模较大时, 计算可以很好隐藏存储访问的延迟, 而实验中局部存储器的深度保持不变, 双缓冲优化对性能的提升作用逐渐降低.</p>
                </div>
                <div class="area_img" id="387">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_387.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 双缓冲优化技术对矩阵乘协处理器性能的提升" src="Detail/GetImg?filename=images/JFYZ201902016_387.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 双缓冲优化技术对矩阵乘协处理器性能的提升  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_387.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Effect of double-buffering optimization technique on coprocessor performance</p>

                </div>
                <h4 class="anchor-tag" id="388" name="388"><b>4.2</b><b>局部存储器深度对矩阵乘协处理器性能的影响</b></h4>
                <div class="area_img" id="389">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902016_389.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 局部存储器深度对矩阵乘协处理器性能的影响" src="Detail/GetImg?filename=images/JFYZ201902016_389.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 局部存储器深度对矩阵乘协处理器性能的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902016_389.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Effect of local memory depth on coprocessor  performance</p>

                </div>
                <div class="p1">
                    <p id="390">局部存储器的深度直接影响协处理器硬件实现的开销, 因而是矩阵乘协处理器结构设计中一个重要的参数.实验在128, 1 024, 8 192, 16 384这4种矩阵规模下, 分别将计算单元局部存储器的深度<i>h</i>从128增加至8 192, 统计矩阵乘协处理器实际计算性能的变化情况, 实验结果如图7所示:</p>
                </div>
                <div class="p1">
                    <p id="391">从图7可以看出, 4种矩阵规模下, 矩阵乘协处理器的实际计算性能均随局部存储器深度的增加而提升, 局部存储器深度增加大于2 048时, 矩阵乘协处理器的性能基本保持不变.协处理器实际计算性能随局部存储器深度的变化可以根据式 (2) 中计算完全隐藏存储访问延迟的约束条件进行分析.当<i>S</i><sub><i>t</i></sub>&lt;<i>S</i><sub><i>p</i></sub>时, 提前加载矩阵<b><i>A</i></b>中子块数据的延迟无法完全被计算隐藏, 此时增加局部存储器的深度可以有效提升协处理器的计算性能.而当局部存储器的深度大于PE个数时, 协处理器的实际计算性能不再受局部存储器深度的影响, 因而其保持不变.基于上述实验结果和分析, 当协处理器线性阵列计算结构的PE数为1 024时, 局部存储器最优的深度为2 048.</p>
                </div>
                <h4 class="anchor-tag" id="392" name="392"><b>4.3</b><b>矩阵规模对矩阵乘协处理器计算效率的影响</b></h4>
                <div class="p1">
                    <p id="393">尽管本文矩阵乘协处理器的设计面向数值计算领域的大规模应用, 矩阵规模对协处理器实际计算性能和效率的影响仍然值得关注.实验在应用双缓冲优化、局部存储器深度设置为2 048时, 统计了不同矩阵规模下, 矩阵乘协处理器实际的计算性能和效率, 以及存储访问延迟占矩阵乘运算核心总执行时间的比例, 实验结果如表2所示:</p>
                </div>
                <div class="area_img" id="394">
                    <p class="img_tit"><b>表2</b><b>矩阵规模对矩阵乘协处理器性能和效率的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2</b><b>Effect of Matrix Size on Coprocessor Performance and Efficiency</b></p>
                    <p class="img_note"></p>
                    <table id="394" border="1"><tr><td><br /><i>n</i></td><td>Memory Ratio/%</td><td>Performance</td><td>Efficiency/%</td></tr><tr><td><br />128</td><td>50.00</td><td>191.97</td><td>6.25</td></tr><tr><td><br />256</td><td>50.00</td><td>383.98</td><td>12.50</td></tr><tr><td><br />512</td><td>50.00</td><td>767.99</td><td>25.00</td></tr><tr><td><br />1 024</td><td>27.27</td><td>1 535.99</td><td>50.00</td></tr><tr><td><br />2 048</td><td>3.03</td><td>2 234.28</td><td>72.73</td></tr><tr><td><br />4 096</td><td>0.04</td><td>2 978.93</td><td>96.97</td></tr><tr><td><br />8 192</td><td>0.00</td><td>3 060.04</td><td>99.61</td></tr><tr><td><br />16 384</td><td>0.00</td><td>3 070.50</td><td>99.97</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="395">从表2中的数据可以看出, 随着矩阵规模的增加, 矩阵乘协处理器的实际计算效率不断提高:矩阵规模为128时, 协处理器的计算效率仅为6.25%;矩阵规模增加至4 096时, 协处理器的计算效率达到96.97%;矩阵规模大于等于8 192时, 协处理器的计算效率可以达到99%.</p>
                </div>
                <div class="p1">
                    <p id="396">对协处理器计算效率与矩阵规模相关性的分析如下:当PE数<i>S</i><sub><i>p</i></sub>=1 024、局部存储器深度<i>S</i><sub><i>t</i></sub>=2 048时, 式 (3) 表示的计算完全隐藏访存延迟的约束条件是否满足仅取决于计算中的矩阵规模<i>K</i>.随着矩阵规模的增大, 计算可以更好地隐藏存储访问的延迟, 协处理器的计算效率也进一步提升.表2中存储访问延迟占矩阵乘总执行时间的比例验证了上述分析, 即较大的矩阵规模对应较低的存储访问延迟占比.</p>
                </div>
                <h4 class="anchor-tag" id="397" name="397"><b>4.4</b><b>功能正确性验证与硬件实现开销的评估</b></h4>
                <div class="p1">
                    <p id="398">本文基于Verilog硬件描述语言实现了双精度浮点矩阵乘协处理器.其中, 线性阵列计算结构中各计算单元采用双缓冲优化技术, PE数设置为1 024、局部存储器的深度设置为2 048.</p>
                </div>
                <div class="p1">
                    <p id="399">对于矩阵乘协处理器功能正确性的验证, 本文以Linpack性能测试程序HPL<sup><a class="sup">[25]</a></sup>随机生成的双精度浮点矩阵数据作为测试数据集, 通过调用OpenBLAS<sup>[<a class="sup">26</a>,<a class="sup">27</a>]</sup>线性代数计算函数库实现了双精度浮点矩阵乘运算核心.本文将OpenBLAS的计算结果与矩阵乘协处理器的仿真计算结果进行了比对, 从而验证了协处理器功能实现的正确性.</p>
                </div>
                <div class="p1">
                    <p id="400">对于矩阵乘协处理器硬件开销的评估, 本文基于Synopsys Design Compiler设计工具在1.5 GHz、主流工艺下对协处理器的硬件设计进行了逻辑综合, 综合过程中的参数配置如表3所示:</p>
                </div>
                <div class="area_img" id="401">
                    <p class="img_tit"><b>表3</b><b>逻辑综合的参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3</b><b>Configurations for Synopsys Design Compiler</b></p>
                    <p class="img_note"></p>
                    <table id="401" border="1"><tr><td><br />Parameters</td><td>Configuration</td></tr><tr><td><br />period</td><td>0.66</td></tr><tr><td><br /><i>setup</i></td><td>0.083</td></tr><tr><td><br /><i>hold</i></td><td>0.150</td></tr><tr><td><br /><i>route</i>_<i>type</i></td><td>Mesh</td></tr><tr><td><br /><i>max</i>_<i>layer</i></td><td>M4</td></tr><tr><td><br /><i>clk</i>_<i>uncertainty</i></td><td>0.06</td></tr><tr><td><br /><i>clk</i>_<i>transition</i></td><td>0.04</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="402">根据Design Compiler输出的综合结果, 矩阵乘协处理器的硬件设计完全满足1.5 GHz频率下的时序要求.其功耗为38.57 W, 总面积为116.26 mm<sup>2</sup>.其中, 组合逻辑的面积为16.78 mm<sup>2</sup>, 非组合逻辑的面积为99.48 mm<sup>2</sup>.</p>
                </div>
                <div class="p1">
                    <p id="403">本文将矩阵乘协处理器与采用同等工艺的NVIDIA K40 GPGPU<sup><a class="sup">[19]</a></sup>芯片进行了对比, 比较的内容包括时钟频率、峰值计算性能, 进行矩阵乘运算的实际性能和效率以及硬件开销等, 结果如表4所示.针对双精度浮点矩阵乘运算, 本文协处理器的实际性能是K40 GPGPU芯片的1.95倍, 而面积开销仅为后者的21.05% (不考虑GPGPU芯片中PCIe、图形显示和存储控制器等接口的硬件开销) .</p>
                </div>
                <div class="area_img" id="404">
                    <p class="img_tit"><b>表4</b><b>协处理器与K40 GPGPU计算性能和硬件开销的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4</b><b>Coprocessor versus K40 GPGPU on Performance and Hardware Overhead</b></p>
                    <p class="img_note"></p>
                    <table id="404" border="1"><tr><td><br />Design Parameters</td><td>K40</td><td>Coprocessor</td></tr><tr><td><br />Base Clock/MHz</td><td>745</td><td>1 500</td></tr><tr><td><br />Boost Clock/MHz</td><td>875</td><td></td></tr><tr><td><br />Peak Performance/GFLOPS</td><td>1 680</td><td>3 072</td></tr><tr><td><br />Matrix Multiplication Efficiency/%</td><td>93</td><td>99</td></tr><tr><td><br />Sustainable Performance</td><td>1 562</td><td>3 041</td></tr><tr><td><br />Area/mm<sup>2</sup></td><td>551</td><td>116</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="405" name="405" class="anchor-tag"><b>5</b><b>结束语</b></h3>
                <div class="p1">
                    <p id="406">矩阵乘运算广泛应用于科学与工程领域.因计算复杂度较高, 且在CPU, GPGPU, FPGA等现有计算平台上的性能和效率受限, 双精度浮点矩阵乘运算往往成为大规模数值计算应用的性能瓶颈.近年来, 面向应用的硬件定制成为高性能、高效计算结构设计的重要趋势.本文将这一结构设计方法应用到矩阵乘的加速计算中, 设计了双精度浮点矩阵乘定制协处理器和基于协处理器的加速计算系统.</p>
                </div>
                <div class="p1">
                    <p id="407">本文双精度浮点矩阵乘协处理器的设计以线性阵列计算结构为基础, 应用了双缓冲技术并对存储访问调度进行了优化.同时, 本文构建了针对协处理器的性能模型, 并基于性能模型深入分析了协处理器的各结构设计参数对其计算性能和效率的影响.此外, 本文还验证了矩阵乘协处理器功能实现的正确性, 并在主流工艺下评估了其硬件开销.本文双精度浮点矩阵乘协处理器的实际计算性能可达3 TFLOPS, 计算效率最高可达99%, 有效实现了大规模数值计算应用的加速.与同等工艺的GPGPU计算平台相比, 本文设计的矩阵乘协处理器在计算性能和硬件开销方面均具有明显优势.</p>
                </div>
                <div class="p1">
                    <p id="408">本文探索了硬件定制的结构设计在高性能计算中的应用.后续研究将基于BLAS库和Linpack性能测试程序评估本文双精度浮点矩阵乘协处理器的应用对大规模计算系统性能和效率的提升效果;同时分析协处理器实际性能随阵列计算结构中计算单元数增加的可扩展性, 从而进一步优化协处理器的结构设计.另外, 协处理器的应用适应性也是未来研究工作的重点.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="445">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Numerical Linear Algebra on High-Performance Computers">

                                <b>[1]</b>Dongarra J J, Duff I S, Sorensen D C, et al.Numerical Linear Algebra on High-Performance Computers[M].Philadelphia, PA:SIAM, 1998
                            </a>
                        </p>
                        <p id="447">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000150164&amp;v=MDYyOTU1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxWYjNBSWw4PU5pZmNhck80SHRITXJvcEZaZTBMWTNr&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Dongarra J J, Luszczek P, Petitet A.The LINPACKbenchmark:Past, present and future[J].Concurrency and Computation:Practice and Experience, 2003, 15 (9) :803-820
                            </a>
                        </p>
                        <p id="449">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HighPerformance Computing on the Intel/Xeon Phi TM">

                                <b>[3]</b>Wang Endong, Zhang Qing, Shen Bo, et al.HighPerformance Computing on the Intel/Xeon Phi TM[M].Berlin:Springer, 2014
                            </a>
                        </p>
                        <p id="451">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Engineering and scientific subroutine library (ESSL)and parallel ESSL">

                                <b>[4]</b>IBM Corporation.Engineering and scientific subroutine library (ESSL) and parallel ESSL[EB/OL].[2017-08-04].https:www-03.ibm.com/systems/power/software/essl/
                            </a>
                        </p>
                        <p id="453">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluation and tuning of the level 3CUBLAS for graphics processors">

                                <b>[5]</b>Barrachina S, Castillo M, Igual F, et al.Evaluation and tuning of the level 3CUBLAS for graphics processors[C]Proc of the 22nd IEEE Int Parallel&amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2008:3103-3111
                            </a>
                        </p>
                        <p id="455">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design and implementation of the Linpack benchmark for single and multi-node systems based on Intel Xeon Phi coprocessor">

                                <b>[6]</b>Heinecke A, Vaidyanathan K, Smelyanskiy M, et al.Design and implementation of the Linpack benchmark for single and multi-node systems based on Intel Xeon Phi coprocessor[C]Proc of the 27th IEEE Int Parallel&amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2013:126-137
                            </a>
                        </p>
                        <p id="457">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=IBM Power8CPU overview">

                                <b>[7]</b>Oak Ridge National Laboratory.IBM Power8CPU overview[EB/OL].[2017-07-27].https:www.olcf.ornl.gov/wpcontent/uploads/2017/01/Summit/Dev_IBM-Power8-CPUs_Walkup.pdf
                            </a>
                        </p>
                        <p id="459">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201216020&amp;v=MjY1NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzNKTHo3QmJiRzRIOVBOcVk5SFpJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Wang Shen, Qi Fengbing, Gu Hongfeng, et al.Linpack parallel performance model and its prediction[J].Computer Engineering, 2012, 38 (16) :81-84 (in Chinese) (王申, 漆锋滨, 谷洪峰, 等.Linpack并行性能模型及其预测[J].计算机工程, 2012, 38 (16) :81-84) 
                            </a>
                        </p>
                        <p id="461">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pushing the limits of accelerator efficiency while retaining programmability">

                                <b>[9]</b>Nowatzki T, Gangadhan V, Sankaralingam K, et al.Pushing the limits of accelerator efficiency while retaining programmability[C]Proc of the 22nd Int Symp on High Performance Computer Architecture.Piscataway, NJ:IEEE, 2016:27-39
                            </a>
                        </p>
                        <p id="463">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance modeling of matrix multiplication on 3D memory integrated FPGA">

                                <b>[10]</b>Singapura S G, Panangadan A, Prasanna V K.Performance modeling of matrix multiplication on 3D memory integrated FPGA[C]Proc of the 29th IEEE Int Parallel&amp;Distributed Processing Symp Workshops.Piscataway, NJ:IEEE, 2015:154-162
                            </a>
                        </p>
                        <p id="465">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110200827487&amp;v=MTQzNDFsd2NhQk09Tmo3QmFySzlIOURNclk5RmJPa0lDSFErb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Kumar V B Y, Joshi S, Patkar S B, et al.FPGA based high performance double-precision matrix multiplication[J].International Journal of Parallel Programming, 2010, 38 (3) :322-338
                            </a>
                        </p>
                        <p id="467">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1011303506.nh&amp;v=MjYzMjdlWmVWdkZ5N25XNzNKVkYyNkg3QzRIZFRNcVpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Wu Guiming.Parallel algorithms and architectures for matrix computations on FPGA[D].Changsha:National University of Defense and Technology, 2011 (in Chinese) (邬贵明.FPGA矩阵计算并行算法与结构[D].长沙:国防科学技术大学, 2011) 
                            </a>
                        </p>
                        <p id="469">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High performance and memory efficient implementation of matrix multiplication on FPGAs">

                                <b>[13]</b>Wu Guiming, Dou Yong, Wang Miao.High performance and memory efficient implementation of matrix multiplication on FPGAs[C]Proc of the 9th IEEE Int Conf on Field Programmable Technology.Piscataway, NJ:IEEE, 2010:134-137
                            </a>
                        </p>
                        <p id="471">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201509005&amp;v=MTI0OThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blc3M0pMejdCWmJHNEg5VE1wbzlGWVk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Zhou Leitao, Tao Yaodong, Liu Sheng, et al.Research on systolic multiplication and technology based on FPGA[J].Journal of Computer Science and Engineering, 2015, 37 (9) :1632-1636 (in Chinese) (周磊涛, 陶耀东, 刘生, 等.基于FPGA的Systolic乘法技术研究[J].计算机工程与科学, 2015, 37 (9) :1632-1636) 
                            </a>
                        </p>
                        <p id="473">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FPGA accelerator for floating-point matrix multiplication">

                                <b>[15]</b>Jovanovi, Milutinovi V.FPGA accelerator for floatingpoint matrix multiplication[J].IET Computers&amp;Digital Techniques, 2012, 6 (4) :249-256
                            </a>
                        </p>
                        <p id="475">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201607002&amp;v=MjgwOTNJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVzczSkx5dlNkTEc0SDlmTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Lei Yuanwu, Chen Xiaowen, Peng Yuanxi.A high energy efficiency FFT accelerator on DSP chip[J].Journal of Computer Research and Development, 2016, 53 (7) :1438-1446 (in Chinese) (雷元武, 陈小文, 彭元喜.DSP芯片中的高能效FFT加速器[J].计算机研究与发展, 2016, 53 (7) :1438-1446) 
                            </a>
                        </p>
                        <p id="477">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201607001&amp;v=MTk0MTlHRnJDVVJMT2VaZVZ2Rnk3blc3M0pMeXZTZExHNEg5Zk1xSTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Qian Lei, Zhao Jinming, Peng Dajia, et al.Energy-efficient fingerprint matching based on reconfigurable micro server[J].Journal of Computer Research and Development, 2016, 53 (7) :1425-1437 (in Chinese) (钱磊, 赵锦明, 彭达佳, 等.基于可重构微服务器的高能效指纹比对方法[J].计算机研究与发展, 2016, 53 (7) :1425-1437) 
                            </a>
                        </p>
                        <p id="479">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In-datacenter performance analysis of a tensor processing unit">

                                <b>[18]</b>Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C]Proc of the 44th IEEE Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2017:1-12
                            </a>
                        </p>
                        <p id="481">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inside volta:The world&amp;#39;&amp;#39;s most advanced data-center GPU">

                                <b>[19]</b>NVIDIA Corporation.Inside volta:The world’s most advanced data-center GPU[EB/OL].[2017-06-17].https:devblogs.nvidia.com/parallelforall/inside-volta/
                            </a>
                        </p>
                        <p id="483">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hardware for machine learning:Challenges and opportunities">

                                <b>[20]</b>Sze V, Chen Y H, Suleiman, A, et al.Hardware for machine learning:Challenges and opportunities[C]Proc of the 30th IEEE Custom Integrated Circuits Conf.Piscataway, NJ:IEEE, 2017:299-306
                            </a>
                        </p>
                        <p id="485">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning with limited numerical precision">

                                <b>[21]</b>Gupta S, Agrawal A, Gopalakrishnan K, et al.Deep learning with limited numerical precision[C]Proc of the32nd Int Conf on Machine Learning.New York:ACM, 2015:1737-1746
                            </a>
                        </p>
                        <p id="487">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DDR3 SDRAM standard">

                                <b>[22]</b>JDJEC Organization.DDR3 SDRAM standard[EB/OL].[2017-06-18].https:www.jedec.org/standards-documents/docs/jesd-79-3d
                            </a>
                        </p>
                        <p id="489">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A 45nm 1.3GHz 16.7Double-Precision GFLOPS/W RISC-V Processor with Vector Accelerators">

                                <b>[23]</b>Lee Y, Waterman A, Avizienis R, et al.A 45nm 1.3GHz16.7double-precision GFLOPS/W RISC-V processor with vector accelerators[C]Proc of the 40th IEEE European Solid-State Circuit Conf.Piscataway, NJ:IEEE, 2014:199-202
                            </a>
                        </p>
                        <p id="491">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Numerical Computing with IEEE Floating Point Arithmetic">

                                <b>[24]</b>Overton M L.Numerical Computing with IEEE Floating Point Arithmetic[M].Philadelphia, PA:SIAM, 2001
                            </a>
                        </p>
                        <p id="493">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000150606&amp;v=MDA2NTd1c0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxWYjNBSWw4PU5pZmNhck80SHRITXJvcEZZ&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>Barrett R F, Chan T, D’Azevedo E F, et al.Complex version of high performance computing LINPACK benchmark (HPL) [J].Concurrency&amp;Computation Practice&amp;Experience, 2010, 22 (5) :573-587
                            </a>
                        </p>
                        <p id="495">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                Zhang Xianyi, Wang Qian, Zhang Yunquan.OpenBLAS:Ahigh-performance BLAS library on Loongson 3A CPU[J].Journal of Software, 2011, 22 (2) :208-216 (in Chinese) (张先轶, 王茜, 张云泉.OpenBLAS:龙芯3A CPU的高性能BLAS库[J].软件学报, 2011, 22 (2) :208-216) 
                            </a>
                        </p>
                        <p id="497">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OpenBLAS:An optimized BLAS library">

                                <b>[27]</b>GitHub.OpenBLAS:An optimized BLAS library[EB/OL].[2017-08-04].https:github.com/xianyi/OpenBLAS
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902016&amp;v=MDQ1ODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzNKTHl2U2RMRzRIOWpNclk5RVlvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
