<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133238116533750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201903018%26RESULT%3d1%26SIGN%3dvoPdeSPPDw6rj2bheW7jy7HVo5g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201903018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201903018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201903018&amp;v=MDYzNTlKTHl2U2RMRzRIOWpNckk5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;2 步态算法分析与风险控制架构&lt;/b&gt; "><b>2 步态算法分析与风险控制架构</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;2.1 步态融合模型分析&lt;/b&gt;"><b>2.1 步态融合模型分析</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;2.2 风险控制模块架构&lt;/b&gt;"><b>2.2 风险控制模块架构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;3.1 实验结果&lt;/b&gt;"><b>3.1 实验结果</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.2 实验分析&lt;/b&gt;"><b>3.2 实验分析</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;3.3 人机交互体验问卷&lt;/b&gt;"><b>3.3 人机交互体验问卷</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="&lt;b&gt;4 结束语&lt;/b&gt; "><b>4 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="图1 步态融合模型流程图">图1 步态融合模型流程图</a></li>
                                                <li><a href="#77" data-title="图2 步态技术风险控制系统结构示意图">图2 步态技术风险控制系统结构示意图</a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表1 用户行为与智能设备位置关系组合&lt;/b&gt;"><b>表1 用户行为与智能设备位置关系组合</b></a></li>
                                                <li><a href="#105" data-title="图3 常见模型与融合模型身份识别准确率">图3 常见模型与融合模型身份识别准确率</a></li>
                                                <li><a href="#108" data-title="图4 常见模型行为位置识别准确率">图4 常见模型行为位置识别准确率</a></li>
                                                <li><a href="#110" data-title="图 5 不同位置下传统与融合模型准确率">图 5 不同位置下传统与融合模型准确率</a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表2 OS-ELM 志愿者二元预测平均结果&lt;/b&gt; %"><b>表2 OS-ELM 志愿者二元预测平均结果</b> %</a></li>
                                                <li><a href="#114" data-title="图6 OS-ELM增量学习准确率图">图6 OS-ELM增量学习准确率图</a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表3 人机交互体验调研评分表&lt;/b&gt;"><b>表3 人机交互体验调研评分表</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="153">


                                    <a id="bibliography_1" title="Wang Chendi. Alibaba Internet financial risk control measures [D]. Urumchi: Xinjiang University of Finance and Economics, 2016 (in Chinese) (王晨迪. 阿里巴巴互联网金融风险控制措施研究[D]. 乌鲁木齐: 新疆财经大学, 2016) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016729819.nh&amp;v=Mjc2MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0lWRjI2R0xTNkY5bk5wcEViUElRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Wang Chendi. Alibaba Internet financial risk control measures [D]. Urumchi: Xinjiang University of Finance and Economics, 2016 (in Chinese) (王晨迪. 阿里巴巴互联网金融风险控制措施研究[D]. 乌鲁木齐: 新疆财经大学, 2016) 
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_2" title="Zhang Ping. Tencent mobile payment how to do risk control [J]. Gold Card Project, 2015 (8) : 8- 9 (in Chinese) (张平. 腾讯移动支付如何做风控[J]. 金卡工程, 2015 (8) : 8- 9) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JKGC201508004&amp;v=MjA1MzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci9JTHliTWJiRzRIOVRNcDQ5Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Zhang Ping. Tencent mobile payment how to do risk control [J]. Gold Card Project, 2015 (8) : 8- 9 (in Chinese) (张平. 腾讯移动支付如何做风控[J]. 金卡工程, 2015 (8) : 8- 9) 
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_3" title="Mayagoitia R E, Nene A V, Veltink P H. Accelerometer and rate gyroscope measurement of kinematics: An inexpensive alternative to optical motion analysis systems [J]. Journal of Biomechanics, 2002, 35 (4) : 537- 542" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601326425&amp;v=MjE1NDhKbHdWYWhJPU5pZk9mYks3SHRETnFZOUVaK2tKQ0g0OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Mayagoitia R E, Nene A V, Veltink P H. Accelerometer and rate gyroscope measurement of kinematics: An inexpensive alternative to optical motion analysis systems [J]. Journal of Biomechanics, 2002, 35 (4) : 537- 542
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_4" title="Nishiguchi S, Yamada M, Nagai K, et al. Reliability and validity of gait analysis by android-based smartphone [J]. Telemedicine and e -Health, 2012, 18 (4) : 292- 296" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reliability and Validity of Gait Analysis by Android-Based Smartphone">
                                        <b>[4]</b>
                                        Nishiguchi S, Yamada M, Nagai K, et al. Reliability and validity of gait analysis by android-based smartphone [J]. Telemedicine and e -Health, 2012, 18 (4) : 292- 296
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_5" title="Shan Yanhu, Zhang Zhang, Huang Kaiqi. Visual human action recognition: History, status and prospects [J]. Journal of Computer Research and Development, 2016, 53 (1) : 93- 112 (in Chinese) (单言虎, 张彰, 黄凯奇. 人的视觉行为识别研究回顾、 现状及展望[J]. 计算机研究与发展, 2016, 53 (1) : 93- 112) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201601010&amp;v=MDY0ODNTZExHNEg5Zk1ybzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0lMeXY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Shan Yanhu, Zhang Zhang, Huang Kaiqi. Visual human action recognition: History, status and prospects [J]. Journal of Computer Research and Development, 2016, 53 (1) : 93- 112 (in Chinese) (单言虎, 张彰, 黄凯奇. 人的视觉行为识别研究回顾、 现状及展望[J]. 计算机研究与发展, 2016, 53 (1) : 93- 112) 
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_6" title="Isho T, Tashiro H, Usuda S. Accelerometry-based gait characteristics evaluated using a smartphone and their association with fall risk in people with chronic stroke [J]. Journal of Stroke and Cerebrovascular Diseases, 2015, 24 (6) : 1305- 1311" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES45891DE3186335C11AF05A3A8595AA8B&amp;v=Mjc1OTUrd0tnPU5pZk9mYmU5RnRqTjIvcEdaZU1KRDM4OHZCY1NtMGw5VFE3aDNSb3djTGZsTkxMdENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRsaHdycQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Isho T, Tashiro H, Usuda S. Accelerometry-based gait characteristics evaluated using a smartphone and their association with fall risk in people with chronic stroke [J]. Journal of Stroke and Cerebrovascular Diseases, 2015, 24 (6) : 1305- 1311
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_7" title="Liu Rong, Zhou Jianzhong, Liu Ming, et al. A wearable acceleration sensor system for gait recognition [C] //Proc of the 2nd IEEE Conf on Industrial Electronics and Applications. Piscataway, NJ: IEEE, 2007: 2654- 2659" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A wearable acceleration sensor system for gait recognition">
                                        <b>[7]</b>
                                        Liu Rong, Zhou Jianzhong, Liu Ming, et al. A wearable acceleration sensor system for gait recognition [C] //Proc of the 2nd IEEE Conf on Industrial Electronics and Applications. Piscataway, NJ: IEEE, 2007: 2654- 2659
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_8" title="Chen Shi, Ma Tianjun, Gao Youxing. Gait recognition using distributions of silhouette feature [J]. Journal of Computer Research and Development, 2009, 46 (2) : 295- 301 (in Chinese) (陈实, 马天骏, 高有行. 用行人轮廓的分布直方图分类和识别步态 [J]. 计算机研究与发展, 2009, 46 (2) : 295- 301) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200902019&amp;v=MDQwNDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0lMeXZTZExHNEh0ak1yWTlFYllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Chen Shi, Ma Tianjun, Gao Youxing. Gait recognition using distributions of silhouette feature [J]. Journal of Computer Research and Development, 2009, 46 (2) : 295- 301 (in Chinese) (陈实, 马天骏, 高有行. 用行人轮廓的分布直方图分类和识别步态 [J]. 计算机研究与发展, 2009, 46 (2) : 295- 301) 
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_9" title="Mantyjarvi J, Lindholm M, Vildjiounaite E, et al. Identifying users of portable devices from gait pattern with accelerometers [C] //Proc of the 5th IEEE Int Conf on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2005: 973- 976" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identifying users of portable devices from gait pattern with accelerometers">
                                        <b>[9]</b>
                                        Mantyjarvi J, Lindholm M, Vildjiounaite E, et al. Identifying users of portable devices from gait pattern with accelerometers [C] //Proc of the 5th IEEE Int Conf on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2005: 973- 976
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_10" title="Chen Changyou, Zhang Junping. An iterative gait prototype learning algorithm based on tangent distance [J]. Journal of Computer Research and Development, 2008, 45 (7) : 1177- 1182 (in Chinese) (陈昌由, 张军平. 基于迭代切距离原型学习算法的步态识别 [J]. 计算机研究与发展, 2008, 45 (7) : 1177- 1182" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200807009&amp;v=MDM2MDJ2U2RMRzRIdG5NcUk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci9JTHk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Chen Changyou, Zhang Junping. An iterative gait prototype learning algorithm based on tangent distance [J]. Journal of Computer Research and Development, 2008, 45 (7) : 1177- 1182 (in Chinese) (陈昌由, 张军平. 基于迭代切距离原型学习算法的步态识别 [J]. 计算机研究与发展, 2008, 45 (7) : 1177- 1182
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_11" title="Yang Yingchang. Metod and continuously wearable noninvasive apparatus for automatically detecting a stroke and other abnormal health conditions: US, US20140276123[P]. 2014-09-18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Metod and continuously wearable noninvasive apparatus for automatically detecting a stroke and other abnormal health conditions">
                                        <b>[11]</b>
                                        Yang Yingchang. Metod and continuously wearable noninvasive apparatus for automatically detecting a stroke and other abnormal health conditions: US, US20140276123[P]. 2014-09-18
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_12" title="Huang Guangbin, Zhu Qinyu, Siew C K. Extreme learning machine: A new learning scheme of feedforward neural networks [C] //Proc of the IEEE Int Joint Conf on Neural Networks. Piscataway, NJ: IEEE, 2004: 985- 990" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extreme learning machine:A new learning scheme of feedforward neural networks">
                                        <b>[12]</b>
                                        Huang Guangbin, Zhu Qinyu, Siew C K. Extreme learning machine: A new learning scheme of feedforward neural networks [C] //Proc of the IEEE Int Joint Conf on Neural Networks. Piscataway, NJ: IEEE, 2004: 985- 990
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_13" title="Tong Kaiyu, Granat M H. A practical gait analysis system using gyroscopes [J]. Medical Engineering &amp;amp; Physics, 1999, 21 (2) : 87- 94" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300845719&amp;v=MjQzNTBLN0h0RE5ySTlGYk84S0MzMHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lKbHdWYWhJPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Tong Kaiyu, Granat M H. A practical gait analysis system using gyroscopes [J]. Medical Engineering &amp;amp; Physics, 1999, 21 (2) : 87- 94
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_14" title="Lisetti C L, Nasoz F. Using noninvasive wearable computers to recognize human emotions from physiological signals [J]. EURASIP Journal on Advances in Signal Processing, 2004 (11) : 1672- 1687" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using Noninvasive Wearable Computers to Recognize Human Emotions from Physiological Signals">
                                        <b>[14]</b>
                                        Lisetti C L, Nasoz F. Using noninvasive wearable computers to recognize human emotions from physiological signals [J]. EURASIP Journal on Advances in Signal Processing, 2004 (11) : 1672- 1687
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_15" title="Hayfron-Acquah J, Nixon M, Carter J. Automatic gait recognition by symmetry analysis[C] //Proc of the 3rd Int Conf on Audio and Video-Based Biometric Person Authenti-cation. Berlin: Springer, 2001: 272- 277" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic gait recognition by symmetry analysis">
                                        <b>[15]</b>
                                        Hayfron-Acquah J, Nixon M, Carter J. Automatic gait recognition by symmetry analysis[C] //Proc of the 3rd Int Conf on Audio and Video-Based Biometric Person Authenti-cation. Berlin: Springer, 2001: 272- 277
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                    Mayagoitia R E, Nene A V, Veltink P H. Accelerometer and rate gyroscope measurement of kinematics: An inexpensive alternative to optical motion analysis systems [J]. Journal of Biomechanics, 2002, 35 (4) : 537- 542</a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_17" title="Huang Guangbin, Zhou Hongming, Ding Xiaojian, et al. Extreme learning machine for regression and multiclass classification [J]. IEEE Transactions on Systems Man &amp;amp; Cybernetics: Part B, 2012, 42 (2) : 513- 529" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extreme Learning Machine for Regression and Multiclass Classification">
                                        <b>[17]</b>
                                        Huang Guangbin, Zhou Hongming, Ding Xiaojian, et al. Extreme learning machine for regression and multiclass classification [J]. IEEE Transactions on Systems Man &amp;amp; Cybernetics: Part B, 2012, 42 (2) : 513- 529
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_18" title="Han Ju, Bhanu B. Individual recognition using gait energy image [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (2) : 316- 322" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Individual recognition using gait energy image">
                                        <b>[18]</b>
                                        Han Ju, Bhanu B. Individual recognition using gait energy image [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (2) : 316- 322
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_19" title="Arora P, Srivastava S. Gait recognition using gait Gaussian image[C] //Proc of the 2nd Int Conf on Signal Processing and Integrated Networks. Piscataway, NJ: IEEE, 2015: 791- 794" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gait recognition using gait Gaussian image">
                                        <b>[19]</b>
                                        Arora P, Srivastava S. Gait recognition using gait Gaussian image[C] //Proc of the 2nd Int Conf on Signal Processing and Integrated Networks. Piscataway, NJ: IEEE, 2015: 791- 794
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_20" title="Liu Rong, Duan Zhiguo, Zhou Jianzhong, et al. Identification of individual walking patterns using gait acceleration [C] //Proc of the 1st Int Conf on Bioinformatics and Biomedical Engineering. Piscataway, NJ: IEEE, 2007: 543- 546" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identification of individual walking Patterns using gaitacceleration">
                                        <b>[20]</b>
                                        Liu Rong, Duan Zhiguo, Zhou Jianzhong, et al. Identification of individual walking patterns using gait acceleration [C] //Proc of the 1st Int Conf on Bioinformatics and Biomedical Engineering. Piscataway, NJ: IEEE, 2007: 543- 546
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_21" title="Raziff A R A, Sulaiman M N, Mustapha N, et al. Single classifier, OvO, OvA and RCC multiclass classification method in handheld based smartphone gait identification[C] //Proc of the 5th Int Conf on Applied Science and Technology. New York: AIP Publishing: 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single classifier, OvO, OvA and RCC multiclass classification method in handheld based smartphone gait identification">
                                        <b>[21]</b>
                                        Raziff A R A, Sulaiman M N, Mustapha N, et al. Single classifier, OvO, OvA and RCC multiclass classification method in handheld based smartphone gait identification[C] //Proc of the 5th Int Conf on Applied Science and Technology. New York: AIP Publishing: 2017
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(03),635-642 DOI:10.7544/issn1000-1239.2019.20170807            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>多元数据融合的非干扰身份识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E4%BD%83%E5%AD%98&amp;code=37799766&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于佃存</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%9B%8A%E5%BC%BA&amp;code=09559492&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈益强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E6%99%93%E6%99%96&amp;code=37799767&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭晓晖</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%84%A6%E5%B8%85&amp;code=26679603&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">焦帅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%95%B8%E6%B5%B7&amp;code=37799768&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李啸海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%9F%E4%B9%A0&amp;code=35454277&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钟习</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0142480&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院计算技术研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0237047&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山东大学软件学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统的步态识别技术在智能终端设备风险控制领域的应用还存在一些问题, 已有的方案是通过加速度、陀螺仪等多传感器对步态进行身份识别与验证.由于现有的识别方法设置了许多限制条件, 给该技术的使用与推广俱造成了困难.例如:需要把传感器设备固定在脚踝、膝盖、腰部等位置, 设备有指定的朝向, 用户做特定的动作.通过步态进行身份识别与验证的技术应用到风险控制领域需要一套完整可靠的系统架构, 现有架构还存在较大问题.因此, 提出一种与位置、行为无关的非干扰的身份识别与验证方法, 该方法仅使用加速度传感器, 并以此方法为核心建立了一套完整的系统实现架构, 该架构方法的实现提高了系统的整体精度与可用性.首先对用户的行为及设备所在的位置进行预测;然后针对性地进行步态分析与识别.实验中仅使用智能手机中内置的加速度传感器采集数据, 最后对步态进行位置无关的分析与识别最重确定用户身份, 从而起到降低智能手机使用风险提高安全系数的作用.实验结果表明设计的系统架构有利于系统整体精度的提升, 且该方法具有较高的识别率和极低的假阳率 (false positive rate, FPR) , 且在非干扰用户的情况下提高了APP和智能手机等智能终端设备的安全性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人机交互;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%89%E5%85%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安全;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A5%E6%80%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">步态;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A3%8E%E9%99%A9%E6%8E%A7%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">风险控制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E5%B9%B2%E6%89%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非干扰;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    于佃存, yudiancun@ict.ac.cn;
                                </span>
                                <span>
                                    *陈益强, yqchen@ict.ac.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-10-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划基金项目 (2017YFB1002801);</span>
                                <span>国家自然科学基金项目 (61572471);</span>
                                <span>广东省科技计划项目 (2015B010105001);</span>
                                <span>中国科学院率先行动“百人计划”项目 (Y704061000);</span>
                    </p>
            </div>
                    <h1><b>Multi-Model Data Fusion Based Unobtrusive Identification Method</b></h1>
                    <h2>
                    <span>Yu Diancun</span>
                    <span>Chen Yiqiang</span>
                    <span>Peng Xiaohui</span>
                    <span>Jiao Shuai</span>
                    <span>Li Xiaohai</span>
                    <span>Zhong Xi</span>
            </h2>
                    <h2>
                    <span>Institute of Computing Technology, Chinese Academy of Sciences</span>
                    <span>School of Software, Shandong University</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The traditional gait recognition technology in the field of intelligent terminal equipment risk control still has some problems. The existing program is using accelerometer, gyroscope and other multi-sensor gait for identification and verification. Due to the existing identification methods set a number of restrictions, hinder the use and promotion of this technology. For example: the sensor device needs to be fixed at the same position as the ankle, knee, waist and so on; the device has a designated orientation; the user does a specific action. In addition, the application of the technology of identity verification and verification through gait to the field of risk control requires a complete and reliable system architecture. There is still a big problem with the existing architecture. Therefore, this paper presents a non-interference and location-independent identification and verification method that uses only accelerometers and builds a complete set of system implementation architecture with this method as the core. The implementation of this architecture method has improved the overall system accuracy and availability. Firstly, the user's behavior and the location of the device are predicted; then the gait analysis and identification are carried out. In this experiment, we only use the built-in accelerometer in the smart phone to collect data, finally position-independent gait analysis and identification to identify the user to determine which is the most important, so as to reduce the risk of using smart phones and improve the safety factor. The experimental results show that the system architecture designed in this paper is conducive to the improvement of overall system accuracy. The method has the characteristics of high recognition rate and very low FPR (false positive rate) , and improves the APP and the smartphone in the case of non-interfering users such as intelligent terminal equipment security.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human-computer%20interaction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human-computer interaction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=safety&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">safety;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gait&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gait;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=risk%20control&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">risk control;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noninterference&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noninterference;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Yu Diancun, born in 1991.MSc candidate.Student member of CCF.His main research interests include the pervasive computing and human-computer interaction.<image id="146" type="formula" href="images/JFYZ201903018_14600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Chen Yiqiang, born in 1973.PhD, professor.Member of CCF.His main research interests include the pervasive computing and human-computer interaction.<image id="147" type="formula" href="images/JFYZ201903018_14700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Peng Xiaohui, born in 1984.PhD, associate professor.Member of CCF.His main research interests include the architccturc and computing models of things computing system.<image id="148" type="formula" href="images/JFYZ201903018_14800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Jiao Shuai, born in 1984.PhD, assistant professor.His main research interests include the pervasive computing and computer architecture.<image id="149" type="formula" href="images/JFYZ201903018_14900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Li Xiaohai, born in 1989.PhD candidate.Student member of CCF.His main research interests include the pervasive computing and human-computer interaction.<image id="150" type="formula" href="images/JFYZ201903018_15000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhong Xi, born in 1991.MSc, student member of CCF.Her main research interests include the pervasive computing and human-computer interaction.<image id="151" type="formula" href="images/JFYZ201903018_15100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-10-25</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2017YFB1002801);</span>
                                <span>the National Natural Science Foundation of China (61572471);</span>
                                <span>the Science and Technology Planning Project of Guangdong Province of China (2015B010105001);</span>
                                <span>the CAS Pioneer Hundred Talents Program (Y704061000);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="51">近年来, 随着智能终端设备特别是智能手机的普及, 手机端的应用与系统的安全性较低, 如何做好终端设备的风险控制成为具有挑战性的研究问题.根据Newzoo 2018年预测, 到2021年全球手机网民规模将达38亿<citation id="195" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.“互联网+”如火如荼的发展, 使得越来越多的配套服务也在不断的发展与完善<citation id="197" type="reference"><link href="155" rel="bibliography" /><link href="157" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>, 各行各业都在积极地扩展互联网业务, 从而为广大的智能终端设备用户提供便捷、多样、个性化的APP服务, 手机也渐渐成为个人的“私人助理”、“支付助手”<citation id="198" type="reference"><link href="159" rel="bibliography" /><link href="161" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>.因此智能手机设备中用户隐私数据以及支付环境的安全性越来越引起我们的重视.然而在人机交互方面, 系统往往为了提高用户体验而降低了对隐私数据的保护, 例如记住密码、自动登录等.如何在确保这些数据安全的同时, 又能给用户提供友好便捷的交互方式已经成为越来越突出的问题.如:蚂蚁金服推出风险控制系统取消手势密码<citation id="196" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 通过对用户账号、设备、位置、行为、关系、偏好等维度的分析, 提高安全系数、降低使用的复杂度, 从而使支付宝APP使用更便捷安全等.</p>
                </div>
                <div class="p1">
                    <p id="52">步态作为一种非干扰的身份识别方法获得了广泛的认同.虽然通过图像进行步态分析的方法已经很成熟, 但是由于受到图像步态信息数据采集条件的约束, 该技术的推广使用受到了严格的限制<citation id="201" type="reference"><link href="161" rel="bibliography" /><link href="163" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>.利用加速度传感器采集到的数据进行步态分析, 是一种典型的非干扰步态分析方法<citation id="199" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.这种传感器设备作为基础设备内嵌在智能手机中, 进行步态分析, 能够很方便地进行推广普及.将基于步态的身份识别技术应用到智能手机的风险控制系统中能够在非干扰用户的情况下对用户的身份进行识别, 避免用户重复进行身份认证如:输入密码、录入指纹以及人脸扫描等, 提高了身份识别的隐蔽性, 同时也提高了安全性<citation id="202" type="reference"><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>.考虑到易用性以及易推广性可以将步态识别验证模块内嵌到智能手机的APP应用中或将识别验证模块作为智能手机操作系统的独立模块, 为系统上的应用提供统一服务, 本文提出了一套通用的步态分析系统架构方法可将其应用到智能手机APP风险控制中.与传统方法相比步态识别与手机的位置、用户行为无关, 同时, 采用OS-ELM (online sequential extreme learning machine) 在线增量的方式不断更新模型从而提高准确度<citation id="200" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="54">风险控制一直是备受关注的话题, 尤其是在互联网世界里信息数据都是以电子方式存储容易泄露、丢失或被恶意篡改等.近年来, 基于大数据的风险控制系统在不断发展与完善, 2015年7月“蚂蚁金服”推出风险控制系统<citation id="203" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 对用户的日常行为、情绪以及行为进行监控分析, 发现问题及时介入处理.在智能手机中有很多APP为了提高用户体验而提供记住密码或自动登录功能等, 使得用户信息暴露在风险之下.虽然有些APP通过GPS或IP等信息分析用户活动区域, 如果用户不在经常活动的区域内需要重新进行身份认证才能继续使用APP以达到保护用户信息的作用, 但这些方法都不能对用户身份做出准确的判断<citation id="204" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>;所以, 将可以识别用户身份的步态信息纳入风险控制体系中能极大地提高对用户信息的保护能力, 同时也能提高用户非干扰风险控制的体验<citation id="205" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="55">传统的身份验证主要采用如密码、声音、指纹、虹膜或人脸等<citation id="211" type="reference"><link href="179" rel="bibliography" /><link href="181" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>, 这些方法相互配合能很好地保护用户的隐私信息, 但是这些方法都需要用户显式操作或者对设备与环境有特殊的要求<citation id="206" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.这样使得人机交互变得复杂, 用户体验在降低.步态作为一种新的身份识别与验证方式越来越受国内外研究人员的关注, 并取得了一些研究成果.早在1999年Tong等人<citation id="207" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>使用陀螺仪固定在志愿者的脚踝处进行步态分析.之后Mayagoitia等人<citation id="208" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>使用加速度传感器和陀螺仪进行步态分析, 并与光学方式进行的步态分析进行了对比, 但是这些传统步态研究都存在一个问题:通过加速度传感器或陀螺仪等设备进行步态分析的时候都需要将特有设备放在用户的固定位置如:脚踝、膝盖、腰部等, 并保持特定的方向<citation id="209" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.同样已存在的基于智能终端的步态身份识别系统也受到诸多类似的问题限制, 例如:需要将手机放在特定的位置如:腰部;手机的摆放方向也受到约束等<citation id="210" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.这样都极大限制了步态身份识别在手机风险控制上的应用与发展, 因而, 位置、行为无关性研究在步态身份分析识别方面具有重大意义.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>2 步态算法分析与风险控制架构</b></h3>
                <h4 class="anchor-tag" id="57" name="57"><b>2.1 步态融合模型分析</b></h4>
                <div class="p1">
                    <p id="58">本文提出了一种基于智能手机的与终端设备位置和用户行为无关的步态融合身份识别方法, 如图1所示, 该方法在进行步态识别的过程中极大的消除了传统步态识别方法对传感器摆放的位置、朝向和用户行为的约束.本文使用智能手机设备内嵌的加速度传感器设备进行加速度数据的采集, 从而获取原始步态数据.通过对用户日常行为习惯的统计分析, 用户行走的过程中智能手机设备绝大多数放在裤口袋或摆动的手中, 因此步态分析方法建立在手持智能手机设备自然摆动行走和智能手机放在裤口袋内行走2种行为位置组合之上.针对用户不可枚举的行为与位置组合, 首先对用户的行为和手机的位置采用随机森林 (random forest, RF) 方法进行识别过滤, 然后再使用OS-ELM方法对2种行为与位置组合所获取的原始数据进行特征提取并生成特征向量, 最后采用增量的方式更新模型以提高识别的准确度.步态融合模型流程如图1所示:</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903018_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 步态融合模型流程图" src="Detail/GetImg?filename=images/JFYZ201903018_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 步态融合模型流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903018_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Gait fusion model flow chart</p>

                </div>
                <div class="p1">
                    <p id="60">我们用概率的方法, 将本文研究的问题表示为</p>
                </div>
                <div class="p1">
                    <p id="61"><i>P</i> (<i>X</i>|<i>Y</i>) =<i>P</i> (<i>Y</i>|<i>AB</i>) <i>P</i> (<i>A</i>|<i>X</i>) <i>P</i> (<i>B</i>|<i>AX</i>) =</p>
                </div>
                <div class="p1">
                    <p id="62"><i>P</i> (<i>Y</i>|<i>AB</i>) <i>P</i> (<i>A</i>|<i>X</i>) <i>P</i> (<i>B</i>|<i>AX</i>) =</p>
                </div>
                <div class="p1">
                    <p id="63"><i>P</i> (<i>Y</i>|<i>AB</i>) <i>P</i> (<i>A</i>|<i>X</i>) <i>P</i> (<i>B</i>|<i>A</i>) <i>P</i> (<i>A</i>|<i>X</i>) , </p>
                </div>
                <div class="p1">
                    <p id="64">其中, <i>X</i>为原始特征数据, <i>Y</i>为识别结果, <i>A</i>, <i>B</i>分别为识别的行为与位置, <i>P</i> (<i>A</i>|<i>Y</i>) 对应行为识别模型, <i>P</i> (<i>B</i>|<i>X</i>) 对应位置识别模型.</p>
                </div>
                <div class="p1">
                    <p id="65">假设有<i>n</i>个任意加速度数据样本<b><i>a</i></b><sub><i>i</i></sub>, <i>i</i>∈ (0, <i>n</i>) , 采集到的加速度向量:</p>
                </div>
                <div class="p1">
                    <p id="66"><b><i>a</i></b><sub><i>i</i></sub>= (<i>α</i><sub><i>i</i></sub>, <i>β</i><sub><i>i</i></sub>, <i>γ</i><sub><i>i</i></sub>) , </p>
                </div>
                <div class="p1">
                    <p id="67">其中<i>α</i>, <i>β</i>, <i>γ</i>为加速度传感器的3轴分量.将3轴加速度值进行位置无关处理:</p>
                </div>
                <div class="p1">
                    <p id="68"><i>A</i><sub><i>i</i></sub>=<i>α</i><sub><i>i</i></sub>+<i>β</i><sub><i>i</i></sub>+<i>γ</i><sub><i>i</i></sub>, </p>
                </div>
                <div class="p1">
                    <p id="69">将采集的加速度数据流进行时间窗划分用<i>m</i>表示, 假设第<i>j</i>个时间窗表示为</p>
                </div>
                <div class="p1">
                    <p id="70"><b><i>Q</i></b><sub><i>i</i></sub>=[<i>A</i><sub> (<i>i</i>, 1) </sub>, <i>A</i><sub> (<i>i</i>, 2) </sub>, <i>L</i>, <i>A</i><sub> (<i>i</i>, 256) </sub>], <i>i</i>∈[1, <i>m</i>]</p>
                </div>
                <div class="p1">
                    <p id="71"><b><i>Q</i></b><sub><i>j</i></sub>中时间窗前后重叠50%.得到矩阵<b><i>M</i></b>:</p>
                </div>
                <div class="p1">
                    <p id="72"><b><i>M</i></b>=[<b><i>Q</i></b><sub>1</sub>, <b><i>Q</i></b><sub>2</sub>, …, <b><i>Q</i></b><sub><i>m</i></sub>]<sup>T</sup>, </p>
                </div>
                <div class="p1">
                    <p id="73">获得<i>m</i>个模型训练样本 (<b><i>V</i></b><sub><i>i</i></sub>, <b><i>T</i></b><sub><i>i</i></sub>) , <i>i</i>∈[1, <i>m</i>], 其中<b><i>V</i></b><sub><i>i</i></sub>是<b><i>Q</i></b><sub><i>i</i></sub>中提取的<i>x</i>维的行为与位置特征向量:</p>
                </div>
                <div class="p1">
                    <p id="74"><b><i>V</i></b><sub><i>i</i></sub>=[<i>v</i><sub> (<i>i</i>, 1) </sub>, <i>v</i><sub> (<i>i</i>, 2) </sub>, …, <i>v</i><sub> (<i>i</i>, <i>x</i>) </sub>]∈R<sup><i>x</i></sup>, </p>
                </div>
                <div class="p1">
                    <p id="75"><b><i>T</i></b><sub><i>i</i></sub>=[<i>t</i><sub> (<i>i</i>, 1) </sub>, <i>t</i><sub> (<i>i</i>, 2) </sub>]</p>
                </div>
                <div class="p1">
                    <p id="76">为对应的行为标量, 位置也如上处理, 仅提取的特征向量略有不同.最后将行为与位置进行分别且串联预测.</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903018_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 步态技术风险控制系统结构示意图" src="Detail/GetImg?filename=images/JFYZ201903018_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 步态技术风险控制系统结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903018_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Gait technology risk control system structure diagram</p>

                </div>
                <div class="p1">
                    <p id="78">RF算法使用决策树 (decision tree, DT) 作为基本分类器, 具有训练速度快精度高的特点.使用RF算法对智能手机设备持有者的行为以及设备所在的位置进行分别预测, 通过信息增益 (information entropy) .尽可能地使得分支节点的包含节点属于同一类.假设离散属性<i>b</i>有<i>v</i>个可取值{<i>b</i><sub>2</sub>, <i>b</i><sub>2</sub>, …, <i>b</i><sub><i>v</i></sub>}, </p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mtext> </mtext><mi>p</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow><mi>v</mi></mfrac></mrow></mstyle><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mspace width="0.25em" /><mtext>l</mtext><mtext>b</mtext><mtext> </mtext><mi>p</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">获得信息增益越大, 则意味着使用属性<i>a</i>进行划分获得效果越好.从而构造出较好的分类器以此识别出用户行为与位置.</p>
                </div>
                <div class="p1">
                    <p id="81">OS-ELM算法是基于极限学习机器<citation id="212" type="reference"><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation> (extr-eme learning machine, ELM) 的在线增量模型算法, 使用该算法可以不断积累用户步态行为数据, 以此不断完善更新步态识别模型提高识别的准确性.假设有<i>m</i>个任意样本 (<b><i>V</i></b><sub><i>i</i></sub>, <b><i>T</i></b><sub><i>i</i></sub>) , <i>i</i>∈[1, <i>m</i>], <b><i>V</i></b><sub><i>i</i></sub> 是<i>z</i>维的步态输入向量<b><i>V</i></b><sub><i>i</i></sub>=[<i>v</i><sub> (<i>i</i>, 1) </sub>, <i>v</i><sub> (<i>i</i>, 2) </sub>, …, <i>v</i><sub> (<i>i</i>, <i>z</i>) </sub>]∈R<sup><i>z</i></sup>, <b><i>V</i></b><sub><i>i</i></sub>是1维的身份目标向量.其中有<i>L</i>个节点的单隐含层前馈神经网络 (single hidden layer feed-forward neural networks, SLFNs) 表示为</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi mathvariant="bold-italic">β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>G</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>j</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>Ν</mi><mo stretchy="false">]</mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <i>a</i><sub><i>i</i></sub>, <i>b</i><sub><i>i</i></sub>中是隐含层节点的学习参数, <i>β</i><sub><i>i</i></sub>是隐含层第<i>i</i>个节点与输出节点间的权重系数.<i>G</i> (<i>a</i><sub><i>i</i></sub>, <i>b</i><sub><i>i</i></sub>, <i>x</i><sub><i>i</i></sub>) 为激活函数.计算SLFNs使得输出的误差最小, 使得:</p>
                </div>
                <div class="p1">
                    <p id="85"><i>y</i><sub><i>i</i></sub>=<i>t</i><sub><i>i</i></sub>, <i>j</i>∈[1, <i>N</i>], <i>i</i>∈[1, <i>L</i>].</p>
                </div>
                <div class="p1">
                    <p id="86">矩阵表示为<b><i>H</i></b><i>β</i>=<b><i>T</i></b>:</p>
                </div>
                <div class="area_img" id="152">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903018_15200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="90">当<i>a</i><sub><i>i</i></sub>, <i>b</i><sub><i>i</i></sub>的值随机确定后, 隐含层节点的输出矩阵<b><i>H</i></b>的值也唯一确定.所以求出<b><i>H</i></b><i>β</i>=<b><i>T</i></b>的过程即为训练单隐含层前馈神经网络的过程.且输出的权重<i>β</i>可以被确定<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mo>+</mo></msup><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>, 其中, <b><i>H</i></b><sup>+</sup>是矩阵的Moore-Penrose广义逆.通过以上步骤, 求解出<i>β</i>.那么对于特定的输入<i>x</i><sub><i>j</i></sub>, 和<i>x</i><sub><i>j</i></sub>相对应的实际输出<i>y</i><sub><i>j</i></sub>表示为</p>
                </div>
                <div class="area_img" id="94">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903018_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="95">另外, OS-ELM采用的是在线增量的方式进行模型更新, 能及时更新识别模型提高算法的精度.</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>2.2 风险控制模块架构</b></h4>
                <div class="p1">
                    <p id="97">图2描述的是步态识别系统的结构示意图, 加速度数据依次经过行为模型、位置模型和步态模型 (步骤1, 2, 3) 对用户的身份进行识别, 如果没有通过则进入步骤4.1激活其他验证方式, 直到步骤5.1验证通过后则执行步骤6将该用户的加速度数据上传云端.行为模型与位置模型区分度强, 不需要在线增量更新, 在严重影响准确度的时候执行步骤7.1, 7.2更新整个模型, 本文使用OS-ELM算法构建步态模型, 通过步骤7.3在线增量更新模型从而提高模型识别的准确率.</p>
                </div>
                <div class="p1">
                    <p id="98">本文设计与实现的步态识别系统可以作为应用程序的非干扰风险控制模块为智能设备的应用提供很好的风险安全参考;同时可以将该模块升级到操作系统层面, 为其上的应用提供便捷身份识别接口, 从而使用户验证变的简单、便捷与高效提高用户体验.</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="100">现有的步态识别方法对传感器设备的位置都有较高的要求, 如固定在脚踝、膝盖、腰部或放在口袋里等.在识别的过程中采用多种传感器设备, 分析过程过为复杂以及采用较高的采样频率等方法不利于其在智能手机设备中的普遍应用.这些方法都不能很好地应用到实际的系统中进行风险控制.本文采用的加速度传感器采样频率仅为64 Hz, 通过过滤模型将日常较为普遍的用户行为和智能手机设备放的位置进行预判断, 利用融合模型的方法对加速度数据进行具体的分析以识别出用户身份, 从而对用户智能手机进行风险控制达到提高设备安全性的目的<citation id="213" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="101">通过分析可知, 智能手机设备放在手中和口袋中的概率最大, 因此本文的身份认证模型系统针对的是用户行进过程中智能手机设备放在摆动的手中和裤口袋里这2种组合进行实验分析, 对于不同的场合可以根据需要增加组合, 更新对应模型从而覆盖更多的行为类别.本文使用随机森林算法对用户的行为进行分析预测, 并将预测的结果以及提取的特征向量通过OS-ELM算法进行识别与验证.本文中, 我们对3种行为 (走路、跑步、其他) 和3种位置 (摆动的手中、固定在胸前、裤口袋) 进行了实验.主要针对走路的步态进行研究分析以达到非干扰身份识别与验证, 从而进行风险控制, 提高设备安全性和用户体验.行为与位置的组合如表1所示:</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表1 用户行为与智能设备位置关系组合</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 User Behavior And Smart Device Location Relationship Portfolio</b></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td rowspan="2"><br />User<br />Behavior</td><td colspan="3"><br />Smart Device Location</td></tr><tr><td><br />Swing Hand</td><td>Fixed Chest</td><td>Panst Packet</td></tr><tr><td><br />Walking</td><td>√</td><td>√</td><td>√</td></tr><tr><td><br />Running</td><td>√</td><td></td><td>√</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: √ is the combination of body parts when sampling the data.</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>3.1 实验结果</b></h4>
                <div class="p1">
                    <p id="104">本文实验邀请了32名志愿者 (21男11女) 按照表1所述的用户行为与智能手机位置的不同组合情况, 进行数据采集并完成步态身份识别与验证实验.32位志愿者分别完成“走路-摆动的手中”、“走路-固定在胸前”、“走路-裤口袋”、“跑步-摆动的手中”、“跑步-裤口袋” 5种行为-位置组合, 对于每种组合, 使用智能手机内置的采样频率为64 Hz的加速度传感器采样3 min, 并以4 s为一个时间窗, 每一个时间窗即为一个样本.每种情况共获得样本数为1 440个.经过多次实验验证, 行为模型与位置模型使用RF算法识别的准确率较高, 如图3所示.将过滤后的数据输入不同的模型进行数据分析, 其结果如图3所示:</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903018_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 常见模型与融合模型身份识别准确率" src="Detail/GetImg?filename=images/JFYZ201903018_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 常见模型与融合模型身份识别准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903018_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Common model and fusion model identification  accuracy</p>

                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.2 实验分析</b></h4>
                <div class="p1">
                    <p id="107">如图4所示, 可以明显看出RF对行为与位置的识别准确率较高, 并且RF算法的复杂度较低, 更有利于在智能手机上使用, 所以在行为与位置识别过程中采用的是RF算法.图4是将常用的5种算法模型应用到步态身份识别中, 并与先进行步态与位置预测的融合模型对比, 发现融合模型识别精度高于其他模型, 所以在进行步态识别时采用本文提出的融合步态识别模型能有效地提高预测准确率.因此, 本文提出的方法为智能终端风险控制模块提供新的非干扰式的身份判断依据, 能进一步提高了用户信息的安全性和用户体验.</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903018_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 常见模型行为位置识别准确率" src="Detail/GetImg?filename=images/JFYZ201903018_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 常见模型行为位置识别准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903018_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Common model behavior location recognition accuracy</p>

                </div>
                <div class="p1">
                    <p id="109">进一步比较本文步态识别算法和传统步态识别算法的准确率, 使用相同的数据集进行对比实验, 实验对比用户在走路的时候智能手机设备在“摆动的手中”、“固定在胸前”、“裤口袋”3个不同位置, 实验结果如图5所示:</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图 5 不同位置下传统与融合模型准确率" src="Detail/GetImg?filename=images/JFYZ201903018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图 5 不同位置下传统与融合模型准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The accuracy of traditional and fusion models in different positions</p>

                </div>
                <div class="p1">
                    <p id="111">通过图5可以得出:对于在用户走路行为中, 智能手机设备放在“摆动的手中”、“固定在胸前”、“裤口袋”这3个位置, 本文提出的仅使用加速度传感器数据的算法精度均优于传统的步态识别算法.传感器、智能设备摆放的位置是用户步态识别准确率的重要因素, 即使是同一个用户在不同位置采集到的数据对用户步态的识别也有着较大的影响.因此在进行用户步态识别之前, 先对用户的行为以及智能手机设备的位置的预测对随后的用户步态识别与验证有很大的帮助.二元预测的结果如表2所示:</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表2 OS-ELM 志愿者二元预测平均结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 OS-ELM Volunteers Bivariate Prediction Average Results</b></p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td rowspan="2"><br />Really<br />Category</td><td colspan="2"><br />Forecast Category</td></tr><tr><td><br />Yes</td><td>No</td></tr><tr><td><br />Yes</td><td>81</td><td>19</td></tr><tr><td><br />No</td><td>8</td><td>92</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="113">本文在进行步态识别的时候采用的OS-ELM在线更新模型, 在用户的使用过程中其步态信息会随着首次身份确认, 在用户授权的前提下同步将数据上传保存到云端的步态指纹库中.指纹库的目的是不断地更新步态识别模型, 从而不断提高步态识别的准确度, 为用户步态安全模块提供在线更新支持.步态识别模型在线增量更新准确率如图6所示:</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903018_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 OS-ELM增量学习准确率图" src="Detail/GetImg?filename=images/JFYZ201903018_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 OS-ELM增量学习准确率图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903018_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 OS-ELM incremental learning accuracy map</p>

                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>3.3 人机交互体验问卷</b></h4>
                <div class="p1">
                    <p id="116">在每位志愿者完成人机交互实验之后, 邀请他们完成一份关于人机交互体验的简单问卷, 每个问题满分为10分, 分数越高表现越好.如表3所示, 本文提出的步态识别方法达到了非常优秀的效果, 人机交互体验也得到了志愿者的认同.</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表3 人机交互体验调研评分表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Human Computer Interaction Experience Questionnaire Survey Result</b></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />Problems</td><td>Fusion method</td><td>Common Method</td></tr><tr><td><br />User Identity<br />Accuracy Rate</td><td>8.8</td><td>8.3</td></tr><tr><td><br />Interactive Experience</td><td>9</td><td></td></tr><tr><td><br />Error User <br />Recognition Rate</td><td>9</td><td>6.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="118" name="118" class="anchor-tag"><b>4 结束语</b></h3>
                <div class="p1">
                    <p id="119">本文提出了一种基于步态分析的非干扰身份验证方法, 并将其应用在智能终端风险控制中, 同时我们构建了一套风险控制识别系统.本文提出融合步态识别方法与智能终端设备的位置、朝向无关, 同时只过滤出走路-摆动的手中、走路-固定在胸前和走路-裤口兜中3种情况的数据作为步态识别的输入.鞋子的材质类型以及人的情绪都会对用户的步态产生影响, 这也是以后工作要着重解决的问题, 这方面问题增加了对步态影响的因素, 因此, 要想将步态识别与验证模块更好地纳入到操作系统内置服务中, 还需对更多的行为位置组合进行步态分析, 进一步提高识别精度, 确保用户信息安全.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="153">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016729819.nh&amp;v=MTAyOTE0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVXIvSVZGMjZHTFM2RjluTnBwRWJQSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Wang Chendi. Alibaba Internet financial risk control measures [D]. Urumchi: Xinjiang University of Finance and Economics, 2016 (in Chinese) (王晨迪. 阿里巴巴互联网金融风险控制措施研究[D]. 乌鲁木齐: 新疆财经大学, 2016) 
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JKGC201508004&amp;v=MzA4MzZVci9JTHliTWJiRzRIOVRNcDQ5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N24=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Zhang Ping. Tencent mobile payment how to do risk control [J]. Gold Card Project, 2015 (8) : 8- 9 (in Chinese) (张平. 腾讯移动支付如何做风控[J]. 金卡工程, 2015 (8) : 8- 9) 
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601326425&amp;v=MDkzNzQ3SUpsd1ZhaEk9TmlmT2ZiSzdIdEROcVk5RVora0pDSDQ4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Mayagoitia R E, Nene A V, Veltink P H. Accelerometer and rate gyroscope measurement of kinematics: An inexpensive alternative to optical motion analysis systems [J]. Journal of Biomechanics, 2002, 35 (4) : 537- 542
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reliability and Validity of Gait Analysis by Android-Based Smartphone">

                                <b>[4]</b>Nishiguchi S, Yamada M, Nagai K, et al. Reliability and validity of gait analysis by android-based smartphone [J]. Telemedicine and e -Health, 2012, 18 (4) : 292- 296
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201601010&amp;v=MTEwMjBGckNVUkxPZVplVnZGeTduVXIvSUx5dlNkTEc0SDlmTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Shan Yanhu, Zhang Zhang, Huang Kaiqi. Visual human action recognition: History, status and prospects [J]. Journal of Computer Research and Development, 2016, 53 (1) : 93- 112 (in Chinese) (单言虎, 张彰, 黄凯奇. 人的视觉行为识别研究回顾、 现状及展望[J]. 计算机研究与发展, 2016, 53 (1) : 93- 112) 
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES45891DE3186335C11AF05A3A8595AA8B&amp;v=MDQ1OTZ3S2c9TmlmT2ZiZTlGdGpOMi9wR1plTUpEMzg4dkJjU20wbDlUUTdoM1Jvd2NMZmxOTEx0Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod3JxKw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Isho T, Tashiro H, Usuda S. Accelerometry-based gait characteristics evaluated using a smartphone and their association with fall risk in people with chronic stroke [J]. Journal of Stroke and Cerebrovascular Diseases, 2015, 24 (6) : 1305- 1311
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A wearable acceleration sensor system for gait recognition">

                                <b>[7]</b>Liu Rong, Zhou Jianzhong, Liu Ming, et al. A wearable acceleration sensor system for gait recognition [C] //Proc of the 2nd IEEE Conf on Industrial Electronics and Applications. Piscataway, NJ: IEEE, 2007: 2654- 2659
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200902019&amp;v=MjQwMjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0lMeXZTZExHNEh0ak1yWTlFYllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Chen Shi, Ma Tianjun, Gao Youxing. Gait recognition using distributions of silhouette feature [J]. Journal of Computer Research and Development, 2009, 46 (2) : 295- 301 (in Chinese) (陈实, 马天骏, 高有行. 用行人轮廓的分布直方图分类和识别步态 [J]. 计算机研究与发展, 2009, 46 (2) : 295- 301) 
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identifying users of portable devices from gait pattern with accelerometers">

                                <b>[9]</b>Mantyjarvi J, Lindholm M, Vildjiounaite E, et al. Identifying users of portable devices from gait pattern with accelerometers [C] //Proc of the 5th IEEE Int Conf on Acoustics, Speech, and Signal Processing. Piscataway, NJ: IEEE, 2005: 973- 976
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200807009&amp;v=MjczNDhaZVZ2Rnk3blVyL0lMeXZTZExHNEh0bk1xSTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Chen Changyou, Zhang Junping. An iterative gait prototype learning algorithm based on tangent distance [J]. Journal of Computer Research and Development, 2008, 45 (7) : 1177- 1182 (in Chinese) (陈昌由, 张军平. 基于迭代切距离原型学习算法的步态识别 [J]. 计算机研究与发展, 2008, 45 (7) : 1177- 1182
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Metod and continuously wearable noninvasive apparatus for automatically detecting a stroke and other abnormal health conditions">

                                <b>[11]</b>Yang Yingchang. Metod and continuously wearable noninvasive apparatus for automatically detecting a stroke and other abnormal health conditions: US, US20140276123[P]. 2014-09-18
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extreme learning machine:A new learning scheme of feedforward neural networks">

                                <b>[12]</b>Huang Guangbin, Zhu Qinyu, Siew C K. Extreme learning machine: A new learning scheme of feedforward neural networks [C] //Proc of the IEEE Int Joint Conf on Neural Networks. Piscataway, NJ: IEEE, 2004: 985- 990
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300845719&amp;v=MjMxOTlOckk5RmJPOEtDMzB3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSmx3VmFoST1OaWZPZmJLN0h0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Tong Kaiyu, Granat M H. A practical gait analysis system using gyroscopes [J]. Medical Engineering &amp; Physics, 1999, 21 (2) : 87- 94
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using Noninvasive Wearable Computers to Recognize Human Emotions from Physiological Signals">

                                <b>[14]</b>Lisetti C L, Nasoz F. Using noninvasive wearable computers to recognize human emotions from physiological signals [J]. EURASIP Journal on Advances in Signal Processing, 2004 (11) : 1672- 1687
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic gait recognition by symmetry analysis">

                                <b>[15]</b>Hayfron-Acquah J, Nixon M, Carter J. Automatic gait recognition by symmetry analysis[C] //Proc of the 3rd Int Conf on Audio and Video-Based Biometric Person Authenti-cation. Berlin: Springer, 2001: 272- 277
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                Mayagoitia R E, Nene A V, Veltink P H. Accelerometer and rate gyroscope measurement of kinematics: An inexpensive alternative to optical motion analysis systems [J]. Journal of Biomechanics, 2002, 35 (4) : 537- 542
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extreme Learning Machine for Regression and Multiclass Classification">

                                <b>[17]</b>Huang Guangbin, Zhou Hongming, Ding Xiaojian, et al. Extreme learning machine for regression and multiclass classification [J]. IEEE Transactions on Systems Man &amp; Cybernetics: Part B, 2012, 42 (2) : 513- 529
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Individual recognition using gait energy image">

                                <b>[18]</b>Han Ju, Bhanu B. Individual recognition using gait energy image [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2006, 28 (2) : 316- 322
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gait recognition using gait Gaussian image">

                                <b>[19]</b>Arora P, Srivastava S. Gait recognition using gait Gaussian image[C] //Proc of the 2nd Int Conf on Signal Processing and Integrated Networks. Piscataway, NJ: IEEE, 2015: 791- 794
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identification of individual walking Patterns using gaitacceleration">

                                <b>[20]</b>Liu Rong, Duan Zhiguo, Zhou Jianzhong, et al. Identification of individual walking patterns using gait acceleration [C] //Proc of the 1st Int Conf on Bioinformatics and Biomedical Engineering. Piscataway, NJ: IEEE, 2007: 543- 546
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single classifier, OvO, OvA and RCC multiclass classification method in handheld based smartphone gait identification">

                                <b>[21]</b>Raziff A R A, Sulaiman M N, Mustapha N, et al. Single classifier, OvO, OvA and RCC multiclass classification method in handheld based smartphone gait identification[C] //Proc of the 5th Int Conf on Applied Science and Technology. New York: AIP Publishing: 2017
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201903018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201903018&amp;v=MDYzNTlKTHl2U2RMRzRIOWpNckk5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
