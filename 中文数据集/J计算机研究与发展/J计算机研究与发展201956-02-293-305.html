<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243423096250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902006%26RESULT%3d1%26SIGN%3dKNCRyjYEkoqqjUg01wflVxw0zeQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902006&amp;v=MTYxOTBWdkZ5N25XNzdJTHl2U2RMRzRIOWpNclk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#343" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;相关工作&lt;/b&gt; "><b>1</b><b>相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#345" data-title="&lt;b&gt;1.1&lt;/b&gt;&lt;b&gt;弹幕分析及应用&lt;/b&gt;"><b>1.1</b><b>弹幕分析及应用</b></a></li>
                                                <li><a href="#347" data-title="&lt;b&gt;1.2&lt;/b&gt;&lt;b&gt;表征学习模型&lt;/b&gt;"><b>1.2</b><b>表征学习模型</b></a></li>
                                                <li><a href="#351" data-title="&lt;b&gt;1.3&lt;/b&gt;&lt;b&gt;文本表征模型&lt;/b&gt;"><b>1.3</b><b>文本表征模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#353" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;问题定义与方法&lt;/b&gt; "><b>2</b><b>问题定义与方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#355" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;问题定义&lt;/b&gt;"><b>2.1</b><b>问题定义</b></a></li>
                                                <li><a href="#382" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;深度语义表征模型结构&lt;/b&gt;"><b>2.2</b><b>深度语义表征模型结构</b></a></li>
                                                <li><a href="#394" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;基于深度语义表征的弹幕检索&lt;/b&gt;"><b>2.3</b><b>基于深度语义表征的弹幕检索</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#406" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;实验验证&lt;/b&gt; "><b>3</b><b>实验验证</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#407" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;数据集介绍&lt;/b&gt;"><b>3.1</b><b>数据集介绍</b></a></li>
                                                <li><a href="#411" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;实验步骤&lt;/b&gt;"><b>3.2</b><b>实验步骤</b></a></li>
                                                <li><a href="#429" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;对比方法&lt;/b&gt;"><b>3.3</b><b>对比方法</b></a></li>
                                                <li><a href="#438" data-title="&lt;b&gt;3.4&lt;/b&gt;&lt;b&gt;评价指标&lt;/b&gt;"><b>3.4</b><b>评价指标</b></a></li>
                                                <li><a href="#458" data-title="&lt;b&gt;3.5&lt;/b&gt;&lt;b&gt;结果分析&lt;/b&gt;"><b>3.5</b><b>结果分析</b></a></li>
                                                <li><a href="#462" data-title="&lt;b&gt;3.6&lt;/b&gt;&lt;b&gt;案例分析&lt;/b&gt;"><b>3.6</b><b>案例分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#469" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;结论和展望&lt;/b&gt; "><b>4</b><b>结论和展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#337" data-title="图1 视频实时评论">图1 视频实时评论</a></li>
                                                <li><a href="#385" data-title="图2 深度语义表征训练">图2 深度语义表征训练</a></li>
                                                <li><a href="#388" data-title="图3 基于字符级别编码解码的循环神经网络模型">图3 基于字符级别编码解码的循环神经网络模型</a></li>
                                                <li><a href="#402" data-title="图4 基于语义检索的弹幕解释框架">图4 基于语义检索的弹幕解释框架</a></li>
                                                <li><a href="#409" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;弹幕数据集&lt;/b&gt;"><b>表1</b><b>弹幕数据集</b></a></li>
                                                <li><a href="#426" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;模型参数选取&lt;/b&gt;"><b>表2</b><b>模型参数选取</b></a></li>
                                                <li><a href="#460" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;实验验证结果&lt;/b&gt;"><b>表3</b><b>实验验证结果</b></a></li>
                                                <li><a href="#465" data-title="图5 案例展示1">图5 案例展示1</a></li>
                                                <li><a href="#466" data-title="图6 案例展示2">图6 案例展示2</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="525">


                                    <a id="bibliography_1" title="He Ming, Ge Yong, Chen Enhong, et al.Exploring the emerging type of comment for online videos:DanMu[J].ACM Transactions on the Web, 2017, 12 (1) :1-23" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring the emerging type of comment for online videos:DanMu">
                                        <b>[1]</b>
                                        He Ming, Ge Yong, Chen Enhong, et al.Exploring the emerging type of comment for online videos:DanMu[J].ACM Transactions on the Web, 2017, 12 (1) :1-23
                                    </a>
                                </li>
                                <li id="527">


                                    <a id="bibliography_2" title="He Ming, Ge Yong, Wu Le, et al.Predicting the popularity of DanMu-enabled videos:A multi-factor view[C]Proc of the 21st Int Conf on Database Systems for Advanced Applications.Berlin:Springer, 2016:351-366" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting the popularity of DanMu-enabled videos:A multi-factor view">
                                        <b>[2]</b>
                                        He Ming, Ge Yong, Wu Le, et al.Predicting the popularity of DanMu-enabled videos:A multi-factor view[C]Proc of the 21st Int Conf on Database Systems for Advanced Applications.Berlin:Springer, 2016:351-366
                                    </a>
                                </li>
                                <li id="529">


                                    <a id="bibliography_3" title="Figueiredo F, Benevenuto F, Almeida J M.The tube over time:Characterizing popularity growth of YouTube videos[C]Proc of the 4th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2011:745-754" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The tube over time: characterizing popularity growth of youtube videos">
                                        <b>[3]</b>
                                        Figueiredo F, Benevenuto F, Almeida J M.The tube over time:Characterizing popularity growth of YouTube videos[C]Proc of the 4th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2011:745-754
                                    </a>
                                </li>
                                <li id="531">


                                    <a id="bibliography_4" title="Cha M, Mislove A, Gummadi K P.A measurement-driven analysis of information propagation in the flickr social network[C]Proc of the 18th Int Conf on World Wide Web.New York:ACM, 2009:721-730" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A measurement-driven analysis of information propagation in the Flickr social network">
                                        <b>[4]</b>
                                        Cha M, Mislove A, Gummadi K P.A measurement-driven analysis of information propagation in the flickr social network[C]Proc of the 18th Int Conf on World Wide Web.New York:ACM, 2009:721-730
                                    </a>
                                </li>
                                <li id="533">


                                    <a id="bibliography_5" title="Xiao Lin, Ito E, Hirokawa S.Chinese tag analysis for foreign movie contents[C]Proc of the 13th IEEE/ACIS Int Conf on Computer and Information Science.Piscataway, NJ:IEEE, 2014:163-166" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chinese tag analysis for foreign movie contents">
                                        <b>[5]</b>
                                        Xiao Lin, Ito E, Hirokawa S.Chinese tag analysis for foreign movie contents[C]Proc of the 13th IEEE/ACIS Int Conf on Computer and Information Science.Piscataway, NJ:IEEE, 2014:163-166
                                    </a>
                                </li>
                                <li id="535">


                                    <a id="bibliography_6" title="Wu Zechen, Ito E.Correlation analysis between user’s emotional comments and popularity measures[C]Proc of the3rd Int Conf on Advanced Applied Informatics.Piscataway, NJ:IEEE, 2014:280-283" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correlation analysis between user&amp;#39;&amp;#39;s emotional comments and popularity measures">
                                        <b>[6]</b>
                                        Wu Zechen, Ito E.Correlation analysis between user’s emotional comments and popularity measures[C]Proc of the3rd Int Conf on Advanced Applied Informatics.Piscataway, NJ:IEEE, 2014:280-283
                                    </a>
                                </li>
                                <li id="537">


                                    <a id="bibliography_7" title="Zheng Yangyang, Xu Jian, Xiao Zhuo.Application of emotion analysis and visualization method in network video barrage data analysis[J].Modern Library and Information Technology, 2015, 31 (11) :82-90 (in Chinese) (郑飏飏, 徐健, 肖卓.情感分析及可视化方法在网络视频弹幕数据分析中的应用[J].现代图书情报技术, 2015, 31 (11) :82-90) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201511016&amp;v=MjM4MDNVUkxPZVplVnZGeTduVzc3SVBTbmZmN0c0SDlUTnJvOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        Zheng Yangyang, Xu Jian, Xiao Zhuo.Application of emotion analysis and visualization method in network video barrage data analysis[J].Modern Library and Information Technology, 2015, 31 (11) :82-90 (in Chinese) (郑飏飏, 徐健, 肖卓.情感分析及可视化方法在网络视频弹幕数据分析中的应用[J].现代图书情报技术, 2015, 31 (11) :82-90) 
                                    </a>
                                </li>
                                <li id="539">


                                    <a id="bibliography_8" title="Wu Bin, Zhong Erheng, Tan Ben.Crowd-sourced time-sync video tagging using temporal and personalized topic modeling[C]Proc of the 20th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM, 2014:721-730" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crowdsourced time-sync video tagging using temporal and personalized topic modeling">
                                        <b>[8]</b>
                                        Wu Bin, Zhong Erheng, Tan Ben.Crowd-sourced time-sync video tagging using temporal and personalized topic modeling[C]Proc of the 20th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM, 2014:721-730
                                    </a>
                                </li>
                                <li id="541">


                                    <a id="bibliography_9" title="Deng Yang, Zhang Chenxi, Li Jiangfeng.Video shot recommendation model based on emotion analysis using timesync comments[J].Journal of Computer Applications, 2017, 37 (4) :1065-1070 (in Chinese) (邓扬, 张晨曦, 李江峰.基于弹幕情感分析的视频片段推荐模型[J].计算机应用, 2017, 37 (4) :1065-1070) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201704028&amp;v=MjEzMjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzdJTHo3QmQ3RzRIOWJNcTQ5SGJJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Deng Yang, Zhang Chenxi, Li Jiangfeng.Video shot recommendation model based on emotion analysis using timesync comments[J].Journal of Computer Applications, 2017, 37 (4) :1065-1070 (in Chinese) (邓扬, 张晨曦, 李江峰.基于弹幕情感分析的视频片段推荐模型[J].计算机应用, 2017, 37 (4) :1065-1070) 
                                    </a>
                                </li>
                                <li id="543">


                                    <a id="bibliography_10" title="Murray C J L, Vos T, Lozano R, et al.Disability-adjusted life years (DALYs) for 291 diseases and injuries in 21regions, 1990-2010:A systematic analysis for the global burden of disease study 2010[J].Lancet, 2012, 380 (9859) :2197-2223" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14081500000004&amp;v=MTYwMzZVYjdJSmx3Y2F4ST1OaWZPZmJLOEh0bk5xbzlGWk9zUERIdzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Murray C J L, Vos T, Lozano R, et al.Disability-adjusted life years (DALYs) for 291 diseases and injuries in 21regions, 1990-2010:A systematic analysis for the global burden of disease study 2010[J].Lancet, 2012, 380 (9859) :2197-2223
                                    </a>
                                </li>
                                <li id="545">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                    LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436-444</a>
                                </li>
                                <li id="547">


                                    <a id="bibliography_12" title="Deng Li, Li Jinyu, Huang Juiting, et al.Recent advances in deep learning for speech research at Microsoft[C]Proc of the 38th EEE Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2013:8604-8608" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recent advances in deep learning for speech research atMicrosoft">
                                        <b>[12]</b>
                                        Deng Li, Li Jinyu, Huang Juiting, et al.Recent advances in deep learning for speech research at Microsoft[C]Proc of the 38th EEE Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2013:8604-8608
                                    </a>
                                </li>
                                <li id="549">


                                    <a id="bibliography_13" title="Tan Ming, Santos C N, Xiang Bing, et al.Improved representation learning for question answer matching[C]Proc of the 54th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2016:464-473" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved representation learning for question answer matching">
                                        <b>[13]</b>
                                        Tan Ming, Santos C N, Xiang Bing, et al.Improved representation learning for question answer matching[C]Proc of the 54th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2016:464-473
                                    </a>
                                </li>
                                <li id="551">


                                    <a id="bibliography_14" title="Hinton G, Deng Li, Yu Dong, et al.Deep neural networks for acoustic model-ing in speech recognition:The shared views of four research groups[J].IEEE Signal Processing Magazine, 2012, 29 (6) :82-97" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks for Acoustic Modeling in Speech Recognition">
                                        <b>[14]</b>
                                        Hinton G, Deng Li, Yu Dong, et al.Deep neural networks for acoustic model-ing in speech recognition:The shared views of four research groups[J].IEEE Signal Processing Magazine, 2012, 29 (6) :82-97
                                    </a>
                                </li>
                                <li id="553">


                                    <a id="bibliography_15" title="Hermansky H.Speech recognition from spectral dynamics[J].Sadhana-Academy Proceedings in Engineering Sciences, 2011, 36 (5) :729-744" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003800817&amp;v=MDY4ODBTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxWYjNBSVY0PU5qN0Jhck80SHRIUHA0OUZiT29JWTNrNXpCZGg0ajk5&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        Hermansky H.Speech recognition from spectral dynamics[J].Sadhana-Academy Proceedings in Engineering Sciences, 2011, 36 (5) :729-744
                                    </a>
                                </li>
                                <li id="555">


                                    <a id="bibliography_16" title="Bengio Y, Courville A, Vincent P.Representation learning:A review and new perspectives[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1798-1828" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Representation Learning A Review and New Perspectives">
                                        <b>[16]</b>
                                        Bengio Y, Courville A, Vincent P.Representation learning:A review and new perspectives[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1798-1828
                                    </a>
                                </li>
                                <li id="557">


                                    <a id="bibliography_17" title="Hamel P, Lemieux S, Bengio Y, et al.Temporal pooling and multiscale learning for automatic annotation and ranking of music audio[C]Proc of the 12th Int Society for Music Information Retrieval Conf.Piscataway, NJ:IEEE, 2011:729-734" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal pooling and multiscale learning for automatic annotation and ranking of music audio">
                                        <b>[17]</b>
                                        Hamel P, Lemieux S, Bengio Y, et al.Temporal pooling and multiscale learning for automatic annotation and ranking of music audio[C]Proc of the 12th Int Society for Music Information Retrieval Conf.Piscataway, NJ:IEEE, 2011:729-734
                                    </a>
                                </li>
                                <li id="559">


                                    <a id="bibliography_18" title="Hinton G E, Osindero S, Teh Y W.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MjA0NTF1SHlqbVViN0lKbHdjYXhJPU5pZkpaYks5SHRqTXFvOUZaT29OQ1g4eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Hinton G E, Osindero S, Teh Y W.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554
                                    </a>
                                </li>
                                <li id="561">


                                    <a id="bibliography_19" title="Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 25th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deepconvolutional neural networks">
                                        <b>[19]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 25th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105
                                    </a>
                                </li>
                                <li id="563">


                                    <a id="bibliography_20" title="Khan M E, Babanezhad R, Wu Lin, et al.Faster stochastic variational inference using proximal-gradient methods with general divergence functions[J].Journal of Comparative Neurology, 2015, 319 (3) :359-86" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster stochastic variational inference using proximal-gradient methods with general divergence functions">
                                        <b>[20]</b>
                                        Khan M E, Babanezhad R, Wu Lin, et al.Faster stochastic variational inference using proximal-gradient methods with general divergence functions[J].Journal of Comparative Neurology, 2015, 319 (3) :359-86
                                    </a>
                                </li>
                                <li id="565">


                                    <a id="bibliography_21" title="Glorot X, Bordes A, Bengio Y.Domain adaptation for largescale sentiment classification:A deep learning approach[C]Proc of the 28th Int Conf on Machine Learning (ICML-11) .New York:ACM, 2011:513-520" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Domain adaptation for largescale sentiment classification:a deep learning approach">
                                        <b>[21]</b>
                                        Glorot X, Bordes A, Bengio Y.Domain adaptation for largescale sentiment classification:A deep learning approach[C]Proc of the 28th Int Conf on Machine Learning (ICML-11) .New York:ACM, 2011:513-520
                                    </a>
                                </li>
                                <li id="567">


                                    <a id="bibliography_22" title="Lange S, Riedmiller M.Deep auto-encoder neural networks in reinforcement learning[C]Proc of Int Joint Conf Neural Networks (IJCNN) .Piscataway, NJ:IEEE, 2010:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep auto-encoder neural networks in reinforcement learning">
                                        <b>[22]</b>
                                        Lange S, Riedmiller M.Deep auto-encoder neural networks in reinforcement learning[C]Proc of Int Joint Conf Neural Networks (IJCNN) .Piscataway, NJ:IEEE, 2010:1-8
                                    </a>
                                </li>
                                <li id="569">


                                    <a id="bibliography_23" title="Paccanaro A, Hinton G E.Learning distributed representations of concepts using linear relational embedding[J].IEEETransactions on Knowledge&amp;amp;Data Engineering, 2002, 13 (2) :232-244" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning distributed representations of concepts using Linear Relational Embedding">
                                        <b>[23]</b>
                                        Paccanaro A, Hinton G E.Learning distributed representations of concepts using linear relational embedding[J].IEEETransactions on Knowledge&amp;amp;Data Engineering, 2002, 13 (2) :232-244
                                    </a>
                                </li>
                                <li id="571">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                    Bengio Y, Ducharme R, Vincent P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155</a>
                                </li>
                                <li id="573">


                                    <a id="bibliography_25" >
                                        <b>[25]</b>
                                    Collobert R, Weston J, Bottou L, et al.Natural language processing (almost) from scratch[J].Journal of Machine Learning Research, 2011, 12 (1) :2493-2537</a>
                                </li>
                                <li id="575">


                                    <a id="bibliography_26" title="Turian J, Ratinov L, Bengio Y.Word representations:Asimple and general method for semi-supervised learning[C]Proc of the 48th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2010:384-394" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Word representations:a simple and general method for semi-supervised learning.">
                                        <b>[26]</b>
                                        Turian J, Ratinov L, Bengio Y.Word representations:Asimple and general method for semi-supervised learning[C]Proc of the 48th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2010:384-394
                                    </a>
                                </li>
                                <li id="577">


                                    <a id="bibliography_27" title="Cambria E, White B.Jumping NLP curves:A review of natural language processing research[J].IEEEComputational Intelligence Magazine, 2014, 9 (2) :48-57" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Jumping NLP curves:a review of natural language processing research">
                                        <b>[27]</b>
                                        Cambria E, White B.Jumping NLP curves:A review of natural language processing research[J].IEEEComputational Intelligence Magazine, 2014, 9 (2) :48-57
                                    </a>
                                </li>
                                <li id="579">


                                    <a id="bibliography_28" title="Wang Ling, Lu&#237;s T, Marujo L, et al.Finding function in form:Compositional character models for open vocabulary word representation[C]Proc of the 2015Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:1520-1530" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finding function in form:Compositional character models for open vocabulary word representation">
                                        <b>[28]</b>
                                        Wang Ling, Lu&#237;s T, Marujo L, et al.Finding function in form:Compositional character models for open vocabulary word representation[C]Proc of the 2015Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:1520-1530
                                    </a>
                                </li>
                                <li id="581">


                                    <a id="bibliography_29" title="Arel I, Rose D C, Karnowski T P.Deep machine learning-Anew frontier in artificial intelligence research[J].IEEEComputational Intelligence Magazine, 2010, 5 (4) :13-18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep machine learning-A new frontier in artificial intelligence research">
                                        <b>[29]</b>
                                        Arel I, Rose D C, Karnowski T P.Deep machine learning-Anew frontier in artificial intelligence research[J].IEEEComputational Intelligence Magazine, 2010, 5 (4) :13-18
                                    </a>
                                </li>
                                <li id="583">


                                    <a id="bibliography_30" title="Larkey L S, Ballesteros L, Connell M E.Improving stemming for arabic information retrieval:Light stemming and co-occurrence analysis[C]Proc of the 25th Annual Int ACM SIGIR Conf on Research and Development in Information Retrieval.New York:ACM, 2002:275-282" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving stemming for Arabic information retrieval:light stemming and co-occurrence analysis.">
                                        <b>[30]</b>
                                        Larkey L S, Ballesteros L, Connell M E.Improving stemming for arabic information retrieval:Light stemming and co-occurrence analysis[C]Proc of the 25th Annual Int ACM SIGIR Conf on Research and Development in Information Retrieval.New York:ACM, 2002:275-282
                                    </a>
                                </li>
                                <li id="585">


                                    <a id="bibliography_31" title="Sordoni A, Bengio Y, Vahabi H, et al.A hierarchical recurrent encoder-decoder for generative context-aware query suggestion[C]Proc of the 24th ACM Int on Conf on Information and Knowledge Management.New York:ACM, 2015:553-562" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A hierarchical recurrent encoder-decoder for generative context-aware query suggestion">
                                        <b>[31]</b>
                                        Sordoni A, Bengio Y, Vahabi H, et al.A hierarchical recurrent encoder-decoder for generative context-aware query suggestion[C]Proc of the 24th ACM Int on Conf on Information and Knowledge Management.New York:ACM, 2015:553-562
                                    </a>
                                </li>
                                <li id="587">


                                    <a id="bibliography_32" title="Liu Yingfan.Research on approximate nearest neighbor query based on local sensitive Hash[D].Xi’an:Xidian University, 2014 (in Chinese) (刘英帆.基于局部敏感哈希的近似最近邻查询研究[D].西安:西安电子科技大学, 2014) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014319713.nh&amp;v=MDk4MzQ3blc3N0lWRjI2R3JDNUY5Yk5ySkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                        Liu Yingfan.Research on approximate nearest neighbor query based on local sensitive Hash[D].Xi’an:Xidian University, 2014 (in Chinese) (刘英帆.基于局部敏感哈希的近似最近邻查询研究[D].西安:西安电子科技大学, 2014) 
                                    </a>
                                </li>
                                <li id="589">


                                    <a id="bibliography_33" title="Ling Kang.Research on similarity search technology based on location sensitive Hash[D].Nanjing:Nanjing University, 2012 (in Chinese) (凌康.基于位置敏感哈希的相似性搜索技术研究[D].南京:南京大学, 2012) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1012376885.nh&amp;v=MDI1Nzc3N0lWRjI2SExDL0dObkVxcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[33]</b>
                                        Ling Kang.Research on similarity search technology based on location sensitive Hash[D].Nanjing:Nanjing University, 2012 (in Chinese) (凌康.基于位置敏感哈希的相似性搜索技术研究[D].南京:南京大学, 2012) 
                                    </a>
                                </li>
                                <li id="591">


                                    <a id="bibliography_34" title="Liu Genping.Review on locality sensitive hashing in centralized environment[J].Mobile Communications, 2015 (10) :46-51 (in Chinese) (刘根平.集中式环境下的局部敏感哈希算法综述[J].移动通信, 2015 (10) :46-51) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YDTX201510012&amp;v=MDQ1MDRkckc0SDlUTnI0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVzc3SVBDbmY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[34]</b>
                                        Liu Genping.Review on locality sensitive hashing in centralized environment[J].Mobile Communications, 2015 (10) :46-51 (in Chinese) (刘根平.集中式环境下的局部敏感哈希算法综述[J].移动通信, 2015 (10) :46-51) 
                                    </a>
                                </li>
                                <li id="593">


                                    <a id="bibliography_35" title="Belz A, Reiter E.Comparing automatic and human evaluation of NLG systems[C]Proc of the 11th Conf of the European Chapter of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2006:313-320" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparing automatic and human evaluation of NLG systems">
                                        <b>[35]</b>
                                        Belz A, Reiter E.Comparing automatic and human evaluation of NLG systems[C]Proc of the 11th Conf of the European Chapter of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2006:313-320
                                    </a>
                                </li>
                                <li id="595">


                                    <a id="bibliography_36" title="L&#252;Guangyi, Xu Tong, Chen Enhong, et al.Reading the videos:Temporal labeling for crowd sourced time-sync videos based on semantic embedding[C]Proc of the 30th AAAIConf on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:3000-3006" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reading the videos:Temporal labeling for crowdsourced time-sync videos based on semantic embedding">
                                        <b>[36]</b>
                                        L&#252;Guangyi, Xu Tong, Chen Enhong, et al.Reading the videos:Temporal labeling for crowd sourced time-sync videos based on semantic embedding[C]Proc of the 30th AAAIConf on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:3000-3006
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),293-305 DOI:10.7544/issn1000-1239.2019.20170752            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>视频实时评论的深度语义表征方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%B3%95%E6%B0%91&amp;code=41255386&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴法民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E5%B9%BF%E5%A5%95&amp;code=41255387&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕广奕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%B7%87&amp;code=24386328&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘淇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E6%98%8E&amp;code=39737692&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%B8%E6%A0%87&amp;code=31654476&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常标</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E4%BC%9F%E6%A0%8B&amp;code=41255388&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何伟栋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%9F%E8%BE%89&amp;code=41255389&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钟辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B9%90&amp;code=25018558&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张乐</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0002522&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学技术大学软件学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%89%E5%BE%BD%E7%9C%81%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学技术大学计算机学院大数据分析与应用安徽省重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着互联网技术的进步, 以视频实时评论为代表的众包短文本 (又称弹幕) 逐渐流行, 对在线媒体分享平台和娱乐产业都带来了重要影响.针对此类短文本展开研究, 为推荐系统以及人工智能等领域的发展提供了新的机遇, 在各行各业都具有巨大价值.然而在弹幕带来机遇的同时, 理解和分析这种面向视频的众包短文本也面临诸多挑战:视频实时评论的高噪声、不规范表达和隐含语义等特性, 使得传统自然语言处理 (natural language processing, NLP) 技术具有很大局限性, 因此亟需一种容错性强、能刻画短文本深度语义的理解方法.针对以上挑战, 在“相近时间段内的视频实时评论具有相似语义”假设的基础上, 提出了一种基于循环神经网络 (recurrent neural network, RNN) 的深度语义表征模型.该模型由于引入了字符级别的循环神经网络, 避免了弹幕噪声对文本分词带来的影响.通过使用神经网络, 使所得的语义向量能够表达弹幕的隐含语义.在此基础上, 进一步设计了基于语义检索的弹幕解释框架, 同时作为对语义表征结果的应用验证.最后, 设计了多种对比方法, 并采用不同指标对所提出的模型进行充分的验证.该模型能够精准地刻画弹幕短文本的语义, 也证明了关于弹幕相关假设的合理性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E5%AE%9E%E6%97%B6%E8%AF%84%E8%AE%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频实时评论;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%B9%E5%B9%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">弹幕;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度语义表征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E6%A3%80%E7%B4%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义检索;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%97%E7%AC%A6%E7%BA%A7%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">字符级循环神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘淇 (qiliuql@ustc.edu.cn) ;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-09-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2016YFB1000904);</span>
                                <span>国家自然科学基金项目 (61672483, U1605251);</span>
                                <span>中国科学院青年创新促进会会员专项基金项目 (2014299);</span>
                    </p>
            </div>
                    <h1><b>Deep Semantic Representation of Time-Sync Comments for Videos</b></h1>
                    <h2>
                    <span>Wu Famin</span>
                    <span>Lü Guangyi</span>
                    <span>Liu Qi</span>
                    <span>He Ming</span>
                    <span>Chang Biao</span>
                    <span>He Weidong</span>
                    <span>Zhong Hui</span>
                    <span>Zhang Le</span>
            </h2>
                    <h2>
                    <span>School of Software Engineering, University of Science and Technology of China</span>
                    <span>Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science, University of Science and Technology of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the development of Internet, crowdsourcing short texts such as time-sync comments for videos are of significant importance for online media sharing platforms and leisure industry. It also provides a new research opportunity for the evolution of recommender system, artificial intelligence and so on, which have tremendous values for every walk of life. At the same time, there are many challenges for crowdsourcing short text analysis, because of its high noise, non-standard expressions and latent semantic implication. These have limited the application of traditional natural language processing (NLP) techniques, thus it needs a novel short text understanding method which is of high fault tolerance, and can capture the deep semantics. To this end, this paper proposes a deep semantic representation model based on recurrent neural network (RNN) . It can avoid the effect of noise on text segmentation by exploiting the character-based RNN. To achieve the semantic representation, we apply the neural network to represent the latent semantics such that the outputted semantic vectors can deeply reflect the time-sync comments. Then we further design a time-sync comment explanation framework based on semantic retrieval, used for the validation of semantic representation. Finally, we compare them with others baselines, and apply many measures to validate the proposed model. The experimental results show that model can capture the semantics in these short texts more precisely, and the assumptions related to time-sync comments are reasonable.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=time-sync%20comment%20for%20videos&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">time-sync comment for videos;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=bullet-screen&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">bullet-screen;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20semantic%20representation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep semantic representation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20retrieval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic retrieval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=character-based%20recurrent%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">character-based recurrent neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Wu Famin, born in 1988.Master.His main research interests include machine learning method and its application, natural language processing.<image id="510" type="formula" href="images/JFYZ201902006_51000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Lü Guangyi, born in 1990.PhD candidate.His main research interests include deep learning, natural language processing.<image id="512" type="formula" href="images/JFYZ201902006_51200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Qi, born in 1986.PhD.Associate professor.Member of CCF, Member of CCF Big Data Expert Committee.His main research interests include data mining and knowledge discovery, machine learning and application.<image id="514" type="formula" href="images/JFYZ201902006_51400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    He Ming, born in 1988.PhD.His main research interests include recommendation system, natural language processing.<image id="516" type="formula" href="images/JFYZ201902006_51600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Chang Biao, born in 1990.PhD.His main research interests include social networking analysis, personalized recommendation.<image id="518" type="formula" href="images/JFYZ201902006_51800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    He Weidong, born in 1993.Master candidate.His main research interests include natural language processing, deep learning.<image id="520" type="formula" href="images/JFYZ201902006_52000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhong Hui, born in 1994.Master candidate.His main research interests include data mining, random optimization.<image id="522" type="formula" href="images/JFYZ201902006_52200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhang Le, born in 1995.Master candidate.His main research interests include data mining, social network analysis.<image id="524" type="formula" href="images/JFYZ201902006_52400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-09-30</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2016YFB1000904);</span>
                                <span>the National Natural Science Foundation of China (61672483, U1605251);</span>
                                <span>the Youth Innovation Promotion Association of CAS (2014299);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="area_img" id="337">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902006_337.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 视频实时评论" src="Detail/GetImg?filename=images/JFYZ201902006_337.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 视频实时评论  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902006_337.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Time-sync comments for videos</p>

                </div>
                <div class="p1">
                    <p id="338">随着互联网技术的进步, 在线共享媒体已经得到了突飞猛进的发展, 并极大地丰富了人们的生活.与此同时, 一种被称作“弹幕”的新型视频实时评论在国内外视频共享平台中越来越受到大家的关注, 如中国的bilibili、爱奇艺、优酷, 日本的niconico等网站, 弹幕在视频中扮演着极其重要的角色.在这些视频共享平台中, 用户可以在观看视频的同时发送评论消息 (称为弹幕) .与传统评论不同, 弹幕评论可以在视频的播放过程中实时呈现, 增进了用户之间的互动, 改善了用户的体验.弹幕不仅包含文本信息, 还包含该评论在视频中出现的时间信息, 即:允许用户针对视频中的某个片段进行实时评论, 使得在播放视频时, 评论像“大量子弹飞过屏幕”, “弹幕”也因此得名<sup><a class="sup">[1]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="339">弹幕机制具有诸多特点.一方面, 弹幕实时性的特点使得其与视频的联系更加密切, 视频实时评论在很大程度上影响视频的流行程度<sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup>.有研究表明, 视频的流行程度和该视频之前的评论观点以及评论数据量呈正相关性<sup>[<a class="sup">2</a>,<a class="sup">4</a>]</sup>.另一方面, 在观看视频的同时阅读或发送弹幕, 也成为了一种独特的社交方式, 这种观众之间通过评论交流的方式极大地满足了现代人排解寂寞的心理需求, 使得越来越多的用户更青睐于观看带有弹幕的视频.因此, 作为一种众包短文本的代表, 弹幕拉近了人与互联网信息的关系, 也促进了人与人之间的交流, 成为以人为中心的媒体信息交互纽带.总的来说, 以弹幕为代表的众包短文本, 对于在线媒体分享平台, 甚至娱乐产业都有着重要意义, 而针对此类短文本展开研究, 为推荐系统、计算广告学以及人工智能等领域的发展提供了新的机遇, 对于互联网、经济、教育、科研等行业具有巨大价值.</p>
                </div>
                <div class="p1">
                    <p id="340">然而在弹幕带来新机遇的同时, 这种面向视频的众包短文本分析也存在着诸多挑战, 如图1所示.首先, 由数以万计的用户生成的弹幕数据具有很高的噪声.弹幕的噪声主要源于2方面:一方面, 用户的弹幕内容具有随意性.在共享视频评论的场景下, 用户受到其他用户或者自身情绪的影响, 可能发布与视频内容无关的弹幕, 如:“看饿了…”、“<i>Σ</i> (° △ °|||) ︴”等.另一方面, 用户的表达方式具有随意性.在通常情况下, 用户不会像一般的评论那样刻意严谨的对待所发布的弹幕内容, 进而会产生一些输入的错误, 比如“何晕东好堎”, 事实上是用户想表达“何润东好嫩”, 但由于拼写时带有方言导致产生了错别字.这些表达均具有偶然性, 不属于用户约定行为, 因此没有规律难以过滤, 给弹幕的理解和研究带来了困难.其次, 弹幕中充斥着大量网络用语.视频弹幕来源于网络共享视频平台, 具有网络平台的共有特性, 存在大量网络用语和不规范的表达.比如数字的谐音:“233”来源于“哈哈”笑的表情库, 被用来指代“啊哈哈”, 表示大笑的意思, “7456”则指代“气死我了”等;英语拼音的谐音:“海皮”在视频评论中可能就是指代“happy”缩写;汉字的谐音:由于为了增加幽默效果和方言种类繁多等导致的替用, 比如“内流满面”指代“泪流满面”等.这些网络用语的大量使用进一步增加了弹幕相关研究的困难.最后, 弹幕文本中普遍蕴含着隐含语义.这一点在以动漫为代表的ACG (animation, comic, game) 视频中体现得尤为明显.由于视频观众中存在各种小众群体, 这些群体经过长时间的交流, 约定俗成了一系列独特的表达, 如“前方高能”、“失踪人口回归”;与此同时, 对于像“元首”、“哲学家”、“老师”等词语在某些特定剧情下则具有和原来完全不同的含义 ;而诸如“鬼畜”、“蓝蓝路”等则属于完全新造的词语.弹幕中的隐含语义, 有悖于正常的自然语言, 如何正确理解弹幕的深度含义是对弹幕及视频进行分析建模的最大挑战.综上所述, 视频实时评论的高噪声、不规范表达和隐含语义等特性, 使得传统自然语言处理 (natural language processing, NLP) 技术具有很大局限性, 因此亟需一种容错性强、能刻画短文本的深度语义理解方法.</p>
                </div>
                <div class="p1">
                    <p id="341">针对以上挑战, 本文提出了一种基于循环神经网络 (recurrent neural network, RNN) 的深度语义表征模型.该模型建立在“相近时间段内的视频实时评论具有相似语义”的假设上, 实现将离散的、不定长的文本序列映射为连续取值的、低维的语义向量, 使得语义向量能够精准地刻画对应弹幕所表达的语义信息.特别地, 该模型由于引入了字符级别的循环神经网络, 避免了弹幕噪声对文本分词带来的影响;而在实现语义表征的过程中, 通过使用神经网络, 使得所得的语义向量能够对弹幕进行更深层次的刻画, 表达其隐含语义.在此基础上, 针对弹幕文本具有隐含语义的特点, 本文进一步设计了基于语义检索的弹幕解释框架, 同时作为对语义表征结果的验证.该框架利用语义向量创建索引, 从而对于给定弹幕, 通过检索与其语义相似但表达不同的弹幕来对其进行解释.</p>
                </div>
                <div class="p1">
                    <p id="342">最后, 本文设计了序列自编码、词袋特征索引、编码器解码器等多种对比方法, 并通过BLEU (bilingual evaluation understudy) 、流畅度、多样性等多种指标以及人工评价对本文所提出的模型进行充分验证.实验结果表明这种基于循环神经网络的深度语义表征模型能够精准地刻画弹幕短文本的语义, 也证明了关于弹幕相关假设的合理性.</p>
                </div>
                <h3 id="343" name="343" class="anchor-tag"><b>1</b><b>相关工作</b></h3>
                <div class="p1">
                    <p id="344">本节将从弹幕分析及应用、表征学习模型、文本表征模型这3个方面介绍视频实时评论 (弹幕) 的相关工作.</p>
                </div>
                <h4 class="anchor-tag" id="345" name="345"><b>1.1</b><b>弹幕分析及应用</b></h4>
                <div class="p1">
                    <p id="346">弹幕视频是一种新型视频互动方式, 以其独特的互动交流方式备受广大互联网和视频爱好者的喜爱, 并迅速风靡国内外视频共享平台, 如对于中国的bilibili、爱奇艺、优酷, 日本的niconico等网站, 弹幕视频扮演着极其重要的角色.然而目前关于弹幕视频的研究还很少.国内外对于弹幕的分析大多停留在基于弹幕的统计特征, 以统计和自然语言处理技术, 研究视频实时评论情况以及视频评论和视频之间的关系.其中, 基于统计的有:文献[5]提出基于统计的方法识别一个弹幕评论的词汇是否是个外来词或视频内容无关词.基于自然语言处理的有:文献[6]借助自然语言处理技术和统计学知识, 研究视频评论的情感和视频流行度之间的关系;郑飏飏等人<sup><a class="sup">[7]</a></sup>利用自然语言处理技术, 提取弹幕中的情感数据实现对视频的评论的情感可视化, 获取网络视频的情感特征和走势, 并通过给视频打情感标签的方式, 达到从情感角度实现视频的检索.文献[8]提出基于时间的个性化主题模型 (TPTM) , 该模型结合视频评论, 为相应时间段的视频生成主题标签.文献[9]提出了基于语义关联的视频标签提取方法, 通过对弹幕数据的相似度分析, 建立语义关联图, 根据关联图的模型获取视频的主题分布给视频打标签, 同时根据提取的弹幕主题信息, 过滤跟视频无关的弹幕.文献[10]提出了基于隐语义模型的网络视频推荐算法 (video recommender fusing comment analysis and latent factor model, VRFCL) , 从网络视频入手, 分析观看者对某特定视频的感情倾向值, 抽取评论关键词作为视频元数据, 从隐语义特征的角度建立&lt;用户-视频&gt;二元组.然而, 目前这些基于统计和自然语言处理技术的研究, 并不能解决视频实时评论的高噪声、不规范表达和隐含语义等问题.</p>
                </div>
                <h4 class="anchor-tag" id="347" name="347"><b>1.2</b><b>表征学习模型</b></h4>
                <div class="p1">
                    <p id="348">深度学习是一种多层描述的表征学习, 把原始数据通过一些简单的非线性的模型转变成为更高层次的、更加抽象的表达.通过积累足够多的上述表征转化, 机器能学习非常复杂的函数<sup><a class="sup">[11]</a></sup>.深度学习中的重要思想就是自动提取特征, 也就是表征学习, 故深度学习有时也被称作表征学习或者无监督特征学习<sup><a class="sup">[12]</a></sup>, 通过设定所需达到的学习目标, 自动地从原始数据学习有效的特征, 而无需具体的领域知识作为先导<sup><a class="sup">[13]</a></sup>.近些年, 深度学习在语音识别、图像处理、文本处理等多个领域取得重大进展, 证明了表征模型是个很有效的处理方式<sup><a class="sup">[14]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="349">学术界和工业界的研究者, 将深度学习、表征学习等算法应用在语音领域, 通过将语音特征学习和语音识别的目标转化为对原始光谱或可能的波形的特征学习的过程<sup><a class="sup">[15]</a></sup>, 给语音识别带来巨大影响和突破性的成果.2012年, 微软发布了新版本的音频视频搜索服务语音系统, 正是基于表征学习<sup><a class="sup">[16]</a></sup>.在音乐方面, 表征学习的应用使得在复调转录中击败了其他技术, 获得了极大改善, 并赢得了MIREX音乐信息检索比赛<sup><a class="sup">[17]</a></sup>.图像识别方面, 早在2006年通过MNIST数字图像分类, 以1.4%的错误率优势超越了支持向量机<sup><a class="sup">[18]</a></sup>, 从此在数字图像识别方面表征学习一直保持独特的优势.鉴于表征学习在数字图像识别方面的效果, 相关专家学者利用表征学习从数字图像的识别, 应用到自然图像的识别.比如在ImageNet数据集上, 通过表征学习实现了将错误率从26.1%下降到15.3%的突破<sup><a class="sup">[19]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="350">自编码器是深度学习中非常常见的一个表征模型框架.该框架最早使用在机器翻译领域, 机器翻译是把一种语言转换成另一种语言的过程, 即输入一个文本序列, 输出另外一个语义相同但是结构不同的文本序列.随着自动编码框架在机器翻译领域的成功应用并取得不错的效果, 该框架已经从机器翻译扩展到其他领域.输入数据到编码器, 解码器还原出原始的输入数据, 自动编码器可以分为2个部分, 即编码器和解码器.编码器部分生成语义向量, 当前使用最多的表示技术是循环神经网络, 实际应用过程中根据处理问题的情况, 经常用到是基于循环神经网络的变种模型:长短期记忆神经网络 (long short-term memory, LSTM) 、门控制单元循环神经网络 (gated recurrent unit, GRU) 、双向循环神经网络 (bidirectional recurrent neural network, BiRNN) 等模型.解码器是对编码器生成的序列进行解码的过程, 最常见的模型是循环神经网络语言模型 (recurrent neural network language model, RNNLM) <sup><a class="sup">[20]</a></sup>, 在自然语言处理中具有很好的效果, 越来越受到自然语言处理相关领域的人员的重视.Glorot等人<sup><a class="sup">[21]</a></sup>通过提取出评论的深层特征, 解决了传统文本分类算法跨领域分类不理想的问题.文献[22]采用深度自编码器, 通过改进词汇的翻译模型, 从而有效地提取特征集, 在机器翻译过程中取得很好的效果.</p>
                </div>
                <h4 class="anchor-tag" id="351" name="351"><b>1.3</b><b>文本表征模型</b></h4>
                <div class="p1">
                    <p id="352">近年来, 随着深度学习技术在自然语言领域的发展, 词表征模型由于其低维、连续的特征表示方式和挖掘文本潜在语义的能力, 在自然语言处理领域越来越受到重视.通过对文本数据进行深层次的抽象和挖掘, 建立数据表征来进行特征表示和复杂映射, 从而训练有用的表征模型.Hinton等人<sup><a class="sup">[23]</a></sup>引入分布式表征用于符号数据的分布式表示, Bengio等人<sup><a class="sup">[24]</a></sup>首次将词分布式表征通过神经网络模型应用于上下文的统计语义模型.基于学习词的分布式表征又称词嵌入, Collobert等人<sup><a class="sup">[25]</a></sup>通过增加卷积层开发了senna系统, 实现了在语言建模、词性标注、命名实体识别、语法分析等任务中共享表征.文献[26-27]指出自然语言处理领域通过将词、字符转化为低维的实数向量的词嵌入技术, 使得处理结果得到明显改进和提升.文献[28]设计了一个字符级别的双向LSTM的循环神经网络 (RNN) 模型, 该模型在语言表征和词性标注 (part-of-speech tagging, POS) 标签方面展现出强大的性能.在机器翻译领域, 文献[29]对原输入数据或目标输出数据使用字符级别的RNN结构, 产生一个“字符-字符”的翻译生成结构.在隐含语义表示方面, 深度语义匹配模型 (deep structured semantic models, DSSM) <sup><a class="sup">[10]</a></sup>利用多层神经网络把搜索关键词和文档注入到低维空间, 通过计算相似度, 挖掘隐含语义.在信息检索领域, 使用字符的<i>n</i>-gram作为神经网络的输入, 进行信息检索模型的训练<sup><a class="sup">[30]</a></sup>.</p>
                </div>
                <h3 id="353" name="353" class="anchor-tag"><b>2</b><b>问题定义与方法</b></h3>
                <div class="p1">
                    <p id="354">传统自然语言处理技术具有很大局限性, 无法解决视频实时评论的高噪声、不规范表达和隐含语义等特性, 因此亟需一种容错性强、能刻画深度语义的短文本理解方法的需求.本文基于“相近时间段内的视频实时评论具有相似语义”的假设, 提出了一种基于循环神经网络 (RNN) 的深度语义表征模型, 并设计了基于语义检索的弹幕解释框架.本节对相关问题、深度语义表征模型、基于语义检索的弹幕解释框架进行介绍.</p>
                </div>
                <h4 class="anchor-tag" id="355" name="355"><b>2.1</b><b>问题定义</b></h4>
                <div class="p1">
                    <p id="356">弹幕跟视频和时间具有高度关联性, 按如下格式符号化一个弹幕:<i>D</i>=&lt;<i>Vid</i>, <i>Did</i>, <i>s</i>, <i>t</i>&gt;, 其中<i>Vid</i>是弹幕所在视频的标识符, <i>Did</i>是弹幕的标识符, <i>s</i>是弹幕的文本内容, <i>t</i>为弹幕的时间, 该时间为弹幕在视频中出现的时刻.</p>
                </div>
                <div class="p1">
                    <p id="357"><b>定义1</b>. 视频实时评论的深度语义表征.给定的弹幕<i>D</i>=&lt;<i>Vid</i>, <i>Did</i>, <i>s</i>, <i>t</i>&gt;, 该表征的目的是通过<i>D</i>学习一个表征模型 (编码器) <i>φ</i>, 使得对于任意弹幕<i>Did</i>可获取相应的语义向量<b><i>v</i></b><sub><i>i</i></sub>=<i>φ</i> (<i>s</i><sub><i>i</i></sub>) , 并且满足对任意的<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>的真实语义相似或具有相关性, 则<b><i>v</i></b><sub><i>i</i></sub>, <b><i>v</i></b><sub><i>j</i></sub>具有相近的距离, 否则<b><i>v</i></b><sub><i>i</i></sub>, <b><i>v</i></b><sub><i>j</i></sub>距离较远.</p>
                </div>
                <div class="p1">
                    <p id="358">视频实时评论的深度语义表征模型学习过程中, 需要使用语义相似或相关弹幕进行训练, 关于语义相似性弹幕的获取存在如下2个挑战:</p>
                </div>
                <div class="p1">
                    <p id="359">1) 若语义相似或相关弹幕的获取采取人工标注的方式获取, 将会带来巨大的人力成本, 同时也会限制模型的实际应用范围.</p>
                </div>
                <div class="p1">
                    <p id="360">2) 鉴于模型的实际应用性, 需能自动获取语义相似弹幕.然而, 如何使选取的语义相似弹幕具有最佳近似语义相似性, 是语义相似弹幕获取的最大挑战.</p>
                </div>
                <div class="p1">
                    <p id="361">鉴于语义相似弹幕获取的挑战, 本文从弹幕的特征出发, 分析弹幕数据的特点.弹幕实时性的特点, 使得其与视频的联系更加密切, 导致视频的同一个情节、一个画面出现的弹幕大多数都是基于这个情节或者画面的评论;另一方面, 在观看视频的同时阅读或发送弹幕, 也成为了一种独特的社交方式, 有时候, 弹幕的内容不一定是针对视频内容的评价, 很可能是弹幕发送者之间的对话, 也有可能出现某个观众很感兴趣的弹幕, 其他弹幕发送者对该弹幕的评价.然而, 不管是对视频内容的评价还是弹幕发送者之间的交互, 特定时间内的弹幕一般都具有相似性.通过对弹幕特性的研究和大量的统计, 本文提出弹幕语义相似性假设.</p>
                </div>
                <div class="p1">
                    <p id="362"><b>假设1</b>. 弹幕数据语义相似性假设.基于视频的实时评论是对视频内容的评价或弹幕发送者之间的交互, 往往一个情节、一个画面中一起出现的评论具有语义相似性, 相近时间段内的实时评论具有语义相似.</p>
                </div>
                <div class="p1">
                    <p id="363">视频中会有视频场景突然转换的场景, 往往伴随着弹幕的语义也会跟着转换.同时, 当出现观众感兴趣的弹幕, 往往也会伴随着弹幕话题的转变.这种弹幕语义的突然转变, 是弹幕语义相似性假设的一大挑战.然而, 无论视频情节、画面还是弹幕发送者的话题都具有连续性, 当弹幕数据量达到一定时, 这种干扰情况比例很少.接下来, 通过定义对语义相似弹幕的获取进行量化, 以便能通过实验对弹幕语义数据语义相似性假设的合理性和科学性进行验证.</p>
                </div>
                <div class="p1">
                    <p id="364"><b>定义2</b>. 语义相似弹幕.对有&lt;<i>Vid</i>, <i>Did</i>, <i>s</i>, <i>t</i>&gt;格式的弹幕, 若<i>Vid</i><sub><i>i</i></sub>=<i>Vid</i><sub><i>j</i></sub>, |<i>t</i><sub><i>i</i></sub>-<i>t</i><sub><i>j</i></sub>|&lt;<i>δ</i>, 则<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>为语义相似弹幕.其中, <i>δ</i>的取值需根据实验结果, 选取合适的大小.</p>
                </div>
                <div class="p1">
                    <p id="365">由语义相似弹幕的定义, 可以得到语义相似弹幕集合<i>G</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>n</i></sub>}, ∀<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>∈<i>G</i>, <i>i</i>, <i>j</i>∈{1, 2, …, <i>n</i>}, 有|<i>t</i><sub><i>i</i></sub>-<i>t</i><sub><i>j</i></sub>|&lt;<i>δ</i>.</p>
                </div>
                <div class="p1">
                    <p id="366">接下来介绍实现深度语义表征的方法和损失函数.根据语义相似弹幕的定义, 可以对弹幕按时间切分, 寻找语义相似弹幕.基于自编码是深度学习领域非常常用的框架, 已成功用于降维和信息检索任务并且在机器翻译、文本生成方面具有独特的优势, 为了挖掘语义相似弹幕的深层语义表征, 本文采用自编码方法, 学习输入弹幕数据的特征, 生成语义向量.下面介绍弹幕的自编码.</p>
                </div>
                <div class="p1">
                    <p id="367">对于弹幕文本<i>s</i>, 自编码过程如下:</p>
                </div>
                <div class="p1">
                    <p id="368" class="code-formula">
                        <mathml id="368"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">v</mi><mo>=</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><msup><mi>s</mi><mo>′</mo></msup><mo>=</mo><mi>ψ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="369"> (1) </p>
                </div>
                <div class="p1">
                    <p id="370">其中, <i>φ</i>表示编码过程, <i>φ</i> (<i>s</i>) 为对弹幕<i>s</i>的编码, 生成<i>s</i>的语义向量<b><i>v</i></b>;<i>ψ</i>表示解码过程, <i>ψ</i> (<b><i>v</i></b>) 为对弹幕语义向量<b><i>v</i></b>的解码, 解码生成弹幕<i>s</i>′.</p>
                </div>
                <div class="p1">
                    <p id="371">实现深度语义表征, 必须保证弹幕在经过自编码进行重构的同时, 保证语义相似弹幕的语义向量距离相近, 通过选取合适的损失函数, 使得对任意的语义相似的弹幕<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>, 其语义向量<b><i>v</i></b><sub><i>i</i></sub>, <b><i>v</i></b><sub><i>j</i></sub>具有相近的距离.下面介绍深度语义表征的损失函数.</p>
                </div>
                <div class="p1">
                    <p id="372"><i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>为从语义相似弹幕集合<i>G</i>中随机选取的弹幕, <i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>∈<i>G</i>, 对弹幕数据长度进行处理, 设置成定长.弹幕<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>由字符组成, <i>s</i><sub><i>i</i></sub>={<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>}, <i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub> 为<i>s</i><sub><i>i</i></sub>的字符序列, <i>s</i><sub><i>j</i></sub>={<i>c</i>′<sub>1</sub>, <i>c</i>′<sub>2</sub>, …, <i>c</i>′<sub><i>n</i></sub>}, <i>c</i>′<sub>1</sub>, <i>c</i>′<sub>2</sub>, …, <i>c</i>′<sub><i>n</i></sub>为<i>s</i><sub><i>j</i></sub>的字符序列, <i>n</i>为字符序列的长度, <b><i>v</i></b><sub><i>i</i></sub>, <b><i>v</i></b><sub><i>j</i></sub>分别为<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>的语义向量.</p>
                </div>
                <div class="p1">
                    <p id="373"><b>定义3</b>. 深度语义表征的损失函数.深度语义表征的损失函数由弹幕重构的损失函数<i>L</i><sub>rec</sub>和相似弹幕语义向量的距离损失函数<i>L</i><sub>sem</sub>构成.其中弹幕重构的损失函数为每一步的似然函数的负对数之和, 如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="374" class="code-formula">
                        <mathml id="374"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext></mrow></msub><mo>=</mo><mo>-</mo><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mspace width="0.25em" /></mstyle><mi>lg</mi><mtext> </mtext><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo></mrow></mrow><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mspace width="0.25em" /></mstyle><mi>lg</mi><mtext> </mtext><mi>Ρ</mi><mo stretchy="false"> (</mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mrow><mo>, </mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><msup><mi>c</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="375">语义向量之间的距离采用余弦相似度, 两语义向量余弦距离越大越相似.语义向量距离的损失函数<i>L</i><sub>sem</sub>如式 (3) 所示, 以达到训练过程中可以不断最小化损失率.</p>
                </div>
                <div class="p1">
                    <p id="376"><mathml id="377"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>e</mtext><mtext>m</mtext></mrow></msub><mo>=</mo><mn>1</mn><mo>-</mo><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>⋅</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mrow></math></mathml>. (3) </p>
                </div>
                <div class="p1">
                    <p id="378">深度语义表征的训练过程就是不断地最小化<i>L</i><sub>rec</sub>+<i>L</i><sub>sem</sub>的损失率以达到收敛.</p>
                </div>
                <div class="p1">
                    <p id="379">在语义相似弹幕深度语义表征的基础上, 针对弹幕文本具有隐含语义的特点, 本文进一步设计了基于语义检索的弹幕解释框架, 同时作为对深度语义表征结果的验证.本文对基于语义检索的弹幕解释框架的语义相似弹幕检索过程给出如定义4所示的定义.</p>
                </div>
                <div class="p1">
                    <p id="380"><b>定义4</b>. 基于语义的相似弹幕检索.初始弹幕<i>s</i><sub>+</sub>, 使用已经训练好的深度语义表征模型进行编码<i>φ</i> (<i>s</i><sub>+</sub>) , 生成<i>s</i><sub>+</sub>的语义向量<b><i>v</i></b><sub>+</sub>, 在深度语义表征空间中检索与语义向量<b><i>v</i></b><sub>+</sub>距离最近的<i>k</i>个语义向量, 组成语义相似向量集合{<b><i>v</i></b><sub>1</sub>, <b><i>v</i></b><sub>2</sub>, …, <b><i>v</i></b><sub><i>k</i></sub>}, 分别对检索到的语义向量使用已经训练好的深度语义表征模型进行解码, 生成<i>s</i><sub>+</sub>的语义相似弹幕集合<i>s</i><sub><i>s</i><sub>+</sub></sub>.</p>
                </div>
                <div class="p1">
                    <p id="381">基于语义检索的弹幕解释框架可以检索初始弹幕的语义相似弹幕, 以解决弹幕文本具有隐含语义不易理解的问题, 同时, 通过比较初始弹幕与初始弹幕检索到的语义向量之间的语义相似性, 对语义表征结果的应用验证进行验证.</p>
                </div>
                <h4 class="anchor-tag" id="382" name="382"><b>2.2</b><b>深度语义表征模型结构</b></h4>
                <div class="p1">
                    <p id="383">基于假设1:同一个视频的弹幕, 如果时间间隔小于<i>δ</i>, 为语义相似弹幕.本节对深度语义表征模型训练过程和模型结构进行相关介绍.</p>
                </div>
                <div class="p1">
                    <p id="384">对所有弹幕划分为模型训练数据和测试数据, 其中模型训练数据用来进行深度语义表征模型训练.由定义2, 若<i>Vid</i><sub><i>i</i></sub>=<i>Vid</i><sub><i>j</i></sub>, |<i>t</i><sub><i>i</i></sub>-<i>t</i><sub><i>j</i></sub>|&lt;<i>δ</i>, 则<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>为相似弹幕, 因此对弹幕按所在的视频<i>Vid</i>进行聚合, 同一个视频的弹幕聚合在一起并按弹幕出现的时间进行排序切分, 得到语义相似弹幕集合<i>G</i>.每次从语义相似弹幕集合中随机取2个弹幕<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>进行深度语义表征训练, 训练过程如图2.语义相似弹幕<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>首先经过编码器 (encoder) 进行编码, 生成语义向量<b><i>v</i></b><sub><i>i</i></sub>, <b><i>v</i></b><sub><i>j</i></sub>;接着<b><i>v</i></b><sub><i>i</i></sub>, <b><i>v</i></b><sub><i>j</i></sub>经过解码器 (decoder) 生成弹幕<i>s</i>′<sub><i>i</i></sub>, <i>s</i>′<sub><i>j</i></sub>;最后, 通过模型的损失率变化情况和输出弹幕<i>s</i>′<sub><i>i</i></sub>, <i>s</i>′<sub><i>j</i></sub>的情况, 判断模型的训练情况, 当损失率收敛时, 选取此时的模型.</p>
                </div>
                <div class="area_img" id="385">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902006_385.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 深度语义表征训练" src="Detail/GetImg?filename=images/JFYZ201902006_385.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 深度语义表征训练  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902006_385.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Deep semantic representation training</p>

                </div>
                <div class="p1">
                    <p id="386">为了避免弹幕噪声对文本分词带来的影响和实现语义表征使得所得的语义向量能够对弹幕进行更深层次的刻画, 表达其隐含含义, 引入字符级别的循环神经网络, 设计了基于字符级别编码解码的循环神经网络模型, 以一对字符级循环神经网络作为编码器和解码器, 弹幕以字符的形式输入到循环神经网络.该模型隐层包括字符表征层、GRU单元, 如图3所示.其中<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>为弹幕<i>s</i>的字符序列, <i>c</i>′<sub>1</sub>, <i>c</i>′<sub>2</sub>, …, <i>c</i>′<sub><i>n</i></sub>为语义向量解码的弹幕<i>s</i>′的字符序列, <i>α</i>, <i>β</i>分别为字符表征的维度和弹幕语义表征的维度, 其中<i>α</i>, <i>β</i>的取值大小需要根据模型训练结果选取.</p>
                </div>
                <div class="p1">
                    <p id="387">接下来从字符表征层、GRU单元、编码器、解码器4个方面, 对基于字符级别编码解码的循环神经网络模型进行介绍.</p>
                </div>
                <div class="area_img" id="388">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902006_388.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于字符级别编码解码的循环神经网络模型" src="Detail/GetImg?filename=images/JFYZ201902006_388.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于字符级别编码解码的循环神经网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902006_388.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 RNN model based on character-based encode-decode</p>

                </div>
                <div class="p1">
                    <p id="389">1) 字符表征层.字符表征层是一个线性 (linear model) 结构模型.字符表征输入为字符<i>c</i><sub><i>i</i></sub>, 转化成字符<i>c</i><sub><i>i</i></sub>的one-hot向量<b><i>x</i></b><sub><i>i</i></sub>, 向量的维度等于词表的大小<i>m</i>, 是个高维稀疏向量, 其中词表为模型训练数据和测试数据中所有字符的无重复的集合, 词表的大小即集合的字符个数.通过分布式表示 (distributed representations) 将高维稀疏向量<b><i>x</i></b><sub><i>i</i></sub>转化为<i>α</i>维分布式表示向量<b><i>l</i></b><sub><i>i</i></sub>, 以达到降维.弹幕逐字符输入字符表征层, 通过转化one-hot向量并进行分布式表示, 最终字符表征层的输出为该字符的分布式表示向量, 作为GRU单元的输入.</p>
                </div>
                <div class="p1">
                    <p id="390">2) GRU单元.在传统RNN模型中, 输入<b><i>l</i></b><sub><i>i</i></sub>, 第<i>i</i>步的值为<b><i>g</i></b><sub><i>i</i></sub>=<i>σ</i> (<b><i>Ul</i></b><sub><i>i</i></sub>+<b><i>Wg</i></b><sub><i>i</i>-1</sub>) , 其中<b><i>U</i></b>为<b><i>l</i></b><sub><i>i</i></sub>作为输入时的权重, <b><i>g</i></b><sub><i>i</i>-1</sub>为上一步的值, <b><i>W</i></b>为上一步值<b><i>g</i></b><sub><i>i</i>-1</sub>作为本步输入时的权重, <i>σ</i>为非线性激活函数.针对传统RNN难以保存长距离信息的缺点, LSTM和GRU, 通过在隐藏层计算时, 引入门 (gate) 的机制来解决RNN的梯度消失的问题, 以达到处理长序列依赖.GRU<sup><a class="sup">[31]</a></sup>可以看作是LSTM的变种, 它的门单元结构与LSTM非常相似, 都在一定程度上解决了长距离依赖问题, 使梯度可以更好地传播而不用面临太多梯度衰减的影响.GRU将LSTM中的遗忘门和输入门用更新门替代, GRU需要的参数较少, 训练速度较快, 而且需要的样本也较少.LSTM具有较多的参数, 当大量样本的情况, 可能会很难训练得到最优模型.因此采用GRU处理弹幕数据, GRU通过更新门, 决定是否保留上一步的状态和是否接受此本步的外部输入.GRU单元接受字符表征层的输出和GRU单元上一步的值作为输入, 输出是维度为<i>β</i>的向量.</p>
                </div>
                <div class="p1">
                    <p id="391">3) 编码器.针对弹幕<i>s</i>={<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>}中的字符<i>c</i><sub><i>i</i></sub>, <i>i</i>=1, 2, …, <i>n</i>, 逐字符输入解码器转化为一个维度为词表大小<i>m</i>的高维稀疏one-hot向量<b><i>x</i></b><sub><i>i</i></sub>, <i>i</i>=1, 2, …, <i>n</i>, 经过线性映射转化为<i>α</i>维的分布式表示向量<b><i>l</i></b><sub><i>i</i></sub>, <i>i</i>=1, 2, …, <i>n</i>.进入GRU单元层, GRU单元根据上一步的值和本步字符表征层的输出, 生成此刻的输出<b><i>g</i></b><sub><i>i</i></sub>, <b><i>g</i></b><sub><i>i</i></sub>=<i>ENC</i><sub>GRU</sub> ({<b><i>l</i></b><mathml id="392"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>}) .编码器的结果为GRU单元的最后一个输出<b><i>g</i></b><sub><i>n</i></sub>, 即语义向量<b><i>v</i></b>=<b><i>g</i></b><sub><i>n</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="393">4) 解码器.<b><i>g</i></b>′<sub><i>i</i></sub>为GRU单元的第<i>i</i>步的状态, 是网络的记忆单元.<b><i>g</i></b>′<sub><i>i</i></sub>根据当前字符表征层的输出与上一步的GRU单元的值进行计算.当计算<b><i>g</i></b>′<sub>0</sub>时, 需要用到<b><i>g</i></b>′<sub>-1</sub>, <b><i>l</i></b>′<sub>-1</sub>, 本模型解码过程中<b><i>g</i></b>′<sub>-1</sub>=<b><i>v</i></b>, <b><i>v</i></b>为弹幕语义向量, 作为<i>i</i>=0步的上一步的状态.而此时由于<i>c</i>′<sub>-1</sub>不存在, GRU单元将输入<b><i>l</i></b>′<sub>-1</sub>置为<i>α</i>维的<b>0</b>向量.<i>i</i>≠0时的输入为前一步生成的字符<i>c</i>′<sub><i>n</i>-1</sub>经过字符表征层转化成的<i>α</i>维分布式表示的向量<b><i>l</i></b>′<sub><i>i</i>-1</sub>.</p>
                </div>
                <h4 class="anchor-tag" id="394" name="394"><b>2.3</b><b>基于深度语义表征的弹幕检索</b></h4>
                <div class="p1">
                    <p id="395">在基于循环神经网络 (RNN) 的深度语义表征模型的基础上, 进一步设计了基于语义检索的弹幕解释框架.该框架利用语义向量创建深度表征空间, 从而对给定的初始弹幕, 通过检索与其语义相似但表达不同的上下文相关弹幕来对其进行解释, 同时作为对语义表征结果的应用验证.</p>
                </div>
                <div class="p1">
                    <p id="396">对弹幕数据, 划分为训练数据和测试数据, 训练集用于建立深度语义表征空间, 测试数据中的弹幕作为初始弹幕, 通过基于空间划分的索引, 对深度语义表征空间检索其上下文相关的弹幕.其中上下文相关弹幕为初始弹幕通过基于语义检索的弹幕解释框架检索所得的语义相似弹幕.</p>
                </div>
                <div class="p1">
                    <p id="397">弹幕语义检索的弹幕解释框架如图4所示, 分为如下过程:</p>
                </div>
                <h4 class="anchor-tag" id="398" name="398">1) 建立深度语义空间模型</h4>
                <div class="p1">
                    <p id="399">通过训练好的深度语义表征模型, 对训练数据进行编码, 生成语义向量, 组成语义向量集合.</p>
                </div>
                <h4 class="anchor-tag" id="400" name="400">2) 基于空间划分的索引</h4>
                <div class="p1">
                    <p id="401">高维空间中的近似最近邻 (approximate nearest neighbor, ANN) <sup><a class="sup">[32]</a></sup>查询问题是一个基本的查询范式, 尤其是在在数据挖掘、信息检索、推荐系统等领域的相似性查询上有重要的应用价值.局部敏感散列 (locality sensitive hashing, LSH) 是近似最近邻搜索算法中最流行的一种, 它有坚实的理论依据并且在高维数据空间中表现优异<sup><a class="sup">[33]</a></sup>.由于能够克服维度灾难, 且算法的精度和效率能够满足应用需求, 因而在许多应用中都被使用, 比如图像、视频、音频和DNA序列等相似性查询<sup><a class="sup">[34]</a></sup>.</p>
                </div>
                <div class="area_img" id="402">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902006_402.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于语义检索的弹幕解释框架" src="Detail/GetImg?filename=images/JFYZ201902006_402.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于语义检索的弹幕解释框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902006_402.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Time-sync comment for videos explanation framework based on semantic retrieval</p>

                </div>
                <div class="p1">
                    <p id="403">对生成的语义向量集合, 使用局部敏感散列 (LSH) 算法建立高维数据空间索引, 按照语义向量之间的距离, 进行高维空间划分.</p>
                </div>
                <h4 class="anchor-tag" id="404" name="404">3) 初始弹幕语义检索</h4>
                <div class="p1">
                    <p id="405">依次从测试数据中逐条选取弹幕作为初始弹幕, 使用训练好的深度语义表征模型进行编码, 生成语义向量.利用初始弹幕生成的语义向量通过基于空间划分的索引, 查找最近的<i>k</i>个语义向量, 使用训练好的深度语义表征模型解码生成上下文相关弹幕, 作为初始弹幕的解释.</p>
                </div>
                <h3 id="406" name="406" class="anchor-tag"><b>3</b><b>实验验证</b></h3>
                <h4 class="anchor-tag" id="407" name="407"><b>3.1</b><b>数据集介绍</b></h4>
                <div class="p1">
                    <p id="408">实验数据来源于国内知名视频共享平台bilibili (https://www.bilibili.com) 爬取的真实的弹幕数据, 弹幕数据集如表1所示:</p>
                </div>
                <div class="area_img" id="409">
                    <p class="img_tit"><b>表1</b><b>弹幕数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1</b><b>Bullet-Screen Data Set</b></p>
                    <p class="img_note"></p>
                    <table id="409" border="1"><tr><td><br />Entry</td><td>Number</td></tr><tr><td><br />Videos</td><td>2 716</td></tr><tr><td><br />Bullet-Screen</td><td>9 661 369</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="410">所有的弹幕数据随机划分为训练数据和测试数据, 训练数据用于训练深度语义表征模型和建立深度语义表征空间, 测试数据作为初始弹幕, 用于弹幕语义检索.其中训练数据取弹幕数据的90%, 剩下的10%作为测试数据.</p>
                </div>
                <h4 class="anchor-tag" id="411" name="411"><b>3.2</b><b>实验步骤</b></h4>
                <div class="p1">
                    <p id="412">实验步骤分为数据预处理、模型训练过程、语义检索.</p>
                </div>
                <h4 class="anchor-tag" id="413" name="413">1) 数据预处理</h4>
                <div class="p1">
                    <p id="414">鉴于弹幕数据存在高频、热点等重复出现的情况, 如“哈哈哈哈哈”、“前方高能”、“23333”等, 为防止语义检索出现检索的上下文弹幕存在大量与初始弹幕完全相同的弹幕, 给语义检索的验证带来困难, 同时考虑对比方法中检索出上下文相关弹幕完全和初始弹幕一样的情况, 影响实验评测的科学性, 本文对弹幕进行去重.</p>
                </div>
                <div class="p1">
                    <p id="415">根据语义相似性定义, 存在<i>δ</i>使得得到的语义相似弹幕集合<i>G</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>n</i></sub>}里面的弹幕语义相似.此时面临的问题为<i>δ</i>取值的选取, 若<i>δ</i>过大, 语义相似弹幕集合<i>G</i>中无关弹幕过多, 导致<i>G</i>中的弹幕平均语义相似度低, 影响模型的表征效果;若<i>δ</i>小, 语义相似弹幕集合<i>G</i>中弹幕过少, 导致噪声占的比重反而更大.因此, 需要选择合适的<i>δ</i>对弹幕进行切分, 使得所得到的语义相似性集合<i>G</i>的平均语义相似度最高.然而, <i>δ</i>的取值获取需要根据模型训练的结果进行定量分析, 即其他参数不变的情况下不断改变<i>δ</i>的取值, 使得模型损失函数的损失率最低, 选取此时的<i>δ</i>作为最终的<i>δ</i>.虽然一开始无法确定最优的<i>δ</i>的大小, 但是数据预处理部分必须对弹幕进行按<i>δ</i>切分, 得到当前切分时间片<i>δ</i>的语义相似度弹幕集合.本文, 在<i>δ</i>选取时, 最初通过人为观察选择一个<i>δ</i>值, 在此<i>δ</i>的情况下, 根据模型训练结果进行调整.考虑最初<i>δ</i>的选取, 过大或者过小, 都会给最终<i>δ</i>的拟合带来大量的训练次数, 所以最初<i>δ</i>选择为3 s.</p>
                </div>
                <div class="p1">
                    <p id="416">据对弹幕数据的人工观察, 进一步, 本文发现一定时间段内, 弹幕数量越多, 这段时间内弹幕的语义相似度越高;一定时间段内, 弹幕越少, 噪声的可能性越大, 语义相关性越小.同时如果一定时间内, 弹幕数据越多噪声弹幕的所占比重也越少, 所造成的干扰的影响也越小.结合此规律, 对按一定时间段切分的语义相似弹幕集合, 根据长度排序并筛选.</p>
                </div>
                <div class="p1">
                    <p id="417">为了便于字符级循环神经网络的处理, 弹幕长度设置为定长.鉴于过短或者过长的弹幕, 所占比重很少、对模型意义不大, 通过人工对弹幕数据的观察, 选取长度在 (5, 20) 之间的弹幕.同时对于弹幕数据设置成定长21, 不足部分补0.</p>
                </div>
                <h4 class="anchor-tag" id="418" name="418">2) 模型训练过程</h4>
                <h4 class="anchor-tag" id="419" name="419">① 模型初始化</h4>
                <div class="p1">
                    <p id="420">设置模型的初始权为<mathml id="421"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mo>-</mo><msqrt><mrow><mn>6</mn><mo>/</mo><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo></mrow></msqrt></mrow></mrow><mo>, </mo><mrow><mrow><mrow><msqrt><mrow><mn>6</mn><mo>/</mo><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo></mrow></msqrt></mrow><mo>) </mo></mrow></mrow></mrow></math></mathml>之间的随机值<sup><a class="sup">[35]</a></sup>, 对模型进行初始化.其中<i>n</i><sub>in</sub>代表输入的神经元个数, <i>n</i><sub>out</sub>表示输出的神经元个数.</p>
                </div>
                <h4 class="anchor-tag" id="422" name="422">② 数据输入</h4>
                <div class="p1">
                    <p id="423">每次取<i>batch</i>_<i>size</i>个语义相似弹幕集合, 并在其每个弹幕集合中随机取2条弹幕, 作为模型的数据输入.</p>
                </div>
                <h4 class="anchor-tag" id="424" name="424">③ 参数选取</h4>
                <div class="p1">
                    <p id="425">结合模型训练过程, 不断调整弹幕切片时间 (slice time) <i>T</i>、字符表征向量的维度 (word repre-sentation dimension) <i>α</i>、GRU单元弹幕表征向量的维度 (bullet-screen representation dimension) <i>β</i>、每轮训练所取语义相似度集合数据数量 (batch size) 、学习率 (learning rate) , 使语义表征模型的损失率最低并保持一定范围内变动.经过多次训练, 最终选取的参数如表2所示, 记录此时的训练次数 (nloop) 、损失率 (loss rate) , 保存此时的训练模型.</p>
                </div>
                <div class="area_img" id="426">
                    <p class="img_tit"><b>表2</b><b>模型参数选取</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2</b><b>The Parameter of Model</b></p>
                    <p class="img_note"></p>
                    <table id="426" border="1"><tr><td><br />Parameter</td><td>Value</td></tr><tr><td><br />Slice Time <i>T</i>/s</td><td>1</td></tr><tr><td><br />Word Representation Dimension <i>α</i></td><td>200</td></tr><tr><td><br />Bullet-Screen Representation Dimension <i>β</i></td><td>1 000</td></tr><tr><td><br />Batch Size</td><td>30</td></tr><tr><td><br />Learning Rate</td><td>0.001</td></tr><tr><td><br />nloop</td><td>50 000</td></tr><tr><td><br />Loss Rate</td><td>0.017 25</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="427" name="427">3) 语义检索</h4>
                <div class="p1">
                    <p id="428">利用训练好的深度语义表征模型, 对训练数据解码生成深度语义空间, 深度语义空间是所有训练数据的语义向量在空间的分布.对深度语义空间的语义向量建立基于空间划分的索引.对测试弹幕逐条选取作为初始弹幕, 经过训练好的深度语义表征模型进行解码, 生成初始弹幕的深度语义向量, 并基于空间划分的索引检索与初始弹幕语义相近的语义向量, 对检索到的语义向量经过训练好的深度语义表征模型的解码, 解码生成上下文相关弹幕, 其中上下文相关弹幕即为检索到的与初始弹幕语义相似的弹幕.本实验根据检索的相似语义距离, 每条初始弹幕选取语义距离最近的10条语义向量.最终, 每条初始弹幕存在10条上下文相关弹幕, 并且语义相似度依次递减.</p>
                </div>
                <h4 class="anchor-tag" id="429" name="429"><b>3.3</b><b>对比方法</b></h4>
                <div class="p1">
                    <p id="430">为了验证基于语义检索的弹幕解释框架, 本文设计了如下3个对比方法.</p>
                </div>
                <div class="p1">
                    <p id="431">1) 序列自编码.<i>s</i><sub><i>i</i></sub>∈<i>G</i>, <i>G</i>为相似弹幕集合, <i>s</i><sub><i>i</i></sub>={<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>}, <i>s</i><sub><i>i</i></sub>的语义向量<b><i>v</i></b><sub><i>i</i></sub>, 序列自编码模型损失函数为如式 (4) 所示, 训练弹幕自编码语义表征模型.初始弹幕通过训练好的序列自编码模型解码, 解码生成语义向量, 利用基于语义检索的弹幕解释框架, 检索与初始弹幕语义距离最近的10个向量作为语义相似向量, 并对检索到的语义相似向量使用训练好的序列自编码模型进行解码生成上下文相关弹幕.</p>
                </div>
                <div class="p1">
                    <p id="432"><mathml id="433"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mspace width="0.25em" /></mstyle><mrow><mi>lg</mi></mrow><mtext> </mtext><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow><mo>, </mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>. (4) </p>
                </div>
                <div class="p1">
                    <p id="434">2) 词袋特征索引.使用词袋模型将训练数据中的弹幕转化为向量, 建立语义向量空间.通过初始弹幕的语义向量在语义空间寻找相似向量.</p>
                </div>
                <div class="p1">
                    <p id="435">3) 编码器解码器.<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>∈<i>G</i>, <i>G</i>为相似弹幕集合, <i>s</i><sub><i>i</i></sub>={<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>}, <i>s</i><sub><i>j</i></sub>={<i>c</i>′<sub>1</sub>, <i>c</i>′<sub>2</sub>, …, <i>c</i>′<sub><i>n</i></sub>}, <i>s</i><sub><i>j</i></sub>的语义向量<b><i>v</i></b><sub><i>j</i></sub>, 损失函数如式 (5) 所示, 训练弹幕编码器解码器语义表征模型.初始弹幕通过训练好的编码器解码器模型, 编码生成语义向量, 利用基于语义检索的弹幕解释框架, 检索与初始弹幕语义距离最近的10条向量作为语义相似向量, 并对检索到的语义相似向量使用训练好的编码器解码器模型进行解码生成上下文相关弹幕.</p>
                </div>
                <div class="p1">
                    <p id="436"><mathml id="437"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mspace width="0.25em" /></mstyle><mrow><mi>lg</mi></mrow><mtext> </mtext><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mrow><mo>, </mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>. (5) </p>
                </div>
                <h4 class="anchor-tag" id="438" name="438"><b>3.4</b><b>评价指标</b></h4>
                <div class="p1">
                    <p id="439">本节通过BLEU-4、流畅度、多样性、人工评测对实验结果进行评价.</p>
                </div>
                <div class="p1">
                    <p id="440">BLEU<sup></sup><sup><a class="sup">[35]</a></sup>是一种流行的机器翻译评价指标, 用于分析候选词和参考序列中<i>n</i>元组共同出现的程度, 不考虑词的位置.本实验用初始弹幕检索所得上下文弹幕和初始弹幕所在的语义相似集合的弹幕的<i>n</i>元单位切片 (<i>n</i>-gram) 进行比较, 并通过计算出匹配片段的个数来计算得分.匹配的片段数越多, 检索的上下文相关弹幕越好.BLEU值的取值范围是0～1的数值, 只有2个弹幕完全一样的情况下才会取值1.本实验<i>n</i>=4, 即BLEU-4标准.</p>
                </div>
                <div class="p1">
                    <p id="441">除此之外, 流畅度和多样性也是评价上下文相关弹幕的重要指标<sup><a class="sup">[36]</a></sup>, 其中流畅度衡量了检索的上下文相关弹幕在表达上与人类自然语言相似程度, 多样性衡量了检索的上下文相关弹幕表达的丰富程度.具体而言, 流程度和多样性指标的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="442"><b>定义5</b>. 流畅度.<i>T</i><sub>0</sub>表示所有弹幕 (训练数据和测试数据) 的<i>n</i> -gram划分块集合, <i>T</i>表示检索的上下文相关弹幕<i>n</i> -gram划分块集合.</p>
                </div>
                <div class="p1">
                    <p id="443"><mathml id="444"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>l</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>Τ</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>t</mi><mo>∈</mo><mi>Τ</mi><msub><mrow></mrow><mn>0</mn></msub><mspace width="0.25em" /><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext><mspace width="0.25em" /><mn>0</mn><mo stretchy="false">) </mo><mspace width="0.25em" /><mi>l</mi><mi>e</mi><mi>n</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>Τ</mi></mrow></munder><mi>l</mi></mstyle><mi>e</mi><mi>n</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>. (6) </p>
                </div>
                <div class="p1">
                    <p id="445">本实验对流畅度的<i>n</i> -gram中<i>n</i>设置为<i>n</i>∈{2, 3, 4, 5, 6}.</p>
                </div>
                <div class="p1">
                    <p id="446">对于初始弹幕检索出来的10条上下文相关弹幕随机取3条进行<i>n</i> -gram划分, 得到该初始弹幕上下文相关弹幕的<i>n</i> -gram划分块集合<i>T</i>.逐个取<i>T</i>中的元素<i>t</i>, 并将<i>t</i>的权重设置为<i>len</i> (<i>t</i>) , 若存在<i>T</i><sub>0</sub>中则取1, 若不存在取0, 得到该初始弹幕上下文相关弹幕的流畅度.本实验最终的流畅度为所有初始弹幕的上下文相关弹幕的流畅度的求和均值.</p>
                </div>
                <div class="p1">
                    <p id="447"><b>定义6</b>. 多样性.对初始弹幕的上下文相关弹幕两两选取, 进行<i>n</i> -gram划分.</p>
                </div>
                <div class="p1">
                    <p id="448"><mathml id="449"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>s</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></munder><mrow><mfrac><mrow><mn>2</mn><mrow><mo>|</mo><mrow><mi>Τ</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mstyle displaystyle="true"><mo>∩</mo><mi>Τ</mi></mstyle><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>Τ</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>Τ</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub></mrow><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle></mrow></math></mathml>. (7) </p>
                </div>
                <div class="p1">
                    <p id="450"><i>T</i><sub><i>s</i><sub><i>i</i></sub></sub>, <i>T</i><sub><i>s</i><sub><i>j</i></sub></sub>来源于上下文相关弹幕<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>的<i>n</i> -gram划分块, |<i>T</i><sub><i>s</i><sub><i>i</i></sub></sub>∩<i>T</i><sub><i>s</i><sub><i>j</i></sub></sub>|为<i>T</i><sub><i>s</i><sub><i>i</i></sub></sub>, <i>T</i><sub><i>s</i><sub><i>j</i></sub></sub>的交集的<i>n</i> -gram划分块长度之和, |<i>T</i><sub><i>s</i><sub><i>i</i></sub></sub>|, |<i>T</i><sub><i>s</i><sub><i>j</i></sub></sub>|分别表示<i>T</i><sub><i>s</i><sub><i>i</i></sub></sub>, <i>T</i><sub><i>s</i><sub><i>j</i></sub></sub>的所有<i>n</i> -gram划分块长度之和, <i>N</i>是所有<i>s</i><sub><i>i</i></sub>, <i>s</i><sub><i>j</i></sub>两两组合的个数.本实验最终的多样性为所有初始弹幕的上下文相关弹幕的多样性的求和均值.</p>
                </div>
                <div class="p1">
                    <p id="451">本文结合弹幕这类短文本特点, 多样性的<i>n</i> -gram中<i>n</i>设置为<i>n</i>∈{1, 2, 3}.</p>
                </div>
                <div class="p1">
                    <p id="452">为了更好地从语义的角度评测检索的上下文相关弹幕与初始弹幕的语义相似性, 进一步, 本文提出了人工评测<sup><a class="sup">[26]</a></sup>, 具体指标的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="453"><b>定义7</b>. 人工评测.</p>
                </div>
                <div class="p1">
                    <p id="454"><mathml id="455"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>u</mtext><mtext>m</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>Τ</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munder><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>c</mi><mo>∈</mo><mi>Τ</mi><msub><mrow></mrow><mi>c</mi></msub><mspace width="0.25em" /><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext><mspace width="0.25em" /><mn>0</mn><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>Τ</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi>Τ</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mfrac></mrow></math></mathml>. (8) </p>
                </div>
                <div class="p1">
                    <p id="456">其中, <i>T</i><sub><i>s</i></sub>表示所有上下文相关弹幕, |<i>T</i><sub><i>s</i></sub>|为上下文相关弹幕的数量.<i>T</i><sub><i>c</i></sub>表示为所有初始弹幕, <i>c</i>∈<i>T</i><sub><i>c</i></sub>表示弹幕<i>c</i>与初始弹幕相似.</p>
                </div>
                <div class="p1">
                    <p id="457">通过人工对上下文相关弹幕进行标注, 若与初始弹幕语义相似则为1, 否则为0, 得分为所有取值之和除以所有上下文弹幕个数.鉴于语义相似性, 无法单纯从字面进行判别并且不同评测者对相似性的判定和理解可能存在偏差, 人工评测部分采取多人评测.评测过程中, 对于每条上下文相关弹幕若有一半以上结果认为与初始弹幕语义相似, 则此弹幕判定为语义相似弹幕.</p>
                </div>
                <h4 class="anchor-tag" id="458" name="458"><b>3.5</b><b>结果分析</b></h4>
                <div class="p1">
                    <p id="459">实验的结果如表3所示, 深度语义表征模型从BLEU、流畅度、多样性、人工评测方面都取得了较好的效果.其中多样性、人工评测2项指标得分高于其他模型, 可见基于语义检索的弹幕解释框架, 能检索与其语义相似但表达不同的弹幕, 从而验证深度语义表征模型的合理性.其中BLEU、流畅度2项指标词袋模型得分高于其他模型, 是由于词袋模型是通过向量检索, 所得到的弹幕是原有检索空间存在的弹幕而非根据初始弹幕生成的上下文相关弹幕, 所以BLEU和流畅度得分取值较高, 超过其他模型的得分.同时, 考虑到词袋模型可能存在多条上下文弹幕与初始弹幕相同, 影响结果的科学性和合理性, 在数据预处理部分对重复弹幕进行了剔除.因此, 词袋特征索引模型人工语义相似度评测结果的得分优于编码器解码器模型和序列自编码, 低于深度语义表征模型.</p>
                </div>
                <div class="area_img" id="460">
                    <p class="img_tit"><b>表3</b><b>实验验证结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3</b><b>Performance of These Models</b></p>
                    <p class="img_note"></p>
                    <table id="460" border="1"><tr><td><br />Models</td><td>BLEU-4</td><td>Fluency</td><td>Diversity</td><td>Human<br />Recall</td></tr><tr><td><br />Deep Semantic Representation</td><td>0.204 94</td><td>0.871 13</td><td><b>0.965</b><b>51</b></td><td><b>0.628</b></td></tr><tr><td><br />Sequence Self-Coding</td><td>0.199 61</td><td>0.807 95</td><td>0.965 39</td><td>0.510</td></tr><tr><td><br />Word Bag Feature Index</td><td><b>0.344</b><b>54</b></td><td><b>1.0</b></td><td>0.680 81</td><td>0.584</td></tr><tr><td><br />Encoder-Decoder</td><td>0.081 42</td><td>0.596 70</td><td>0.832 01</td><td>0.364</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="461">综上所述, 通过在BLEU、多样性、流畅性、人工评测, 对本文所提出的模型进行了充分验证.结果表明这种基于循环神经网络的深度语义表征模型能够精准地刻画弹幕短文本的语义, 也证明了关于弹幕相关假设的合理性.</p>
                </div>
                <h4 class="anchor-tag" id="462" name="462"><b>3.6</b><b>案例分析</b></h4>
                <div class="p1">
                    <p id="463">在实验结果的基础上, 通过案例对检索的上下文相关弹幕和初始弹幕语义进行分析.首先通过初始弹幕与上下文相关弹幕的语义关系, 利用上下文相关弹幕对初始弹幕进行解释, 使用语义相似弹幕集合分析解释的合理性.然后, 结合弹幕语义和弹幕视频之间的关联, 通过弹幕视频分析检索弹幕和初始弹幕的语义关系.</p>
                </div>
                <div class="p1">
                    <p id="464">结合案例使用上下文相关弹幕解释初始弹幕, 使用语义相似弹幕集分析解释的合理性.如图5所示, 黄色为初始弹幕, 白色为初始弹幕的语义相似弹幕, 红色为基于语义检索的弹幕解释框架检索出的上下文相关弹幕.初始弹幕“五毛…五毛”, 包含隐含语义, 很难理解语义.通过检索出来的上下文相关弹幕, 很好地解释了“五毛”的语义, 同时根据语义相似弹幕也验证了解释的合理性.对于如初始弹幕为“哈哈哈哈”, 检索出来的上下文相关弹幕中出现了“噗哈哈哈哈, 结局很赞呢!”、“23333我不行了”、“哈哈哈哈哈哈哈达”.检索的上下文弹幕和初始弹幕存在大量重复的字符, 恰恰正是初始弹幕的相似语义的表达.其中“2333”正是代表初始弹幕“哈哈哈”的语义, 如图6所示.</p>
                </div>
                <div class="area_img" id="465">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902006_465.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 案例展示1" src="Detail/GetImg?filename=images/JFYZ201902006_465.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 案例展示1  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902006_465.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Case study figure 1</p>

                </div>
                <div class="area_img" id="466">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902006_466.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 案例展示2" src="Detail/GetImg?filename=images/JFYZ201902006_466.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 案例展示2  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902006_466.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Case study figure 2</p>

                </div>
                <div class="p1">
                    <p id="467">结合弹幕语义和弹幕视频之间的关联, 对检索得到的上下文相关弹幕, 通过结合视频进行解释分析, 比如弹幕“这才是开始”, 从弹幕文本的自身角度, 很难理解此句弹幕所表达的真实语义, 通过深度语义检索出来的相似弹幕为“不敢放大看”、“这个最吓人”等令人费解的语句.结合初始弹幕“不敢放大看”的视频标识符和时间, 定位到所在视频的出现地方.通过人工验证, 此视频为一部恐怖片, 弹幕所出现的情节是灵异事件的开场部分.通过视频本身的内容, 很好地验证了初始弹幕检索出来的上下文弹幕的合理性.</p>
                </div>
                <div class="p1">
                    <p id="468">综上所述, 通过具体案例分析得到的上下文相关弹幕大多是初始弹幕和视频情节的语义相似解释和表达, 从而也验证了检索的上下文相关弹幕的合理性和准确性.</p>
                </div>
                <h3 id="469" name="469" class="anchor-tag"><b>4</b><b>结论和展望</b></h3>
                <div class="p1">
                    <p id="470">针对视频实时评论的高噪声、不规范表达和隐含语义等特性, 使得传统自然语言处理技术具有很大局限性, 本文提出了一种基于循环神经网络 (RNN) 的深度语义表征模型.该模型由于引入了字符级别的循环神经网络, 避免了弹幕噪声对文本分词带来的影响, 在实现语义表征的过程中, 通过使用神经网络, 使得所得的语义向量能够对弹幕进行更深层次的刻画, 表达其隐含语义.在此基础上, 针对弹幕文本具有隐含语义的特点, 本文进一步设计了基于语义检索的弹幕解释框架, 同时作为对语义表征结果的验证.本文设计了包括序列自编码、词袋特征索引、编码器解码器等多种对比方法, 并通过BLEU、流畅度、多样性等多种指标以及人工评测对本文所提出的模型进行了充分地验证, 表明这种基于循环神经网络的深度语义表征模型能够精准地刻画弹幕短文本的语义, 也证明了关于弹幕相关假设的合理性.</p>
                </div>
                <div class="p1">
                    <p id="471">本文在研究弹幕深度语义表征的基础上, 提出基于弹幕深度语义表征的弹幕语义检索, 用于解决高噪声、不规范表达和隐含语义等特性.针对视频实时评论的研究未来可进一步从以下4点更深入的研究:1) 弹幕数据较传统的短文本最大区别在于用语的随意性, 任何人都能发表自己的看法, 而不同的人拥有不同的风格, 因此利用用户ID信息对语义的分析可能有一定的帮助, 更好地体现弹幕的价值.2) 若对视频类型进行分类, 分析不同类别视频中用户行为的差异性, 将具有巨大价值.3) 未来的工作进一步将通过实验对诸如搜索引擎搜索结果、论坛评论、微博等短文本适用性进行探究, 并将本文中对视频短文本的分析推广到搜索引擎搜索结果、论坛评论、微博等短文本.4) 若考虑引入文本生成模型, 在弹幕深度语义表征模型的基础上, 进行弹幕生成, 设计弹幕自动回复、评论自动生成, 将具有重大实际应用价值, 也是未来的研究方向之一.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="525">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring the emerging type of comment for online videos:DanMu">

                                <b>[1]</b>He Ming, Ge Yong, Chen Enhong, et al.Exploring the emerging type of comment for online videos:DanMu[J].ACM Transactions on the Web, 2017, 12 (1) :1-23
                            </a>
                        </p>
                        <p id="527">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting the popularity of DanMu-enabled videos:A multi-factor view">

                                <b>[2]</b>He Ming, Ge Yong, Wu Le, et al.Predicting the popularity of DanMu-enabled videos:A multi-factor view[C]Proc of the 21st Int Conf on Database Systems for Advanced Applications.Berlin:Springer, 2016:351-366
                            </a>
                        </p>
                        <p id="529">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The tube over time: characterizing popularity growth of youtube videos">

                                <b>[3]</b>Figueiredo F, Benevenuto F, Almeida J M.The tube over time:Characterizing popularity growth of YouTube videos[C]Proc of the 4th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2011:745-754
                            </a>
                        </p>
                        <p id="531">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A measurement-driven analysis of information propagation in the Flickr social network">

                                <b>[4]</b>Cha M, Mislove A, Gummadi K P.A measurement-driven analysis of information propagation in the flickr social network[C]Proc of the 18th Int Conf on World Wide Web.New York:ACM, 2009:721-730
                            </a>
                        </p>
                        <p id="533">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chinese tag analysis for foreign movie contents">

                                <b>[5]</b>Xiao Lin, Ito E, Hirokawa S.Chinese tag analysis for foreign movie contents[C]Proc of the 13th IEEE/ACIS Int Conf on Computer and Information Science.Piscataway, NJ:IEEE, 2014:163-166
                            </a>
                        </p>
                        <p id="535">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correlation analysis between user&amp;#39;&amp;#39;s emotional comments and popularity measures">

                                <b>[6]</b>Wu Zechen, Ito E.Correlation analysis between user’s emotional comments and popularity measures[C]Proc of the3rd Int Conf on Advanced Applied Informatics.Piscataway, NJ:IEEE, 2014:280-283
                            </a>
                        </p>
                        <p id="537">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201511016&amp;v=MjczOTR6cXFCdEdGckNVUkxPZVplVnZGeTduVzc3SVBTbmZmN0c0SDlUTnJvOUVZb1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>Zheng Yangyang, Xu Jian, Xiao Zhuo.Application of emotion analysis and visualization method in network video barrage data analysis[J].Modern Library and Information Technology, 2015, 31 (11) :82-90 (in Chinese) (郑飏飏, 徐健, 肖卓.情感分析及可视化方法在网络视频弹幕数据分析中的应用[J].现代图书情报技术, 2015, 31 (11) :82-90) 
                            </a>
                        </p>
                        <p id="539">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crowdsourced time-sync video tagging using temporal and personalized topic modeling">

                                <b>[8]</b>Wu Bin, Zhong Erheng, Tan Ben.Crowd-sourced time-sync video tagging using temporal and personalized topic modeling[C]Proc of the 20th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM, 2014:721-730
                            </a>
                        </p>
                        <p id="541">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201704028&amp;v=MTQ0NTVuVzc3SUx6N0JkN0c0SDliTXE0OUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Deng Yang, Zhang Chenxi, Li Jiangfeng.Video shot recommendation model based on emotion analysis using timesync comments[J].Journal of Computer Applications, 2017, 37 (4) :1065-1070 (in Chinese) (邓扬, 张晨曦, 李江峰.基于弹幕情感分析的视频片段推荐模型[J].计算机应用, 2017, 37 (4) :1065-1070) 
                            </a>
                        </p>
                        <p id="543">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14081500000004&amp;v=MDE1MTdIdzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lKbHdjYXhJPU5pZk9mYks4SHRuTnFvOUZaT3NQRA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Murray C J L, Vos T, Lozano R, et al.Disability-adjusted life years (DALYs) for 291 diseases and injuries in 21regions, 1990-2010:A systematic analysis for the global burden of disease study 2010[J].Lancet, 2012, 380 (9859) :2197-2223
                            </a>
                        </p>
                        <p id="545">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436-444
                            </a>
                        </p>
                        <p id="547">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recent advances in deep learning for speech research atMicrosoft">

                                <b>[12]</b>Deng Li, Li Jinyu, Huang Juiting, et al.Recent advances in deep learning for speech research at Microsoft[C]Proc of the 38th EEE Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2013:8604-8608
                            </a>
                        </p>
                        <p id="549">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved representation learning for question answer matching">

                                <b>[13]</b>Tan Ming, Santos C N, Xiang Bing, et al.Improved representation learning for question answer matching[C]Proc of the 54th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2016:464-473
                            </a>
                        </p>
                        <p id="551">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks for Acoustic Modeling in Speech Recognition">

                                <b>[14]</b>Hinton G, Deng Li, Yu Dong, et al.Deep neural networks for acoustic model-ing in speech recognition:The shared views of four research groups[J].IEEE Signal Processing Magazine, 2012, 29 (6) :82-97
                            </a>
                        </p>
                        <p id="553">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003800817&amp;v=Mjk1MDFWND1OajdCYXJPNEh0SFBwNDlGYk9vSVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNubFZiM0FJ&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>Hermansky H.Speech recognition from spectral dynamics[J].Sadhana-Academy Proceedings in Engineering Sciences, 2011, 36 (5) :729-744
                            </a>
                        </p>
                        <p id="555">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Representation Learning A Review and New Perspectives">

                                <b>[16]</b>Bengio Y, Courville A, Vincent P.Representation learning:A review and new perspectives[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (8) :1798-1828
                            </a>
                        </p>
                        <p id="557">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal pooling and multiscale learning for automatic annotation and ranking of music audio">

                                <b>[17]</b>Hamel P, Lemieux S, Bengio Y, et al.Temporal pooling and multiscale learning for automatic annotation and ranking of music audio[C]Proc of the 12th Int Society for Music Information Retrieval Conf.Piscataway, NJ:IEEE, 2011:729-734
                            </a>
                        </p>
                        <p id="559">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MDExODdQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SUpsd2NheEk9TmlmSlpiSzlIdGpNcW85RlpPb05DWDh4b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Hinton G E, Osindero S, Teh Y W.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554
                            </a>
                        </p>
                        <p id="561">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deepconvolutional neural networks">

                                <b>[19]</b>Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 25th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105
                            </a>
                        </p>
                        <p id="563">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster stochastic variational inference using proximal-gradient methods with general divergence functions">

                                <b>[20]</b>Khan M E, Babanezhad R, Wu Lin, et al.Faster stochastic variational inference using proximal-gradient methods with general divergence functions[J].Journal of Comparative Neurology, 2015, 319 (3) :359-86
                            </a>
                        </p>
                        <p id="565">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Domain adaptation for largescale sentiment classification:a deep learning approach">

                                <b>[21]</b>Glorot X, Bordes A, Bengio Y.Domain adaptation for largescale sentiment classification:A deep learning approach[C]Proc of the 28th Int Conf on Machine Learning (ICML-11) .New York:ACM, 2011:513-520
                            </a>
                        </p>
                        <p id="567">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep auto-encoder neural networks in reinforcement learning">

                                <b>[22]</b>Lange S, Riedmiller M.Deep auto-encoder neural networks in reinforcement learning[C]Proc of Int Joint Conf Neural Networks (IJCNN) .Piscataway, NJ:IEEE, 2010:1-8
                            </a>
                        </p>
                        <p id="569">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning distributed representations of concepts using Linear Relational Embedding">

                                <b>[23]</b>Paccanaro A, Hinton G E.Learning distributed representations of concepts using linear relational embedding[J].IEEETransactions on Knowledge&amp;Data Engineering, 2002, 13 (2) :232-244
                            </a>
                        </p>
                        <p id="571">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                Bengio Y, Ducharme R, Vincent P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155
                            </a>
                        </p>
                        <p id="573">
                            <a id="bibliography_25" >
                                    <b>[25]</b>
                                Collobert R, Weston J, Bottou L, et al.Natural language processing (almost) from scratch[J].Journal of Machine Learning Research, 2011, 12 (1) :2493-2537
                            </a>
                        </p>
                        <p id="575">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Word representations:a simple and general method for semi-supervised learning.">

                                <b>[26]</b>Turian J, Ratinov L, Bengio Y.Word representations:Asimple and general method for semi-supervised learning[C]Proc of the 48th Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2010:384-394
                            </a>
                        </p>
                        <p id="577">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Jumping NLP curves:a review of natural language processing research">

                                <b>[27]</b>Cambria E, White B.Jumping NLP curves:A review of natural language processing research[J].IEEEComputational Intelligence Magazine, 2014, 9 (2) :48-57
                            </a>
                        </p>
                        <p id="579">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finding function in form:Compositional character models for open vocabulary word representation">

                                <b>[28]</b>Wang Ling, Luís T, Marujo L, et al.Finding function in form:Compositional character models for open vocabulary word representation[C]Proc of the 2015Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:1520-1530
                            </a>
                        </p>
                        <p id="581">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep machine learning-A new frontier in artificial intelligence research">

                                <b>[29]</b>Arel I, Rose D C, Karnowski T P.Deep machine learning-Anew frontier in artificial intelligence research[J].IEEEComputational Intelligence Magazine, 2010, 5 (4) :13-18
                            </a>
                        </p>
                        <p id="583">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving stemming for Arabic information retrieval:light stemming and co-occurrence analysis.">

                                <b>[30]</b>Larkey L S, Ballesteros L, Connell M E.Improving stemming for arabic information retrieval:Light stemming and co-occurrence analysis[C]Proc of the 25th Annual Int ACM SIGIR Conf on Research and Development in Information Retrieval.New York:ACM, 2002:275-282
                            </a>
                        </p>
                        <p id="585">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A hierarchical recurrent encoder-decoder for generative context-aware query suggestion">

                                <b>[31]</b>Sordoni A, Bengio Y, Vahabi H, et al.A hierarchical recurrent encoder-decoder for generative context-aware query suggestion[C]Proc of the 24th ACM Int on Conf on Information and Knowledge Management.New York:ACM, 2015:553-562
                            </a>
                        </p>
                        <p id="587">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014319713.nh&amp;v=Mjk5ODhaZVZ2Rnk3blc3N0lWRjI2R3JDNUY5Yk5ySkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b>Liu Yingfan.Research on approximate nearest neighbor query based on local sensitive Hash[D].Xi’an:Xidian University, 2014 (in Chinese) (刘英帆.基于局部敏感哈希的近似最近邻查询研究[D].西安:西安电子科技大学, 2014) 
                            </a>
                        </p>
                        <p id="589">
                            <a id="bibliography_33" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1012376885.nh&amp;v=MjU0NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XNzdJVkYyNkhMQy9HTm5FcXBFYlBJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[33]</b>Ling Kang.Research on similarity search technology based on location sensitive Hash[D].Nanjing:Nanjing University, 2012 (in Chinese) (凌康.基于位置敏感哈希的相似性搜索技术研究[D].南京:南京大学, 2012) 
                            </a>
                        </p>
                        <p id="591">
                            <a id="bibliography_34" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YDTX201510012&amp;v=MDM4NzlHRnJDVVJMT2VaZVZ2Rnk3blc3N0lQQ25mZHJHNEg5VE5yNDlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[34]</b>Liu Genping.Review on locality sensitive hashing in centralized environment[J].Mobile Communications, 2015 (10) :46-51 (in Chinese) (刘根平.集中式环境下的局部敏感哈希算法综述[J].移动通信, 2015 (10) :46-51) 
                            </a>
                        </p>
                        <p id="593">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparing automatic and human evaluation of NLG systems">

                                <b>[35]</b>Belz A, Reiter E.Comparing automatic and human evaluation of NLG systems[C]Proc of the 11th Conf of the European Chapter of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2006:313-320
                            </a>
                        </p>
                        <p id="595">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reading the videos:Temporal labeling for crowdsourced time-sync videos based on semantic embedding">

                                <b>[36]</b>LüGuangyi, Xu Tong, Chen Enhong, et al.Reading the videos:Temporal labeling for crowd sourced time-sync videos based on semantic embedding[C]Proc of the 30th AAAIConf on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:3000-3006
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902006&amp;v=MTYxOTBWdkZ5N25XNzdJTHl2U2RMRzRIOWpNclk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
