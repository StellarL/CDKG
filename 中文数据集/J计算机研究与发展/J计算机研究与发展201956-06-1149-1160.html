

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128625137150000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201906003%26RESULT%3d1%26SIGN%3dNsODEMcrSsUTovQcmwjfMn5e%252b1k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906003&amp;v=MDAwNDFyQ1VSTE9lWmVScUZpRG1VN3ZLTHl2U2RMRzRIOWpNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="&lt;b&gt;2 FMC架构设计&lt;/b&gt; "><b>2 FMC架构设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="&lt;b&gt;2.1 FMC架构&lt;/b&gt;"><b>2.1 FMC架构</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;2.2 FMC的忆阻器阵列&lt;/b&gt;"><b>2.2 FMC的忆阻器阵列</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;2.3 FMC的互联结构&lt;/b&gt;"><b>2.3 FMC的互联结构</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.4 FMC的共享资源池&lt;/b&gt;"><b>2.4 FMC的共享资源池</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="&lt;b&gt;3 FDM策略设计&lt;/b&gt; "><b>3 FDM策略设计</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#127" data-title="&lt;b&gt;4.1 实验方法&lt;/b&gt;"><b>4.1 实验方法</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;4.2 FMC的功能单元资源利用率&lt;/b&gt;"><b>4.2 FMC的功能单元资源利用率</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;4.3 FMC空间占用&lt;/b&gt;"><b>4.3 FMC空间占用</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;4.4 FMC+FDM性能和能耗&lt;/b&gt;"><b>4.4 FMC+FDM性能和能耗</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#152" data-title="&lt;b&gt;5 总  结&lt;/b&gt; "><b>5 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="图1 基于忆阻器阵列的内存计算硬件结构">图1 基于忆阻器阵列的内存计算硬件结构</a></li>
                                                <li><a href="#88" data-title="图2 基于功能池的忆阻器立方体的概要图">图2 基于功能池的忆阻器立方体的概要图</a></li>
                                                <li><a href="#89" data-title="图3 忆阻器阵列的平面互联">图3 忆阻器阵列的平面互联</a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表1 功能单元的参数表示&lt;/b&gt;"><b>表1 功能单元的参数表示</b></a></li>
                                                <li><a href="#103" data-title="图4 共享资源池各个分池之间的互联结构">图4 共享资源池各个分池之间的互联结构</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表2 功能单元的配置参数&lt;/b&gt;"><b>表2 功能单元的配置参数</b></a></li>
                                                <li><a href="#120" data-title="图5 训练一个神经网络的数据图">图5 训练一个神经网络的数据图</a></li>
                                                <li><a href="#116" data-title="图6 FDM策略的一个示例">图6 FDM策略的一个示例</a></li>
                                                <li><a href="#124" data-title="图7 FMC的流程总体流程">图7 FMC的流程总体流程</a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表3 硬件配置&lt;/b&gt;"><b>表3 硬件配置</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表4 FMC单元的实验配置参数&lt;/b&gt;"><b>表4 FMC单元的实验配置参数</b></a></li>
                                                <li><a href="#137" data-title="图8 单个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较">图8 单个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较</a></li>
                                                <li><a href="#140" data-title="图9 多个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较">图9 多个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较</a></li>
                                                <li><a href="#144" data-title="图10 FMC中各个单元的空间占用比率 (包含一层忆阻器阵列) ">图10 FMC中各个单元的空间占用比率 (包含一层忆阻器阵列) </a></li>
                                                <li><a href="#148" data-title="图11 FMC与2D-PIM的性能比较">图11 FMC与2D-PIM的性能比较</a></li>
                                                <li><a href="#149" data-title="图12 FMC与2D-PIM的能耗比较">图12 FMC与2D-PIM的能耗比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="276">


                                    <a id="bibliography_1" title="Chi Ping, Li Shuangchen, Xu Cong, et al.Prime:A novel processing-in-memory architecture for neural network computation in reram-based main memory[C] //Proc of 2016 ACM/IEEE the 43rd Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2016:27- 39" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PRIME:A Novel Processing-inmemory Architecture for Neural Network Computation in Re RAM-based Main Memory">
                                        <b>[1]</b>
                                        Chi Ping, Li Shuangchen, Xu Cong, et al.Prime:A novel processing-in-memory architecture for neural network computation in reram-based main memory[C] //Proc of 2016 ACM/IEEE the 43rd Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2016:27- 39
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_2" title="Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[J].ACM SIGARCH Computer Architecture News, 2016, 44 (3) :14- 26" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMEA2268EBF73745312D0ED0A3CA273E4C&amp;v=MTg0NjdIWWZPR1FsZkNwYlEzNU54Z3pMdS94S289TmlmSVk4YkpITlBLcC9vM0V1d01DM2c4ekJjUm5qOElQSCtUcjJGRWU3V1hNTDdzQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[J].ACM SIGARCH Computer Architecture News, 2016, 44 (3) :14- 26
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_3" title="Song Linghao, Qian Xuehai, Li Hai, et al.PipeLayer:A pipelined ReRAM-based accelerator for deep learning[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2017:541- 552" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PipeL ayer:Apipelined reR AM-based accelerator for deep learning">
                                        <b>[3]</b>
                                        Song Linghao, Qian Xuehai, Li Hai, et al.PipeLayer:A pipelined ReRAM-based accelerator for deep learning[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2017:541- 552
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_4" title="Stefano A, Pritish N, Hsinyu T, et al.Equivalent-accuracy accelerated neural-network training using analogue memory[J].Nature, 2018, 558 (7708) :60- 67" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Equivalent-accuracy accelerated neural-network training using analogue memory">
                                        <b>[4]</b>
                                        Stefano A, Pritish N, Hsinyu T, et al.Equivalent-accuracy accelerated neural-network training using analogue memory[J].Nature, 2018, 558 (7708) :60- 67
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_5" title="Keckler S W, Dally W J, Khailany B, et al.Gpus and the future of parallel computing[J].IEEE Micro, 2011, 31 (5) :7- 17" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPUS AND THE FUTURE OF PARALLEL COMPUTING">
                                        <b>[5]</b>
                                        Keckler S W, Dally W J, Khailany B, et al.Gpus and the future of parallel computing[J].IEEE Micro, 2011, 31 (5) :7- 17
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_6" title="Li Shuangchen, Xu Cong, Zou Qiaosha, et al.Pinatubo:A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories[C] //Proc of Design Automation Conf.Piscataway, NJ:IEEE, 2016:173- 179" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pinatubo:A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories">
                                        <b>[6]</b>
                                        Li Shuangchen, Xu Cong, Zou Qiaosha, et al.Pinatubo:A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories[C] //Proc of Design Automation Conf.Piscataway, NJ:IEEE, 2016:173- 179
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_7" title="Mao Haiyu, Song Mingcong, Li Tao, et al.Lergan:A zero-free, low data movement and pim-based gan architecture[C] //Proc of the 51st Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2018:669- 681" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lergan A zero-free low data movement and pim-based gan architecture">
                                        <b>[7]</b>
                                        Mao Haiyu, Song Mingcong, Li Tao, et al.Lergan:A zero-free, low data movement and pim-based gan architecture[C] //Proc of the 51st Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2018:669- 681
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_8" title="Wenqin Huangfu, Li Shuangchen, Hu Xing, et al.Radar:A 3D-ReRAM based DNA alignment accelerator architecture[C] //Proc of 2018 the 55th ACM/ESDA/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2018:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Radar A 3D-ReRAM based DNA alignment accelerator architecture">
                                        <b>[8]</b>
                                        Wenqin Huangfu, Li Shuangchen, Hu Xing, et al.Radar:A 3D-ReRAM based DNA alignment accelerator architecture[C] //Proc of 2018 the 55th ACM/ESDA/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2018:1- 6
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_9" title="Cheng Ming, Xia Lixue, Zhu Zhenhua, et al.Time:A training-in-memory architecture for memristor-based deep neural networks[C] //Proc of 2017 the 54th IEEE Design Automation Conf (DAC) .New York:ACM, 2017:26- 31" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Time A training-in-memory architecture for memristor-based deep neural networks">
                                        <b>[9]</b>
                                        Cheng Ming, Xia Lixue, Zhu Zhenhua, et al.Time:A training-in-memory architecture for memristor-based deep neural networks[C] //Proc of 2017 the 54th IEEE Design Automation Conf (DAC) .New York:ACM, 2017:26- 31
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_10" title="Bojnordi M N.Memristive Boltzmann machine:A hardware accelerator for combinatorial optimization and deep learning[C] //Proc of the 22nd IEEE Int Symp on High Performance Computer Architecture.Los Alamitos, CA:IEEE Computer Society, 2016:1- 13" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Memristive Boltzmann machine:A hardware accelerator for combinatorial optimization and deep learning">
                                        <b>[10]</b>
                                        Bojnordi M N.Memristive Boltzmann machine:A hardware accelerator for combinatorial optimization and deep learning[C] //Proc of the 22nd IEEE Int Symp on High Performance Computer Architecture.Los Alamitos, CA:IEEE Computer Society, 2016:1- 13
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_11" title="Kim D, Kung J, Chai S, et al.Neurocube:A programmable digital neuromorphic architecture with high-density 3D memory[C] //Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:380- 392" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neurocube:A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory">
                                        <b>[11]</b>
                                        Kim D, Kung J, Chai S, et al.Neurocube:A programmable digital neuromorphic architecture with high-density 3D memory[C] //Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:380- 392
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_12" title="Gao Mingyu, Kozyrakis C.Hrl:Efficient and flexible reconfigurable logic for near-data processing[C] //Proc of the 22nd IEEE Int Symp on High Performance Computer Architecture.Los Alamitos, CA:IEEE Computer Society, 2016:126- 137" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hrl:Efficient and flexible reconfigurable logic for near-data processing">
                                        <b>[12]</b>
                                        Gao Mingyu, Kozyrakis C.Hrl:Efficient and flexible reconfigurable logic for near-data processing[C] //Proc of the 22nd IEEE Int Symp on High Performance Computer Architecture.Los Alamitos, CA:IEEE Computer Society, 2016:126- 137
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_13" title="Kim H, Kim H, Yalamanchili S, et al.Understanding energy aspects of processing-near-memory for HPC workloads[C] //Proc of the 2015 Int Symp on Memory Systems.New York:ACM:276- 282" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding energy aspects of processing-near-memory for HPC workloads">
                                        <b>[13]</b>
                                        Kim H, Kim H, Yalamanchili S, et al.Understanding energy aspects of processing-near-memory for HPC workloads[C] //Proc of the 2015 Int Symp on Memory Systems.New York:ACM:276- 282
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_14" title="Farmahini- Farahani A, Ahn J H, Morrow K, et al.Drama:An architecture for accelerated processing near memory[J].IEEE Computer Architecture Letters, 2017, 14 (1) :26- 29" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Drama:An architecture for accelerated processing near memory">
                                        <b>[14]</b>
                                        Farmahini- Farahani A, Ahn J H, Morrow K, et al.Drama:An architecture for accelerated processing near memory[J].IEEE Computer Architecture Letters, 2017, 14 (1) :26- 29
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_15" title="Ahn J, Yoo S, Mutlu O, et al.Pim-enabled instructions:A low-overhead, locality-aware processing-in-memory architecture[J].ACM SIGARCH Computer Architecture News, 2015, 43 (3) :336- 348" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCME1F7DEC8D36B382B0BB389570B223BEA&amp;v=MDYzMzRRMzVOeGd6THUveEtvPU5pZklZOGE1YU5hNDJ2eE5FT2dKZm44eHpXUVRtRTErUUhibnF4SkhlN0NYTjgvdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        Ahn J, Yoo S, Mutlu O, et al.Pim-enabled instructions:A low-overhead, locality-aware processing-in-memory architecture[J].ACM SIGARCH Computer Architecture News, 2015, 43 (3) :336- 348
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_16" title="Chen Yunji, Luo Tao, Liu Shaoli, et al.Dadiannao:A machine-learning supercomputer[C] //Proc of the 47th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2014:609- 622" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dadiannao:Amachine-learning supercomputer">
                                        <b>[16]</b>
                                        Chen Yunji, Luo Tao, Liu Shaoli, et al.Dadiannao:A machine-learning supercomputer[C] //Proc of the 47th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2014:609- 622
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_17" title="Liu T Y, Yan T H, Scheuerlein R, et al.A 130.7 mm2, 2-layer 32-GB ReRAM memory device in 24-nm technology[C] //Proc of IEEE Int Solid-State Circuits Conf (Digest of Technical Papers) .Piscataway, NJ:IEEE, 2013:140- 153" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A 130.7 mm2 2-layer 32Gb ReRAM memory device in 24nm technology">
                                        <b>[17]</b>
                                        Liu T Y, Yan T H, Scheuerlein R, et al.A 130.7 mm2, 2-layer 32-GB ReRAM memory device in 24-nm technology[C] //Proc of IEEE Int Solid-State Circuits Conf (Digest of Technical Papers) .Piscataway, NJ:IEEE, 2013:140- 153
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_18" title="Akinaga H, Shima H.Resistive random access memory (ReRAM) based on metal oxides[J].Proceedings of the IEEE, 2010, 98 (12) :2237- 2251" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Resistive Random Access Memory (ReRAM) Based on Metal Oxides">
                                        <b>[18]</b>
                                        Akinaga H, Shima H.Resistive random access memory (ReRAM) based on metal oxides[J].Proceedings of the IEEE, 2010, 98 (12) :2237- 2251
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_19" title="Wei Z, Kanzawa Y, Arita K, et al.Highly reliable taox ReRAM and direct evidence of redox reaction mechanism[C] //Proc of IEEE Int Electron Devices Meeting.Piscataway, NJ:IEEE, 2008:1- 4" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Highly reliable TaOx ReRAM and direct evidence of redox reaction mechanism">
                                        <b>[19]</b>
                                        Wei Z, Kanzawa Y, Arita K, et al.Highly reliable taox ReRAM and direct evidence of redox reaction mechanism[C] //Proc of IEEE Int Electron Devices Meeting.Piscataway, NJ:IEEE, 2008:1- 4
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_20" title="Liu Qi, Sun Jun, L&#252; Hangbing, et al.Real-time observation on dynamic growth/dissolution of conductive filaments in oxide-electrolyte-based ReRAM[J].Advanced Materials, 2012, 24 (14) :1844- 1849" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-Time Observation on Dynamic Growth/Dissolution of Conductive Filaments in Oxide-Electrolyte-Based ReRAM">
                                        <b>[20]</b>
                                        Liu Qi, Sun Jun, L&#252; Hangbing, et al.Real-time observation on dynamic growth/dissolution of conductive filaments in oxide-electrolyte-based ReRAM[J].Advanced Materials, 2012, 24 (14) :1844- 1849
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_21" title="Burr G W, Shelby R M, Sidler S, et al.Experimental demonstration and tolerancing of a large-scale neural network (165000synapses) using phase-change memory as the synaptic weight element[J].IEEE Transactions on Electron Devices, 62 (11) :3498- 3507" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Experimental demonstration and tolerancing of a large-scale neural network (165000synapses) using phase-change memory as the synaptic weight element">
                                        <b>[21]</b>
                                        Burr G W, Shelby R M, Sidler S, et al.Experimental demonstration and tolerancing of a large-scale neural network (165000synapses) using phase-change memory as the synaptic weight element[J].IEEE Transactions on Electron Devices, 62 (11) :3498- 3507
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_22" title="Burgt Y, Lubberman E, Fuller E, et al.A non-volatile organic electrochemical device as a low-voltage artificial synapse for neuromorphic computing[J].Nature Materials, 2017, 16 (4) :414- 418" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-volatile organic electrochemical device as a low-voltage artificial synapse for neuromorphic computing">
                                        <b>[22]</b>
                                        Burgt Y, Lubberman E, Fuller E, et al.A non-volatile organic electrochemical device as a low-voltage artificial synapse for neuromorphic computing[J].Nature Materials, 2017, 16 (4) :414- 418
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_23" title="Agarwal S, Gedrim R B J, Hsia A H, et al.Achieving ideal accuracies in analog neuromorphic computing using periodic carry[C] //Proc of Symp on VLSI Technology.Piscataway, NJ:IEEE, 2017:174- 175" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Achieving ideal accuracies in analog neuromorphic computing using periodic carry">
                                        <b>[23]</b>
                                        Agarwal S, Gedrim R B J, Hsia A H, et al.Achieving ideal accuracies in analog neuromorphic computing using periodic carry[C] //Proc of Symp on VLSI Technology.Piscataway, NJ:IEEE, 2017:174- 175
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_24" title="Narayanan P, Fumarola A, Sanches L L, et al.Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory[J].IBM Journal of Research and Development, 2017, 61 (4) :1- 11" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory">
                                        <b>[24]</b>
                                        Narayanan P, Fumarola A, Sanches L L, et al.Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory[J].IBM Journal of Research and Development, 2017, 61 (4) :1- 11
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_25" title="Muralimanohar N, Balasubramonian R, Jouppi N.Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0[C] //Proc of the 38th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2007:3- 14" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing NUCA Organizations and Wiring Alternatives for Large Caches With CACTI 6.0">
                                        <b>[25]</b>
                                        Muralimanohar N, Balasubramonian R, Jouppi N.Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0[C] //Proc of the 38th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2007:3- 14
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_26" title="Jouppi N P, Kahng A B, Muralimanohar N, et al.CACTI-IO:CACTI with off-chip power-area-timing models[J].IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2015, 23 (7) :1254- 1267" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CACTI-IO CACTI with off-chip power-area-timing models">
                                        <b>[26]</b>
                                        Jouppi N P, Kahng A B, Muralimanohar N, et al.CACTI-IO:CACTI with off-chip power-area-timing models[J].IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2015, 23 (7) :1254- 1267
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_27" title="Poremba M, Mittal S, Li D, et al.DESTINY:A tool for modeling emerging 3D NVM and eDRAM caches[C] //Proc of Design Automation and Test in Europe (DATE) .Piscataway, NJ:IEEE, 2015:1543- 1546" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DESTINY A tool for modeling emerging 3D NVM and eDRAM caches">
                                        <b>[27]</b>
                                        Poremba M, Mittal S, Li D, et al.DESTINY:A tool for modeling emerging 3D NVM and eDRAM caches[C] //Proc of Design Automation and Test in Europe (DATE) .Piscataway, NJ:IEEE, 2015:1543- 1546
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_28" title="Lecun Y.LeNet[OL].[2019-01-04].http://yann.lecun.com/exdb/lenet/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LeNet[OL]">
                                        <b>[28]</b>
                                        Lecun Y.LeNet[OL].[2019-01-04].http://yann.lecun.com/exdb/lenet/
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_29" title="Liu Liu.ConvNet[OL].[2019-01-04].http://libccv.org/doc/doc-convnet/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ConvNet[OL]">
                                        <b>[29]</b>
                                        Liu Liu.ConvNet[OL].[2019-01-04].http://libccv.org/doc/doc-convnet/
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_30" title="Subramanian A S.Caffe Model Zoo[OL].[2019-01-04].https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe Model Zoo[OL]">
                                        <b>[30]</b>
                                        Subramanian A S.Caffe Model Zoo[OL].[2019-01-04].https://github.com/BVLC/caffe/wiki/Model-Zoo
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_31" title="Lecun Y.MNIST[OL].[2019-01-04].http://yann.lecun.com/exdb/mnist/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MNIST[OL]">
                                        <b>[31]</b>
                                        Lecun Y.MNIST[OL].[2019-01-04].http://yann.lecun.com/exdb/mnist/
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_32" title="Stanford University, Princeton University.ImageNet[OL].[2019-01-04].http://www.image-net.org/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet[OL]">
                                        <b>[32]</b>
                                        Stanford University, Princeton University.ImageNet[OL].[2019-01-04].http://www.image-net.org/
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(06),1149-1160 DOI:10.7544/issn1000-1239.2019.20190099            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于3D忆阻器阵列的神经网络内存计算架构</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AF%9B%E6%B5%B7%E5%AE%87&amp;code=42095394&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">毛海宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%88%92%E7%BB%A7%E6%AD%A6&amp;code=08179569&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">舒继武</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学计算机科学与技术系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现如今, 由于人工智能的飞速发展, 基于忆阻器的神经网络内存计算 (processing in memory, PIM) 架构吸引了很多研究者的兴趣, 因为其性能远优于传统的冯·诺依曼计算机体系结构的性能.配备了支持功能单元的外围电路, 忆阻器阵列可以以高并行度以及相比于CPU和GPU更少的数据移动来处理一个前向传播.然而, 基于忆阻器的内存计算硬件存在忆阻器的外围电路面积过大以及不容忽视的功能单元利用率过低的问题.提出了一种基于3D忆阻器阵列的神经网络内存计算架构FMC (function-pool based memristor cube) , 通过把实现功能单元的外围电路聚集到一起, 形成一个功能单元池来供多个堆叠在其上的忆阻器阵列共享.还提出了一种针对基于3D忆阻器阵列的内存计算的数据映射策略, 进一步提高功能单元的利用率并减少忆阻器立方体之间的数据传输.这种针对基于3D忆阻器阵列的内存计算的软硬件协同设计不仅充分利用了功能单元, 并且缩短了互联电路、提供了高性能且低能耗的数据传输.实验结果表明:在只训练单个神经网络时, 提出的FMC能使功能单元的利用率提升43.33倍;在多个神经网络训练任务的情况下, 能提升高达58.51倍.同时, 和有相同数目的Compute Array及Storage Array的2D-PIM比较, FMC所占空间仅为2D-PIM的42.89%.此外, FMC相比于2D-PIM有平均1.5倍的性能提升, 并且有平均1.7倍的能耗节约.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%E5%BF%86%E9%98%BB%E5%99%A8%E9%98%B5%E5%88%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D忆阻器阵列;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">内存计算;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%96%E5%9B%B4%E7%94%B5%E8%B7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">外围电路;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%92%E8%81%94%E7%BA%BF%E8%B7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">互联线路;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    毛海宇, mhy15@mails.tsinghua.edu.cn, born in 1993.PhD candidate.Her main research interests include NVM based main memory, processing in memory and machine learning.;
                                </span>
                                <span>
                                    *舒继武, shujw@tsinghua.edu.cn, born in 1968.Professor and PhD supervisor.Fellow of CCF. His main research interests include non-volatile memory systems and technologies, network (cloud/big data) storage system, storage security and reliability, and parallel and distributed computing.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2018YFB1003301);</span>
                                <span>国家自然科学基金项目 (61832011);</span>
                    </p>
            </div>
                    <h1><b>3D Memristor Array Based Neural Network Processing in Memory Architecture</b></h1>
                    <h2>
                    <span>Mao Haiyu</span>
                    <span>Shu Jiwu</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science and Technology, Tsinghua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Nowadays, due to the rapid development of artificial intelligence, the memristor-based processing in memory (PIM) architecture for neural network (NN) attracts a lot of researchers' interests since it performs much better than traditional von Neumann architecture. Equipped with the peripheral circuit to support function units, memristor arrays can process a forward propagation with higher parallelism and much less data movement than that in CPU and GPU. However, the hardware of the memristor-based PIM suffers from the large area overhead of peripheral circuit outside the memristor array and non-trivial under-utilization of function units. This paper proposes a 3 D memristor array based PIM architecture for NNs (FMC) by gathering the peripheral circuit of function units into a function pool for sharing among memristor arrays that pile up on the pool. We also propose a data mapping scheme for the 3 D memristor array based PIM architecture to further increase the utilization of function units and reduce the data transmission among different cubes. The software-hardware co-design for the 3 D memristor array based PIM not only makes the most of function units but also shortens the wire interconnections for better high-performance and energy-efficient data transmission. Experiments show that when training a single neural network, our proposed FMC can achieve up to 43.33 times utilization of the function units and can achieve up to 58.51 times utilization of the function units when training multiple neural networks. At the same time, compared with the 2 D-PIM which has the same amount of compute array and storage array, FMC only occupies 42.89% area of 2 D-PIM. What's more, FMC has 1.5 times speedup and 1.7 times energy saving compared with 2 D-PIM.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20memristor%20array&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D memristor array;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=processing%20in%20memory%20(PIM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">processing in memory (PIM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=peripheral%20circuit&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">peripheral circuit;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=wire%20interconnection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">wire interconnection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2018YFB1003301);</span>
                                <span>the National Natural Science Foundation of China (61832011);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="69">近年来, 基于忆阻器的神经网络内存计算加速器倍受学术研究者和工业界的关注<citation id="343" type="reference"><link href="276" rel="bibliography" /><link href="278" rel="bibliography" /><link href="280" rel="bibliography" /><link href="282" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>.研究表明, 数据在CPU和片外存储之间的传输消耗的能量比一个浮点运算所消耗的能量高2个数量级<citation id="340" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.基于忆阻器的内存加速器将计算与存储紧密结合, 从而省去传统的冯·诺依曼体系结构的中心处理器和内存之间的数据传输, 进而提升整体系统的性能并节省大部分的系统能耗<citation id="341" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.此类加速器通过在忆阻器阵列外部加入一些功能单元, 使忆阻器阵列能在几乎一个读操作的延迟内完成一次向量乘矩阵操作 (matrix-vector-multiplication, MVM) <citation id="342" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 它是神经网络计算中的主要操作.</p>
                </div>
                <div class="p1">
                    <p id="70">虽然基于忆阻器的神经网络内存计算加速器有着很高的性能和很低的能耗, 但是当它用于神经网络训练任务时, 忆阻器阵列的外围电路利用率很低.这是因为:1) 当执行前向传播时, 用于反向传播的忆阻器阵列的外围功能单元都处于空闲状态; 2) 当训练的批大小 (<i>batch</i>_<i>size</i>) 较小时, 无论是在前向还是反向的传播过程中, 都有一些忆阻器阵列的外围功能单元处于空闲状态.不仅如此, 忆阻器阵列的外围功能单元还占据了极大的面积.例如一个8-bit ADC (模拟转数字的原件) 的面积就是一个128×128的忆阻器阵列的面积的48倍<citation id="344" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.由于目前的基于忆阻器的神经网络内存计算加速器的外围电路存在上述2个问题, 使得整个芯片的面积大且利用率低.</p>
                </div>
                <div class="p1">
                    <p id="71">因此, 本文针对上述问题提出了一种基于3D忆阻器阵列的神经网络内存计算架构:基于功能单元池的忆阻器立方体 (function-pool based memristor cube, FMC) 通过提供一个功能单元池给多个堆叠在其上的忆阻器阵列共享, 而不是为每一个忆阻器阵列配备所有的功能单元电路, 从而达到减小芯片面积并提高功能单元利用率的目的.这种通过3D堆叠的方式进行功能单元共享的结构不仅减小了所有功能单元的占用面积, 还极大地缩短了互联线, 使得整体的互联面积减小.与此同时, 由于互联线路的缩短, FMC还减少了数据的传输, 从而使得整体加速器结构的性能提高且能耗降低.</p>
                </div>
                <div class="p1">
                    <p id="72">为了更好地利用FMC, 本文用软硬件协同设计的方式, 进一步提出了基于FMC的计算数据排布策略——功能单元池感知的数据排布 (function-pool aware data mapping, FDM) .FDM通过配合FMC工作, 使得数据移动更少, 功能单元的利用率更高, 进而提高整体架构的性能.</p>
                </div>
                <div class="p1">
                    <p id="73">实验结果表明:在单个训练任务的情况下, 我们提出的FMC能使功能单元的利用率提升43.33倍, 在多个任务的情况下能提升高达58.51倍.同时, 和有相同数目的Compute Array及Storage Array的2D-PIM比较, FMC所占空间仅为2D-PIM的42.89%.而且, FMC相比于2D-PIM有1.5倍的性能提升, 且有1.7倍的能耗节约.</p>
                </div>
                <div class="p1">
                    <p id="74">本文的主要贡献有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="75">1) 分析并发现基于忆阻器的神经网络内存计算加速器的外围电路存在占用面积大且在神经网络的训练过程中外围电路存在利用率极低的问题.</p>
                </div>
                <div class="p1">
                    <p id="76">2) 提出了一种基于3D忆阻器阵列的神经网络内存计算架构, 通过将忆阻器阵列3D堆叠在根据系统结构配置设计的功能单元池上来共享外围功能电路资源, 从而达到减小芯片面积的占用、提高资源利用率的目的.</p>
                </div>
                <div class="p1">
                    <p id="77">3) 提出了一种基于3D忆阻器阵列的计算数据排布策略, 通过设计基于3D忆阻器阵列的硬件架构的数据排布策略来更好地利用此架构, 使得数据移动尽可能少且资源利用率尽可能高.</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="79">由于神经网络的计算 (推理和训练) 受限于传统的冯·诺依曼体系结构中片上处理器到片外的存储之间有限的带宽, 研究者们提出了内存计算, 即将计算单元和存储单元相结合, 从而避免两者间大量的数据传输.现在的内存计算可分为两大类:基于忆阻器的内存计算 (processing in memory, PIM) <citation id="345" type="reference"><link href="276" rel="bibliography" /><link href="278" rel="bibliography" /><link href="280" rel="bibliography" /><link href="282" rel="bibliography" /><link href="290" rel="bibliography" /><link href="292" rel="bibliography" /><link href="294" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>和基于DRAM的内存计算——近数据计算 (near data computing, NDC) <citation id="346" type="reference"><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><link href="300" rel="bibliography" /><link href="302" rel="bibliography" /><link href="304" rel="bibliography" /><link href="306" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>.PIM通过直接利用忆阻器的特性, 将存储资源直接用于做计算, 取得计算存储相融合的效果.而NDC则是通过将计算资源靠近存储单元摆放, 并通过一些高带宽的连接使得计算资源能快速地访问存储资源, 例如混合记忆立方体 (hybrid memory cube, HMC) .</p>
                </div>
                <div class="p1">
                    <p id="80">图1描述了基于忆阻器的PIM的基本电路结构和计算原理<citation id="347" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.如图1 (a) 所示, 每个忆阻器单元的阻值代表一个数值, 每个电压代表一个输入值, 根据Kirchoff定律, 通过模拟电路域的电流加, 能得到一个代表输出的电流值, 如图1 (a) 中公式所示.因此, 在忆阻器阵列外围加上一些功能单元, 如图1 (b) 所示, 一个4×4的忆阻器阵列能够用来存储一个4×4的矩阵, 然后加上代表4个1×4的向量的输入电压, 就能在几乎一个读延迟内完成一次向量乘矩阵的运算.而向量乘矩阵的运算是神经网络计算中的主要运算, 因此, 基于忆阻器的PIM加速器能极大地提升神经网络计算的性能.</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于忆阻器阵列的内存计算硬件结构" src="Detail/GetImg?filename=images/JFYZ201906003_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于忆阻器阵列的内存计算硬件结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_081.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Structure of memristor-based PIM array</p>

                </div>
                <div class="p1">
                    <p id="82">NDC最常用的硬件结构就是HMC.HMC通过在逻辑晶粒 (计算单元) 上堆叠一些DRAM晶粒 (存储单元) , 然后用一些垂直贯穿DRAM的穿过硅片通道 (through silicon vias, TSV) 使它们相连, 从而使得计算单元到存储单元拥有很高的访存带宽.但是其本质上还是计算和存储相分离的, 不同于基于忆阻器的PIM, NDC中用于存储的DRAM晶粒并不能直接用来做计算.因此, PIM的性能通常优于NDC.</p>
                </div>
                <div class="p1">
                    <p id="83">目前对于PIM的研究更多地针对推理, 因为推理的计算以及数据流要比训练简单得多, 但是它们所用到的硬件功能单元基本一样.文献<citation id="348" type="reference">[<a class="sup">1</a>]</citation>提出了用具有高存储密度、低读写延迟、相比于其余NVM有较长寿命的ReRAM<citation id="356" type="reference"><link href="308" rel="bibliography" /><link href="310" rel="bibliography" /><link href="312" rel="bibliography" /><link href="314" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>来作为PIM的基础硬件单元, 并设计ReRAM阵列可被配置为3种模式:计算、存储和缓存, 再通过加上支持其余神经网络计算的硬件单元来做神经网络的推理计算.文献<citation id="349" type="reference">[<a class="sup">2</a>]</citation>通过使用ReRAM阵列做向量乘矩阵的计算单元, 并利用eDRAM作为其存储单元, 再通过设计推理的流水线计算, 从而提高用PIM来做神经网络推理的效率.文献<citation id="350" type="reference">[<a class="sup">3</a>]</citation>通过复制多份计算单元, 减少当PIM用来做神经网络训练时流水线计算中的空闲, 从而提升PIM用来做神经网络训练任务的效率.文献<citation id="351" type="reference">[<a class="sup">4</a>]</citation>使用比ReRAM有更接近于线性电阻值更新的PCM<citation id="357" type="reference"><link href="316" rel="bibliography" /><link href="318" rel="bibliography" /><link href="320" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>加上CMOS作为PIM的基础硬件单元<citation id="352" type="reference"><link href="322" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 在同一个阵列中完成神经网络训练时的前向传播和反向传播操作.但是文献<citation id="353" type="reference">[<a class="sup">4</a>]</citation>只能支持仅有全连接层的神经网络的训练, 不能够支持卷积神经网络的训练, 而文献<citation id="354" type="reference">[<a class="sup">3</a>]</citation>可以支持.文献<citation id="355" type="reference">[<a class="sup">7</a>]</citation>提出了一种用来支持生成对抗网络的基于ReRAM的PIM, 它通过软硬件结合的设计, 省去生成对抗网络中的冗余计算和存储, 并通过运行时可重配的互联提升PIM支持复杂训练数据流时的性能.</p>
                </div>
                <div class="p1">
                    <p id="84">然而目前针对PIM的研究都以提高PIM的计算效率为目的, 并没有关注PIM的芯片面积以及其中的资源利用率.本文考虑了PIM芯片中的功能单元的面积占用问题及其利用率问题.还需说明的是:本文提出的基于忆阻器的3D结构不同于HMC的结构, 其存储本身能够用来做计算, 且最下层的逻辑晶粒 (本文中的资源池) 不仅仅是计算资源, 还有用来支撑忆阻器做计算的功能单元 (例如图1中的DAC和ADC) .</p>
                </div>
                <div class="p1">
                    <p id="85">下文首先介绍设计的FMC架构, 然后介绍基于FMC的数据映射, 再后给出FMC的功能单元资源利用率、空间占用情况和FMC+FDM的性能和能耗等实验结果及相应的分析, 最后给出本文总结.</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag"><b>2 FMC架构设计</b></h3>
                <div class="p1">
                    <p id="87">在本节中, 我们首先给出了FMC的架构概要图, 如图2所示, 然后我们分别介绍FMC的忆阻器阵列、互联结构、共享资源池.</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于功能池的忆阻器立方体的概要图" src="Detail/GetImg?filename=images/JFYZ201906003_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于功能池的忆阻器立方体的概要图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Outline of the function-pool based memristor cube</p>

                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 忆阻器阵列的平面互联" src="Detail/GetImg?filename=images/JFYZ201906003_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 忆阻器阵列的平面互联  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Intra-connections of 2D memristors</p>

                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.1 FMC架构</b></h4>
                <div class="p1">
                    <p id="91">FMC架构主要分成3个部分:忆阻器阵列、3D互联结构和共享资源池 (如图2的中部所示) .共享功能单元池放置于最下方, 其上堆叠若干个忆阻器阵列晶粒层.每一层有若干个忆阻器阵列晶粒, 一部分用来做普通的存储 (本文中称为Storage Array) , 一部分既可以通过配置用来做存储也可以设置其用来做计算 (本文中称为Compute Array) .每一层均通过垂直的连接和共享资源层相连.其层数以及每层忆阻器阵列的数目受限于工艺大小以及资源池的资源多少.</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>2.2 FMC的忆阻器阵列</b></h4>
                <div class="p1">
                    <p id="93">FMC使用cross-bar结构的ReRAM作为其基础硬件单元, 如图2左部所示, 保留了作为存储器时的外围电路单元, 例如写驱动 (未在图2中标出) .每一个正方形表示一个ReRAM cell.当这个忆阻器阵列被配置成Compute Array时, 每个ReRAM cell用来存储一个神经网络中权重的值或者存储值的一部分 (即用多个cell存储一个值, 每个cell存储多个位) ;当这个忆阻器阵列被配置成Storage Array时, 整个阵列就被当做普通存储器使用, 一个cell存储一个位.</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>2.3 FMC的互联结构</b></h4>
                <div class="p1">
                    <p id="95">FMC中的每层忆阻器阵列Storage Array中的忆阻器阵列通过H-tree的方式进行连接, Compute Array中的忆阻器阵列通过基于H-tree的可重配的连接方式进行连接 (与文献<citation id="358" type="reference">[<a class="sup">7</a>]</citation>的平面连接方式相同) .Storage Array和Compute Array之间通过一个高速的共享线路进行连接.</p>
                </div>
                <div class="p1">
                    <p id="96">图3给出了忆阻器阵列的平面互联结构.左边是Storage Array的互联结构.和普通存储一样, Storage Array用H-tree的连接方式:灰色圆圈代表merging node, 连接的父节点的线路宽是连接子节点线路宽的2倍;灰色方块代表multiplexing node, 连接的父节点的线路宽和连接子节点线路宽一样.右边是Compute Array的连接方式:我们保留原来的H-tree的连接方式 (图3中灰色线条) , 并在同一层连接节点中不共父节点的节点 (如图3中黄色节点所示) 之间加上一条电路线 (如图3中绿色线条所示) .新加的这条电路线的宽度与其连接其父节点的线路宽度相同.由于接口有限, 图3中每个黄色节点被增加了一个转换接口, 使得它们能选择连到它们的父节点或者相邻节点.这种能够在网络训练时的动态配置的连接方式不仅能够支持快速的Compute Array之间的数据传输 (开关拨到横向连接线上, 跨过H-tree结构进行通信) , 还能支持快速的权重更新, 即Compute Array的读写 (开关配置成H-tree的连接方式) .图3中间蓝色部分表示的是Compute Array和Storage Array之间的共享高速连接, 这部分并不是将它们直接相连, 而是经由共享资源池将它们相连 (如图2中蓝色部分所示) .</p>
                </div>
                <div class="p1">
                    <p id="97">Compute Array中每一个忆阻器阵列 (橘黄色方块) 都配备有一个转换接口, 能控制每个忆阻器阵列是垂直连接 (连向共享资源池或者不同平面的忆阻器阵列) 还是平面连接 (连向同平面的忆阻器阵列) .如果一个忆阻器阵列连接到下一个忆阻器阵列, 那么我们称这2个忆阻器阵列是在同一个模拟电路域的, 反之则不是.我们把一个模拟域内的所有连向共享资源池的阵列连接叫做这个模拟域的出入口, 入口连接共享资源池的DAC分池, 出口连接S+H池.</p>
                </div>
                <div class="p1">
                    <p id="98">当有多个FMC时, 各个FMC之间通过C-mesh的连接方式进行互联, 每个FMC的共享资源层能够访问其他FMC的Storage Array, 但是不能使用其他FMC的Compute Array, 即FMC之间只在数字域进行通信而不用模拟信号通信, 这样确保了模拟信号的稳定性和计算的准确性.</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.4 FMC的共享资源池</b></h4>
                <div class="p1">
                    <p id="100">共享资源池由六大部分组成 (如图2右部所示) :DAC池、S+H池、ADC池、S+A池、MP (Max Pool) 池和激活函数单元池.每个分池由若干个功能单元组成.表1给出了这些功能单元的名称及其对应的释义.</p>
                </div>
                <div class="p1">
                    <p id="101">图4给出了各个分池之间的互联结构.DAC池连接模拟电路域的入口, S+H池连接其出口.S+H池和ADC池相连接, ADC池可选择连接S+A池、MP池、激活函数单元池和Storage Array部分.S+A池可选择连接MP池、激活函数单元池、DAC池和Storage Array部分.激活函数单元池可选择连接MP池、DAC池和Storage Array部分.MP池可选择连接DAC池和Storage Array部分.每个分池内的计算资源通过C-mesh进行连接, 分池之间通过高速共享线路进行连接.</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表1 功能单元的参数表示</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Notations of Function Units</b></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td><br />Function Unit</td><td>Description</td></tr><tr><td><br />DAC</td><td>Digital to Analog</td></tr><tr><td><br />ADC</td><td>Analog to Digital</td></tr><tr><td><br />S+H</td><td>Sample and Hold</td></tr><tr><td><br />S+A</td><td>Shift and Add</td></tr><tr><td><br />MP</td><td>Max Pool Unit</td></tr><tr><td><br />Sigmoid</td><td>Sigmoid Unit</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 共享资源池各个分池之间的互联结构" src="Detail/GetImg?filename=images/JFYZ201906003_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 共享资源池各个分池之间的互联结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Intra-connection of sub-pools in function unit pool</p>

                </div>
                <div class="p1">
                    <p id="104">表2提供了共享资源池中功能单元的各个配置参数表示, 我们将使用这些配置参数来介绍共享资源池的具体设计方法.</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表2 功能单元的配置参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Configurations of Function Units</b></p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br />Function Unit</td><td>Area/mm<sup>2</sup></td><td>Number</td><td>Configuration</td></tr><tr><td><br />DAC</td><td><i>A</i><sub>DAC</sub></td><td><i>N</i><sub>DAC</sub></td><td><i>x</i>-bit</td></tr><tr><td><br />ADC</td><td><i>A</i><sub>ADC</sub></td><td><i>N</i><sub>ADC</sub></td><td><i>y</i>-bit, <i>f</i> GSps</td></tr><tr><td><br />S+H</td><td><i>A</i><sub>SH</sub></td><td><i>N</i><sub>SH</sub></td><td></td></tr><tr><td><br />S+A</td><td><i>A</i><sub>SA</sub></td><td><i>N</i><sub>SA</sub></td><td></td></tr><tr><td><br />MP</td><td><i>A</i><sub>MP</sub></td><td><i>N</i><sub>MP</sub></td><td></td></tr><tr><td><br />Sigmoid</td><td><i>A</i><sub>S</sub></td><td><i>N</i><sub>S</sub></td><td></td></tr><tr><td rowspan="2"><br />Memristor Array</td><td rowspan="2"><i>A</i><sub>MA</sub></td><td><br /><i>N</i><sub>storage</sub></td><td><i>n</i>-bit/cell</td></tr><tr><td><br /><i>N</i><sub>compute</sub></td><td><i>m</i>×<i>m</i> cell/array</td></tr><tr><td><br />Hyper Tr</td><td></td><td></td><td><i>b</i> GBps</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="106">表2是各个单元的相关配置参数表示, FMC的功能单元池的面积<i>A</i><sub>FUP</sub>可表示为</p>
                </div>
                <div class="p1">
                    <p id="107"><i>A</i><sub>FUP</sub>=<i>A</i><sub>DAC</sub><i>N</i><sub>DAC</sub>+<i>A</i><sub>ADC</sub><i>N</i><sub>ADC</sub>+<i>A</i><sub>SH</sub><i>N</i><sub>SH</sub>+<i>A</i><sub>SA</sub><i>N</i><sub>SA</sub>+<i>A</i><sub>MP</sub><i>N</i><sub>MP</sub>+<i>A</i><sub>S</sub><i>N</i><sub>S</sub>, </p>
                </div>
                <div class="p1">
                    <p id="109">即功能单元池的总面积等于各个功能分池的面积和, 各个分池的面积又取决于单个功能单元的面积以及其个数.为了节省FMC总体占用空间, 其功能单元池的面积<i>A</i><sub>FUP</sub>需满足:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo>≤</mo><mfrac><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>U</mtext><mtext>Ρ</mtext></mrow></msub><mo>-</mo><mi>A</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>A</mtext></mrow></msub><mrow><mo> (</mo><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>t</mtext><mtext>o</mtext><mtext>r</mtext><mtext>a</mtext><mtext>g</mtext><mtext>e</mtext></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>m</mtext><mtext>p</mtext><mtext>u</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msub></mrow><mo>) </mo></mrow></mrow><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>U</mtext><mtext>Ρ</mtext></mrow></msub></mrow></mfrac><mo>≤</mo><mi>θ</mi><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">其中, 0≤<i>θ</i>≤0.1.即功能单元池的面积既不能小于其上堆叠的ReRAM阵列的面积, 否则无法充分利用功能单元池的空间;也不能过分大于ReRAM阵列的面积, 否则会导致3D堆叠时的连接线路过长, 不高效.同时, 为了使得模拟电路域到数字电路域的顺畅转换, ADC和DAC单元的参数需满足:</p>
                </div>
                <div class="p1">
                    <p id="112"><i>x</i>×<i>N</i><sub>DAC</sub>=<i>n</i>×<i>m</i>      (1) </p>
                </div>
                <div class="p1">
                    <p id="113"><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>u</mtext><mtext>b</mtext><mtext>C</mtext><mtext>o</mtext><mtext>m</mtext></mrow></msub><mo>×</mo><mfrac><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><mrow><mi>l</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>y</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mn>9</mn></msup><mo>≤</mo><mi>f</mi><mo>×</mo><mi>y</mi><mo>×</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>D</mtext><mtext>A</mtext><mtext>C</mtext></mrow></msub><mo>≤</mo><mn>8</mn><mi>b</mi></mrow></math></mathml>,      (2) </p>
                </div>
                <div class="p1">
                    <p id="115">其中, <i>latency</i><sub>PIM</sub>是每个忆阻器阵列一次模拟域向量乘矩阵运算的延迟, 单位是ns;<i>N</i><sub>SubCom</sub>是在一个共享线路上的所有用于计算工作的忆阻器阵列的数目.式 (1) 主要使得DAC转换的位数和一个阵列中一行的位数相等, 避免计算延迟.式 (2) 主要使得ADC转换的位数不能小于计算速度, 防止计算出来的数据阻塞在ADC之前;且ADC转换速率不能大于传输速度, 否则算出的数据会被阻塞在ADC之后.</p>
                </div>
                <div class="p1">
                    <p id="117">有了这些公式的限制, 我们就能根据实际系统情况的限制以及任务的需求, 很好地制定出我们所需的功能单元池中每个功能单元的数目, 从而设计高效的功能单元池.</p>
                </div>
                <h3 id="118" name="118" class="anchor-tag"><b>3 FDM策略设计</b></h3>
                <div class="p1">
                    <p id="119">在本节我们介绍FDM策略, 配合FMC的硬件结构, 使得存储与计算资源分配合理、数据传输尽可能少、共享资源池的资源利用率尽可能高.由于FMC中有多层PIM阵列层, 每层只有有限个PIM阵列, 所以在神经网络的训练过程中可能需要多个PIM层的多个阵列.每一层中PIM阵列的数据传输要比每层间的数据传输延迟小, 并且神经网络的每个层之内、每个层之间已经训练的不同阶段都会有数据依赖.因此FDM的核心思想是将有数据依赖的数据块尽量存在一个PIM阵列层上, 将没有数据依赖的计算块放置于不同PIM层以方便它们共享资源池中的资源.图5给出了在PIM中训练一个神经网络的数据存储和数据流图.正方形表示数据存储在Compute Array中, 直接用来做计算;圆角矩形表示数据存储在Storage Array中, 用来暂存数据供给Compute Array做计算.图5中的<i>W</i>表示Weight, <i>E</i>表示Error, <i>O</i>表示Output.训练时的前向传播和反向传播是串行的, 而反向传播包含2部分:误差传播和权值计算, 二者是并行的.因此, 当只有一个训练任务时 (训练一个网络) , 我们尽量把串行的部分切割排布到同一个FMC中, 使得它们能够时分复用共享资源池;把并行的部分排布到不同的FMC中, 避免它们对资源池的访问造成冲突.</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_120.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 训练一个神经网络的数据图" src="Detail/GetImg?filename=images/JFYZ201906003_120.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 训练一个神经网络的数据图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_120.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Data graph of training a neural network</p>

                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_116.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 FDM策略的一个示例" src="Detail/GetImg?filename=images/JFYZ201906003_116.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 FDM策略的一个示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_116.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 An example of FDM</p>

                </div>
                <div class="p1">
                    <p id="121">图6给出了FDM策略的一个例子:用2个资源共享池相连的FMC来训练一个神经网络时的数据排布情况.我们先将整个网络的训练按照时间序列把每层使用的权值矩阵排列, 如图6中橘黄色方块所示.然后我们将所有的权值矩阵两等分 (如图6中虚线框所示) , 依据的准则是尽可能地使需要并行的权值矩阵阵列分配到不同的FMC上, 同时保证相邻的神经网络的权值矩阵尽量被分配到FMC上相邻的位置.如果我们计划使用<i>m</i>个FMC来训练一个神经网络 (共需要<i>n</i>个Compute Array来存储它的权值) , <i>p</i>=-<i>n</i>/<i>m</i>-, 那么我们依次从{<i>W</i>′<sub>2</sub>, <i>W</i>′<sub>3</sub>, …, <i>W</i>′<sub><i>n</i></sub>}, {<i>W</i><sub><i>n</i></sub>, <i>W</i><sub><i>n</i>-1</sub>, …, <i>W</i><sub>1</sub>}, {<i>E</i>′<sub><i>n</i></sub>, <i>E</i>′<sub><i>n</i>-1</sub>, …, <i>E</i>′<sub>1</sub>}这3个序列中每次取<i>p</i>个权值矩阵存到相应的FMC中.</p>
                </div>
                <div class="p1">
                    <p id="122">如果一层的权值需要多个忆阻器阵列进行存储, 那么我们把它们均匀地分布到一个FMC中的所有忆阻器阵列层上 (图6中从第1层到第<i>n</i>层进行循环分配, 直到分配结束) , 因为它们之间不需要进行通信, 所以不必在同一个平面层中, 其余各层按照这个方式进行分配.这样做还为了使得所有神经网络层能有几乎一样的计算效率, 能够有几乎相等的访问共享资源池的延迟 (避免如将第1层神经网络权值分配到FMC的第1层忆阻器阵列、最后一层神经网络分配到FMC的最后一层忆阻器阵列, 从而导致因最后一层访问共享资源池的延迟大而造成性能差的问题) .当有多个训练任务时, 我们将多个训练任务均匀分配到所有的FMC中.</p>
                </div>
                <div class="p1">
                    <p id="123">FDM的整体流程如图7所示.首先将多个网络分配到多个FMC中, 每个网络可能在一个或多个FMC里;然后将每个网络的不同层部分分配到FMC中;最后把网络每个层的各个部分分配到FMC的各个PIM阵列层中.</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 FMC的流程总体流程" src="Detail/GetImg?filename=images/JFYZ201906003_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 FMC的流程总体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Flow chart of FMC</p>

                </div>
                <h3 id="125" name="125" class="anchor-tag"><b>4 实验与分析</b></h3>
                <div class="p1">
                    <p id="126">本节首先介绍实验环境、参数配置以及实验方法, 并简要介绍所用的对比实验的结构和参数配置;接着给出实验结果和相应的分析, 包括FMC的功能单元资源利用率、空间占用情况和FMC+FDM在训练各个神经网络时的性能和能耗比较.</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>4.1 实验方法</b></h4>
                <div class="p1">
                    <p id="128">表3给出了实验环境和具体的基于ReRAM的主存参数配置.整个系统将CPU作为中心任务处理器, 统计好训练任务的数量以及每个任务所占空间大小后, 按照FMD的策略将训练任务发布到整个PIM中.发布完任务后CPU不再参与整个训练工作, 所有的训练工作包括计算和数据存取都在基于ReRAM的主存中进行.本系统不考虑DRAM和ReRAM的混合主存, 所有用来支持训练的数据都存放在基于ReRAM的主存中.</p>
                </div>
                <div class="area_img" id="129">
                                            <p class="img_tit">
                                                <b>表3 硬件配置</b>
                                                    <br />
                                                <b>Table 3 Hardware Configurations</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JFYZ201906003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 硬件配置" src="Detail/GetImg?filename=images/JFYZ201906003_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="130">我们使用CACTI 6.5<citation id="359" type="reference"><link href="324" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>对所有FMC中的连接进行建模;使用CACTI-IO<citation id="360" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>来对FMC之间的连接进行建模.关于ReRAM的参数配置, 我们使用DESTINY<citation id="361" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>进行模拟获取.需要说明的是, 我们的模拟系统中使用的基于忆阻器的主存包含2个chip;每个chip包含16个tile, 通过H-tree方式连接;每个tile包含16个FMC.</p>
                </div>
                <div class="p1">
                    <p id="131">如2.4节所述, FMC的资源池里各个资源的数目在不同的系统限制下会有不同的取值;同时, 在同一个系统配置下, FMC的资源池的配置也可能会有多种可能.该实验部分测试了在表3的系统配置情况下的所有配置可能, 并选取了FMC的资源池的资源利用率最高的一种配置来做性能和能耗的相关实验.</p>
                </div>
                <div class="p1">
                    <p id="132">表4给出了FMC的单元配置参数, 其中忆阻器阵列给出的参数是一层的配置, 每个FMC有4层忆阻器阵列 (考虑到传输线路长度和共享资源池的资源利用率) .共享资源池中每个单元的占用面积直接使用ISAAC中的配置<citation id="362" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>;每个单元的数目根据2.4节中的限制条件给出.</p>
                </div>
                <div class="area_img" id="133">
                                            <p class="img_tit">
                                                <b>表4 FMC单元的实验配置参数</b>
                                                    <br />
                                                <b>Table 4 Parameters of Units in FMC</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JFYZ201906003_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 FMC单元的实验配置参数" src="Detail/GetImg?filename=images/JFYZ201906003_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="134">我们将配备如表4所示的FMC结构和2D的PIM进行实验比较, 用PipeLayer<citation id="363" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的结构.即一个PIM由多个PIM阵列组成, 它们之间采用H-tree的连接方式进行连接, 并且每个PIM阵列周围都配备有表4中的功能单元, 这些功能单元之间互相不能共享, 属于每个阵列私有.我们使用LeNet<citation id="364" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, ConvNet<citation id="365" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>和Caffe Model Zoo<citation id="366" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>中的6个流行的网络来作为实验的测试网络, 包含: AlexNet, NiN, GoogLeNet, VGG_M, VGG_S和VGG_19.它们中既有大网络也有小网络, 既有全连接层也有卷积层, 数据集也涵盖了用来做图像分类的黑白MNIST手写数字集<citation id="367" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>和著名的用来做图像分类、目标定位和检测、场景分类的彩色图片集ImageNet<citation id="368" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>.我们使用1, 8, 16, 32, 64和128六种<i>batch</i>_<i>size</i>来训练测试网络集, 每个单独的网络分别执行1 000次训练迭代 (用来测试单个任务时的架构性能) , 并将所有网络同时执行1 000次训练迭代 (用来测试多个任务时的架构性能) .</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135"><b>4.2 FMC的功能单元资源利用率</b></h4>
                <div class="p1">
                    <p id="136">我们首先将2D的PIM的外围电路资源利用率和FMC的共享资源池中的功能单元利用率在训练单个神经网络的情况下进行比较, 结果如图8所示.我们将训练8个测试网络时2D-PIM的外围电路资源利用率归一化成1, 给出在每个测试网络下FMC的资源利用率与2D-PIM的倍数比较.图8中FMC-<i>x</i>的<i>x</i>表示训练时所用的<i>batch</i>_<i>size</i>, 所有网络测试结果呈现按照名称首字母降序排序.</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_137.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 单个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较" src="Detail/GetImg?filename=images/JFYZ201906003_137.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 单个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_137.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Utilization of function units in FMC compared with 2D-PIM when training a single network</p>

                </div>
                <div class="p1">
                    <p id="138">我们可以从图8中看到, FMC的资源利用率要比2D-PIM的高 (所有的资源利用率的提升均在1倍以上) , 且在<i>batch</i>_<i>size</i>=1时有非常显著的利用率提升 (如图8中FMC-1的柱形所示) , 最高能达到43.33倍 (训练GoogLeNet时) .另外FMC的资源利用率随<i>batch</i>_<i>size</i>增大而减小.这是因为当<i>batch</i>_<i>size</i>增大时, 2D-PIM中整个传播过程可以很好地利用管道并行 (pipeline) 起来, 从而提高功能单元的资源利用率.因此FMC在<i>batch</i>_<i>size</i>较大时, 资源利用率的提升相比于2D-PIM不是很明显, 但仍然有一定提升.例如用<i>batch</i>_<i>size</i>=128来训练GoogLeNet时, 资源利用率有1.33倍的提升.这样的提升来源于pipeline时候的气泡部分以及前向传播和反向传播的中断部分.图8还显示出训练不同深度的网络时FMC资源利用率提升的差别:当训练深度大的网络 (GoogLeNet, VGG_19) 时, FMC的资源率提升相对大;当训练深度小的网络 (LeNet, ConvNet) 时, FMC的资源利用率提升较小.这是因为网络的深度越大, 处理后面的网络层需要等待的时间就越长, 进而导致处理后面网络层的2D-PIM的功能单元空闲时间过长, 最终使得2D-PIM的资源利用率低, 而FMC通过使前后层共享功能单元的方式提高了资源利用率, 因此FMC的资源利用率相比较于2D-PIM的提升会随着网络层数的增大而更高, 反之亦然.</p>
                </div>
                <div class="p1">
                    <p id="139">图9展示了多个神经网络同时训练时在FMC中的资源利用率和在2D-PIM中的资源利用率的比较结果.其中, L代表LeNet, C代表ConvNet, G代表GoogLeNet, V19代表VGG_19, ALL代表所有8个测试网络的组合.</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 多个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较" src="Detail/GetImg?filename=images/JFYZ201906003_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 多个网络训练时FMC中功能单元的资源利用率和2D-PIM的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Utilization of function units in FMC compared with 2D-PIM when training multiple networks</p>

                </div>
                <div class="p1">
                    <p id="141">从图9中我们可以看出, 当同时训练的网络越多且网络深度越大时, FMC相比较于2D-PIM的资源利用率提升更大.这是因为, 当同时训练多个网络时, 2D-PIM的资源利用率取决于最小网络训练时的资源利用率, 而FMC因为能够使得多个网络共享功能单元, 从而在训练网络任务多时, 由于各个网络之间不存在依赖关系可以并行, 因此它们可以通过pipeline的方式共享功能单元, 从而提升FMC的资源利用率, 进而使得FMC相比较于2D-PIM的资源利用率的提升更为明显.但是这样的提升速率并不会随着网络大小和网络深度的增长速率而增长, 这是因为, 当网络的大小和数目大到一定程度时, 共享资源池的资源利用率趋近于100% (不能到达100%, 因为训练起步时会有一定的气泡) .</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142"><b>4.3 FMC空间占用</b></h4>
                <div class="p1">
                    <p id="143">图10给出了FMC中的各个功能单元和一层忆阻器的面积占用的比较.其中, 一层忆阻器的面积占用为总共面积占用的49.97%, 也就意味着一层忆阻器的面积稍稍小于功能单元池的面积.在功能单元池中, DAC和ADC是占主要面积的分单元池, 分别为21.24%和18.74%.</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_144.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 FMC中各个单元的空间占用比率 (包含一层忆阻器阵列)" src="Detail/GetImg?filename=images/JFYZ201906003_144.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 FMC中各个单元的空间占用比率 (包含一层忆阻器阵列)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_144.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Area breakdown of FMC (one layer of memristor) </p>

                </div>
                <div class="p1">
                    <p id="145">FMC中新加的开关和连接线占总空间的6.79%, 而一个FMC所占空间仅是具有同等数目的Compute Array及Storage Array的2D-PIM所占空间的42.89%.</p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>4.4 FMC+FDM性能和能耗</b></h4>
                <div class="p1">
                    <p id="147">由于<i>batch</i>_<i>size</i>对FMC的性能和能耗与2D-PIM的比较几乎没有影响, 因此我们选取<i>batch</i>_<i>size</i>=64 (常用的<i>batch</i>_<i>size</i>) 作为FMC的性能和能耗测试结果呈现, 具体如图11和图12所示.</p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_148.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 FMC与2D-PIM的性能比较" src="Detail/GetImg?filename=images/JFYZ201906003_148.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 FMC与2D-PIM的性能比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_148.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Performance of FMC compared with 2D-PIM</p>

                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906003_149.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 FMC与2D-PIM的能耗比较" src="Detail/GetImg?filename=images/JFYZ201906003_149.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 FMC与2D-PIM的能耗比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906003_149.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Energy consumption of FMC compared with 2D-PIM</p>

                </div>
                <div class="p1">
                    <p id="150">从图11中我们可以看出, 相比于2D-PIM, 使用FMC训练越大的神经网络取得的性能加速要大.这是因为FMC各层忆阻器阵列之间是3D堆叠的, 并通过3D堆叠的方式共享资源池.这使得忆阻器阵列之间的连接充分缩短, 而网络越大需要的忆阻器阵列就越多, 因此FMC相比于2D-PIM的优势就更为明显.而用FMC训练LeNet时几乎没有性能上的提升, 这是因为LeNet很小, 从而使得FMC的3D堆叠的连接方式相比较于2D-PIM的优势小, 且数据传输的延迟被计算的延迟隐藏.总体来看, FMC相比于2D-PIM有1.5倍的性能提升.</p>
                </div>
                <div class="p1">
                    <p id="151">图12展示了用FMC和2D-PIM训练单个神经网络时能耗比较.FMC在训练神经网络时能比2D-PIM有明显的能耗节省, 也是因为FMC的3D堆叠的结构比2D-PIM的平面结构大大减少数据传输的线路长度.因此, 所训练的神经网络越大, FMC相比于2D-PIM的能耗节省就更加明显.当训练GoogLeNet时, 能耗节省能达到2.47倍.而当训练LeNet时, FMC也比2D-PIM有1.2倍的能耗节省.总的来看, FMC平均比2D-PIM节省1.7倍的能耗.</p>
                </div>
                <h3 id="152" name="152" class="anchor-tag"><b>5 总  结</b></h3>
                <div class="p1">
                    <p id="153">现如今, 基于忆阻器的内存计算 (PIM) 如火如荼, 但是它存在着除忆阻器阵列之外的电路单元面积过大且利用率低的问题.本文提出了一种基于3D忆阻器阵列的神经网络内存计算架构, 将功能单元抽取出来形成一个资源池提供给忆阻器阵列共享, 并通过3D堆叠的方式缩短各个忆阻器阵列的连接以及忆阻器阵列和功能单元池之间的连接.同时, 我们还提出了一种基于3D忆阻器阵列的计算数据排布策略, 配合上3D忆阻器阵列的结构, 使得训练神经网络时的数据移动尽可能小.实验结果显示, 我们提出的基于3D忆阻器阵列加共享资源池的架构能使功能单元的利用率在单个训练任务的情况下提升43.33倍, 在多个任务的情况下最高提升58.51倍.同时, 我们提出3D架构所占空间是有相同数目的Compute Array及Storage Array的2D-PIM所占空间的42.89%.此外, 我们提出3D架构相比于2D-PIM有平均1.5倍的性能提升, 且有平均1.7倍的能耗节约.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="371" type="formula" href="images/JFYZ201906003_37100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">毛海宇</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="372" type="formula" href="images/JFYZ201906003_37200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">舒继武</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="276">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PRIME:A Novel Processing-inmemory Architecture for Neural Network Computation in Re RAM-based Main Memory">

                                <b>[1]</b>Chi Ping, Li Shuangchen, Xu Cong, et al.Prime:A novel processing-in-memory architecture for neural network computation in reram-based main memory[C] //Proc of 2016 ACM/IEEE the 43rd Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2016:27- 39
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMEA2268EBF73745312D0ED0A3CA273E4C&amp;v=MjY1MDBZOGJKSE5QS3AvbzNFdXdNQzNnOHpCY1JuajhJUEgrVHIyRkVlN1dYTUw3c0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54Z3pMdS94S289TmlmSQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[J].ACM SIGARCH Computer Architecture News, 2016, 44 (3) :14- 26
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PipeL ayer:Apipelined reR AM-based accelerator for deep learning">

                                <b>[3]</b>Song Linghao, Qian Xuehai, Li Hai, et al.PipeLayer:A pipelined ReRAM-based accelerator for deep learning[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2017:541- 552
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Equivalent-accuracy accelerated neural-network training using analogue memory">

                                <b>[4]</b>Stefano A, Pritish N, Hsinyu T, et al.Equivalent-accuracy accelerated neural-network training using analogue memory[J].Nature, 2018, 558 (7708) :60- 67
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPUS AND THE FUTURE OF PARALLEL COMPUTING">

                                <b>[5]</b>Keckler S W, Dally W J, Khailany B, et al.Gpus and the future of parallel computing[J].IEEE Micro, 2011, 31 (5) :7- 17
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pinatubo:A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories">

                                <b>[6]</b>Li Shuangchen, Xu Cong, Zou Qiaosha, et al.Pinatubo:A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories[C] //Proc of Design Automation Conf.Piscataway, NJ:IEEE, 2016:173- 179
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lergan A zero-free low data movement and pim-based gan architecture">

                                <b>[7]</b>Mao Haiyu, Song Mingcong, Li Tao, et al.Lergan:A zero-free, low data movement and pim-based gan architecture[C] //Proc of the 51st Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2018:669- 681
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Radar A 3D-ReRAM based DNA alignment accelerator architecture">

                                <b>[8]</b>Wenqin Huangfu, Li Shuangchen, Hu Xing, et al.Radar:A 3D-ReRAM based DNA alignment accelerator architecture[C] //Proc of 2018 the 55th ACM/ESDA/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2018:1- 6
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Time A training-in-memory architecture for memristor-based deep neural networks">

                                <b>[9]</b>Cheng Ming, Xia Lixue, Zhu Zhenhua, et al.Time:A training-in-memory architecture for memristor-based deep neural networks[C] //Proc of 2017 the 54th IEEE Design Automation Conf (DAC) .New York:ACM, 2017:26- 31
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Memristive Boltzmann machine:A hardware accelerator for combinatorial optimization and deep learning">

                                <b>[10]</b>Bojnordi M N.Memristive Boltzmann machine:A hardware accelerator for combinatorial optimization and deep learning[C] //Proc of the 22nd IEEE Int Symp on High Performance Computer Architecture.Los Alamitos, CA:IEEE Computer Society, 2016:1- 13
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neurocube:A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory">

                                <b>[11]</b>Kim D, Kung J, Chai S, et al.Neurocube:A programmable digital neuromorphic architecture with high-density 3D memory[C] //Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:380- 392
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hrl:Efficient and flexible reconfigurable logic for near-data processing">

                                <b>[12]</b>Gao Mingyu, Kozyrakis C.Hrl:Efficient and flexible reconfigurable logic for near-data processing[C] //Proc of the 22nd IEEE Int Symp on High Performance Computer Architecture.Los Alamitos, CA:IEEE Computer Society, 2016:126- 137
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding energy aspects of processing-near-memory for HPC workloads">

                                <b>[13]</b>Kim H, Kim H, Yalamanchili S, et al.Understanding energy aspects of processing-near-memory for HPC workloads[C] //Proc of the 2015 Int Symp on Memory Systems.New York:ACM:276- 282
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Drama:An architecture for accelerated processing near memory">

                                <b>[14]</b>Farmahini- Farahani A, Ahn J H, Morrow K, et al.Drama:An architecture for accelerated processing near memory[J].IEEE Computer Architecture Letters, 2017, 14 (1) :26- 29
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCME1F7DEC8D36B382B0BB389570B223BEA&amp;v=MTkxNjlIYm5xeEpIZTdDWE44L3VDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGd6THUveEtvPU5pZklZOGE1YU5hNDJ2eE5FT2dKZm44eHpXUVRtRTErUQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>Ahn J, Yoo S, Mutlu O, et al.Pim-enabled instructions:A low-overhead, locality-aware processing-in-memory architecture[J].ACM SIGARCH Computer Architecture News, 2015, 43 (3) :336- 348
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dadiannao:Amachine-learning supercomputer">

                                <b>[16]</b>Chen Yunji, Luo Tao, Liu Shaoli, et al.Dadiannao:A machine-learning supercomputer[C] //Proc of the 47th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2014:609- 622
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A 130.7 mm2 2-layer 32Gb ReRAM memory device in 24nm technology">

                                <b>[17]</b>Liu T Y, Yan T H, Scheuerlein R, et al.A 130.7 mm2, 2-layer 32-GB ReRAM memory device in 24-nm technology[C] //Proc of IEEE Int Solid-State Circuits Conf (Digest of Technical Papers) .Piscataway, NJ:IEEE, 2013:140- 153
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Resistive Random Access Memory (ReRAM) Based on Metal Oxides">

                                <b>[18]</b>Akinaga H, Shima H.Resistive random access memory (ReRAM) based on metal oxides[J].Proceedings of the IEEE, 2010, 98 (12) :2237- 2251
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Highly reliable TaOx ReRAM and direct evidence of redox reaction mechanism">

                                <b>[19]</b>Wei Z, Kanzawa Y, Arita K, et al.Highly reliable taox ReRAM and direct evidence of redox reaction mechanism[C] //Proc of IEEE Int Electron Devices Meeting.Piscataway, NJ:IEEE, 2008:1- 4
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-Time Observation on Dynamic Growth/Dissolution of Conductive Filaments in Oxide-Electrolyte-Based ReRAM">

                                <b>[20]</b>Liu Qi, Sun Jun, Lü Hangbing, et al.Real-time observation on dynamic growth/dissolution of conductive filaments in oxide-electrolyte-based ReRAM[J].Advanced Materials, 2012, 24 (14) :1844- 1849
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Experimental demonstration and tolerancing of a large-scale neural network (165000synapses) using phase-change memory as the synaptic weight element">

                                <b>[21]</b>Burr G W, Shelby R M, Sidler S, et al.Experimental demonstration and tolerancing of a large-scale neural network (165000synapses) using phase-change memory as the synaptic weight element[J].IEEE Transactions on Electron Devices, 62 (11) :3498- 3507
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-volatile organic electrochemical device as a low-voltage artificial synapse for neuromorphic computing">

                                <b>[22]</b>Burgt Y, Lubberman E, Fuller E, et al.A non-volatile organic electrochemical device as a low-voltage artificial synapse for neuromorphic computing[J].Nature Materials, 2017, 16 (4) :414- 418
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Achieving ideal accuracies in analog neuromorphic computing using periodic carry">

                                <b>[23]</b>Agarwal S, Gedrim R B J, Hsia A H, et al.Achieving ideal accuracies in analog neuromorphic computing using periodic carry[C] //Proc of Symp on VLSI Technology.Piscataway, NJ:IEEE, 2017:174- 175
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory">

                                <b>[24]</b>Narayanan P, Fumarola A, Sanches L L, et al.Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory[J].IBM Journal of Research and Development, 2017, 61 (4) :1- 11
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing NUCA Organizations and Wiring Alternatives for Large Caches With CACTI 6.0">

                                <b>[25]</b>Muralimanohar N, Balasubramonian R, Jouppi N.Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0[C] //Proc of the 38th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Los Alamitos, CA:IEEE Computer Society, 2007:3- 14
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CACTI-IO CACTI with off-chip power-area-timing models">

                                <b>[26]</b>Jouppi N P, Kahng A B, Muralimanohar N, et al.CACTI-IO:CACTI with off-chip power-area-timing models[J].IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2015, 23 (7) :1254- 1267
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DESTINY A tool for modeling emerging 3D NVM and eDRAM caches">

                                <b>[27]</b>Poremba M, Mittal S, Li D, et al.DESTINY:A tool for modeling emerging 3D NVM and eDRAM caches[C] //Proc of Design Automation and Test in Europe (DATE) .Piscataway, NJ:IEEE, 2015:1543- 1546
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LeNet[OL]">

                                <b>[28]</b>Lecun Y.LeNet[OL].[2019-01-04].http://yann.lecun.com/exdb/lenet/
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ConvNet[OL]">

                                <b>[29]</b>Liu Liu.ConvNet[OL].[2019-01-04].http://libccv.org/doc/doc-convnet/
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe Model Zoo[OL]">

                                <b>[30]</b>Subramanian A S.Caffe Model Zoo[OL].[2019-01-04].https://github.com/BVLC/caffe/wiki/Model-Zoo
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MNIST[OL]">

                                <b>[31]</b>Lecun Y.MNIST[OL].[2019-01-04].http://yann.lecun.com/exdb/mnist/
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet[OL]">

                                <b>[32]</b>Stanford University, Princeton University.ImageNet[OL].[2019-01-04].http://www.image-net.org/
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201906003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906003&amp;v=MDAwNDFyQ1VSTE9lWmVScUZpRG1VN3ZLTHl2U2RMRzRIOWpNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZIdm80WDU2YzVxeVN5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

