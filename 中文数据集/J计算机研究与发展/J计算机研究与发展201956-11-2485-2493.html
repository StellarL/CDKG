

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127120142957500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJFYZ201911017%26RESULT%3d1%26SIGN%3dqgdZorgD0D%252b3sFdDAkSsTywwH8c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201911017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201911017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201911017&amp;v=MzIwNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSc0Z5em5VYjNOTHl2U2RMRzRIOWpOcm85RVk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#87" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="&lt;b&gt;2 本文算法&lt;/b&gt; "><b>2 本文算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;2.1 生成器&lt;/b&gt;"><b>2.1 生成器</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;2.2 全连接卷积&lt;/b&gt;"><b>2.2 全连接卷积</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;2.3 局部属性对抗网络&lt;/b&gt;"><b>2.3 局部属性对抗网络</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;2.4 损失函数&lt;/b&gt;"><b>2.4 损失函数</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;2.5 缺失区域&lt;/b&gt;"><b>2.5 缺失区域</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#120" data-title="&lt;b&gt;3 实  验&lt;/b&gt; "><b>3 实  验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#121" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;3.2 训练方法&lt;/b&gt;"><b>3.2 训练方法</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;3.3 人脸修复&lt;/b&gt;"><b>3.3 人脸修复</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;3.4 局限性&lt;/b&gt;"><b>3.4 局限性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="4&lt;b&gt; 总  结&lt;/b&gt; ">4<b> 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="图1 模型结构图">图1 模型结构图</a></li>
                                                <li><a href="#103" data-title="图2 效果改善展示图">图2 效果改善展示图</a></li>
                                                <li><a href="#119" data-title="图3 整体区域缺失">图3 整体区域缺失</a></li>
                                                <li><a href="#130" data-title="图4 修复结果展示图">图4 修复结果展示图</a></li>
                                                <li><a href="#134" data-title="图5 测试用例">图5 测试用例</a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表1 基于PSNR评价指标的对比结果&lt;/b&gt;"><b>表1 基于PSNR评价指标的对比结果</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表2 基于SSIM评价指标的对比结果&lt;/b&gt;"><b>表2 基于SSIM评价指标的对比结果</b></a></li>
                                                <li><a href="#140" data-title="图6 缺陷的改善展示">图6 缺陷的改善展示</a></li>
                                                <li><a href="#144" data-title="图7 模型局限性">图7 模型局限性</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="187">


                                    <a id="bibliography_1" title="Bertalmio M,Sapiro G,Caselles V,et al.Image inpainting[C] //Proc of the 27th Annual Conf on Computer Graphics and Interactive Techniques.New York:ACM,2000:417- 424" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image inpainting">
                                        <b>[1]</b>
                                        Bertalmio M,Sapiro G,Caselles V,et al.Image inpainting[C] //Proc of the 27th Annual Conf on Computer Graphics and Interactive Techniques.New York:ACM,2000:417- 424
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_2" title="Komodakis N.Image completion using global optimization[C] //Proc of 2006 IEEE Computer Society Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2006:442- 452" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Completion Using Global Optimization">
                                        <b>[2]</b>
                                        Komodakis N.Image completion using global optimization[C] //Proc of 2006 IEEE Computer Society Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2006:442- 452
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_3" title="Wexler Y,Shechtman E,Irani M.Spacetime video completion[C] //Proc of the 2004 IEEE Computer Society Conf on Computer Vision and Pattern Recognition.Los Alamitos,CA:IEEE Computer Society,2004:100- 111" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Space-time videocompletion">
                                        <b>[3]</b>
                                        Wexler Y,Shechtman E,Irani M.Spacetime video completion[C] //Proc of the 2004 IEEE Computer Society Conf on Computer Vision and Pattern Recognition.Los Alamitos,CA:IEEE Computer Society,2004:100- 111
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_4" title="Barnes C,Shechtman E,Finkelstein A,et al.Patch match:A randomized correspondence algorithm for structural image editing[J].ACM Transactions on Graphics,2009,28(3):21- 28" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PatchMatch: A randomized correspondence algorithm for structural image editing">
                                        <b>[4]</b>
                                        Barnes C,Shechtman E,Finkelstein A,et al.Patch match:A randomized correspondence algorithm for structural image editing[J].ACM Transactions on Graphics,2009,28(3):21- 28
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_5" title="Bertalmio M,Vese L,Sapiro G,et al.Simultaneous structure and texture image inpainting[J].IEEE Transactions on Image Processing,2003,12(8):882- 889" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous structure and texture image inpainting">
                                        <b>[5]</b>
                                        Bertalmio M,Vese L,Sapiro G,et al.Simultaneous structure and texture image inpainting[J].IEEE Transactions on Image Processing,2003,12(8):882- 889
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_6" title="Wilczkowiak M,Brostow G J,Tordoﬀ B,et al.Hole ﬁlling through photomontage[C] //Proc of the 2005 Conf of the British Machine Vision.Piscataway,NJ:IEEE,2015:30- 41" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hole ?lling through photomontage">
                                        <b>[6]</b>
                                        Wilczkowiak M,Brostow G J,Tordoﬀ B,et al.Hole ﬁlling through photomontage[C] //Proc of the 2005 Conf of the British Machine Vision.Piscataway,NJ:IEEE,2015:30- 41
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_7" title="Efros A A,Leung T K.Texture synthesis by nonparametric sampling[C] //Proc of the 7th IEEE Int Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,1999:1033- 1038" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Texture synthesis by non-parametric sampling">
                                        <b>[7]</b>
                                        Efros A A,Leung T K.Texture synthesis by nonparametric sampling[C] //Proc of the 7th IEEE Int Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,1999:1033- 1038
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_8" title="Efros A A,Freeman W T.Image quilting for texture synthesis and transfer[C] //Proc of the 28th Conf on Computer Graphics and Interactive Techniques.New York:ACM,2001:341- 346" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Quilting for Texture Synthesis and Transfer">
                                        <b>[8]</b>
                                        Efros A A,Freeman W T.Image quilting for texture synthesis and transfer[C] //Proc of the 28th Conf on Computer Graphics and Interactive Techniques.New York:ACM,2001:341- 346
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_9" title="Hays J,Efros A A.Scene completion using millions of photographs[J].ACM Transactions on Graphics,2007,26(4):36- 40" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098456&amp;v=MDM0MTFOcjQ5RlpPSUhDSGsvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZ3V2FCYz1OaWZJWTdLN0h0ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Hays J,Efros A A.Scene completion using millions of photographs[J].ACM Transactions on Graphics,2007,26(4):36- 40
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_10" title="Whyte O,Sivic J,Zisserman A.Get out of my picture![C] //Proc of 2009 British Machine Vision.Piscataway,NJ:IEEE,2009:126- 138" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Get out of my picture!">
                                        <b>[10]</b>
                                        Whyte O,Sivic J,Zisserman A.Get out of my picture![C] //Proc of 2009 British Machine Vision.Piscataway,NJ:IEEE,2009:126- 138
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_11" title="Kwatra V,Essa I,Bobick A,et al.Texture optimization for example based synthesis[J].ACM Transactions on Graphics,2005,24:795- 802" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098255&amp;v=MzA0OTBNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZ3V2FCYz1OaWZJWTdLN0h0ak5yNDlGWk9JSERuazhvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Kwatra V,Essa I,Bobick A,et al.Texture optimization for example based synthesis[J].ACM Transactions on Graphics,2005,24:795- 802
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_12" title="Iizuka S,Simo-Serra E,Ishikawa H.Globally and locally consistent image completion[J].ACM Transactions on Graphics,2017,36(4):107- 114" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM408B7C15F867A55233C65423FF57BC91&amp;v=MjI2Mjc5d3EwPU5pZklZN2U0RnFQTDNJNUFFdU1KQ3cwOHloUVE2VXg3VFh2Z3IyUkRmTFhtTnJPZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5waHdMcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Iizuka S,Simo-Serra E,Ishikawa H.Globally and locally consistent image completion[J].ACM Transactions on Graphics,2017,36(4):107- 114
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_13" title="Li Yijun,Liu Sifei,Yang Jimei,et al.Generative face completion[C] //Proc of the 2017 IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:5892- 5900" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Face Completion">
                                        <b>[13]</b>
                                        Li Yijun,Liu Sifei,Yang Jimei,et al.Generative face completion[C] //Proc of the 2017 IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:5892- 5900
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_14" title="Pathak D,Krahenbuhl P,Donahue J,et al.Context encoders:Feature learning by inpainting[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2016).Piscataway,NJ:IEEE,2016:2536- 2544" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context encoders:Feature learning by inpainting">
                                        <b>[14]</b>
                                        Pathak D,Krahenbuhl P,Donahue J,et al.Context encoders:Feature learning by inpainting[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2016).Piscataway,NJ:IEEE,2016:2536- 2544
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_15" title="Yeh R,Chen Chen,Lim T Y,et al.Semantic image inpainting with perceptual and contextual losses[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:5009- 5018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic image inpainting with perceptual and contextual losses">
                                        <b>[15]</b>
                                        Yeh R,Chen Chen,Lim T Y,et al.Semantic image inpainting with perceptual and contextual losses[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:5009- 5018
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_16" title="Yang Chao,Lu Xin,Lin Zhe,et al.High-resolution image inpainting using multi-scale neural patch synthesis[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:4076- 4084" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution image inpainting using multi-scale neural patch synthesis">
                                        <b>[16]</b>
                                        Yang Chao,Lu Xin,Lin Zhe,et al.High-resolution image inpainting using multi-scale neural patch synthesis[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:4076- 4084
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_17" title="Ronneberger O,Fischer P,Brox P.U-net:Convolutional networks for biomedical image segmentation[C] //Proc of the 18th Int Conf on Medical Image Computing and Computer Assisted Intervention (MICCAI 2015).Piscataway,NJ:IEEE,2015:3157- 3165" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">
                                        <b>[17]</b>
                                        Ronneberger O,Fischer P,Brox P.U-net:Convolutional networks for biomedical image segmentation[C] //Proc of the 18th Int Conf on Medical Image Computing and Computer Assisted Intervention (MICCAI 2015).Piscataway,NJ:IEEE,2015:3157- 3165
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_18" title="Liu Ziwei,Luo Ping,Wang Xiaogang,et al.Deep learning face attributes in the wild[C] //Proc of 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:3730- 3738" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning face attributes in the wild">
                                        <b>[18]</b>
                                        Liu Ziwei,Luo Ping,Wang Xiaogang,et al.Deep learning face attributes in the wild[C] //Proc of 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:3730- 3738
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_19" title="Coloma B,Marcelo B,Vicent C et al.Filling-in by joint interpolation of vector ﬁelds and black levels[J].IEEE Transactions on Image Processing,2001,10(8):1200- 1211" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Filling-in by joint interpolation of vector fields and gray levels">
                                        <b>[19]</b>
                                        Coloma B,Marcelo B,Vicent C et al.Filling-in by joint interpolation of vector ﬁelds and black levels[J].IEEE Transactions on Image Processing,2001,10(8):1200- 1211
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_20" title="Telea A.An image inpainting technique based on the fast marching method[J].Journal of Graphics Tools,2004,9(1):23- 34" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD0DE016AB29E895183FB16BF6ACA81005&amp;v=MjEzNDB3cTA9TmpuQmFyUE1hOUhOcWY0M1p1SjZCSFU4emg0UW5FMThUZzJVcW1OR0NMcVZSYnFhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnBod0xxOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        Telea A.An image inpainting technique based on the fast marching method[J].Journal of Graphics Tools,2004,9(1):23- 34
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_21" title="Goodfellow I,Pouget-Abadie J,Mirza M,et al.Generative adversarial nets[G] //Advances in Neural Information Processing Systems.New York:ACM,2014:2672- 2680" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[21]</b>
                                        Goodfellow I,Pouget-Abadie J,Mirza M,et al.Generative adversarial nets[G] //Advances in Neural Information Processing Systems.New York:ACM,2014:2672- 2680
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_22" title="Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].arXiv preprint arXiv:1511.06434,2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">
                                        <b>[22]</b>
                                        Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].arXiv preprint arXiv:1511.06434,2015
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_23" title="Arjovsky M,Chintala S,Bottou L.Wasserstein GAN[J].arXiv preprint arXiv:1701.07875,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">
                                        <b>[23]</b>
                                        Arjovsky M,Chintala S,Bottou L.Wasserstein GAN[J].arXiv preprint arXiv:1701.07875,2017
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_24" title="Gulrajani I,Ahmed F,Arjovsky M,et al.Improved training of wasserstein GANs[J].arXiv preprint arXiv:1704.00028,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved training of wasserstein GANs">
                                        <b>[24]</b>
                                        Gulrajani I,Ahmed F,Arjovsky M,et al.Improved training of wasserstein GANs[J].arXiv preprint arXiv:1704.00028,2017
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_25" title="Mao Xudong,Li Qing,Xie Haoran,et al.Least squares generative adversarial networks[J].arXiv preprint ArXiv:1611.04076,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Least squares generative adversarial networks">
                                        <b>[25]</b>
                                        Mao Xudong,Li Qing,Xie Haoran,et al.Least squares generative adversarial networks[J].arXiv preprint ArXiv:1611.04076,2016
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_26" title="Chen Xi,Duan Yan,Houthooft R,et al.InfoGAN:Interpretable representation learning by information maximizing generative adversarial nets[C] //Proc of the 30th Conf on Neural Information Processing Systems (NIPS 2016).New York:ACM,2016:5600- 5611" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=InfoGAN:interpretable representation learning by information maximizing generative adversarial nets">
                                        <b>[26]</b>
                                        Chen Xi,Duan Yan,Houthooft R,et al.InfoGAN:Interpretable representation learning by information maximizing generative adversarial nets[C] //Proc of the 30th Conf on Neural Information Processing Systems (NIPS 2016).New York:ACM,2016:5600- 5611
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_27" title="Tero K,Timo A,Samuli L,et al.Progressive growing of GANs for improved quality,stability,and variation[J].arXiv preprint arXiv:1710.10196,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Progressive growing of GANs for improved quality,stability,and variation">
                                        <b>[27]</b>
                                        Tero K,Timo A,Samuli L,et al.Progressive growing of GANs for improved quality,stability,and variation[J].arXiv preprint arXiv:1710.10196,2017
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_28" title="Raymond A Y,Chen Chen,Teck Y L,et al.Semantic image inpainting with deep generative models[C] //Proc of the 2017 IEEE Conf on Computer Vision and Pattern Recognition (CVPR2017).Piscataway,NJ:IEEE,2017:5485- 5493" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic image inpainting with deep generative models">
                                        <b>[28]</b>
                                        Raymond A Y,Chen Chen,Teck Y L,et al.Semantic image inpainting with deep generative models[C] //Proc of the 2017 IEEE Conf on Computer Vision and Pattern Recognition (CVPR2017).Piscataway,NJ:IEEE,2017:5485- 5493
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_29" title="Yu Jiahui,Lin Zhe,Yang Jimei,et al.Generative image inpainting with contextual attention[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2018).Piscataway,NJ:IEEE,2018:4076- 4084" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative image inpainting with contextual attention">
                                        <b>[29]</b>
                                        Yu Jiahui,Lin Zhe,Yang Jimei,et al.Generative image inpainting with contextual attention[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2018).Piscataway,NJ:IEEE,2018:4076- 4084
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_30" title="Ulyanov D,Vedaldi A,Lempitsky V.Deep image prior[J].arXiv preprint arXiv:1711.10925,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep image prior">
                                        <b>[30]</b>
                                        Ulyanov D,Vedaldi A,Lempitsky V.Deep image prior[J].arXiv preprint arXiv:1711.10925,2017
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_31" title="Song Yuhang,Yang Chao,Lin Zhe,et al.Image inpainting using multi-scale feature image translation[J].arXiv preprint arXiv:1711.08590,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image inpainting using multi-scale feature image translation">
                                        <b>[31]</b>
                                        Song Yuhang,Yang Chao,Lin Zhe,et al.Image inpainting using multi-scale feature image translation[J].arXiv preprint arXiv:1711.08590,2017
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_32" title="Liu Guilin,Fitsum A,Kevin J,et al.Image inpainting for irregular holes using partial convolutions[J].arXiv preprint arXiv:1804.07723,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image inpainting for irregular holes using partial convolutions">
                                        <b>[32]</b>
                                        Liu Guilin,Fitsum A,Kevin J,et al.Image inpainting for irregular holes using partial convolutions[J].arXiv preprint arXiv:1804.07723,2018
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_33" title="Isola P,Zhu Junyan,Zhou Tinghui,et al.Image-to-image translation with conditional adversarial networks[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:6822- 6834" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-to-image translation with conditional adversarial networks">
                                        <b>[33]</b>
                                        Isola P,Zhu Junyan,Zhou Tinghui,et al.Image-to-image translation with conditional adversarial networks[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:6822- 6834
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_34" title="Wang Zhou,Bovik A C,Sheikh H R,et al.Image quality assessment:From error vsibility to structural similarity[J].IEEE Transactions on Image Processing,2004,13(4):32- 40" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">
                                        <b>[34]</b>
                                        Wang Zhou,Bovik A C,Sheikh H R,et al.Image quality assessment:From error vsibility to structural similarity[J].IEEE Transactions on Image Processing,2004,13(4):32- 40
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(11),2485-2493 DOI:10.7544/issn1000-1239.2019.20180656            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于局部属性生成对抗网络的人脸修复算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E6%96%8C&amp;code=07418215&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E8%99%B9%E9%9B%A8&amp;code=41999554&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘虹雨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E8%B6%85&amp;code=07427897&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B6%82%E6%96%87%E8%BD%A9&amp;code=41999553&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">涂文轩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%AD%90%E9%BE%99&amp;code=27121019&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵子龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8D%97%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0060047&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖南大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>最近对神经网络模型的研究在图像修复任务中显示出巨大的潜力,其核心任务是理解图像语义信息并重建缺失的图像内容.这些研究可以生成语义和内容上合理的结构和纹理,但通常会导致与孔洞周围区域不一致的扭曲结构或模糊纹理,特别是人脸图像修复问题.人脸图像修复工作经常需要为包含大量外观元素以及局部属性的缺失区域(例如眼睛或嘴巴)生成语义上的新内容,这些缺失区域往往具有独特的属性和语义信息从而导致生成内容不合理.为了解决以上问题,提出了一个有效的深度神经网络模型,模型的生成器结合全连接卷积和U-net网络的优越特性,同时提出局部属性辨别器使修复内容具有创新性的同时也能够使整体与局部保持语义一致性.模型不仅提升了对于人脸图像整体语义信息的感知能力,同时也基于局部属性能够有效地修复人脸关键部位,通过在CelebA数据集上的实验证明了该模型能够有效地修复人脸缺失部分并且能够生成新颖的修复内容.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E4%BF%AE%E5%A4%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸修复;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%80%E9%83%A8%E5%B1%9E%E6%80%A7%E8%BE%A8%E5%88%AB%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">局部属性辨别器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E9%93%BE%E6%8E%A5%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全链接卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=U-net&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">U-net;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    蒋斌,jiangbin@hnu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61702176);</span>
                                <span>湖南省自然科学基金项目(2017JJ3038);</span>
                    </p>
            </div>
                    <h1><b>A Face Inpainting Algorithm with Local Attribute Generative Adversarial Networks</b></h1>
                    <h2>
                    <span>Jiang Bin</span>
                    <span>Liu Hongyu</span>
                    <span>Yang Chao</span>
                    <span>Tu Wenxuan</span>
                    <span>Zhao Zilong</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Electronic Engineering, Hunan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Recent researches in neural network models have shown great potential in image inpainting task, which focuses on understanding image semantic information and reconstructs missing image content. These researches can generate visually reasonable image structures and textures, however, they usually produce distorted structures or blurry textures that are inconsistent with the surrounding areas, especially for the face inpainting task. The face inpainting task is often necessary to gene the advantages of fully connected convolution and U-net network, and the model proposes locally attributes discriminator to make the inpainted contents more innovative and is able to keep the global and local semantic consistency. The model not only improves the perception of the overall semantic information of the face image, but also restores the key parts of the face based on the local attributes. Experiments on the CelebA dataset have shown that our model can effectively deal with face image repair problems and generate novel results.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20inpainting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face inpainting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=local%20attributes%20discriminator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">local attributes discriminator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fully%20connected%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fully connected convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=U-net&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">U-net;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Jiang Bin,born in 1971.PhD.Associate professor.His main research interests include big data technology,computer vision and machine learning.<image id="299" type="formula" href="images/JFYZ201911017_29900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Hongyu,born in 1994.Master candidate.His main research interests include computer vision and machine learning.<image id="300" type="formula" href="images/JFYZ201911017_30000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Yang Chao,born in 1978.PhD.Associate professor.Her main research interests include big data technology,social network computing and intelligent information processing.<image id="301" type="formula" href="images/JFYZ201911017_30100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Tu Wenxuan,born in 1995.Master candidate.His main research interests include computer vision and machine learning.<image id="302" type="formula" href="images/JFYZ201911017_30200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhao Zilong,born in 1995.Master candidate.His main research interests include computer vision.and machine learning.<image id="303" type="formula" href="images/JFYZ201911017_30300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-18</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61702176);</span>
                                <span>the Natural Science Foundation of Hunan Province of China(2017JJ3038);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="76">图像修复又被称为图像还原,主要任务是为图像缺失的区域填充合理的像素.该技术被应用于多种场景,如用于去除图像中的多余内容,或者利用一个图像中的内容和语义来填充另一个图像中的空缺区域.图像修复的核心挑战是为与上下内容和边界相关的缺失区域创建语义上合理的内容以及逼真的纹理细节.</p>
                </div>
                <div class="p1">
                    <p id="77">传统的图像修复工程<citation id="258" type="reference"><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><link href="201" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>尝试使用纹理合成技术<citation id="255" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>解决问题,利用已知区域中与未知区域最相关的部分修补孔洞或从孔洞边界开始传播并且匹配合理像素值.在文献<citation id="256" type="reference">[<a class="sup">4</a>]</citation>中,Barnes等人提出了Patch-Match算法,通过迭代搜索最相似的补丁来重建缺失的区域;文献<citation id="257" type="reference">[<a class="sup">2</a>]</citation>进一步精确了搜索区域并更快速地匹配最佳补丁.但是这些方法没有视觉语义的概念,只是利用已知背景的低级信号来模拟未知像素.另外一些早期工作<citation id="259" type="reference"><link href="203" rel="bibliography" /><link href="205" rel="bibliography" /><link href="207" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>的重点是利用与需修补图像相似图像中的内容填补缺失区域.从相关数据集中提取相似图像,并且转移到原始的空白部分,但是这些方法通常会修补出不一致的区域并且相似图像常常难以匹配.</p>
                </div>
                <div class="p1">
                    <p id="78">最近,随着神经网络的发展,修复任务被建模为条件图像生成问题,其中高级特征和语义内容被公式化为编码器-解码器网络并且总是与生成性对抗网络(generative adversarial network, GAN)相结合以激励修复区域与原始像素之间的一致性<citation id="264" type="reference"><link href="207" rel="bibliography" /><link href="209" rel="bibliography" /><link href="211" rel="bibliography" /><link href="213" rel="bibliography" /><link href="215" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>.上下文编码器<citation id="260" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>(context encoder, CE)是最早利用深度神经网络进行图像修复的方法之一,它对编码器-解码器生成网络进行训练,并与对抗性网络相结合,可以大大改善完成效果,但结果充满了低频内容以及包含明显的修复痕迹.此外,为了处理高分辨率的修复问题,Yang等人<citation id="261" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>旨在通过提取固定区域中的高级特征来优化修复后图像结果,然后从非空洞区域传播纹理信息以填充孔区域作为后处理,然而该方法经常导致与图像边界发散并且容易被人眼检测出修复痕迹.Iizuka等人<citation id="262" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>使用扩张卷积和2个尺度辨别器改善结果,结果比以往的方法更好,但需要后处理<citation id="263" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>并且不能重建大孔洞.</p>
                </div>
                <div class="p1">
                    <p id="79">对于人脸修复工作,图像中缺失的部分不能通过图像已知区域进行推断,从而不能通过先前的方法采用复制粘贴策略或者提取语义信息进行修复.因为许多缺失部分包含独特的属性,不能与人脸中其他属性(例如眼睛或嘴巴)进行关联.同时人脸修复往往需要网络针对单属性缺失(如眼睛或嘴巴)情况进行有效的修复,然而先前方法往往只能修复图像固定区域,对于修复图像局部信息则泛化性能不够优秀.</p>
                </div>
                <div class="p1">
                    <p id="80">文献<citation id="265" type="reference">[<a class="sup">13</a>]</citation>中Li等人提出的基于生成对抗网络的人脸修复(face generation, FG)方法对人脸修复问题做出了有效的修复效果,通过局部和总体的辨别器的运用,同时将人脸语义分割的图像作为辅助信息提供五官的相对位置,从而对人脸的不同属性进行有效的修复,然而对于不对称的人脸以及图像像素的连续性修复并不优秀.这主要是因为其生成器没有很好地建立深层特征之间的关系,并且其生成器采用编码器与解码器结构在通过瓶颈层时会有大量的隐含特征信息损失.</p>
                </div>
                <div class="p1">
                    <p id="81">本文提出了一种新颖并且有效的深度学习修复算法,该算法采用生成对抗训练方式,首先将整体属性缺失源图像放入架构为编码器和解码器(基于U-net<citation id="266" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>网络)的生成网络中得到修复结果,使生成网络学习到人脸的整体属性语义信息,再通过我们提出的局部属性辨别器加强生成网络对于局部属性的语义理解,从而使修复后的图像在整体和局部上达到语义以及内容上的一致性.</p>
                </div>
                <div class="p1">
                    <p id="82">相比于方法FG,本文的模型能够有效地修复不对称人脸(非正脸),获得图像局部特征以及其对应的特征相关性,同时能够生成新颖的图像,并且在色彩重构方面达到了更好的效果,这是因为我们的生成器基于U-net网络进行改进,而U-net网络拥有很好的色彩感知性.同时,我们提出的局部属性辨别器能够使模型更好地学习到局部特征.在CelebA<citation id="267" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>数据集上训练以及验证本文提出的模型,从修复的结果展示和利用标准的评测指标2方面验证了模型的有效性.</p>
                </div>
                <div class="p1">
                    <p id="83">本文的主要贡献有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="84">1) 提出了一个新颖的深度学习框架,从生成器到对抗器都适用于人脸修复工作;</p>
                </div>
                <div class="p1">
                    <p id="85">2) 对于人脸修复工作中的像素连接问题以及非对称图像修复问题作了有效的改善;</p>
                </div>
                <div class="p1">
                    <p id="86">3) 相比较于以前的方法局限于修复中心缺失区域,本文的方法能够有效地学习到人脸局部信息,更加符合实际场景.</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="88">近年来图像修复技术的发展迅速,从深度学习到非机器学习方法都对其有研究.我们接下来将介绍最具有代表性的工作.</p>
                </div>
                <div class="p1">
                    <p id="89">图像修复首先在文献<citation id="268" type="reference">[<a class="sup">1</a>]</citation>中引入,其利用扩散函数将来自周围已知区域的低级特征传播到未知区域.与文献<citation id="269" type="reference">[<a class="sup">1</a>]</citation>类似的其他方法<citation id="270" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>使用一些距离场将外观信息从像素传播到缺失区域.然而,这些方法仅处理小孔洞以及颜色和纹理重建的低级细节.后来的工作使用基于补丁的方法<citation id="272" type="reference"><link href="201" rel="bibliography" /><link href="225" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">20</a>]</sup></citation>,以迭代方式匹配非孔洞区域中最相关的内容.但它总是需要大量的时间消耗,为了应对挑战,Patch-Match<citation id="271" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一个快速的最近邻匹配算法,并且达到了优秀的效果,然而基于神经网络的方法无法理解图像的深层语义信息并且不能够修复较大的缺失区域.</p>
                </div>
                <div class="p1">
                    <p id="90">最近,基于深度学习的方法已经成为图像修复中一个很有研究价值的技术领域,尤其是基于生成对抗网络(GAN)<citation id="273" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.然而,这些方法很难保证稳定的训练并获得高分辨率的结果.随着GAN的高速发展,许多方法如DC-GAN<citation id="274" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>,Wasserstein GAN<citation id="275" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,WGAN-GP<citation id="276" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>,LS-GAN<citation id="277" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>,infoGAN<citation id="278" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>,Progressive GAN<citation id="279" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>都能够解决训练不稳定的问题,所以基于深度学习的图像修复方法能够持续发展.上下文编码器<citation id="280" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>在图像修复中利用GAN思想训练深度神经网络,编码大小为128×128不完整图像到低维,然后解码并生成缺失区域的内容,最终使用重建损失和对抗性损失作为约束训练网络参数.在文献<citation id="289" type="reference">[<a class="sup">15</a>,<a class="sup">28</a>]</citation>中,Yeh等人搜索受损图像最相关的特征空间作为网络的辅助信息来修复缺失内容.Yang等人<citation id="281" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>将上下文编码器<citation id="282" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的结果作为输入,逐渐增加纹理细节,以实现高分辨率的修复.最近,Iizuka等人<citation id="283" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了全局和局部鉴别器并在编码器-解码器中使用空洞卷积提升修复效果,这2种技术都能够增加输出神经元的感知域从而更加有效地利用已知区域的语义信息.与文献<citation id="284" type="reference">[<a class="sup">15</a>]</citation>一样,Yu等人<citation id="285" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>用语义关注层取代了后处理.在文献<citation id="286" type="reference">[<a class="sup">30</a>]</citation>中,Ulyanov等人认为生成网络不需要额外的训练,可以依靠自己修补不完整的图像.在文献<citation id="287" type="reference">[<a class="sup">31</a>]</citation>中,Song等人使用网络迭代替换来自特征空间中最接近的完整图像的补丁.在文献<citation id="288" type="reference">[<a class="sup">32</a>]</citation>中,Liu等人提出一种新颖的部分卷积层,同时利用自动掩模以减少伪影.但是这些工作并不能为人脸图像修复工作提供高质量的关联属性以及局部特征,同时其只能针对中心区域或随机区域进行修复,无法拓展到具体的局部属性修复工作</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag"><b>2 本文算法</b></h3>
                <div class="p1">
                    <p id="92">本文提出一种基于局部和整体属性一致性的人脸图像修复算法,将整体属性遮盖的图像作为输入(<i>I</i><sub>in</sub>),同时联合局部属性辨别器促使生成器能够更好地学习到人脸的关键属性特征.本文所提出的模型架构如图1所示.其中<i>I</i><sub>gt</sub>为原始图像,<i>I</i><sub>re</sub>为修复结果.</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 模型结构图" src="Detail/GetImg?filename=images/JFYZ201911017_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 模型结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Model architecture</p>

                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>2.1 生成器</b></h4>
                <div class="p1">
                    <p id="95">生成器以编码器和解码器的形式构建,编码器获得图像的未知区域和已知区域之间的变化和关系,接着将这些信息反馈到解码器中以生成图像.</p>
                </div>
                <div class="p1">
                    <p id="96">在这样结构中,源图像通过逐渐下采样操作直到瓶颈层,此时该过程被反转到编码器.这种网络结构要求信息通过所有网络层,包括瓶颈层.对于图像转换问题,在输入和输出之间共享大量低级信息能够有效地提高转换效果,并且希望直接在网络之间传送该信息而不需要多层的采样过程.图像修复问题其实也是一种图像转换问题,将有缺失的图像转换成完整的图像.所以本模型中的生成器采用了跳跃连接,编码器中卷积核大小为4×4,步长为2,补零操作为1,每经过一个卷积操作,图像缩小二分之一,卷积核个数翻倍,同时在经过激活函数之前将得到的特征映射图信息通过跳跃连接传递到解码器相对应的位置.输入图像的大小为256×256×3,下采样到大小为2×2×512的特征图之后,再经过全链接卷积将特征信息传递到解码器.</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>2.2 全连接卷积</b></h4>
                <div class="p1">
                    <p id="98">由于卷积只能将所有特征映射图连接在一起,不直接连接单个特征映射图内的特征值.以前提出的方法中常用全连接层进行连接,然而全连接层所用参数过多会加长训练时间以及占用更多GPU显存,所以我们的模型采用与上下文编码器<citation id="290" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中一样的方法,使用全连接卷积.将图像下采样到512×2×2大小后,通过最后一层全连接卷积将图像特征压缩到4 000×1×1大小模拟全连接层,使单个特征映射层达到全连接效果.如果采用全连接层进行连接,对于<i>m</i>个大小为<i>n</i>×<i>n</i>的特征映射图,需要<i>m</i><sup>4</sup>×<i>n</i>×<i>n</i>个参数,然而运用全连接卷积操作则只需要<i>m</i><sup>2</sup>×<i>n</i>×<i>n</i>个参数.</p>
                </div>
                <div class="p1">
                    <p id="99">由于生成器采用全连接卷积层(fully connected convolutional layer)以及U-net模型架构作为基础,所为我们的生成器模型称为FU-net(fully connected convolutional U-net).</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>2.3 局部属性对抗网络</b></h4>
                <div class="p1">
                    <p id="101">生成器对于修复人脸整体区域已经有着良好的效果,但是对于局部信息的学习却不够优秀.</p>
                </div>
                <div class="p1">
                    <p id="102">如果缺失部分仅仅是中心的固定区域,如图2(c)<i>F</i><sub>2</sub>所示,我们的模型在图像内容以及语义上修复效果很优秀,如果缺失部分是人脸的一部分属性(眼睛或嘴巴),如图2(c)所示,修复效果则会明显变差,这是因为生成网络FU-net在跳转连接各层信息的同时也会将孔洞信息传递过去,这些孔洞的像素值为0,由于我们只用中心遮盖图像作为源图像,所以生成器只有当脸部整体被遮盖时才能将这些0像素值通过通过网络准换成合理的特征值.如果修复局部人脸特征,FU-net中的编码器会将0像素值直接传递到解码器对应位置中,从而导致修复效果模糊并且色彩感知度低.所以,我们利用局部属性辨别器对生成后的图像进行局部加强,局部属性辨别器基于补丁辨别器<citation id="291" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>设计,将生成图像的中心区域通过多层卷积后转换成1×256×256大小的特征图,特征图上的值越接近0则与真实图像差距越小,而越接近1则差距越大.补丁辨别器有一个良好属性,其能够适应任意大小的输入.但是原始补丁辨别器并不能帮助局部修复效果更加优秀,因为其只能提高图像固定区域修复效果,所以我们提出局部属性辨别器,辨别器同样判别固定大小的中心区域是否为真实图像,但是生成器在与辨别器对抗时并不是将修复后的中心区域与辨别器对抗,我们利用补丁辨别器对于输入大小没有强制性的特性,将生成的图像中的关键属性(例如眼睛或嘴巴)截取出来进行对抗训练,使生成器与辨别器在人脸局部属性范围内相互对抗相互加强,这样能够保证网络不仅在整体修复上更加优秀,同时也会解决局部内容和语义修复问题.运用对抗网络之前和加入对抗网络之后的局部修复效果如图2(d)<i>F</i><sub>3</sub>所示.通过对抗网络我们明显改善了生成器的修复效果,提高了其在局部属性上的泛化性能,而不是局限于中心区域.</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 效果改善展示图" src="Detail/GetImg?filename=images/JFYZ201911017_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 效果改善展示图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Effect improvement display</p>

                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.4 损失函数</b></h4>
                <div class="p1">
                    <p id="105">本文通过将修复后的图像尽量回归到真实图像来训练我们的网络,通过联合多个损失函数来使最终的修复结果<i>I</i><sub>re</sub>与原始图像<i>I</i><sub>gt</sub>的距离减小.</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">2.4.1 重构损失</h4>
                <div class="p1">
                    <p id="107">使用<i>L</i><sub>1</sub>损失作为图像重构损失函数<i>Lr</i><sub>1</sub>,其定义为</p>
                </div>
                <div class="p1">
                    <p id="108"><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>E</mi><mrow><mo>|</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext></mrow></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>,(1)</p>
                </div>
                <div class="p1">
                    <p id="109"><i>E</i>代表求均值操作.重构损失<i>Lr</i><sub>1</sub>对于捕获修复区域的整体结构以及内容的一致性具有优越的效果,但倾向于将预测中的多种模式平均化.</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">2.4.2 对抗损失</h4>
                <div class="p1">
                    <p id="111">对抗性损失试图使修复结果看起来真实,并具有从分布中选择特定模式的效果,从而弥补重构损失的不足.以往的局部辨别器<citation id="292" type="reference"><link href="209" rel="bibliography" /><link href="211" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>虽然能够针对图像局部进行加强修复,但是对于原图像缺失部分大小和尺寸却需要固定,并不能完全适用于人脸中任意属性的修复.为了不用针对不同大小和尺寸的缺失部分去训练不同尺寸的辨别器,我们基于补丁辨别器设计了适用于任意输入尺寸的局部属性辨别器.在训练辨别器时只需判断图像中心区域是否为真实图像,当训练生成器去对抗辨别器时,我们不用中心区域去对抗而是用局部区域(例如眼睛和嘴巴),所以损失函数为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mtext>G</mtext><mtext>A</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false">(</mo><mi>G</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">(</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>,</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113"><i>k</i>代表不同局部属性的截取图像,<i>D</i>是局部属性辨别器.综上所述,本文的总体函数结合对抗损失以及重构损失为</p>
                </div>
                <div class="p1">
                    <p id="114"><i>L</i><sub>all</sub>=<i>λ</i><sub>1</sub>×<i>Lr</i><sub>1</sub>+<i>λ</i><sub>2</sub>×<i>L</i><sub>GAN</sub>,(3)</p>
                </div>
                <div class="p1">
                    <p id="115">其中<i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>分别为重构损失以及对抗损失的权重.</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"><b>2.5 缺失区域</b></h4>
                <div class="p1">
                    <p id="117">在本文的模型中,输入是1张人脸图像<i>I</i><sub>in</sub>,灰色部分像素值为0.去除的区域必须对应人脸的整体大小情况以及人脸的各个属性区域分布情况.</p>
                </div>
                <div class="p1">
                    <p id="118">经过多次覆盖实验发现,170×170像素的补丁在CelebA数据集中能覆盖人脸的所有关键属性(如眼睛和嘴巴),也就是人脸的中心区域,如图3所示,图3(a)是添加170×170补丁的人脸<i>I</i><sub>in</sub>,图3(b)是原始图像<i>I</i><sub>gt</sub>.运用这种遮盖方式,在进行对抗训练时,能够截取到所有修复后的关键属性,从而对这些区域进行细节加强.</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 整体区域缺失" src="Detail/GetImg?filename=images/JFYZ201911017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 整体区域缺失  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Missing overall area</p>

                </div>
                <h3 id="120" name="120" class="anchor-tag"><b>3 实  验</b></h3>
                <h4 class="anchor-tag" id="121" name="121"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="122">我们的模型采用CelebA数据集训练以及验证我们的模型.数据集由202 599幅人脸图像组成,每张人脸图像都通过OpenCV自带人脸检测函数进行裁剪,保证2眼的位置大致对齐,并且被缩放成3×256×256的像素大小.我们遵循该数据集标准分割,其中162 770张图片作为训练集,19 867张图片作为验证集,19 962张作为测试集.为了防止模型过拟合,实施了随机反转、打乱数据等操作.</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>3.2 训练方法</b></h4>
                <div class="p1">
                    <p id="124">我们通过预训练使生成器学习脸部的整体属性,生成器中编码器激活函数采用LeakyReLU,解码器中的激活函数采用ReLU函数.编码器与解码器中所有正则操作都采用实例正则化(instance normalization),因为本文的模型批量大小(batch size)设置为1,而批量正则化(batch normalization)适合于模型批量大小(batch size)的值大于1的情况,使用实例正则化更适合训练要求,局部属性辨别器同样采用实例正则化(instance normalization),激活函数采用ReLU.</p>
                </div>
                <div class="p1">
                    <p id="125">本文采用分步训练的方法以稳定辨别器的效果:1)利用中心缺失图像训练生成器,使产生的图像尽量回归到真实图像;2)将生成器参数固定,只训练局部属性辨别器;3)将辨别器以及生成器联合训练并利用总体函数对训练进行约束从而达到最终的效果.</p>
                </div>
                <div class="p1">
                    <p id="126">本文实验使用深度学习框架Pytorch3.0.编程语言为python3.5,操作系统为Windows10.实验的硬件环境为Intel<sup>®</sup> Core<sup>TM</sup> i5-8700k处理器,内存为16 GB,NVIDIA 1080TI显卡,该配置是目前深度学习计算的主流配置.</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>3.3 人脸修复</b></h4>
                <h4 class="anchor-tag" id="128" name="128">3.3.1 修复效果</h4>
                <div class="p1">
                    <p id="129">如图4展示了在CelebA测试集上的修复效果,图4分3个实验,每个实验分3组,每一组中第1列是真实图像,第2列是需要修复的图像,第3列是修复结果.每一组需修复的图像中至少遮盖住人脸的一个重要属性(眼睛、嘴巴)甚至多个属性,遮盖区域不能够超过170×170的人脸中心区域,需要在这个区域内修复才能得到最佳效果.图4展现出本文模型生成内容的真实性、创新性以及美观性,无论是针对人脸的整体或者局部都有良好的修复效果.</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 修复结果展示图" src="Detail/GetImg?filename=images/JFYZ201911017_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 修复结果展示图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Inpainted result</p>

                </div>
                <div class="p1">
                    <p id="131">如图4(a)(b)的第3行实验结果所示,部分原图像本身就有对人脸的重要属性进行遮盖(比如眼镜遮盖眼睛),是否运用这种遮盖方式可以通过我们的方法控制,如果想去除眼镜对眼睛的遮盖,通过将眼镜完全覆盖就能够还原出眼睛.因为数据集中大部分数据都是没有使用眼镜进行遮挡,我们的方法能够有效地学习到人脸的眼睛属性.</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">3.3.2 算法对比</h4>
                <div class="p1">
                    <p id="133">我们将CE和FG作为基准方法用于验证模型有效性.在测试集上运用2个评价指标对本文的方法进行评价,这2个指标也是图像修复中的通用指标,在FG与CE中都采用了这2个标准.第1个指标是峰值信噪比(PSNR),直接比较图之间像素值的差距.第2个是结构相似性指数(SSIM)<citation id="293" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>,这是一种全参考的图像质量评价指标,它分别从亮度、对比度、结构3方面度量图像的相似程度.对于测试用例,本文采用与FG中同一张图片并且采用同样的6种遮盖方式,如图5所示.FG中采用的是128×128像素图片,而本文的模型采用的图像大小是256×256,所以将修复后的图像压缩成128×128进行对比.</p>
                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 测试用例" src="Detail/GetImg?filename=images/JFYZ201911017_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 测试用例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Test case</p>

                </div>
                <div class="p1">
                    <p id="135">表1和表2分别展示了本文模型与基准模型在PSNR以及SSIM指标上的实验比较结果.其中FU表示只有生成器进行图像修复没有局部属性辨别器对其局部属性修复效果进行加强;而Ours表示本文的完整方法;列4和列5分别表示上下文编码器CE和人脸生成方法FG.</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表1 基于PSNR评价指标的对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison Results Based on PSNR</b></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />Input</td><td>FU</td><td>Ours</td><td>CE</td><td>FG</td></tr><tr><td><br />O1</td><td>19.90</td><td><b>20.04</b></td><td>18.6</td><td>20.0</td></tr><tr><td><br />O2</td><td>21.39</td><td><b>21.41</b></td><td>18.4</td><td>19.8</td></tr><tr><td><br />O3</td><td>19.87</td><td><b>20.35</b></td><td>17.9</td><td>18.8</td></tr><tr><td><br />O4</td><td>20.92</td><td><b>20.94</b></td><td>19.0</td><td>19.7</td></tr><tr><td><br />O5</td><td>20.57</td><td><b>20.92</b></td><td>19.1</td><td>19.5</td></tr><tr><td><br />O6</td><td>18.70</td><td><b>20.02</b></td><td>19.3</td><td>20.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best results are bold.</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表2 基于SSIM评价指标的对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Comparison Results Based on SSIM</b></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />Input</td><td>FU</td><td>Ours</td><td>CE</td><td>FG</td></tr><tr><td><br />O1</td><td>0.730</td><td>0.740</td><td>0.772</td><td><b>0.824</b></td></tr><tr><td><br />O2</td><td>0.730</td><td>0.797</td><td>0.774</td><td><b>0.826</b></td></tr><tr><td><br />O3</td><td>0.777</td><td><b>0.789</b></td><td>0.719</td><td>0.759</td></tr><tr><td><br />O4</td><td>0.817</td><td><b>0.823</b></td><td>0.754</td><td>0.789</td></tr><tr><td><br />O5</td><td>0.813</td><td><b>0.820</b></td><td>0.757</td><td>0.784</td></tr><tr><td><br />O6</td><td>0.707</td><td>0.717</td><td>0.818</td><td><b>0.841</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best results are bold.</p>
                </div>
                <div class="p1">
                    <p id="138">如表1以及表2所示,PSNR以及SSIM评测指标都测定出本文的完整方法效果更优秀,这证明本文的模型在经过局部属性辨别器进行对抗训练后,模型整体对于图像深层语义的理解能力有了显著的提高.同时在PSNR数值上本文模型针对CE和FG都有明显的提高,这也证明了本模型的优越效果.</p>
                </div>
                <div class="p1">
                    <p id="139">本文的模型虽然在PSNR上对比结果比较优秀,但与FG在SSIM数值方面进行比较发现:仅仅在O3,O4,O5上比FG高,这是因为除了O3,O4,O5以外的遮盖区域都有大部分超过了图像中心遮盖(170×170)范围,这些区域会模糊并且变暗,存在色彩差距,图像整体性被破坏.但是另一小部分在中心区域内,这一部分的修复效果会比较优秀,像素值得到了还原.PSNR检测的是图像像素值的差距,而SSIM从亮度、对比度、结构3方面衡量,在O1,O2,O4,O5,O6区域还原像素值差距并不大,而亮度和对比度差距相对较大,所以会出现这种PSNR值较高而SSIM值较小的反差结果.</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 缺陷的改善展示" src="Detail/GetImg?filename=images/JFYZ201911017_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 缺陷的改善展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The display of defect improvement</p>

                </div>
                <div class="p1">
                    <p id="141">本文的方法不仅仅在评测指标上体现出优秀的结果,同时也解决了FG中2个局限,第1个局限是不对称图像修复效果不够优秀,这是因为其生成网络所采用的编码器-解码器结构虽然能够有效地还原图像,但是由于编码器与解码器之间的特征没有直接的关联,在特征信息传递通过瓶颈层时会出现不可避免的信息损失,从而FG中的生成器并不能充分理解语义.第2个局限是该模型没有充分理解相邻像素之间的空间相关性,无法通过已知的上下文信息推断出要生成的信息,特别是针对颜色的连续性修复.对于以上2个问题,本文都进行了有效的解决.由于本文的生成器FU-net是基于U-net网络进行改进,利用特征图跳跃传播的特性有效地将相邻像素连接起来,学习到像素点的空间相关性.同时由于采用局部属性辨别器进行对抗训练,在学习到人脸的整体属性后再去学习关键属性,在非对称图中能够有效地进行局部修复.如图6所示.图6包含3组实验结果,图6(a)是真实图像,图6(b)是需修复的图像,图6(c)是修复结果.如行1、行2所示,我们的模型能够很好地修复不对称人脸中的局部属性;如行3、行4所示,本文方法通过下嘴唇的色彩特征完好地修复了上嘴唇色,解决了相邻像素的空间不相关问题.</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142"><b>3.4 局限性</b></h4>
                <div class="p1">
                    <p id="143">本文的方法仍然有着局限性,在训练过程中我们采用局部属性辨别器进行加强,然而局部属性辨别器只能针对截取出来的人脸的重要属性特征(眼睛、鼻子)加强生成器修复效果,当我们的遮盖范围不是170×170的中心区域或者不是局部属性区域时修复效果就会不够真实,如图7所示,图7有(a)(b)两组实验结果,每一组实验结果包括最左侧的真实图像、中间的输入图像和最右侧的修复后的图像.这2组实验结果中的遮盖区域都超过了170×170的中心区域,其中第1组实验遮盖区域与表1和表2所示的O6区域相近,这也是本文方法在SSIM数值上相对于其他方法较低的原因.</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911017_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 模型局限性" src="Detail/GetImg?filename=images/JFYZ201911017_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 模型局限性  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911017_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Model limitations</p>

                </div>
                <h3 id="145" name="145" class="anchor-tag">4<b> 总  结</b></h3>
                <div class="p1">
                    <p id="146">本文提出了一种新颖的模型用于解决人脸图像修复问题,该模型从人脸总体以及局部属性出发,基于局部属性对抗网络和FU-net生成器,从图像语义到图像视觉都达到优秀的效果,并且能够有效地修复人脸的重要属性.未来我们会改进我们的方法,在输入时覆盖任意随机区域都能够达到优秀的修复效果,我们会构建图像模具,基于模具进行精确修复,并且改善FU-net网络,使我们的修复效果进一步精进.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="187">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image inpainting">

                                <b>[1]</b>Bertalmio M,Sapiro G,Caselles V,et al.Image inpainting[C] //Proc of the 27th Annual Conf on Computer Graphics and Interactive Techniques.New York:ACM,2000:417- 424
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Completion Using Global Optimization">

                                <b>[2]</b>Komodakis N.Image completion using global optimization[C] //Proc of 2006 IEEE Computer Society Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2006:442- 452
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Space-time videocompletion">

                                <b>[3]</b>Wexler Y,Shechtman E,Irani M.Spacetime video completion[C] //Proc of the 2004 IEEE Computer Society Conf on Computer Vision and Pattern Recognition.Los Alamitos,CA:IEEE Computer Society,2004:100- 111
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PatchMatch: A randomized correspondence algorithm for structural image editing">

                                <b>[4]</b>Barnes C,Shechtman E,Finkelstein A,et al.Patch match:A randomized correspondence algorithm for structural image editing[J].ACM Transactions on Graphics,2009,28(3):21- 28
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous structure and texture image inpainting">

                                <b>[5]</b>Bertalmio M,Vese L,Sapiro G,et al.Simultaneous structure and texture image inpainting[J].IEEE Transactions on Image Processing,2003,12(8):882- 889
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hole ?lling through photomontage">

                                <b>[6]</b>Wilczkowiak M,Brostow G J,Tordoﬀ B,et al.Hole ﬁlling through photomontage[C] //Proc of the 2005 Conf of the British Machine Vision.Piscataway,NJ:IEEE,2015:30- 41
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Texture synthesis by non-parametric sampling">

                                <b>[7]</b>Efros A A,Leung T K.Texture synthesis by nonparametric sampling[C] //Proc of the 7th IEEE Int Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,1999:1033- 1038
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Quilting for Texture Synthesis and Transfer">

                                <b>[8]</b>Efros A A,Freeman W T.Image quilting for texture synthesis and transfer[C] //Proc of the 28th Conf on Computer Graphics and Interactive Techniques.New York:ACM,2001:341- 346
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098456&amp;v=MjE5OTFUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMM0lKRndXYUJjPU5pZklZN0s3SHRqTnI0OUZaT0lIQ0hrL29CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Hays J,Efros A A.Scene completion using millions of photographs[J].ACM Transactions on Graphics,2007,26(4):36- 40
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Get out of my picture!">

                                <b>[10]</b>Whyte O,Sivic J,Zisserman A.Get out of my picture![C] //Proc of 2009 British Machine Vision.Piscataway,NJ:IEEE,2009:126- 138
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098255&amp;v=MTc0MjZJSERuazhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMM0lKRndXYUJjPU5pZklZN0s3SHRqTnI0OUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Kwatra V,Essa I,Bobick A,et al.Texture optimization for example based synthesis[J].ACM Transactions on Graphics,2005,24:795- 802
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM408B7C15F867A55233C65423FF57BC91&amp;v=MjY1MjNRNlV4N1RYdmdyMlJEZkxYbU5yT2VDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOcGh3THE5d3EwPU5pZklZN2U0RnFQTDNJNUFFdU1KQ3cwOHloUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Iizuka S,Simo-Serra E,Ishikawa H.Globally and locally consistent image completion[J].ACM Transactions on Graphics,2017,36(4):107- 114
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Face Completion">

                                <b>[13]</b>Li Yijun,Liu Sifei,Yang Jimei,et al.Generative face completion[C] //Proc of the 2017 IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:5892- 5900
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context encoders:Feature learning by inpainting">

                                <b>[14]</b>Pathak D,Krahenbuhl P,Donahue J,et al.Context encoders:Feature learning by inpainting[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2016).Piscataway,NJ:IEEE,2016:2536- 2544
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic image inpainting with perceptual and contextual losses">

                                <b>[15]</b>Yeh R,Chen Chen,Lim T Y,et al.Semantic image inpainting with perceptual and contextual losses[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:5009- 5018
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution image inpainting using multi-scale neural patch synthesis">

                                <b>[16]</b>Yang Chao,Lu Xin,Lin Zhe,et al.High-resolution image inpainting using multi-scale neural patch synthesis[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:4076- 4084
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:convolutional networks for biomedical image segmentation">

                                <b>[17]</b>Ronneberger O,Fischer P,Brox P.U-net:Convolutional networks for biomedical image segmentation[C] //Proc of the 18th Int Conf on Medical Image Computing and Computer Assisted Intervention (MICCAI 2015).Piscataway,NJ:IEEE,2015:3157- 3165
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning face attributes in the wild">

                                <b>[18]</b>Liu Ziwei,Luo Ping,Wang Xiaogang,et al.Deep learning face attributes in the wild[C] //Proc of 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:3730- 3738
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Filling-in by joint interpolation of vector fields and gray levels">

                                <b>[19]</b>Coloma B,Marcelo B,Vicent C et al.Filling-in by joint interpolation of vector ﬁelds and black levels[J].IEEE Transactions on Image Processing,2001,10(8):1200- 1211
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD0DE016AB29E895183FB16BF6ACA81005&amp;v=MDExNzVHUWxmQ3BiUTM1TnBod0xxOXdxMD1Oam5CYXJQTWE5SE5xZjQzWnVKNkJIVTh6aDRRbkUxOFRnMlVxbU5HQ0xxVlJicWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>Telea A.An image inpainting technique based on the fast marching method[J].Journal of Graphics Tools,2004,9(1):23- 34
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[21]</b>Goodfellow I,Pouget-Abadie J,Mirza M,et al.Generative adversarial nets[G] //Advances in Neural Information Processing Systems.New York:ACM,2014:2672- 2680
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">

                                <b>[22]</b>Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].arXiv preprint arXiv:1511.06434,2015
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">

                                <b>[23]</b>Arjovsky M,Chintala S,Bottou L.Wasserstein GAN[J].arXiv preprint arXiv:1701.07875,2017
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved training of wasserstein GANs">

                                <b>[24]</b>Gulrajani I,Ahmed F,Arjovsky M,et al.Improved training of wasserstein GANs[J].arXiv preprint arXiv:1704.00028,2017
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Least squares generative adversarial networks">

                                <b>[25]</b>Mao Xudong,Li Qing,Xie Haoran,et al.Least squares generative adversarial networks[J].arXiv preprint ArXiv:1611.04076,2016
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=InfoGAN:interpretable representation learning by information maximizing generative adversarial nets">

                                <b>[26]</b>Chen Xi,Duan Yan,Houthooft R,et al.InfoGAN:Interpretable representation learning by information maximizing generative adversarial nets[C] //Proc of the 30th Conf on Neural Information Processing Systems (NIPS 2016).New York:ACM,2016:5600- 5611
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Progressive growing of GANs for improved quality,stability,and variation">

                                <b>[27]</b>Tero K,Timo A,Samuli L,et al.Progressive growing of GANs for improved quality,stability,and variation[J].arXiv preprint arXiv:1710.10196,2017
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic image inpainting with deep generative models">

                                <b>[28]</b>Raymond A Y,Chen Chen,Teck Y L,et al.Semantic image inpainting with deep generative models[C] //Proc of the 2017 IEEE Conf on Computer Vision and Pattern Recognition (CVPR2017).Piscataway,NJ:IEEE,2017:5485- 5493
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative image inpainting with contextual attention">

                                <b>[29]</b>Yu Jiahui,Lin Zhe,Yang Jimei,et al.Generative image inpainting with contextual attention[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2018).Piscataway,NJ:IEEE,2018:4076- 4084
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep image prior">

                                <b>[30]</b>Ulyanov D,Vedaldi A,Lempitsky V.Deep image prior[J].arXiv preprint arXiv:1711.10925,2017
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image inpainting using multi-scale feature image translation">

                                <b>[31]</b>Song Yuhang,Yang Chao,Lin Zhe,et al.Image inpainting using multi-scale feature image translation[J].arXiv preprint arXiv:1711.08590,2017
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image inpainting for irregular holes using partial convolutions">

                                <b>[32]</b>Liu Guilin,Fitsum A,Kevin J,et al.Image inpainting for irregular holes using partial convolutions[J].arXiv preprint arXiv:1804.07723,2018
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-to-image translation with conditional adversarial networks">

                                <b>[33]</b>Isola P,Zhu Junyan,Zhou Tinghui,et al.Image-to-image translation with conditional adversarial networks[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2017).Piscataway,NJ:IEEE,2017:6822- 6834
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">

                                <b>[34]</b>Wang Zhou,Bovik A C,Sheikh H R,et al.Image quality assessment:From error vsibility to structural similarity[J].IEEE Transactions on Image Processing,2004,13(4):32- 40
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201911017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201911017&amp;v=MzIwNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSc0Z5em5VYjNOTHl2U2RMRzRIOWpOcm85RVk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZk5pUjBwelhReWJPVnl3VT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

