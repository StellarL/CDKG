<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243653565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902014%26RESULT%3d1%26SIGN%3daCqyicjOHPDHHlBOgZXUIVgDo2E%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902014&amp;v=MTU0MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blc3ek5MeXZTZExHNEg5ak1yWTlFWUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#232" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;相关工作&lt;/b&gt; "><b>1</b><b>相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#237" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;基于单帧图像的环境光遮蔽算法&lt;/b&gt; "><b>2</b><b>基于单帧图像的环境光遮蔽算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#238" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;问题定义&lt;/b&gt;"><b>2.1</b><b>问题定义</b></a></li>
                                                <li><a href="#249" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;编码器-解码器设计&lt;/b&gt;"><b>2.2</b><b>编码器-解码器设计</b></a></li>
                                                <li><a href="#254" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;损失函数&lt;/b&gt;"><b>2.3</b><b>损失函数</b></a></li>
                                                <li><a href="#258" data-title="&lt;b&gt;2.4&lt;/b&gt;&lt;b&gt;卷积网络结构设计&lt;/b&gt;"><b>2.4</b><b>卷积网络结构设计</b></a></li>
                                                <li><a href="#273" data-title="&lt;b&gt;2.5&lt;/b&gt;&lt;b&gt;仿真数据生成&lt;/b&gt;"><b>2.5</b><b>仿真数据生成</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#276" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;实验结果与讨论&lt;/b&gt; "><b>3</b><b>实验结果与讨论</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#278" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;模型评估&lt;/b&gt;"><b>3.1</b><b>模型评估</b></a></li>
                                                <li><a href="#283" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;仿真数据评估结果&lt;/b&gt;"><b>3.2</b><b>仿真数据评估结果</b></a></li>
                                                <li><a href="#291" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;真实图像&lt;/b&gt;"><b>3.3</b><b>真实图像</b></a></li>
                                                <li><a href="#296" data-title="&lt;b&gt;3.4&lt;/b&gt;&lt;b&gt;失败案例&lt;/b&gt;"><b>3.4</b><b>失败案例</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#300" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;结&lt;/b&gt;&lt;b&gt;语&lt;/b&gt; "><b>4</b><b>结</b><b>语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#251" data-title="图1 编码器-解码器结构示意图">图1 编码器-解码器结构示意图</a></li>
                                                <li><a href="#262" data-title="图2 多级训练网络结构示意图">图2 多级训练网络结构示意图</a></li>
                                                <li><a href="#266" data-title="图3 联合训练网络结构示意图">图3 联合训练网络结构示意图</a></li>
                                                <li><a href="#272" data-title="图4 端到端训练网络结构示意图">图4 端到端训练网络结构示意图</a></li>
                                                <li><a href="#281" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;3种网络结构数值精度比较&lt;/b&gt;"><b>表1</b><b>3种网络结构数值精度比较</b></a></li>
                                                <li><a href="#282" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;3种网络结构视觉效果比较&lt;/b&gt;"><b>表2</b><b>3种网络结构视觉效果比较</b></a></li>
                                                <li><a href="#287" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;不同方法数值误差对比&lt;/b&gt;"><b>表3</b><b>不同方法数值误差对比</b></a></li>
                                                <li><a href="#290" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;b&gt;不同方法视觉效果对比&lt;/b&gt;"><b>表4</b><b>不同方法视觉效果对比</b></a></li>
                                                <li><a href="#294" data-title="图5 真实场景结果图">图5 真实场景结果图</a></li>
                                                <li><a href="#298" data-title="图6 失败案例">图6 失败案例</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="343">


                                    <a id="bibliography_1" title="Bavoil L, Sainz M, Dimitrov R.Image-space horizon-based ambient occlusion[C]Proc of the 35th ACM SIGGRAPH.New York:ACM, 2008[2017-11-28].https:dl.acm.org/citation.cfm/id=1401061" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-space horizon-based ambient occlusion">
                                        <b>[1]</b>
                                        Bavoil L, Sainz M, Dimitrov R.Image-space horizon-based ambient occlusion[C]Proc of the 35th ACM SIGGRAPH.New York:ACM, 2008[2017-11-28].https:dl.acm.org/citation.cfm/id=1401061
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_2" title="Bavoil L, Sainz M.Multi-layer dual-resolution screen-space ambient occlusion[C]Proc of the 36th ACM SIGGRAPH.New York:ACM, 2009[2017-11-28].https:dl.acm.org/citation.cfm/id=1598035" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-layer dual-resolution screen-space ambient occlusion">
                                        <b>[2]</b>
                                        Bavoil L, Sainz M.Multi-layer dual-resolution screen-space ambient occlusion[C]Proc of the 36th ACM SIGGRAPH.New York:ACM, 2009[2017-11-28].https:dl.acm.org/citation.cfm/id=1598035
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_3" title="McGuire M, Osman B, Bukowski M, et al.The alchemy screen-space ambient obscurance algorithm[C]Proc of the38th ACM SIGGRAPH Symp on High Performance Graphics.New York:ACM, 2011:25-32" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Alchemy Screen-Space Ambient Obscurance Algorithm">
                                        <b>[3]</b>
                                        McGuire M, Osman B, Bukowski M, et al.The alchemy screen-space ambient obscurance algorithm[C]Proc of the38th ACM SIGGRAPH Symp on High Performance Graphics.New York:ACM, 2011:25-32
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_4" title="Li Wenyao, Ren Zhong.Self-adaptive multilayer screen space ambient occlusion[J].Journal of Computer-Aided Design&amp;amp;Computer Graphics, 2011, 23 (8) :1294-1303 (in Chinese) (李文耀, 任重.自适应的多层屏幕空间环境光遮蔽[J].计算机辅助设计与图形学学报, 2011, 23 (8) :1294-1303) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201108002&amp;v=MTU2NjVMRzRIOURNcDQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XN3pOTHo3QmE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Li Wenyao, Ren Zhong.Self-adaptive multilayer screen space ambient occlusion[J].Journal of Computer-Aided Design&amp;amp;Computer Graphics, 2011, 23 (8) :1294-1303 (in Chinese) (李文耀, 任重.自适应的多层屏幕空间环境光遮蔽[J].计算机辅助设计与图形学学报, 2011, 23 (8) :1294-1303) 
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_5" title="Langer M S, Zucker S W.Shape-from-shading on a cloudy day[J].Journal of the Optical Society of America:A, 1994, 11 (2) :467-478" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape-from-shading on a cloudy day">
                                        <b>[5]</b>
                                        Langer M S, Zucker S W.Shape-from-shading on a cloudy day[J].Journal of the Optical Society of America:A, 1994, 11 (2) :467-478
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_6" title="Prados E, Jindal N, Soatto S.A non-local approach to shape from ambient shading[C]Proc of Scale Space and Variational Methods in Computer Vision.Berlin:Springer, 2011:696-708" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-local approach to shape from ambient shading">
                                        <b>[6]</b>
                                        Prados E, Jindal N, Soatto S.A non-local approach to shape from ambient shading[C]Proc of Scale Space and Variational Methods in Computer Vision.Berlin:Springer, 2011:696-708
                                    </a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_7" title="Wu Chenglei, Wilburn B, Matsushita Y, et al.High-quality shape from multi-view stereo and shading under general illumination[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:969-976" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-quality shape from multi-view stereo and shading under general illumination">
                                        <b>[7]</b>
                                        Wu Chenglei, Wilburn B, Matsushita Y, et al.High-quality shape from multi-view stereo and shading under general illumination[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:969-976
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_8" title="Aldrian O, Smith W A P.Inverse rendering of faces on a cloudy day[C]Proc of the 12th European Conf on Computer Vision.Berlin:Springer, 2012:201-214" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverse rendering of faces on a cloudy day">
                                        <b>[8]</b>
                                        Aldrian O, Smith W A P.Inverse rendering of faces on a cloudy day[C]Proc of the 12th European Conf on Computer Vision.Berlin:Springer, 2012:201-214
                                    </a>
                                </li>
                                <li id="359">


                                    <a id="bibliography_9" title="Beeler T, Bradley D, Zimmer H, et al.Improved reconstruction of deforming surfaces by cancelling ambient occlusion[C]Proc of the 12th European Conf on Computer Vision.Berlin:Springer, 2012:30-43" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved reconstruction of deforming surfaces by cancelling ambient occlusion">
                                        <b>[9]</b>
                                        Beeler T, Bradley D, Zimmer H, et al.Improved reconstruction of deforming surfaces by cancelling ambient occlusion[C]Proc of the 12th European Conf on Computer Vision.Berlin:Springer, 2012:30-43
                                    </a>
                                </li>
                                <li id="361">


                                    <a id="bibliography_10" title="Laffont P Y, Bousseau A, Paris S, et al.Coherent intrinsic images from photo collections[J].ACM Transactions on Graphics, 2012, 31 (6) :202-213" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007838&amp;v=MzE1OTFHZXJxUVRNbndaZVp1SHlqbVViN0lKbHdjYVJjPU5pZklZN0s3SHRqTnI0OUZaT3NJQkg4eG9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Laffont P Y, Bousseau A, Paris S, et al.Coherent intrinsic images from photo collections[J].ACM Transactions on Graphics, 2012, 31 (6) :202-213
                                    </a>
                                </li>
                                <li id="363">


                                    <a id="bibliography_11" title="Hauagge D, Wehrwein S, Bala K, et al.Photometric ambient occlusion[C]Proc of the 31st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2013:2515-2522" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photometric ambient occlusion">
                                        <b>[11]</b>
                                        Hauagge D, Wehrwein S, Bala K, et al.Photometric ambient occlusion[C]Proc of the 31st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2013:2515-2522
                                    </a>
                                </li>
                                <li id="365">


                                    <a id="bibliography_12" title="Yang Wei, Ji Yu, Lin Haiting, et al.Ambient occlusion via compressive visibility estimation[C]Proc of the 33rd IEEEConf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3882-3889" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ambient occlusion via compressive visibility estimation">
                                        <b>[12]</b>
                                        Yang Wei, Ji Yu, Lin Haiting, et al.Ambient occlusion via compressive visibility estimation[C]Proc of the 33rd IEEEConf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3882-3889
                                    </a>
                                </li>
                                <li id="367">


                                    <a id="bibliography_13" title="Ma Yuwei, Shang Yafei, Wan Liang, et al.Photometric ambient occlusion from sparsely sampled illuminations[C]Proc of the 17th IEEE Int Conf on Multimedia&amp;amp;Expo Workshops (ICMEW) .Piscataway, NJ:IEEE, 2016:1-6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photometric ambient occlusion from sparsely sampled illuminations">
                                        <b>[13]</b>
                                        Ma Yuwei, Shang Yafei, Wan Liang, et al.Photometric ambient occlusion from sparsely sampled illuminations[C]Proc of the 17th IEEE Int Conf on Multimedia&amp;amp;Expo Workshops (ICMEW) .Piscataway, NJ:IEEE, 2016:1-6
                                    </a>
                                </li>
                                <li id="369">


                                    <a id="bibliography_14" title="Barrow H G, Tenenbaum J M.Recovering intrinsic scene characteristics from images[J].Computer Vision Systems, 1978, 2:3-26" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recovering intrinsic scene characteristics from images">
                                        <b>[14]</b>
                                        Barrow H G, Tenenbaum J M.Recovering intrinsic scene characteristics from images[J].Computer Vision Systems, 1978, 2:3-26
                                    </a>
                                </li>
                                <li id="371">


                                    <a id="bibliography_15" title="Land E H, McCann J J.Lightness and Retinex theory[J].Journal of the Optical Society of America, 1971, 61 (1) :1-11" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lightness and retinex theory">
                                        <b>[15]</b>
                                        Land E H, McCann J J.Lightness and Retinex theory[J].Journal of the Optical Society of America, 1971, 61 (1) :1-11
                                    </a>
                                </li>
                                <li id="373">


                                    <a id="bibliography_16" title="Tappen M F, Freeman W T, Adelson E H.Recovering intrinsic images from a single image[J].IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence, 2005, 27 (9) :1459-1472" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recovering Intrinsic Images from a Single Image">
                                        <b>[16]</b>
                                        Tappen M F, Freeman W T, Adelson E H.Recovering intrinsic images from a single image[J].IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence, 2005, 27 (9) :1459-1472
                                    </a>
                                </li>
                                <li id="375">


                                    <a id="bibliography_17" title="Shen Jianbing, Yang Xiaoshan, Jia Yunde, et al.Intrinsic images using optimization[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:3481-3487" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intrinsic images using optimization">
                                        <b>[17]</b>
                                        Shen Jianbing, Yang Xiaoshan, Jia Yunde, et al.Intrinsic images using optimization[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:3481-3487
                                    </a>
                                </li>
                                <li id="377">


                                    <a id="bibliography_18" title="Barron J T, Malik J.Shape, illumination, and reflectance from shading[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (8) :1670-1687" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape, illumination, and reflectance from shading">
                                        <b>[18]</b>
                                        Barron J T, Malik J.Shape, illumination, and reflectance from shading[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (8) :1670-1687
                                    </a>
                                </li>
                                <li id="379">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                    Ren Shaoqing, He Kaiming, Girshick R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence, 2017, 39 (6) :1137-1149</a>
                                </li>
                                <li id="381">


                                    <a id="bibliography_20" title="Jonathon L, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]Proc of the 33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3431-3440" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[20]</b>
                                        Jonathon L, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]Proc of the 33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3431-3440
                                    </a>
                                </li>
                                <li id="383">


                                    <a id="bibliography_21" title="He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Deep residual learning for image recognition[C]Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[21]</b>
                                        He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Deep residual learning for image recognition[C]Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778
                                    </a>
                                </li>
                                <li id="385">


                                    <a id="bibliography_22" title="Yu Kai, Jia Lei, Chen Yuqiang, et al.Deep learning:Yesterday, today, and tomorrow[J].Journal of Computer Research and Development, 2013, 50 (9) :1799-1804 (in Chinese) (余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201309002&amp;v=MTYyMTZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVzd6Tkx5dlNkTEc0SDlMTXBvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        Yu Kai, Jia Lei, Chen Yuqiang, et al.Deep learning:Yesterday, today, and tomorrow[J].Journal of Computer Research and Development, 2013, 50 (9) :1799-1804 (in Chinese) (余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804) 
                                    </a>
                                </li>
                                <li id="387">


                                    <a id="bibliography_23" title="Narihira T, Maire M, Yu S X.Learning lightness from human judgement on relative reflectance[C]Proc of the33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:2965-2973" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning lightness from human judgement on relative reflectance">
                                        <b>[23]</b>
                                        Narihira T, Maire M, Yu S X.Learning lightness from human judgement on relative reflectance[C]Proc of the33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:2965-2973
                                    </a>
                                </li>
                                <li id="389">


                                    <a id="bibliography_24" title="Zhou Tinghui, Krahenbuhl P, Efros A A.Learning datadriven reflectance priors for intrinsic image decomposition[C]Proc of the 33rd IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE.2015:3469-3477" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning datadriven reflectance priors for intrinsic image decomposition">
                                        <b>[24]</b>
                                        Zhou Tinghui, Krahenbuhl P, Efros A A.Learning datadriven reflectance priors for intrinsic image decomposition[C]Proc of the 33rd IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE.2015:3469-3477
                                    </a>
                                </li>
                                <li id="391">


                                    <a id="bibliography_25" title="Narihira T, Maire M, Yu S X.Direct intrinsics:Learning albedo-shading decomposition by convolutional regression[C]Proc of the 33rd IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:2992-3001" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Direct intrinsics:Learning albedo-shading decomposition by convolutional regression">
                                        <b>[25]</b>
                                        Narihira T, Maire M, Yu S X.Direct intrinsics:Learning albedo-shading decomposition by convolutional regression[C]Proc of the 33rd IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:2992-3001
                                    </a>
                                </li>
                                <li id="393">


                                    <a id="bibliography_26" title="Shi Jian, Dong Yue, Su Hao, et al.Learning nonLambertian object intrinsics across ShapeNet categories[C]Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5844-5853" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning nonLambertian object intrinsics across ShapeNet categories">
                                        <b>[26]</b>
                                        Shi Jian, Dong Yue, Su Hao, et al.Learning nonLambertian object intrinsics across ShapeNet categories[C]Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5844-5853
                                    </a>
                                </li>
                                <li id="395">


                                    <a id="bibliography_27" title="Chang A X, Funkhouser T, Guibas L, et al.ShapeNet:An information-rich 3D model repository[J/OL].2015[2017-01-20].https:arxiv.org/pdf/1512.03012.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ShapeNet:an information-rich 3D model repository">
                                        <b>[27]</b>
                                        Chang A X, Funkhouser T, Guibas L, et al.ShapeNet:An information-rich 3D model repository[J/OL].2015[2017-01-20].https:arxiv.org/pdf/1512.03012.pdf
                                    </a>
                                </li>
                                <li id="397">


                                    <a id="bibliography_28" title="Grosse R, Johnson M K, Adelson E H, et al.Ground truth dataset and baseline evaluations for intrinsic image algorithms[C]Proc of the 27th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2009:2335-2342" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ground truth dataset and baseline evaluations for intrinsic image algorithms">
                                        <b>[28]</b>
                                        Grosse R, Johnson M K, Adelson E H, et al.Ground truth dataset and baseline evaluations for intrinsic image algorithms[C]Proc of the 27th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2009:2335-2342
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),385-393 DOI:10.7544/issn1000-1239.2019.20170897            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>单帧图像下的环境光遮蔽估计</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E9%9B%A8%E6%BD%87&amp;code=35419594&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭雨潇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E9%9B%B7%E9%9C%86&amp;code=06557747&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈雷霆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E6%82%A6&amp;code=41255390&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董悦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0178313&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电子科技大学计算机科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%B9%BF%E4%B8%9C%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E4%BF%A1%E6%81%AF%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=0163839&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电子科技大学广东电子工程信息研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%BE%AE%E8%BD%AF%E4%BA%9A%E6%B4%B2%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">微软亚洲研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>环境光遮蔽 (ambient occlusion) 被广泛用于近似计算低频全局光照、消除间接光照和阴影等计算机图形学和视觉应用中.已有算法直接通过场景的3维几何, 或不同光照下的多幅图像计算每个点的环境光遮蔽, 存在着对光照和输入图像数量要求高等问题.针对以上不足, 提出了一种基于单张图像的环境光遮蔽估计算法.算法利用一个在大量仿真图像数据集上训练的卷积神经网络, 直接从自然光照条件下场景的单张图像中恢复每个点的环境光遮蔽.提出并比较了3种不同的神经网络结构设计, 实验分析验证了端到端的设计方案可以获得最佳的结果.和已有的环境光遮蔽算法方法比较, 所提出的方法不仅计算速度快, 而且在数值和视觉上具有更好的结果.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%8E%AF%E5%A2%83%E5%85%89%E9%81%AE%E8%94%BD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">环境光遮蔽;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%AC%E5%BE%81%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">本征图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E5%85%89%E7%85%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然光照;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动编码器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郭雨潇 yuxiao.guo@outlook.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-11-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>广东省应用型科技研发专项资金项目 (2015B010131002);</span>
                    </p>
            </div>
                    <h1><b>Inferring Ambient Occlusion from a Single Image</b></h1>
                    <h2>
                    <span>Guo Yuxiao</span>
                    <span>Chen Leiting</span>
                    <span>Dong Yue</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering, University of Electronic Science and Technology of China</span>
                    <span>Institute of Electronic and Information Engineering in Guangdong, University of Electronic Science and Technology of China</span>
                    <span>Microsoft Research Asia</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Ambient occlusion has been widely used in many graphics and vision tasks for approxi-mating low frequency global illumination, removing inter-reflection, and inferring scene geometry. Existing methods need either scene geometry or scene appearances under different lightings to compute the ambient occlusion of the scene, which limits the application of these methods in many real applications. In this paper, we present a new method for computing the ambient occlusion from a single image taken under unknown natural lighting. Our method needn't any scene geometry or illumination information. To this end, we utilize a convolutional neural network (CNN) , trained on a large amount of synthetic data, to directly recovery the pre-pixel ambient occlusion. We propose three network structures for ambient occlusion estimation, a cascade one and a parallel one that are based on previous CNN solution for intrinsic images, as well as an end-to-end neural network. We analyze their performance and demonstrate that comparing with parallel and cascade designs, the end-to-end design could achieve the best performance. We also valid the efficiency and effectiveness of our method on both synthetic and real images. It is not only faster, but also more accurate than previous methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ambient%20occlusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ambient occlusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=intrinsic%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">intrinsic image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=natural%20illumination&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">natural illumination;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=autoencoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">autoencoder;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Guo Yuxiao, born in 1989.PhD.His main research interests include intrinsic image, deep learning.<image id="336" type="formula" href="images/JFYZ201902014_33600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Chen Leiting, born in 1967.PhD, professor, PhD supervisor.His main research interests include computer graphics, virtual reality.<image id="338" type="formula" href="images/JFYZ201902014_33800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Dong Yue, born in 1983.PhD.His main research interests include appearance modeling.<image id="340" type="formula" href="images/JFYZ201902014_34000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-11-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Application Science and Technology Planning Project of Guangdong Province (2015B010131002);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="229">在真实三维场景中, 每个点的颜色不仅取决于光源的直接照射, 也包括周围环境全局光照的贡献.快速评估周围环境全局光照产生的亮度, 对实时绘制、求解直射分量都有很重要的作用.环境光遮蔽在计算机图形学和视觉中用于近似场景中每个点对环境的总体可见性.常常用于快速计算在低频环境光照下的全局光照.</p>
                </div>
                <div class="p1">
                    <p id="230">在图形学中, 一系列算法从已知的3维场景几何形状中直接计算环境光遮蔽.这些算法虽然可以做到实时, 但是都需要知道场景几何信息, 因而无法直接用于计算几何未知的真实场景中环境光遮蔽.针对真实场景, 计算机视觉的算法主要通过观察和统计不同方向光源照射下, 物体上同一点的光照强度变化从而对场景中每个点的环境光遮蔽属性进行估计.这类方法具有以下2点主要缺陷:1) 输入图像数量过多.为了有效地统计物体随光照变换的趋势, 已有方法往往需要对单个视点在不同光照下采集多达数十帧的图像.2) 场景光照限制.该类方法往往要求场景中存在一个或者数个已知位置和满足特定分布的方向光源.同时, 对场景的环境光强度亦做出了限制.显然, 以上2点要求在难以在大部分的应用场景中得到满足, 限制了该类方法的应用场景以及辅助其他计算机视觉问题求解的可行性.</p>
                </div>
                <div class="p1">
                    <p id="231">针对以上不足, 本文提出了一种基于单幅输入图像的环境光遮蔽估计算法.给定在自然光照下的单幅输入图像, 我们设计了一个多层的卷积神经网络, 直接估计图像中每个像素对应点的环境光遮蔽值.为了有效地训练这一卷积神经网络, 我们算法利用大量的3维物体模型, 通过真实感渲染生成大量的仿真图像和对应的环境光遮蔽贴图作为训练数据.我们提出了3种不同的网络结构和相应的训练方法.通过分析和比较, 得到了针对这一问题非常有效的一种卷积神经网络结构.仿真和对比实验证明了本文提出的方法在任意光照单张图像的输入条件下, 能够取得比现有多帧估计算法在更多输入条件下更好的结果.同时, 在真实图像中的测试表明该方法能够对真实世界的物体进行较为准确环境光遮蔽的估计.和已有的算法相比, 我们的算法极大地减少了输入图像的数目并放宽了图像的光照条件, 加快了求解速度, 扩展了算法的普适性.</p>
                </div>
                <h3 id="232" name="232" class="anchor-tag"><b>1</b><b>相关工作</b></h3>
                <div class="p1">
                    <p id="233">在图形学绘制中, 给定场景几何信息, 一些算法在场景在当前视点下的深度图像上直接快速计算环境光遮蔽<sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup>.这些方法通过对相邻像素间进行采样, 通过像素间的深度变化对沿各个方向的可视角进行估计, 进而计算环境光遮蔽.</p>
                </div>
                <div class="p1">
                    <p id="234">针对几何未知的真实场景, 一些视觉算法利用场景在不同光照下的图像计算环境光遮蔽<sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup>.文献[7]通过假设物体由单一材质构成, 利用同一方向光源在不同视点下的多帧图像中求解其环境光遮蔽.文献[8-9]假设环境光照均匀且光照强度固定, 通过单张图像针对人脸的环境光遮蔽进行计算.文献[10]借助多帧图像重建的点云信息对环境光遮蔽进行估计, 并且将其带入到了光照与物体表面反射率的估计中.在该类方法中, 环境光遮蔽常作为其算法流程的中间步骤.通过在特定的输入条件下对环境光遮蔽求解, 从而进一步优化光照或者几何结构估计精度.</p>
                </div>
                <div class="p1">
                    <p id="235">另外, 一些工作旨在对光照做有限假设和未知物体几何结构、表面材质的条件下, 通过引入额外先验求解环境光遮蔽.文献[11]提出了一种多帧图像的估计方法.通过假设物体的局部可见面呈圆锥形和独立统计各像素光照强度的变化, 建立了物体反射率与环境光遮蔽关系的数学模型, 进而使用优化方法求解问题.该方法最大的问题在于对要求输入大量不同光照方向下相同视角的图像, 在实际的应用中难以满足.同时, 该方法还存在全局模糊等问题.文献[12]允许在单张图像中存在多个光源, 结合压缩感知技术同时求解满足该图像成立最稀疏的光源分布和局部可见性关系, 有效地减少了对输入图像数量的依赖.然而, 其需要预先知道所有光源在3维空间中的位置, 使其难以推广和扩展.文献[13]在文献[11]的基础上, 通过约束各像素在多帧图像中对应的光照入射角也应该满足均匀分布, 使其在10帧左右图像的输入条件下亦能取得较为合理的效果.基于图像估计的方法往往需要从多张图像中推测物体的光照和阴影关系, 同时还需要对光照进行特殊布置或满足一些基本条件 (例如无环境光以及光源分布均匀等) .与该类方法相同, 本文通过图像对物体的环境光遮蔽进行估计.但是, 本文使用单张图像作为输入, 对物体的环境光遮蔽属性进行估计.同时, 本文算法能够在自然光照的条件下适用.</p>
                </div>
                <div class="p1">
                    <p id="236">环境光遮蔽计算可以被看作本征图像分解的扩展和延伸.本征图像假设场景材质为漫反射, 从而将输入图像分解为场景的反射率图和光照图.环境光遮蔽计算是对本征图像恢复的光照图像的进一步分解和求解.本征图像最早由文献[14]提出, 并衍生了一些具有代表性的工作<sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup>.对于该类问题, 由于未知量多于已知等式, 重点在引入合理的假设和先验使得问题可解.近年来, 由深度学习在相关视觉任务中的进展启发<sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup>, 文献[23-24]将卷积神经网络作为特征提取器, 引入到了本征图像分解中.随后, 文献[25]提出了以光照贴图和反射率贴图为卷积神经网络输出的端到端网络结构.在此基础上, 文献[26]通过在大量仿真数据中训练网络, 实现了非漫反射场景下本征图像的分解.目前, 尚未见算法将深度学习引入环境光遮蔽的计算中.</p>
                </div>
                <h3 id="237" name="237" class="anchor-tag"><b>2</b><b>基于单帧图像的环境光遮蔽算法</b></h3>
                <h4 class="anchor-tag" id="238" name="238"><b>2.1</b><b>问题定义</b></h4>
                <div class="p1">
                    <p id="239">对于环境光遮蔽, 通常定义如下:</p>
                </div>
                <div class="p1">
                    <p id="240" class="code-formula">
                        <mathml id="240"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ο</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>π</mi></mfrac><mstyle displaystyle="true"><mrow><msub><mo>∫</mo><mrow><mi>S</mi><msup><mrow></mrow><mo>+</mo></msup></mrow></msub><mi>V</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>⋅</mo><mo>&lt;</mo><mi mathvariant="bold-italic">n</mi><mo>⋅</mo><mi mathvariant="bold-italic">w</mi><mo>&gt;</mo><mtext>d</mtext><mi mathvariant="bold-italic">w</mi><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="241">其中, <i>x</i>为物体表面上的点;<i>AO</i>为该点对应的环境光遮蔽值, 其取值范围<i>AO</i>∈[0, 1];<i>S</i><sup>+</sup>是以该点法向量<b><i>n</i></b>为中心的上半球面积分区域;<i>V</i> (·) 为一个布尔函数, 表示该点与当前环境光源方向<b><i>w</i></b>间是否被其他几何结构遮挡;&lt;·&gt;为点乘操作.由定义可知, 环境光遮蔽的值仅由物体的几何形状决定, 与光照强度和物体表面反射率无关.</p>
                </div>
                <div class="p1">
                    <p id="242">给定输入的图像<i>I</i>, 假设对应场景的表面为漫反射表面, 其像素值<i>I</i> (<i>x</i>) 为</p>
                </div>
                <div class="area_img" id="342">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201902014_34200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="245">其中, <i>I</i> (<i>x</i>) 为其图像中的辐照度, 在本文中以3通道RGB表示;<i>ρ</i> (<i>x</i>) 为该像素对应的物体表面点的反射率;<i>l</i><sub>a</sub> (<i>x</i>, <b><i>w</i></b>) 为光源位置<b><i>w</i></b>的光照强度;<i>AO</i> (<i>x</i>) 为该像素对应在物体表面的环境光遮蔽值;<i>l</i>′<sub></sub>a为该像素对应在物体表面收到的总环境光强度, 定义为</p>
                </div>
                <div class="p1">
                    <p id="246"><i>l</i>′<sub></sub>a (<i>x</i>) =∫<sub><i>S</i><sup>+</sup></sub><i>l</i><sub>a</sub> (<i>x</i>, <b><i>w</i></b>) d<b><i>w</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="247">本文并不假设环境光位于无穷远, 所以对于每一像素, 其环境光强度可能不同.</p>
                </div>
                <div class="p1">
                    <p id="248">我们的目标是从输入图像<i>I</i> (<i>x</i>) 中恢复每个像素的环境遮蔽值<i>AO</i> (<i>x</i>) .与本征图像的其他问题相似, 对于单个等式存在多个未知量, 是一个典型的病态问题.我们将问题形式化为一个输入图像<i>I</i>到环境遮蔽图<i>AO</i>的映射.采用卷积神经网络对这一映射进行建模, 并通过基于大量数据的训练对模型进行回归.</p>
                </div>
                <h4 class="anchor-tag" id="249" name="249"><b>2.2</b><b>编码器-解码器设计</b></h4>
                <div class="p1">
                    <p id="250">本文采用了多层编码器-解码器卷积神经网络结构, 如图1所示:</p>
                </div>
                <div class="area_img" id="251">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902014_251.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 编码器-解码器结构示意图" src="Detail/GetImg?filename=images/JFYZ201902014_251.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 编码器-解码器结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902014_251.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Encoder-decoder network structure</p>

                </div>
                <div class="p1">
                    <p id="252">这一结构被广泛应用于计算机视觉中的不同图像解构问题.具体地, 我们使用的编码器由5层独立编码单元构成, 各编码单元包含卷积层、向下采样层 (down sampling) 、批单位化层 (batch normalization) 以及线性激活单元 (ReLU) 等操作.经过编码单元后, 特征图的大小变为输入的一半, 但是特征图数量变为2倍.即对于通过5层编码器的输入特征图边长依次变为256, 128, 64, 32, 16, 其对应的输入/输出特征图数目为3/8, 8/16, 16/32, 32/64, 64/128.随后, 将经过编码器的特征图进行特征提取, 特征提取阶段包含3个连续的卷积层、批单位化层以及线性激活单元操作.在此阶段, 保持特征图的数量以及大小不变.解码器阶段亦包含5层结构, 其解码单元包含卷积层、批单位化层、线性激活单元以及向上采样层等操作.根据文献[24], 采用近道连接 (shortcut connection) 能够很好地处理利用卷积神经网络求解图像回归问题中的输出图像模糊问题.在本文中, </p>
                </div>
                <div class="p1">
                    <p id="253">亦使用了近道连接对网络进行改进, 将编码器的特征图直接并联到同尺度的解码单元中.故对于各解码器, 其输入特征图边长依次为16, 32, 64, 128, 256, 对应的输入/输出特征图数目为128/128, 256/64, 128/32, 64/16, 32/8.随后, 将根据不同的网络结构设计方案对编码器-解码器进行组合和扩展.</p>
                </div>
                <h4 class="anchor-tag" id="254" name="254"><b>2.3</b><b>损失函数</b></h4>
                <div class="p1">
                    <p id="255">由于图像分解中各分量仅保持了相对关系, 故估计值和真实值间往往具有一个全局放缩量, 该任意性的存在使得均方估计损失函数 (MSE) 并不适用于本征图像相关的工作中.为了同时确保训练精确性和收敛性, 本文采用了放缩不变性L2损失函数<sup><a class="sup">[25]</a></sup>作为损失函数:</p>
                </div>
                <div class="p1">
                    <p id="256" class="code-formula">
                        <mathml id="256"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>Ι</mtext><mtext>L</mtext><mn>2</mn></mrow></msub><mo stretchy="false"> (</mo><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></munder><mi>y</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mn>2</mn></msubsup><mo>-</mo><mi>λ</mi><mfrac><mn>1</mn><mrow><mi>n</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></munder><mi>y</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="257">其中, <i>Y</i><sup>*</sup>为预测值;<i>Y</i>为真实值;<i>y</i>=<i>Y</i><sup>*</sup>-<i>Y</i>为两者间的估计误差;<i>i</i>, <i>j</i>分别对应图像长宽坐标;<i>n</i>为参与误差估计的总像素个数 (仅包含图像中属于目标物体的部分) ;<i>λ</i>是调节系数.当<i>λ</i>=0时, 该联合损失函数变为均方损失函数;当<i>λ</i>=1时, 该函数为一个纯粹的放缩不变损失函数.考虑到训练数据集中包含各种光照条件下的样本, 较大的<i>λ</i>能够取得更好的效果.在本文中, 所有的损失函数均采用了<i>λ</i>=0.95的联合损失函数.</p>
                </div>
                <h4 class="anchor-tag" id="258" name="258"><b>2.4</b><b>卷积网络结构设计</b></h4>
                <div class="p1">
                    <p id="259">基于编码器-解码器结构, 本文设计了3种不同的网络结构来尝试求解这一问题.</p>
                </div>
                <h4 class="anchor-tag" id="260" name="260">2.4.1 光照/环境光遮蔽多级训练结构</h4>
                <div class="p1">
                    <p id="261">一个直接的解决方案是将该问题建模为本征图像分解的扩展问题, 这样将映射分解为2步:1) 由单张图像估计物体的光照贴图;2) 由光照贴图估计物体的环境光遮蔽结果.基于这一分解, 设计一种2级的编码-解码器结构, 结构如图2所示:</p>
                </div>
                <div class="area_img" id="262">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902014_262.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多级训练网络结构示意图" src="Detail/GetImg?filename=images/JFYZ201902014_262.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多级训练网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902014_262.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Cascade training network structure</p>

                </div>
                <div class="p1">
                    <p id="263">每一级网络针对单一问题进行求解, 并且输出结果作为下一级网络的输入.具体地, 第1级网络以图像为输入, 以光照贴图为预测目标进行训练;第2级以第1级预测的光照贴图为输入, 以环境光遮蔽结果为预测目标进行训练.在优化时, 采用了交替优化的策略.即首先以光照贴图为标签, 训练第1级网络.随后, 以环境光遮蔽为标签, 在固定第1级网络参数的条件下, 对第2级网络进行训练.最后, 以环境光遮蔽为标签, 同时优化第1/第2级网络.本文亦尝试了同时优化的策略, 发现训练过程无法收敛.</p>
                </div>
                <h4 class="anchor-tag" id="264" name="264">2.4.2 光照/环境光遮蔽联合训练结构</h4>
                <div class="p1">
                    <p id="265">在本征图像中, 文献[26]通过共享参数, 对反射率、高光以及光照3种不同属性使用联合优化的方法进行求解.在第2个设计中, 我们尝试了使用类似方法对网络进行改进, 如图3所示:</p>
                </div>
                <div class="area_img" id="266">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902014_266.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 联合训练网络结构示意图" src="Detail/GetImg?filename=images/JFYZ201902014_266.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 联合训练网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902014_266.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Union training network structure</p>

                </div>
                <div class="p1">
                    <p id="267">在解码器后, 额外接入了2个独立输出通道分别为3和1的卷积操作, 预测光照贴图和环境光遮蔽.在训练时, 采用同时优化的方式进行训练.2优化目标均采用了2.3节中提到了混合损失函数, 整体优化目标<i>L</i><sub>total</sub>:</p>
                </div>
                <div class="p1">
                    <p id="268"><i>L</i><sub>total</sub>=<i>αL</i><sub>shading</sub>+ (1-<i>α</i>) <i>L</i><sub>AO</sub>, (5) </p>
                </div>
                <div class="p1">
                    <p id="269">其中, <i>L</i><sub>shading</sub>为光照贴图部分的损失函数;<i>L</i><sub>AO</sub>为环境光遮蔽部分的损失函数;<i>α</i>负责调节因光照和环境光遮蔽输出通道不同带来的数值不平衡问题.针对该问题, 取<i>α</i>=0.25.</p>
                </div>
                <h4 class="anchor-tag" id="270" name="270">2.4.3 环境光遮蔽端到端训练结构</h4>
                <div class="p1">
                    <p id="271">2.4.1节和2.4.2节的2种设计分别代表了串行和并行的方法, 对将该问题视作一个本征图像的扩展问题进行求解.在第3种设计中, 本文直接将该问题作为一个独立问题进行端到端的求解.为此, 我们将编码器-解码器的输出结果直接与一个输出通道为1的卷积层相连, 输出预测的环境光遮蔽估计值.结构如图4所示:</p>
                </div>
                <div class="area_img" id="272">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902014_272.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 端到端训练网络结构示意图" src="Detail/GetImg?filename=images/JFYZ201902014_272.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 端到端训练网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902014_272.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 End-to-end training network structure</p>

                </div>
                <h4 class="anchor-tag" id="273" name="273"><b>2.5</b><b>仿真数据生成</b></h4>
                <div class="p1">
                    <p id="274">卷积神经网络需要大量的标注数据作为训练样本, 从训练数据中抽取相关任务所需的特征.文献[27]中所提出数据集包含了大约50 000个物体的3维模型, 近年来被计算机图形学用于训练数据的生成.考虑到环境光遮蔽与物体的几何特点相关, 而ShapeNet中的大部分物体的几何构成简单, 难以作为训练样本.本文从中选取了6类物体共计2 500余模型作为训练目标, 包含飞机、摩托车、洗手台、耳机、照相机以及吉他.其中, 飞机和吉他的几何构成与环境光遮蔽分布的相似性较高.其余4类具有更加复杂的几何结构和环境光遮蔽强弱分布.对于环境光照, 为了保证光照的随机性和复杂性, 本文选取了计算机图形学研究中一些常见的环境光贴图.其包含了室内室外以及晴天、阴天等各种条件下的环境光照情况, 共计92张.</p>
                </div>
                <div class="p1">
                    <p id="275">本文使用MITSUBA作为物理渲染引擎, 使用路径追踪作为光线追踪渲染算法, 单像素采样次数为512次.渲染时, 对于每个模型, 将其置于世界坐标中心.对于每一张环境光贴图, 随机挑选位于上半球面中半径为2 (所有模型已经归一化) 的一点作为摄像机位置.同时, 使用采样次数512次的环境光遮蔽采样结果作为真实值进行训练和评估.仿真数据共计生成约270 000左右的数据, 对于每个模型, 我们按80/20的比例划分训练/测试数据集.</p>
                </div>
                <h3 id="276" name="276" class="anchor-tag"><b>3</b><b>实验结果与讨论</b></h3>
                <div class="p1">
                    <p id="277">本文在1台PC上用Torch实现了上述的算法.训练使用AdaDelta算法作为梯度更新方法, 对3个模型, 设置初始学习率为0.01, 初始L2正则惩罚项为0.000 1.同时, 设定单次迭代的批样本数量为32, 总迭代为40次全数据周期 (epoch) .对于输入图像, 将其放缩到256×256.训练网络消耗的内存大约为4 GB, 训练使用的显卡为Nvidia TITAN X.</p>
                </div>
                <h4 class="anchor-tag" id="278" name="278"><b>3.1</b><b>模型评估</b></h4>
                <div class="p1">
                    <p id="279">首先对提出的3个模型进行了评估, 从数据集中选择摩托车作为训练数据集, 因为其具有更复杂的几何结构以及精细的局部特征, 能够较好地反映估计结果.</p>
                </div>
                <div class="p1">
                    <p id="280">3种网络的数值对比结果如表1所示, 发现端到端的模型在给定的数据集上得到了最好的训练和测试结果.而另外2个基于本征图像分解扩展的模型对环境光遮蔽回归结果与端到端直接训练相比并无提高, 甚至还略微有下降.一个可能的原因来自于本征分解和环境遮蔽估计的问题属性决定的.本征图像分解具有较高的难度, 因为在反射属性和光照中都同时包含高频和低频属性, 例如深色贴图和阴影的区分.而在环境遮蔽中只包含低频量.所以, 与端到端的训练相比, 以上2种网络结构如果在本征分解精度不高的时候, 很难再从光照图中得到准确的环境遮蔽结果.同时网络层数的增多导致参数增多, 导致网络的训练变得更加困难和不稳定.使得在同样的训练数据集后, 其结果不如端到端的直接训练准确有效.一些具有代表性的视觉对比结果如表2所示.</p>
                </div>
                <div class="area_img" id="281">
                    <p class="img_tit"><b>表1</b><b>3种网络结构数值精度比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1</b><b>Numerical Comparison Among Three Networks</b></p>
                    <p class="img_note"></p>
                    <table id="281" border="1"><tr><td><br />Network</td><td>Cascade</td><td>Union</td><td>End-to-End</td></tr><tr><td><br />RMSE</td><td>0.036</td><td>0.032</td><td>0.031</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="282">
                    <p class="img_tit"><b>表2</b><b>3种网络结构视觉效果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2</b><b>Visual Comparison Among Three Networks</b></p>
                    <p class="img_note"></p>
                    <table id="282" border="1"><tr><td><br />Network</td><td>Case 1</td><td>Case 2</td></tr><tr><td><br />Ground Truth</td><td><img src="0897b2a.jpg" /></td><td><img src="0897b2b.jpg" /></td></tr><tr><td><br />Cascade</td><td><img src="0897b2c.jpg" /></td><td><img src="0897b2d.jpg" /></td></tr><tr><td><br />Union</td><td><img src="0897b2e.jpg" /></td><td><img src="0897b2f.jpg" /></td></tr><tr><td><br />End-to-End</td><td><img src="0897b2g.jpg" /></td><td><img src="0897b2h.jpg" /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="283" name="283"><b>3.2</b><b>仿真数据评估结果</b></h4>
                <div class="p1">
                    <p id="284">随后, 评估算法在仿真数据上的性能.由于之前没有其他基于单张图像的环境光遮蔽算法, 我们选择了文献[11]的方法进行对比.为了满足文献[11]的输入要求, 对于每个测试项, 本文在固定方向光源光照强度的情况下, 在物体中心向着摄像机为中心的半径为2半球面上, 以Halton序列随机生成约60组光源位置, 并采集其在该光照下的渲染结果.由于场景中不存在环境光源, 本文选择其非迭代结果作为比较项 (根据其报告, 无环境光时估计结果更加准确) .</p>
                </div>
                <div class="p1">
                    <p id="285">由于文献[11]对于每个测试样本需要生成大量数据, 受到计算资源限制, 本文在训练数据集中具有代表性的水龙头、摩托车和飞机3个类别中随机选取了约2 000个样本进行了对比实验.</p>
                </div>
                <div class="p1">
                    <p id="286">在数值误差评估时, 选择了RMSE (root mean square error) , LMSE (local mean square error) , DSSIM (district structural similarity) 3种指标对结果的数值精度进行评估.其中, RMSE是均方误差开根结果, 其衡量了估计值与真实值间的像素误差.LMSE由文献[28]提出, 其通过衡量局部均方误差, 能够更好的表达视觉准确度与数值误差的相关性.DSSIM是常见的衡量图像间相似程度的指标.其对比结果如表3所示.显然地, 本文算法在仿真数据上的数值误差远好于测试基准线和文献[11]的方法.值得注意的是, 在本文所用的数据集中, 文献[11]的结果在数值精度上与基准值相比仍有差距, 显示出了该类方法对复杂物体的局限性.</p>
                </div>
                <div class="area_img" id="287">
                    <p class="img_tit"><b>表3</b><b>不同方法数值误差对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3</b><b>Numerical Error among Different Methods</b></p>
                    <p class="img_note"></p>
                    <table id="287" border="1"><tr><td><br />Methods</td><td>RMSE</td><td>LMSE</td><td>DSSIM</td></tr><tr><td><br />Baseline</td><td>0.076</td><td>0.006</td><td>0.392</td></tr><tr><td><br />Hauagge<sup>[11]</sup></td><td>0.119</td><td>0.019</td><td>0.393</td></tr><tr><td><br />Ours</td><td>0.062</td><td>0.004</td><td>0.171</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="288">另外, 本文从视觉上将算法结果与真实值和文献[11]方法结果进行了对比, 其结果如表4所示.表4中4列分别包含了本文算法所用的输入图像、真实值、本文所用算法结果和文献[11]所有算法结果 (其输入图像为前文所述方法生成) .另外, 为了更好地对比局部结果, 本文每个物体环境光遮蔽较为复杂和明显的区域进行了放大比较 (表4中实/虚方框) , 同时展示了误差热度图 (图像下方区域) .</p>
                </div>
                <div class="p1">
                    <p id="289">总的来说, 本文与文献[11]方法均能对物体大致的环境光遮蔽分布和趋势进行估计.在飞机的环境光遮蔽估计中, 2种方法均能对引擎、机身和机翼接缝处的环境光遮蔽进行准确的估计.在水龙头的估计中, 文献[11]如其论文以及文献[12]报告的一样, 在整体的估计上存在模糊和估计值失真等问题.然而, 本文的方法能够对其各处的环境光遮蔽进行较为准确的估计.值得注意的是, 在该场景中, 本文算法的输入图像具有明显明暗变化, 在最终的估计结果中, 算法能够准确地处理光照的变化.摩托车代表了较难的场景, 其存在较多的几何细节和复杂的局部结构.在该类场景中, 我们的方法和文献[11]方法类似, 能够对大致的环境光遮蔽趋势进行估计, 但在具体局部细节的估计上均存在着模糊的问题.但是和文献[10]相比, 本文方法只需要单张图像作为输入.</p>
                </div>
                <div class="area_img" id="290">
                    <p class="img_tit"><b>表4</b><b>不同方法视觉效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4</b><b>Visual Comparison Among Different Methods</b></p>
                    <p class="img_note"></p>
                    <table id="290" border="1"><tr><td><br />Input Image</td><td>Ground Truth</td><td>Our Method</td><td>Hauagge et al<sup>[11]</sup></td></tr><tr><td><br /><img src="0897b4a1.jpg" /></td><td><img src="0897b4a2.jpg" /></td><td><img src="0897b4a3.jpg" /></td><td><img src="0897b4a4.jpg" /></td></tr><tr><td><br /><img src="0897b4b1.jpg" /></td><td><img src="0897b4b2.jpg" /></td><td><img src="0897b4b3.jpg" /></td><td><img src="0897b4b4.jpg" /></td></tr><tr><td><br /><img src="0897b4c1.jpg" /></td><td><img src="0897b4c2.jpg" /></td><td><img src="0897b4c3.jpg" /></td><td><img src="0897b4c4.jpg" /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="291" name="291"><b>3.3</b><b>真实图像</b></h4>
                <div class="p1">
                    <p id="292">最后, 为评估算法在真实场景的图像上的性能, 我们从网上下载了一些常见的物体图像, 通过本文算法对其环境光遮蔽值进行估计, 结果如图5所示.其中, 吉他、摩托车、飞机以及耳机等有类似物体的图像在训练数据集中.而玩偶和椅子2个类别均不包含在训练数据集里.</p>
                </div>
                <div class="p1">
                    <p id="293">从图5中结果可以看出, 即使作用于真实图像中, 本文所训练的网络能够较为合理地估计物体的环境光遮蔽.</p>
                </div>
                <div class="area_img" id="294">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902014_294.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 真实场景结果图" src="Detail/GetImg?filename=images/JFYZ201902014_294.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 真实场景结果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902014_294.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Real cases results</p>

                </div>
                <div class="p1">
                    <p id="295">在真实物体上的实验说明了我们的估计算法有以下2个优点:1) 能够区分阴影和环境光遮蔽的关系.在飞机和玩偶的示例中, 均存在一个较为明显的方向光源.处在阴影中的区域与非阴影区域相比, 亮度较低, 具有与环境光遮蔽较大区域相似的特征.在估计结果中, 算法准确地估计了阴影处的环境光遮蔽结果;2) 能够较为准确地区分暗色系材质与环境光遮蔽的关系.在吉他中心偏右处, 玩偶眼部材质近乎黑色, 与环境光较大时的表现相似.本文的算法能够较为准确地识别该区域正确的环境光遮蔽结果 (玩偶的眼部和吉他下部仍然存在一个较浅的环境光遮蔽估计值) .值得注意的是, 由于漫反射场景在真实世界中几乎不存在, 以上真实物体并不完全符合算法的输入假设.但是, 本文算法在一定程度内仍然能够对环境光遮蔽结果进行合理估计.其中, 耳机存在着一定的高光和反光部分, 本文算法在对该物体进行预测时效果与其他物体较比相对较差.不过仍然能够反映该物体环境光遮蔽的总体趋势.</p>
                </div>
                <h4 class="anchor-tag" id="296" name="296"><b>3.4</b><b>失败案例</b></h4>
                <div class="p1">
                    <p id="297">本文方法仍存在一定的局限性, 现就2个典型的失败案例进行讨论, 如图6所示:</p>
                </div>
                <div class="area_img" id="298">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902014_298.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 失败案例" src="Detail/GetImg?filename=images/JFYZ201902014_298.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 失败案例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902014_298.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Failure cases</p>

                </div>
                <div class="p1">
                    <p id="299">在飞机的例子中, 网络将机翼和机头黑色贴图部分误估计为环境光遮蔽较大的区域.在本征图像中, 对于阴影区域和贴图较暗区域的区分一直是一个较为困难的问题, 在环境光遮蔽的估计中也存在相似的问题.在相机的例子中, 镜头以及周边存在环境光遮蔽估计失真的情况.这是由于镜头周围的镜面反射明显, 使得本文非漫反射的前提条件被破坏, 使得估计出现较大误差.</p>
                </div>
                <h3 id="300" name="300" class="anchor-tag"><b>4</b><b>结</b><b>语</b></h3>
                <div class="p1">
                    <p id="301">本文提出了一种基于卷积神经网络的从原始图像到环境光遮蔽估计算法.算法仅要求单帧图像作为输入, 同时对光照没有特别假设.本文提出并评估了3种不同的卷积神经网络设计, 其中端对端的网络取得了最好的效果.本文对算法在仿真图像和真实图像上的性能进行了评估, 实验结果表明, 本文方法具有较好的鲁棒性.与已有方法相比, 本文提出的算法结果精度更高, 极大地减轻了传统基于图像的环境光遮蔽估计对输入图像数量和光照布置均有较高的限制, 使得在单张图像估计环境光遮蔽成为可能.</p>
                </div>
                <div class="p1">
                    <p id="302">本文的算法假设场景为漫反射, 在未来的工作中, 我们将对非漫反射场景的环境遮蔽问题进行研究.另外, 下一步通过将本文的方法与其他以环境光遮蔽作为中间步骤的方法进行结合, 使得该类其能够在更加通用的条件下进行求解并应用于不同的视觉应用.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="343">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-space horizon-based ambient occlusion">

                                <b>[1]</b>Bavoil L, Sainz M, Dimitrov R.Image-space horizon-based ambient occlusion[C]Proc of the 35th ACM SIGGRAPH.New York:ACM, 2008[2017-11-28].https:dl.acm.org/citation.cfm/id=1401061
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-layer dual-resolution screen-space ambient occlusion">

                                <b>[2]</b>Bavoil L, Sainz M.Multi-layer dual-resolution screen-space ambient occlusion[C]Proc of the 36th ACM SIGGRAPH.New York:ACM, 2009[2017-11-28].https:dl.acm.org/citation.cfm/id=1598035
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Alchemy Screen-Space Ambient Obscurance Algorithm">

                                <b>[3]</b>McGuire M, Osman B, Bukowski M, et al.The alchemy screen-space ambient obscurance algorithm[C]Proc of the38th ACM SIGGRAPH Symp on High Performance Graphics.New York:ACM, 2011:25-32
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201108002&amp;v=Mjk3NTJGeTduVzd6Tkx6N0JhTEc0SDlETXA0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Li Wenyao, Ren Zhong.Self-adaptive multilayer screen space ambient occlusion[J].Journal of Computer-Aided Design&amp;Computer Graphics, 2011, 23 (8) :1294-1303 (in Chinese) (李文耀, 任重.自适应的多层屏幕空间环境光遮蔽[J].计算机辅助设计与图形学学报, 2011, 23 (8) :1294-1303) 
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape-from-shading on a cloudy day">

                                <b>[5]</b>Langer M S, Zucker S W.Shape-from-shading on a cloudy day[J].Journal of the Optical Society of America:A, 1994, 11 (2) :467-478
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-local approach to shape from ambient shading">

                                <b>[6]</b>Prados E, Jindal N, Soatto S.A non-local approach to shape from ambient shading[C]Proc of Scale Space and Variational Methods in Computer Vision.Berlin:Springer, 2011:696-708
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-quality shape from multi-view stereo and shading under general illumination">

                                <b>[7]</b>Wu Chenglei, Wilburn B, Matsushita Y, et al.High-quality shape from multi-view stereo and shading under general illumination[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:969-976
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverse rendering of faces on a cloudy day">

                                <b>[8]</b>Aldrian O, Smith W A P.Inverse rendering of faces on a cloudy day[C]Proc of the 12th European Conf on Computer Vision.Berlin:Springer, 2012:201-214
                            </a>
                        </p>
                        <p id="359">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved reconstruction of deforming surfaces by cancelling ambient occlusion">

                                <b>[9]</b>Beeler T, Bradley D, Zimmer H, et al.Improved reconstruction of deforming surfaces by cancelling ambient occlusion[C]Proc of the 12th European Conf on Computer Vision.Berlin:Springer, 2012:30-43
                            </a>
                        </p>
                        <p id="361">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007838&amp;v=MDc3MjZxUVRNbndaZVp1SHlqbVViN0lKbHdjYVJjPU5pZklZN0s3SHRqTnI0OUZaT3NJQkg4eG9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Laffont P Y, Bousseau A, Paris S, et al.Coherent intrinsic images from photo collections[J].ACM Transactions on Graphics, 2012, 31 (6) :202-213
                            </a>
                        </p>
                        <p id="363">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photometric ambient occlusion">

                                <b>[11]</b>Hauagge D, Wehrwein S, Bala K, et al.Photometric ambient occlusion[C]Proc of the 31st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2013:2515-2522
                            </a>
                        </p>
                        <p id="365">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ambient occlusion via compressive visibility estimation">

                                <b>[12]</b>Yang Wei, Ji Yu, Lin Haiting, et al.Ambient occlusion via compressive visibility estimation[C]Proc of the 33rd IEEEConf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3882-3889
                            </a>
                        </p>
                        <p id="367">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photometric ambient occlusion from sparsely sampled illuminations">

                                <b>[13]</b>Ma Yuwei, Shang Yafei, Wan Liang, et al.Photometric ambient occlusion from sparsely sampled illuminations[C]Proc of the 17th IEEE Int Conf on Multimedia&amp;Expo Workshops (ICMEW) .Piscataway, NJ:IEEE, 2016:1-6
                            </a>
                        </p>
                        <p id="369">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recovering intrinsic scene characteristics from images">

                                <b>[14]</b>Barrow H G, Tenenbaum J M.Recovering intrinsic scene characteristics from images[J].Computer Vision Systems, 1978, 2:3-26
                            </a>
                        </p>
                        <p id="371">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lightness and retinex theory">

                                <b>[15]</b>Land E H, McCann J J.Lightness and Retinex theory[J].Journal of the Optical Society of America, 1971, 61 (1) :1-11
                            </a>
                        </p>
                        <p id="373">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recovering Intrinsic Images from a Single Image">

                                <b>[16]</b>Tappen M F, Freeman W T, Adelson E H.Recovering intrinsic images from a single image[J].IEEE Transactions on Pattern Analysis&amp;Machine Intelligence, 2005, 27 (9) :1459-1472
                            </a>
                        </p>
                        <p id="375">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intrinsic images using optimization">

                                <b>[17]</b>Shen Jianbing, Yang Xiaoshan, Jia Yunde, et al.Intrinsic images using optimization[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:3481-3487
                            </a>
                        </p>
                        <p id="377">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape, illumination, and reflectance from shading">

                                <b>[18]</b>Barron J T, Malik J.Shape, illumination, and reflectance from shading[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (8) :1670-1687
                            </a>
                        </p>
                        <p id="379">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                Ren Shaoqing, He Kaiming, Girshick R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis&amp;Machine Intelligence, 2017, 39 (6) :1137-1149
                            </a>
                        </p>
                        <p id="381">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[20]</b>Jonathon L, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]Proc of the 33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3431-3440
                            </a>
                        </p>
                        <p id="383">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[21]</b>He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Deep residual learning for image recognition[C]Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:770-778
                            </a>
                        </p>
                        <p id="385">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201309002&amp;v=MDk4NDZHNEg5TE1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blc3ek5MeXZTZEw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>Yu Kai, Jia Lei, Chen Yuqiang, et al.Deep learning:Yesterday, today, and tomorrow[J].Journal of Computer Research and Development, 2013, 50 (9) :1799-1804 (in Chinese) (余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804) 
                            </a>
                        </p>
                        <p id="387">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning lightness from human judgement on relative reflectance">

                                <b>[23]</b>Narihira T, Maire M, Yu S X.Learning lightness from human judgement on relative reflectance[C]Proc of the33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:2965-2973
                            </a>
                        </p>
                        <p id="389">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning datadriven reflectance priors for intrinsic image decomposition">

                                <b>[24]</b>Zhou Tinghui, Krahenbuhl P, Efros A A.Learning datadriven reflectance priors for intrinsic image decomposition[C]Proc of the 33rd IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE.2015:3469-3477
                            </a>
                        </p>
                        <p id="391">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Direct intrinsics:Learning albedo-shading decomposition by convolutional regression">

                                <b>[25]</b>Narihira T, Maire M, Yu S X.Direct intrinsics:Learning albedo-shading decomposition by convolutional regression[C]Proc of the 33rd IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:2992-3001
                            </a>
                        </p>
                        <p id="393">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning nonLambertian object intrinsics across ShapeNet categories">

                                <b>[26]</b>Shi Jian, Dong Yue, Su Hao, et al.Learning nonLambertian object intrinsics across ShapeNet categories[C]Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:5844-5853
                            </a>
                        </p>
                        <p id="395">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ShapeNet:an information-rich 3D model repository">

                                <b>[27]</b>Chang A X, Funkhouser T, Guibas L, et al.ShapeNet:An information-rich 3D model repository[J/OL].2015[2017-01-20].https:arxiv.org/pdf/1512.03012.pdf
                            </a>
                        </p>
                        <p id="397">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ground truth dataset and baseline evaluations for intrinsic image algorithms">

                                <b>[28]</b>Grosse R, Johnson M K, Adelson E H, et al.Ground truth dataset and baseline evaluations for intrinsic image algorithms[C]Proc of the 27th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2009:2335-2342
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902014" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902014&amp;v=MTU0MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blc3ek5MeXZTZExHNEg5ak1yWTlFWUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
