

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127125496395000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201910003%26RESULT%3d1%26SIGN%3dm%252bZDThiwHH5pyMPG1YKVyRV2%252b8A%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201910003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201910003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201910003&amp;v=MjQzOTMzenFxQnRHRnJDVVJMT2VaZVJzRnl6Z1VMekFMeXZTZExHNEg5ak5yNDlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#311" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#316" data-title="&lt;b&gt;2 机器学习概述&lt;/b&gt; "><b>2 机器学习概述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#317" data-title="&lt;b&gt;2.1 机器学习系统&lt;/b&gt;"><b>2.1 机器学习系统</b></a></li>
                                                <li><a href="#324" data-title="&lt;b&gt;2.2 安全威胁&lt;/b&gt;"><b>2.2 安全威胁</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#330" data-title="&lt;b&gt;3 隐  私&lt;/b&gt; "><b>3 隐  私</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#332" data-title="&lt;b&gt;3.1 隐私问题简介&lt;/b&gt;"><b>3.1 隐私问题简介</b></a></li>
                                                <li><a href="#337" data-title="&lt;b&gt;3.2 隐私问题研究工作&lt;/b&gt;"><b>3.2 隐私问题研究工作</b></a></li>
                                                <li><a href="#348" data-title="&lt;b&gt;3.3 攻击方法&lt;/b&gt;"><b>3.3 攻击方法</b></a></li>
                                                <li><a href="#368" data-title="&lt;b&gt;3.4 防御方法&lt;/b&gt;"><b>3.4 防御方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#389" data-title="&lt;b&gt;4 安  全&lt;/b&gt; "><b>4 安  全</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#390" data-title="&lt;b&gt;4.1 安全问题简介&lt;/b&gt;"><b>4.1 安全问题简介</b></a></li>
                                                <li><a href="#393" data-title="&lt;b&gt;4.2 安全问题研究工作&lt;/b&gt;"><b>4.2 安全问题研究工作</b></a></li>
                                                <li><a href="#397" data-title="&lt;b&gt;4.3 投毒攻击&lt;/b&gt;"><b>4.3 投毒攻击</b></a></li>
                                                <li><a href="#405" data-title="&lt;b&gt;4.4 对抗攻击&lt;/b&gt;"><b>4.4 对抗攻击</b></a></li>
                                                <li><a href="#455" data-title="&lt;b&gt;4.5 投毒防御&lt;/b&gt;"><b>4.5 投毒防御</b></a></li>
                                                <li><a href="#459" data-title="&lt;b&gt;4.6 对抗防御&lt;/b&gt;"><b>4.6 对抗防御</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#468" data-title="&lt;b&gt;5 总  结&lt;/b&gt; "><b>5 总  结</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#703" data-title="1) 提高数据质量,增强数据安全.">1) 提高数据质量,增强数据安全.</a></li>
                                                <li><a href="#704" data-title="2) 保证个人数据隐私,防止模型滥用隐私信息.">2) 保证个人数据隐私,防止模型滥用隐私信息.</a></li>
                                                <li><a href="#705" data-title="3) 通过模型解释性的研究解决模型安全性滞后性现状.">3) 通过模型解释性的研究解决模型安全性滞后性现状.</a></li>
                                                <li><a href="#706" data-title="4) 加强对人工智能在实际应用中的安全问题研究.">4) 加强对人工智能在实际应用中的安全问题研究.</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#302" data-title="图1 近年来相关研究数量">图1 近年来相关研究数量</a></li>
                                                <li><a href="#305" data-title="图2 不同攻击类型的相关研究数量">图2 不同攻击类型的相关研究数量</a></li>
                                                <li><a href="#318" data-title="图3 机器学习系统攻击概述">图3 机器学习系统攻击概述</a></li>
                                                <li><a href="#323" data-title="&lt;b&gt;表1 机器学习系统的符号化&lt;/b&gt;"><b>表1 机器学习系统的符号化</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="707">


                                    <a id="bibliography_1" title="Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[OL].[2018-05-28].http://arxiv.org/abs/1312.6199" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks[OL]">
                                        <b>[1]</b>
                                        Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[OL].[2018-05-28].http://arxiv.org/abs/1312.6199
                                    </a>
                                </li>
                                <li id="709">


                                    <a id="bibliography_2" title="Athalye A,Engstrom L,Ilyas A,et al.Synthesizing robust adversarial examples[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM,2018:284- 293" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synthesizing robust adversarial examples">
                                        <b>[2]</b>
                                        Athalye A,Engstrom L,Ilyas A,et al.Synthesizing robust adversarial examples[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM,2018:284- 293
                                    </a>
                                </li>
                                <li id="711">


                                    <a id="bibliography_3" title="Nasr M,Shokri R,Houmansadr A.Machine learning with membership privacy using adversarial regularization[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:634- 646" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning with membership privacy using adversarial regularization">
                                        <b>[3]</b>
                                        Nasr M,Shokri R,Houmansadr A.Machine learning with membership privacy using adversarial regularization[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:634- 646
                                    </a>
                                </li>
                                <li id="713">


                                    <a id="bibliography_4" title="Phong L T,Aono Y,Hayashi T,et al.Privacy-preserving deep learning via additively homomorphic encryption[J].IEEE Transactions on Information Forensics and Security,2018,13(5):1333- 1345" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving deep learning via additively homomorphic encryption">
                                        <b>[4]</b>
                                        Phong L T,Aono Y,Hayashi T,et al.Privacy-preserving deep learning via additively homomorphic encryption[J].IEEE Transactions on Information Forensics and Security,2018,13(5):1333- 1345
                                    </a>
                                </li>
                                <li id="715">


                                    <a id="bibliography_5" title="Zhang Tianwei,He Zecheng,Lee R B.Privacy-preserving machine learning through data obfuscation[OL].[2018-05-28].http://arxiv.org/abs/1807.01860" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving machine learning through data obfuscation[OL]">
                                        <b>[5]</b>
                                        Zhang Tianwei,He Zecheng,Lee R B.Privacy-preserving machine learning through data obfuscation[OL].[2018-05-28].http://arxiv.org/abs/1807.01860
                                    </a>
                                </li>
                                <li id="717">


                                    <a id="bibliography_6" title="Abadi M,Chu A,Goodfellow I,et al.Deep learning with differential privacy[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2016:308- 318" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning with differential privacy">
                                        <b>[6]</b>
                                        Abadi M,Chu A,Goodfellow I,et al.Deep learning with differential privacy[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2016:308- 318
                                    </a>
                                </li>
                                <li id="719">


                                    <a id="bibliography_7" title="Juuti M,Szyller S,Dmitrenko A,et al.PRADA:Protecting against DNN model stealing attacks[OL].[2018-05-28].http://arxiv.org/abs/1805.02628" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PRADA:Protecting against DNN model stealing attacks[OL]">
                                        <b>[7]</b>
                                        Juuti M,Szyller S,Dmitrenko A,et al.PRADA:Protecting against DNN model stealing attacks[OL].[2018-05-28].http://arxiv.org/abs/1805.02628
                                    </a>
                                </li>
                                <li id="721">


                                    <a id="bibliography_8" title="Lee T,Edwards B,Molloy I,et al.Defending against machine learning model stealing attacks using deceptive perturbations[OL].[2018-05-28].http://arxiv.org/abs/1806.00054" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Defending against machine learning model stealing attacks using deceptive perturbations[OL]">
                                        <b>[8]</b>
                                        Lee T,Edwards B,Molloy I,et al.Defending against machine learning model stealing attacks using deceptive perturbations[OL].[2018-05-28].http://arxiv.org/abs/1806.00054
                                    </a>
                                </li>
                                <li id="723">


                                    <a id="bibliography_9" title="Hua Weizhe,Zhang Zhiru,Suh G.Reverse engineering convolutional neural networks through side-channel information leaks[C] //Proc of the 55th IEEE Design Automation Conference.Piscataway,NJ:IEEE,2018:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reverse engineering convolutional neural networks through side-channel information leaks">
                                        <b>[9]</b>
                                        Hua Weizhe,Zhang Zhiru,Suh G.Reverse engineering convolutional neural networks through side-channel information leaks[C] //Proc of the 55th IEEE Design Automation Conference.Piscataway,NJ:IEEE,2018:1- 6
                                    </a>
                                </li>
                                <li id="725">


                                    <a id="bibliography_10" title="Kesarwani M,Mukhoty B,Arya V,et al.Model extraction warning in MLaaS paradigm[OL].[2018-05-28].http://arxiv.org/abs/1711.07221" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model extraction warning in MLaaS paradigm[OL]">
                                        <b>[10]</b>
                                        Kesarwani M,Mukhoty B,Arya V,et al.Model extraction warning in MLaaS paradigm[OL].[2018-05-28].http://arxiv.org/abs/1711.07221
                                    </a>
                                </li>
                                <li id="727">


                                    <a id="bibliography_11" title="Barreno M,Nelson B,Joseph A D,et al.The security of machine learning[J].Machine Learning,2010,81(2):121- 148" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003762447&amp;v=MDM1NDg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ3JsVjdyTEkxWT1OajdCYXJPNEh0SFBxSWxIWU84SVkzazV6QmRo&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Barreno M,Nelson B,Joseph A D,et al.The security of machine learning[J].Machine Learning,2010,81(2):121- 148
                                    </a>
                                </li>
                                <li id="729">


                                    <a id="bibliography_12" title="Amodei D,Olah C,Steinhardt J,et al.Concrete problems in AI safety[OL].[2018-05-28].http://arxiv.org/abs/1606.06565" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Concrete problems in AI safety[OL]">
                                        <b>[12]</b>
                                        Amodei D,Olah C,Steinhardt J,et al.Concrete problems in AI safety[OL].[2018-05-28].http://arxiv.org/abs/1606.06565
                                    </a>
                                </li>
                                <li id="731">


                                    <a id="bibliography_13" title="Papernot N,McDaniel P,Sinha A,et al.Towards the science of security and privacy in machine learning[OL].[2018-05-28].http://arxiv.org/abs/1611.03814" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards the science of security and privacy in machine learning[OL]">
                                        <b>[13]</b>
                                        Papernot N,McDaniel P,Sinha A,et al.Towards the science of security and privacy in machine learning[OL].[2018-05-28].http://arxiv.org/abs/1611.03814
                                    </a>
                                </li>
                                <li id="733">


                                    <a id="bibliography_14" title="Bae H,Jang J,Jung D,et al.Security and privacy issues in deep learning[OL].[2018-05-28].http://arxiv.org/abs/1807.11655" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Security and privacy issues in deep learning[OL]">
                                        <b>[14]</b>
                                        Bae H,Jang J,Jung D,et al.Security and privacy issues in deep learning[OL].[2018-05-28].http://arxiv.org/abs/1807.11655
                                    </a>
                                </li>
                                <li id="735">


                                    <a id="bibliography_15" title="Papernot N,McDaniel P,Sinha A,et al.SoK:Security and privacy in machine learning[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2018:399- 414" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SoK:Security and privacy in machine learning">
                                        <b>[15]</b>
                                        Papernot N,McDaniel P,Sinha A,et al.SoK:Security and privacy in machine learning[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2018:399- 414
                                    </a>
                                </li>
                                <li id="737">


                                    <a id="bibliography_16" title="Liu Qiang,Li Pan,Zhao Wentao,et al.A survey on security threats and defensive techniques of machine learning:A data driven view[J].IEEE Access,2018,(6):12103- 12117" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey on security threats and defensive techniques of machine learning:a data driven view">
                                        <b>[16]</b>
                                        Liu Qiang,Li Pan,Zhao Wentao,et al.A survey on security threats and defensive techniques of machine learning:A data driven view[J].IEEE Access,2018,(6):12103- 12117
                                    </a>
                                </li>
                                <li id="739">


                                    <a id="bibliography_17" title="Akhtar N,Mian A.Threat of adversarial attacks on deep learning in computer vision:A survey[J].IEEE Access,2018,(6):14410- 14430" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Threat of adversarial attacks on deep learning in computer vision:a survey">
                                        <b>[17]</b>
                                        Akhtar N,Mian A.Threat of adversarial attacks on deep learning in computer vision:A survey[J].IEEE Access,2018,(6):14410- 14430
                                    </a>
                                </li>
                                <li id="741">


                                    <a id="bibliography_18" title="Ling Xiang,Ji Shouling,Zou Jiaxu,et al.DeepSec:A uniform platform for security analysis of deep learning model[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2019:673- 690" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepSec:A uniform platform for security analysis of deep learning model">
                                        <b>[18]</b>
                                        Ling Xiang,Ji Shouling,Zou Jiaxu,et al.DeepSec:A uniform platform for security analysis of deep learning model[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2019:673- 690
                                    </a>
                                </li>
                                <li id="743">


                                    <a id="bibliography_19" title="Alfeld S,Zhu Xiaojin,Barford P.Data poisoning attacks against autoregressive models[C] //Proc of the 30th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2016:1452- 1458" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data poisoning attacks against autoregressive models">
                                        <b>[19]</b>
                                        Alfeld S,Zhu Xiaojin,Barford P.Data poisoning attacks against autoregressive models[C] //Proc of the 30th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2016:1452- 1458
                                    </a>
                                </li>
                                <li id="745">


                                    <a id="bibliography_20" title="Hitaj B,Ateniese G,Perez-Cruz F.Deep models under the GAN:Information leakage from collaborative deep learning[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:603- 618" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep models under the GAN:Information leakage from collaborative deep learning">
                                        <b>[20]</b>
                                        Hitaj B,Ateniese G,Perez-Cruz F.Deep models under the GAN:Information leakage from collaborative deep learning[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:603- 618
                                    </a>
                                </li>
                                <li id="747">


                                    <a id="bibliography_21" title="Song Congzheng,Ristenpart T,Shmatikov V.Machine learning models that remember too much[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:587- 601" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning models that remember too much">
                                        <b>[21]</b>
                                        Song Congzheng,Ristenpart T,Shmatikov V.Machine learning models that remember too much[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:587- 601
                                    </a>
                                </li>
                                <li id="749">


                                    <a id="bibliography_22" title="Veale M,Binns R,Edwards L.Algorithms that remember:Model inversion attacks and data protection law[OL].[2018-05-28].http://arxiv.org/abs/1807.04644" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Algorithms that remember:Model inversion attacks and data protection law[OL]">
                                        <b>[22]</b>
                                        Veale M,Binns R,Edwards L.Algorithms that remember:Model inversion attacks and data protection law[OL].[2018-05-28].http://arxiv.org/abs/1807.04644
                                    </a>
                                </li>
                                <li id="751">


                                    <a id="bibliography_23" title="Buolamwini J,Gebru T.Gender shades:Intersectional accuracy disparities in commercial gender classification[C] //Proc of the Conf on Fairness,Accountability and Transparency.New York:ACM,2018:77- 91" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gender Shades:Intersectional Accuracy Disparities in Commercial Gender Classification">
                                        <b>[23]</b>
                                        Buolamwini J,Gebru T.Gender shades:Intersectional accuracy disparities in commercial gender classification[C] //Proc of the Conf on Fairness,Accountability and Transparency.New York:ACM,2018:77- 91
                                    </a>
                                </li>
                                <li id="753">


                                    <a id="bibliography_24" title="Wang Di,Ye Minwei,Xu Jinhui.Differentially private empirical risk minimization revisited:Faster and more general[C] //Proc of the Annual Conf on Neural Information Processing Systems.New York:NIPS,2017:2719- 2728" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Differentially private empirical risk minimization revisited:Faster and more general">
                                        <b>[24]</b>
                                        Wang Di,Ye Minwei,Xu Jinhui.Differentially private empirical risk minimization revisited:Faster and more general[C] //Proc of the Annual Conf on Neural Information Processing Systems.New York:NIPS,2017:2719- 2728
                                    </a>
                                </li>
                                <li id="755">


                                    <a id="bibliography_25" title="Bachrach R,Dowlin M,Laine K,et al.CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy[C] //Proc of the 33nd Int Conf on Machine Learning.New York:ACM,2016:201- 210" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy">
                                        <b>[25]</b>
                                        Bachrach R,Dowlin M,Laine K,et al.CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy[C] //Proc of the 33nd Int Conf on Machine Learning.New York:ACM,2016:201- 210
                                    </a>
                                </li>
                                <li id="757">


                                    <a id="bibliography_26" title="Jiang Xiaoqian,Kim M,Lauter K,et al.Secure outsourced matrix computation and application to neural networks[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:1209- 1222" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Secure outsourced matrix computation and application to neural networks">
                                        <b>[26]</b>
                                        Jiang Xiaoqian,Kim M,Lauter K,et al.Secure outsourced matrix computation and application to neural networks[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:1209- 1222
                                    </a>
                                </li>
                                <li id="759">


                                    <a id="bibliography_27" title="Mohassel P,Zhang Yupeng.SecureML:A system for scalable privacy-preserving machine learning[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:19- 38" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SecureML:A system for scalable privacy-preserving machine learning">
                                        <b>[27]</b>
                                        Mohassel P,Zhang Yupeng.SecureML:A system for scalable privacy-preserving machine learning[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:19- 38
                                    </a>
                                </li>
                                <li id="761">


                                    <a id="bibliography_28" title="Wagh S,Gupta D,Chandran N.SecureNN:Efficient and private neural network training[OL].[2018-05-28].https://eprint.iacr.org/2018/442" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SecureNN:Efficient and private neural network training[OL]">
                                        <b>[28]</b>
                                        Wagh S,Gupta D,Chandran N.SecureNN:Efficient and private neural network training[OL].[2018-05-28].https://eprint.iacr.org/2018/442
                                    </a>
                                </li>
                                <li id="763">


                                    <a id="bibliography_29" title="Wang Binghui,Gong Zhenqiang.Stealing hyperparameters in machine learning[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2018:36- 52" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stealing hyperparameters in machine learning">
                                        <b>[29]</b>
                                        Wang Binghui,Gong Zhenqiang.Stealing hyperparameters in machine learning[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2018:36- 52
                                    </a>
                                </li>
                                <li id="765">


                                    <a id="bibliography_30" title="Proserpio D,Goldberg S,Mcsherry F.Calibrating data to sensitivity in private data analysis[J].Proceedings of the VLDB Endowment,2014,7(8):637- 648" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Calibrating data to sensitivity in private data analysis">
                                        <b>[30]</b>
                                        Proserpio D,Goldberg S,Mcsherry F.Calibrating data to sensitivity in private data analysis[J].Proceedings of the VLDB Endowment,2014,7(8):637- 648
                                    </a>
                                </li>
                                <li id="767">


                                    <a id="bibliography_31" title="Dwork C,Kenthapadi K,Mcsherry F,et al.Our data,ourselves:Privacy via distributed noise generation[C] //Proc of the 25th Annual Int Conf on the Theory and Applications of Cryptographic Techniques.Berlin:Springer,2006:486- 503" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Our data,ourselves:Privacy via distributed noise generation">
                                        <b>[31]</b>
                                        Dwork C,Kenthapadi K,Mcsherry F,et al.Our data,ourselves:Privacy via distributed noise generation[C] //Proc of the 25th Annual Int Conf on the Theory and Applications of Cryptographic Techniques.Berlin:Springer,2006:486- 503
                                    </a>
                                </li>
                                <li id="769">


                                    <a id="bibliography_32" title="Tram&#232;r F,Zhang F,Juels A,et al.Stealing machine learning models via prediction APIs[C] //Proc of the 25th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2016:601- 618" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stealing machine learning models via prediction APIs">
                                        <b>[32]</b>
                                        Tram&#232;r F,Zhang F,Juels A,et al.Stealing machine learning models via prediction APIs[C] //Proc of the 25th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2016:601- 618
                                    </a>
                                </li>
                                <li id="771">


                                    <a id="bibliography_33" title="Baluja S,Fischer I.Learning to attack:Adversarial transformation networks[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:2687- 2695" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to attack:Adversarial transformation networks">
                                        <b>[33]</b>
                                        Baluja S,Fischer I.Learning to attack:Adversarial transformation networks[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:2687- 2695
                                    </a>
                                </li>
                                <li id="773">


                                    <a id="bibliography_34" title="Papernot N,Mcdaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 2017 ACM on Asia Conf on Computer and Communications Security.New York:ACM,2017:506- 519" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Practical black-box attacks against machine learning">
                                        <b>[34]</b>
                                        Papernot N,Mcdaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 2017 ACM on Asia Conf on Computer and Communications Security.New York:ACM,2017:506- 519
                                    </a>
                                </li>
                                <li id="775">


                                    <a id="bibliography_35" title="Papernot N,Mcdaniel P,Goodfellow I.Transferability in machine learning:From phenomena to black-box attacks using adversarial samples[OL].[2018-05-28].http://arxiv.org/abs/1605.07277" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transferability in machine learning:From phenomena to black-box attacks using adversarial samples[OL]">
                                        <b>[35]</b>
                                        Papernot N,Mcdaniel P,Goodfellow I.Transferability in machine learning:From phenomena to black-box attacks using adversarial samples[OL].[2018-05-28].http://arxiv.org/abs/1605.07277
                                    </a>
                                </li>
                                <li id="777">


                                    <a id="bibliography_36" title="Truex S,Liu Ling,Gursoy M E,et al.Towards demystifying membership inference attacks[OL].[2018-05-28].http://arxiv.org/abs/1807.09173" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards demystifying membership inference attacks[OL]">
                                        <b>[36]</b>
                                        Truex S,Liu Ling,Gursoy M E,et al.Towards demystifying membership inference attacks[OL].[2018-05-28].http://arxiv.org/abs/1807.09173
                                    </a>
                                </li>
                                <li id="779">


                                    <a id="bibliography_37" title="Shokri R,Stronati M,Song Congzheng,et al.Membership inference attacks against machine learning models[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:3- 18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Membership inference attacks against machine learning models">
                                        <b>[37]</b>
                                        Shokri R,Stronati M,Song Congzheng,et al.Membership inference attacks against machine learning models[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:3- 18
                                    </a>
                                </li>
                                <li id="781">


                                    <a id="bibliography_38" title="Salem A,Zhang Yang,Humbert M,et al.ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[OL].[2018-05-28].http://arxiv.org/abs/1806.01246" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[OL]">
                                        <b>[38]</b>
                                        Salem A,Zhang Yang,Humbert M,et al.ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[OL].[2018-05-28].http://arxiv.org/abs/1806.01246
                                    </a>
                                </li>
                                <li id="783">


                                    <a id="bibliography_39" title="Pyrgelis A,Troncoso C,De Cristofaro E.Knock knock,who&#39;s there?membership inference on aggregate location data[C/OL] //Proc of the 25th Network and Distributed System Security Symp.Reston VA:The Internet Society,2018 [2019-06-11].https://arxiv.org/pdf/1708.06145.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Knock knock,who&amp;#39;&amp;#39;s there?membership inference on aggregate location data[C/OL]">
                                        <b>[39]</b>
                                        Pyrgelis A,Troncoso C,De Cristofaro E.Knock knock,who&#39;s there?membership inference on aggregate location data[C/OL] //Proc of the 25th Network and Distributed System Security Symp.Reston VA:The Internet Society,2018 [2019-06-11].https://arxiv.org/pdf/1708.06145.pdf
                                    </a>
                                </li>
                                <li id="785">


                                    <a id="bibliography_40" title="Fredrikson M,Jha S,Ristenpart T.Model inversion attacks that exploit confidence information and basic countermeasures[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2015:1322- 1333" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model inversion attacks that exploit confidence information and basic countermeasures">
                                        <b>[40]</b>
                                        Fredrikson M,Jha S,Ristenpart T.Model inversion attacks that exploit confidence information and basic countermeasures[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2015:1322- 1333
                                    </a>
                                </li>
                                <li id="787">


                                    <a id="bibliography_41" title="Long Yunhui,Bindschaedler V,Wang Lei,et al.Understanding membership inferences on well-generalized learning models[OL].[2018-05-28].http://arxiv.org/abs/1802.04889" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding membership inferences on well-generalized learning models[OL]">
                                        <b>[41]</b>
                                        Long Yunhui,Bindschaedler V,Wang Lei,et al.Understanding membership inferences on well-generalized learning models[OL].[2018-05-28].http://arxiv.org/abs/1802.04889
                                    </a>
                                </li>
                                <li id="789">


                                    <a id="bibliography_42" title="Liu Kin,Li Bo,Gao Jie.Generative model:Membership attack,generalization and diversity[OL].[2018-05-28].http://arxiv.org/abs/1805.09898" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative model:Membership attack,generalization and diversity[OL]">
                                        <b>[42]</b>
                                        Liu Kin,Li Bo,Gao Jie.Generative model:Membership attack,generalization and diversity[OL].[2018-05-28].http://arxiv.org/abs/1805.09898
                                    </a>
                                </li>
                                <li id="791">


                                    <a id="bibliography_43" title="Hayes J,Melis L,Danezis G,et al.LOGAN:Evaluating privacy leakage of generative models using generative adversarial networks[OL].[2018-05-28].http://arxiv.org/abs/1705.07663" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LOGAN:Evaluating privacy leakage of generative models using generative adversarial networks[OL]">
                                        <b>[43]</b>
                                        Hayes J,Melis L,Danezis G,et al.LOGAN:Evaluating privacy leakage of generative models using generative adversarial networks[OL].[2018-05-28].http://arxiv.org/abs/1705.07663
                                    </a>
                                </li>
                                <li id="793">


                                    <a id="bibliography_44" title="Ateniese G,Mancini L V,Spognardi A,et al.Hacking smart machines with smarter ones:How to extract meaningful data from machine learning classifiers[J].International Journal of Security &amp;amp; Networks,2015,10(3):137- 150" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCW0E2654789EA8F4F43B2D00D65E2B815A&amp;v=MTk2MjlIK1dxaGRBZThDY1JML3VDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOcGh3TDI4dzZBPU5pZkllYlBOSE5mSnE0aE5iWjUrQkFvOXVSSVFtRDBKUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[44]</b>
                                        Ateniese G,Mancini L V,Spognardi A,et al.Hacking smart machines with smarter ones:How to extract meaningful data from machine learning classifiers[J].International Journal of Security &amp;amp; Networks,2015,10(3):137- 150
                                    </a>
                                </li>
                                <li id="795">


                                    <a id="bibliography_45" title="Ganju K,Wang Qi,Yang Wei,et al.Property inference attacks on fully connected neural networks using permutation invariant representations[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:619- 633" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Property inference attacks on fully connected neural networks using permutation invariant representations">
                                        <b>[45]</b>
                                        Ganju K,Wang Qi,Yang Wei,et al.Property inference attacks on fully connected neural networks using permutation invariant representations[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:619- 633
                                    </a>
                                </li>
                                <li id="797">


                                    <a id="bibliography_46" title="Melis L,Song Congzheng,Cristofaro E,et al.Inference attacks against collaborative learning[OL].[2018-05-28].http://arxiv.org/abs/1805.04049" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inference attacks against collaborative learning[OL]">
                                        <b>[46]</b>
                                        Melis L,Song Congzheng,Cristofaro E,et al.Inference attacks against collaborative learning[OL].[2018-05-28].http://arxiv.org/abs/1805.04049
                                    </a>
                                </li>
                                <li id="799">


                                    <a id="bibliography_47" title="Chaudhuri K,Monteleoni C.Privacy-preserving logistic regression[C] //Proc of the 22nd Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2008:289- 296" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving logistic regres-sion">
                                        <b>[47]</b>
                                        Chaudhuri K,Monteleoni C.Privacy-preserving logistic regression[C] //Proc of the 22nd Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2008:289- 296
                                    </a>
                                </li>
                                <li id="801">


                                    <a id="bibliography_48" title="Zhang Jiaqi,Zheng Kai,Mou Wenlong,et al.Efficient private ERM for smooth objectives[C] //Proc of the 26th Int Joint Conf on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann,2017:3922- 3928" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient private ERM for smooth objectives">
                                        <b>[48]</b>
                                        Zhang Jiaqi,Zheng Kai,Mou Wenlong,et al.Efficient private ERM for smooth objectives[C] //Proc of the 26th Int Joint Conf on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann,2017:3922- 3928
                                    </a>
                                </li>
                                <li id="803">


                                    <a id="bibliography_49" title="Chaudhuri K,Monteleoni C,Sarwate D.Differentially private empirical risk minimization[J].Journal of Machine Learning Research,2011,12:1069- 1109" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Differentially private empirical risk minimization">
                                        <b>[49]</b>
                                        Chaudhuri K,Monteleoni C,Sarwate D.Differentially private empirical risk minimization[J].Journal of Machine Learning Research,2011,12:1069- 1109
                                    </a>
                                </li>
                                <li id="805">


                                    <a id="bibliography_50" title="Kifer D,Smith A,Thakurta A.Private convex optimization for empirical risk minimization with applications to high-dimensional regression[C] //Proc of the 25th Annual Conf on Learning Theory.Berlin:Springer,2012:No.25" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Private convex optimization for empirical risk minimization with applications to high-dimensional regression">
                                        <b>[50]</b>
                                        Kifer D,Smith A,Thakurta A.Private convex optimization for empirical risk minimization with applications to high-dimensional regression[C] //Proc of the 25th Annual Conf on Learning Theory.Berlin:Springer,2012:No.25
                                    </a>
                                </li>
                                <li id="807">


                                    <a id="bibliography_51" title="Talwar K,Thakurta A,Zhang Li.Private empirical risk minimization beyond the worst case:The effect of the constraint set geometry[OL].[2018-05-28].http://arxiv.org/abs/1411.5417" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Private empirical risk minimization beyond the worst case:The effect of the constraint set geometry[OL]">
                                        <b>[51]</b>
                                        Talwar K,Thakurta A,Zhang Li.Private empirical risk minimization beyond the worst case:The effect of the constraint set geometry[OL].[2018-05-28].http://arxiv.org/abs/1411.5417
                                    </a>
                                </li>
                                <li id="809">


                                    <a id="bibliography_52" title="Song Shuang,Chaudhuri K,Sarwate A D.Stochastic gradient descent with differentially private updates[C] //Proc of the IEEE Global Conf on Signal and Information Processing.Piscataway,NJ:IEEE,2013:245- 248" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent with differentially private updates">
                                        <b>[52]</b>
                                        Song Shuang,Chaudhuri K,Sarwate A D.Stochastic gradient descent with differentially private updates[C] //Proc of the IEEE Global Conf on Signal and Information Processing.Piscataway,NJ:IEEE,2013:245- 248
                                    </a>
                                </li>
                                <li id="811">


                                    <a id="bibliography_53" title="Bassily R,Thakurta A.Private empirical risk minimization:ffficient algorithms and tight error bounds[C] //Proc of the IEEE Symp on Foundations of Computer Science.Piscataway,NJ:IEEE,2014:464- 473" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Private empirical risk minimization:ffficient algorithms and tight error bounds">
                                        <b>[53]</b>
                                        Bassily R,Thakurta A.Private empirical risk minimization:ffficient algorithms and tight error bounds[C] //Proc of the IEEE Symp on Foundations of Computer Science.Piscataway,NJ:IEEE,2014:464- 473
                                    </a>
                                </li>
                                <li id="813">


                                    <a id="bibliography_54" title="Zhang Tao,Zhu Quanqing.A dual perturbation approach for differential private ADMM-based distributed empirical risk minimization[C] //Proc of the 2016 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2016:129- 137" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A dual perturbation approach for differential private ADMM-based distributed empirical risk minimization">
                                        <b>[54]</b>
                                        Zhang Tao,Zhu Quanqing.A dual perturbation approach for differential private ADMM-based distributed empirical risk minimization[C] //Proc of the 2016 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2016:129- 137
                                    </a>
                                </li>
                                <li id="815">


                                    <a id="bibliography_55" title="Hamm J,Cao Yingjun,Belkin M.Learning privately from multiparty data[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM,2016:555- 563" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning privately from multiparty data">
                                        <b>[55]</b>
                                        Hamm J,Cao Yingjun,Belkin M.Learning privately from multiparty data[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM,2016:555- 563
                                    </a>
                                </li>
                                <li id="817">


                                    <a id="bibliography_56" title="Hynes N,Cheng R,Song D.Efficient deep learning on multi-source private data[OL].[2018-05-28].http://arxiv.org/abs/1807.06689" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient deep learning on multi-source private data[OL]">
                                        <b>[56]</b>
                                        Hynes N,Cheng R,Song D.Efficient deep learning on multi-source private data[OL].[2018-05-28].http://arxiv.org/abs/1807.06689
                                    </a>
                                </li>
                                <li id="819">


                                    <a id="bibliography_57" title="McSherry F.Privacy integrated queries:An extensible platform for privacy-preserving data analysis[J].Communications of the ACM,2010,53(9):89- 97" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000031480&amp;v=MTI2MjNRVE1ud1plWnVIeWptVUwzSUpGc1hhUm89TmlmSVk3SzdIdGpOcjQ5RlpPZ09DSFE1b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[57]</b>
                                        McSherry F.Privacy integrated queries:An extensible platform for privacy-preserving data analysis[J].Communications of the ACM,2010,53(9):89- 97
                                    </a>
                                </li>
                                <li id="821">


                                    <a id="bibliography_58" title="Long Yunhui,Bindschaedler V,Gunter C.Towards measuring membership privacy[OL].[2018-05-28].http://arxiv.org/abs/1712.09136" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards measuring membership privacy[OL]">
                                        <b>[58]</b>
                                        Long Yunhui,Bindschaedler V,Gunter C.Towards measuring membership privacy[OL].[2018-05-28].http://arxiv.org/abs/1712.09136
                                    </a>
                                </li>
                                <li id="823">


                                    <a id="bibliography_59" title="Liu Jian,Juuti M,Lu Yao,et al.Oblivious neural network predictions via miniONN transformations[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:619- 631" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Oblivious neural network predictions via miniONN transformations">
                                        <b>[59]</b>
                                        Liu Jian,Juuti M,Lu Yao,et al.Oblivious neural network predictions via miniONN transformations[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:619- 631
                                    </a>
                                </li>
                                <li id="825">


                                    <a id="bibliography_60" title="Hesamifard E,Takabi H,Ghasemi M.CryptoDL:Deep neural networks over encrypted data[OL].[2018-05-28].http://arxiv.org/abs/1711.05189" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CryptoDL:Deep neural networks over encrypted data[OL]">
                                        <b>[60]</b>
                                        Hesamifard E,Takabi H,Ghasemi M.CryptoDL:Deep neural networks over encrypted data[OL].[2018-05-28].http://arxiv.org/abs/1711.05189
                                    </a>
                                </li>
                                <li id="827">


                                    <a id="bibliography_61" title="Shokri R,Shmatikov V.Privacy-preserving deep learning[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2015:1310- 1321" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy-Preserving Deep Learning">
                                        <b>[61]</b>
                                        Shokri R,Shmatikov V.Privacy-preserving deep learning[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2015:1310- 1321
                                    </a>
                                </li>
                                <li id="829">


                                    <a id="bibliography_62" title="Phong L T,Phuong T T.Privacy-preserving deep learning for any activation function[OL].[2018-05-28].http://arxiv.org/abs/1809.03272" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving deep learning for any activation function[OL]">
                                        <b>[62]</b>
                                        Phong L T,Phuong T T.Privacy-preserving deep learning for any activation function[OL].[2018-05-28].http://arxiv.org/abs/1809.03272
                                    </a>
                                </li>
                                <li id="831">


                                    <a id="bibliography_63" title="Xu Ke,Cao Tongyi,Shah S,et al.Cleaning the null space:A privacy mechanism for predictors[C] //Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2017:2789- 2795" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cleaning the null space:A privacy mechanism for predictors">
                                        <b>[63]</b>
                                        Xu Ke,Cao Tongyi,Shah S,et al.Cleaning the null space:A privacy mechanism for predictors[C] //Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2017:2789- 2795
                                    </a>
                                </li>
                                <li id="833">


                                    <a id="bibliography_64" title="Cao Yinzhi,Yang Junfeng.Towards making systems forget with machine unlearning[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2015:463- 480" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards making systems forget with machine unlearning">
                                        <b>[64]</b>
                                        Cao Yinzhi,Yang Junfeng.Towards making systems forget with machine unlearning[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2015:463- 480
                                    </a>
                                </li>
                                <li id="835">


                                    <a id="bibliography_65" title="Hunt T,Song Congzheng,Shokri R,et al.Chiron:Privacy-preserving machine learning as a service[OL].[2018-05-28].http://arxiv.org/abs/1803.05961" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chiron:Privacy-preserving machine learning as a service[OL]">
                                        <b>[65]</b>
                                        Hunt T,Song Congzheng,Shokri R,et al.Chiron:Privacy-preserving machine learning as a service[OL].[2018-05-28].http://arxiv.org/abs/1803.05961
                                    </a>
                                </li>
                                <li id="837">


                                    <a id="bibliography_66" title="Ohrimenko O,Schuster F,Fournet C,et al.Oblivious multi-party machine learning on trusted processors[C] //Proc of the 25th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2016:619- 636" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Oblivious multi-party machine learning on trusted processors">
                                        <b>[66]</b>
                                        Ohrimenko O,Schuster F,Fournet C,et al.Oblivious multi-party machine learning on trusted processors[C] //Proc of the 25th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2016:619- 636
                                    </a>
                                </li>
                                <li id="839">


                                    <a id="bibliography_67" title="Bruckner M,Scheffer T.Nash equilibria of static prediction games[C] //Proc of the 22nd Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2009:171- 179" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nash equilibria of static prediction games">
                                        <b>[67]</b>
                                        Bruckner M,Scheffer T.Nash equilibria of static prediction games[C] //Proc of the 22nd Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2009:171- 179
                                    </a>
                                </li>
                                <li id="841">


                                    <a id="bibliography_68" title="Nelson B,Barreno M,Chi F J,et al.Exploiting machine learning to subvert your spam filter[C] //Proc of the 1st USENIX Workshop on Large-Scale Exploits and Emergent Threats.Berkeley,CA:USENIX Assocaiation,2008" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting machine learning to subvert your spam filter">
                                        <b>[68]</b>
                                        Nelson B,Barreno M,Chi F J,et al.Exploiting machine learning to subvert your spam filter[C] //Proc of the 1st USENIX Workshop on Large-Scale Exploits and Emergent Threats.Berkeley,CA:USENIX Assocaiation,2008
                                    </a>
                                </li>
                                <li id="843">


                                    <a id="bibliography_69" title="Mei Shike,Zhu Xiaojin.The security of latent dirichlet allocation[C] //Proc of the 8th Int Conf on Artificial Intelligence and Statistics.Cambridge,MA:MIT Press,2015:681- 689" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The security of latent Dirichlet allocation">
                                        <b>[69]</b>
                                        Mei Shike,Zhu Xiaojin.The security of latent dirichlet allocation[C] //Proc of the 8th Int Conf on Artificial Intelligence and Statistics.Cambridge,MA:MIT Press,2015:681- 689
                                    </a>
                                </li>
                                <li id="845">


                                    <a id="bibliography_70" title="Xiao Huang,Biggio B,Brown G,et al.Is feature selection secure against training data poisoning?[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM,2015:1689- 1698" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Is feature selection secure against training data poisoning?">
                                        <b>[70]</b>
                                        Xiao Huang,Biggio B,Brown G,et al.Is feature selection secure against training data poisoning?[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM,2015:1689- 1698
                                    </a>
                                </li>
                                <li id="847">


                                    <a id="bibliography_71" title="Liu Chang,Li Bo,Vorobeychik Y,et al.Robust linear regression against training data poisoning[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:91- 102" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust linear regression against training data poisoning">
                                        <b>[71]</b>
                                        Liu Chang,Li Bo,Vorobeychik Y,et al.Robust linear regression against training data poisoning[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:91- 102
                                    </a>
                                </li>
                                <li id="849">


                                    <a id="bibliography_72" title="Baracaldo N,Chen B,Ludwig H,et al.Mitigating poisoning attacks on machine learning models:A data provenance based approach[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:103- 110" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mitigating poisoning attacks on machine learning models:A data provenance based approach">
                                        <b>[72]</b>
                                        Baracaldo N,Chen B,Ludwig H,et al.Mitigating poisoning attacks on machine learning models:A data provenance based approach[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:103- 110
                                    </a>
                                </li>
                                <li id="851">


                                    <a id="bibliography_73" title="Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1412.6572" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining and harnessing adversarial examples[OL]">
                                        <b>[73]</b>
                                        Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1412.6572
                                    </a>
                                </li>
                                <li id="853">


                                    <a id="bibliography_74" title="Papernot N,Mcdaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:372- 387" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">
                                        <b>[74]</b>
                                        Papernot N,Mcdaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:372- 387
                                    </a>
                                </li>
                                <li id="855">


                                    <a id="bibliography_75" title="Carlini N,Wagner D.Towards evaluating the robustness of neural networks[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:39- 57" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards evaluating the robustness of neural networks">
                                        <b>[75]</b>
                                        Carlini N,Wagner D.Towards evaluating the robustness of neural networks[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:39- 57
                                    </a>
                                </li>
                                <li id="857">


                                    <a id="bibliography_76" title="Moosavi-Dezfooli S M,Fawzi A,Frossard P.DeepFool:A simple and accurate method to fool deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2574- 2582" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepfool:A simple and accurate method to fool deep neural networks">
                                        <b>[76]</b>
                                        Moosavi-Dezfooli S M,Fawzi A,Frossard P.DeepFool:A simple and accurate method to fool deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2574- 2582
                                    </a>
                                </li>
                                <li id="859">


                                    <a id="bibliography_77" title="Yuan Xuejing,Chen Yuxuan,Zhao Yue,et al.CommanderSong:A systematic approach for practical adversarial voice recognition[C] //Proc of the 27th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2018:49- 64" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CommanderSong:A systematic approach for practical adversarial voice recognition">
                                        <b>[77]</b>
                                        Yuan Xuejing,Chen Yuxuan,Zhao Yue,et al.CommanderSong:A systematic approach for practical adversarial voice recognition[C] //Proc of the 27th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2018:49- 64
                                    </a>
                                </li>
                                <li id="861">


                                    <a id="bibliography_78" title="Carlini N,Wagner D.Audio adversarial examples:Targeted attacks on speech-to-text[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:1- 7" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Audio adversarial examples:Targeted attacks on speech-to-text">
                                        <b>[78]</b>
                                        Carlini N,Wagner D.Audio adversarial examples:Targeted attacks on speech-to-text[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:1- 7
                                    </a>
                                </li>
                                <li id="863">


                                    <a id="bibliography_79" title="Gao Ji,Lanchantin J,Soffa M L,et al.Black-box generation of adversarial text sequences to evade deep learning classifiers[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:50- 56" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Black-box generation of adversarial text sequences to evade deep learning classifiers">
                                        <b>[79]</b>
                                        Gao Ji,Lanchantin J,Soffa M L,et al.Black-box generation of adversarial text sequences to evade deep learning classifiers[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:50- 56
                                    </a>
                                </li>
                                <li id="865">


                                    <a id="bibliography_80" title="Kreuk F,Barak A,Aviv-Reuven S,et al.Deceiving end-to-end deep learning malware detectors using adversarial examples[C/OL] //Proc of the 32nd Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2018 [2019-06-11].https://arxiv.org/abs/1802.04528" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deceiving end-to-end deep learning malware detectors using adversarial examples[C/OL]">
                                        <b>[80]</b>
                                        Kreuk F,Barak A,Aviv-Reuven S,et al.Deceiving end-to-end deep learning malware detectors using adversarial examples[C/OL] //Proc of the 32nd Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2018 [2019-06-11].https://arxiv.org/abs/1802.04528
                                    </a>
                                </li>
                                <li id="867">


                                    <a id="bibliography_81" title="Moosavi-Dezfooli S M,Fawzi A,Fawzi O,et al.Universal adversarial perturbations[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:86- 94" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Universal adversarial perturbations">
                                        <b>[81]</b>
                                        Moosavi-Dezfooli S M,Fawzi A,Fawzi O,et al.Universal adversarial perturbations[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:86- 94
                                    </a>
                                </li>
                                <li id="869">


                                    <a id="bibliography_82" title="Huang Ruitong,Xu Bing,Schuurmans D,et al.Learning with a strong adversary[OL].[2018-05-28].http://arxiv.org/abs/1511.03034" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning with a strong adversary[OL]">
                                        <b>[82]</b>
                                        Huang Ruitong,Xu Bing,Schuurmans D,et al.Learning with a strong adversary[OL].[2018-05-28].http://arxiv.org/abs/1511.03034
                                    </a>
                                </li>
                                <li id="871">


                                    <a id="bibliography_83" title="Pang Tianyu,Du Chao,Zhu Jun.Robust deep learning via reverse cross-entropy training and thresholding test[OL].[2018-05-28].http://arxiv.org/abs/1706.00633" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust deep learning via reverse cross-entropy training and thresholding test[OL]">
                                        <b>[83]</b>
                                        Pang Tianyu,Du Chao,Zhu Jun.Robust deep learning via reverse cross-entropy training and thresholding test[OL].[2018-05-28].http://arxiv.org/abs/1706.00633
                                    </a>
                                </li>
                                <li id="873">


                                    <a id="bibliography_84" title="Song Yang,Kim T,Nowozin S,et al.PixelDefend:Leveraging generative models to understand and defend against adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1710.10766" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pixel defend:Leveraging generative models to understand and defend against adversarial examples">
                                        <b>[84]</b>
                                        Song Yang,Kim T,Nowozin S,et al.PixelDefend:Leveraging generative models to understand and defend against adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1710.10766
                                    </a>
                                </li>
                                <li id="875">


                                    <a id="bibliography_85" title="Madry A,Makelov A,Schmidt L,et al.Towards deep learning models resistant to adversarial attacks[OL].[2018-05-28].http://arxiv.org/abs/1706.06083" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards deep learning models resistant to adversarial attacks[OL]">
                                        <b>[85]</b>
                                        Madry A,Makelov A,Schmidt L,et al.Towards deep learning models resistant to adversarial attacks[OL].[2018-05-28].http://arxiv.org/abs/1706.06083
                                    </a>
                                </li>
                                <li id="877">


                                    <a id="bibliography_86" title="Papernot N,Mcdaniel P.On the effectiveness of defensive distillation[OL].[2018-05-28].http://arxiv.org/abs/1607.05113" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the effectiveness of defensive distillation[OL]">
                                        <b>[86]</b>
                                        Papernot N,Mcdaniel P.On the effectiveness of defensive distillation[OL].[2018-05-28].http://arxiv.org/abs/1607.05113
                                    </a>
                                </li>
                                <li id="879">


                                    <a id="bibliography_87" title="Zantedeschi V,Nicolae M I,Rawat A.Efficient defenses against adversarial attacks[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:39- 49" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient defenses against adversarial attacks">
                                        <b>[87]</b>
                                        Zantedeschi V,Nicolae M I,Rawat A.Efficient defenses against adversarial attacks[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:39- 49
                                    </a>
                                </li>
                                <li id="881">


                                    <a id="bibliography_88" title="Gu Shixiang,Rigazio L.Towards deep neural network architectures robust to adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1412.5068" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards deep neural network architectures robust to adversarial examples[OL]">
                                        <b>[88]</b>
                                        Gu Shixiang,Rigazio L.Towards deep neural network architectures robust to adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1412.5068
                                    </a>
                                </li>
                                <li id="883">


                                    <a id="bibliography_89" title="Biggio B,Nelson B,Laskov P.Poisoning attacks against support vector machines[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM,2012" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Poisoning Attacks against Support Vector Machines">
                                        <b>[89]</b>
                                        Biggio B,Nelson B,Laskov P.Poisoning attacks against support vector machines[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM,2012
                                    </a>
                                </li>
                                <li id="885">


                                    <a id="bibliography_90" title="Xiao Han,Xiao Huang,Eckert C.Adversarial label flips attack on support vector machines[C] //Proc of the 20th European Conf on Artificial Intelligence.Ohmsha,Japan:IOS,2012:870- 875" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD15122400027214&amp;v=MjQwODRxNDlGWk9rSURuMDlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMM0lKRnNYYVJvPU5pVEJhcks5SDlQTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[90]</b>
                                        Xiao Han,Xiao Huang,Eckert C.Adversarial label flips attack on support vector machines[C] //Proc of the 20th European Conf on Artificial Intelligence.Ohmsha,Japan:IOS,2012:870- 875
                                    </a>
                                </li>
                                <li id="887">


                                    <a id="bibliography_91" title="Biggio B,Pillai I,Bulo S R,et al.Is data clustering in adversarial settings secure?[C] //Proc of the 2013 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2013:87- 98" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Is data clustering in adversarial settings secure?">
                                        <b>[91]</b>
                                        Biggio B,Pillai I,Bulo S R,et al.Is data clustering in adversarial settings secure?[C] //Proc of the 2013 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2013:87- 98
                                    </a>
                                </li>
                                <li id="889">


                                    <a id="bibliography_92" title="Mei Shike,Zhu Xiaojin.Using machine teaching to identify optimal training-set attacks on machine learners[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2015:2871- 2877" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using machine teaching to identify optimal training-set attacks on machine learners">
                                        <b>[92]</b>
                                        Mei Shike,Zhu Xiaojin.Using machine teaching to identify optimal training-set attacks on machine learners[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2015:2871- 2877
                                    </a>
                                </li>
                                <li id="891">


                                    <a id="bibliography_93" title="Jagielski M,Oprea A,Biggio B,et al.Manipulating machine learning:Poisoning attacks and countermeasures for regression learning[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:19- 35" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manipulating machine learning:Poisoning attacks and countermeasures for regression learning">
                                        <b>[93]</b>
                                        Jagielski M,Oprea A,Biggio B,et al.Manipulating machine learning:Poisoning attacks and countermeasures for regression learning[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:19- 35
                                    </a>
                                </li>
                                <li id="893">


                                    <a id="bibliography_94" title="Shafahi A,Huang W R,Najibi M,et al.Poison frogs! targeted clean-label poisoning attacks on neural networks[OL].[2018-05-28].http://arxiv.org/abs/1804.00792" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Poison frogs! targeted clean-label poisoning attacks on neural networks[OL]">
                                        <b>[94]</b>
                                        Shafahi A,Huang W R,Najibi M,et al.Poison frogs! targeted clean-label poisoning attacks on neural networks[OL].[2018-05-28].http://arxiv.org/abs/1804.00792
                                    </a>
                                </li>
                                <li id="895">


                                    <a id="bibliography_95" title="Steinhardt J,Koh P W,Liang Pang.Certified defenses for data poisoning attacks[C] //Proc of Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2017:3520- 3532" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Certified defenses for data poisoning attacks">
                                        <b>[95]</b>
                                        Steinhardt J,Koh P W,Liang Pang.Certified defenses for data poisoning attacks[C] //Proc of Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2017:3520- 3532
                                    </a>
                                </li>
                                <li id="897">


                                    <a id="bibliography_96" title="Muňoz-Gonz&#225;lez L,Biggio B,Demontis A,et al.Towards poisoning of deep learning algorithms with back-gradient optimization[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:27- 38" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards poisoning of deep learning algorithms with back-gradient optimization">
                                        <b>[96]</b>
                                        Muňoz-Gonz&#225;lez L,Biggio B,Demontis A,et al.Towards poisoning of deep learning algorithms with back-gradient optimization[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:27- 38
                                    </a>
                                </li>
                                <li id="899">


                                    <a id="bibliography_97" title="Wang Yizhen,Chaudhuri K.Data poisoning attacks against online learning[OL].[2018-05-28].http://arxiv.org/abs/1808.08994" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data poisoning attacks against online learning[OL]">
                                        <b>[97]</b>
                                        Wang Yizhen,Chaudhuri K.Data poisoning attacks against online learning[OL].[2018-05-28].http://arxiv.org/abs/1808.08994
                                    </a>
                                </li>
                                <li id="901">


                                    <a id="bibliography_98" title="Gong Yuan,Poellabauer C.Crafting adversarial examples for speech paralinguistics applications[OL].[2018-05-28].http://arxiv.org/abs/1711.03280" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crafting adversarial examples for speech paralinguistics applications[OL]">
                                        <b>[98]</b>
                                        Gong Yuan,Poellabauer C.Crafting adversarial examples for speech paralinguistics applications[OL].[2018-05-28].http://arxiv.org/abs/1711.03280
                                    </a>
                                </li>
                                <li id="903">


                                    <a id="bibliography_99" title="Huang Wenyi,Stokes J W.MtNet:A multi-task neural network for dynamic malware classification[C] //Proc of Detection of Intrusions and Malware,and Vulnerability Assessment.Berlin:Springer,2016:399- 418" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MtNet:A multi-task neural network for dynamic malware classification">
                                        <b>[99]</b>
                                        Huang Wenyi,Stokes J W.MtNet:A multi-task neural network for dynamic malware classification[C] //Proc of Detection of Intrusions and Malware,and Vulnerability Assessment.Berlin:Springer,2016:399- 418
                                    </a>
                                </li>
                                <li id="905">


                                    <a id="bibliography_100" title="Papernot N,Mcdaniel P,Swami A,et al.Crafting adversarial input sequences for recurrent neural networks[C] //Proc of the IEEE Military Communications Conf.Piscataway,NJ:IEEE,2016:49- 54" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crafting adversarial input sequences for recurrent neural networks">
                                        <b>[100]</b>
                                        Papernot N,Mcdaniel P,Swami A,et al.Crafting adversarial input sequences for recurrent neural networks[C] //Proc of the IEEE Military Communications Conf.Piscataway,NJ:IEEE,2016:49- 54
                                    </a>
                                </li>
                                <li id="907">


                                    <a id="bibliography_101" title="Pascanu R,Stokes J W,Sanossian H,et al.Malware classification with recurrent networks[C] //Proc of the IEEE Int Conf on Acoustics,Speech and Signal.Piscataway,NJ:IEEE,2015:1916- 1920" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Malware classification with recurrent networks">
                                        <b>[101]</b>
                                        Pascanu R,Stokes J W,Sanossian H,et al.Malware classification with recurrent networks[C] //Proc of the IEEE Int Conf on Acoustics,Speech and Signal.Piscataway,NJ:IEEE,2015:1916- 1920
                                    </a>
                                </li>
                                <li id="909">


                                    <a id="bibliography_102" title="Rigaki M,Garcia S.Bringing a GAN to a knife-fight:Adapting malware communication to avoid detection[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:70- 75" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bringing a GAN to a knife-fight:Adapting malware communication to avoid detection">
                                        <b>[102]</b>
                                        Rigaki M,Garcia S.Bringing a GAN to a knife-fight:Adapting malware communication to avoid detection[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:70- 75
                                    </a>
                                </li>
                                <li id="911">


                                    <a id="bibliography_103" title="Hu Weiwei,Tan Ying.Black-box attacks against RNN based malware detection algorithms[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:245- 251" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Black-box attacks against RNN based malware detection algorithms">
                                        <b>[103]</b>
                                        Hu Weiwei,Tan Ying.Black-box attacks against RNN based malware detection algorithms[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:245- 251
                                    </a>
                                </li>
                                <li id="913">


                                    <a id="bibliography_104" title="Hu Weiwei,Tan Ying.Generating adversarial malware examples for black-box attacks based on GAN[OL].[2018-05-28].http://arxiv.org/abs/1702.05983" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating adversarial malware examples for black-box attacks based on GAN[OL]">
                                        <b>[104]</b>
                                        Hu Weiwei,Tan Ying.Generating adversarial malware examples for black-box attacks based on GAN[OL].[2018-05-28].http://arxiv.org/abs/1702.05983
                                    </a>
                                </li>
                                <li id="915">


                                    <a id="bibliography_105" title="Rosenberg I,Shabtai A,Rokach L,et al.Generic black-box end-to-end attack against state of the art API call based malware classifiers[C] //Proc of the 21st Int Symp on Research in Attacks,Intrusions and Defenses.Berlin:Springer,2018:490- 510" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generic black-box end-to-end attack against state of the art API call based malware classifiers">
                                        <b>[105]</b>
                                        Rosenberg I,Shabtai A,Rokach L,et al.Generic black-box end-to-end attack against state of the art API call based malware classifiers[C] //Proc of the 21st Int Symp on Research in Attacks,Intrusions and Defenses.Berlin:Springer,2018:490- 510
                                    </a>
                                </li>
                                <li id="917">


                                    <a id="bibliography_106" title="Al-Dujaili A,Huang A,Hemberg E,et al.Adversarial deep learning for robust detection of binary encoded malware[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:76- 82" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial deep learning for robust detection of binary encoded malware">
                                        <b>[106]</b>
                                        Al-Dujaili A,Huang A,Hemberg E,et al.Adversarial deep learning for robust detection of binary encoded malware[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:76- 82
                                    </a>
                                </li>
                                <li id="919">


                                    <a id="bibliography_107" title="Kurakin A,Goodfellow I,Bengio S.Adversarial examples in the physical world[OL].[2018-05-28].http://arxiv.org/abs/1607.02533" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial examples in the physical world">
                                        <b>[107]</b>
                                        Kurakin A,Goodfellow I,Bengio S.Adversarial examples in the physical world[OL].[2018-05-28].http://arxiv.org/abs/1607.02533
                                    </a>
                                </li>
                                <li id="921">


                                    <a id="bibliography_108" title="Tram&#232;r F,Kurakin A,Papernot N,et al.Ensemble adversarial training:Attacks and defenses[OL].[2018-05-28].http://arxiv.org/abs/1705.07204" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble adversarial training:attacks and defenses">
                                        <b>[108]</b>
                                        Tram&#232;r F,Kurakin A,Papernot N,et al.Ensemble adversarial training:Attacks and defenses[OL].[2018-05-28].http://arxiv.org/abs/1705.07204
                                    </a>
                                </li>
                                <li id="923">


                                    <a id="bibliography_109" title="Kurakin A,Goodfellow I,Bengio S.Adversarial machine learning at scale[OL].[2018-05-28].http://arxiv.org/abs/1611.01236" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial machine learning at scale[OL]">
                                        <b>[109]</b>
                                        Kurakin A,Goodfellow I,Bengio S.Adversarial machine learning at scale[OL].[2018-05-28].http://arxiv.org/abs/1611.01236
                                    </a>
                                </li>
                                <li id="925">


                                    <a id="bibliography_110" title="Dong Yinpeng,Liao Fangzhou,Pang Tianyu,et al.Boosting adversarial attacks with momentum[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:9185- 9193" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boosting adversarial attacks with momentum">
                                        <b>[110]</b>
                                        Dong Yinpeng,Liao Fangzhou,Pang Tianyu,et al.Boosting adversarial attacks with momentum[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:9185- 9193
                                    </a>
                                </li>
                                <li id="927">


                                    <a id="bibliography_111" title="Papernot N,Mcdaniel P,Wu Xi,et al.Distillation as a defense to adversarial perturbations against deep neural networks[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2016:582- 597" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distillation as a defense to adversarial perturbations against deep neural networks">
                                        <b>[111]</b>
                                        Papernot N,Mcdaniel P,Wu Xi,et al.Distillation as a defense to adversarial perturbations against deep neural networks[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2016:582- 597
                                    </a>
                                </li>
                                <li id="929">


                                    <a id="bibliography_112" title="Chen P Y,Sharma Y,Zhang Huan,et al.EAD:Elastic-net attacks to deep neural networks via adversarial examples[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:10- 17" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EAD:Elastic-net attacks to deep neural networks via adversarial examples">
                                        <b>[112]</b>
                                        Chen P Y,Sharma Y,Zhang Huan,et al.EAD:Elastic-net attacks to deep neural networks via adversarial examples[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:10- 17
                                    </a>
                                </li>
                                <li id="931">


                                    <a id="bibliography_113" title="He W,Li Bo,Song D.Decision boundary analysis of adversarial examples[C/OL] //Proc of Int Conf on Learning Representations.2018 [2019-06-11].https://openreview.net/pdf?id=BkpiPMbA-" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decision boundary analysis of adversarial examples[C/OL]">
                                        <b>[113]</b>
                                        He W,Li Bo,Song D.Decision boundary analysis of adversarial examples[C/OL] //Proc of Int Conf on Learning Representations.2018 [2019-06-11].https://openreview.net/pdf?id=BkpiPMbA-
                                    </a>
                                </li>
                                <li id="933">


                                    <a id="bibliography_114" title="Jang U,Wu Xi,Jha S.Objective metrics and gradient descent algorithms for adversarial examples in machine learning[C] //Proc of the 33rd Annual Computer Security Applications Conf.New York:ACM,2017:262- 277" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective metrics and gradient descent algorithms for adversarial examples in machine learning">
                                        <b>[114]</b>
                                        Jang U,Wu Xi,Jha S.Objective metrics and gradient descent algorithms for adversarial examples in machine learning[C] //Proc of the 33rd Annual Computer Security Applications Conf.New York:ACM,2017:262- 277
                                    </a>
                                </li>
                                <li id="935">


                                    <a id="bibliography_115" title="Hayes J,Danezis G.Learning universal adversarial perturbations with generative models[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:43- 49" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning universal adversarial perturbations with generative models">
                                        <b>[115]</b>
                                        Hayes J,Danezis G.Learning universal adversarial perturbations with generative models[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:43- 49
                                    </a>
                                </li>
                                <li id="937">


                                    <a id="bibliography_116" title="Tram&#232;r F,Papernot N,Goodfellow I,et al.The space of transferable adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1704.03453" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The space of transferable adversarial examples[OL]">
                                        <b>[116]</b>
                                        Tram&#232;r F,Papernot N,Goodfellow I,et al.The space of transferable adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1704.03453
                                    </a>
                                </li>
                                <li id="939">


                                    <a id="bibliography_117" title="Narodytska N,Kasiviswanathan S.Simple black-box adversarial attacks on deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:1310- 1318" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simple black-box adversarial attacks on deep neural networks">
                                        <b>[117]</b>
                                        Narodytska N,Kasiviswanathan S.Simple black-box adversarial attacks on deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:1310- 1318
                                    </a>
                                </li>
                                <li id="941">


                                    <a id="bibliography_118" title="Ilyas A,Engstrom L,Athalye A,et al.Query-efficient black-box adversarial examples (superceded)[OL].[2018-05-28].http://arxiv.org/abs/1712.07113" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Query-efficient black-box adversarial examples (superceded)[OL]">
                                        <b>[118]</b>
                                        Ilyas A,Engstrom L,Athalye A,et al.Query-efficient black-box adversarial examples (superceded)[OL].[2018-05-28].http://arxiv.org/abs/1712.07113
                                    </a>
                                </li>
                                <li id="943">


                                    <a id="bibliography_119" title="Mei Shike,Zhu Xiaojin.Using machine teaching to identify optimal training-set attacks on machine learners[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2015:2871- 2877" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using machine teaching to identify optimal training-set attacks on machine learners">
                                        <b>[119]</b>
                                        Mei Shike,Zhu Xiaojin.Using machine teaching to identify optimal training-set attacks on machine learners[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2015:2871- 2877
                                    </a>
                                </li>
                                <li id="945">


                                    <a id="bibliography_120" title="Huang S,Papernot N,Goodfellow I,et al.Adversarial attacks on neural network policies[OL].[2018-05-28].http://arxiv.org/abs/1702.02284" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial attacks on neural network policies[OL]">
                                        <b>[120]</b>
                                        Huang S,Papernot N,Goodfellow I,et al.Adversarial attacks on neural network policies[OL].[2018-05-28].http://arxiv.org/abs/1702.02284
                                    </a>
                                </li>
                                <li id="947">


                                    <a id="bibliography_121" title="Kos J,Fischer I,Song D.Adversarial examples for generative models[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:36- 42" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial examples for generative models">
                                        <b>[121]</b>
                                        Kos J,Fischer I,Song D.Adversarial examples for generative models[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:36- 42
                                    </a>
                                </li>
                                <li id="949">


                                    <a id="bibliography_122" title="Wang Xinlei,Zeng Kai,Govindan K,et al.Chaining for securing data provenance in distributed information networks[C] //Proc of the 31st IEEE Military Communications Conf.Piscataway,NJ:IEEE,2012:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chaining for securing data provenance in distributed information networks">
                                        <b>[122]</b>
                                        Wang Xinlei,Zeng Kai,Govindan K,et al.Chaining for securing data provenance in distributed information networks[C] //Proc of the 31st IEEE Military Communications Conf.Piscataway,NJ:IEEE,2012:1- 6
                                    </a>
                                </li>
                                <li id="951">


                                    <a id="bibliography_123" title="Lyle J,Martin A P.Trusted computing and provenance:Better together[C/OL] //Proc of the 2nd Workshop on the Theory and Practice of Provenance.Berkeley,CA:USENIX Association,2010 [2019-06-11].https://www.usenix.org/legacy/event/tapp10/tech/full_papers/lyle.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trusted computing and provenance:Better together[C/OL]">
                                        <b>[123]</b>
                                        Lyle J,Martin A P.Trusted computing and provenance:Better together[C/OL] //Proc of the 2nd Workshop on the Theory and Practice of Provenance.Berkeley,CA:USENIX Association,2010 [2019-06-11].https://www.usenix.org/legacy/event/tapp10/tech/full_papers/lyle.pdf
                                    </a>
                                </li>
                                <li id="953">


                                    <a id="bibliography_124" title="Hasan R,Sion R,Winslett M.The case of the fake picasso:Preventing history forgery with secure provenance[C] //Proc of the Conf on File and Storage Technologies.Berkeley,CA:USENIX Assocaiation,2009:1- 14" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Case of the Fake Picasso:Preventing History Forgery with Secure Provenance">
                                        <b>[124]</b>
                                        Hasan R,Sion R,Winslett M.The case of the fake picasso:Preventing history forgery with secure provenance[C] //Proc of the Conf on File and Storage Technologies.Berkeley,CA:USENIX Assocaiation,2009:1- 14
                                    </a>
                                </li>
                                <li id="955">


                                    <a id="bibliography_125" title="Olufowobi H,Engel R,Baracaldo N,et al.Data provenance model for internet of things (IoT) systems[C] //Proc of Int Conf on Service-oriented Computing.Berlin:Springer,2016:85- 91" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data provenance model for internet of things (IoT) systems">
                                        <b>[125]</b>
                                        Olufowobi H,Engel R,Baracaldo N,et al.Data provenance model for internet of things (IoT) systems[C] //Proc of Int Conf on Service-oriented Computing.Berlin:Springer,2016:85- 91
                                    </a>
                                </li>
                                <li id="957">


                                    <a id="bibliography_126" title="Chakarov A,Nori A,Rajamani S,et al.Debugging machine learning tasks[OL].[2018-05-28].http://arxiv.org/abs/1603.07292" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Debugging machine learning tasks[OL]">
                                        <b>[126]</b>
                                        Chakarov A,Nori A,Rajamani S,et al.Debugging machine learning tasks[OL].[2018-05-28].http://arxiv.org/abs/1603.07292
                                    </a>
                                </li>
                                <li id="959">


                                    <a id="bibliography_127" title="Cand&#232;s E J,Li Xiaodong,Ma Yi,et al.Robust principal component analysis?[J].Journal of the ACM,2011,58(3):11.1- 11.37" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000001869&amp;v=MTk5NTQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMM0lKRnNYYVJvPU5pZklZN0s3SHRqTnI0OUZaT3NPQkhvd29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[127]</b>
                                        Cand&#232;s E J,Li Xiaodong,Ma Yi,et al.Robust principal component analysis?[J].Journal of the ACM,2011,58(3):11.1- 11.37
                                    </a>
                                </li>
                                <li id="961">


                                    <a id="bibliography_128" title="Chen Yudong,Caramanis C,Mannor S.Robust high dimensional sparse regression and matching pursuit[OL].[2018-05-28].http://arxiv.org/abs/1301.2725" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust high dimensional sparse regression and matching pursuit[OL]">
                                        <b>[128]</b>
                                        Chen Yudong,Caramanis C,Mannor S.Robust high dimensional sparse regression and matching pursuit[OL].[2018-05-28].http://arxiv.org/abs/1301.2725
                                    </a>
                                </li>
                                <li id="963">


                                    <a id="bibliography_129" title="Feng Jiashi,Xu Huan,Mannor S,et al.Robust logistic regression and classification[C] //Proc of Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:253- 261" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust logistic regression and classification">
                                        <b>[129]</b>
                                        Feng Jiashi,Xu Huan,Mannor S,et al.Robust logistic regression and classification[C] //Proc of Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:253- 261
                                    </a>
                                </li>
                                <li id="965">


                                    <a id="bibliography_130" title="Cao Xiaoyu,Gong N Z.Mitigating evasion attacks to deep neural networks via region-based classification[C] //Proc of the 33rd Annual Computer Security Applications Conf.New York:ACM,2017:278- 287" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mitigating evasion attacks to deep neural networks via region-based classification">
                                        <b>[130]</b>
                                        Cao Xiaoyu,Gong N Z.Mitigating evasion attacks to deep neural networks via region-based classification[C] //Proc of the 33rd Annual Computer Security Applications Conf.New York:ACM,2017:278- 287
                                    </a>
                                </li>
                                <li id="967">


                                    <a id="bibliography_131" title="Ma Xingjun,Li Bo,Wang Yisen,et al.Characterizing adversarial subspaces using local intrinsic dimensionality[OL].[2018-05-28].http://arxiv.org/abs/1801.02613" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Characterizing adversarial subspaces using local intrinsic dimensionality">
                                        <b>[131]</b>
                                        Ma Xingjun,Li Bo,Wang Yisen,et al.Characterizing adversarial subspaces using local intrinsic dimensionality[OL].[2018-05-28].http://arxiv.org/abs/1801.02613
                                    </a>
                                </li>
                                <li id="969">


                                    <a id="bibliography_132" title="Mccoyd M,Wagner D.Background class defense against adversarial examples[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:96- 102" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Background class defense against adversarial examples">
                                        <b>[132]</b>
                                        Mccoyd M,Wagner D.Background class defense against adversarial examples[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:96- 102
                                    </a>
                                </li>
                                <li id="971">


                                    <a id="bibliography_133" title="Guo Chuan,Rana M,Cisse M,et al.Countering adversarial images using input transformations[OL].[2018-05-28].http://arxiv.org/abs/1711.00117" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Countering adversarial images using input transformations">
                                        <b>[133]</b>
                                        Guo Chuan,Rana M,Cisse M,et al.Countering adversarial images using input transformations[OL].[2018-05-28].http://arxiv.org/abs/1711.00117
                                    </a>
                                </li>
                                <li id="973">


                                    <a id="bibliography_134" title="Xie Cihang,Wang Jianyu,Zhang Zhishuai,et al.Mitigating adversarial effects through randomization[OL].[2018-05-28].http://arxiv.org/abs/1711.01991" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mitigating adversarial effects through randomization">
                                        <b>[134]</b>
                                        Xie Cihang,Wang Jianyu,Zhang Zhishuai,et al.Mitigating adversarial effects through randomization[OL].[2018-05-28].http://arxiv.org/abs/1711.01991
                                    </a>
                                </li>
                                <li id="975">


                                    <a id="bibliography_135" title="Wang Jingyi,Sun Jun,Zhang Peixin,et al.Detecting adversarial samples for deep neural networks through mutation testing[OL].[2018-05-28].http://arxiv.org/abs/1805.05010" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting adversarial samples for deep neural networks through mutation testing[OL]">
                                        <b>[135]</b>
                                        Wang Jingyi,Sun Jun,Zhang Peixin,et al.Detecting adversarial samples for deep neural networks through mutation testing[OL].[2018-05-28].http://arxiv.org/abs/1805.05010
                                    </a>
                                </li>
                                <li id="977">


                                    <a id="bibliography_136" title="Tian Shixin,Yang Guolei,Cai Ying.Detecting adversarial examples through image transformation[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:4139- 4146" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting adversarial examples through image transformation">
                                        <b>[136]</b>
                                        Tian Shixin,Yang Guolei,Cai Ying.Detecting adversarial examples through image transformation[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:4139- 4146
                                    </a>
                                </li>
                                <li id="979">


                                    <a id="bibliography_137" title="Buckman J,Roy A,Raffel C,et al.Thermometer encoding:One hot way to resist adversarial examples[C/OL] //Proc of the Int Conf on Learning Representations.2018 [2019-06-11].https://openreview.net/pdf?id=S18Su--CW" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Thermometer encoding:One hot way to resist adversarial examples[C/OL]">
                                        <b>[137]</b>
                                        Buckman J,Roy A,Raffel C,et al.Thermometer encoding:One hot way to resist adversarial examples[C/OL] //Proc of the Int Conf on Learning Representations.2018 [2019-06-11].https://openreview.net/pdf?id=S18Su--CW
                                    </a>
                                </li>
                                <li id="981">


                                    <a id="bibliography_138" title="Ross A S,Doshi-Velez F.Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:1660- 1669" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients">
                                        <b>[138]</b>
                                        Ross A S,Doshi-Velez F.Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:1660- 1669
                                    </a>
                                </li>
                                <li id="983">


                                    <a id="bibliography_139" title="Liang Bin,Li Hongcheng,Su Miaoqiang,et al.Detecting adversarial image examples in deep networks with adaptive noise reduction[C/OL] //Proc of the IEEE Transactions on Dependable and Secure Computing.Piscataway,NJ:IEEE,2018:1- 1 [2019-06-11].https://arxiv.org/pdf/1705.08378.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting adversarial image examples in deep networks with adaptive noise reduction[C/OL]">
                                        <b>[139]</b>
                                        Liang Bin,Li Hongcheng,Su Miaoqiang,et al.Detecting adversarial image examples in deep networks with adaptive noise reduction[C/OL] //Proc of the IEEE Transactions on Dependable and Secure Computing.Piscataway,NJ:IEEE,2018:1- 1 [2019-06-11].https://arxiv.org/pdf/1705.08378.pdf
                                    </a>
                                </li>
                                <li id="985">


                                    <a id="bibliography_140" title="Xu Weilin,Evans D,Qi Yanjun.Feature squeezing:Detecting adversarial examples in deep neural networks[C/OL] //Proc of the 25th Network and Distributed System Security Symp.Reston,VA:The Internet Society,2018 [2019-06-11].https://arxiv.org/pdf/1704.01155.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature squeezing:Detecting adversarial examples in deep neural networks[C/OL]">
                                        <b>[140]</b>
                                        Xu Weilin,Evans D,Qi Yanjun.Feature squeezing:Detecting adversarial examples in deep neural networks[C/OL] //Proc of the 25th Network and Distributed System Security Symp.Reston,VA:The Internet Society,2018 [2019-06-11].https://arxiv.org/pdf/1704.01155.pdf
                                    </a>
                                </li>
                                <li id="987">


                                    <a id="bibliography_141" title="Akhtar N,Liu Jian,Mian A.Defense against universal adversarial perturbations[OL].[2018-05-28].http://arxiv.org/abs/1711.05929" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Defense against universal adversarial perturbations[OL]">
                                        <b>[141]</b>
                                        Akhtar N,Liu Jian,Mian A.Defense against universal adversarial perturbations[OL].[2018-05-28].http://arxiv.org/abs/1711.05929
                                    </a>
                                </li>
                                <li id="989">


                                    <a id="bibliography_142" title="Meng Dongyu,Chen Hao.MagNet:A two-pronged defense against adversarial examples[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:135- 147" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Magnet:a two-pronged defense against adversarial examples">
                                        <b>[142]</b>
                                        Meng Dongyu,Chen Hao.MagNet:A two-pronged defense against adversarial examples[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:135- 147
                                    </a>
                                </li>
                                <li id="991">


                                    <a id="bibliography_143" title="Zhao Qianqian,Chen Kai,Li Tongxin,et al.Detecting telecommunication fraud by understanding the contents of a call[J/OL].Cybersecurity,2018 [2019-06-11].https://cybersecurity.springeropen.com/articles/10.1186/s42400-018-0008-5" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting telecommunication fraud by understanding the contents of a call">
                                        <b>[143]</b>
                                        Zhao Qianqian,Chen Kai,Li Tongxin,et al.Detecting telecommunication fraud by understanding the contents of a call[J/OL].Cybersecurity,2018 [2019-06-11].https://cybersecurity.springeropen.com/articles/10.1186/s42400-018-0008-5
                                    </a>
                                </li>
                                <li id="993">


                                    <a id="bibliography_144" title="Chen Yi,Zha Mingming,Zhang Nan,et al.Demystifying hidden privacy settings in mobile apps[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2019:570- 586" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Demystifying hidden privacy settings in mobile apps">
                                        <b>[144]</b>
                                        Chen Yi,Zha Mingming,Zhang Nan,et al.Demystifying hidden privacy settings in mobile apps[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2019:570- 586
                                    </a>
                                </li>
                                <li id="995">


                                    <a id="bibliography_145" >
                                        <b>[145]</b>
                                    You Wei,Zong Peiyuan,Chen Kai,et al.SemFuzz:Semantics-based automatic generation of proof-of-concept exploits[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:2139- 2154■He Yingzhe,born in 1995.PhD.His main research interests include machine learning,artificial intelligence security,and adversarial attack.■Hu Xingbo,born in 1996.Master.Her main research interests include software engineering and machine learning.(huxinbo@iie.ac.cn)■He Jinwen,born in 1997.PhD.Her main research interests include AI security and code analysis.(hejinwen@iie.ac.cn)■Meng Guozhu,born in 1987.PhD from the Nanyang Technological University,Singapore in 2017.Research Fellow at Nanyang Technological University.Visiting Research Fellow at the University of Luxembourg.Associate professor with the Institute of Information Engineering,Chinese Academy of Sciences.His main research interests include mobile security,big data analysis,vulnerability detection,program analysis,and machine learning.(mengguozhu@iie.ac.cn)■Chen Kai,born in 1982.PhD from the University of Chinese Academy of Science in 2010.Professor with the Institute of Information Engineering,Chinese Academy of Sciences.Professor with the University of Chinese Academy of Sciences.His main research interests include software analysis and testing;smartphones and privacy.(chenkai@iie.ac.cn)</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(10),2049-2070 DOI:10.7544/issn1000-1239.2019.20190437            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>机器学习系统的隐私和安全问题综述</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">何英哲</a>
                                <a href="javascript:;">胡兴波</a>
                                <a href="javascript:;">何锦雯</a>
                                <a href="javascript:;">孟国柱</a>
                                <a href="javascript:;">陈恺</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E6%89%80)&amp;code=1698670&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息安全国家重点实验室(中国科学院信息工程研究所)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院信息工程研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E5%AE%89%E5%85%A8%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学网络空间安全学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>人工智能已经渗透到生活的各个角落,给人类带来了极大的便利.尤其是近年来,随着机器学习中深度学习这一分支的蓬勃发展,生活中的相关应用越来越多.不幸的是,机器学习系统也面临着许多安全隐患,而机器学习系统的普及更进一步放大了这些风险.为了揭示这些安全隐患并实现一个强大的机器学习系统,对主流的深度学习系统进行了调查.首先设计了一个剖析深度学习系统的分析模型,并界定了调查范围.调查的深度学习系统跨越了4个领域——图像分类、音频语音识别、恶意软件检测和自然语言处理,提取了对应4种类型的安全隐患,并从复杂性、攻击成功率和破坏等多个维度对其进行了表征和度量.随后,调研了针对深度学习系统的防御技术及其特点.最后通过对这些系统的观察,提出了构建健壮的深度学习系统的建议.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%89%E5%85%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器学习安全;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%89%E5%85%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习安全;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%BB%E9%98%B2%E7%AB%9E%E8%B5%9B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">攻防竞赛;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">对抗攻击;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%88%90%E5%91%98%E6%8E%A8%E7%90%86%E6%94%BB%E5%87%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">成员推理攻击;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">隐私保护;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    He Yingzhe,born in 1995.PhD.His mainresearch interests include machine learning,artificial intelligence security,and adversarialattack.heyingzhe@iie.ac.cn&lt;image id="677" type="formula" href="images/JFYZ201910003_67700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Hu Xingbo,born in 1996.Master.Hermain research interests include softwareengineering and machine learning.(huxinbo@iie.ac.cn)&lt;image id="679" type="formula" href="images/JFYZ201910003_67900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    He Jinwen,born in 1997.PhD.Her mainresearch interests include AI security andcode analysis.(hejinwen@iie.ac.cn)&lt;image id="681" type="formula" href="images/JFYZ201910003_68100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *Meng Guozhu,born in 1987.PhD from theNanyang Technological University,Singaporein 2017.Research Fellow at NanyangTechnological University.Visiting ResearchFellow at the University of Luxembourg.Associate professor with the Institute ofInformation Engineering,Chinese Academyof Sciences.His main research interestsinclude mobile security,big data analysis,vulnerability detection,program analysis,and machine learning.(mengguozhu@iie.ac.cn)&lt;image id="683" type="formula" href="images/JFYZ201910003_68300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Chen Kai,born in 1982.PhD from theUniversity of Chinese Academy of Sciencein 2010.Professor with the Institute ofInformation Engineering,Chinese Academyof Sciences.Professor with the Universityof Chinese Academy of Sciences.His mainresearch interests include software analysisand testing;smartphones and privacy.(chenkai@iie.ac.cn)&lt;image id="685" type="formula" href="images/JFYZ201910003_68500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2016QY04W0805);</span>
                                <span>国家自然科学基金项目(U1836211,61728209);</span>
                                <span>中国科学院青年创新促进会;</span>
                                <span>北京市科技新星计划;</span>
                                <span>北京市自然科学基金项目(JQ18011);</span>
                                <span>国家前沿科技创新项目(YJKYYQ20170070);</span>
                    </p>
            </div>
                    <h1><b>Privacy and Security Issues in Machine Learning Systems: A Survey</b></h1>
                    <h2>
                    <span>He Yingzhe</span>
                    <span>Hu Xingbo</span>
                    <span>He Jinwen</span>
                    <span>Meng Guozhu</span>
                    <span>Chen Kai</span>
            </h2>
                                    <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Artificial intelligence has penetrated into every corners of our life and brought humans great convenience. Especially in recent years, with the vigorous development of the deep learning branch in machine learning, there are more and more related applications in our life. Unfortunately, machine learning systems are suffering from many security hazards. Even worse, the popularity of machine learning systems further magnifies these hazards. In order to unveil these security hazards and assist in implementing a robust machine learning system, we conduct a comprehensive investigation of the mainstream deep learning systems. In the beginning of the study, we devise an analytical model for dissecting deep learning systems, and define our survey scope. Our surveyed deep learning systems span across four fields-image classification, audio speech recognition, malware detection, and natural language processing. We distill four types of security hazards and manifest them in multiple dimensions such as complexity, attack success rate, and damage. Furthermore, we survey defensive techniques for deep learning systems as well as their characteristics. Finally, through the observation of these systems, we propose the practical proposals of constructing robust deep learning system.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20learning%20security&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine learning security;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning%20security&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning security;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attack%20and%20defense%20race&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attack and defense race;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adversarial%20attack&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adversarial attack;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=membership%20inference%20attack&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">membership inference attack;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=privacy-preserving&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">privacy-preserving;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-06-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Rearch and Development Program of China(2016QY04W0805);</span>
                                <span>the National Natural Science Foundation of China(U1836211,61728209);</span>
                                <span>the Program of Youth Innovation Promotion Association CAS;</span>
                                <span>the Beijing Nova Program;</span>
                                <span>the Beijing Natural Science Foundation(JQ18011);</span>
                                <span>the National Frontier Science and Technology Innovation Project(YJKYYQ20170070);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="298">深度学习的广泛应用所带来的成功并不能保证其安全性,新的威胁和攻击每天都在出现,它们危及深度学习模型,进而危及人们的隐私、金融资产和安全.作为一种新兴的技术,深度学习的安全问题往往被忽视.因此,系统地研究深度学习的安全问题并进一步提出有效的措施,是迫切而关键的.</p>
                </div>
                <div class="p1">
                    <p id="299">深度学习已广泛应用于图像分类、语音识别、自然语言处理、恶意软件检测等多个领域.由于计算能力的巨大进步和数据量的急剧增加,深度学习在这些场景中显示出了优越的潜力.深度学习尤其擅长无监督特征学习,加深对一个对象的理解,具有强大的预测能力.然而,深度学习正遭受精心策划的攻击所带来的一系列威胁.例如深度学习系统很容易被对抗样本所欺骗,从而导致错误的分类.另一方面,使用在线深度学习系统进行分类的用户不得不向服务器公开他们的数据,这会导致隐私泄露.更糟糕的是,深度学习的广泛使用加剧了这些安全风险.</p>
                </div>
                <div class="p1">
                    <p id="300">研究人员正在探索和研究针对深度学习系统的潜在攻击以及相应的防御技术.文献<citation id="997" type="reference">[<a class="sup">1</a>]</citation>是探索神经网络安全性的先驱,Szegedy等人用难以察觉的扰动(对抗样本)揭示了神经网络的脆弱特性.自此以后,对抗攻击迅速成为人工智能和安全领域的热门术语.许多工作都致力于披露不同深度学习模型(例如深度神经网络(DNN)、卷积神经网络(CNN)、循环神经网络(RNN))中的漏洞和提高对抗样本的健壮性<citation id="998" type="reference"><link href="709" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.另一方面,深度学习系统的大量商业部署提出了对专有资产(如训练数据<citation id="999" type="reference"><link href="711" rel="bibliography" /><link href="713" rel="bibliography" /><link href="715" rel="bibliography" /><link href="717" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>、模型参数<citation id="1000" type="reference"><link href="719" rel="bibliography" /><link href="721" rel="bibliography" /><link href="723" rel="bibliography" /><link href="725" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>)保护的要求,它引发了一场“军备竞赛”,在这场竞争中,攻击者从竞争对手那里偷取隐私信息,而相应的防御者则采取广泛的措施来抵御攻击.</p>
                </div>
                <div class="p1">
                    <p id="301">为了全面了解深度学习中的隐私和安全问题,我们对相关文献和系统进行了调查,研究了150篇左右的相关研究,跨越了图像分类、语音识别、自然语言处理和恶意软件检测4个领域.由于很难完成包罗万象的调查,所以我们选择了更具代表性的研究:例如那些在著名会议和期刊上获得发表的研究;虽然只发表在研讨会或专题讨论会上,但被引用次数高(超过50次)的研究;在公共平台上(如arXiv)最近发表的热点方向论文.基于调研工作,我们将这些攻击归纳为4类:模型提取攻击(model extrac-tion attack)、模型逆向攻击(model inversion attack)、投毒攻击(poisoning attack)和对抗攻击(adversarial attack).其中,模型提取和逆向攻击针对的是隐私,前者主要窃取模型的信息,后者主要获得训练数据集的信息;投毒攻击和对抗攻击针对的是安全,前者主要在训练阶段投放恶意数据从而降低模型的分类准确率,后者主要在预测阶段制造对抗样本来欺骗模型.</p>
                </div>
                <div class="area_img" id="302">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910003_302.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 近年来相关研究数量" src="Detail/GetImg?filename=images/JFYZ201910003_302.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 近年来相关研究数量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910003_302.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The number of publications in recent years</p>

                </div>
                <div class="p1">
                    <p id="303">图1展示了过去5年与机器学习系统安全有关的研究数据,包括对模型的各种攻击以及隐私保护、安全防御等研究.在过去的5年里,相关研究的数量急剧增长,2017年增长100%,2018年增长61.5%,近2年的文章数量占了接近70%,这也说明了深度学习、机器学习乃至人工智能领域的安全问题越来越引起人们的重视.</p>
                </div>
                <div class="p1">
                    <p id="304">图2显示了我们所研究的4类攻击的相关研究数量.其中对抗攻击是最引人注目的,对模型实施对抗攻击的研究占据了50%,它可以直接使模型判断错误,因此威胁范围很广.模型提取攻击作为近年来新兴的攻击类型,由于其奠基性(模型提取攻击获得的模型可以为其他攻击提供白盒基础),难度较大,故相关的研究数量最少,未来还有很大的研究空间.我们调研的文章主要来自人工智能社区和安全社区,其中大部分来自人工智能社区.根据发表地点来对二者区分,具体来说,ICML,CVPR,AAAI,IJCAI,TPAMI等属于人工智能社区,IEEE S&amp;P,CCS,USENIX Security,NDSS,AISec等属于安全社区.</p>
                </div>
                <div class="area_img" id="305">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910003_305.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同攻击类型的相关研究数量" src="Detail/GetImg?filename=images/JFYZ201910003_305.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同攻击类型的相关研究数量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910003_305.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Numbers of related researches on different 
 attack types</p>

                </div>
                <div class="p1">
                    <p id="306">本文主要研究机器学习安全的范围、整个学习系统基本的组成部分、攻击方法、防御措施、实用性评价以及有价值的现象与结论,主要包含4方面贡献:</p>
                </div>
                <div class="p1">
                    <p id="307">1) 攻击和防御技术的系统分析.总结了4种攻击类型和3种防御类型,全面地对机器学习系统的隐私和安全问题进行了调研和总结.</p>
                </div>
                <div class="p1">
                    <p id="308">2) 机器学习系统的模块化.对机器学习系统进行剖析,按准备过程、训练过程、预测过程的时间线,按训练数据集、训练算法、模型结构、模型参数、预测数据及结果的空间线,系统地总结了机器学习的安全知识.</p>
                </div>
                <div class="p1">
                    <p id="309">3) 各个攻击和防御类型内部具体技术的划分.对每种攻击防御类型内的攻防技术进行了剖析,将庞杂的技术文章进行分类,并分析了不同技术之间的差异和优劣.</p>
                </div>
                <div class="p1">
                    <p id="310">4) 通过对机器学习系统的安全问题的观察和总结,以及对这些攻击和防御技术的分析和研究,提出了构建安全健壮的机器学习系统和保护机器学习所有参与者隐私安全的经验和建议.</p>
                </div>
                <h3 id="311" name="311" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="312">目前已有部分文献对机器学习的攻击和防御进行了调研和评估.在早期的工作中,Barreno等人<citation id="1001" type="reference"><link href="727" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对机器学习安全性进行了调研,并对针对机器学习系统的攻击进行了分类.他们在一个统计垃圾邮件的过滤器上进行了实验,从攻击的操作方式、对输入的影响和普遍性3个维度对攻击进行了剖析.Amodei等人<citation id="1002" type="reference"><link href="729" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>介绍了机器学习中与事故风险相关的5个可能的研究问题,并根据其工作原理,以清洁机器人为例,讨论了可能的解决方法.</p>
                </div>
                <div class="p1">
                    <p id="313">Papernot等人<citation id="1003" type="reference"><link href="731" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>回顾了之前关于机器学习系统攻击和相应防御的工作.与以往的调研和综述不同,他们针对的是关于安全威胁的全面文献综述.Bae等人<citation id="1004" type="reference"><link href="733" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>总结了安全与隐私概念下关于AI的攻击与防御方法.他们在黑盒子和白盒子里检查对抗和投毒攻击.随后,Papernot等人<citation id="1005" type="reference"><link href="735" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>系统地研究了机器学习的安全性和隐私性,并提出了一种机器学习的威胁模型.他们按照训练过程和预测过程、黑盒模型和白盒模型的分类来介绍攻击方法.但他们没有过多涉及应用广泛的深度学习模型.Liu等人<citation id="1006" type="reference"><link href="737" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>主要关注机器学习的2个阶段,即训练阶段和预测阶段,并提供了较全面的文献综述.他们将相应的防御措施分为4类.另外,他们的研究更关注对抗样本导致的数据分布漂移和机器学习算法导致的敏感信息泄露等问题.</p>
                </div>
                <div class="p1">
                    <p id="314">Akhtar等人<citation id="1007" type="reference"><link href="739" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>全面研究了计算机视觉领域中深度学习受到的对抗攻击,总结了12种不同类别的攻击方法.除常用的CNN外,他们还研究了对其他模型的攻击(如自动编码器、生成模型、RNN)以及物理世界中的攻击,此外他们也总结了多种防御方法.然而,这项工作的研究内容只限于计算机视觉领域的对抗攻击.Ling等人开发的DeepSec<citation id="1008" type="reference"><link href="741" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>是一个统一的评测平台.DeepSec集成了对抗学习中16种攻击方法和13种防御方法,旨在衡量深度学习模型的脆弱性,并评估各种攻击和防御的有效性.</p>
                </div>
                <div class="p1">
                    <p id="315">本文对机器学习系统特别是深度学习中的隐私和安全问题进行调研和总结,对攻击和防御方法进行分类,分析不同类别下的攻防技术,并介绍其在图像分类、语音识别、自然语言处理和恶意软件检测等不同领域的应用.</p>
                </div>
                <h3 id="316" name="316" class="anchor-tag"><b>2 机器学习概述</b></h3>
                <h4 class="anchor-tag" id="317" name="317"><b>2.1 机器学习系统</b></h4>
                <div class="area_img" id="318">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910003_318.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 机器学习系统攻击概述" src="Detail/GetImg?filename=images/JFYZ201910003_318.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 机器学习系统攻击概述  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910003_318.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Overview of attacks in machine learning system</p>

                </div>
                <div class="p1">
                    <p id="319">有监督的机器学习主要分为2个阶段:模型训练阶段和模型预测(推理)阶段.模型训练阶段将训练数据集作为输入,最后生成模型;模型预测阶段接受用户或攻击者的输入并提供预测结果.为了完成这2个阶段,模型设计人员必须指定使用的训练数据和训练算法.模型训练阶段生成经过调优的训练模型以及相关参数.而在运行训练算法之前,传统机器学习需要人工提取和选择特征,深度学习则委托训练算法自动识别可靠而有效的特征.通常,经过训练的模型可以部署用于商业用途.在商业应用中,模型根据接收到的输入计算最可能的结果.以恶意软件检测为例,安全分析人员首先从恶意软件中收集数据(可能是原始数据),提取有代表性的特征并构建分类模型,以检测恶意软件.</p>
                </div>
                <div class="p1">
                    <p id="320">深度学习是机器学习这个广泛的家族的一部分,深度神经网络受到生物神经系统的启发,由成千上万个神经元组成,用来传递信息.深度学习受益于人工神经网络,通常使用更多的层来提取和转换特征.</p>
                </div>
                <div class="p1">
                    <p id="321">为了使机器学习系统的过程形式化,我们在表1中给出了一些符号.给定一个机器学习任务,收集的数据可以表示为<i><b>x</b></i>=(<i>x</i><sup>(1)</sup>,<i>x</i><sup>(2)</sup>,…,<i>x</i><sup>(</sup><sup><i>n</i></sup><sup>)</sup>).数据集<i>D</i>就是很多<i><b>x</b></i>组成的集合.假设<i>F</i>是一个机器学习系统,它可以根据给定的输入<i><b>x</b></i>,计算相应的结果<i><b>y</b></i>,即<i><b>y</b></i>=<i>F</i>(<i><b>x</b></i>).在模型训练过程中,使用损失函数测量对真实结果的预测误差,训练过程希望通过微调参数获得最小的误差值.损失函数可以计算为<i>L</i>=<mathml id="475"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>n</mi></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>p</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>,其中<i><b>y</b></i><sub><i>p</i></sub>表示真实结果.因此模型训练过程可以表示为</p>
                </div>
                <div class="p1">
                    <p id="322" class="code-formula">
                        <mathml id="322"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>F</mi></munder><mtext> </mtext><mi>L</mi><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="323">
                    <p class="img_tit"><b>表1 机器学习系统的符号化</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Formalization in Machine Learning System</b></p>
                    <p class="img_note"></p>
                    <table id="323" border="1"><tr><td><br />Symbol</td><td>Definition</td></tr><tr><td><br /><i>D</i></td><td>Dataset</td></tr><tr><td><br /><i>x</i><sup>(1)</sup>,<i>x</i><sup>(2)</sup>,…,<i>x</i><sup>(<i>n</i>)</sup></td><td>Input Data</td></tr><tr><td><br /><i>y</i><sup>(1)</sup>,<i>y</i><sup>(2)</sup>,…,<i>y</i><sup>(<i>n</i>)</sup></td><td>Output Result</td></tr><tr><td><br /><i>F</i></td><td>Model</td></tr><tr><td><br /><i>w</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>k</mi></msubsup></mrow></math></td><td><i>Weights Parameters</i></td></tr><tr><td><br />b<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>k</mi></msubsup></mrow></math></td><td><i>Bias Parameters</i></td></tr><tr><td><br />λ</td><td><i>Hyperparameters</i></td></tr><tr><td><br /><b><i>x</i></b><sub><i>t</i></sub></td><td>Prediction Input</td></tr><tr><td><br /><b><i>y</i></b><sub><i>t</i></sub></td><td>Prediction Output</td></tr><tr><td><br /><i>δ</i></td><td>Perturbation</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="324" name="324"><b>2.2 安全威胁</b></h4>
                <div class="p1">
                    <p id="325">图3展示了一个经典的深度学习模型在训练阶段、预测阶段的过程容易受到的威胁.最近的研究表明,机器学习系统是脆弱的,很容易受到特定攻击的影响.根据攻击目标,这些攻击可以分为4类:投毒攻击、模型提取攻击、模型逆向攻击和对抗攻击.在本节中,我们将通过示例及其正式定义来详细说明这些攻击.</p>
                </div>
                <h4 class="anchor-tag" id="686" name="686">1) 投毒攻击.</h4>
                <div class="p1">
                    <p id="326">投毒攻击主要是指在训练或再训练过程中,通过攻击训练数据集或算法来操纵机器学习模型的预测.由于在安全机器学习领域中,数据通常是非平稳的,其分布可能随时间而变化,因此一些模型不仅在训练过程中生成,而且在周期性再训练过程中随时间而变化.攻击训练数据集的方法主要包括污染源数据、向训练数据集中添加恶意样本、修改训练数据集中的部分标签、删除训练数据集中的一些原有样本等.攻击算法利用了不安全的特征选择方法或训练过程算法的弱点.投毒攻击会增加训练合适模型的难度.它还可以在生成的模型中为攻击者添加一个后门,攻击者可以使模型的预测偏向他想要的方向<citation id="1009" type="reference"><link href="743" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="687" name="687">2) 模型提取攻击.</h4>
                <div class="p1">
                    <p id="327">模型提取攻击发生在训练好的模型上,主要用于窃取模型参数及非法获取模型.它违反了训练模型的保密性.在新的业务机器学习即服务(machine learning as a service, MLaaS)设置中,模型本身托管在一个安全的云服务中,它允许用户通过基于云的预测API查询模型.模型所有者通过让用户为预测API付费来实现模型的业务价值,所以机器学习模型属于商业秘密.此外,一个模型的训练过程需要收集大量的数据集,也需要大量的时间和巨大的计算能力,所以一旦提取出模型并对其滥用,就会给模型拥有者带来巨大的经济损失.</p>
                </div>
                <h4 class="anchor-tag" id="688" name="688">3) 模型逆向攻击.</h4>
                <div class="p1">
                    <p id="328">在早期的认识中,训练数据集和训练模型之间只有一个信息流,即从数据集到模型.事实上,许多研究表明还存在一个逆向信息流,即从模型信息中恢复数据集信息,这称为模型逆向攻击.模型逆向攻击是指将训练数据集信息从模型中逆向提取出来.它主要包括成员推理攻击(membership inference attack, MIA)和属性推理攻击(property inference attack, PIA).MIA主要对数据集中是否出现特定记录进行推断,即判断隶属度,这是目前研究的热点.PIA则主要获取数据集的如性别分布、年龄分布、收入分布、患病率等属性信息.模型逆向攻击窃取了训练数据集中成员的私有信息,也损害了数据集所有者的商业价值.发生这种情况有2个原因:①不充分的隐私保护,如信息泄露<citation id="1010" type="reference"><link href="745" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>;②不安全的算法<citation id="1011" type="reference"><link href="747" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.为了加强对个人隐私的保护,欧盟于2018年颁布GDPR,它明确界定了个人资料的隐私,并对其进行严格保护<citation id="1012" type="reference"><link href="749" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="689" name="689">4) 对抗攻击.</h4>
                <div class="p1">
                    <p id="329">对抗攻击是指将对抗样例提交到训练好的模型中,从而使模型预测错误,它也被称为逃避攻击(evasion attack).对抗样本是从原来正常的样本上添加了轻微的扰动,可以导致分类模型分类错误的样本.对抗样本另外一个特点是仅造成模型分类错误,人还是可以将它正确分类.同样,在语音和文本识别领域,对抗样本也未对原文进行令人察觉的修改.在恶意软件检测领域,恶意软件作者在其软件上添加一些特殊的语句可以逃避反病毒软件的检测.</p>
                </div>
                <h3 id="330" name="330" class="anchor-tag"><b>3 隐  私</b></h3>
                <div class="p1">
                    <p id="331">隐私是信息安全领域一个普遍存在但又难以解决的问题.广义上说,隐私包括有价值的资产和数据不受窃取、推断和干预的权利.由于深度学习是建立在海量数据之上的,经过训练的模型实际上是一个数据模型,而经过训练的模型需要与来自个人的测试数据进行大量交互,因此隐私显得更加重要,也需要更强的保护.在本节中,我们将介绍深度学习系统中存在的隐私问题,并从攻击和防御2个方面介绍当前的研究现状.</p>
                </div>
                <h4 class="anchor-tag" id="332" name="332"><b>3.1 隐私问题简介</b></h4>
                <div class="p1">
                    <p id="333">从本质上讲,深度学习将大量的数据转换为一个数据模型,该数据模型可以进一步地根据输入数据预测结果,凡是涉及到数据的部分都需要关注其隐私问题.基于整个深度学习过程,我们将隐私保护的对象分类为:1)训练数据集;2)模型结构、算法和模型参数;3)预测数据与结果.</p>
                </div>
                <div class="p1">
                    <p id="334">高质量的训练数据对深度学习的表现至关重要.一般来说,训练数据的收集是一个耗时耗钱的过程:来自互联网的免费数据集通常不符合要求;从专业公司购买数据需要花费大量金钱;手工标记数据需要花费很多时间.此外,训练数据在最终传递到深度学习系统之前,还需要经过清洗、去噪和过滤等过程.因此,训练数据对于一个公司来说是至关重要的,也是非常有价值的,它的泄露意味着公司资产的损失.</p>
                </div>
                <div class="p1">
                    <p id="335">深度学习中的训练模型是一种数据模型,是训练数据的抽象表示.在现代深度学习系统中,训练阶段需要处理大量的数据和多层训练,对高性能计算和海量存储有着严格的要求.也就是说,经过训练的模型被认为是深度学习系统的核心竞争力.通常,训练模型包含3种类型的数据资产:1)模型,例如传统的机器学习和深度神经网络;2)超参数,设计了训练算法的结构如网络层数和神经元个数;3)参数,为多层神经网络中一层到另一层的计算系数.</p>
                </div>
                <div class="p1">
                    <p id="336">在这种情况下,经过训练的模型具有极其重要的商业和创新价值.一旦模型被复制、泄露或提取,模型所有者的利益将受到严重损害.在预测输入和预测结果方面,隐私来自于深度学习系统的使用者和提供者.恶意的服务提供者可能会保留用户的预测数据和结果,以便从中提取敏感信息,或者用于其他目的.另一方面,预测输入和结果可能会受到不法分子的攻击,他们可以利用这些数据来为自己创造利润.</p>
                </div>
                <h4 class="anchor-tag" id="337" name="337"><b>3.2 隐私问题研究工作</b></h4>
                <div class="p1">
                    <p id="338">为了对隐私问题提供一个全面的概述,我们调查了48篇相关的文章,21篇与破坏隐私相关的文章和27篇与保护隐私相关的文章.</p>
                </div>
                <div class="p1">
                    <p id="339">目前主流的隐私破坏方法主要有模型提取攻击(model extraction attack)和模型逆向攻击(model inversion attack).二者的主要区别是,前者关注模型的隐私信息,后者关注数据集的隐私信息.在模型提取攻击中,攻击者通过深度学习系统提供的API向模型发送大量的预测数据,然后接收模型返回的类标签和置信度系数,计算出模型的参数,最后还原原始模型.这种攻击可以破坏模型本身的隐私,损害模型所有者的利益,为攻击者创造商业价值,还可以帮助实现模型逆向攻击和对抗攻击.</p>
                </div>
                <div class="p1">
                    <p id="340">在模型逆向攻击中,攻击者通过向模型提供预测数据得到模型的置信度系数,破坏用户或数据集的隐私(例如恢复人脸识别系统中的人脸信息).如第2节所述,逆向攻击包括成员推理攻击(MIA)和属性推理攻击(PIA).在MIA中,攻击者可以推断训练数据集中是否包含特定的记录.在PIA中,攻击者可以推测训练数据集中是否存在一定的统计特征.最近的研究发现,在人口训练数据集中,某些阶层的人(如妇女和少数民族)的样本代表性不足,会影响最终模型的表现<citation id="1013" type="reference"><link href="751" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>.模型逆向攻击表明,信息不仅可以从数据集流向模型和预测结果,还可以从模型和预测结果反向流向数据集.</p>
                </div>
                <div class="p1">
                    <p id="341">现实中存在很多隐私风险,因此隐私保护是深度学习的关键.在训练过程中,用户不能自动删除公司收集的数据,不能控制自己如何使用数据,甚至不知道是否从数据中学习到了敏感信息.用户还承担着公司存储的数据被其他部门合法或非法访问的风险.在推理过程中,他们的预测数据和结果也会受到影响.模型提供者需要保护他们的模型和数据集不被公开.</p>
                </div>
                <div class="p1">
                    <p id="342">在实施方面,隐私保护可以分为4种技术:1)差分隐私(DP-differential privacy)<citation id="1014" type="reference"><link href="717" rel="bibliography" /><link href="753" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">24</a>]</sup></citation>;2)同态加密(HE-homomorphic encryption)<citation id="1015" type="reference"><link href="755" rel="bibliography" /><link href="757" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>;3)安全多方计算(SMC-secure multi-party computation)<citation id="1016" type="reference"><link href="759" rel="bibliography" /><link href="761" rel="bibliography" /><sup>[<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>;4)次优选择(SC-suboptimal choice)<citation id="1017" type="reference"><link href="721" rel="bibliography" /><link href="763" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">29</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="343">差分隐私是密码学中的一种手段,旨在最大限度地提高数据查询的准确性,同时尽可能减少从统计数据库<citation id="1018" type="reference"><link href="765" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>查询时识别其记录的机会.它主要通过删除个体特征并保留统计特征的方式来保护用户隐私.Dwork等人<citation id="1019" type="reference"><link href="767" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>首先提出了严格的数学定义,称为<i>ε</i>-indistinguishability和<i>δ</i>-approximate <i>ε</i>-indistinguishability,后来分别被称为<i>ε</i>-差分隐私和(<i>ε</i>,<i>δ</i>)-差分隐私.由于差分隐私在数据库中的应用,在深度学习中它经常被用来保护训练数据集的隐私.</p>
                </div>
                <div class="p1">
                    <p id="344">同态加密是一种关注数据处理的加密技术,最早由Rivest在20世纪70年代提出,包括加法同态加密和乘法同态加密.Gentry在2009年首次设计了一个真正的全同态加密方案.同态加密是这样一种加密函数:对明文进行环上的加法和乘法运算,然后对其进行加密,和先对明文进行加密,再对密文进行相应的运算,可以得到等价的结果,即<i>En</i>(<i>x</i>)♁<i>En</i>(<i>y</i>)=<i>En</i>(<i>x</i>+<i>y</i>).在深度学习中,同态加密通常被用来保护用户的预测数据和结果.一些工作也保留了训练模型的隐私.用户加密他们的数据并以加密的形式将其发送到MLaaS中,云服务将其应用于模型进行加密预测,然后以加密的形式返回给用户.</p>
                </div>
                <div class="p1">
                    <p id="345">安全多方计算主要是为了在没有可信第三方的情况下,保证约定函数的安全计算,这始于百万富翁的问题.它主要采用的技术包括多方计算、加密电路和不经意传输.在深度学习过程中,其应用场景是多个数据方希望使用多个服务器对其联合数据进行模型训练.它们要求任何数据方或服务器不能从该过程中的任何其他数据方了解训练数据.安全多方计算可以保护训练数据集和训练模型.</p>
                </div>
                <div class="p1">
                    <p id="346">与上述3种系统保护技术不同,次优选择是一种独特的保护方法.该方法易于实现,且具有较低的时间成本,但其效果尚未经过大规模实践的检验.例如,为防止盗窃模型参数,一些研究人员可能对模型参数进行四舍五入处理<citation id="1020" type="reference"><link href="763" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>,将噪声添加到类概率<citation id="1021" type="reference"><link href="721" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,拒绝特征空间里的异常请求<citation id="1022" type="reference"><link href="725" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,返回第2或第3类的最大概率<citation id="1023" type="reference"><link href="719" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等.所有这些方法都在一定程度上失去了一些准确性,从而换取隐私保护的改善.因此,在应用这些防御技术之前,需要仔细考虑得失之间的平衡.</p>
                </div>
                <div class="p1">
                    <p id="347">综上所述,我们将这些攻击目标与攻击方法和防御方法相关联.模型逆向攻击通常获取训练数据集的信息.模型提取攻击针对训练好的模型.预测数据和结果在传输过程中容易受到窃取等传统攻击.此外,差分隐私通常保护训练数据集,同态加密模型和预测数据,安全多方计算在训练过程中保护数据集和模型,次优选择主要针对训练模型.</p>
                </div>
                <h4 class="anchor-tag" id="348" name="348"><b>3.3 攻击方法</b></h4>
                <div class="p1">
                    <p id="349">本节详细介绍3种隐私攻击的技术方法.</p>
                </div>
                <h4 class="anchor-tag" id="350" name="350">3.3.1 模型提取攻击</h4>
                <div class="p1">
                    <p id="351">模型提取攻击破坏了模型本身的隐私,攻击者试图窃取模型的参数和超参数.目前主流方法通过构建精确模型或相似模型来实现模型的提取.精确模型是指攻击者试图重建原始模型,或从原始模型计算参数或超参数;而相似模型是攻击者构建的一个在预测性能上相近的替代模型.窃取精确模型会损害模型所有者的核心商业资产,并为攻击者获取价值,而窃取相似模型通常用于生成可迁移的对抗样本.众所周知,对抗样本对深度学习是一个不小的威胁,但如果攻击者对模型一无所知,则很难生成可靠的对抗样本.通过发动模型提取攻击,攻击者以某种方式提取到原始模型、参数或结构,便可以利用这些知识来确定决策边界,从而生成相应的反例.</p>
                </div>
                <div class="p1">
                    <p id="352">模型提取攻击的研究大多是在黑盒模型下进行的,在黑盒模型下只能得到训练模型的算法.攻击者通常构造特殊的输入,向预测API提交查询,并接收输出,获得许多输入输出对.由于训练后的机器学习模型本质上是一个函数,因此只要攻击者获得足够的输入输出对并有足够的时间,从理论上就可以恢复模型参数.实际上,攻击者需要做的是利用模型特性来生成包含更多信息的样本,以减少查询个数的需求和时间成本,有时甚至要牺牲一些准确性.</p>
                </div>
                <h4 class="anchor-tag" id="690" name="690">1) 精确模型.</h4>
                <div class="p1">
                    <p id="353">在模型的精确参数重构中,方程求解攻击方法在机器学习模型中具有良好的效果.Tramèr 等人<citation id="1024" type="reference"><link href="769" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>介绍了一种通过预测API提取模型的方法.他们通过发送大量的查询建立了模型方程,并得到了相应的预测结果.但该方法仅适用于决策树、逻辑回归、简单神经网络等特定的机器学习模型,不适用于DNN.Wang等人<citation id="1025" type="reference"><link href="763" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>试图在已知模型算法和训练数据的前提下窃取超参数.超参数在文中称为<i>λ</i>,用于平衡目标函数中的损失函数和正则化项.由于训练过程要求目标函数最小,所以目标函数在模型参数处的梯度为0.根据这个性质,攻击者可以通过对模型的查询得到很多线性方程,即超参数、模型参数和输入数据之间的关系.最后,利用线性最小二乘法对超参数进行估计.Baluja等人<citation id="1026" type="reference"><link href="771" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>训练了一个名为元模型(meta model)的分类器来预测模型属性.攻击者将查询输入提交给目标模型,并将目标模型提供的输出作为元模型的输入,然后元模型尝试输出目标模型的属性.元模型可以推断系统架构、操作方法、训练数据集大小等信息.</p>
                </div>
                <h4 class="anchor-tag" id="691" name="691">2) 相似模型.</h4>
                <div class="p1">
                    <p id="354">相似模型只要求在模型的表现上与原模型近似,主要用于生成对抗样本等.Papernot等人<citation id="1027" type="reference"><link href="773" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>试图生成可迁移的、无目标的对抗样本.攻击者利用基于雅可比矩阵的数据集增强(Jacobian-based dataset augmentation, JbDA)技术生成合成样本来查询目标模型,并建立了一个近似于目标模型决策边界的攻击模型.然后攻击者利用攻击模型生成对抗样本,由于可移植性,这些样本会被目标模型误分类.Juuti等人<citation id="1028" type="reference"><link href="719" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过对DNN训练的正则化和对JbDA的一般化,提出了一种新的合成数据生成方法,生成了对抗样本.经过扩展后的JbDA技术在生成可迁移的有针对性的对抗样本和复制预测行为方面具有较高的效率.考虑到不同模型之间的差异,Papernot他们<citation id="1029" type="reference"><link href="775" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>还发现,关于目标模型体系结构的知识是不必要的,因为任何机器学习模型都可以用更复杂的模型来代替,比如DNN.</p>
                </div>
                <h4 class="anchor-tag" id="355" name="355">3.3.2 成员推理攻击</h4>
                <div class="p1">
                    <p id="356">Truex等人<citation id="1030" type="reference"><link href="777" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>提出了MLaaS平台中成员推理攻击(membership inference attack, MIA)的一种通用的系统方案.给定实例<i>x</i>和对在数据集<i>D</i>上训练的分类模型<i>F</i><sub><i>t</i></sub>的黑盒访问权,当训练<i>F</i><sub><i>t</i></sub>时,对手是否能够在<i>D</i>中很有信心地推断实例<i>x</i>是否包含在<i>D</i>中.在MIA中,对手更关心<i>x</i>是否在<i>D</i>中,而不是<i>x</i>的内容.目前成员推理攻击可以通过3种方法实现:</p>
                </div>
                <div class="p1">
                    <p id="357">1) 训练攻击模型.攻击模型是一个二元分类器,用来推断目标记录的信息.它将成员推理问题转化为分类问题,可用于白盒和黑盒攻击.很多研究还引入了影子模型来训练攻击模型,影子模型主要用来模拟目标模型,并生成攻击模型所需的数据集.当然,对影子模型的训练也会增加攻击代价.</p>
                </div>
                <div class="p1">
                    <p id="358">Shokri等人<citation id="1031" type="reference"><link href="779" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>利用机器学习中的API调用,设计、实现并评估了黑盒模型的MIA攻击方法.他们生成了类似于目标训练数据集的数据集,并使用相同的MLaaS来训练影子模型.这些数据集是通过基于模型的综合、基于统计的综合、有噪声的真实数据等方法得到的.使用影子模型为攻击模型提供训练集,训练集输入是某个记录的类标签、预测向量.输出是该记录是否属于影子模型训练集.训练好的攻击模型以类标签和预测向量作为输入,输出该记录是否在目标训练集中.Salem等人<citation id="1032" type="reference"><link href="781" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>放宽了文献<citation id="1033" type="reference">[<a class="sup">37</a>]</citation>中的部分约束条件 (要在同一MLaaS上训练影子模型,影子模型和目标模型的数据集具有相同分布),并在没有目标模型的知识结构和训练数据集分布的情况下只使用一个影子模型.攻击模型以模型概率向量输出的前3个最大值作为输入确定隶属度.</p>
                </div>
                <div class="p1">
                    <p id="359">Pyrgelis等人<citation id="1034" type="reference"><link href="783" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>实现了在聚合位置数据上的MIA.其主要思想是利用先验位置信息,通过具有识别功能的可识别博弈过程进行攻击.他们训练了一个分类器(即攻击模型)作为识别函数来确定数据是否在目标数据集中,无需影子模型.</p>
                </div>
                <div class="p1">
                    <p id="360">2) 概率信息计算.该方法利用概率信息推断隶属度,无需攻击模型.举例来说,假设<i>a</i>,<i>b</i>都属于类别<i>A</i>,其中<i>a</i>属于训练数据集而<i>b</i>不属于,由于<i>a</i>参与了训练过程,模型可能以0.9的置信概率将<i>a</i>分类为<i>A</i>;考虑<i>b</i>,由于它对模型而言是新出现的,尽管模型也能将<i>b</i>分类为<i>A</i>,但可能只有0.6的置信概率.于是可以根据模型返回的最大类概率实施攻击.但这种方法需要一定的前提假设和辅助信息来获得可靠的概率向量或二元结果,这也是该方法在使用时的一个限制条件.</p>
                </div>
                <div class="p1">
                    <p id="361">Fredrikson等人<citation id="1035" type="reference"><link href="785" rel="bibliography" /><sup>[<a class="sup">40</a>]</sup></citation>试图根据概率信息来构造某一数据是否出现在目标训练数据集中的概率.然后寻找概率最大的输入数据,得到的数据与目标训练数据集中的数据相似.Salem等人<citation id="1036" type="reference"><link href="781" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>中的第3种攻击方法只需要记录通过目标模型输出的概率向量,并使用统计测量方法比较最大分类概率是否超过一个阈值,若超过则认为该记录属于数据集.Long等人<citation id="1037" type="reference"><link href="787" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>提出了广义MIA方法,与文献<citation id="1038" type="reference">[<a class="sup">37</a>]</citation>不同,它更容易攻击非过拟合数据.他们训练了大量类似于目标模型的参考模型(类似影子模型),根据参考模型的输出的概率信息选择易受攻击的数据,然后将目标模型和参考模型的输出进行比较,计算出数据属于目标训练数据集的概率.</p>
                </div>
                <div class="p1">
                    <p id="362">3) 相似样本生成.该方法通过训练生成的模型(如生成对抗网络(GAN))生成训练记录,其生成的样本与目标训练数据集的样本相似.通过提高生成样本的相似度将使该方法更加有效.</p>
                </div>
                <div class="p1">
                    <p id="363">Liu等人<citation id="1039" type="reference"><link href="789" rel="bibliography" /><sup>[<a class="sup">42</a>]</sup></citation>和Hayes等人<citation id="1040" type="reference"><link href="791" rel="bibliography" /><sup>[<a class="sup">43</a>]</sup></citation>都探索了攻击生成模型的方法,不同于判别模型,生成模型通常用于学习数据的分布并生成相似的数据.文献<citation id="1041" type="reference">[<a class="sup">42</a>]</citation>提出一种白盒攻击,用于单成员攻击和联合成员攻击.其基本思想是用目标模型训练生成的模型,以目标模型的输出为输入,以相似的目标模型输入为输出.训练后,攻击模型可以生成与目标训练数据集相似的数据.考虑到文献<citation id="1042" type="reference">[<a class="sup">37</a>]</citation>中的方法难以攻击CNN,Hitaj等人<citation id="1043" type="reference"><link href="745" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出了一种更为通用的MIA方法,在协作深度学习模型的场景中执行了白盒攻击.他们构建了一个目标分类模型生成器,并利用该生成器形成了一个GAN.经过训练后,GAN可以生成与目标训练集相似的数据,但是这种方法的局限性在于,属于同一分类的所有样本都需要在视觉上相似,因此无法在同一个类别下区分它们.</p>
                </div>
                <h4 class="anchor-tag" id="364" name="364">3.3.3 属性推理攻击</h4>
                <div class="p1">
                    <p id="365">属性推理攻击(property inference attack, PIA)是指对训练数据集的统计属性进行推理.推理的属性主要是一些统计信息,例如人口数据集中男女比例是否均衡、人口样本中是否存在少数民族样本、医疗数据集中患癌病人的比重等.</p>
                </div>
                <div class="p1">
                    <p id="366">Ateniese等人<citation id="1044" type="reference"><link href="793" rel="bibliography" /><sup>[<a class="sup">44</a>]</sup></citation>首先提出了一种训练元分类器的白盒攻击方法.分类器以模型的特征信息作为输入,以训练该模型的数据集中是否包含特定属性为输出.他们还训练影子模型来为元分类器提供训练数据.由于他们主要提取机器学习模型的特征信息,这种方法在DNN上并不奏效.为了解决这个问题,Ganju等人<citation id="1045" type="reference"><link href="795" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>构建了一个元分类器模型,该模型研究了如何提取DNN的特征值,使其作为元分类器的输入,其他部分与文献<citation id="1046" type="reference">[<a class="sup">44</a>]</citation>非常相似.</p>
                </div>
                <div class="p1">
                    <p id="367">另外,针对文献<citation id="1047" type="reference">[<a class="sup">20</a>]</citation>中存在的不足,Melis等人<citation id="1048" type="reference"><link href="797" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>提出了一种协作式学习的白盒攻击方法.其理论基础是,深度学习模型会记住太多数据特征<citation id="1049" type="reference"><link href="747" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.攻击者可以多次下载最新的模型,得到每个阶段的更新模型,减去不同阶段的聚合更新,并分析更新的信息来推断成员和属性.他们训练了一个二元分类器来判断数据集的属性,该分类器使用更新的梯度值作为输入.</p>
                </div>
                <h4 class="anchor-tag" id="368" name="368"><b>3.4 防御方法</b></h4>
                <div class="p1">
                    <p id="369">为了保护深度学习系统的隐私,一系列研究工作开发了不同的防御机制.基于对27篇文章的研究,我们将这些防御机制分为4类:差分隐私、同态加密、安全多方计算和次优选择.</p>
                </div>
                <h4 class="anchor-tag" id="370" name="370">3.4.1 差分隐私</h4>
                <div class="p1">
                    <p id="371">差分隐私是一种密码学工具,旨在最大限度地提高数据查询的准确性,同时最大限度地减少查询统计数据库时识别其记录的机会.基于保护目标,差分隐私的方法可以从输出扰动、目标扰动和梯度扰动等方面进行扩展<citation id="1050" type="reference"><link href="753" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>.这些方法分别指将随机扰动加到输出上、目标函数上和反向传播的梯度上.</p>
                </div>
                <div class="p1">
                    <p id="372">Chaudhuri等人<citation id="1051" type="reference"><link href="799" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>首先提出了输出和目标扰动,严格证明了凸损失函数机器学习模型中保持隐私,并将其实现为正则逻辑回归.输出扰动包括以边界灵敏度和增加灵敏度为基础的噪声训练模型.而Wang等人<citation id="1052" type="reference"><link href="753" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>表明,在非光滑条件下,输出扰动不能推广.Zhang等人提出<citation id="1053" type="reference"><link href="801" rel="bibliography" /><sup>[<a class="sup">48</a>]</sup></citation>,在强凸的情况下,可以使用适当的学习率来提高操作速度和实用性.目标扰动是训练包含随机项的目标函数最小化的模型,它在理论上和经验上都优于输出扰动<citation id="1054" type="reference"><link href="803" rel="bibliography" /><sup>[<a class="sup">49</a>]</sup></citation>,但在实践中很难得到既保证隐私又保证效用的最优解.为了获得更好的性能或支持其他场景,Kifer等人<citation id="1055" type="reference"><link href="805" rel="bibliography" /><sup>[<a class="sup">50</a>]</sup></citation>通过选择高斯分布代替伽马分布提高了精度,并引入了第一个用于高维稀疏回归的差分隐私算法;文献<citation id="1056" type="reference">[<a class="sup">51</a>]</citation>和文献<citation id="1057" type="reference">[<a class="sup">50</a>]</citation>给出了Lipschitz损失函数的算法和证明.</p>
                </div>
                <div class="p1">
                    <p id="373">Song等人<citation id="1058" type="reference"><link href="809" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>提出了梯度扰动,其主要思想是在每次迭代更新参数时添加噪声.该方法不受强凸函数或强摄动优化问题的限制,在实际应用中具有一定的优越性.然而,由于随机梯度下降(SGD)或梯度下降(GD)的计算过程非常耗时,如果数据集很大,计算可能会花费很多时间.对于强凸前提,Bassily等人<citation id="1059" type="reference"><link href="811" rel="bibliography" /><sup>[<a class="sup">53</a>]</sup></citation>和Talwar等人<citation id="1060" type="reference"><link href="807" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>放宽了对Lipschitz凸函数的限制和严格的误差边界.然后,Abadi等人<citation id="1061" type="reference"><link href="717" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>处理了非凸目标函数,并在适度隐私损失的情况下以适中的成本训练DNN.他们对DP-SGD进行了修改和扩展,允许不同层的限幅阈值和噪声尺度不同.随后,Zhang等人<citation id="1062" type="reference"><link href="801" rel="bibliography" /><sup>[<a class="sup">48</a>]</sup></citation>首先给出了非凸优化问题的理论结果.文献<citation id="1063" type="reference">[<a class="sup">24</a>]</citation>实现了满足Polyak-Lojasiewicz条件的非凸情况,并产生了更紧致的上界.Zhang等人<citation id="1064" type="reference"><link href="813" rel="bibliography" /><sup>[<a class="sup">54</a>]</sup></citation>与其他算法相结合,在分布式ERM中也应用了梯度扰动.</p>
                </div>
                <div class="p1">
                    <p id="374">Hamm等人<citation id="1065" type="reference"><link href="815" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>提出了一种使用局部分类器构造全局差分私有分类器的方法,该方法不需要访问任何一方的私有数据.Hynes等人<citation id="1066" type="reference"><link href="817" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>提出了一种深度学习框架Myelin,用于在可信硬件领域实现高效的、私有的、数据无关的实际深度学习模型.</p>
                </div>
                <div class="p1">
                    <p id="375">目前已有一些度量标准被用来估计隐私风险.基于差分隐私的可组合性,最简单的度量方法是计算隐私消耗的总和<citation id="1067" type="reference"><link href="819" rel="bibliography" /><sup>[<a class="sup">57</a>]</sup></citation>.然而,直接将它们相加可能会得到松散的测量边界.文献<citation id="1068" type="reference">[<a class="sup">6</a>]</citation>提出了一个更强的方法,主要采用标准Markov不等式来跟踪隐私损失,它在经验上获得了更严格的隐私损失约束.但上述指标仅限于DP框架,Long等人<citation id="1069" type="reference"><link href="821" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>提出了差分训练隐私(DTP),可以测量不使用DP的分类器的隐私风险.</p>
                </div>
                <h4 class="anchor-tag" id="376" name="376">3.4.2 同态加密</h4>
                <div class="p1">
                    <p id="377">一般的加密方案侧重于数据存储的安全性,而同态加密(HE)侧重于数据处理的安全性.HE通常是在有数据泄漏风险中使用的.由于解密的高度复杂性,HE可以有效地保护敏感数据不被解密和窃取.在深度学习中,它主要用于保护预测输入和结果,训练神经网络模型等.应用HE的主要负面影响是效率的降低,即错误传输问题、对密文的操作时间较长、加密后数据量急剧增加等.</p>
                </div>
                <div class="p1">
                    <p id="378">Liu等人<citation id="1070" type="reference"><link href="823" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>提出了MiniONN,这是一个支持隐私保护的神经网络,并确保服务器不了解输入,客户端不了解模型.其主要思想是允许服务器和客户端为神经网络的每一层额外地共享输入和输出值.Jiang等人<citation id="1071" type="reference"><link href="757" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>给出了一种矩阵和密码矩阵算术运算的实用算法.Phong等人<citation id="1072" type="reference"><link href="713" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一个隐私保护DL系统,使用异步随机梯度下降应用于神经网络连接深度学习和密码学,并结合加性HE.在其他方面,Hesamifard 等人<citation id="1073" type="reference"><link href="825" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>开发了CryptoDL,用于在加密数据上运行DNN, 在CIFAR-10上的准确率为91.5%.他们在CNN中利用低次多项式设计一个近似函数,然后用近似多项式代替原始的激活函数来训练CNN,最后在加密数据上实现CNN.</p>
                </div>
                <h4 class="anchor-tag" id="379" name="379">3.4.3 安全多方计算</h4>
                <div class="p1">
                    <p id="380">在现实中,经常会遇到多个数据方希望在一台服务器上共同学习一个模型的场景.然而,每个数据方都不愿意将自己的数据共享给其他方.在多方数据只有一台服务器情况下,Shokri等人<citation id="1074" type="reference"><link href="827" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>实现了一个系统,它允许多方在非共享输入数据集的情况下共同学习模型.各方都可以独立使用最终模型.在训练过程中,各个数据方对其局部数据集进行模型训练,再将所选参数的关键梯度上传到全局参数库,然后下载所需参数的最新值.基于这些性质,他们采用分布式选择性SGD方法来选择参数:1)梯度下降过程中不同参数的更新具有内在的独立性;2)不同的训练数据集对参数的贡献不同.Phong等人<citation id="1075" type="reference"><link href="713" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>基于文献<citation id="1076" type="reference">[<a class="sup">61</a>]</citation>进行了改进.每个数据方上传经过加法HE加密的梯度,并对模型应用异步SGD.另外,Phong等人<citation id="1077" type="reference"><link href="829" rel="bibliography" /><sup>[<a class="sup">62</a>]</sup></citation>还提出了服务器辅助网络拓扑和全连接网络拓扑系统.各方共享神经网络的权值而不是梯度.他们不仅可以防范恶意的服务器,而且可以在即使只有一个诚实方的情况下防范数据方合谋.</p>
                </div>
                <div class="p1">
                    <p id="381">在多方计算中另一个场景是,数据方不希望将所有训练数据交给一台服务器来训练模型.他希望将数据集分布到多个服务器,共同训练模型,每个服务器不会了解其他服务器的训练数据.SecureML<citation id="1078" type="reference"><link href="759" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>是一种保护隐私的双服务器模型协议.数据所有者将私有数据分配给2个非合谋服务器,并用安全的两方计算技术训练联合数据,支持安全的算术运算.采用了不经意传输和加密电路,并采用了面向多方计算友好的激活函数.Liu等人<citation id="1079" type="reference"><link href="823" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>提出了一种支持隐私保护的神经网络MiniONN.它确保服务器对输入一无所知,而客户机对模型一无所知.其主要思想是允许服务器和客户端额外地共享神经网络每一层的输入和输出值.</p>
                </div>
                <div class="p1">
                    <p id="382">多方计算更一般化的场景是有<i>M</i>个数据方希望使用<i>N</i>台服务器对他们的联合数据进行模型训练.要求任何数据方或服务器对任何其他数据方的训练数据一无所知. 在SecureNN<citation id="1080" type="reference"><link href="761" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>里,<i>N</i>=3或4,<i>M</i>可以是任意值.此外,经过训练的模型作为一个秘密共享,并对任何单个服务器或数据端隐藏.这些秘密共享可以由服务器或任何其他方组合起来重构模型.</p>
                </div>
                <h4 class="anchor-tag" id="383" name="383">3.4.4 次优选择</h4>
                <div class="p1">
                    <p id="384">为了抵御模型提取攻击,许多研究都试图在一定程度上向用户提供次优模型.Tramèr等人<citation id="1081" type="reference"><link href="769" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>提出了第一个量化模型提取攻击预测概率的防御方法.他们只允许攻击者提取给定的类标签,而不提供置信度评分,或者提供四舍五入的置信度.该方法减少了向攻击者提供的信息量,但也减少了合法的服务.后来文献<citation id="1082" type="reference">[<a class="sup">7</a>]</citation>表明,即使不使用预测概率,模型提取攻击也是有效的.但是Lee等人<citation id="1083" type="reference"><link href="721" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>发现,在类概率中注入噪声仍然可以延长攻击时间.攻击者被迫放弃概率信息,只使用标签信息,这大大增加了查询数量和攻击时间.Wang等人<citation id="1084" type="reference"><link href="763" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>发现对模型参数进行四舍五入会增加攻击者对超参数攻击的估计误差.不幸的是,该误差对测试性能的影响可以忽略不计.</p>
                </div>
                <div class="p1">
                    <p id="385">还有一种方法是从用户提交的查询请求中发现异常.Kesarwani等人<citation id="1085" type="reference"><link href="725" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>依赖于记录客户端发出的所有请求,并计算正常请求组成的特征空间.当检测到新的请求空间超过预定阈值时,认为模型提取攻击发生.因此,他们需要在输入中对预测类进行线性分离来评估特征空间.此外,PRADA<citation id="1086" type="reference"><link href="719" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是基于给定客户提交的样本分布的突然变化检测攻击,假设是攻击者提交的样本中的特征分布比良性查询中更不稳定.一旦PRADA检测到攻击,根据目标模型的预测,以最大概率返回第2类或第3类分类.PRADA在检测对文献<citation id="1087" type="reference">[<a class="sup">34</a>]</citation>的攻击时需要数百个查询,对文献<citation id="1088" type="reference">[<a class="sup">32</a>]</citation>的攻击需要数千个查询.</p>
                </div>
                <h4 class="anchor-tag" id="386" name="386">3.4.5 其他方法</h4>
                <div class="p1">
                    <p id="387">Xu等人<citation id="1089" type="reference"><link href="831" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>将数据清洗以保护隐私.他们将原始数据用密码加密后发送给服务商.为了保护MLaaS中数据集的隐私,Zhang等人<citation id="1090" type="reference"><link href="715" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>引入了一个混淆函数,并将其输入到模型训练任务中.混淆函数向现有样本添加随机噪声,或使用新样本增强数据集.因此,关于单个样本的特征或一组样本的统计特性的敏感信息是隐藏的.Nasr等人<citation id="1091" type="reference"><link href="711" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>设计了一个min-max游戏,它在最小化模型的预测损失的同时,最大化推理攻击的收益,目标是共同最大化隐私和预测精度.</p>
                </div>
                <div class="p1">
                    <p id="388">Cao等人<citation id="1092" type="reference"><link href="833" rel="bibliography" /><sup>[<a class="sup">64</a>]</sup></citation>提出了机器学习去除的思想,目标使机器学习模型完全忘记一段训练数据,并去除其对模型和特征的影响.它们将训练数据样本转化为一种求和形式,用来快速计算新模型.Hunt等人<citation id="1093" type="reference"><link href="835" rel="bibliography" /><sup>[<a class="sup">65</a>]</sup></citation>提出一个保护SGX上的MLaaS隐私的系统.它向服务运营商隐藏训练数据,既不向用户显示算法也不显示模型结构,只提供对训练模型的黑盒访问.Ohrimenko等人<citation id="1094" type="reference"><link href="837" rel="bibliography" /><sup>[<a class="sup">66</a>]</sup></citation>针对支持向量机、神经网络、决策树和<i>K</i>-means聚类问题,提出了一种基于Intel Skylake处理器的数据无关机器学习算法.</p>
                </div>
                <h3 id="389" name="389" class="anchor-tag"><b>4 安  全</b></h3>
                <h4 class="anchor-tag" id="390" name="390"><b>4.1 安全问题简介</b></h4>
                <div class="p1">
                    <p id="391">安全与隐私在很多方面是密不可分的,但在这里,我们需要在人工智能领域区分安全与隐私问题.深度学习模型的形成依赖于对大量数据进行耗时耗力的训练.直观地说,训练数据、训练模型和预测输入都是所有者私有的,值得保护.众所周知,人工智能系统中已经存在着隐私研究对象,例如所收集的训练数据集、训练模型的参数、用户准备提交的预测数据以及模型返回的结果.要保护系统中原本存在的合法数据(模型参数、数据集等),就是隐私问题.</p>
                </div>
                <div class="p1">
                    <p id="392">然而,在人工智能系统中,造成安全问题的恶意样本通常是未知的.例如投毒攻击将恶意数据添加到训练数据集中,会对深度学习的预测产生负面影响.这些恶意样本不应该存在于其中.如何抵御这样的未知样本就是一个安全问题.此外,受到攻击的分类模型在训练过程中不会接触到这些对抗样本,这些恶意数据原本不在学习模型中.要防范系统中原本不存在的、可能引起模型出错的恶意数据,就是安全问题.</p>
                </div>
                <h4 class="anchor-tag" id="393" name="393"><b>4.2 安全问题研究工作</b></h4>
                <div class="p1">
                    <p id="394">在深度学习系统中,训练数据集和预测数据需要与用户交互,而训练过程和训练模型一般是封闭的.因此,训练数据集和预测数据更容易受到未知恶意样本的攻击.更具体地说,如果在训练数据集中出现恶意样本,我们称之为投毒攻击;如果在预测数据中出现恶意样本,我们称之为对抗攻击.我们共调查了89篇相关论文,其中15篇与投毒攻击相关,11篇与投毒防御相关,36篇与对抗攻击相关,27篇与对抗防御相关.</p>
                </div>
                <div class="p1">
                    <p id="395">投毒攻击在训练过程中添加恶意样本,从而影响生成的模型.大多数恶意样本搜索方法都是通过发现算法或训练过程的漏洞来实现的.早期的机器学习算法也容易受到投毒攻击<citation id="1098" type="reference"><link href="839" rel="bibliography" /><link href="841" rel="bibliography" /><link href="843" rel="bibliography" /><sup>[<a class="sup">67</a>,<a class="sup">68</a>,<a class="sup">69</a>]</sup></citation>.投毒攻击主要在2个方面影响了正常模型.1)直接改变分类器的决策边界,破坏分类器的正常使用,使其不能正确地对正常样本进行分类,破坏了模型的可用性.这主要是通过错误标记数据实现的.攻击者使用错误的标签提交数据记录,或恶意修改训练数据集中现有数据的标签.2)在分类器中创建后门.它能正确地对正常样本进行分类,但会导致对特定数据的分类错误.攻击者可以通过后门进行有针对性的攻击,破坏模型的完整性.这主要是通过加入特定的数据实现的.它们向数据集提交包含特定特征(如水印)和标签的数据,而在其他数据记录中很可能没有这样的特征.此外,他们还可以直接攻击特征选择算法<citation id="1095" type="reference"><link href="845" rel="bibliography" /><sup>[<a class="sup">70</a>]</sup></citation>.相应地,防御方法主要是通过增强训练算法<citation id="1096" type="reference"><link href="847" rel="bibliography" /><sup>[<a class="sup">71</a>]</sup></citation>的鲁棒性和保护数据集<citation id="1097" type="reference"><link href="849" rel="bibliography" /><sup>[<a class="sup">72</a>]</sup></citation>的安全性来实现的.</p>
                </div>
                <div class="p1">
                    <p id="396">在预测过程中,对抗攻击会对正常样本增加恶意干扰.对抗样本既要欺骗分类器,又要让人无法察觉.该攻击广泛应用于图像识别领域<citation id="1116" type="reference"><link href="851" rel="bibliography" /><link href="853" rel="bibliography" /><link href="855" rel="bibliography" /><link href="857" rel="bibliography" /><sup>[<a class="sup">73</a>,<a class="sup">74</a>,<a class="sup">75</a>,<a class="sup">76</a>]</sup></citation>,也用于语音处理<citation id="1099" type="reference"><link href="859" rel="bibliography" /><sup>[<a class="sup">77</a>]</sup></citation>、语音到文本转换<citation id="1100" type="reference"><link href="861" rel="bibliography" /><sup>[<a class="sup">78</a>]</sup></citation>、文本识别<citation id="1101" type="reference"><link href="863" rel="bibliography" /><sup>[<a class="sup">79</a>]</sup></citation>、恶意软件检测<citation id="1102" type="reference"><link href="865" rel="bibliography" /><sup>[<a class="sup">80</a>]</sup></citation>等.目前,主流方法寻找扰动包括FGSM<citation id="1103" type="reference"><link href="851" rel="bibliography" /><sup>[<a class="sup">73</a>]</sup></citation>,JSMA<citation id="1104" type="reference"><link href="853" rel="bibliography" /><sup>[<a class="sup">74</a>]</sup></citation>,C&amp;W<citation id="1105" type="reference"><link href="855" rel="bibliography" /><sup>[<a class="sup">75</a>]</sup></citation>,DeepFool<citation id="1106" type="reference"><link href="857" rel="bibliography" /><sup>[<a class="sup">76</a>]</sup></citation>,UAP<citation id="1107" type="reference"><link href="867" rel="bibliography" /><sup>[<a class="sup">81</a>]</sup></citation>,ATN<citation id="1108" type="reference"><link href="771" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>和一些变种.也有一些研究攻击了CNN,DNN之外的其他深度学习模型,甚至在现实世界中产生了对抗的实例.防御策略主要从对抗样本的生成和攻击的过程进行考虑,包括对抗训练<citation id="1109" type="reference"><link href="869" rel="bibliography" /><sup>[<a class="sup">82</a>]</sup></citation>、基于区域的分类<citation id="1110" type="reference"><link href="871" rel="bibliography" /><sup>[<a class="sup">83</a>]</sup></citation>、输入变化<citation id="1111" type="reference"><link href="873" rel="bibliography" /><sup>[<a class="sup">84</a>]</sup></citation>、梯度正则化<citation id="1112" type="reference"><link href="875" rel="bibliography" /><sup>[<a class="sup">85</a>]</sup></citation>、蒸馏<citation id="1113" type="reference"><link href="877" rel="bibliography" /><sup>[<a class="sup">86</a>]</sup></citation>、数据处理<citation id="1114" type="reference"><link href="879" rel="bibliography" /><sup>[<a class="sup">87</a>]</sup></citation>和训练防御网络<citation id="1115" type="reference"><link href="881" rel="bibliography" /><sup>[<a class="sup">88</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="397" name="397"><b>4.3 投毒攻击</b></h4>
                <div class="p1">
                    <p id="398">投毒攻击试图通过污染训练数据来降低深度学习系统的预测.由于它发生在训练阶段之前,通过调整相关参数或采用替代模型,所造成的污染是很难解决的.在机器学习的早期,投毒攻击被认为是对主流算法的一种重要威胁.例如,支持向量机<citation id="1120" type="reference"><link href="839" rel="bibliography" /><link href="883" rel="bibliography" /><link href="885" rel="bibliography" /><sup>[<a class="sup">67</a>,<a class="sup">89</a>,<a class="sup">90</a>]</sup></citation>、贝叶斯分类器<citation id="1117" type="reference"><link href="841" rel="bibliography" /><sup>[<a class="sup">68</a>]</sup></citation>、层次聚类<citation id="1118" type="reference"><link href="887" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>、逻辑回归<citation id="1119" type="reference"><link href="889" rel="bibliography" /><sup>[<a class="sup">92</a>]</sup></citation>都受到了投毒攻击的危害.随着深度学习的广泛使用,攻击者也将他们的注意力转移到深度学习上了<citation id="1121" type="reference"><link href="891" rel="bibliography" /><link href="893" rel="bibliography" /><link href="895" rel="bibliography" /><sup>[<a class="sup">93</a>,<a class="sup">94</a>,<a class="sup">95</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="399">Muňoz-González等人<citation id="1122" type="reference"><link href="897" rel="bibliography" /><sup>[<a class="sup">96</a>]</sup></citation>首先对基于反向梯度优化的多类问题进行了投毒攻击.该算法自动分步计算梯度,并对学习过程进行倒转,以降低攻击复杂度.通过添加一个投毒点,实现了通用或特定的错误攻击.这种攻击对许多深度学习任务都很有效,包括垃圾邮件过滤、恶意软件检测和手写数字识别.大多数投毒攻击研究集中在离线环境中,分类器在固定的输入上进行训练.然而,很多训练过程中数据以流的形式按顺序到达,即在线学习.Wang等人<citation id="1123" type="reference"><link href="899" rel="bibliography" /><sup>[<a class="sup">97</a>]</sup></citation>对在线学习的数据投毒攻击进行了调查.他们将问题形式化为半在线和全在线2种设置,采用增量式、区间式和教学强化式3种攻击算法.他们的在线攻击比无视输入数据的在线特性的攻击要好.</p>
                </div>
                <div class="p1">
                    <p id="400">综上所述,投毒攻击本质上是在训练数据上寻求全局或局部分布的扰动.众所周知,机器学习和深度学习的性能在很大程度上取决于训练数据的质量.高质量的数据通常应该是全面的、无偏见的和有代表性的.在数据投毒的过程中,错误的标签或有偏差的数据被有意地加工并添加到训练数据中,降低了整体质量.据观察,投毒有2方面原因:</p>
                </div>
                <h4 class="anchor-tag" id="692" name="692">1) 错误标记数据.</h4>
                <div class="p1">
                    <p id="401">在分类任务中,深度学习模型通常会在标记数据下提前进行训练.也就是说,<i>L</i>:{<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>}→<i>Y</i>,其中<i>Y</i>是给定输入的特定标签.通过将标签操作为<i>L</i>:{<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>}→<i>Y</i>′来生成错误标记的数据,其中<i>Y</i>′是一个错误的标签.错误标记数据的接受可能导致2种结果:深度学习不能有效地学习决策边界;将决策边界显著地推到不正确的区域.结果表明:该算法在容错条件下不能收敛.后者可以以相当小的损失终止,但是决策边界与正确边界之间的距离很大.</p>
                </div>
                <div class="p1">
                    <p id="402">Xiao等人<citation id="1124" type="reference"><link href="885" rel="bibliography" /><sup>[<a class="sup">90</a>]</sup></citation>通过翻转标签来调整训练集来攻击支持向量机,他们提出了一个优化的框架来寻找标签翻转,使得分类误差最大化,从而降低了分类器的准确率.Biggio等人<citation id="1125" type="reference"><link href="887" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>实现了针对单链接层次聚类的投毒攻击.它依靠启发式算法来寻找最优的攻击策略.他们使用模糊攻击来最大程度地降低聚类结果.Alfeld等人<citation id="1126" type="reference"><link href="743" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了一个在线性自回归模型下编码攻击者欲望和约束的框架.攻击者可以通过在训练数据中添加最优的特殊记录来将预测推向某个方向.Jagielski等人<citation id="1127" type="reference"><link href="891" rel="bibliography" /><sup>[<a class="sup">93</a>]</sup></citation>讨论了线性回归模型的投毒攻击.攻击者可以操纵数据集和算法来影响机器学习模型.他们引入了一种快速的统计攻击,这种攻击只需要有限的训练过程知识.</p>
                </div>
                <h4 class="anchor-tag" id="693" name="693">2) 特定混淆数据.</h4>
                <div class="p1">
                    <p id="403">机器学习实践者从大量信息中提取具有代表性的特征,用于学习和训练.这些特征的权重是经过训练确定的,对预测具有重要意义.然而,如果一些精心设计的数据具有无偏倚的特性分布,就会破坏训练,并得到一组误导性的特征权重.例如,将很多炸弹形状的图形标记为限速标志并将其放入数据集中学习,那么可能所有带有炸弹的图像将被标识为限速标志,即使它原本是一个停止(STOP)标志.</p>
                </div>
                <div class="p1">
                    <p id="404">该方法在LASSO,Ridge Regression,Elastic net等特征选择算法中也很常见.Xiao等人<citation id="1128" type="reference"><link href="845" rel="bibliography" /><sup>[<a class="sup">70</a>]</sup></citation>直接研究了常见的特征选择算法在投毒攻击下的鲁棒性.结果表明,在恶意软件检测应用中,特征选择算法在投毒攻击下受到破坏性影响.通过插入少于5%的有毒训练样本,LASSO特征选择过程得到的结果与随机选择几乎没有区别.Shafahi等人<citation id="1129" type="reference"><link href="893" rel="bibliography" /><sup>[<a class="sup">94</a>]</sup></citation>试图找到一个特定的测试实例来控制分类器的行为,而没有控制训练数据的标签.他们提出了一种水印策略,并训练了多个投毒的实例.在投毒实例中添加目标实例的低透明度水印,以允许某些不可分割的特性重叠.该方法为攻击者打开了一个分类器的后门,攻击者无需访问任何数据收集或标记过程.</p>
                </div>
                <h4 class="anchor-tag" id="405" name="405"><b>4.4 对抗攻击</b></h4>
                <div class="p1">
                    <p id="406">对抗攻击利用对抗样本(adversarial examples, AEs)使模型预测错误,也称为逃避攻击.对抗攻击是一种探索性攻击,它破坏了模型的可用性.AEs是通过在原始样本中添加扰动而产生的.它们混淆了训练有素的模型,但在人类看来它们很正常,这保证了攻击的有效性.在图像处理中,通常使用小扰动来保证原样例与AEs之间的相似性.在语音和文本中,它确保AEs也是有意义的和上下文相关的.恶意软件检测保证AEs在添加扰动后仍具有原始恶意功能.</p>
                </div>
                <div class="p1">
                    <p id="407">模型的误分类有目标性和非目标性两大类.前者要求AEs被错误地分类为特定的标签,以达到特殊的恶意目的.后者只要求AEs被错误分类(可以是任意错误标签),用于抵抗检测或其他场景.AEs的生成过程通常需要最小化扰动,因为越小的扰动对人的影响也就越小.最小距离通常用<i>L</i><sub><i>p</i></sub>距离(或称Minkowski距离)来度量,常用的有<i>L</i><sub>0</sub>,<i>L</i><sub>1</sub>,<i>L</i><sub>2</sub>和<i>L</i><sub>∞</sub>:</p>
                </div>
                <div class="p1">
                    <p id="408" class="code-formula">
                        <mathml id="408"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mi>p</mi></msup></mrow><mo>)</mo></mrow><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mi>p</mi></mfrac></mrow></msup><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><mo>=</mo><mo stretchy="false">(</mo><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="409">对抗攻击可以应用于许多领域,其中应用最广泛的是图像分类.通过添加小的扰动,我们可以生成对抗的图像,这些图像对人类而言很难区分,但是能造成模型的分类错误.对抗攻击也用在其他领域,比如音频<citation id="1135" type="reference"><link href="859" rel="bibliography" /><link href="901" rel="bibliography" /><sup>[<a class="sup">77</a>,<a class="sup">98</a>]</sup></citation>、文本<citation id="1130" type="reference"><link href="863" rel="bibliography" /><sup>[<a class="sup">79</a>]</sup></citation>、恶意软件检测<citation id="1136" type="reference"><link href="903" rel="bibliography" /><link href="905" rel="bibliography" /><link href="907" rel="bibliography" /><sup>[<a class="sup">99</a>,<a class="sup">100</a>,<a class="sup">101</a>]</sup></citation>等.Carlini等人<citation id="1131" type="reference"><link href="861" rel="bibliography" /><sup>[<a class="sup">78</a>]</sup></citation>提出了一种基于语音到文本神经网络的文本对抗攻击系统DeepSearch.它可以通过添加小扰动将任意给定的波形转换成任意期望的目标短语.他们使用序列到序列的神经网络,产生超过99.9%的相似波形,并达到100%的攻击率.Gao等人<citation id="1132" type="reference"><link href="863" rel="bibliography" /><sup>[<a class="sup">79</a>]</sup></citation>提出框架DeepWordBug来在黑盒设置中生成对抗文本序列.他们使用不同的评分函数来处理更好的突变词.它们几乎最小化了编辑距离,并将文本分类精度从90%降低到30%～60%.Rigaki等人<citation id="1133" type="reference"><link href="909" rel="bibliography" /><sup>[<a class="sup">102</a>]</sup></citation>使用GANs通过修改网络行为来模拟合法应用程序的流量来避免恶意软件检测.他们可以通过修改恶意软件的源代码来调整命令和控制(C2)通道来模拟Facebook聊天网络流量.最好的GAN模型在经过300～400个训练阶段后,每分钟产生一个以上的C2流量.文献<citation id="1137" type="reference">[<a class="sup">103</a>,<a class="sup">104</a>,<a class="sup">105</a>]</citation>提出了在黑盒中生成恶意软件实例以进行攻击检测模型的方法.此外,文献<citation id="1134" type="reference">[<a class="sup">106</a>]</citation>提出了一种针对二进制编码恶意软件检测的反攻击算法,实现了91.9%的准确率.</p>
                </div>
                <div class="p1">
                    <p id="410">在图像领域,对抗攻击主要通过梯度下降法、最优化、神经网络自动化等方法搜索对抗样本来实现.一些研究也开始考虑现实世界中对抗样本的问题.在这里,我们定义<i>F</i>:R<sup><i>n</i></sup>→{1,2,…,<i>k</i>}是将图像值向量映射到类标签的模型分类器.<i>Z</i>(·)是倒数第2层的输出,通常表示类概率.<i>δ</i>是扰动,<mathml id="476"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">δ</mi><mo>|</mo></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>表示求<i>L</i><sub><i>i</i></sub>距离.我们接下来详细地介绍在图像领域里生成对抗扰动的12种方法.</p>
                </div>
                <div class="p1">
                    <p id="411">1) L-BFGS攻击.Szegedy等人<citation id="1138" type="reference"><link href="707" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出盒约束的L-BFGS,用于生成AEs.他们还发现了2个与直觉相反的特性.首先,该空间包含的语义信息位于神经网络的高层,而不是单个单元.其次,扰动或AEs具有较强的鲁棒性,可以在不同的神经网络或训练数据集之间共享.这些性质为今后的研究奠定了基础.</p>
                </div>
                <div class="p1">
                    <p id="412" class="code-formula">
                        <mathml id="412"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">δ</mi></munder><mspace width="0.25em" /><mtext> </mtext><mi>c</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">δ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>F</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo>,</mo><mi>l</mi><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mtext> </mtext><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mi>n</mi></msup><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="413">其中,<i>l</i>是分类错误的标签,<i><b>x</b></i>+<i>δ</i>是对抗样本,他们试图找到满足<i>F</i>(<i><b>x</b></i>+<i>δ</i>)=<i>l</i>的<i>δ</i>,要求扰动<i>δ</i>尽量小,同时<i><b>x</b></i>+<i>δ</i>被分类为<i>l</i>的损失(即<i>Loss</i><sub><i>F</i></sub>(<i><b>x</b></i>+<i>δ</i>,<i>l</i>))也尽量小.损失函数满足<i>Loss</i><sub><i>F</i></sub>(<i><b>x</b></i>,<i>F</i>(<i><b>x</b></i>))=0,<i>c</i>&gt;0是一个平衡2个最小值的超参数,<i><b>x</b></i>+<i>δ</i>∈[0,1]<sup><i>n</i></sup>保证添加扰动后的对抗样本仍在正常图像的取值范围内.</p>
                </div>
                <div class="p1">
                    <p id="414">2) FGSM攻击.FGSM(fast gradient sign method)是由Goodfellow等人<citation id="1139" type="reference"><link href="851" rel="bibliography" /><sup>[<a class="sup">73</a>]</sup></citation>提出的.文章解释说,AEs产生的原因是神经网络在高维空间中的线性行为,而不是非线性.设<i>l</i><sub><i><b>x</b></i></sub>是<i><b>x</b></i>的实际分类.损失函数描述输入<i><b>x</b></i>的损失.扰动<i>δ</i>的方向是利用反向传播计算的梯度确定的.每个像素在梯度方向上的大小为<i>ε</i>.随着<i>ε</i>的增加,扰动的大小和攻击成功率增加,被人发现的可能性也增加.</p>
                </div>
                <div class="p1">
                    <p id="415"><i>δ</i>=<i>ε</i>×sign(∇<sub><i><b>x</b></i></sub><i>Loss</i><sub><i>F</i></sub>(<i><b>x</b></i>,<i>l</i><sub><i><b>x</b></i></sub>)).</p>
                </div>
                <div class="p1">
                    <p id="416">3) BIM攻击.BIM(basic iteration method)<citation id="1140" type="reference"><link href="919" rel="bibliography" /><sup>[<a class="sup">107</a>]</sup></citation>是FGSM的迭代版本,也称为I-FGSM.<i>Clip</i><sub><i><b>x</b></i></sub><sub>,</sub><sub><i>ε</i></sub>(<i><b>x</b></i>)函数对每个像素的图像进行剪切,并使生成的AE在每次迭代时满足<i>L</i><sub>∞</sub>的边界.I-FGSM在白盒攻击中强于FGSM,但其可移植性较差<citation id="1141" type="reference"><link href="921" rel="bibliography" /><link href="923" rel="bibliography" /><sup>[<a class="sup">108</a>,<a class="sup">109</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="417"><i><b>x</b></i><sub>0</sub>=<i><b>x</b></i>,</p>
                </div>
                <div class="p1">
                    <p id="418"><i><b>x</b></i><sub><i>i</i></sub><sub>+1</sub>=<i>Clip</i><sub><i><b>x</b></i></sub><sub>,</sub><sub><i>ε</i></sub>(<i><b>x</b></i><sub><i>i</i></sub>+<i>α</i>×sign(∇<sub><i><b>x</b></i></sub><i>Loss</i><sub><i>F</i></sub>(<i><b>x</b></i><sub><i>i</i></sub>,<i>l</i><sub><i><b>x</b></i></sub>)).</p>
                </div>
                <div class="p1">
                    <p id="419">4) MI-FGSM攻击.MI-FGSM(momentum iterative FGSM)<citation id="1142" type="reference"><link href="925" rel="bibliography" /><sup>[<a class="sup">110</a>]</sup></citation>是基于梯度引入的.Momentum用于摆脱局部极值,迭代用于稳定优化.在白盒或黑盒模型上,该方法比基于梯度的单步法具有更强的可移植性.</p>
                </div>
                <div class="p1">
                    <p id="420" class="code-formula">
                        <mathml id="420"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>C</mi><mi>l</mi><mi>i</mi><mi>p</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi>ε</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>α</mi><mo>×</mo><mfrac><mrow><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>μ</mi><mo>×</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mo>∇</mo><msub><mrow></mrow><mi mathvariant="bold-italic">x</mi></msub><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>F</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mrow><mo>|</mo><mrow><mo>∇</mo><msub><mrow></mrow><mi mathvariant="bold-italic">x</mi></msub><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>F</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo>,</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="421">其中,<i>y</i>是要被分类错误的目标类.与BIM不同的是,计算<i><b>x</b></i><sub><i>i</i></sub><sub>+1</sub>时,不仅和当前损失函数的梯度方向有关,也和之前求出的损失函数(即<i><b>g</b></i><sub><i>i</i></sub>)有关.</p>
                </div>
                <div class="p1">
                    <p id="422">5) JSMA攻击.JSMA (Jacobian-based saliency map attack)<citation id="1143" type="reference"><link href="853" rel="bibliography" /><sup>[<a class="sup">74</a>]</sup></citation>只改变了少量像素,而没有影响整个图像,它限制了<i>L</i><sub>0</sub>距离,而不是<i>L</i><sub>2</sub>和<i>L</i><sub>∞</sub>.它们每次修改图像的个别像素,记录其对分类结果的影响,然后迭代地进行下去.对于任意一对像素<i>p</i>,<i>q</i>,求解</p>
                </div>
                <div class="p1">
                    <p id="423" class="code-formula">
                        <mathml id="423"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy="false">}</mo></mrow></munder><mrow><mfrac><mrow><mo>∂</mo><mi>Ζ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow></mstyle><mo>,</mo></mtd></mtr><mtr><mtd><mi>β</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy="false">}</mo></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mrow><mfrac><mrow><mo>∂</mo><mi>Ζ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow></mstyle></mrow></mstyle></mrow><mo>)</mo></mrow><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="424">其中,<i>α</i><sub><i>pq</i></sub>表示像素<i>p</i>,<i>q</i>对目标分类的影响,<i>β</i><sub><i>pq</i></sub>表示对所有其他输出的影响.这张映射图上的值越大,意味着欺骗网络的可能性越大.</p>
                </div>
                <div class="p1">
                    <p id="425" class="code-formula">
                        <mathml id="425"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">(</mo><mi>p</mi><msup><mrow></mrow><mo>*</mo></msup><mo>,</mo><mi>q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mo stretchy="false">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></munder><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>×</mo><mi>β</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo stretchy="false">)</mo><mo>×</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>&gt;</mo><mn>0</mn><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>β</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>&lt;</mo><mn>0</mn><mo stretchy="false">)</mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="426">上式表明算法当前选择(<i>p</i><sup>*</sup>,<i>q</i><sup>*</sup>)像素对添加扰动从而实施攻击.(<i>p</i><sup>*</sup>,<i>q</i><sup>*</sup>)满足:像素<i>p</i><sup>*</sup>,<i>q</i><sup>*</sup>对目标分类的影响为正,对所有其他输出的影响为负,且对目标分类的影响(<i>α</i><sub><i>pq</i></sub>)以及对所有其他输出影响的绝对值(-<i>β</i><sub><i>pq</i></sub>)两者乘积最大.</p>
                </div>
                <div class="p1">
                    <p id="427">6) C&amp;W攻击.C&amp;W<citation id="1144" type="reference"><link href="855" rel="bibliography" /><sup>[<a class="sup">75</a>]</sup></citation>在<i>L</i><sub>0</sub>,<i>L</i><sub>2</sub>和<i>L</i><sub>∞</sub>中实现了对蒸馏防御方法<citation id="1145" type="reference"><link href="927" rel="bibliography" /><sup>[<a class="sup">111</a>]</sup></citation>的攻击.他们试图找到尽可能小的<i>δ</i>,并欺骗分类器.与L-BFGS类似,C&amp;W主要优化了目标:</p>
                </div>
                <div class="p1">
                    <p id="428" class="code-formula">
                        <mathml id="428"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">δ</mi></munder><mrow><mo>|</mo><mi mathvariant="bold-italic">δ</mi><mo>|</mo></mrow><msub><mrow></mrow><mi>p</mi></msub><mo>+</mo><mi>c</mi><mo>×</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mi>n</mi></msup><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="429"><i>c</i>&gt;0是一个超参数,用于控制2个目标函数之间的平衡.<i>f</i>(·)是一个人工定义的函数,这里列举文中使用的函数:</p>
                </div>
                <div class="p1">
                    <p id="430"><i>f</i>(<i><b>x</b></i>+<i>δ</i>)=max(max{<i>Z</i>(<i><b>x</b></i>+<i>δ</i>)<sub><i>i</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="431"><i>i</i>≠<i>t</i>}-<i>Z</i>(<i><b>x</b></i>+<i>δ</i>)<sub><i>t</i></sub>,-<i>K</i>).</p>
                </div>
                <div class="p1">
                    <p id="432">这里,<i>f</i>(·)≤0当且仅当分类结果为对抗目标标签<i>t</i>时.<i>K</i>保证<i><b>x</b></i>+<i>δ</i>将被高度信任地分类为<i>t</i>.因此在最小化式子中,既要让扰动<i>δ</i>尽量小,也要让<i>f</i>(·)尽量小,即分类结果为目标标签<i>t</i>.C&amp;W保证生成的AEs一定会被错误分类,但由于计算量大,造成时间开销较大.</p>
                </div>
                <div class="p1">
                    <p id="433">7) EAD攻击.EAD (Elastic-net attacks to DNNs)<citation id="1146" type="reference"><link href="929" rel="bibliography" /><sup>[<a class="sup">112</a>]</sup></citation>是用于制作AEs的弹性网络正则化攻击框架,它结合了<i>L</i><sub>1</sub>,<i>L</i><sub>2</sub>度量,提供了很少使用的面向<i>L</i><sub>1</sub>的样例,并将最好的<i>L</i><sub>2</sub>攻击作为一个特例.结果显示,EAD设计的基于<i>L</i><sub>1</sub>的示例执行得和其他最佳攻击一样好.最优化公式为</p>
                </div>
                <div class="p1">
                    <p id="434" class="code-formula">
                        <mathml id="434"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">δ</mi></munder><mspace width="0.25em" /><mi>c</mi><mo>×</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">δ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">δ</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>,</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mi>n</mi></msup><mo>,</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="435">其中,<i>f</i>(<i><b>x</b></i>+<i>δ</i>)与C&amp;W中相同,<i>t</i>是目标标签.相较于C&amp;W,EAD的优化公式中多了一个扰动项.显然,当<i>β</i>=0时,C&amp;W中的<i>L</i><sub>2</sub>目标攻击是EAD的一个特殊的情况.</p>
                </div>
                <div class="p1">
                    <p id="436">8) OptMargin攻击.OptMargin<citation id="1147" type="reference"><link href="931" rel="bibliography" /><sup>[<a class="sup">113</a>]</sup></citation>可以在有限的输入空间内躲避基于区域分类的防御.与以前的研究不同,它的目标是低维的子空间,不受空间周围邻居点的限制.该方法产生的AEs的判定边界与良性样本不同.然而,它无法模仿良性样本.OptMargin是C&amp;W在L<sub>2</sub>攻击的扩展,它在<i>x</i>的周围添加了许多目标函数.具体优化公式为</p>
                </div>
                <div class="p1">
                    <p id="437" class="code-formula">
                        <mathml id="437"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">δ</mi></munder><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><mo>×</mo><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>f</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">(</mo><mi>Ζ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msub><mrow></mrow><mi>y</mi></msub><mo>-</mo><mrow><mi>max</mi></mrow><mo stretchy="false">{</mo><mi>Ζ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msub><mrow></mrow><mi>j</mi></msub><mo>:</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="438"><i>j</i>≠<i>y</i>},-<i>K</i>).</p>
                </div>
                <div class="p1">
                    <p id="439">这里,<i><b>x</b></i><sub>0</sub>是原始样例.<i><b>x</b></i>=<i><b>x</b></i><sub>0</sub>+<i>δ</i>是对抗样本.<i>y</i>是<i><b>x</b></i><sub>0</sub>的真实标签.<i>f</i><sub><i>i</i></sub>(<i><b>x</b></i>)是类似于C&amp;W的目标函数,共<i>m</i>个.<i><b>v</b></i><sub><i>i</i></sub>是应用于<i><b>x</b></i>的扰动,共<i>m</i>个.OptMargin不仅保证对抗样本<i><b>x</b></i>可以欺骗神经网络,还保证它周围的邻居<i><b>x</b></i>+<i><b>v</b></i><sub><i>i</i></sub>也可以.</p>
                </div>
                <div class="p1">
                    <p id="440">9) DeepFool攻击.DeepFool<citation id="1148" type="reference"><link href="857" rel="bibliography" /><sup>[<a class="sup">76</a>]</sup></citation>以迭代方式产生最小的归一化扰动.他们将图像逐步推入分类边界,直到符号发生变化.在相近的成功欺骗率下,DeepFool产生的扰动比FGSM要小.</p>
                </div>
                <div class="p1">
                    <p id="441">10) NewtonFool攻击.NewtonFool<citation id="1149" type="reference"><link href="933" rel="bibliography" /><sup>[<a class="sup">114</a>]</sup></citation>提出了一个强假设,即攻击者可以使用倒数第2层输出的类概率向量<i>Z</i>(<i><b>x</b></i>).假设<i>l</i>=<i>F</i>(<i><b>x</b></i><sub>0</sub>),他们的目的是找到小的<i>δ</i>,使<i>Z</i>(<i><b>x</b></i><sub>0</sub>+<i>δ</i>)<sub><i>l</i></sub>=0.他们用迭代方法把<i>Z</i>(<i><b>x</b></i><sub>0</sub>)<sub><i>l</i></sub>尽可能快地降到0.从<i>Z</i>(<i><b>x</b></i><sub>0</sub>)<sub><i>l</i></sub>开始,他们在每一步使用线性函数逼近新的<i>Z</i>(<i><b>x</b></i>)<sub><i>l</i></sub>,即</p>
                </div>
                <div class="p1">
                    <p id="442"><i>Z</i>(<i><b>x</b></i><sub><i>i</i></sub><sub>+1</sub>)<sub><i>l</i></sub>≈<i>Z</i>(<i><b>x</b></i><sub><i>i</i></sub>)<sub><i>l</i></sub>+∇<i>Z</i>(<i><b>x</b></i><sub><i>i</i></sub>)<sub><i>l</i></sub>×</p>
                </div>
                <div class="p1">
                    <p id="443">(<i><b>x</b></i><sub><i>i</i></sub><sub>+1</sub>-<i><b>x</b></i><sub><i>i</i></sub>), <i>i</i>=0,1,2,…,</p>
                </div>
                <div class="p1">
                    <p id="444">其中,<i>δ</i><sub><i>i</i></sub>=<i><b>x</b></i><sub><i>i</i></sub><sub>+1</sub>-<i><b>x</b></i><sub><i>i</i></sub>是第<i>i</i>步迭代的扰动,最终扰动<i>δ</i>=<i>δ</i><sub>0</sub>+<i>δ</i><sub>1</sub>+…+<i>δ</i><sub><i>i</i></sub>.结果表明它比FGSM,JSMA,DeepFool都快.</p>
                </div>
                <div class="p1">
                    <p id="445">11) UAP攻击.UAP(universal adversarial pertur-bations)<citation id="1150" type="reference"><link href="867" rel="bibliography" /><sup>[<a class="sup">81</a>]</sup></citation>可以以高概率在几乎任何输入数据上导致目标模型的错误分类.UAP对于数据和网络架构来说是通用的.让<i>μ</i>表示包含所有样例的数据集.它主要目的是寻找扰动<i>δ</i>,这个<i>δ</i>可以在几乎所有<i>μ</i>中的样本上欺骗<i>F</i>(·).</p>
                </div>
                <div class="p1">
                    <p id="446"><i>F</i>(<i><b>x</b></i>+<i>δ</i>)≠<i>F</i>(<i><b>x</b></i>), 大部分<i><b>x</b></i>∈<i>μ</i>,</p>
                </div>
                <div class="p1">
                    <p id="447">其中扰动<i>δ</i>应满足约束条件:</p>
                </div>
                <div class="p1">
                    <p id="448" class="code-formula">
                        <mathml id="448"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>|</mo><mi mathvariant="bold-italic">δ</mi><mo>|</mo></mrow><msub><mrow></mrow><mi>p</mi></msub><mo>≤</mo><mi>ξ</mi><mo>,</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><mo>∈</mo><mi>μ</mi></mrow></msub><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>+</mo><mi mathvariant="bold-italic">δ</mi><mo stretchy="false">)</mo><mo>≠</mo><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>≥</mo><mn>1</mn><mo>-</mo><mi>λ</mi><mo>,</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="449"><i>P</i>表示概率,通常0&lt;<i>λ</i>≪1.在没有优化或梯度计算的情况下,他们将每次迭代计算的最小扰动集合起来.Hayes等人<citation id="1151" type="reference"><link href="935" rel="bibliography" /><sup>[<a class="sup">115</a>]</sup></citation>使用通用对抗网络(UANs)在有目标和无目标攻击中自动生成UAP.</p>
                </div>
                <div class="p1">
                    <p id="450">12) ATN攻击.ATN<citation id="1152" type="reference"><link href="771" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>是一种训练有素的神经网络,可以高效、自动地攻击另一个目标.ATN通过添加最小扰动将任何输入转换为AE.他们使用有针对性的白盒ATNs来生成AEs,并成功地将83%～92%的图像输入转换为对ImageNet的对抗攻击.</p>
                </div>
                <div class="p1">
                    <p id="451">13) 其他攻击方法.Papernot等人<citation id="1153" type="reference"><link href="773" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>提出了一种基于黑盒综合数据生成替代训练算法的新方法,在Google和Amazon上分别实现了96.19%和88.94%的准确率.Tramèr等人<citation id="1154" type="reference"><link href="937" rel="bibliography" /><sup>[<a class="sup">116</a>]</sup></citation>提出了梯度对齐对子空间,它用于估计输入空间的未知维度.他们发现,子空间的很大一部分被2个不同的模型共享,从而实现了可移植性.他们首先寻找多个独立的攻击方向,定量研究模型决策边界的相似性.Narodytska等人<citation id="1155" type="reference"><link href="939" rel="bibliography" /><sup>[<a class="sup">117</a>]</sup></citation>利用一种基于局部搜索的新技术构造了网络梯度的数值逼近,然后利用该技术构造了图像中的一组像素在黑盒中扰动.此外,Ilyas等人<citation id="1156" type="reference"><link href="941" rel="bibliography" /><sup>[<a class="sup">118</a>]</sup></citation>引入了一个更加严格和实用的黑盒威胁模型.他们使用自然进化策略来执行黑盒攻击,减少了2～3个数量级的查询.</p>
                </div>
                <div class="p1">
                    <p id="452">除了DNN外,还有很多研究人员对生成模型、强化学习和机器学习算法进行了深入的研究.Mei等人<citation id="1157" type="reference"><link href="943" rel="bibliography" /><sup>[<a class="sup">119</a>]</sup></citation>为支持向量机、逻辑回归和线性回归确定最优训练集攻击.证明了最优攻击可以描述为一个双层优化问题,可以用梯度法求解.Huang等人<citation id="1158" type="reference"><link href="945" rel="bibliography" /><sup>[<a class="sup">120</a>]</sup></citation>证明了对抗攻击策略在强化学习中也是有效的.Kos等人<citation id="1159" type="reference"><link href="947" rel="bibliography" /><sup>[<a class="sup">121</a>]</sup></citation>对深度生成模型(如变分自编码器(VAE))进行了对抗攻击.他们的方法包括基于分类器的攻击,以及对潜在空间的攻击,这些攻击在MNIST,SVHN和CelebA上都表现得很好.</p>
                </div>
                <div class="p1">
                    <p id="453">4.4.1 物理世界的对抗攻击</p>
                </div>
                <div class="p1">
                    <p id="454">在图像识别领域,考虑到观察点、光照和相机噪声的影响,传统技术产生的AEs可能无法在物理世界中欺骗分类器.Kurakin等人<citation id="1160" type="reference"><link href="919" rel="bibliography" /><sup>[<a class="sup">107</a>]</sup></citation>使用从手机摄像头拍摄的图像作为Inception v3图像分类神经网络的输入.结果表明,由原始网络构造的大量对抗图像,即使通过摄像机输入到分类器,也会产生误分类.Athalye等人<citation id="1161" type="reference"><link href="709" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出了一种EOT(expectation over transformation)算法,用于合成对物理世界具有鲁棒性的对抗样本.他们使用EOT的特殊应用,并通过3D渲染过程进行区分,从而生成对抗对象.结果表明,3D打印对象可以从各个角度欺骗现实世界的系统.</p>
                </div>
                <h4 class="anchor-tag" id="455" name="455"><b>4.5 投毒防御</b></h4>
                <div class="p1">
                    <p id="456">大多数的投毒攻击都集中在数据和算法上,因此防御方法主要考虑从保护数据和算法入手.</p>
                </div>
                <h4 class="anchor-tag" id="694" name="694">1) 保护数据.</h4>
                <div class="p1">
                    <p id="457">数据保护主要包括保护收集到的数据不受篡改、抵抗重写攻击、防止拒绝、防止数据伪造、检测有毒数据等<citation id="1165" type="reference"><link href="949" rel="bibliography" /><link href="951" rel="bibliography" /><link href="953" rel="bibliography" /><sup>[<a class="sup">122</a>,<a class="sup">123</a>,<a class="sup">124</a>]</sup></citation>.Olufowobi等人<citation id="1162" type="reference"><link href="955" rel="bibliography" /><sup>[<a class="sup">125</a>]</sup></citation>提出了一种物联网系统的数据来源模型,以提高数据的可信度和可靠性.该模型描述了创建或修改数据点的上下文.他们未来的工作是将该模型集成到物联网设备的数据源完整性检测算法中.Chakarov等人<citation id="1163" type="reference"><link href="957" rel="bibliography" /><sup>[<a class="sup">126</a>]</sup></citation>通过评价单个数据点对训练模型性能的影响,采用一种检测投毒数据的方法.他们需要通过比较可信数据集上的性能来评估模型.Baracaldo等人<citation id="1164" type="reference"><link href="849" rel="bibliography" /><sup>[<a class="sup">72</a>]</sup></citation>通过使用源信息作为过滤算法的一部分来检测投毒攻击.该方法提高了检测率.他们使用训练数据点的来源和转换上下文来识别有害数据.它是在部分可信和完全不可信的数据集上实现的.</p>
                </div>
                <h4 class="anchor-tag" id="695" name="695">2) 保护算法.</h4>
                <div class="p1">
                    <p id="458">学习算法总是要在防止正则化和减少损失函数之间做出权衡,这种不确定性可能导致学习算法的脆弱性.一些投毒攻击是根据自身的弱点来实施的,因此研究鲁棒机器学习算法是预防投毒攻击的有效途径.Candès等人<citation id="1166" type="reference"><link href="959" rel="bibliography" /><sup>[<a class="sup">127</a>]</sup></citation>首先研究了鲁棒PCA的鲁棒机器学习算法.它假定底层数据集的一小部分是随机销毁的,而不是有针对地销毁.Chen等人<citation id="1167" type="reference"><link href="961" rel="bibliography" /><sup>[<a class="sup">128</a>]</sup></citation>研究了对抗破坏下的鲁棒线性回归问题,Feng等人<citation id="1168" type="reference"><link href="963" rel="bibliography" /><sup>[<a class="sup">129</a>]</sup></citation>研究鲁棒的逻辑回归,他们都需要对特征独立性和亚高斯分布做出强有力的假设.Goodfellow等人<citation id="1169" type="reference"><link href="851" rel="bibliography" /><sup>[<a class="sup">73</a>]</sup></citation>提出了一种鲁棒线性回归方法,该方法放松了对特征独立性和低方差亚高斯噪声的假设,只假设特征矩阵可以用低秩矩阵逼近.该方法将鲁棒低秩矩阵近似与鲁棒主成分相结合,获得了较强的性能保证.Jagielski等人<citation id="1170" type="reference"><link href="891" rel="bibliography" /><sup>[<a class="sup">93</a>]</sup></citation>在训练过程中加入有毒的数据训练模型,而不是简单地删除它们.该方法迭代地估计回归参数,并将其训练在每次迭代中残差最小的点的子集上.本质上,它使用了一个根据每次迭代中残差的不同子集计算的被修剪的损失函数.</p>
                </div>
                <h4 class="anchor-tag" id="459" name="459"><b>4.6 对抗防御</b></h4>
                <div class="p1">
                    <p id="460">对抗攻击的防御方法主要从阻止对抗样本生成和检测对抗样本2个目标出发,本文总结了以下7种方法.</p>
                </div>
                <h4 class="anchor-tag" id="696" name="696">1) 对抗训练.</h4>
                <div class="p1">
                    <p id="461">对抗训练选择AEs作为训练数据集的一部分,使训练后的模型能够学习AEs的特征.Huang等人<citation id="1171" type="reference"><link href="869" rel="bibliography" /><sup>[<a class="sup">82</a>]</sup></citation>提出了一个较早的防御方法,即通过生成AEs作为中间步骤来学习具有强大对手的鲁棒分类器.同时他们也提出了一种新的AEs搜索方法.Kurakin等人<citation id="1172" type="reference"><link href="923" rel="bibliography" /><sup>[<a class="sup">109</a>]</sup></citation>将对抗训练应用于更大的数据集,如ImageNet.其主要创新之处是批处理规范化、训练数据集(包括干净的和敌对的示例)和相对权重.他们还发现一步攻击比迭代攻击更具有可移植性.但是这种训练在正常样本上丧失了部分准确性.此外,集成对抗训练<citation id="1173" type="reference"><link href="921" rel="bibliography" /><sup>[<a class="sup">108</a>]</sup></citation>包含了从其他预训练模型传输的每个输入.然而,对抗训练只能使训练模型对训练集中的AEs具有较强的鲁棒性,该模型不能学习训练集之外的AEs的特性.</p>
                </div>
                <h4 class="anchor-tag" id="697" name="697">2) 基于区域的分类.</h4>
                <div class="p1">
                    <p id="462">了解对抗样本区域的性质,并使用更健壮的基于区域的分类也可以抵御对抗攻击.Cao等人<citation id="1174" type="reference"><link href="965" rel="bibliography" /><sup>[<a class="sup">130</a>]</sup></citation>使用基于区域的分类(RC)代替基于点的分类开发了新的DNNs.他们通过从以测试样本为中心的超立方体中随机选择几个点来预测标签.RC将C&amp;W攻击的成功率从100%降低到16%,但它对OptMargin攻击很难起作用.Pang等人<citation id="1175" type="reference"><link href="871" rel="bibliography" /><sup>[<a class="sup">83</a>]</sup></citation>使用了一种反向交叉熵防御方法.该分类器将正常样本映射到最终隐藏层空间的低维流形邻域.Ma等人<citation id="1176" type="reference"><link href="967" rel="bibliography" /><sup>[<a class="sup">131</a>]</sup></citation>提出了局部固有维数来表征对抗区域的维数特性.他们基于样本到邻域的距离分布,对样本区域的空间填充能力进行了评价.另外,Mccoyd等人<citation id="1177" type="reference"><link href="969" rel="bibliography" /><sup>[<a class="sup">132</a>]</sup></citation>在训练数据集中添加了大量不同类别的背景图像,以帮助检测AEs.他们在EMNIST数据集中的关键类之间添加了背景类,背景类充斥在关键类之间的空白区域.该方法易于实施,但对C&amp;W攻击没有效果.</p>
                </div>
                <h4 class="anchor-tag" id="698" name="698">3) 输入数据变换.</h4>
                <div class="p1">
                    <p id="463">改变或转换输入可以防御对抗攻击.Song等人<citation id="1178" type="reference"><link href="873" rel="bibliography" /><sup>[<a class="sup">84</a>]</sup></citation>发现AEs主要位于训练区域的低概率区域.因此他们设计了PixelDefend,通过自适应地将AE向分布方向移动来净化AE.Guo等人<citation id="1179" type="reference"><link href="971" rel="bibliography" /><sup>[<a class="sup">133</a>]</sup></citation>通过图像转换探索了图像分类系统的模型无关防御.他们的目的是消除输入的对抗扰动.他们的图像转换包括图像裁剪和重新缩放、位深度缩减、JPEG压缩、总方差最小化和图像拼接.Xie等人<citation id="1180" type="reference"><link href="973" rel="bibliography" /><sup>[<a class="sup">134</a>]</sup></citation>在预测过程使用对输入的随机化来防御对抗攻击并减轻影响,包括随机调整图片大小和随机填充.该方法计算量小,与其他防御方法兼容.另外,Wang等人<citation id="1181" type="reference"><link href="975" rel="bibliography" /><sup>[<a class="sup">135</a>]</sup></citation>认为AEs比正常样本更敏感.如果将大量随机扰动添加到对抗样本和正常样本中,标签变化的比例会有显著差异,这样就可以识别对抗样本.他们在MNIST和CIFAR-10上实现了高准确度和低成本的差异判别.Tian等人<citation id="1182" type="reference"><link href="977" rel="bibliography" /><sup>[<a class="sup">136</a>]</sup></citation>认为AEs对某些图像变换操作(如旋转和移位)比正常图像更敏感.他们用这种方法在图像分类中抵御了白盒的C&amp;W攻击.Buckman等人<citation id="1183" type="reference"><link href="979" rel="bibliography" /><sup>[<a class="sup">137</a>]</sup></citation>提出了一种对神经网络进行简单修改的方法TE(thermometer encoding).他们发现TE和热码离散化显著提高了网络对AEs的鲁棒性.</p>
                </div>
                <h4 class="anchor-tag" id="699" name="699">4) 梯度正则化.</h4>
                <div class="p1">
                    <p id="464">梯度正则化(或梯度掩蔽)是另一种有效的防御方法.Madry等人<citation id="1184" type="reference"><link href="875" rel="bibliography" /><sup>[<a class="sup">85</a>]</sup></citation>通过优化鞍点公式实现了这一点,鞍点公式包括由投影梯度下降(PGD)求解的内部最大值和由随机梯度下降(SGD)求解的外部最小值.但他们发现这不能保证在合理的时间内实现.Ross等人<citation id="1185" type="reference"><link href="981" rel="bibliography" /><sup>[<a class="sup">138</a>]</sup></citation>分析了输入梯度正则化,其目的是训练可微模型,以惩罚输入的微小变化.结果表明,输入梯度正则化增强了鲁棒性,与防御蒸馏和对抗训练有质的区别.</p>
                </div>
                <h4 class="anchor-tag" id="700" name="700">5) 防御蒸馏.</h4>
                <div class="p1">
                    <p id="465">Papernot等人<citation id="1186" type="reference"><link href="877" rel="bibliography" /><sup>[<a class="sup">86</a>]</sup></citation>提出了一种防御蒸馏方法.蒸馏主要是指将知识从复杂的结构转移到简单的结构中,从而降低DNN结构的计算复杂度.该方法能够成功地抑制FGSM和基于Jacobian的迭代攻击构造的AEs.Papernot等人<citation id="1187" type="reference"><link href="927" rel="bibliography" /><sup>[<a class="sup">111</a>]</sup></citation>还利用防御蒸馏提取的知识对模型进行平滑处理,并降低了网络梯度的大小.网络梯度大意味着小的扰动会引起输出结果大的变化,有利于寻找对抗样本.</p>
                </div>
                <h4 class="anchor-tag" id="701" name="701">6) 数据处理.</h4>
                <div class="p1">
                    <p id="466">Liang等人<citation id="1188" type="reference"><link href="983" rel="bibliography" /><sup>[<a class="sup">139</a>]</sup></citation>引入标量量化和平滑空间滤波,以减小扰动的影响.他们使用图像熵作为度量标准,并对各种图像进行了自适应降噪.文献<citation id="1189" type="reference">[<a class="sup">87</a>]</citation>中使用有界ReLU激活函数对冲对抗扰动的正向传播,并使用高斯数据增强方法增强泛化能力.Xu等人<citation id="1190" type="reference"><link href="985" rel="bibliography" /><sup>[<a class="sup">140</a>]</sup></citation>提出了基于特征压缩的反例检测方法,包括降低每个像素上颜色位的深度和空间平滑.</p>
                </div>
                <h4 class="anchor-tag" id="702" name="702">7) 防御网络.</h4>
                <div class="p1">
                    <p id="467">一些研究使用神经网络等工具对AEs进行自动对抗.Gu等人<citation id="1191" type="reference"><link href="881" rel="bibliography" /><sup>[<a class="sup">88</a>]</sup></citation>使用了带有收缩自编码器(CAEs)和去噪自编码器(DAEs)的深度收缩网络(DCN),它可以通过额外的噪声腐蚀和预处理去除大量的对抗噪声.Akhtar等人<citation id="1192" type="reference"><link href="987" rel="bibliography" /><sup>[<a class="sup">141</a>]</sup></citation>提出了一种微扰整流网络作为目标模型的预输入层,用于对抗UAPs.它可以在不修改网络的情况下为已部署的网络提供防御,并抵御看不见的敌对干扰.MagNet<citation id="1193" type="reference"><link href="989" rel="bibliography" /><sup>[<a class="sup">142</a>]</sup></citation>利用探测网络对远离流形边界的AEs进行探测,利用重整器对靠近边界的AEs进行改造.该过程不需要AEs或生成过程的知识.</p>
                </div>
                <h3 id="468" name="468" class="anchor-tag"><b>5 总  结</b></h3>
                <div class="p1">
                    <p id="469">随着人工智能领域在生活中各个方面的广泛应用<citation id="1194" type="reference"><link href="991" rel="bibliography" /><link href="993" rel="bibliography" /><link href="995" rel="bibliography" /><sup>[<a class="sup">143</a>,<a class="sup">144</a>,<a class="sup">145</a>]</sup></citation>,相关的安全问题也显现出来.本文调研了机器学习安全领域相关的145篇论文,并对机器学习系统所遇到的安全问题进行了完整而详细的划分.我们将该领域分为隐私和安全两大块,并按攻击目的、攻击目标、攻击过程将攻击分为4类.在每种攻击内部,按时间线和所采用的技术,将繁杂的研究进行总结归纳,划分了不同的技术,并对技术之间的优劣进行了比对和分析.在防御方面,我们着重保护机器学习系统的隐私和抵抗安全攻击,将每种防御类型内部的防御技术进行归类总结,并介绍了防御技术对攻击技术的适应性.另外,根据对这些攻击和防御技术的总结和研究,我们还提出了构建安全健壮的机器学习系统、保护机器学习所有参与者隐私安全的经验,也对目前机器学习系统以及人工智能领域的热点问题进行了讨论.</p>
                </div>
                <h4 class="anchor-tag" id="703" name="703">1) 提高数据质量,增强数据安全.</h4>
                <div class="p1">
                    <p id="470">机器学习在收集数据中可能会收集到脏数据,或者攻击者为实现投毒攻击所提供的数据,因此要对收集的数据做清洗,提高数据质量.一方面可以采用人工的方法对脏数据进行剔除,另一方面可以采用防御方法中对数据集进行清洗、保护的技术<citation id="1195" type="reference"><link href="715" rel="bibliography" /><link href="831" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">63</a>]</sup></citation>.面对数据量不足的情况,还可以通过构建生成模型(如GAN)得到相似的数据.总之,训练的数据质量越高,训练获得的模型也越安全.</p>
                </div>
                <h4 class="anchor-tag" id="704" name="704">2) 保证个人数据隐私,防止模型滥用隐私信息.</h4>
                <div class="p1">
                    <p id="471">在目前的机器学习系统中,个人数据很难得到安全保障,模型可能从个人数据中推断出大量隐私信息.为保障用户的隐私安全,我们建议:引入监管部门对模型监控,严格监管模型对数据的使用,只允许模型提取允许范围内的特征,不可以擅自对敏感信息进行提取和推断;数据源保护,模型收集的数据必须进行去隐私化处理,模糊掉无关信息;建立健全相关法律法规,监管数据的收集、存储、使用和删除过程.</p>
                </div>
                <h4 class="anchor-tag" id="705" name="705">3) 通过模型解释性的研究解决模型安全性滞后性现状.</h4>
                <div class="p1">
                    <p id="472">目前来说,由于我们还没有实现对深度神经网络的深入理解(不清楚某条数据为什么预测出这个结果,不清楚不同数据对模型参数的影响程度),因此寻找安全问题进行攻击比提前预防要容易.因此我们亟需研究深度神经网络可解释性,尤其是2018年欧盟颁布了GDPR条例,更促进了神经网络可解释性的发展,相信随着对神经网络模型理解的加强,安全滞后性的问题将有效缓解.</p>
                </div>
                <h4 class="anchor-tag" id="706" name="706">4) 加强对人工智能在实际应用中的安全问题研究.</h4>
                <div class="p1">
                    <p id="473">人工智能的应用已经延伸至人类生活的物理世界,如自动驾驶应用大量的图像识别技术.若其中存在安全问题将会直接造成对人身的物理伤害,从而导致了人们对AI安全的极大的恐慌.为了解决这个问题,我们要全面地研究机器学习系统易受的安全威胁,加强对模型的保护,加强对攻击方法的抵御.同时还要力求解释AI在何种情况下可能会出现状况、为什么会出现状况以及如何防止这种状况出现,并提出相应的防范措施,从而增强人们对AI技术应用的信任度.</p>
                </div>
                <div class="p1">
                    <p id="474">总的来说,本文将机器学习系统所面临的安全问题进行了详细的分类,对未来的攻击防御技术的研究和发展有着重要意义.攻击和防御本身就是一场军备竞赛,对特定的攻击技术,可以研究专门的防御去抵抗它;而这种防御技术又会被其他的攻击技术所攻克.正是在这种攻防竞赛中,机器学习系统的安全性得以螺旋式的上升.在未来工作中,我们要继续研究机器学习领域的技术、应用和伦理方面的安全问题,并将模型提取攻击、模型逆向攻击、投毒攻击和对抗攻击中先进的攻防工作进行部署,从而对攻击和防御方法形成更统一和完整的度量.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="707">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks[OL]">

                                <b>[1]</b>Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[OL].[2018-05-28].http://arxiv.org/abs/1312.6199
                            </a>
                        </p>
                        <p id="709">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synthesizing robust adversarial examples">

                                <b>[2]</b>Athalye A,Engstrom L,Ilyas A,et al.Synthesizing robust adversarial examples[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM,2018:284- 293
                            </a>
                        </p>
                        <p id="711">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning with membership privacy using adversarial regularization">

                                <b>[3]</b>Nasr M,Shokri R,Houmansadr A.Machine learning with membership privacy using adversarial regularization[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:634- 646
                            </a>
                        </p>
                        <p id="713">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving deep learning via additively homomorphic encryption">

                                <b>[4]</b>Phong L T,Aono Y,Hayashi T,et al.Privacy-preserving deep learning via additively homomorphic encryption[J].IEEE Transactions on Information Forensics and Security,2018,13(5):1333- 1345
                            </a>
                        </p>
                        <p id="715">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving machine learning through data obfuscation[OL]">

                                <b>[5]</b>Zhang Tianwei,He Zecheng,Lee R B.Privacy-preserving machine learning through data obfuscation[OL].[2018-05-28].http://arxiv.org/abs/1807.01860
                            </a>
                        </p>
                        <p id="717">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning with differential privacy">

                                <b>[6]</b>Abadi M,Chu A,Goodfellow I,et al.Deep learning with differential privacy[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2016:308- 318
                            </a>
                        </p>
                        <p id="719">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PRADA:Protecting against DNN model stealing attacks[OL]">

                                <b>[7]</b>Juuti M,Szyller S,Dmitrenko A,et al.PRADA:Protecting against DNN model stealing attacks[OL].[2018-05-28].http://arxiv.org/abs/1805.02628
                            </a>
                        </p>
                        <p id="721">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Defending against machine learning model stealing attacks using deceptive perturbations[OL]">

                                <b>[8]</b>Lee T,Edwards B,Molloy I,et al.Defending against machine learning model stealing attacks using deceptive perturbations[OL].[2018-05-28].http://arxiv.org/abs/1806.00054
                            </a>
                        </p>
                        <p id="723">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reverse engineering convolutional neural networks through side-channel information leaks">

                                <b>[9]</b>Hua Weizhe,Zhang Zhiru,Suh G.Reverse engineering convolutional neural networks through side-channel information leaks[C] //Proc of the 55th IEEE Design Automation Conference.Piscataway,NJ:IEEE,2018:1- 6
                            </a>
                        </p>
                        <p id="725">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model extraction warning in MLaaS paradigm[OL]">

                                <b>[10]</b>Kesarwani M,Mukhoty B,Arya V,et al.Model extraction warning in MLaaS paradigm[OL].[2018-05-28].http://arxiv.org/abs/1711.07221
                            </a>
                        </p>
                        <p id="727">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003762447&amp;v=MTg3NTZhck80SHRIUHFJbEhZTzhJWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ3JsVjdyTEkxWT1OajdC&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Barreno M,Nelson B,Joseph A D,et al.The security of machine learning[J].Machine Learning,2010,81(2):121- 148
                            </a>
                        </p>
                        <p id="729">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Concrete problems in AI safety[OL]">

                                <b>[12]</b>Amodei D,Olah C,Steinhardt J,et al.Concrete problems in AI safety[OL].[2018-05-28].http://arxiv.org/abs/1606.06565
                            </a>
                        </p>
                        <p id="731">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards the science of security and privacy in machine learning[OL]">

                                <b>[13]</b>Papernot N,McDaniel P,Sinha A,et al.Towards the science of security and privacy in machine learning[OL].[2018-05-28].http://arxiv.org/abs/1611.03814
                            </a>
                        </p>
                        <p id="733">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Security and privacy issues in deep learning[OL]">

                                <b>[14]</b>Bae H,Jang J,Jung D,et al.Security and privacy issues in deep learning[OL].[2018-05-28].http://arxiv.org/abs/1807.11655
                            </a>
                        </p>
                        <p id="735">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SoK:Security and privacy in machine learning">

                                <b>[15]</b>Papernot N,McDaniel P,Sinha A,et al.SoK:Security and privacy in machine learning[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2018:399- 414
                            </a>
                        </p>
                        <p id="737">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey on security threats and defensive techniques of machine learning:a data driven view">

                                <b>[16]</b>Liu Qiang,Li Pan,Zhao Wentao,et al.A survey on security threats and defensive techniques of machine learning:A data driven view[J].IEEE Access,2018,(6):12103- 12117
                            </a>
                        </p>
                        <p id="739">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Threat of adversarial attacks on deep learning in computer vision:a survey">

                                <b>[17]</b>Akhtar N,Mian A.Threat of adversarial attacks on deep learning in computer vision:A survey[J].IEEE Access,2018,(6):14410- 14430
                            </a>
                        </p>
                        <p id="741">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepSec:A uniform platform for security analysis of deep learning model">

                                <b>[18]</b>Ling Xiang,Ji Shouling,Zou Jiaxu,et al.DeepSec:A uniform platform for security analysis of deep learning model[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2019:673- 690
                            </a>
                        </p>
                        <p id="743">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data poisoning attacks against autoregressive models">

                                <b>[19]</b>Alfeld S,Zhu Xiaojin,Barford P.Data poisoning attacks against autoregressive models[C] //Proc of the 30th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2016:1452- 1458
                            </a>
                        </p>
                        <p id="745">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep models under the GAN:Information leakage from collaborative deep learning">

                                <b>[20]</b>Hitaj B,Ateniese G,Perez-Cruz F.Deep models under the GAN:Information leakage from collaborative deep learning[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:603- 618
                            </a>
                        </p>
                        <p id="747">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning models that remember too much">

                                <b>[21]</b>Song Congzheng,Ristenpart T,Shmatikov V.Machine learning models that remember too much[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:587- 601
                            </a>
                        </p>
                        <p id="749">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Algorithms that remember:Model inversion attacks and data protection law[OL]">

                                <b>[22]</b>Veale M,Binns R,Edwards L.Algorithms that remember:Model inversion attacks and data protection law[OL].[2018-05-28].http://arxiv.org/abs/1807.04644
                            </a>
                        </p>
                        <p id="751">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gender Shades:Intersectional Accuracy Disparities in Commercial Gender Classification">

                                <b>[23]</b>Buolamwini J,Gebru T.Gender shades:Intersectional accuracy disparities in commercial gender classification[C] //Proc of the Conf on Fairness,Accountability and Transparency.New York:ACM,2018:77- 91
                            </a>
                        </p>
                        <p id="753">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Differentially private empirical risk minimization revisited:Faster and more general">

                                <b>[24]</b>Wang Di,Ye Minwei,Xu Jinhui.Differentially private empirical risk minimization revisited:Faster and more general[C] //Proc of the Annual Conf on Neural Information Processing Systems.New York:NIPS,2017:2719- 2728
                            </a>
                        </p>
                        <p id="755">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy">

                                <b>[25]</b>Bachrach R,Dowlin M,Laine K,et al.CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy[C] //Proc of the 33nd Int Conf on Machine Learning.New York:ACM,2016:201- 210
                            </a>
                        </p>
                        <p id="757">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Secure outsourced matrix computation and application to neural networks">

                                <b>[26]</b>Jiang Xiaoqian,Kim M,Lauter K,et al.Secure outsourced matrix computation and application to neural networks[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:1209- 1222
                            </a>
                        </p>
                        <p id="759">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SecureML:A system for scalable privacy-preserving machine learning">

                                <b>[27]</b>Mohassel P,Zhang Yupeng.SecureML:A system for scalable privacy-preserving machine learning[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:19- 38
                            </a>
                        </p>
                        <p id="761">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SecureNN:Efficient and private neural network training[OL]">

                                <b>[28]</b>Wagh S,Gupta D,Chandran N.SecureNN:Efficient and private neural network training[OL].[2018-05-28].https://eprint.iacr.org/2018/442
                            </a>
                        </p>
                        <p id="763">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stealing hyperparameters in machine learning">

                                <b>[29]</b>Wang Binghui,Gong Zhenqiang.Stealing hyperparameters in machine learning[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2018:36- 52
                            </a>
                        </p>
                        <p id="765">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Calibrating data to sensitivity in private data analysis">

                                <b>[30]</b>Proserpio D,Goldberg S,Mcsherry F.Calibrating data to sensitivity in private data analysis[J].Proceedings of the VLDB Endowment,2014,7(8):637- 648
                            </a>
                        </p>
                        <p id="767">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Our data,ourselves:Privacy via distributed noise generation">

                                <b>[31]</b>Dwork C,Kenthapadi K,Mcsherry F,et al.Our data,ourselves:Privacy via distributed noise generation[C] //Proc of the 25th Annual Int Conf on the Theory and Applications of Cryptographic Techniques.Berlin:Springer,2006:486- 503
                            </a>
                        </p>
                        <p id="769">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stealing machine learning models via prediction APIs">

                                <b>[32]</b>Tramèr F,Zhang F,Juels A,et al.Stealing machine learning models via prediction APIs[C] //Proc of the 25th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2016:601- 618
                            </a>
                        </p>
                        <p id="771">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to attack:Adversarial transformation networks">

                                <b>[33]</b>Baluja S,Fischer I.Learning to attack:Adversarial transformation networks[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:2687- 2695
                            </a>
                        </p>
                        <p id="773">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Practical black-box attacks against machine learning">

                                <b>[34]</b>Papernot N,Mcdaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 2017 ACM on Asia Conf on Computer and Communications Security.New York:ACM,2017:506- 519
                            </a>
                        </p>
                        <p id="775">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transferability in machine learning:From phenomena to black-box attacks using adversarial samples[OL]">

                                <b>[35]</b>Papernot N,Mcdaniel P,Goodfellow I.Transferability in machine learning:From phenomena to black-box attacks using adversarial samples[OL].[2018-05-28].http://arxiv.org/abs/1605.07277
                            </a>
                        </p>
                        <p id="777">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards demystifying membership inference attacks[OL]">

                                <b>[36]</b>Truex S,Liu Ling,Gursoy M E,et al.Towards demystifying membership inference attacks[OL].[2018-05-28].http://arxiv.org/abs/1807.09173
                            </a>
                        </p>
                        <p id="779">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Membership inference attacks against machine learning models">

                                <b>[37]</b>Shokri R,Stronati M,Song Congzheng,et al.Membership inference attacks against machine learning models[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:3- 18
                            </a>
                        </p>
                        <p id="781">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[OL]">

                                <b>[38]</b>Salem A,Zhang Yang,Humbert M,et al.ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[OL].[2018-05-28].http://arxiv.org/abs/1806.01246
                            </a>
                        </p>
                        <p id="783">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Knock knock,who&amp;#39;&amp;#39;s there?membership inference on aggregate location data[C/OL]">

                                <b>[39]</b>Pyrgelis A,Troncoso C,De Cristofaro E.Knock knock,who's there?membership inference on aggregate location data[C/OL] //Proc of the 25th Network and Distributed System Security Symp.Reston VA:The Internet Society,2018 [2019-06-11].https://arxiv.org/pdf/1708.06145.pdf
                            </a>
                        </p>
                        <p id="785">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model inversion attacks that exploit confidence information and basic countermeasures">

                                <b>[40]</b>Fredrikson M,Jha S,Ristenpart T.Model inversion attacks that exploit confidence information and basic countermeasures[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2015:1322- 1333
                            </a>
                        </p>
                        <p id="787">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding membership inferences on well-generalized learning models[OL]">

                                <b>[41]</b>Long Yunhui,Bindschaedler V,Wang Lei,et al.Understanding membership inferences on well-generalized learning models[OL].[2018-05-28].http://arxiv.org/abs/1802.04889
                            </a>
                        </p>
                        <p id="789">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative model:Membership attack,generalization and diversity[OL]">

                                <b>[42]</b>Liu Kin,Li Bo,Gao Jie.Generative model:Membership attack,generalization and diversity[OL].[2018-05-28].http://arxiv.org/abs/1805.09898
                            </a>
                        </p>
                        <p id="791">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LOGAN:Evaluating privacy leakage of generative models using generative adversarial networks[OL]">

                                <b>[43]</b>Hayes J,Melis L,Danezis G,et al.LOGAN:Evaluating privacy leakage of generative models using generative adversarial networks[OL].[2018-05-28].http://arxiv.org/abs/1705.07663
                            </a>
                        </p>
                        <p id="793">
                            <a id="bibliography_44" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCW0E2654789EA8F4F43B2D00D65E2B815A&amp;v=MTE4NDBhQnVIWWZPR1FsZkNwYlEzNU5waHdMMjh3NkE9TmlmSWViUE5ITmZKcTRoTmJaNStCQW85dVJJUW1EMEpTSCtXcWhkQWU4Q2NSTC91Q09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[44]</b>Ateniese G,Mancini L V,Spognardi A,et al.Hacking smart machines with smarter ones:How to extract meaningful data from machine learning classifiers[J].International Journal of Security &amp; Networks,2015,10(3):137- 150
                            </a>
                        </p>
                        <p id="795">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Property inference attacks on fully connected neural networks using permutation invariant representations">

                                <b>[45]</b>Ganju K,Wang Qi,Yang Wei,et al.Property inference attacks on fully connected neural networks using permutation invariant representations[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:619- 633
                            </a>
                        </p>
                        <p id="797">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inference attacks against collaborative learning[OL]">

                                <b>[46]</b>Melis L,Song Congzheng,Cristofaro E,et al.Inference attacks against collaborative learning[OL].[2018-05-28].http://arxiv.org/abs/1805.04049
                            </a>
                        </p>
                        <p id="799">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving logistic regres-sion">

                                <b>[47]</b>Chaudhuri K,Monteleoni C.Privacy-preserving logistic regression[C] //Proc of the 22nd Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2008:289- 296
                            </a>
                        </p>
                        <p id="801">
                            <a id="bibliography_48" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient private ERM for smooth objectives">

                                <b>[48]</b>Zhang Jiaqi,Zheng Kai,Mou Wenlong,et al.Efficient private ERM for smooth objectives[C] //Proc of the 26th Int Joint Conf on Artificial Intelligence.San Francisco,CA:Morgan Kaufmann,2017:3922- 3928
                            </a>
                        </p>
                        <p id="803">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Differentially private empirical risk minimization">

                                <b>[49]</b>Chaudhuri K,Monteleoni C,Sarwate D.Differentially private empirical risk minimization[J].Journal of Machine Learning Research,2011,12:1069- 1109
                            </a>
                        </p>
                        <p id="805">
                            <a id="bibliography_50" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Private convex optimization for empirical risk minimization with applications to high-dimensional regression">

                                <b>[50]</b>Kifer D,Smith A,Thakurta A.Private convex optimization for empirical risk minimization with applications to high-dimensional regression[C] //Proc of the 25th Annual Conf on Learning Theory.Berlin:Springer,2012:No.25
                            </a>
                        </p>
                        <p id="807">
                            <a id="bibliography_51" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Private empirical risk minimization beyond the worst case:The effect of the constraint set geometry[OL]">

                                <b>[51]</b>Talwar K,Thakurta A,Zhang Li.Private empirical risk minimization beyond the worst case:The effect of the constraint set geometry[OL].[2018-05-28].http://arxiv.org/abs/1411.5417
                            </a>
                        </p>
                        <p id="809">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent with differentially private updates">

                                <b>[52]</b>Song Shuang,Chaudhuri K,Sarwate A D.Stochastic gradient descent with differentially private updates[C] //Proc of the IEEE Global Conf on Signal and Information Processing.Piscataway,NJ:IEEE,2013:245- 248
                            </a>
                        </p>
                        <p id="811">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Private empirical risk minimization:ffficient algorithms and tight error bounds">

                                <b>[53]</b>Bassily R,Thakurta A.Private empirical risk minimization:ffficient algorithms and tight error bounds[C] //Proc of the IEEE Symp on Foundations of Computer Science.Piscataway,NJ:IEEE,2014:464- 473
                            </a>
                        </p>
                        <p id="813">
                            <a id="bibliography_54" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A dual perturbation approach for differential private ADMM-based distributed empirical risk minimization">

                                <b>[54]</b>Zhang Tao,Zhu Quanqing.A dual perturbation approach for differential private ADMM-based distributed empirical risk minimization[C] //Proc of the 2016 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2016:129- 137
                            </a>
                        </p>
                        <p id="815">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning privately from multiparty data">

                                <b>[55]</b>Hamm J,Cao Yingjun,Belkin M.Learning privately from multiparty data[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM,2016:555- 563
                            </a>
                        </p>
                        <p id="817">
                            <a id="bibliography_56" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient deep learning on multi-source private data[OL]">

                                <b>[56]</b>Hynes N,Cheng R,Song D.Efficient deep learning on multi-source private data[OL].[2018-05-28].http://arxiv.org/abs/1807.06689
                            </a>
                        </p>
                        <p id="819">
                            <a id="bibliography_57" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000031480&amp;v=MDQ5NjZmSVk3SzdIdGpOcjQ5RlpPZ09DSFE1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZzWGFSbz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[57]</b>McSherry F.Privacy integrated queries:An extensible platform for privacy-preserving data analysis[J].Communications of the ACM,2010,53(9):89- 97
                            </a>
                        </p>
                        <p id="821">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards measuring membership privacy[OL]">

                                <b>[58]</b>Long Yunhui,Bindschaedler V,Gunter C.Towards measuring membership privacy[OL].[2018-05-28].http://arxiv.org/abs/1712.09136
                            </a>
                        </p>
                        <p id="823">
                            <a id="bibliography_59" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Oblivious neural network predictions via miniONN transformations">

                                <b>[59]</b>Liu Jian,Juuti M,Lu Yao,et al.Oblivious neural network predictions via miniONN transformations[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:619- 631
                            </a>
                        </p>
                        <p id="825">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CryptoDL:Deep neural networks over encrypted data[OL]">

                                <b>[60]</b>Hesamifard E,Takabi H,Ghasemi M.CryptoDL:Deep neural networks over encrypted data[OL].[2018-05-28].http://arxiv.org/abs/1711.05189
                            </a>
                        </p>
                        <p id="827">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy-Preserving Deep Learning">

                                <b>[61]</b>Shokri R,Shmatikov V.Privacy-preserving deep learning[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2015:1310- 1321
                            </a>
                        </p>
                        <p id="829">
                            <a id="bibliography_62" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy-preserving deep learning for any activation function[OL]">

                                <b>[62]</b>Phong L T,Phuong T T.Privacy-preserving deep learning for any activation function[OL].[2018-05-28].http://arxiv.org/abs/1809.03272
                            </a>
                        </p>
                        <p id="831">
                            <a id="bibliography_63" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cleaning the null space:A privacy mechanism for predictors">

                                <b>[63]</b>Xu Ke,Cao Tongyi,Shah S,et al.Cleaning the null space:A privacy mechanism for predictors[C] //Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2017:2789- 2795
                            </a>
                        </p>
                        <p id="833">
                            <a id="bibliography_64" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards making systems forget with machine unlearning">

                                <b>[64]</b>Cao Yinzhi,Yang Junfeng.Towards making systems forget with machine unlearning[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2015:463- 480
                            </a>
                        </p>
                        <p id="835">
                            <a id="bibliography_65" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chiron:Privacy-preserving machine learning as a service[OL]">

                                <b>[65]</b>Hunt T,Song Congzheng,Shokri R,et al.Chiron:Privacy-preserving machine learning as a service[OL].[2018-05-28].http://arxiv.org/abs/1803.05961
                            </a>
                        </p>
                        <p id="837">
                            <a id="bibliography_66" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Oblivious multi-party machine learning on trusted processors">

                                <b>[66]</b>Ohrimenko O,Schuster F,Fournet C,et al.Oblivious multi-party machine learning on trusted processors[C] //Proc of the 25th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2016:619- 636
                            </a>
                        </p>
                        <p id="839">
                            <a id="bibliography_67" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nash equilibria of static prediction games">

                                <b>[67]</b>Bruckner M,Scheffer T.Nash equilibria of static prediction games[C] //Proc of the 22nd Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2009:171- 179
                            </a>
                        </p>
                        <p id="841">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting machine learning to subvert your spam filter">

                                <b>[68]</b>Nelson B,Barreno M,Chi F J,et al.Exploiting machine learning to subvert your spam filter[C] //Proc of the 1st USENIX Workshop on Large-Scale Exploits and Emergent Threats.Berkeley,CA:USENIX Assocaiation,2008
                            </a>
                        </p>
                        <p id="843">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The security of latent Dirichlet allocation">

                                <b>[69]</b>Mei Shike,Zhu Xiaojin.The security of latent dirichlet allocation[C] //Proc of the 8th Int Conf on Artificial Intelligence and Statistics.Cambridge,MA:MIT Press,2015:681- 689
                            </a>
                        </p>
                        <p id="845">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Is feature selection secure against training data poisoning?">

                                <b>[70]</b>Xiao Huang,Biggio B,Brown G,et al.Is feature selection secure against training data poisoning?[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM,2015:1689- 1698
                            </a>
                        </p>
                        <p id="847">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust linear regression against training data poisoning">

                                <b>[71]</b>Liu Chang,Li Bo,Vorobeychik Y,et al.Robust linear regression against training data poisoning[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:91- 102
                            </a>
                        </p>
                        <p id="849">
                            <a id="bibliography_72" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mitigating poisoning attacks on machine learning models:A data provenance based approach">

                                <b>[72]</b>Baracaldo N,Chen B,Ludwig H,et al.Mitigating poisoning attacks on machine learning models:A data provenance based approach[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:103- 110
                            </a>
                        </p>
                        <p id="851">
                            <a id="bibliography_73" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining and harnessing adversarial examples[OL]">

                                <b>[73]</b>Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1412.6572
                            </a>
                        </p>
                        <p id="853">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">

                                <b>[74]</b>Papernot N,Mcdaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:372- 387
                            </a>
                        </p>
                        <p id="855">
                            <a id="bibliography_75" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards evaluating the robustness of neural networks">

                                <b>[75]</b>Carlini N,Wagner D.Towards evaluating the robustness of neural networks[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:39- 57
                            </a>
                        </p>
                        <p id="857">
                            <a id="bibliography_76" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepfool:A simple and accurate method to fool deep neural networks">

                                <b>[76]</b>Moosavi-Dezfooli S M,Fawzi A,Frossard P.DeepFool:A simple and accurate method to fool deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2574- 2582
                            </a>
                        </p>
                        <p id="859">
                            <a id="bibliography_77" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CommanderSong:A systematic approach for practical adversarial voice recognition">

                                <b>[77]</b>Yuan Xuejing,Chen Yuxuan,Zhao Yue,et al.CommanderSong:A systematic approach for practical adversarial voice recognition[C] //Proc of the 27th USENIX Security Symp.Berkeley,CA:USENIX Assocaiation,2018:49- 64
                            </a>
                        </p>
                        <p id="861">
                            <a id="bibliography_78" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Audio adversarial examples:Targeted attacks on speech-to-text">

                                <b>[78]</b>Carlini N,Wagner D.Audio adversarial examples:Targeted attacks on speech-to-text[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:1- 7
                            </a>
                        </p>
                        <p id="863">
                            <a id="bibliography_79" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Black-box generation of adversarial text sequences to evade deep learning classifiers">

                                <b>[79]</b>Gao Ji,Lanchantin J,Soffa M L,et al.Black-box generation of adversarial text sequences to evade deep learning classifiers[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:50- 56
                            </a>
                        </p>
                        <p id="865">
                            <a id="bibliography_80" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deceiving end-to-end deep learning malware detectors using adversarial examples[C/OL]">

                                <b>[80]</b>Kreuk F,Barak A,Aviv-Reuven S,et al.Deceiving end-to-end deep learning malware detectors using adversarial examples[C/OL] //Proc of the 32nd Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2018 [2019-06-11].https://arxiv.org/abs/1802.04528
                            </a>
                        </p>
                        <p id="867">
                            <a id="bibliography_81" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Universal adversarial perturbations">

                                <b>[81]</b>Moosavi-Dezfooli S M,Fawzi A,Fawzi O,et al.Universal adversarial perturbations[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:86- 94
                            </a>
                        </p>
                        <p id="869">
                            <a id="bibliography_82" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning with a strong adversary[OL]">

                                <b>[82]</b>Huang Ruitong,Xu Bing,Schuurmans D,et al.Learning with a strong adversary[OL].[2018-05-28].http://arxiv.org/abs/1511.03034
                            </a>
                        </p>
                        <p id="871">
                            <a id="bibliography_83" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust deep learning via reverse cross-entropy training and thresholding test[OL]">

                                <b>[83]</b>Pang Tianyu,Du Chao,Zhu Jun.Robust deep learning via reverse cross-entropy training and thresholding test[OL].[2018-05-28].http://arxiv.org/abs/1706.00633
                            </a>
                        </p>
                        <p id="873">
                            <a id="bibliography_84" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pixel defend:Leveraging generative models to understand and defend against adversarial examples">

                                <b>[84]</b>Song Yang,Kim T,Nowozin S,et al.PixelDefend:Leveraging generative models to understand and defend against adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1710.10766
                            </a>
                        </p>
                        <p id="875">
                            <a id="bibliography_85" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards deep learning models resistant to adversarial attacks[OL]">

                                <b>[85]</b>Madry A,Makelov A,Schmidt L,et al.Towards deep learning models resistant to adversarial attacks[OL].[2018-05-28].http://arxiv.org/abs/1706.06083
                            </a>
                        </p>
                        <p id="877">
                            <a id="bibliography_86" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the effectiveness of defensive distillation[OL]">

                                <b>[86]</b>Papernot N,Mcdaniel P.On the effectiveness of defensive distillation[OL].[2018-05-28].http://arxiv.org/abs/1607.05113
                            </a>
                        </p>
                        <p id="879">
                            <a id="bibliography_87" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient defenses against adversarial attacks">

                                <b>[87]</b>Zantedeschi V,Nicolae M I,Rawat A.Efficient defenses against adversarial attacks[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:39- 49
                            </a>
                        </p>
                        <p id="881">
                            <a id="bibliography_88" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards deep neural network architectures robust to adversarial examples[OL]">

                                <b>[88]</b>Gu Shixiang,Rigazio L.Towards deep neural network architectures robust to adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1412.5068
                            </a>
                        </p>
                        <p id="883">
                            <a id="bibliography_89" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Poisoning Attacks against Support Vector Machines">

                                <b>[89]</b>Biggio B,Nelson B,Laskov P.Poisoning attacks against support vector machines[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM,2012
                            </a>
                        </p>
                        <p id="885">
                            <a id="bibliography_90" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD15122400027214&amp;v=MDczNzZRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZzWGFSbz1OaVRCYXJLOUg5UE9xNDlGWk9rSURuMDlvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[90]</b>Xiao Han,Xiao Huang,Eckert C.Adversarial label flips attack on support vector machines[C] //Proc of the 20th European Conf on Artificial Intelligence.Ohmsha,Japan:IOS,2012:870- 875
                            </a>
                        </p>
                        <p id="887">
                            <a id="bibliography_91" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Is data clustering in adversarial settings secure?">

                                <b>[91]</b>Biggio B,Pillai I,Bulo S R,et al.Is data clustering in adversarial settings secure?[C] //Proc of the 2013 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2013:87- 98
                            </a>
                        </p>
                        <p id="889">
                            <a id="bibliography_92" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using machine teaching to identify optimal training-set attacks on machine learners">

                                <b>[92]</b>Mei Shike,Zhu Xiaojin.Using machine teaching to identify optimal training-set attacks on machine learners[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2015:2871- 2877
                            </a>
                        </p>
                        <p id="891">
                            <a id="bibliography_93" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manipulating machine learning:Poisoning attacks and countermeasures for regression learning">

                                <b>[93]</b>Jagielski M,Oprea A,Biggio B,et al.Manipulating machine learning:Poisoning attacks and countermeasures for regression learning[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:19- 35
                            </a>
                        </p>
                        <p id="893">
                            <a id="bibliography_94" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Poison frogs! targeted clean-label poisoning attacks on neural networks[OL]">

                                <b>[94]</b>Shafahi A,Huang W R,Najibi M,et al.Poison frogs! targeted clean-label poisoning attacks on neural networks[OL].[2018-05-28].http://arxiv.org/abs/1804.00792
                            </a>
                        </p>
                        <p id="895">
                            <a id="bibliography_95" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Certified defenses for data poisoning attacks">

                                <b>[95]</b>Steinhardt J,Koh P W,Liang Pang.Certified defenses for data poisoning attacks[C] //Proc of Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2017:3520- 3532
                            </a>
                        </p>
                        <p id="897">
                            <a id="bibliography_96" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards poisoning of deep learning algorithms with back-gradient optimization">

                                <b>[96]</b>Muňoz-González L,Biggio B,Demontis A,et al.Towards poisoning of deep learning algorithms with back-gradient optimization[C] //Proc of the 2017 ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:27- 38
                            </a>
                        </p>
                        <p id="899">
                            <a id="bibliography_97" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data poisoning attacks against online learning[OL]">

                                <b>[97]</b>Wang Yizhen,Chaudhuri K.Data poisoning attacks against online learning[OL].[2018-05-28].http://arxiv.org/abs/1808.08994
                            </a>
                        </p>
                        <p id="901">
                            <a id="bibliography_98" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crafting adversarial examples for speech paralinguistics applications[OL]">

                                <b>[98]</b>Gong Yuan,Poellabauer C.Crafting adversarial examples for speech paralinguistics applications[OL].[2018-05-28].http://arxiv.org/abs/1711.03280
                            </a>
                        </p>
                        <p id="903">
                            <a id="bibliography_99" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MtNet:A multi-task neural network for dynamic malware classification">

                                <b>[99]</b>Huang Wenyi,Stokes J W.MtNet:A multi-task neural network for dynamic malware classification[C] //Proc of Detection of Intrusions and Malware,and Vulnerability Assessment.Berlin:Springer,2016:399- 418
                            </a>
                        </p>
                        <p id="905">
                            <a id="bibliography_100" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crafting adversarial input sequences for recurrent neural networks">

                                <b>[100]</b>Papernot N,Mcdaniel P,Swami A,et al.Crafting adversarial input sequences for recurrent neural networks[C] //Proc of the IEEE Military Communications Conf.Piscataway,NJ:IEEE,2016:49- 54
                            </a>
                        </p>
                        <p id="907">
                            <a id="bibliography_101" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Malware classification with recurrent networks">

                                <b>[101]</b>Pascanu R,Stokes J W,Sanossian H,et al.Malware classification with recurrent networks[C] //Proc of the IEEE Int Conf on Acoustics,Speech and Signal.Piscataway,NJ:IEEE,2015:1916- 1920
                            </a>
                        </p>
                        <p id="909">
                            <a id="bibliography_102" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bringing a GAN to a knife-fight:Adapting malware communication to avoid detection">

                                <b>[102]</b>Rigaki M,Garcia S.Bringing a GAN to a knife-fight:Adapting malware communication to avoid detection[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:70- 75
                            </a>
                        </p>
                        <p id="911">
                            <a id="bibliography_103" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Black-box attacks against RNN based malware detection algorithms">

                                <b>[103]</b>Hu Weiwei,Tan Ying.Black-box attacks against RNN based malware detection algorithms[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:245- 251
                            </a>
                        </p>
                        <p id="913">
                            <a id="bibliography_104" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating adversarial malware examples for black-box attacks based on GAN[OL]">

                                <b>[104]</b>Hu Weiwei,Tan Ying.Generating adversarial malware examples for black-box attacks based on GAN[OL].[2018-05-28].http://arxiv.org/abs/1702.05983
                            </a>
                        </p>
                        <p id="915">
                            <a id="bibliography_105" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generic black-box end-to-end attack against state of the art API call based malware classifiers">

                                <b>[105]</b>Rosenberg I,Shabtai A,Rokach L,et al.Generic black-box end-to-end attack against state of the art API call based malware classifiers[C] //Proc of the 21st Int Symp on Research in Attacks,Intrusions and Defenses.Berlin:Springer,2018:490- 510
                            </a>
                        </p>
                        <p id="917">
                            <a id="bibliography_106" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial deep learning for robust detection of binary encoded malware">

                                <b>[106]</b>Al-Dujaili A,Huang A,Hemberg E,et al.Adversarial deep learning for robust detection of binary encoded malware[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:76- 82
                            </a>
                        </p>
                        <p id="919">
                            <a id="bibliography_107" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial examples in the physical world">

                                <b>[107]</b>Kurakin A,Goodfellow I,Bengio S.Adversarial examples in the physical world[OL].[2018-05-28].http://arxiv.org/abs/1607.02533
                            </a>
                        </p>
                        <p id="921">
                            <a id="bibliography_108" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble adversarial training:attacks and defenses">

                                <b>[108]</b>Tramèr F,Kurakin A,Papernot N,et al.Ensemble adversarial training:Attacks and defenses[OL].[2018-05-28].http://arxiv.org/abs/1705.07204
                            </a>
                        </p>
                        <p id="923">
                            <a id="bibliography_109" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial machine learning at scale[OL]">

                                <b>[109]</b>Kurakin A,Goodfellow I,Bengio S.Adversarial machine learning at scale[OL].[2018-05-28].http://arxiv.org/abs/1611.01236
                            </a>
                        </p>
                        <p id="925">
                            <a id="bibliography_110" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boosting adversarial attacks with momentum">

                                <b>[110]</b>Dong Yinpeng,Liao Fangzhou,Pang Tianyu,et al.Boosting adversarial attacks with momentum[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:9185- 9193
                            </a>
                        </p>
                        <p id="927">
                            <a id="bibliography_111" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distillation as a defense to adversarial perturbations against deep neural networks">

                                <b>[111]</b>Papernot N,Mcdaniel P,Wu Xi,et al.Distillation as a defense to adversarial perturbations against deep neural networks[C] //Proc of the IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2016:582- 597
                            </a>
                        </p>
                        <p id="929">
                            <a id="bibliography_112" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EAD:Elastic-net attacks to deep neural networks via adversarial examples">

                                <b>[112]</b>Chen P Y,Sharma Y,Zhang Huan,et al.EAD:Elastic-net attacks to deep neural networks via adversarial examples[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:10- 17
                            </a>
                        </p>
                        <p id="931">
                            <a id="bibliography_113" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decision boundary analysis of adversarial examples[C/OL]">

                                <b>[113]</b>He W,Li Bo,Song D.Decision boundary analysis of adversarial examples[C/OL] //Proc of Int Conf on Learning Representations.2018 [2019-06-11].https://openreview.net/pdf?id=BkpiPMbA-
                            </a>
                        </p>
                        <p id="933">
                            <a id="bibliography_114" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective metrics and gradient descent algorithms for adversarial examples in machine learning">

                                <b>[114]</b>Jang U,Wu Xi,Jha S.Objective metrics and gradient descent algorithms for adversarial examples in machine learning[C] //Proc of the 33rd Annual Computer Security Applications Conf.New York:ACM,2017:262- 277
                            </a>
                        </p>
                        <p id="935">
                            <a id="bibliography_115" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning universal adversarial perturbations with generative models">

                                <b>[115]</b>Hayes J,Danezis G.Learning universal adversarial perturbations with generative models[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:43- 49
                            </a>
                        </p>
                        <p id="937">
                            <a id="bibliography_116" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The space of transferable adversarial examples[OL]">

                                <b>[116]</b>Tramèr F,Papernot N,Goodfellow I,et al.The space of transferable adversarial examples[OL].[2018-05-28].http://arxiv.org/abs/1704.03453
                            </a>
                        </p>
                        <p id="939">
                            <a id="bibliography_117" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simple black-box adversarial attacks on deep neural networks">

                                <b>[117]</b>Narodytska N,Kasiviswanathan S.Simple black-box adversarial attacks on deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:1310- 1318
                            </a>
                        </p>
                        <p id="941">
                            <a id="bibliography_118" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Query-efficient black-box adversarial examples (superceded)[OL]">

                                <b>[118]</b>Ilyas A,Engstrom L,Athalye A,et al.Query-efficient black-box adversarial examples (superceded)[OL].[2018-05-28].http://arxiv.org/abs/1712.07113
                            </a>
                        </p>
                        <p id="943">
                            <a id="bibliography_119" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using machine teaching to identify optimal training-set attacks on machine learners">

                                <b>[119]</b>Mei Shike,Zhu Xiaojin.Using machine teaching to identify optimal training-set attacks on machine learners[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2015:2871- 2877
                            </a>
                        </p>
                        <p id="945">
                            <a id="bibliography_120" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial attacks on neural network policies[OL]">

                                <b>[120]</b>Huang S,Papernot N,Goodfellow I,et al.Adversarial attacks on neural network policies[OL].[2018-05-28].http://arxiv.org/abs/1702.02284
                            </a>
                        </p>
                        <p id="947">
                            <a id="bibliography_121" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial examples for generative models">

                                <b>[121]</b>Kos J,Fischer I,Song D.Adversarial examples for generative models[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:36- 42
                            </a>
                        </p>
                        <p id="949">
                            <a id="bibliography_122" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chaining for securing data provenance in distributed information networks">

                                <b>[122]</b>Wang Xinlei,Zeng Kai,Govindan K,et al.Chaining for securing data provenance in distributed information networks[C] //Proc of the 31st IEEE Military Communications Conf.Piscataway,NJ:IEEE,2012:1- 6
                            </a>
                        </p>
                        <p id="951">
                            <a id="bibliography_123" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trusted computing and provenance:Better together[C/OL]">

                                <b>[123]</b>Lyle J,Martin A P.Trusted computing and provenance:Better together[C/OL] //Proc of the 2nd Workshop on the Theory and Practice of Provenance.Berkeley,CA:USENIX Association,2010 [2019-06-11].https://www.usenix.org/legacy/event/tapp10/tech/full_papers/lyle.pdf
                            </a>
                        </p>
                        <p id="953">
                            <a id="bibliography_124" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Case of the Fake Picasso:Preventing History Forgery with Secure Provenance">

                                <b>[124]</b>Hasan R,Sion R,Winslett M.The case of the fake picasso:Preventing history forgery with secure provenance[C] //Proc of the Conf on File and Storage Technologies.Berkeley,CA:USENIX Assocaiation,2009:1- 14
                            </a>
                        </p>
                        <p id="955">
                            <a id="bibliography_125" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data provenance model for internet of things (IoT) systems">

                                <b>[125]</b>Olufowobi H,Engel R,Baracaldo N,et al.Data provenance model for internet of things (IoT) systems[C] //Proc of Int Conf on Service-oriented Computing.Berlin:Springer,2016:85- 91
                            </a>
                        </p>
                        <p id="957">
                            <a id="bibliography_126" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Debugging machine learning tasks[OL]">

                                <b>[126]</b>Chakarov A,Nori A,Rajamani S,et al.Debugging machine learning tasks[OL].[2018-05-28].http://arxiv.org/abs/1603.07292
                            </a>
                        </p>
                        <p id="959">
                            <a id="bibliography_127" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000001869&amp;v=MTM5MzFJSkZzWGFSbz1OaWZJWTdLN0h0ak5yNDlGWk9zT0JIb3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMMw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[127]</b>Candès E J,Li Xiaodong,Ma Yi,et al.Robust principal component analysis?[J].Journal of the ACM,2011,58(3):11.1- 11.37
                            </a>
                        </p>
                        <p id="961">
                            <a id="bibliography_128" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust high dimensional sparse regression and matching pursuit[OL]">

                                <b>[128]</b>Chen Yudong,Caramanis C,Mannor S.Robust high dimensional sparse regression and matching pursuit[OL].[2018-05-28].http://arxiv.org/abs/1301.2725
                            </a>
                        </p>
                        <p id="963">
                            <a id="bibliography_129" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust logistic regression and classification">

                                <b>[129]</b>Feng Jiashi,Xu Huan,Mannor S,et al.Robust logistic regression and classification[C] //Proc of Annual Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2014:253- 261
                            </a>
                        </p>
                        <p id="965">
                            <a id="bibliography_130" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mitigating evasion attacks to deep neural networks via region-based classification">

                                <b>[130]</b>Cao Xiaoyu,Gong N Z.Mitigating evasion attacks to deep neural networks via region-based classification[C] //Proc of the 33rd Annual Computer Security Applications Conf.New York:ACM,2017:278- 287
                            </a>
                        </p>
                        <p id="967">
                            <a id="bibliography_131" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Characterizing adversarial subspaces using local intrinsic dimensionality">

                                <b>[131]</b>Ma Xingjun,Li Bo,Wang Yisen,et al.Characterizing adversarial subspaces using local intrinsic dimensionality[OL].[2018-05-28].http://arxiv.org/abs/1801.02613
                            </a>
                        </p>
                        <p id="969">
                            <a id="bibliography_132" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Background class defense against adversarial examples">

                                <b>[132]</b>Mccoyd M,Wagner D.Background class defense against adversarial examples[C] //Proc of the IEEE Security and Privacy Workshops.Piscataway,NJ:IEEE,2018:96- 102
                            </a>
                        </p>
                        <p id="971">
                            <a id="bibliography_133" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Countering adversarial images using input transformations">

                                <b>[133]</b>Guo Chuan,Rana M,Cisse M,et al.Countering adversarial images using input transformations[OL].[2018-05-28].http://arxiv.org/abs/1711.00117
                            </a>
                        </p>
                        <p id="973">
                            <a id="bibliography_134" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mitigating adversarial effects through randomization">

                                <b>[134]</b>Xie Cihang,Wang Jianyu,Zhang Zhishuai,et al.Mitigating adversarial effects through randomization[OL].[2018-05-28].http://arxiv.org/abs/1711.01991
                            </a>
                        </p>
                        <p id="975">
                            <a id="bibliography_135" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting adversarial samples for deep neural networks through mutation testing[OL]">

                                <b>[135]</b>Wang Jingyi,Sun Jun,Zhang Peixin,et al.Detecting adversarial samples for deep neural networks through mutation testing[OL].[2018-05-28].http://arxiv.org/abs/1805.05010
                            </a>
                        </p>
                        <p id="977">
                            <a id="bibliography_136" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting adversarial examples through image transformation">

                                <b>[136]</b>Tian Shixin,Yang Guolei,Cai Ying.Detecting adversarial examples through image transformation[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:4139- 4146
                            </a>
                        </p>
                        <p id="979">
                            <a id="bibliography_137" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Thermometer encoding:One hot way to resist adversarial examples[C/OL]">

                                <b>[137]</b>Buckman J,Roy A,Raffel C,et al.Thermometer encoding:One hot way to resist adversarial examples[C/OL] //Proc of the Int Conf on Learning Representations.2018 [2019-06-11].https://openreview.net/pdf?id=S18Su--CW
                            </a>
                        </p>
                        <p id="981">
                            <a id="bibliography_138" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients">

                                <b>[138]</b>Ross A S,Doshi-Velez F.Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park,CA:AAAI,2018:1660- 1669
                            </a>
                        </p>
                        <p id="983">
                            <a id="bibliography_139" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting adversarial image examples in deep networks with adaptive noise reduction[C/OL]">

                                <b>[139]</b>Liang Bin,Li Hongcheng,Su Miaoqiang,et al.Detecting adversarial image examples in deep networks with adaptive noise reduction[C/OL] //Proc of the IEEE Transactions on Dependable and Secure Computing.Piscataway,NJ:IEEE,2018:1- 1 [2019-06-11].https://arxiv.org/pdf/1705.08378.pdf
                            </a>
                        </p>
                        <p id="985">
                            <a id="bibliography_140" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature squeezing:Detecting adversarial examples in deep neural networks[C/OL]">

                                <b>[140]</b>Xu Weilin,Evans D,Qi Yanjun.Feature squeezing:Detecting adversarial examples in deep neural networks[C/OL] //Proc of the 25th Network and Distributed System Security Symp.Reston,VA:The Internet Society,2018 [2019-06-11].https://arxiv.org/pdf/1704.01155.pdf
                            </a>
                        </p>
                        <p id="987">
                            <a id="bibliography_141" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Defense against universal adversarial perturbations[OL]">

                                <b>[141]</b>Akhtar N,Liu Jian,Mian A.Defense against universal adversarial perturbations[OL].[2018-05-28].http://arxiv.org/abs/1711.05929
                            </a>
                        </p>
                        <p id="989">
                            <a id="bibliography_142" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Magnet:a two-pronged defense against adversarial examples">

                                <b>[142]</b>Meng Dongyu,Chen Hao.MagNet:A two-pronged defense against adversarial examples[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:135- 147
                            </a>
                        </p>
                        <p id="991">
                            <a id="bibliography_143" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting telecommunication fraud by understanding the contents of a call">

                                <b>[143]</b>Zhao Qianqian,Chen Kai,Li Tongxin,et al.Detecting telecommunication fraud by understanding the contents of a call[J/OL].Cybersecurity,2018 [2019-06-11].https://cybersecurity.springeropen.com/articles/10.1186/s42400-018-0008-5
                            </a>
                        </p>
                        <p id="993">
                            <a id="bibliography_144" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Demystifying hidden privacy settings in mobile apps">

                                <b>[144]</b>Chen Yi,Zha Mingming,Zhang Nan,et al.Demystifying hidden privacy settings in mobile apps[C] //Proc of the IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2019:570- 586
                            </a>
                        </p>
                        <p id="995">
                            <a id="bibliography_145" >
                                    <b>[145]</b>
                                You Wei,Zong Peiyuan,Chen Kai,et al.SemFuzz:Semantics-based automatic generation of proof-of-concept exploits[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2017:2139- 2154■He Yingzhe,born in 1995.PhD.His main research interests include machine learning,artificial intelligence security,and adversarial attack.■Hu Xingbo,born in 1996.Master.Her main research interests include software engineering and machine learning.(huxinbo@iie.ac.cn)■He Jinwen,born in 1997.PhD.Her main research interests include AI security and code analysis.(hejinwen@iie.ac.cn)■Meng Guozhu,born in 1987.PhD from the Nanyang Technological University,Singapore in 2017.Research Fellow at Nanyang Technological University.Visiting Research Fellow at the University of Luxembourg.Associate professor with the Institute of Information Engineering,Chinese Academy of Sciences.His main research interests include mobile security,big data analysis,vulnerability detection,program analysis,and machine learning.(mengguozhu@iie.ac.cn)■Chen Kai,born in 1982.PhD from the University of Chinese Academy of Science in 2010.Professor with the Institute of Information Engineering,Chinese Academy of Sciences.Professor with the University of Chinese Academy of Sciences.His main research interests include software analysis and testing;smartphones and privacy.(chenkai@iie.ac.cn)
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201910003" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201910003&amp;v=MjQzOTMzenFxQnRHRnJDVVJMT2VaZVJzRnl6Z1VMekFMeXZTZExHNEg5ak5yNDlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTNJNVdCTmZTL2VLWjltUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

