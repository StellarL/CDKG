

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128655530275000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201905017%26RESULT%3d1%26SIGN%3doU%252b1nlpzY2SqKEl4CMNPuwZzktQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905017&amp;v=MjUxNzdCdEdGckNVUkxPZVplUnFGeW5sVUwzS0x5dlNkTEc0SDlqTXFvOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="&lt;b&gt;2 视听相关多模态概念集的定义&lt;/b&gt; "><b>2 视听相关多模态概念集的定义</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;2.1 视听相关多模态概念的定义&lt;/b&gt;"><b>2.1 视听相关多模态概念的定义</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;2.2 视听相关多模态概念视频数据的收集&lt;/b&gt;"><b>2.2 视听相关多模态概念视频数据的收集</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="&lt;b&gt;3 端到端的多模态概念检测&lt;/b&gt; "><b>3 端到端的多模态概念检测</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="&lt;b&gt;3.1 单模态网络结构&lt;/b&gt;"><b>3.1 单模态网络结构</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;3.2 视听相关的多模态联合网络学习&lt;/b&gt;"><b>3.2 视听相关的多模态联合网络学习</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;4 多模态概念检测实验分析&lt;/b&gt; "><b>4 多模态概念检测实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="&lt;b&gt;4.1 实验数据&lt;/b&gt;"><b>4.1 实验数据</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;4.2 实验设置&lt;/b&gt;"><b>4.2 实验设置</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;4.3 单模态概念检测结果及分析&lt;/b&gt;"><b>4.3 单模态概念检测结果及分析</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;4.4 多模态概念检测实验结果及分析&lt;/b&gt;"><b>4.4 多模态概念检测实验结果及分析</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;4.5 多模态网络学习的精细化特征&lt;/b&gt;"><b>4.5 多模态网络学习的精细化特征</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;4.6 多模态网络的视听特征有效性分析&lt;/b&gt;"><b>4.6 多模态网络的视听特征有效性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="&lt;b&gt;5 总 结&lt;/b&gt; "><b>5 总 结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 多模态概念的定义和收集">图1 多模态概念的定义和收集</a></li>
                                                <li><a href="#76" data-title="图2 基于视听相关性的联合网络结构图">图2 基于视听相关性的联合网络结构图</a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;表1 多模态概念检测实验数据分布&lt;/b&gt;"><b>表1 多模态概念检测实验数据分布</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表2 基于单模态网络的视频概念检测结果&lt;/b&gt;"><b>表2 基于单模态网络的视频概念检测结果</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表3 &lt;i&gt;AP&lt;/i&gt;最高的前5个概念 (视觉网络&lt;/b&gt;) "><b>表3 <i>AP</i>最高的前5个概念 (视觉网络</b>) </a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;表4 &lt;i&gt;AP&lt;/i&gt;最高的前5个概念 (听觉网络&lt;/b&gt;) "><b>表4 <i>AP</i>最高的前5个概念 (听觉网络</b>) </a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表5 基于视听相关的多模态网络概念检测结果&lt;/b&gt;"><b>表5 基于视听相关的多模态网络概念检测结果</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表6 &lt;i&gt;AP&lt;/i&gt;最高的前5个概念 (基于融合网络3-scratch&lt;/b&gt;) "><b>表6 <i>AP</i>最高的前5个概念 (基于融合网络3-scratch</b>) </a></li>
                                                <li><a href="#112" data-title="图3 融合网络学习到的精细化特征示例">图3 融合网络学习到的精细化特征示例</a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;表7 Huawei视频分类结果&lt;/b&gt;"><b>表7 Huawei视频分类结果</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表8 ESC50音频分类结果&lt;/b&gt;"><b>表8 ESC50音频分类结果</b></a></li>
                                                <li><a href="#169" data-title="表A1 26个动物相关的多模态概念及对应视频数量">表A1 26个动物相关的多模态概念及对应视频数量</a></li>
                                                <li><a href="#170" data-title="表A2 40个日常事物相关的多模态概念及对应视频数量">表A2 40个日常事物相关的多模态概念及对应视频数量</a></li>
                                                <li><a href="#171" data-title="表A3 10个自然相关的多模态概念及对应视频数量">表A3 10个自然相关的多模态概念及对应视频数量</a></li>
                                                <li><a href="#172" data-title="表A4 17个与人相关的多模态概念及对应视频数量">表A4 17个与人相关的多模态概念及对应视频数量</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="209">


                                    <a id="bibliography_1" title="Borth D, Ji Rongrong, Chen Tao, et al.Large-scale visual sentiment ontology and detectors using adjective noun pairs[C] //Proc of the 21st ACM Int Conf on Multimedia.New York:ACM, 2013:223- 232" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale visual sentiment ontology and detectors using adjective noun pairs">
                                        <b>[1]</b>
                                        Borth D, Ji Rongrong, Chen Tao, et al.Large-scale visual sentiment ontology and detectors using adjective noun pairs[C] //Proc of the 21st ACM Int Conf on Multimedia.New York:ACM, 2013:223- 232
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_2" title="Ye Guangnan, Li Yitong, Xu Hongliang, et al.Eventnet:A large scale structured concept library for complex event detection in video[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:471- 480" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eventnet:A large scale structured concept library for complex event detection in video">
                                        <b>[2]</b>
                                        Ye Guangnan, Li Yitong, Xu Hongliang, et al.Eventnet:A large scale structured concept library for complex event detection in video[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:471- 480
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_3" title="Heilbronn F C, Escorial V, Ghanem B, et al.ActivityNet:A large-scale video benchmark for human activity understanding[C] //Proc of 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:961- 970" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Activity Net:A large-scale video benchmark for human activity understanding">
                                        <b>[3]</b>
                                        Heilbronn F C, Escorial V, Ghanem B, et al.ActivityNet:A large-scale video benchmark for human activity understanding[C] //Proc of 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:961- 970
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_4" title="Gemmeke J F, Ellis D P W, Freedman D, et al.Audio set:An ontology and human-labeled dataset for audio events[C] //Proc of 2017 IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:776- 780" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Audio Set:An ontology and human-labeled dataset for audio events">
                                        <b>[4]</b>
                                        Gemmeke J F, Ellis D P W, Freedman D, et al.Audio set:An ontology and human-labeled dataset for audio events[C] //Proc of 2017 IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:776- 780
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_5" title="Liu Yanan, Wu Fei, Zhuang Yueting.Video semantics mining using multi-modality subspace correlation propagation[J].Journal of Computer Research and Development, 2009, 46 (1) :1- 8 (in Chinese) (刘亚楠, 吴飞, 庄越挺.基于多模态子空间相关性传递的视频语义挖掘[J].计算机研究与发展, 2009, 46 (1) :1- 8) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200901002&amp;v=MTMwNDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnlubFVMM0tMeXZTZExHNEh0ak1ybzlGWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Liu Yanan, Wu Fei, Zhuang Yueting.Video semantics mining using multi-modality subspace correlation propagation[J].Journal of Computer Research and Development, 2009, 46 (1) :1- 8 (in Chinese) (刘亚楠, 吴飞, 庄越挺.基于多模态子空间相关性传递的视频语义挖掘[J].计算机研究与发展, 2009, 46 (1) :1- 8) 
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_6" title="Chen Tao, Borth D, Darrell T, et al.Deepsentibank:Visual sentiment concept classification with deep convolutional neural networks[J].arXiv preprint, arXiv, 1410.8586, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepsentibank:Visual sentiment concept classification with deep convolutional neural networks">
                                        <b>[6]</b>
                                        Chen Tao, Borth D, Darrell T, et al.Deepsentibank:Visual sentiment concept classification with deep convolutional neural networks[J].arXiv preprint, arXiv, 1410.8586, 2014
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_7" title="Hershey S, Chaudhuri S, Ellis D P W, et al.CNN architectures for large-scale audio classification[C] //Proc of 2017 IEEE Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2017:131- 135" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CNN architectures for large-scale audio classification">
                                        <b>[7]</b>
                                        Hershey S, Chaudhuri S, Ellis D P W, et al.CNN architectures for large-scale audio classification[C] //Proc of 2017 IEEE Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2017:131- 135
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_8" title="Wang Cheng, Yang Haojin, Meinel C.Exploring multimodal video representation for action recognition[C] //Proc of 2016 IEEE Int Joint Conf on Neural Networks.Piscataway, NJ:IEEE, 2016:1924- 1931" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring multimodal video representation for action recognition">
                                        <b>[8]</b>
                                        Wang Cheng, Yang Haojin, Meinel C.Exploring multimodal video representation for action recognition[C] //Proc of 2016 IEEE Int Joint Conf on Neural Networks.Piscataway, NJ:IEEE, 2016:1924- 1931
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_9" title="Aytar Y, Vondrick C, Torralba A, et al.SoundNet:Learning sound representations from unlabeled video[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc, 2016:892- 900" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SoundNet:Learning sound representations from unlabeled video">
                                        <b>[9]</b>
                                        Aytar Y, Vondrick C, Torralba A, et al.SoundNet:Learning sound representations from unlabeled video[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc, 2016:892- 900
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_10" title="Smeulders A W M, Worring M, Santini S, et al.Content-based image retrieval at the end of the early years[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2000, 22 (12) :1349- 1380" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Content-based image retrieval at the end of the early years">
                                        <b>[10]</b>
                                        Smeulders A W M, Worring M, Santini S, et al.Content-based image retrieval at the end of the early years[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2000, 22 (12) :1349- 1380
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_11" title="Fellbaum C, Miller G.WordNet :An electronic lexical database[J].Library Quarterly Information Community Policy, 1998, 25 (2) :292- 296" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WordNet:an electronic lexical database">
                                        <b>[11]</b>
                                        Fellbaum C, Miller G.WordNet :An electronic lexical database[J].Library Quarterly Information Community Policy, 1998, 25 (2) :292- 296
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                    Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C] //Proc of the 2012 Int Conf on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc, 2012:1097- 1105</a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_13" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint, arXiv:1409.1556, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[13]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint, arXiv:1409.1556, 2014
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_14" title="Arandjelovic R, Zisserman A.Look, listen and learn[C] //Proc of 2017 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:609- 617" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Look,listen and learn">
                                        <b>[14]</b>
                                        Arandjelovic R, Zisserman A.Look, listen and learn[C] //Proc of 2017 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:609- 617
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_15" title="Abadi M, Agarwal A, Barham P, et al.Tensorflow:Large-scale machine learning on heterogeneous distributed systems[C] //Proc of the 12th USENIX Conf on Operating Systems Design and Implementation.Berkeley, CA:USENIX Association, 2016:265- 283" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:a system for large-scale machine learning">
                                        <b>[15]</b>
                                        Abadi M, Agarwal A, Barham P, et al.Tensorflow:Large-scale machine learning on heterogeneous distributed systems[C] //Proc of the 12th USENIX Conf on Operating Systems Design and Implementation.Berkeley, CA:USENIX Association, 2016:265- 283
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_16" title="Kingma D P, Ba J.Adam:A method for stochastic optimization[J].arXiv preprint, arXiv:1412.6980, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization">
                                        <b>[16]</b>
                                        Kingma D P, Ba J.Adam:A method for stochastic optimization[J].arXiv preprint, arXiv:1412.6980, 2014
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_17" title="Sch&#252;tze H, Manning C D, Raghavan P.Introduction to Information Retrieval[M].Cambridge, UK:Cambridge University Press, 2008" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction to Information Retrieval">
                                        <b>[17]</b>
                                        Sch&#252;tze H, Manning C D, Raghavan P.Introduction to Information Retrieval[M].Cambridge, UK:Cambridge University Press, 2008
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_18" title="Piczak K J.ESC:Dataset for environmental sound classification[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:1015- 1018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ESC:dataset for environmental sound classification">
                                        <b>[18]</b>
                                        Piczak K J.ESC:Dataset for environmental sound classification[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:1015- 1018
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(05),1071-1081 DOI:10.7544/issn1000-1239.2019.20180463            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>视听相关的多模态概念检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A5%A0%E9%9B%A8%E6%B4%81&amp;code=38029320&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">奠雨洁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%87%91%E7%90%B4&amp;code=33627965&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">金琴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0198015&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民大学信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着在线视频应用的流行, 互联网上的视频数量快速增长.面对互联网上海量的视频, 人们对视频检索的要求也越来越精细化.如何按照合适的语义概念对视频进行组织和管理, 从而帮助用户更高效、更准确地获取所需视频, 成为亟待解决的问题.在大量的应用场景下, 需要声音和视觉同时出现才能确定某个视频事件.因此, 提出具有视听信息的多模态概念的检测工作.首先, 以名词-动词二元组的形式定义多模态概念, 其中名词表达了视觉信息, 动词表达了听觉信息, 且名词和动词具有语义相关性, 共同表达语义概念所描述的事件.其次, 利用卷积神经网络, 以多模态概念的视听相关性为目标训练多模态联合网络, 进行端到端的多模态概念检测.实验表明:在多模态概念检测任务上, 通过视听相关的联合网络的性能超过了单独的视觉网络和听觉网络.同时, 联合网络能够学习到精细化的特征表示, 利用该网络提取的视觉特征, 在Huawei视频数据集某些特定的类别上超过ImageNet预训练的神经网络特征;联合网络提取的音频特征, 在ESC50数据集上, 也超过在Youtube8m上训练的神经网络音频特征约5.7%.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多模态信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E6%A6%82%E5%BF%B5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义概念;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E6%A6%82%E5%BF%B5%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频概念检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E8%AF%AD%E4%B9%89%E7%90%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频语义理解;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *金琴, qjin@ruc.edu.cn;
                                </span>
                                <span>
                                    奠雨洁, dianyujie-blair@ruc.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61772535);</span>
                                <span>国家重点研发计划基金项目 (2016YFB1001202);</span>
                    </p>
            </div>
                    <h1><b>Audio-Visual Correlated Multimodal Concept Detection</b></h1>
                    <h2>
                    <span>Dian Yujie</span>
                    <span>Jin Qin</span>
            </h2>
                    <h2>
                    <span>School of Information, Renmin University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the wide dissemination of online video sharing applications, massive number of videos are generated online every day. Facing such massive videos, people require more refined retrieval services. How to organize and manage such massive videos on the Internet to help users retrieve videos more efficiently and accurately has become one of the most challenging topics in video analysis. In most scenarios, it is necessary that sounds and visual information appear simultaneously to decide a video event. Therefore, this paper proposes multimodal concept detection task based on audio-visual information. Firstly, a multimodal concept is defined as a noun-verb pair, in which the noun and verb represent visual and audio information separately. The audio and visual information in a multimodal concept is correlated. Secondly, this paper performs end-to-end multimodal concept detection using convolutional neural network. Specifically, audio-visual correlation is considered to train a joint learning network. The experimental results show that performance of the joint network via audio-visual correlation exceeds that of single visual or audio network. Thirdly, the joint network learns fine-grained features. In the Huawei video concept detection task, using visual features extracted from the joint network outperforms features extracted from an ImageNet pre-trained network on some specific concepts. In the ESC 50 audio classification task, acoustic features from the joint network exceeds that from VGG pre-trained on Youtube8 m about 5.7%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multimodal%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multimodal information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20concepts&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic concepts;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20concept%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video concept detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20representation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video representation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=video%20semantic%20understanding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">video semantic understanding;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Dian Yujie, born in 1992.Master.Her main research interests include multimedia information analysis.<image id="151" type="formula" href="images/JFYZ201905017_15100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Jin Qin, born in 1972.PhD, associate professor, PhD supervisor. Her main research interests include intelligent multimedia computing.<image id="154" type="formula" href="images/JFYZ201905017_15400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-06-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61772535);</span>
                                <span>the National Key Research and Development Program of China (2016YFB1001202);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="49">近年来, 计算机和移动网络技术逐渐成熟, 互联网上出现了很多视频分享应用.视频应用的流行, 使互联网上的视频数量快速增长.面对互联网上海量的视频, 如何对视频进行分类、索引, 从而帮助用户更高效、更准确地获取所需视频, 成为亟待解决的问题.视频语义概念检测就是利用计算机自动地检测视频中包含的语义概念, 如场景、事件、动作、事物等.将视频按照语义概念进行组织和管理, 有助于构建结构化的视频检索数据库, 能够更方便地为用户提供基于关键字的检索方式, 从而更有效地为用户提供视频检索服务.</p>
                </div>
                <div class="p1">
                    <p id="50">随着网络视频数量的快速增长, 人们对视频检索的要求也越来越精细化.例如在动物养殖领域, 人们可能需要通过视频了解动物在不同叫声状态下的表现, 因此, 人们可能会检索“dog woof” (狗欢快地叫) 、“dog howl” (狗嚎叫) 或“dog bark” (狗汪汪叫) , 而目前大量的视频层次化管理中并没有提供这样精细化的组织结构, 因此用户只能得到有狗这一视觉形象或者包含任何狗叫声的视频, 而无法区分更细粒度的满足要求的视频.在大量的应用场景下, 需要声音和视觉形象同时出现, 才能确定某个视频事件.目前大量的视频层次化管理中都倾向于将视频按照视觉信息所表达的概念进行组织, 这样的组织方式忽略了视频具有多模态的特性.因此, 本文考虑是否可以将视频语义概念组织成具有视听信息的多模态概念.这类多模态概念通过语义相关的视听信息共同来描述视频主要内容, 将会更有助于满足用户的精细化检索需求.通过以这类语义概念为目标进行概念检测, 应该可以学习到视频底层特征与高层语义之间更精细化的特征表示.</p>
                </div>
                <div class="p1">
                    <p id="51">视频是多模态的信息源, 不同模态的信息通过不同的方式表达和某个语义概念相关的的信息.因此, 对于具有视听信息的多模态语义概念, 自然可以使用视频中内在的视听信息来进行语义概念的检测.视频中的视听信息具有天然的相关性, 与多模态概念本身的视听相关性是一致的, 因此, 本文使用卷积神经网络 (convolutional neural network, CNN) , 利用视听相关性为监督信息进行多模态概念检测的研究.</p>
                </div>
                <div class="p1">
                    <p id="52">本文的主要贡献在于:首先, 本文提出了具有视听信息的多模态语义概念检测任务.多模态概念中视听信息具有相关性, 更能满足用户的精细化检索需求, 也更能准确描述视频中的语义信息.其次, 以多模态概念为检测目标, 本文使用CNN对视频进行端到端的概念检测.除了单模态的概念检测系统, 本文利用视频的视听相关性为目标训练了多模态联合网络, 该网络包含了3个子网:视觉子网、听觉子网、多模态联合学习子网.通过视听相关的多模态联合网络能够学习到精细化的特征表示.利用该网络还能够提取听觉和视觉特征, 进而有效用于其他多媒体分析中.</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="54">目前研究领域常见的语义概念通常包括场景、物体、事件、动作、情感等概念.有多种方式构建不同需求的语义概念.在文献<citation id="245" type="reference">[<a class="sup">1</a>]</citation>中, 作者通过形容词-名词的形式构建了3 000多个具有情感信息的视觉概念集 (visual sentiment ontology, VSO) ;在文献<citation id="246" type="reference">[<a class="sup">2</a>]</citation>中, 作者通过WikiHow (在线问答网站) 中定义的事件标签以及相关文本, 提取了4 490个事件概念形成EventNet, 涵盖日常生活中的运动、家务等事件;在文献<citation id="247" type="reference">[<a class="sup">3</a>]</citation>中, 作者通过美国时间使用调查报告 (American time use survey, ATUS) 中对人类活动的分类, 定义了203个人类活动的概念并组织成层次化的结构形式形成ActivityNet, 这些概念包含了工作、吃喝、家务等不同的日常活动;文献<citation id="248" type="reference">[<a class="sup">4</a>]</citation>中, 作者在网络文本中寻找有声音属性的词, 然后在Freebase等知识库中对原始种子声音词进行匹配和筛选, 得到一个层次化的声音事件概念集AudioSet.</p>
                </div>
                <div class="p1">
                    <p id="55">在这些概念集上进行的语义检测方法大致分为2类:基于模块化流程的语义概念检测和基于神经网络的端到端的语义概念检测.模块化流程的检测方法包括:视频预处理、特征提取、分类器训练和预测.例如, 对于VSO的概念检测, 作者使用相关图片的低层视觉特征 (颜色直方图、局部二值特征等) , 并使用支持向量机 (support vector machine, SVM) 训练了对应的情感概念检测模型<citation id="249" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.对于EventNet的检测, 作者使用在视频上预训练的卷积神经网络 (convolutional neural network, CNN) 提取中间层特征, 基于该特征使用SVM训练了事件概念检测器<citation id="250" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.文献<citation id="251" type="reference">[<a class="sup">5</a>]</citation>中分别提取了视频中图像的颜色和纹理特征, 短时音频帧特征和转录的文本特征, 利用视频的多模态特性和相似度融合的方法, 利用SVM进行常见概念的检测.基于神经网络的端到端的检测方法直接使用视频的帧或音频信息作为输入, 通过网络学习视频与概念之间的相关性.例如文献<citation id="252" type="reference">[<a class="sup">6</a>]</citation>中作者使用CNN输入图像, 进行视觉情感概念的检测.文献<citation id="253" type="reference">[<a class="sup">7</a>]</citation>中, 作者探究了不同的CNN结构, 对一个大规模视频数据集进行分类研究, 其中CNN的输入直接使用视频中音频流的频谱图.文献<citation id="254" type="reference">[<a class="sup">8</a>]</citation>中利用视频的帧图像、光流帧、音频流的声学特征作为CNN的输入, 进行人类动作的识别.</p>
                </div>
                <div class="p1">
                    <p id="56">利用端到端的方式学习视频概念的好处在于, 它保留了视频的原始信息, 减少了特征提取环节可能造成的信息缺失.而且使用CNN通过分层的形式, 可以学习到底层到高层之间的不同层次的特征, 因此网络模型可以当做一个特征提取器, 从中提取特征并有效用于其他任务中.例如文献<citation id="255" type="reference">[<a class="sup">9</a>]</citation>中作者使用了一个学生-老师的CNN结构, 老师网络利用视觉先验知识作为监督, 学生网络输入原始的音频流, 网络能够学习到有效的音频特征表示.通过该网络可以从不同卷积层提取特征, 在音频分类等任务上都表现出优异的性能.因此, 本文将使用CNN探究端到端的概念检测方法, 对视听相关的多模态概念进行概念检测研究.</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag"><b>2 视听相关多模态概念集的定义</b></h3>
                <h4 class="anchor-tag" id="58" name="58"><b>2.1 视听相关多模态概念的定义</b></h4>
                <div class="p1">
                    <p id="59">通过预先定义好的语义概念集为检测目标对视频进行分析, 学习到的视频特征表示更具有语义信息, 可以适当弥补语义鸿沟<citation id="256" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.本文提出具有视听信息的多模态概念.一个多模态概念必须同时具有听觉和视觉信息.因此, 本文提出以名词-动词二元组的形式表示一个语义概念, 从概念的名词中可以感受到视觉信息;从概念的动词中可以感受到听觉信息, 且视听信息是相关的.多模态概念的定义和收集过程如图1所示:</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多模态概念的定义和收集" src="Detail/GetImg?filename=images/JFYZ201905017_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 多模态概念的定义和收集  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The collecting procedure of multimodal concepts</p>

                </div>
                <div class="p1">
                    <p id="61">本文多模态概念, 从声音出发, 其收集过程主要分为2种形式:1) 基于象声词的多模态概念.从英文象声词在WordNet<citation id="257" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>中的定义寻找通过“Sound by (of) ”连接的名词, 该名词表示象声词对应的发声物, 将发声物和象声词组合形成多模态概念;除此之外, 还爬取象声词在字典<citation id="163" type="note"><link href="157" rel="footnote" /><sup>①</sup></citation>中对应的例句, 将例句中抽取的短语关系为“nsubj (名词主语) ”的名词-动词二元组作为多模态概念.2) 从在线音频网站<citation id="164" type="note"><link href="159" rel="footnote" /><sup>②</sup></citation>中对应音频的标题描述中抽取短语关系为“nsubj”的名词-动词二元组, 形成多模态概念.最后, 对收集的名词-动词二元组进行短语清洗和过滤, 删除一些拼写错误或没有意义的词 (如人名、地名、颜色词等) , 形成具有视听信息的多模态概念集, 共包含2 098个概念.</p>
                </div>
                <div class="p1">
                    <p id="62">多模态概念集是从声音出发构建的, 因此自然具有听觉特性.本文将多模态概念的名词输入到ImageNet<citation id="258" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>中查找是否存在相应图片, 分析发现, 78%的多模态概念的名词能够在ImageNet中找到图片, 这说明提出的多模态概念具有视觉信息.概念收集过程中, 通过约束象声词及其发声物和抽取具有语法关系的短语, 也保证了名词和动词之间是具有相关关系的.因此, 最终的多模态概念是视听相关的语义概念.</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63"><b>2.2 视听相关多模态概念视频数据的收集</b></h4>
                <div class="p1">
                    <p id="64">具有视听信息的多模态概念, 目前没有专门的数据集, 本文尝试直接使用2.1节定义的多模态概念到视频网站爬取了一部分视频, 但视频时间长、噪声大、标注过程耗时耗力.由于本文的多模态概念是从声音出发定义的, AudioSet<citation id="259" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>中也包含了对音频事件的标注, 而且提供了对应的视频数据, 因此本文使用AudioSet作为多模态概念的数据来源.将多模态概念与AudioSet中的标签进行比对, 如果概念的名词和动词都在某一个视频的标签里, 则将这个视频作为多模态概念的样例视频.通过这种方式, 共收集了93个概念对应的视频, 作为多模态视频概念检测的数据.视频总数量为88 957, 每个视频为10 s, 总时长约为247 h.93个多模态概念及对应的视频数量分布见附录A.</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag"><b>3 端到端的多模态概念检测</b></h3>
                <div class="p1">
                    <p id="66">针对2.2节中93个多模态概念, 本文使用CNN进行端到端的语义概念检测.在语义概念集给定的情况下, 视频概念检测转化为视频的分类问题.因此, 本文的概念检测系统输入是一个视频和预定义的语义概念集, 输出是与这个视频相关的多模态概念.本文将分别探索单模态的概念检测框架和多模态联合学习的概念检测框架.</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>3.1 单模态网络结构</b></h4>
                <div class="p1">
                    <p id="68">本文基于视觉信息的概念检测系统, 网络的输入是224×224的彩色图像.使用VGG16<citation id="260" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>作为基本网络结构, 修改最后一个预测层神经元个数为93, 并使用softmax输出预测概率.</p>
                </div>
                <div class="p1">
                    <p id="69">基于听觉信息的概念检测系统同样使用VGG16作为基本结构, 听觉网络的输入是大小为96×64的音频频谱图.修改全连接层神经元个数为1 024, 最后一个预测层使用softmax输出93个多模态概念的预测概率.</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>3.2 视听相关的多模态联合网络学习</b></h4>
                <div class="p1">
                    <p id="71">本文的视频概念检测的目标是多模态概念.多模态概念中体现了具有相关关系的视听信息.针对这类概念的检测, 本文认为可以利用视频中视听信息的相关性.在大多数情况下, 视频中的视觉信息和声音信息是同时出现的.因此, 本文提出利用视频的视听相关性作为监督信息, 同时使用视频的视觉信息和听觉信息进行端到端的视频概念检测.</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">3.2.1 联合学习网络结构</h4>
                <div class="p1">
                    <p id="73">本文借鉴L3-net的网络结构<citation id="261" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 具体来说, 网络的输入是语义相关的1帧图像和1个音频片段.语义相关指的是, 图像中反映的语义概念与音频片段中的语义内容是有关联的.网络共包括3个子网结构:视觉子网、声音子网和联合学习子网.其中视觉子网使用VGG16的前5个块 (block) .听觉子网结构与视觉子网结构类似, 使用VGG16的前5个块.联合学习子网由2个全连接层组成, 它的输入是视觉子网和听觉子网各自最后1个池化层的输出进行全局最大下采样后, 再拼接得到的1024维的向量.在融合网络中训练输出视频相关概念的预测概率.详细的网络结构如图2所示.本文提出的基于视听相关性的联合学习框架用于多模态概念检测, 其中视觉信息和听觉信息在本文中使用了VGG16作为网络学习的主要结构, 但其他典型的视觉、听觉网络结构也可以应用于该框架.</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74">3.2.2 视听相关采样</h4>
                <div class="p1">
                    <p id="75">联合学习的网络, 同时接收语义相关的1帧图像和1个音频片段为输入, 以视听信息的相关性为监督信息来学习视频.在数据预处理阶段, 需要采样语义相关的帧和音频片段.本文假定某一个概念的样例视频中截取的帧和音频片段和这个概念是相关的, 且来自一个视频的帧和对应的音频也是相关的.因此, 从每个语义概念对应的视频中随机选择1个视频帧, 选择这一帧所在的视频片段所对应的音频, 由此组成的 (帧-音频片段) 二元组作为与这个语义概念相关的视听信息正例.</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于视听相关性的联合网络结构图" src="Detail/GetImg?filename=images/JFYZ201905017_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于视听相关性的联合网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The proposed multimodal joint network  architecture based on audio-visual correlation</p>

                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>4 多模态概念检测实验分析</b></h3>
                <h4 class="anchor-tag" id="78" name="78"><b>4.1 实验数据</b></h4>
                <div class="p1">
                    <p id="79">本文实验数据使用2.2节中得到的93个多模态概念及对应的视频.实验数据分布如表1所示.</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>4.2 实验设置</b></h4>
                <div class="p1">
                    <p id="81">本文中网络结构的实现使用keras框架<citation id="165" type="note"><link href="155" rel="footnote" /><sup>①</sup></citation>, 后端使用Tensorflow<citation id="262" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.所有网络结构的训练, 使用Adam优化器<citation id="263" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 学习率为10<sup>-4</sup>, 权值衰减为10<sup>-5</sup>.</p>
                </div>
                <div class="p1">
                    <p id="82">对于概念检测的性能评估, 本文使用<i>mAP</i> (mean average precision) 和<i>AUC</i> (area under curve) .这2个指标<i>mAP</i>和<i>AUC</i>的计算见文献<citation id="264" type="reference">[<a class="sup">17</a>]</citation>.</p>
                </div>
                <div class="area_img" id="83">
                    <p class="img_tit"><b>表1 多模态概念检测实验数据分布</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Data Distribution of Our Multimodal Concepts Dataset</b></p>
                    <p class="img_note"></p>
                    <table id="83" border="1"><tr><td><br />Dataset</td><td>Number of<br />Videos</td><td>Total<br />Duration/h</td><td>Average<br />Number of<br />Videos in<br />Each Concept</td></tr><tr><td><br />Training</td><td>81 763</td><td>227</td><td>851</td></tr><tr><td><br />Validation</td><td>3 596</td><td>11</td><td>44</td></tr><tr><td><br />Test</td><td>3 598</td><td>11</td><td>45</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>4.3 单模态概念检测结果及分析</b></h4>
                <h4 class="anchor-tag" id="85" name="85">4.3.1 实验说明</h4>
                <div class="p1">
                    <p id="86">视觉网络实验中, 在预处理阶段, 每一个视频抽取了10帧图像, 每帧图像处理成224×224的固定大小.每帧图像继承其对应视频的标签, 输入到网络中参与训练;测试阶段, 将视频每一帧图像的预测概率进行平均作为视频的概念预测概率.</p>
                </div>
                <div class="p1">
                    <p id="87">听觉网络实验中, 预处理阶段提取视频的音频流, 每个音频流被切分成980 ms的音频片段, 每个音频片段处理成频谱图的形式, 大小为96×64 (音频的预处理同文献<citation id="265" type="reference">[<a class="sup">7</a>]</citation>) .训练阶段, 每个音频片段继承视频的标签, 输入到网络中进行训练.测试阶段, 每个音频片段的概念预测概率进行平均作为整个视频的预测概率.</p>
                </div>
                <div class="p1">
                    <p id="88">实验对比的基准方法是基于模块化流程的概念检测方法.分别对视频帧和音频提取视觉和听觉特征, 使用SVM分类器进行视频的概念检测.</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">4.3.2 实验结果分析</h4>
                <div class="p1">
                    <p id="90">基于单模态神经网络的概念检测实验结果如表2所示.</p>
                </div>
                <div class="p1">
                    <p id="91">由表2中可以看出:1) 基于CNN进行端到端视频概念检测, 其<i>mAP</i>值相较于使用特征加SVM分类器的流程式方法性能更优, 在听觉网络上<i>mAP</i>提高了9%, 在视觉网络上, 基于ImageNet预训练权重初始化的端到端网络比流程式的方法提高了12.5%.流程式方法将特征表示和分类训练分离开, 而CNN端到端网络直接对池化层特征继续做分类, 较好地保持了原始信息, 实验结果验证了CNN网络结构在学习特征上的有效性.2) 视觉网络中, 使用ImageNet预训练权重效果更好 (<i>mAP</i>比从零训练的视觉网络高50%) .这说明了预训练模型的先验知识的重要性.而基于本实验视频帧的网络, 因为实验数据集和多模态概念是弱相关, 因此训练数据的帧图像和最终检测的概念可能存在并不相关的情况.例如多模态概念集中存在“thunder_growl” (电闪雷鸣) 这类听觉特征更明显的概念, 其视觉特征并不能较好地发挥作用.此外, 训练数据较少也可能是潜在原因.3) 听觉网络的检测性能明显优于视觉网络的检测性能.这一方面是由于10 s的视频中听觉信息比视觉信息更丰富, 因此, 更能捕捉到有区分性的信息;另一方面, 可能由于实验数据集的来源本身是一个以声音为标注的数据集, 因此, 选择的多模态概念本身听觉信息比视觉信息更明显, 即听觉信息在这类概念中发挥的作用更明显.</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表2 基于单模态网络的视频概念检测结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Concept Detection Results Based on Single-modal Network</b></p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td><br />Modality</td><td>Method</td><td><i>mAP</i></td><td><i>AUC</i></td></tr><tr><td rowspan="2"><br />Audio</td><td><br />vggish_fc_svm</td><td>33.68</td><td>90.74</td></tr><tr><td><br />Audio-net_scratch</td><td><b>36.01</b></td><td><b>91.06</b></td></tr><tr><td rowspan="3"><br />Vision</td><td><br />vggpool_svm</td><td>23.73</td><td>83.35</td></tr><tr><td><br />Visual-net_finetune</td><td><b>27.63</b></td><td><b>88.08</b></td></tr><tr><td><br />Visual-net_scratch</td><td>18.42</td><td>83.76</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">vggish_fc_svm: SVM classifier with 128 dimensional features from the fully-connected layer of VGG trained on Youtube8M (audio only) . Audio-net_scratch: End-to-end concept detection network with our dataset (audio only) from scratch. vggpool_svm: SVM classifier with 512 dimensional features from pool5 of VGG trained on ImageNet. visual-net_finetune: Fine-tuning model on our dataset from an ImageNet pre-trained VGG model. visual-net_scratch: End-to-end concept detection network with our dataset (visual frames only) from scratch.</p>
                </div>
                <div class="p1">
                    <p id="93">将视觉网络和听觉网络检测出的概念按照<i>AP</i> (average precision) 进行排序.表3展示了<i>AP</i>值最高的前5个概念.基于预训练权重的视觉模型, 对“train_clatter” (火车行驶中) 的检测结果效果最好, 达到85.73%.分析发现, 视觉网络结果最好的这几个概念, 本身就具有更强的视觉信息.这5个概念都包含了具体的物体 (train, computer, boats, blender, ball) , 而在视频中也都有这些物体的具体存在.因此, 基于在ImageNet训练得到的视觉网络自然能够捕捉到这些信息.而在本实验数据集上训练的视觉网络 (视觉网络_scratch) 中, <i>AP</i>值最高的前5个概念, 也有相同的发现:概念本身包含的视觉信息更明显.</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表3 <i>AP</i>最高的前5个概念 (视觉网络</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Top 5 Concepts with the Highest <i>AP</i> (Based on Visual Network</b>) </p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td colspan="2"><br />Visual-net_finetune</td><td colspan="2">Visual-net_scratch</td></tr><tr><td><br />Multimodal<br />Concept</td><td><i>AP</i></td><td>Multimodal<br />Concept</td><td><i>AP</i></td></tr><tr><td><br />train_clatter</td><td>85.73</td><td>train_clatter</td><td>77.11</td></tr><tr><td><br />computer_print</td><td>75.92</td><td>ball_bounce</td><td>58.39</td></tr><tr><td><br />boat_dock</td><td>73.50</td><td>plane_fly</td><td>57.63</td></tr><tr><td><br />blender_chop</td><td>71.18</td><td>crowd_cheer</td><td>53.73</td></tr><tr><td><br />ball_bounce</td><td>69.13</td><td>baby_laugh</td><td>53.13</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="95">而听觉网络<i>AP</i>值最高的前5个概念, 如表4所示, 和视觉网络检测出的概念都不相同, 这说明了这些概念之间的听觉区分信息更强一些.但是“crowd_cheer” (观众欢呼) 在视觉和听觉网络上的<i>AP</i>值都很高.事实上, 将听觉网络和视觉网络检测的概念<i>AP</i>值进行排序, 前50个概念中有32个概念是重合的.这也说明了概念集本身具有了多模态的特性.</p>
                </div>
                <div class="area_img" id="96">
                    <p class="img_tit"><b>表4 <i>AP</i>最高的前5个概念 (听觉网络</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Top 5 Concepts with the Highest <i>AP</i> (Based on Audio Network</b>) </p>
                    <p class="img_note"></p>
                    <table id="96" border="1"><tr><td><br />Multimodal Concept</td><td><i>AP</i></td></tr><tr><td><br />ambulance_roar</td><td>92.98</td></tr><tr><td><br />crowd_cheer</td><td>85.63</td></tr><tr><td><br />telephone_ring</td><td>75.93</td></tr><tr><td><br />cat_purr</td><td>74.70</td></tr><tr><td><br />car_skid</td><td>71.41</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>4.4 多模态概念检测实验结果及分析</b></h4>
                <h4 class="anchor-tag" id="98" name="98">4.4.1 实验说明</h4>
                <div class="p1">
                    <p id="99">视频预处理时, 每个视频抽取10帧图像.视听相关性采样中, 随机选择4帧图像, 并选择每一帧所在视频片段的音频提取频谱图, 将每一个帧-频谱图二元组, 继承视频标签, 输入网络进行训练.实际训练中, 本文采取2种方式更新整个网络权重:是否使用预训练权重初始化视听子网;使用视听相关性策略更新整个网络的所有权重或只更新融合子网 (即全连接层) 权重.预测时, 每个视频的4个帧-频谱图二元组的平均概念预测概率作为这个视频的预测概率.</p>
                </div>
                <div class="p1">
                    <p id="100">与多模态实验对比的基准方法是基于前期融合的流程式概念检测方法.前期融合选择的视觉特征是<i>vggpool</i>, 听觉特征是<i>vggish</i>_<i>fc</i>, 将特征进行拼接, 输入到SVM中进行概念检测的判定.</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">4.4.2 实验结果分析</h4>
                <div class="p1">
                    <p id="102">基于视听相关性学习的多模态联合网络的实验结果如表5所示:</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表5 基于视听相关的多模态网络概念检测结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Concepts Detection Results Based on Audio</b> -<b>Visual Coorrelated Joint Network</b></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br />Method</td><td><i>mAP</i></td><td><i>AUC</i></td></tr><tr><td><br />Joint-net1-a-imagenet-all</td><td><b>42.44</b></td><td><b>94.97</b></td></tr><tr><td><br />Joint-net1-a-imagenet-fc</td><td>34.60</td><td>92.97</td></tr><tr><td><br />Joint-net2-a-v-all</td><td>40.76</td><td>93.89</td></tr><tr><td><br />Joint-net2-a-v-fc</td><td>37.77</td><td>93.04</td></tr><tr><td><br />Joint-net3-scratch</td><td>34.10</td><td>93.72</td></tr><tr><td><br />vggpool-vggish_fc_svm</td><td>41.83</td><td>92.04</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Joint-net1-a-imagenet-all: Initializing the audio subset with audio-net_scratch in table2 and the vision subset with ImageNet pre-trained weights respectively, then the network learns and updates all weights. Joint-net1-a-imagenet-fc: Initializing the audio subset with audio-net_scratch and the vision subset with ImageNet pre-trained weights respectively, then the network learns and updates weights of fully-connected layer. Joint-net2-a-v-all:Initializing the audio subset with audio-net_scratch and the vision subset with visual-net_scratch respectively. then the network learns and updates all weights. Joint-net2-a-v-fc: Initializing the audio subset with audio-net_scratch and the vision subset with visual-net_scratch respectively, then the network learns and updates weights of fully-connected layer. Joint-net3-scratch: The network is trained using audio-visual correlation strategy from scratch with random initialization. vggpool-vggish_fc_svm: SVM classifier with Early fusion with <i>vggpool</i> and <i>vggish</i>_<i>fc</i> in table 2.</p>
                </div>
                <div class="p1">
                    <p id="104">由表5可以看出:</p>
                </div>
                <div class="p1">
                    <p id="105">1) 融合网络1-a-imagenet-all使用预训练权重且基于视听相关性更新融合网络所有权重, 其<i>mAP</i>值有42.44%, 是本文实验中的最高值.这一方面得益于ImageNet上的视觉先验知识, 以及听觉网络通过CNN学到了有效的特征表示, 经过结合能够提供互补信息, 达到增强系统性能的效果;另一方面也体现了视听相关性学习的有效性.</p>
                </div>
                <div class="p1">
                    <p id="106">2) 经过视听相关性更新所有权重的模型表现都比仅更新全连接层权重的表现好.在单独的视觉网络概念检测的<i>mAP</i>只有18.42%, 但使用视听相关性训练的融合网络2-a-v-all, 提升了1倍多的<i>mAP</i>值 (40.76%) .由于检测的是多模态概念, 概念本身视听信息是相关的, 而视频的视听内容也存在天然的相关性.因此, 在基于视听相关性更新所有权重的过程中, 有效地将相关性信息传递到了网络中, 证明了视听相关反馈视觉和听觉子网的重要性.</p>
                </div>
                <div class="p1">
                    <p id="107">3) 融合网络3-scratch使用随机权重从零训练的多模态网络, 其<i>mAP</i>值是34.10%, 比单独的听觉网络低了约2个百分点, 本文推断和网络训练的数据有关系.视频中出现的声音不一定是连续的10 s, 可能有其他声音的干扰, 而听觉网络使用每个视频的所有音频片段训练, 适当减少了这样噪音的影响, 而视听融合网络随机选择4个二元组 (音频数据的40%) , 这就造成了视听信息可能并不相关, 增加了样本的不准确性.但是视听融合网络的<i>AUC</i>却比听觉网络的<i>AUC</i>高了近2个百分点, 也证明了视听相关性在多模态概念检测中的有效性.</p>
                </div>
                <div class="p1">
                    <p id="108">同样地, 本文分析融合网络模型对多模态语义概念的影响.由于本文更关注视听相关性策略在多模态概念中的表现, 因此, 本文主要分析融合网络3-scratch对多模态语义概念检测的影响.表6中呈现了融合网络3-scratch所检测到的<i>AP</i>值最高的前5个概念.“crowd_cheer” (观众欢呼) 的<i>AP</i>值最高, 说明这个概念的视听信息相关度最高.这个概念在单模态网络中检测的<i>AP</i>值都居于前5, 但基于视听相关性的网络仍然提高了其<i>AP</i>值.通过表6也发现了一些有趣的现象:如“rooster_crow” (公鸡报晓) 这个概念的<i>AP</i>值比单纯基于前期融合或单模态网络要高.这个概念是通过象声词及其发声物匹配的方式收集的.象声词本身包含了发声物的信息, 因此通过这种方式收集的多模态概念的视听信息是非常相关的.而这样的强相关性也正是视听融合网络用来学习的目标, 因此检测结果较好.此外, 在诸如“wind_blow” (刮大风) 和“thunder_growl” (电闪雷鸣) 这类语义概念中, 视觉信息比较抽象, 大量的视频中充满了人、雨伞, 大多视频场景是室外, 通过视觉信息不能很好地区分, 但是加入视听信息相关性后就能够有效地与声音信息结合, 从而增大其被检测出来的可能性.</p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表6 <i>AP</i>最高的前5个概念 (基于融合网络3-scratch</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 Top 5 Concepts with the Highest <i>AP</i> (Based on Joint net3-scratch</b>) </p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td><br />Multimodal Concept</td><td><i>AP</i></td></tr><tr><td><br />crowd_cheer</td><td>86.06</td></tr><tr><td><br />rooster_crow</td><td>78.33</td></tr><tr><td><br />thunder_growl</td><td>77.60</td></tr><tr><td><br />wind_blow</td><td>75.68</td></tr><tr><td><br />ball_bounce</td><td>71.11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>4.5 多模态网络学习的精细化特征</b></h4>
                <div class="p1">
                    <p id="111">在视听融合网络中还发现了网络能够学习到一些精细化的特征, 能够区分一些视觉听觉上都很相似的语义概念.图3中给出了测试集上对应概念的预测率最高的5个视频中所截取的帧图像.从“dog_bark” (狗的“汪汪”叫) 和“dog_howl” (狗嚎叫) 图3中可以清楚地看到, “dog_howl”这个概念对应的视频当中的狗, 都是头部呈90度上扬, 狗的嘴巴张开的幅度较小, 动作状态多为静止;而“dog_bark”这个概念对应的视频, 狗几乎都是面向前方, 嘴巴张开的幅度较大, 动作状态多为跑动.实际上, 这就是狗在不同状态下的表现.这2类概念视觉信息都是狗, 而听觉信息都包含了狗叫的声音, 视听信息都很相似的情况下, 基于视听相关性的网络能够很好地将他们区分开来.</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 融合网络学习到的精细化特征示例" src="Detail/GetImg?filename=images/JFYZ201905017_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 融合网络学习到的精细化特征示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Examples of learned fine-grained features  from joint network</p>

                </div>
                <div class="p1">
                    <p id="113">此外, 在听觉信息上也体现了这样的精细化差异.例如“sheep_bleat”和“goat_bleat”, 听觉上都反应的是羊“咩咩”叫的声音, 但视觉上前者更多的是白色的绵羊, 而后者更多的是黑色的山羊.值得一提的是, 这些特征在基于单模态的网络结构或基于多模态特征融合的概念检测方法中都没能体现出来, 这说明了视频视听信息相关性在视频分析中的重要性, 也从侧面反映了基于视听信息的多模态概念的必要性.只有通过精细化的语义概念, 才能更精确地构建起视频底层特征与高层精细化语义之间的桥梁.</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>4.6 多模态网络的视听特征有效性分析</b></h4>
                <div class="p1">
                    <p id="115">本文的目标是通过构建具有视听信息的多模态概念, 获得视频底层特征与高层语义之间更精细化的联系, 因此, 为了验证本文的多模态视听网络是否能够有效表示视频特征, 本文将从文中基于视听相关性的联合网络模型中提取视听特征, 在标准数据集上验证特征的有效性.</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">4.6.1 基于联合网络视觉子网的特征</h4>
                <div class="p1">
                    <p id="117">验证视觉特征有效性的数据集是Huawei视频概念检测数据集<citation id="166" type="note"><link href="161" rel="footnote" /><sup>①</sup></citation>.这个数据集包含了2 666个视频和10个语义概念, 每个视频标注了其中相关的概念和它出现的片段.10个语义概念分别是:“kids”, “flower”, “city_view”, “car”, “beach”, “party”, “Chinese_antique _building”, “dog”, “food”, “football_game”.本文首先将每个视频按照标注信息切割成视频片段, 每个片段有1个或多个概念标签, 每个视频片段作为1个样本 (每个视频片段不超过2 min) .预处理后, 共有5 828个样本, 其中4 187个样本作为训练集, 609个样本作为验证集, 1 153个样本作为测试集.对每个样本, 提取关键帧, 输入到网络结构中得到图像特征, 将每个关键帧的图像特征进行平均作为这个视频整体的视觉特征, 使用SVM进行监督训练.评测指标为<i>mAP</i>.</p>
                </div>
                <div class="area_img" id="118">
                    <p class="img_tit"><b>表7 Huawei视频分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 7 Classification Results on the Huawei Dataset</b></p>
                    <p class="img_note"></p>
                    <table id="118" border="1"><tr><td rowspan="2"><br />Feature</td><td rowspan="2"><i>mAP</i></td><td colspan="3"><br /><i>AP</i></td></tr><tr><td><br />football_game</td><td>city_view</td><td>party</td></tr><tr><td><br /><i>vggpool</i></td><td><b>79.87</b></td><td>72.44</td><td>46.05</td><td>96.78</td></tr><tr><td><br /><i>f</i>_<i>v</i>_<i>pool</i></td><td>77.97</td><td><b>84.39</b></td><td><b>57.95</b></td><td>97.23</td></tr><tr><td><br /><i>scr</i>_<i>v</i>_<i>pool</i></td><td>75.30</td><td>82.84</td><td>50.84</td><td><b>97.30</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"><i>vggpool</i>:512 dimensional features from pool5 of VGG trained on ImageNet with global max pooling. <i>f</i>_<i>v</i>_<i>pool</i>: 512 dimensional features from the output of pool5 of vision subset in joint-net1-a-imagenet-all in table 5 with global max pooling. <i>scr</i>_<i>v</i>_<i>pool</i>: 512 dimensional features from the output of pool5 of vision subset in joint-net3-scratch in table 5 with global max pooling.</p>
                </div>
                <div class="p1">
                    <p id="119">在本文多模态融合网络的实验中, 融合网络在学习过程中, 视觉子网使用了ImageNet预训练权重, 再利用视听相关性更新网络所有权重.因此实验将对比基于视听相关性更新网络前后的特征.表7中给出了在整个测试集上的<i>mAP</i>和在特定概念上的<i>AP</i>值.</p>
                </div>
                <div class="p1">
                    <p id="120">整体来说, 在这个数据集上, <i>vggpool</i>的效果略优于本文的视听融合网络的特征.因为Huawei数据集上某些概念在本文所使用的视频集中没有出现, 但是在ImageNet中出现过, 比如 “flower”, “food”.在这些类别上由于<i>vggpool</i>的模型中包含了先验知识, 因此效果比较好.但是, 对于在ImageNet预训练模型和本文融合网络模型训练中都出现的相关概念, 如“football_game”, “party”, “city_view”, 本文融合网络的视觉特征是优于<i>vggpool</i>的.这是因为这3个概念本身就是具有视听信息的, 这些概念相关的视频中通常都伴随着人们欢呼、说话等听觉信息, 而本文的多模态概念中也包括了如“crowd_cheer”, “crowd_clap”这样的概念, 联合网络通过对这些概念和相关视频的学习, 使得网络具有了相应的特征.<i>scr</i>_<i>v</i>_<i>pool</i>来自于文中的未使用任何先验知识的融合网络3-scratch, 其性能也表现出非常有竞争力的优势.这也证明了本文基于视听相关性的多模态网络的有效性.</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121">4.6.2 基于联合网络听觉子网的特征</h4>
                <div class="p1">
                    <p id="122">本文选择在ESC50验证听觉特征的有效性.ESC50是一个包含了2 000个音频的声音分类的标准数据集<citation id="266" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.这个数据集中的每个音频有5 s, 被划分到50个类别 (大致包括动物声音、自然声音、人类声音、室内声音和室外声音) .数据集被预先划分成5个子集, 每个子集的音频片段来源相同.评测指标是在5个子集上的平均准确率.本文实验中, 先将5 s的每个音频划分成980 ms的音频片段, 每个音频片段提取频谱图 (预处理过程同4.3) , 将该频谱图输入到本文的视听融合网络的听觉子网中提取特征.所有音频片段的特征进行平均作为整个音频的特征, 输入到SVM进行分类验证.</p>
                </div>
                <div class="p1">
                    <p id="123">实验结果如表8所示.基于视听相关性的融合网络所提取的特征 (<i>scr</i>_<i>a</i>_<i>pool</i>) , 在ESC50音频事件数据集的分类效果超过了<i>vggish</i>_<i>fc</i>约5.7%, 而后者训练数据的数量远远大于本文实验中所使用的数据量.这也再次证实了本文方法的有效性.</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表8 ESC50音频分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 8 Classification Results on the ESC50 Dataset</b></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td><br />Feature</td><td>Accuracy</td></tr><tr><td><br /><i>vggish</i>_<i>fc</i></td><td>71.25</td></tr><tr><td><br /><i>scr</i>_<i>a</i>_<i>pool</i></td><td><b>75.30</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"><i>vggish</i>_<i>fc</i>: The 128 dimensional features from the fully-connected layer of VGG trained on Youtube8M (audio only) . <i>scr</i>_<i>a</i>_<i>pool</i>: The 512 dimensional features from the output of pool5 of audio subset in joint-net3-scratch in table 5 with global max pooling.</p>
                </div>
                <h3 id="125" name="125" class="anchor-tag"><b>5 总 结</b></h3>
                <div class="p1">
                    <p id="126">本文探索了具有视听信息的多模态语义概念检测工作.多模态概念是指概念中包含了视听信息的概念, 其中视听信息具有相关性, 共同表达了语义概念所描述的事件.多模态概念的构建更能满足用户的精细化检索需求, 也更能准确确定视频中的语义信息.本文以多模态概念为检测目标, 探索了基于CNN的端到端的概念检测框架.除了使用CNN训练单模态的概念检测系统, 本文利用多模态概念的视听相关性为目标训练了联合学习网络.实验表明:通过视听相关性的联合网络相比目前研究领域通用的视听觉特征, 在多模态概念检测任务上有更好的表现.同时, 联合网络能够学习到精细化的特征表示, 利用该网络提取的视听觉特征也能够有效运用于其他多媒体分析任务.本文的研究工作将有效构建起视频底层特征与高层精细化语义之间的桥梁.</p>
                </div>
                <div class="p1">
                    <p id="127">在今后的工作中, 本文将继续探索更大规模的多模态概念的定义以及更准确的相关视频数据.本文提出的基于视听相关性的联合学习框架用于多模态概念检测, 其中视觉信息和听觉信息在本文工作中只是采用简单的VGG模型作为网络学习的主要结构, 在今后的工作中也将探索利用其他更为复杂的视觉、听觉网络学习模型.此外, 本文的多模态联合学习, 仅仅利用了视频的视听相关性, 并未完全研究视觉信息和听觉信息之间究竟存在怎样的关系以及如何利用这个关系, 这也是今后工作中一个值得深入的研究方向.</p>
                </div>
                <div class="p1">
                    <p id="167">附录A</p>
                </div>
                <div class="area_img" id="169">
                                            <p class="img_tit">
                                                表A1 26个动物相关的多模态概念及对应视频数量
                                                    <br />
                                                Table A1 The 26Animal Related Multimodal Concepts and Number of Videos
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JFYZ201905017_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表A1 26个动物相关的多模态概念及对应视频数量" src="Detail/GetImg?filename=images/JFYZ201905017_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="170">
                                            <p class="img_tit">
                                                表A2 40个日常事物相关的多模态概念及对应视频数量
                                                    <br />
                                                Table A2 The 40Daily Items Related Multimodal Concepts and Number of Videos
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_17000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JFYZ201905017_17000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_17000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表A2 40个日常事物相关的多模态概念及对应视频数量" src="Detail/GetImg?filename=images/JFYZ201905017_17000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="171">
                                            <p class="img_tit">
                                                表A3 10个自然相关的多模态概念及对应视频数量
                                                    <br />
                                                Table A3 The 10Nature Related Multimodal Concepts and Number of Videos
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JFYZ201905017_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表A3 10个自然相关的多模态概念及对应视频数量" src="Detail/GetImg?filename=images/JFYZ201905017_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="172">
                                            <p class="img_tit">
                                                表A4 17个与人相关的多模态概念及对应视频数量
                                                    <br />
                                                Table A4 The 17Human Related Multimodal Concepts and Number of Videos
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905017_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JFYZ201905017_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905017_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表A4 17个与人相关的多模态概念及对应视频数量" src="Detail/GetImg?filename=images/JFYZ201905017_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="209">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale visual sentiment ontology and detectors using adjective noun pairs">

                                <b>[1]</b>Borth D, Ji Rongrong, Chen Tao, et al.Large-scale visual sentiment ontology and detectors using adjective noun pairs[C] //Proc of the 21st ACM Int Conf on Multimedia.New York:ACM, 2013:223- 232
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eventnet:A large scale structured concept library for complex event detection in video">

                                <b>[2]</b>Ye Guangnan, Li Yitong, Xu Hongliang, et al.Eventnet:A large scale structured concept library for complex event detection in video[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:471- 480
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Activity Net:A large-scale video benchmark for human activity understanding">

                                <b>[3]</b>Heilbronn F C, Escorial V, Ghanem B, et al.ActivityNet:A large-scale video benchmark for human activity understanding[C] //Proc of 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:961- 970
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Audio Set:An ontology and human-labeled dataset for audio events">

                                <b>[4]</b>Gemmeke J F, Ellis D P W, Freedman D, et al.Audio set:An ontology and human-labeled dataset for audio events[C] //Proc of 2017 IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:776- 780
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200901002&amp;v=MzEzMTFxRnlubFVMM0tMeXZTZExHNEh0ak1ybzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Liu Yanan, Wu Fei, Zhuang Yueting.Video semantics mining using multi-modality subspace correlation propagation[J].Journal of Computer Research and Development, 2009, 46 (1) :1- 8 (in Chinese) (刘亚楠, 吴飞, 庄越挺.基于多模态子空间相关性传递的视频语义挖掘[J].计算机研究与发展, 2009, 46 (1) :1- 8) 
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepsentibank:Visual sentiment concept classification with deep convolutional neural networks">

                                <b>[6]</b>Chen Tao, Borth D, Darrell T, et al.Deepsentibank:Visual sentiment concept classification with deep convolutional neural networks[J].arXiv preprint, arXiv, 1410.8586, 2014
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CNN architectures for large-scale audio classification">

                                <b>[7]</b>Hershey S, Chaudhuri S, Ellis D P W, et al.CNN architectures for large-scale audio classification[C] //Proc of 2017 IEEE Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2017:131- 135
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring multimodal video representation for action recognition">

                                <b>[8]</b>Wang Cheng, Yang Haojin, Meinel C.Exploring multimodal video representation for action recognition[C] //Proc of 2016 IEEE Int Joint Conf on Neural Networks.Piscataway, NJ:IEEE, 2016:1924- 1931
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SoundNet:Learning sound representations from unlabeled video">

                                <b>[9]</b>Aytar Y, Vondrick C, Torralba A, et al.SoundNet:Learning sound representations from unlabeled video[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc, 2016:892- 900
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Content-based image retrieval at the end of the early years">

                                <b>[10]</b>Smeulders A W M, Worring M, Santini S, et al.Content-based image retrieval at the end of the early years[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2000, 22 (12) :1349- 1380
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WordNet:an electronic lexical database">

                                <b>[11]</b>Fellbaum C, Miller G.WordNet :An electronic lexical database[J].Library Quarterly Information Community Policy, 1998, 25 (2) :292- 296
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C] //Proc of the 2012 Int Conf on Neural Information Processing Systems.North Miami Beach, FL:Curran Associates Inc, 2012:1097- 1105
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[13]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint, arXiv:1409.1556, 2014
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Look,listen and learn">

                                <b>[14]</b>Arandjelovic R, Zisserman A.Look, listen and learn[C] //Proc of 2017 IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:609- 617
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:a system for large-scale machine learning">

                                <b>[15]</b>Abadi M, Agarwal A, Barham P, et al.Tensorflow:Large-scale machine learning on heterogeneous distributed systems[C] //Proc of the 12th USENIX Conf on Operating Systems Design and Implementation.Berkeley, CA:USENIX Association, 2016:265- 283
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization">

                                <b>[16]</b>Kingma D P, Ba J.Adam:A method for stochastic optimization[J].arXiv preprint, arXiv:1412.6980, 2014
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction to Information Retrieval">

                                <b>[17]</b>Schütze H, Manning C D, Raghavan P.Introduction to Information Retrieval[M].Cambridge, UK:Cambridge University Press, 2008
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ESC:dataset for environmental sound classification">

                                <b>[18]</b>Piczak K J.ESC:Dataset for environmental sound classification[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:1015- 1018
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="155" href="javascript:void(0)">
                            <b>1</b> https://github.com/fchollet/keras
                        </span>
                    </p>
                    <p>
                        <span id="157" href="javascript:void(0)">
                            <b>2</b> https://en.oxforddictionaries.com/
                        </span>
                    </p>
                    <p>
                        <span id="159" href="javascript:void(0)">
                            <b>3</b> www.freesoundeffects.com
                        </span>
                    </p>
                    <p>
                        <span id="161" href="javascript:void(0)">
                            <b>4</b> http://www.icme2014.org/huawei-accurate-and-fast-mobile-video-annotation-challenge
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201905017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905017&amp;v=MjUxNzdCdEdGckNVUkxPZVplUnFGeW5sVUwzS0x5dlNkTEc0SDlqTXFvOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dBZnJxRjR3UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

