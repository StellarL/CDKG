

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128655332462500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201905016%26RESULT%3d1%26SIGN%3da1YjC%252f8YDNCSkgS1PIyinwLtkdg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905016&amp;v=MTQ4NDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnlubFVML0tMeXZTZExHNEg5ak1xbzlFWW9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#65" data-title="&lt;b&gt;1 CNN简介&lt;/b&gt; "><b>1 CNN简介</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;2 基于噪声分离模型的NLE算法&lt;/b&gt; "><b>2 基于噪声分离模型的NLE算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="&lt;b&gt;2.1 NLE算法的基本思想&lt;/b&gt;"><b>2.1 NLE算法的基本思想</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;2.2 分离噪声信号&lt;/b&gt;"><b>2.2 分离噪声信号</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;2.3 噪声映射图与GGD建模&lt;/b&gt;"><b>2.3 噪声映射图与GGD建模</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;2.4 噪声水平感知特征选取&lt;/b&gt;"><b>2.4 噪声水平感知特征选取</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;2.5 改进的BP神经网络&lt;/b&gt;"><b>2.5 改进的BP神经网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#144" data-title="&lt;b&gt;3 实验与分析&lt;/b&gt; "><b>3 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#145" data-title="&lt;b&gt;3.1 测试环境&lt;/b&gt;"><b>3.1 测试环境</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;3.2 预测准确性&lt;/b&gt;"><b>3.2 预测准确性</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;3.3 BM3D算法降噪&lt;/b&gt;"><b>3.3 BM3D算法降噪</b></a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;3.4 执行时间的对比&lt;/b&gt;"><b>3.4 执行时间的对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#167" data-title="&lt;b&gt;4 总 结&lt;/b&gt; "><b>4 总 结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="图1 改进算法流程框图">图1 改进算法流程框图</a></li>
                                                <li><a href="#94" data-title="图2 CNN噪声分离模型的网络体系架构图">图2 CNN噪声分离模型的网络体系架构图</a></li>
                                                <li><a href="#97" data-title="图3 系数值分布直方图">图3 系数值分布直方图</a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表1 GGD参数值与噪声水平值的相关系数&lt;/b&gt;"><b>表1 GGD参数值与噪声水平值的相关系数</b></a></li>
                                                <li><a href="#114" data-title="图4 改进BP神经网络工作原理">图4 改进BP神经网络工作原理</a></li>
                                                <li><a href="#147" data-title="图5 各类文献中常用的图像集合">图5 各类文献中常用的图像集合</a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表2 不同噪声水平下各算法在Couple图像上的评估结果&lt;/b&gt;"><b>表2 不同噪声水平下各算法在Couple图像上的评估结果</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表3 各算法在10幅常用图像集合上预测结果的均方根误差&lt;/b&gt;"><b>表3 各算法在10幅常用图像集合上预测结果的均方根误差</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表4 不同噪声水平下不同算法在50幅BSD图像上预测结果的均方根误差&lt;/b&gt;"><b>表4 不同噪声水平下不同算法在50幅BSD图像上预测结果的均方根误差</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表5 本文算法在50幅BSD图像上预测结果的均方根误差&lt;/b&gt;"><b>表5 本文算法在50幅BSD图像上预测结果的均方根误差</b></a></li>
                                                <li><a href="#162" data-title="&lt;b&gt;表6 BM3D分别使用真实噪声值和预测噪声值的降噪结果&lt;/b&gt;"><b>表6 BM3D分别使用真实噪声值和预测噪声值的降噪结果</b></a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表7 各算法在不同噪声级别下的平均执行时间&lt;/b&gt;"><b>表7 各算法在不同噪声级别下的平均执行时间</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="256">


                                    <a id="bibliography_1" title="Xu Shaoping, Zeng Xiaoxia, Tang Yiling.Fast noise level estimation algorithm based on two-stage support vector regression[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2018, 30 (3) :447- 458 (in Chinese) (徐少平, 曾小霞, 唐祎玲.基于两阶段支持向量回归的快速噪声水平估计算法[J].计算机辅助设计与图形学学报, 2018, 30 (3) :447- 458) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201803010&amp;v=MDgyMDJDVVJMT2VaZVJxRnlubFVML0tMejdCYUxHNEg5bk1ySTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Xu Shaoping, Zeng Xiaoxia, Tang Yiling.Fast noise level estimation algorithm based on two-stage support vector regression[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2018, 30 (3) :447- 458 (in Chinese) (徐少平, 曾小霞, 唐祎玲.基于两阶段支持向量回归的快速噪声水平估计算法[J].计算机辅助设计与图形学学报, 2018, 30 (3) :447- 458) 
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_2" title="Wang Lingfeng, Huang Zehao, Gong Yongchao, et al.Ensemble based deep networks for image super-resolution[J].Pattern Recognition, 2017, 68 (8) :191- 198" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES75A3051BE0EF8F1A0F69D7FA08C36E74&amp;v=MTg5MTVXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHhiaTh3S289TmlmT2ZiUzliOUxNcW80M0VldDZlblJQem1jVG5EbDBQSGlVM1JJOUNyR1NNTDJiQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Wang Lingfeng, Huang Zehao, Gong Yongchao, et al.Ensemble based deep networks for image super-resolution[J].Pattern Recognition, 2017, 68 (8) :191- 198
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_3" title="Ren Jingjing, Fang Xianyong, Chen Shangwen, et al.Image deblurring based on fast convolutional neural networks[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2017, 29 (8) :1444- 1456 (in Chinese) (任静静, 方贤勇, 陈尚文, 等.基于快速卷积神经网络的图像去模糊[J].计算机辅助设计与图形学学报, 2017, 29 (8) :1444- 1456) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201708007&amp;v=MDM3NzZVTC9LTHo3QmFMRzRIOWJNcDQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZ5bmw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Ren Jingjing, Fang Xianyong, Chen Shangwen, et al.Image deblurring based on fast convolutional neural networks[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2017, 29 (8) :1444- 1456 (in Chinese) (任静静, 方贤勇, 陈尚文, 等.基于快速卷积神经网络的图像去模糊[J].计算机辅助设计与图形学学报, 2017, 29 (8) :1444- 1456) 
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_4" title="Dabov K, Foi A, Katkovnik V, et al.Image denoising by sparse 3D transform-domain collaborative filtering[J].IEEE Transactions on Image Processing, 2007, 16 (8) :2080- 2095" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering">
                                        <b>[4]</b>
                                        Dabov K, Foi A, Katkovnik V, et al.Image denoising by sparse 3D transform-domain collaborative filtering[J].IEEE Transactions on Image Processing, 2007, 16 (8) :2080- 2095
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_5" title="Huang Dongmei, Dai Liang, Wei Lifei, et al.A secure outsourced fusion denoising scheme in multiple encrypted remote sensing images[J].Journal of Computer Research and Development, 2017, 54 (10) :2378- 2389 (in Chinese) (黄冬梅, 戴亮, 魏立斐, 等.一种安全的多帧遥感图像的外包融合去噪方案[J].计算机研究与发展, 2017, 54 (10) :2378- 2389) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201710024&amp;v=MTM3MzBScUZ5bmxVTC9LTHl2U2RMRzRIOWJOcjQ5SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Huang Dongmei, Dai Liang, Wei Lifei, et al.A secure outsourced fusion denoising scheme in multiple encrypted remote sensing images[J].Journal of Computer Research and Development, 2017, 54 (10) :2378- 2389 (in Chinese) (黄冬梅, 戴亮, 魏立斐, 等.一种安全的多帧遥感图像的外包融合去噪方案[J].计算机研究与发展, 2017, 54 (10) :2378- 2389) 
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_6" title="Dong Weisheng, Zhang Lei, Shi Guangming, et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing, 2013, 22 (4) :1620- 1630" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">
                                        <b>[6]</b>
                                        Dong Weisheng, Zhang Lei, Shi Guangming, et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing, 2013, 22 (4) :1620- 1630
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_7" title="Immerk&#230;r J.Fast noise variance estimation[J].Computer Vision and Image Understanding, 1996, 64 (2) :300- 302" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084580&amp;v=MjcyNjI0WGFoQT1OaWZPZmJLN0h0RE5xbzlFWk9NTENYUTVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMdklJVg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        Immerk&#230;r J.Fast noise variance estimation[J].Computer Vision and Image Understanding, 1996, 64 (2) :300- 302
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_8" title="Zoran D, Weiss Y.Scale invariance and noise in natural images[C] //Proc of the 12th IEEE Int Conf on Computer Vision (ICCV 2009) .Piscataway, NJ:IEEE, 2009:2209- 2216" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale invariance and noise in natural images">
                                        <b>[8]</b>
                                        Zoran D, Weiss Y.Scale invariance and noise in natural images[C] //Proc of the 12th IEEE Int Conf on Computer Vision (ICCV 2009) .Piscataway, NJ:IEEE, 2009:2209- 2216
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_9" title="Katase H, Yamaguchi T, Fujisawa T, et al.Image noise level estimation by searching for smooth patches with discrete cosine transform[C] //Proc of the 59th IEEE Int Midwest Symp on Circuits and Systems.Piscataway, NJ:IEEE, 2016.DOI:10.1109/MWSCAS.2016.7870110" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image noise level estimation by searching for smooth patches with discrete cosine transform">
                                        <b>[9]</b>
                                        Katase H, Yamaguchi T, Fujisawa T, et al.Image noise level estimation by searching for smooth patches with discrete cosine transform[C] //Proc of the 59th IEEE Int Midwest Symp on Circuits and Systems.Piscataway, NJ:IEEE, 2016.DOI:10.1109/MWSCAS.2016.7870110
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_10" title="Rakhshanfar M, Amer M A.Estimation of Gaussian, Poissonian-Gaussian, and processed visual noise and its level function[J].IEEE Transactions on Image Processing, 2016, 25 (9) :4172- 4185" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Estimation of Gaussian,Poissonian-Gaussian,and processed visual noise and its level function">
                                        <b>[10]</b>
                                        Rakhshanfar M, Amer M A.Estimation of Gaussian, Poissonian-Gaussian, and processed visual noise and its level function[J].IEEE Transactions on Image Processing, 2016, 25 (9) :4172- 4185
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_11" title="Chen Guangyong, Zhu Fengyuan, Heng P A.An efficient statistical method for image noise level estimation[C] //Proc of the 18th IEEE Int Conf on Computer Vision (ICCV 2015) .Piscataway, NJ:IEEE, 2015:477- 485" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An efficient statistical method for image noise level estimation">
                                        <b>[11]</b>
                                        Chen Guangyong, Zhu Fengyuan, Heng P A.An efficient statistical method for image noise level estimation[C] //Proc of the 18th IEEE Int Conf on Computer Vision (ICCV 2015) .Piscataway, NJ:IEEE, 2015:477- 485
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_12" title="Xu Shaoping, Hu Lingyan, Yang Xiaohui.Quality-aware features-based noise level estimator for block matching and three-dimensional filtering algorithm[J].Journal of Electronic Imaging, 2016, 25 (1) :013029" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEGB2013CE9FF9C89BD39899797291CC77F&amp;v=MjkwNzJOaWZPYWNHNkh0RFAzUHBNRXAwR2YzUXd2V0lRNHpkMFFYanJxeEE4ZU1IblFyM3BDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGh4Ymk4d0tvPQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Xu Shaoping, Hu Lingyan, Yang Xiaohui.Quality-aware features-based noise level estimator for block matching and three-dimensional filtering algorithm[J].Journal of Electronic Imaging, 2016, 25 (1) :013029
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_13" title="Xu Shaoping, Zeng Xiaoxia, Jiang Yinnan, et al.A multiple image-based noise level estimation algorithm[J].IEEE Signal Processing Letters, 2017, 24 (11) :1701- 1705" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multiple imagebased noise level estimation algorithm">
                                        <b>[13]</b>
                                        Xu Shaoping, Zeng Xiaoxia, Jiang Yinnan, et al.A multiple image-based noise level estimation algorithm[J].IEEE Signal Processing Letters, 2017, 24 (11) :1701- 1705
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                    Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C] //Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2014) .Piscataway, NJ:IEEE, 2014:580- 587</a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_15" title="Taigman Y, Yang Ming, Ranzato M, et al.Web-scale training for face identification[C] //Proc of the 18th IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2015) .Piscataway, NJ:IEEE, 2015:2746- 2754" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Web-scale training for face identification">
                                        <b>[15]</b>
                                        Taigman Y, Yang Ming, Ranzato M, et al.Web-scale training for face identification[C] //Proc of the 18th IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2015) .Piscataway, NJ:IEEE, 2015:2746- 2754
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_16" title="Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640- 651" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[16]</b>
                                        Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640- 651
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_17" title="Chang Liang, Deng Xiaoming, Zhou Mingquan, et al.Convolutional neural networks in image understanding[J].Acta Automatica Sinica, 2016, 42 (9) :1300- 1312 (in Chinese) (常亮, 邓小明, 周明全, 等.图像理解中的卷积神经网络[J].自动化学报, 2016, 42 (9) :1300- 1312) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201609002&amp;v=MTc2MjBGckNVUkxPZVplUnFGeW5sVUwvS0tDTGZZYkc0SDlmTXBvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Chang Liang, Deng Xiaoming, Zhou Mingquan, et al.Convolutional neural networks in image understanding[J].Acta Automatica Sinica, 2016, 42 (9) :1300- 1312 (in Chinese) (常亮, 邓小明, 周明全, 等.图像理解中的卷积神经网络[J].自动化学报, 2016, 42 (9) :1300- 1312) 
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_18" title="Zhou Feiyan, Jin Linpeng, Dong Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229- 1251 (in Chinese) (周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229- 1251) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MTc3NDBGckNVUkxPZVplUnFGeW5sVUwvS0x6N0Jkckc0SDliTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Zhou Feiyan, Jin Linpeng, Dong Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229- 1251 (in Chinese) (周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229- 1251) 
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_19" title="Nair V, Hinton G E.Rectified linear units improve restricted Boltzmann machines[C] //Proc of the 27th Int Conf on Int Conf on Machine Learning.New York:ACM, 2010:807- 814" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted Boltzmann machines">
                                        <b>[19]</b>
                                        Nair V, Hinton G E.Rectified linear units improve restricted Boltzmann machines[C] //Proc of the 27th Int Conf on Int Conf on Machine Learning.New York:ACM, 2010:807- 814
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_20" title="Rawat W, Wang Zenghui.Deep convolutional neural networks for image classification:A comprehensive review[J].Neural Computation, 2017, 29 (9) :2352- 2449" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK95C29D0CF3B7704D03904EAC08C95C8A&amp;v=MzEyNDFTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhoeGJpOHdLbz1OaWZKWmJxOWJkUEYyNDgyRXVoOUMzczV5MklUNlRaOVRBcVQzeEk5Q3J1Uk5yTHVDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        Rawat W, Wang Zenghui.Deep convolutional neural networks for image classification:A comprehensive review[J].Neural Computation, 2017, 29 (9) :2352- 2449
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_21" title="Li Yanghao, Wang Naiyan, Shi Jianping, et al.Adaptive batch normalization for practical domain adaptation[J].Pattern Recognition, 2016, 80:109- 117" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES207B769A00307A2E5CBB3276FBC33EA0&amp;v=MDI3MTR1ZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHhiaTh3S289TmlmT2ZiRzRHYVBMcVlZMFpPc01ESHRJeldNV21VMFBTMzNscW1SSENyR1hNTQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Li Yanghao, Wang Naiyan, Shi Jianping, et al.Adaptive batch normalization for practical domain adaptation[J].Pattern Recognition, 2016, 80:109- 117
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_22" title="Mallat S G.A theory for multiresolution signal decomposition:The wavelet representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1989, 11 (7) :674- 693" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Theory for Multiresolution Signal Decomposition: The Wavelet Representation">
                                        <b>[22]</b>
                                        Mallat S G.A theory for multiresolution signal decomposition:The wavelet representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1989, 11 (7) :674- 693
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_23" title="Sun Bo, Chen Songcan, Wang Jiandong, et al.A robust multi-class AdaBoost algorithm for mislabeled noisy data[J].Knowledge-Based Systems, 2016, 102 (5) :87- 102" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A robust multi-class Adaboost algorithm for mislabeled noisy data">
                                        <b>[23]</b>
                                        Sun Bo, Chen Songcan, Wang Jiandong, et al.A robust multi-class AdaBoost algorithm for mislabeled noisy data[J].Knowledge-Based Systems, 2016, 102 (5) :87- 102
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_24" title="Arbelaez P, Maire M, Fowlkes C, et al.Contour detection and hierarchical image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898- 916" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contour Detection and Hierarchical Image Segmentation">
                                        <b>[24]</b>
                                        Arbelaez P, Maire M, Fowlkes C, et al.Contour detection and hierarchical image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898- 916
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_25" title="Liu Xinhao, Tanaka M, Okutomi M.Single-image noise level estimation for blind denoising[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5226- 5237" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image noise level estimation for blind denoising">
                                        <b>[25]</b>
                                        Liu Xinhao, Tanaka M, Okutomi M.Single-image noise level estimation for blind denoising[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5226- 5237
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(05),1060-1070 DOI:10.7544/issn1000-1239.2019.20180185            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于CNN噪声分离模型的噪声水平估计算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%B0%91%E5%B9%B3&amp;code=08023640&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐少平</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%A9%B7%E4%BA%91&amp;code=41659138&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘婷云</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%B4%87%E7%A6%A7&amp;code=41157576&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李崇禧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E7%A5%8E%E7%8E%B2&amp;code=10558755&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐祎玲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%87%8C%E7%87%95&amp;code=08045747&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡凌燕</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E6%98%8C%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0252160&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南昌大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有的噪声水平估计 (noise level estimation, NLE) 算法通常采取先将图像内容信号与噪声信号分离, 然后基于分离出的噪声信号估计出图像噪声水平值的实现策略.由于仅有噪声图像本身的信息可以利用, 这些算法为保证噪声分离的准确性设计了各种复杂的处理过程, 导致其执行效率偏低.为此, 提出一种新的基于卷积神经网络噪声分离模型的NLE算法.首先, 对大量原始无失真图像施加不同噪声水平的高斯噪声获得噪声图像集合, 然后利用卷积神经网络构建一个专门从噪声图像中分离噪声信号获得噪声映射图 (noise mapping) 的预测模型.考虑到噪声映射图的系数值具有类高斯分布特性, 利用广义高斯分布 (generalized Gaussian distribution, GGD) 模型对噪声映射图建模并以模型参数值作为反映图像噪声水平高低的特征值.最后, 利用改进的BP神经网络将该特征值映射为最终的噪声水平预测值.大量实验数据表明:所提出的NLE算法在预测准确度和执行效率2个方面的综合性能优于现有的NLE算法, 更具实用价值.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%99%AA%E5%A3%B0%E6%B0%B4%E5%B9%B3%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">噪声水平估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%99%AA%E5%A3%B0%E5%88%86%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">噪声分离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%BF%E4%B9%89%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广义高斯模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%99%AA%E5%A3%B0%E6%B0%B4%E5%B9%B3%E6%84%9F%E7%9F%A5%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">噪声水平感知特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%99%AA%E5%A3%B0%E6%B0%B4%E5%B9%B3%E5%80%BC%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">噪声水平值映射;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *唐祎玲, tangyiling@ncu.edu.cn;
                                </span>
                                <span>
                                    徐少平, xushaoping@ncu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61662044, 61163023, 51765042);</span>
                                <span>江西省自然科学基金项目 (20171BAB202017);</span>
                    </p>
            </div>
                    <h1><b>Noise Level Estimation Algorithm Using Convolutional Neural Network-Based Noise Separation Model</b></h1>
                    <h2>
                    <span>Xu Shaoping</span>
                    <span>Liu Tingyun</span>
                    <span>Li Chongxi</span>
                    <span>Tang Yiling</span>
                    <span>Hu Lingyan</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, Nanchang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The existing noise level estimation (NLE) algorithms usually adopt the strategy that separates the noise signal from the content of an image to estimate its noise level. Since only a single noisy image can be exploited, these algorithms usually design a variety of complex processes to ensure the accuracy of noise separation, resulting in low execution efficiency. To this end, a novel NLE algorithm using convolutional neural network (CNN) -based noise separation model is proposed in this paper. Specifically, we first add Gaussian noise with different levels to a great amount of representative undistorted images to obtain a training database. Then, we train a CNN-based noise separation model on the training database to obtain the noise mapping from a given noisy image. Considering the fact that the coefficients of the noise mapping show Gaussian distribution behavior, we utilize the generalized Gaussian distribution (GGD) to model the coefficients of the noise mapping, and use two parameters (scale and shape) of the model as the noise level-aware features (NLAF) to describe the level of a noisy image. Finally, an improved back propagation (BP) neural network is used to map the NLAF features to the final noise level. Extensive experiments demonstrate that our method outperforms the most existing classical NLE algorithms in terms of both computational efficiency and estimation accuracy, which makes it more practical to use.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noise%20level%20estimation%20(NLE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noise level estimation (NLE) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noise%20separation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noise separation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generalized%20Gaussian%20distribution%20(GGD)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generalized Gaussian distribution (GGD) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noise%20level-aware%20feature%20(NLAF)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noise level-aware feature (NLAF) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noise%20level%20mapping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noise level mapping;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Xu Shaoping, born in 1976. PhD, professor and PhD supervisor.His main research interests include digital image processing machine vision, and virtual surgery simulation.<image id="196" type="formula" href="images/JFYZ201905016_19600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Tingyun, born in 1996. Master candidate. Her main research interests include digital image processing, machine vision.<image id="198" type="formula" href="images/JFYZ201905016_19800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Li Chongxi, born in 1994. Master candidate. His main research interests include digital image processing, machine vision.<image id="200" type="formula" href="images/JFYZ201905016_20000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Tang Yiling, born in 1977.PhD candidate, lecturer. Her main research interests include digital image processing and machine vision.<image id="202" type="formula" href="images/JFYZ201905016_20200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Hu Lingyan, born in 1978.PhD, professor and PhD supervisor of Nanchang University.Her main research interests include digital image processing machine vision, and virtual surgery simulation.<image id="204" type="formula" href="images/JFYZ201905016_20400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-03-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61662044, 61163023, 51765042);</span>
                                <span>the Natural Science Foundation of Jiangxi Province of China (20171BAB202017);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="58">对于图像超分辨率重建<citation id="308" type="reference"><link href="256" rel="bibliography" /><link href="258" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>、图像去模糊<citation id="306" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、图像降噪<citation id="309" type="reference"><link href="262" rel="bibliography" /><link href="264" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>等各类图像复原算法来说, 噪声水平值是一个至关重要的输入参数, 不准确的噪声水平值会在很大程度上影响这些算法的性能.然而, 大多数已提出的图像复原算法<citation id="307" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>都属于非盲类型的 (non-blind) , 即需要人工手动设置并调整相应的噪声水平值才能获得最佳的图像复原效果, 这严重限制了它们的实际应用范围.因此, 研究自动化的图像噪声水平评估 (noise level estimation, NLE) 算法是很有必要的.</p>
                </div>
                <div class="p1">
                    <p id="59">现有的大多数NLE算法的核心技术路线是先将图像内容信号和噪声信号分离, 然后根据分离出的噪声信号提取特征进而估计噪声水平值.例如Immerkær利用1个3×3的矩阵掩模, 对图像局部邻域内的加性0均值高斯噪声求和, 再根据方差估算出噪声水平值<citation id="310" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.虽然该算法的执行效率很高, 但其噪声水平估计准确率较低;Zoran等人<citation id="311" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>认为图像噪声是引起边缘带通滤波器响应分布峰度值 (kurtosis value) 变化的主要因素, 通过构建优化模型对噪声水平值进行估计.该算法在中低水平噪声条件下的估计准确度较高, 但是在噪声水平较高的情况下其估计效果不理想, 执行时间较长.近期Katase等人<citation id="312" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了一种基于离散余弦变换 (discrete cosine transform, DCT) 系数特征的NLE算法.该算法根据DCT系数中高频部分的标准方差选取平滑的图块, 利用噪声图像中的高频系数计算出噪声水平值, 它在噪声水平较低的情况下性能良好, 对于高水平噪声, 预测效果不够理想.Rakhshanfar等人<citation id="313" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>根据亮度值和方差对图像分解的图块进行分类, 基于噪声统计特性和图像同质区域, 将与噪声模型相似程度最高的图块群 (patches clusters) 用于估计噪声方差.该方法可以处理多种类型图像噪声, 在预测准确性表现相对良好, 但是其执行效率与图像内容的复杂度密切相关.当噪声水平值较高时, 执行时间显著增加.Chen等人<citation id="314" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>通过研究发现无失真图像中提取的图块通常位于所谓的低维子空间 (low-dimensional subspace) , 可以利用主成分分析 (principal component analysis, PCA) 方法获得冗余维度上的特征值, 将其用于估计噪声方差.该算法用循环穷举的方式来确定冗余维度上的特征值, 导致算法效率不高.</p>
                </div>
                <div class="p1">
                    <p id="60">总体上来说, 上述算法都属于所谓的基于单图像的噪声水平估计 (single-image based noise level estimation, SNLE) 算法.在估计噪声水平值时仅有待评价图像本身的信息可以利用, 因此需要设计复杂的处理流程将噪声信号从图像中分离 (提取) 出来后才能进一步基于噪声信号部分进行估计, 在执行效率方面并不是很理想.NLE算法作为众多图像处理算法的预处理模块, 除了预测准确性外, 执行效率也是重要的评价指标.近年来, 笔者试图通过基于训练的方法, 充分利用蕴藏在已知多幅图像中的规律性的信息, 从图像中提取反映噪声水平值高低的特征值后实现快速的噪声水平映射 (预测) , 这种方法被称为基于多图像的噪声水平估计 (multiple-image based noise level estimation, MNLE) 算法.例如在文献<citation id="315" type="reference">[<a class="sup">12</a>]</citation>中, 提出了用梯度幅值和拉普拉斯变换2种算子提取噪声水平感知特征 (noise level-aware feature, NLAF) , 然后基于在大量训练数据上获得的NLAF特征值以及相应的噪声水平值利用支持向量回归 (support vector regression, SVR) 完成预测模型的训练.预测模型所预测的噪声水平值能使BM3D (block-matching and 3D filtering) 降噪算法获得最佳降噪效果.基于训练的MNLE算法的执行效率比现有经典算法具有明显优势, 但是预测精度水平较主流算法尚有一定差距.提高基于MNLE算法的预测准确性需要从2个方面入手, 即提取高质量的能反映噪声水平高低的NLAF特征和利用机器学习算法训练非线性映射能力更强的预测模型完成从NLAF特征值到噪声水平值的映射<citation id="316" type="reference"><link href="278" rel="bibliography" /><link href="280" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="61">为了提取对噪声水平值更为敏感的NLAF特征, 高质量地将图像内容与噪声信号分离是基础性的工作.为此, 将近年来出现的非线性逼近能力强大的卷积神经网络 (convolutional neural network, CNN) <citation id="317" type="reference"><link href="282" rel="bibliography" /><link href="284" rel="bibliography" /><link href="286" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>技术引入到MNLE算法中来, 提出一种基于CNN噪声分离模型的噪声水平估计算法.该算法利用CNN网络训练一个专门的预测模型完成将噪声信号从噪声图像中剥离出来的任务, 然后利用通用高斯模型 (generalized Gaussian distribution, GGD) 对噪声映射图 (noise mapping) 进行建模并利用模型参数值作为反映图像噪声水平高低的NLAF特征值.最后利用改进的BP映射模型完成从NLAF特征值到噪声水平值的预测任务.相对于已有的各类NLE算法, 所提出的NLE算法的优势主要体现在3个方面:</p>
                </div>
                <div class="p1">
                    <p id="62">1) 噪声信号分离效果好.充分利用了CNN强大的非线性逼近能力, 在已知噪声水平下的样本图像集合中学习先验知识, 高质量地将噪声信号从噪声图像中分离出来, 为构建高敏感度的NLAF特征打下了很好的基础.</p>
                </div>
                <div class="p1">
                    <p id="63">2) NLAF特征到噪声水平的映射能力强.采用改进的BP神经网络技术获得了比常规SVR支持向量回归映射更高的预测正确率.</p>
                </div>
                <div class="p1">
                    <p id="64">3) 执行效率高.改进算法的特征值提取和噪声水平值的映射工作是由CNN和改进BP (back propagation) 神经网络来完成的.一旦训练完成, 其执行效率非常高, 尤其是CNN卷积网络在GPU (graphics processing unit) 硬件支持下运行时间很短, 几乎可以忽略.</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag"><b>1 CNN简介</b></h3>
                <div class="p1">
                    <p id="66">CNN卷积神经网络本质上可以看作是一种特殊的映射函数, 它能够从大量原始图像 (图块) 数据中挖掘隐藏的特征信息, 从而学习输入与输出值之间复杂的映射关系<citation id="318" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.典型的多层卷积神经网络由输入层、多个隐藏层和输出层组成.隐藏层中关键的组成结构是卷积 (convolutional, Conv) 结构<mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo>=</mo><mrow><mi>C</mi><msub><mrow></mrow><mi>W</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><mo>.</mo></mrow></mrow></math></mathml>对于输入的图块<b><i>X</i></b> (<i>u</i>) = (<i>X</i><sub>1</sub> (<i>u</i>) , <i>X</i><sub>2</sub> (<i>u</i>) , …, <i>X</i><sub><i>p</i></sub> (<i>u</i>) ) , 通过1个滤波器<b><i>W</i></b>= (<i>w</i><sub><i>l</i>, <i>l</i>′</sub>) , <i>l</i>=1, 2, …, <i>q</i>, <i>l</i>′=1, 2, …, <i>p</i>和逐点非线性函数 (point-wise nonlinearity function) <i>ψ</i> (·) 进行卷积<citation id="319" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mi>ψ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>l</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>X</mi><msub><mrow></mrow><msup><mi>l</mi><mo>′</mo></msup></msub><mo>*</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><msup><mi>l</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>. (1) </p>
                </div>
                <div class="p1">
                    <p id="70">在卷积层中, 用滤波器与上一层若干个特征映射图 (feature map) 进行卷积, 计算出新的特征映射图, 通过激活函数将特征保留并输出到下一层结构.近年来研究者们主要采用修正线性单元 (rectified linear unit, ReLU) <citation id="320" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>作为激活函数, 其数学形式表述为<i>ψ</i> (<i>x</i>) =max (<i>x</i>, 0) , 将负值置0.ReLU能够加快整个网络的收敛速度, 而不影响卷积层的感受野 (receptive field) , 大大提高了激活函数的性能<citation id="321" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.卷积后产生<i>q</i>维的特征映射图 (feature map) </p>
                </div>
                <div class="p1">
                    <p id="71"><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>, </mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>. (2) </p>
                </div>
                <div class="p1">
                    <p id="73">此外, 池化层<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>˜</mo></mover><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo></mrow></math></mathml>也是CNN网络中常用的组成构件.对于每一个输入<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>˜</mo></mover></math></mathml><sub><i>l</i></sub> (<i>u</i>) , 有:</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>X</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><msup><mi>u</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>:<i>u</i>′∈<i>N</i> (<i>u</i>) }) , (3) </p>
                </div>
                <div class="p1">
                    <p id="78">其中, <i>l</i>=1, 2, …, <i>q</i>, <i>N</i> (<i>u</i>) 是<i>u</i>的邻域, <i>P</i>是某种类型的池化函数 (可以是最大值、均值等类型) .基本的CNN卷积神经网络可以表示为卷积层和可选的池化层的多次组合, 即</p>
                </div>
                <div class="p1">
                    <p id="79"><i>Φ</i><sub><b><i>H</i></b></sub> (<b><i>X</i></b>) = (<i>C</i><sub><b><i>H</i></b><sup> (<i>K</i>) </sup></sub>…<i>P</i>…。<i>C</i><sub><b><i>H</i></b><sup> (2) </sup></sub>。<i>C</i><sub><b><i>H</i></b><sup> (1) </sup></sub>) (<b><i>X</i></b>) , (4) </p>
                </div>
                <div class="p1">
                    <p id="80">其中, <i>P</i>为可缺省的池化操作, <b><i>H</i></b>= (<b><i>H</i></b><sup> (1) </sup>, <b><i>H</i></b><sup> (2) </sup>, …, <b><i>H</i></b><sup> (<i>K</i>) </sup>) 是网络参数 (卷积核的系数值) 的超矢量 (hyper-vector) .通常, 在卷积神经网络中的卷积层或者池化层之后可以连接全连接层 (fully connected layer) , 全连接层在整个CNN中用来获得特征, 为后面的分类或者回归任务做准备.另外, 为了减少内部协变量转移对网络参数的影响, 批归一化层 (batch normalization, BN) <citation id="322" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>常被用来对卷积神经网络隐藏层的输出数据进行批归一化处理, 从而加快神经网络的收敛速度并获得稳定性.</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>2 基于噪声分离模型的NLE算法</b></h3>
                <h4 class="anchor-tag" id="82" name="82"><b>2.1 NLE算法的基本思想</b></h4>
                <div class="p1">
                    <p id="83">如图1所示, 改进算法技术路线的关键之处在于:首先高质量地将噪声图像的内容信号与噪声信号进行分离获得噪声映射图;进而基于噪声映射图提取NLAF特征;最后通过训练学习获得映射能力强的预测模型将NLAF特征准确地映射为噪声水平值.具体地, 在训练阶段:1) 选择大量具有代表性的自然图像添加不同噪声水平的高斯噪声获得噪声图像集合, 噪声图像、无失真图像、噪声图像与无失真图像相减获得的噪声映射图以及对应的噪声水平值构成训练数据集合;2) 通过训练CNN网络在噪声图像和噪声映射图之间构建CNN噪声分离模型, 利用该模型即可完成将噪声信号从噪声图像中剥离出来的任务;3) 分离噪声信号后, 采用GGD模型对噪声映射图进行建模并利用模型参数值作为反映图像噪声水平高低的NLAF特征值;4) 基于从训练集中的噪声图像上提取出的特征值及相应的真实噪声水平值, 训练改进的BP映射模型完成从NLAF特征值到噪声水平值的预测任务.在预测阶段:1) 将噪声图像输入到CNN噪声分离模型分离出噪声映射图;2) 用GGD特征提取模型提取其NLAF特征并由改进的BP映射模型最终映射为噪声水平值.</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905016_084.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 改进算法流程框图" src="Detail/GetImg?filename=images/JFYZ201905016_084.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 改进算法流程框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905016_084.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The overall framework of the proposed method</p>

                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>2.2 分离噪声信号</b></h4>
                <div class="p1">
                    <p id="86">本文主要工作是利用CNN网络强大的学习能力, 达到从噪声图像中分离出噪声信号的目的.一般来讲, 受到噪声干扰的图像的数据模型可以定义为</p>
                </div>
                <div class="p1">
                    <p id="87"><i>y</i>=<i>x</i>+<i>n</i>, (5) </p>
                </div>
                <div class="p1">
                    <p id="88">其中, <i>y</i>是噪声图像, <i>x</i>是原始图像, <i>n</i>是高斯噪声.笔者利用CNN预测模型<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>n</mi><mo>^</mo></mover><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Ν</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>获得噪声映射图, 进而基于噪声映射图估计噪声图像的噪声水平值.</p>
                </div>
                <div class="p1">
                    <p id="90">基于上述设计需求, 与一般利用CNN卷积神经网络完成分类或者回归任务不同, 所提出的算法需要基于输入的噪声图像各个像素点的亮度值得到对应像素点的噪声值, 故输入模型的图像大小和输出图像的大小相同.因此, 所设计的CNN卷积神经网络中不使用池化层和全连接层结构, 仅包括卷积层、BN和ReLU共3种类型的网络部件, 具体结构如图2所示.从图2中可以看出, 网络从左到右共分3段:</p>
                </div>
                <div class="p1">
                    <p id="91">1) Conv+ReLU.第1段用64个大小为3×3的卷积核生成64个特征图像.</p>
                </div>
                <div class="p1">
                    <p id="92">2) Conv+BN+ReLU.第2段与第1段的区别是使用了BN批归一化层, 使用BN层的目的是为了保证每一层网络的输入具有相同的分布、加快训练进程并提高噪声水平预测准确率.</p>
                </div>
                <div class="p1">
                    <p id="93">3) Conv.最后1段采用1个大小为3×3×64的卷积核来输出噪声映射图.</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905016_094.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN噪声分离模型的网络体系架构图" src="Detail/GetImg?filename=images/JFYZ201905016_094.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CNN噪声分离模型的网络体系架构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905016_094.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The network architecture of CNN-based  noise separation model</p>

                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>2.3 噪声映射图与GGD建模</b></h4>
                <div class="p1">
                    <p id="96">为了验证卷积神经网络对噪声图像中的噪声信号部分的实际分离效果, 以1幅512×512大小的Lena图像为例, 对其分别添加噪声级别为5, 10, 20, 40, 50的高斯噪声, 然后利用所提出的CNN分离模型将噪声信号分离出来得到噪声映射图, 计算噪声映射图中系数值分布情况, 系数分布直方图如图3所示 (为了便于显示和比较, 图3中的横坐标值所代表的残差系数值 (噪声值) 被归一化到[-1, 1]之间, 而纵坐标则是像素点个数除以1 000以后的结果) .由图3可知, 噪声映射图的系数值分布具有显著的类高斯分布特征, 可用GGD广义高斯分布对噪声映射图进行建模.</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905016_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 系数值分布直方图" src="Detail/GetImg?filename=images/JFYZ201905016_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 系数值分布直方图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905016_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 The distribution histogram of coefficient values</p>

                </div>
                <div class="p1">
                    <p id="98">GGD模型<citation id="323" type="reference"><link href="298" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>是一种应用非常广泛的分布模型, 一般定义为</p>
                </div>
                <div class="p1">
                    <p id="99"><i>p</i> (<i>x</i>;<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>, </mo><mi>β</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi>β</mi><mrow><mn>2</mn><mi>α</mi><mtext>Γ</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><mi>β</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow></mrow><mi>α</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mi>β</mi></msup></mrow><mo>) </mo></mrow></mrow></math></mathml>, (6) </p>
                </div>
                <div class="p1">
                    <p id="101">其中, Γ (·) 是Gamma函数, 即:</p>
                </div>
                <div class="p1">
                    <p id="102"><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Γ</mtext><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mrow><msubsup><mo>∫</mo><mn>0</mn><mi>∞</mi></msubsup><mi>u</mi></mrow></mstyle><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mi>u</mi></mrow><mo>) </mo></mrow><mtext>d</mtext><mi>u</mi><mo>, </mo><mi>t</mi><mo>&gt;</mo><mn>0</mn></mrow></math></mathml>. (7) </p>
                </div>
                <div class="p1">
                    <p id="104">式 (6) 中的<i>α</i>为尺度 (scale) 参数, 控制着GGD密度函数的扩散程度.<i>β</i>为形状 (shape) 参数, 用于修正峰度值的衰减速度.参数<i>α</i>和<i>β</i>与GGD分布的标准差<i>σ</i>满足关系式:</p>
                </div>
                <div class="p1">
                    <p id="105"><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>=</mo><mi>σ</mi><msqrt><mrow><mfrac><mrow><mtext>Γ</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><mi>β</mi><mo stretchy="false">) </mo></mrow><mrow><mtext>Γ</mtext><mo stretchy="false"> (</mo><mn>3</mn><mo>/</mo><mi>β</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt></mrow></math></mathml>, (8) </p>
                </div>
                <div class="p1">
                    <p id="107">其中, <i>β</i>&gt;0.当<i>β</i>=1时, <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>=</mo><msqrt><mn>2</mn></msqrt><mi>σ</mi><mo>/</mo><mn>2</mn></mrow></math></mathml>, 对应于拉普拉斯分布;当<i>β</i>=2时, <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>=</mo><msqrt><mn>2</mn></msqrt><mi>σ</mi></mrow></math></mathml>, 对应于标准高斯分布;当<i>β</i>→0时, 分布趋于<i>δ</i>函数;当<i>β</i>→+∞时, 分布趋于均匀分布.</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>2.4 噪声水平感知特征选取</b></h4>
                <div class="p1">
                    <p id="111">用GGD模型为噪声映射图系数值分布建模后, 模型中共有2个参数, 即<i>α</i>和<i>β</i>.为了测试模型中的2个参数是否能够作为NLAF特征值使用, 用GGD模型为图2中5个级别的噪声映射图建模, 所提取的模型参数如表1所列.从表1中可以看出:<i>α</i>参数值与相应噪声水平值之间的Pearson线性相关系数 (Pearson linear correlation coefficient, PLCC) 为1, 表明这2者之间具有极强的相关性;<i>β</i>参数值与相应噪声水平值之间的PLCC系数为0.649 6, 表明这2者之间的相关性处于中等水平.为了全面验证该参数是否与噪声值具有强相关性, 在100幅图像上执行与图2相同的处理过程并提取GGD模型的2个参数值, 再计算它们分别与真实噪声水平值之间的PLCC相关系数.经计算, <i>α</i>值与真实噪声水平值的线性相关系数为0.998 7, 继续保持非常高的相关性.而<i>β</i>值与真实噪声水平值的线性相关系数为0.668 3, 有所提高但仍然处于中等水平.为了获得鲁棒的噪声水平值, 将参数值<i>α</i>和<i>β</i>组合作为NLAF噪声水平感知特征矢量<b><i>F</i></b>= (<i>α</i>, <i>β</i>) 来使用.</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表1 GGD参数值与噪声水平值的相关系数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Correlation Coefficients Between the Noise Levels and Parameters of GGD Model</b></p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td rowspan="2"><br />Parameters</td><td colspan="5"><br />Noise Levels</td><td rowspan="2">PLCC</td></tr><tr><td><br />5</td><td>10</td><td>20</td><td>40</td><td>50</td></tr><tr><td><br /><i>α</i></td><td>0.017 1</td><td>0.036 5</td><td>0.074 8</td><td>0.151 7</td><td>0.190 1</td><td>1.000 0</td></tr><tr><td><br /><i>β</i></td><td>1.754 0</td><td>1.961 0</td><td>1.990 0</td><td>1.996 0</td><td>1.989 0</td><td>0.649 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>2.5 改进的BP神经网络</b></h4>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905016_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 改进BP神经网络工作原理" src="Detail/GetImg?filename=images/JFYZ201905016_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 改进BP神经网络工作原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905016_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Pipeline of the improved BP neural network</p>

                </div>
                <div class="p1">
                    <p id="115">用GGD模型在大量训练图像上提取噪声映射图的特征值后, 理论上即可利用任何回归分析技术将特征值 (构成矢量形式) 映射为最终的噪声水平预测值 (标量) .由于所使用的特征值只有2维, 维数较少, 利用常规的SVR构建预测模型的预测准确性不是很理想.如图4所示, 为了进一步提升预测准确性, 首先利用若干个BP网络构建预测能力相对较弱的弱预测器, 然后利用AdaBoost算法<citation id="324" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>在多个弱预测器的基础上构建比较健壮、准确的强预测器.AdaBoost算法原本的设计思想是利用若干个弱分类器构建强分类器的分类问题, 这里将其运用到回归问题中来, 所用的预测模型称为改进的BP映射模型.</p>
                </div>
                <div class="p1">
                    <p id="116">选取<i>K</i>幅图像进行训练, 对每幅图像添加已知噪声水平值为<i>σ</i><sub><i>i</i></sub>的高斯噪声, 用GGD模型提取对应的特征矢量<b><i>F</i></b><sub><i>i</i></sub> (其中<i>i</i>=1, 2, …, <i>K</i>) 构成训练集合{ (<b><i>F</i></b><sub>1</sub>, <i>σ</i><sub>1</sub>) , (<b><i>F</i></b><sub>2</sub>, <i>σ</i><sub>2</sub>) , …, (<b><i>F</i></b><sub><i>K</i></sub>, <i>σ</i><sub><i>K</i></sub>) }⊂R<sup>2</sup>×R, 训练出预测函数<i>φ</i> (·) 以实现特征矢量到噪声水平值之间的映射.改进BP映射算法的具体执行过程如下:首先, 确定弱预测器的数量 (根据实验数据<i>T</i>=5时, 已能满足精度要求且同时兼顾执行效率) .从第1个弱预测器开始, 利用BP神经网络建立特征矢量<b><i>F</i></b><sub><i>i</i></sub>和噪声水平值<i>σ</i><sub><i>i</i></sub>之间的映射关系, 第<i>j</i>条训练数据对预测结果的权重更新方法为</p>
                </div>
                <div class="area_img" id="117">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201905016_11700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="119">其中, <i>D</i><sub><i>i</i>, <i>j</i></sub>代表在第<i>i</i>个弱预测器中第<i>j</i>条训练数据对预测误差的权重, <i>C</i>为[0, 1]之间的常数.对于第1个弱预测器来说, 各个训练数据的权重初始化为1/<i>K</i>, 从第2个弱预测器开始, 则根据前一个预测器预测结果的误差利用二值函数 (binary function) </p>
                </div>
                <div class="area_img" id="120">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201905016_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="122">进行修正.根据实验数据, 设定参数<i>C</i>=0.1, <i>threshold</i>=0.1.第<i>i</i>个弱预测器整体的预测误差可以定义为用第<i>i</i>个预测器所预测的噪声估计值与真实噪声水平值之间的差值和训练数据权重乘积的累积和, 从数学形式上描述为</p>
                </div>
                <div class="p1">
                    <p id="123"><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>r</mi><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>D</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>×</mo><mi>l</mi><mo stretchy="false"> (</mo><mi>σ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mover accent="true"><mi>σ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>. (11) </p>
                </div>
                <div class="p1">
                    <p id="125">利用凸函数将每个弱预测器的预测误差转换为弱预测器在最终预测结果中的权重.这样使得那些预测误差较小的弱预测器的权重增加, 而误差较大的弱预测器的权重减小.第<i>i</i>个弱预测器的权重定义为</p>
                </div>
                <div class="p1">
                    <p id="126"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>b</mi><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi>E</mi><mi>r</mi><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mo>-</mo><mi>c</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (12) </p>
                </div>
                <div class="p1">
                    <p id="128">最后, 基于第<i>i</i>个弱预测器的权重及其所预测的噪声水平值<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>σ</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>, 计算出最终的噪声水平估计值:</p>
                </div>
                <div class="p1">
                    <p id="130"><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>σ</mi><mo>^</mo></mover><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mover accent="true"><mi>σ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>. (13) </p>
                </div>
                <div class="p1">
                    <p id="132">实验数据表明使用过多或者过少的弱预测器都将导致预测准确性下降.另外一方面, 随着使用更多的弱预测器, 执行效率也将随之下降.文中最终使用5个弱预测器构建改进的BP映射模型, 在这个配置下能在预测准确性和执行效率这2个方面获得最佳平衡点 (限于篇幅, 这里不再给出具体实验数据) .综上所述, 基于多个BP网络模型利用AdaBoost算法提升预测准确性的伪代码描述如下:</p>
                </div>
                <div class="p1">
                    <p id="133"><b>算法1</b>. 改进BP神经网络模型训练过程.</p>
                </div>
                <div class="p1">
                    <p id="134">输入:训练图像集{ (<b><i>F</i></b><sub>1</sub>, <i>σ</i><sub>1</sub>) , (<b><i>F</i></b><sub>2</sub>, <i>σ</i><sub>2</sub>) , …, (<b><i>F</i></b><sub><i>K</i></sub>, <i>σ</i><sub><i>K</i></sub>) }⊂R<sup>2</sup>×R;</p>
                </div>
                <div class="p1">
                    <p id="135">输出:各预测器模型{<i>BP</i><sub><i>i</i></sub><citation id="325" type="reference">[<a class="sup">1</a>]</citation>≤<i>i</i>≤5, <i>i</i>∈N<sub>+</sub>}及对应的权重值<i>w</i>={<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub>5</sub>}.</p>
                </div>
                <div class="area_img" id="205">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201905016_20500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="144" name="144" class="anchor-tag"><b>3 实验与分析</b></h3>
                <h4 class="anchor-tag" id="145" name="145"><b>3.1 测试环境</b></h4>
                <div class="p1">
                    <p id="146">从BSD (Berkeley segmentation dataset) 数据库<citation id="326" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>中随机选取了100幅图像, 对这些图像添加51个级别 (0, 1, …, 50) 的高斯噪声作为训练图像集合, 基于训练图像集合训练预测模型.测试图像集包括2个:第1个由在各类图像处理相关文献中常用的10幅图像组成, 如图5所示;为了增加测试难度, 在BSD数据库另选了与训练集合不同的50幅纹理图像构成第2个测试集合.BSD数据库原本作为纹理分割的基准测试数据库, 其图像中的纹理细节非常丰富, 很多NLE算法的性能在该数据库上表现不佳, 比较适合用来作为测试图像集合.参与比较的NLE算法包括Immerkær算法<citation id="327" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、Zoran算法<citation id="328" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Liu算法<citation id="329" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、Chen算法<citation id="330" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、Rakhshanfar算法<citation id="331" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和Xu算法<citation id="332" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 其中Xu算法是笔者之前基于主成分分析和SVR实现的NLE算法.为了验证改进的BP映射模型的性能, 本文对比了4种从NLAF特征到噪声水平值的映射模型.其中, 直接利用常规SVR和BP构建映射模型的NLE算法分别记为CNN+SVR和CNN+BP;基于SVR和BP利用AdaBoost算法改进的映射模型实现的NLE算法分别记为CNN+AdaSVR和CNN+AdaBP, 这样共有10个NLE算法参与对比实验.通过比较以上各个算法在不同噪声水平下的噪声估计准确性以及执行时间, 来测试所提出算法的综合性能.实验在Matlab R2017b (运行在Intel<sup>®</sup> Core<sup>TM</sup> i7-6700 CPU RAM 8 GB主机上) 环境下完成.</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905016_147.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各类文献中常用的图像集合" src="Detail/GetImg?filename=images/JFYZ201905016_147.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各类文献中常用的图像集合  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905016_147.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Commonly used images in the literature</p>

                </div>
                <h4 class="anchor-tag" id="148" name="148"><b>3.2 预测准确性</b></h4>
                <div class="p1">
                    <p id="149">在常用图像集合中的图像上施加噪声级别为5～50、间隔为5的噪声, 统计各对比算法在每幅图像上的预测结果, 以噪声水平预测值与真实值之间的绝对偏差值作为衡量算法预测准确性的指标.限于篇幅, 这里仅给出在Couple图像的预测结果, 预测结果排名前3的算法加粗标出, 准确性最高的另用下划线标出.由表2可知, Immerkær算法、Zoran算法、Chen算法、Xu算法、CNN+SVR算法和CNN+AdaSVR算法在Couple图像上的预测结果较其他对比算法还不够理想;Liu算法、Rakhshanfar算法、CNN+BP算法和所提出的CNN+AdaBP算法在各个噪声级别下表现出良好的预测准确性.从整体上看, Rakhshanfar算法的预测准确性在不同噪声水平值上变化较大, 稳定性较差;Liu算法在各噪声水平下尤其是低水平噪声条件下预测性能最好, 但Liu算法是以执行时间为代价才获得高准确性的.CNN+BP算法和CNN+AdaBP算法的预测准确性与Liu算法的相差不大, 但执行效率则具有显著优势, 因此所提出的CNN+AdaBP算法综合性能更好.</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表2 不同噪声水平下各算法在Couple图像上的评估结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Estimation Results Obtained with Different Algorithms on Couple Image at Different Noise Levels</b></p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="10"><br />Noise Levels</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />Immerkær</td><td>5.64</td><td>10.40</td><td>15.39</td><td>20.38</td><td>25.51</td><td>30.46</td><td>35.47</td><td>40.32</td><td>45.48</td><td>50.56</td></tr><tr><td><br />Zoran</td><td>4.13</td><td>9.13</td><td>14.12</td><td>19.04</td><td>23.75</td><td>38.83</td><td>33.78</td><td>38.83</td><td>43.59</td><td>48.72</td></tr><tr><td><br />Ref [25]</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>4</mn><mo>.</mo><mn>9</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>9.89</b></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mn>4</mn><mo>.</mo><mn>9</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>19.77</b></td><td><b>24.81</b></td><td><b>29.90</b></td><td>34.69</td><td>39.49</td><td>44.72</td><td>49.47</td></tr><tr><td><br />Ref [11]</td><td>5.49</td><td>10.43</td><td><b>15.29</b></td><td>20.41</td><td>25.26</td><td><b>30.29</b></td><td>35.59</td><td>40.40</td><td>45.26</td><td>50.61</td></tr><tr><td><br />Ref [10]</td><td><b>4.93</b></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>1</mn><mn>0</mn><mo>.</mo><mn>0</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>14.52</td><td>18.37</td><td>24.78</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>3</mn><mn>0</mn><mo>.</mo><mn>0</mn><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>35.18</td><td>40.40</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>4</mn><mn>4</mn><mo>.</mo><mn>9</mn><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>5</mn><mn>0</mn><mo>.</mo><mn>0</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />Ref [1]</td><td>4.18</td><td>9.28</td><td>14.52</td><td>19.39</td><td>27.17</td><td>30.98</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>3</mn><mn>5</mn><mo>.</mo><mn>0</mn><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>40.23</b></td><td>44.06</td><td><b>50.30</b></td></tr><tr><td><br />CNN+SVR</td><td>8.09</td><td>12.99</td><td>17.42</td><td>21.40</td><td>26.09</td><td>30.59</td><td><b>35.05</b></td><td>38.94</td><td>43.04</td><td>46.36</td></tr><tr><td><br />CNN+AdaSVR</td><td>8.69</td><td>13.96</td><td>18.28</td><td>22.81</td><td>27.35</td><td>31.51</td><td>35.95</td><td><b>40.23</b></td><td>44.72</td><td>48.65</td></tr><tr><td><br />CNN+BP</td><td><b>4.74</b></td><td>9.63</td><td>14.70</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>2</mn><mn>0</mn><mo>.</mo><mn>1</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>25.05</b></td><td>30.32</td><td><b>35.15</b></td><td><b>40.13</b></td><td><b>45.09</b></td><td>49.38</td></tr><tr><td><br />CNN+AdaBP</td><td>4.56</td><td><b>9.68</b></td><td><b>14.78</b></td><td><b>19.80</b></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>2</mn><mn>5</mn><mo>.</mo><mn>0</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td>29.15</td><td>34.68</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>4</mn><mn>0</mn><mo>.</mo><mn>0</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>44.95</b></td><td><b>49.49</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The top 3 results are bold, and the best values are underlined.</p>
                </div>
                <div class="p1">
                    <p id="152">在单幅图像上的噪声水平估计值的对比并不能全面地评价某个算法的鲁棒性.因此, 对于图5中的10幅测试图像, 计算各个算法对它们噪声水平的估计值与真实值之间的均方根误差 (root mean squared error, RMSE) , 结果列在表3中.在各噪声水平下, 前3个最好的算法加粗标出, 最小的RMSE值另用下划线标出.从表3中的数据可以看出:各算法性能与在单幅图像上情况近似, CNN+SVR和CNN+AdaSVR算法的预测准确性比较差, 未进入前3名, 这说明SVR不适合用于完成噪声水平值的映射任务, 而Adaboost技术也不能显著提高SVR的映射能力;Liu算法、CNN+BP算法和CNN+AdaBP算法的均方根误差排名前3, 其中Liu算法在低噪声水平条件下的均方根误差非常小 (以执行时间为代价) , 在中高噪声水平条件下的均方根误差相对于低噪声水平却要大许多, 稳定性不够好;CNN+BP算法整体上均方根误差的平均值比CNN+AdaBP算法稍差.CNN+AdaBP算法在各个级别的噪声条件下的均方根误差都比较小, 且预测准确性比较稳定.这说明了CNN+AdaBP算法利用Adaboost技术提升BP映射模型的性能是成功的, 这是因为CNN+AdaBP算法中BP网络弱预测器本身具有一定的预测能力, 且适当个数的弱预测器之间能形成互补, 在Adaboost框架下组合利用多个BP弱预测器后其预测准确性得到进一步提升.</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表3 各算法在10幅常用图像集合上预测结果的均方根误差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 RMSE Between Estimated Results and Ground Truths on Ten Commonly Used Images</b></p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="10"><br />Noise Levels</td><td rowspan="2">Average<br />Value</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />Immerkær</td><td>2.08</td><td>1.55</td><td>1.26</td><td>1.06</td><td>0.95</td><td>0.87</td><td>0.84</td><td>0.82</td><td>0.84</td><td>0.90</td><td>1.12</td></tr><tr><td><br />Zoran</td><td>2.34</td><td>1.28</td><td>1.19</td><td>1.37</td><td>1.60</td><td>1.32</td><td>1.38</td><td>1.50</td><td>1.49</td><td>1.81</td><td>1.53</td></tr><tr><td><br />Ref [25]</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>0</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>0</mn><mn>9</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>1</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>1</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>0</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>0.35</b></td><td><b>0.44</b></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />Ref [11]</td><td>2.90</td><td>1.91</td><td>1.48</td><td>1.23</td><td>0.97</td><td>0.74</td><td>0.93</td><td>0.59</td><td>0.64</td><td>0.75</td><td>1.21</td></tr><tr><td><br />Ref [10]</td><td><b>0.59</b></td><td>0.55</td><td>0.48</td><td><b>0.30</b></td><td>0.59</td><td><b>0.34</b></td><td>0.59</td><td>0.56</td><td>0.94</td><td>0.85</td><td>0.58</td></tr><tr><td><br />Ref [1]</td><td>0.87</td><td>1.31</td><td>1.11</td><td>0.84</td><td>2.48</td><td>1.44</td><td>0.84</td><td>1.03</td><td>0.97</td><td><b>0.73</b></td><td>1.16</td></tr><tr><td><br />CNN+SVR</td><td>2.84</td><td>2.85</td><td>2.30</td><td>1.82</td><td>1.42</td><td>0.85</td><td>0.44</td><td>0.73</td><td>1.95</td><td>3.50</td><td>1.87</td></tr><tr><td><br />CNN+AdaSVR</td><td>3.31</td><td>3.42</td><td>3.02</td><td>2.55</td><td>2.05</td><td>1.61</td><td>1.15</td><td>0.59</td><td>0.46</td><td>1.16</td><td>1.93</td></tr><tr><td><br />CNN+BP</td><td>0.60</td><td><b>0.30</b></td><td><b>0.41</b></td><td><b>0.36</b></td><td><b>0.37</b></td><td><b>0.36</b></td><td><b>0.35</b></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>1</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>0.43</b></td><td>0.74</td><td><b>0.42</b></td></tr><tr><td><br />CNN+AdaBP</td><td><b>0.35</b></td><td><b>0.35</b></td><td><b>0.38</b></td><td>0.39</td><td><b>0.36</b></td><td>0.38</td><td><b>0.37</b></td><td><b>0.33</b></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><b>0.52</b></td><td><b>0.37</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The top 3 results are bold, and the best values are underlined.</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表4 不同噪声水平下不同算法在50幅BSD图像上预测结果的均方根误差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 RMSE Between Estimated Results and Ground Truths on Fifty Images from BSD Database</b></p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="10"><br />Noise Levels</td><td rowspan="2">Average<br />Value</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />Immerkær</td><td>2.81</td><td>2.05</td><td>1.65</td><td>1.42</td><td>1.28</td><td>1.17</td><td>1.18</td><td>1.09</td><td>1.11</td><td>1.19</td><td>1.50</td></tr><tr><td><br />Zoran</td><td>1.54</td><td>1.49</td><td>1.52</td><td>1.57</td><td>1.59</td><td>1.55</td><td>1.47</td><td>1.62</td><td>1.63</td><td>1.65</td><td>1.56</td></tr><tr><td><br />Ref [25]</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>1</mn><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>2</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>5</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>2</mn><mn>7</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>8</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>4</mn><mn>6</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>5</mn><mn>4</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder accentunder="true"><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mn>1</mn></mrow><mo stretchy="true">¯</mo></munder></mrow></math></td></tr><tr><td><br />Ref [11]</td><td>4.42</td><td>3.32</td><td>2.54</td><td>1.89</td><td>1.56</td><td>1.49</td><td>1.24</td><td>1.27</td><td>1.09</td><td>1.07</td><td>1.99</td></tr><tr><td><br />Ref [10]</td><td>1.98</td><td>1.18</td><td>0.92</td><td><b>0.70</b></td><td><b>0.73</b></td><td>1.29</td><td>0.91</td><td>1.02</td><td><b>0.74</b></td><td>1.01</td><td>1.05</td></tr><tr><td><br />Ref [1]</td><td>1.39</td><td>1.46</td><td>1.20</td><td>1.09</td><td>2.67</td><td>1.57</td><td>1.20</td><td>1.11</td><td>1.12</td><td>1.10</td><td>1.39</td></tr><tr><td><br />CNN+SVR</td><td>2.38</td><td>2.38</td><td>2.03</td><td>1.64</td><td>1.36</td><td>1.12</td><td>0.95</td><td>1.24</td><td>2.11</td><td>3.69</td><td>1.89</td></tr><tr><td><br />CNN+AdaSVR</td><td>3.22</td><td>3.24</td><td>2.94</td><td>2.55</td><td>2.19</td><td>1.82</td><td>1.35</td><td>0.97</td><td>0.81</td><td>1.22</td><td>2.03</td></tr><tr><td><br />CNN+BP</td><td><b>0.52</b></td><td><b>0.56</b></td><td><b>0.71</b></td><td>0.78</td><td>0.81</td><td><b>0.75</b></td><td><b>0.78</b></td><td><b>0.78</b></td><td>0.83</td><td><b>0.74</b></td><td><b>0.73</b></td></tr><tr><td><br />CNN+AdaBP</td><td><b>0.45</b></td><td><b>0.57</b></td><td><b>0.70</b></td><td><b>0.75</b></td><td><b>0.77</b></td><td><b>0.79</b></td><td><b>0.74</b></td><td><b>0.77</b></td><td><b>0.73</b></td><td><b>0.62</b></td><td><b>0.69</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The top 3 results are bold, and the best values are underlined.</p>
                </div>
                <div class="p1">
                    <p id="156">为了验证算法对测试图像的鲁棒性, 随机选取了BSD数据库50幅图像作为测试集, 分别给图像加入级别为5～50、间隔为5的高斯噪声, 并计算各个算法预测结果与真实值之间的均方根误差, 列在表4中 (排名标记方法与上文相同) .由表4中可知:各个算法的预测准确性在难度很大的纹理图像上有所下降, 但是排名情况总体与表3类似, 所提出的CNN+AdaBP算法仍然具有最好的性能.</p>
                </div>
                <div class="p1">
                    <p id="157">为了验证CNN+AdaBP算法的泛化能力, 对BSD数据库中的50幅图像添加2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5这10个在训练模型时未用到的噪声水平值, 并计算对应的均方根误差, 结果如表5所示.</p>
                </div>
                <div class="p1">
                    <p id="158">从表5可以看出:所提出的CNN+AdaBP算法能够很好地预测噪声级别在0～50之间的任意噪声水平值, 且算法的预测准确性足够稳定.综上所述, CNN+AdaBP算法具有良好且稳定的预测准确性, 其原因在于该算法是根据自然图像统计规律在大量自然图像上训练学习获得的.</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表5 本文算法在50幅BSD图像上预测结果的均方根误差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 RMSE of the Estimated Results at Different Noise Levels</b></p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td><br />Noise Levels</td><td>RMSE</td></tr><tr><td><br />2.5</td><td>0.49</td></tr><tr><td><br />7.5</td><td>0.53</td></tr><tr><td><br />12.5</td><td>0.67</td></tr><tr><td><br />17.5</td><td>0.78</td></tr><tr><td><br />22.5</td><td>0.83</td></tr><tr><td><br />27.5</td><td>0.91</td></tr><tr><td><br />32.5</td><td>0.78</td></tr><tr><td><br />37.5</td><td>0.79</td></tr><tr><td><br />42.5</td><td>0.77</td></tr><tr><td><br />47.5</td><td>0.80</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="160" name="160"><b>3.3 BM3D算法降噪</b></h4>
                <div class="p1">
                    <p id="161">为了进一步验证CNN+AdaBP算法的实际应用效果, 用经典BM3D降噪算法对分别加入不同噪声级别的Lena图像进行降噪, 表6列出了分别使用真实噪声水平值和根据CNN+AdaBP算法预测出的噪声值作为BM3D算法的输入参数进行降噪后的图像的峰值信噪比 (peak signal to noise ratio, PSNR) .</p>
                </div>
                <div class="area_img" id="162">
                    <p class="img_tit"><b>表6 BM3D分别使用真实噪声值和预测噪声值的降噪结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 PSNR of Image Denoised by BM3D Algorithm Using Ground Truth and Estimated Noise Levels</b></p>
                    <p class="img_note"></p>
                    <table id="162" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="10"><br />Noise Levels</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />True Noise Level</td><td>39.92</td><td>36.56</td><td>34.66</td><td>33.33</td><td>32.37</td><td>31.45</td><td>30.68</td><td>30.02</td><td>29.55</td><td>29.11</td></tr><tr><td><br />Estimated Noise Level</td><td>39.87</td><td>36.57</td><td>34.65</td><td>33.32</td><td>32.36</td><td>31.44</td><td>30.70</td><td>30.17</td><td>29.56</td><td>29.10</td></tr><tr><td><br />Absolute Deviation</td><td>0.05</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.02</td><td>0.15</td><td>0.01</td><td>0.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="163">从表6中的数据可以看出, 使用CNN+AdaBP算法预测出的噪声水平值作为参数的降噪效果与使用真实噪声水平值作为参数的降噪效果差别很小, 这充分说明了CNN+AdaBP算法的实用性和有效性.</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164"><b>3.4 执行时间的对比</b></h4>
                <div class="p1">
                    <p id="165">对于一个图像噪声水平估计算法来说, 不应仅仅根据算法的预测准确性来评判该算法性能是否良好, 执行效率也是非常重要的评价指标.为了充分证明所提出的算法性能良好, 笔者对各个算法的执行时间进行了比较.将各算法在1幅大小为512×512的Lena图像上重复执行20次, 计算算法估计该图像噪声水平值的平均执行时间, 各算法在不同噪声级别下平均执行时间如表7所示.从表7可以看出:Zoran算法和Liu算法的执行时间非常长, 不适合应用于时间要求比较严格的系统中.相比而言, Chen算法和Rakhshanfar算法的执行效率一般.由表2～4中的数据可知, 这2个算法的预测精度也一般.Immerkær算法和Xu算法的执行时间非常快, 但是从表3～4中的数据可知, 这2种算法的评价准确性比较其他算法要差一些.CNN+SVR算法、CNN+AdaSVR算法、CNN+BP算法和CNN+AdaBP算法作为基于训练的MNLE算法, 模型一旦训练完成, 执行速度处于所有参与比较算法中的前列.综上所述, 所提出的CNN+AdaBP算法在预测准确率和执行效率这2个方面的综合性能较其他算法更优.</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表7 各算法在不同噪声级别下的平均执行时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 7 Average Execution Time of Different NLE Methods at Different Noise Levels</b></p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="10"><br />Noise Levels</td></tr><tr><td><br />5</td><td>10</td><td>15</td><td>20</td><td>25</td><td>30</td><td>35</td><td>40</td><td>45</td><td>50</td></tr><tr><td><br />Immerkær</td><td>2.9</td><td>3.0</td><td>2.8</td><td>2.8</td><td>2.9</td><td>2.8</td><td>2.8</td><td>2.8</td><td>2.9</td><td>3.0</td></tr><tr><td><br />Zoran</td><td>1 011.8</td><td>1 008.8</td><td>1 013.9</td><td>992.8</td><td>986.4</td><td>999.5</td><td>1 030.0</td><td>1 019.1</td><td>995.7</td><td>987.0</td></tr><tr><td><br />Ref [25]</td><td>607.9</td><td>621.5</td><td>632.4</td><td>637.8</td><td>644.3</td><td>647.3</td><td>656.8</td><td>660.4</td><td>649.4</td><td>651.9</td></tr><tr><td><br />Ref [11]</td><td>162.9</td><td>166.0</td><td>165.4</td><td>165.5</td><td>165.1</td><td>164.9</td><td>165.8</td><td>165.7</td><td>165.6</td><td>166.1</td></tr><tr><td><br />Ref [10]</td><td>17.0</td><td>20.5</td><td>22.8</td><td>24.9</td><td>27.0</td><td>28.0</td><td>28.5</td><td>30.2</td><td>30.8</td><td>31.4</td></tr><tr><td><br />Ref [1]</td><td>15.0</td><td>14.9</td><td>15.1</td><td>15.1</td><td>15.2</td><td>14.8</td><td>14.8</td><td>15.0</td><td>15.2</td><td>15.2</td></tr><tr><td><br />CNN+SVR</td><td>17.7</td><td>17.1</td><td>17.1</td><td>17.0</td><td>17.0</td><td>16.9</td><td>17.0</td><td>17.0</td><td>17.1</td><td>17.1</td></tr><tr><td><br />CNN+AdaSVR</td><td>18.03</td><td>18.09</td><td>18.05</td><td>18.05</td><td>18.03</td><td>18.05</td><td>18.07</td><td>18.06</td><td>18.06</td><td>18.09</td></tr><tr><td><br />CNN+BP</td><td>19.87</td><td>18.90</td><td>17.95</td><td>18.82</td><td>18.60</td><td>17.01</td><td>17.45</td><td>18.66</td><td>18.60</td><td>18.77</td></tr><tr><td><br />CNN+AdaBP</td><td>19.4</td><td>20.0</td><td>20.3</td><td>21.5</td><td>22.7</td><td>21.7</td><td>21.8</td><td>19.9</td><td>20.2</td><td>21.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="167" name="167" class="anchor-tag"><b>4 总 结</b></h3>
                <div class="p1">
                    <p id="168">传统基于单幅图像的SNLE算法中将图像内容与噪声信号分离的关键模块在算法设计上比较复杂, 导致执行效率比较低.为此, 提出了一种利用CNN在多幅图像上通过训练学习实现快速分离噪声信号获得噪声映射图的新方法.然后, 利用广义高斯分布GGD模型为噪声映射图建模, 并以模型参数值作为反映噪声严重程度的NLAF特征值.最后, 训练改进后的BP网络模型实现从NLAF特征值到噪声水平值的预测任务.与现有的算法相比, CNN+AdaBP算法从提取高质量的NLAF特征和构建非线性映射能力强大的映射模型2个方面入手, 在保持高效率的前提下, 进一步提高了MNLE这类算法的预测准确率, 获得了一种预测准确性和执行效率的综合性能更好的噪声水平评估算法.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="256">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201803010&amp;v=MDYzOTdCdEdGckNVUkxPZVplUnFGeW5sVUwvS0x6N0JhTEc0SDluTXJJOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Xu Shaoping, Zeng Xiaoxia, Tang Yiling.Fast noise level estimation algorithm based on two-stage support vector regression[J].Journal of Computer-Aided Design &amp; Computer Graphics, 2018, 30 (3) :447- 458 (in Chinese) (徐少平, 曾小霞, 唐祎玲.基于两阶段支持向量回归的快速噪声水平估计算法[J].计算机辅助设计与图形学学报, 2018, 30 (3) :447- 458) 
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES75A3051BE0EF8F1A0F69D7FA08C36E74&amp;v=MDIxMTJ3S289TmlmT2ZiUzliOUxNcW80M0VldDZlblJQem1jVG5EbDBQSGlVM1JJOUNyR1NNTDJiQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhoeGJpOA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Wang Lingfeng, Huang Zehao, Gong Yongchao, et al.Ensemble based deep networks for image super-resolution[J].Pattern Recognition, 2017, 68 (8) :191- 198
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201708007&amp;v=MDY5NDllUnFGeW5sVUwvS0x6N0JhTEc0SDliTXA0OUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Ren Jingjing, Fang Xianyong, Chen Shangwen, et al.Image deblurring based on fast convolutional neural networks[J].Journal of Computer-Aided Design &amp; Computer Graphics, 2017, 29 (8) :1444- 1456 (in Chinese) (任静静, 方贤勇, 陈尚文, 等.基于快速卷积神经网络的图像去模糊[J].计算机辅助设计与图形学学报, 2017, 29 (8) :1444- 1456) 
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering">

                                <b>[4]</b>Dabov K, Foi A, Katkovnik V, et al.Image denoising by sparse 3D transform-domain collaborative filtering[J].IEEE Transactions on Image Processing, 2007, 16 (8) :2080- 2095
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201710024&amp;v=Mjg4MjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeW5sVUwvS0x5dlNkTEc0SDliTnI0OUhZSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Huang Dongmei, Dai Liang, Wei Lifei, et al.A secure outsourced fusion denoising scheme in multiple encrypted remote sensing images[J].Journal of Computer Research and Development, 2017, 54 (10) :2378- 2389 (in Chinese) (黄冬梅, 戴亮, 魏立斐, 等.一种安全的多帧遥感图像的外包融合去噪方案[J].计算机研究与发展, 2017, 54 (10) :2378- 2389) 
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">

                                <b>[6]</b>Dong Weisheng, Zhang Lei, Shi Guangming, et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing, 2013, 22 (4) :1620- 1630
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084580&amp;v=MzAwMTRNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZJSVY0WGFoQT1OaWZPZmJLN0h0RE5xbzlFWk9NTENYUTVvQg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>Immerkær J.Fast noise variance estimation[J].Computer Vision and Image Understanding, 1996, 64 (2) :300- 302
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale invariance and noise in natural images">

                                <b>[8]</b>Zoran D, Weiss Y.Scale invariance and noise in natural images[C] //Proc of the 12th IEEE Int Conf on Computer Vision (ICCV 2009) .Piscataway, NJ:IEEE, 2009:2209- 2216
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image noise level estimation by searching for smooth patches with discrete cosine transform">

                                <b>[9]</b>Katase H, Yamaguchi T, Fujisawa T, et al.Image noise level estimation by searching for smooth patches with discrete cosine transform[C] //Proc of the 59th IEEE Int Midwest Symp on Circuits and Systems.Piscataway, NJ:IEEE, 2016.DOI:10.1109/MWSCAS.2016.7870110
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Estimation of Gaussian,Poissonian-Gaussian,and processed visual noise and its level function">

                                <b>[10]</b>Rakhshanfar M, Amer M A.Estimation of Gaussian, Poissonian-Gaussian, and processed visual noise and its level function[J].IEEE Transactions on Image Processing, 2016, 25 (9) :4172- 4185
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An efficient statistical method for image noise level estimation">

                                <b>[11]</b>Chen Guangyong, Zhu Fengyuan, Heng P A.An efficient statistical method for image noise level estimation[C] //Proc of the 18th IEEE Int Conf on Computer Vision (ICCV 2015) .Piscataway, NJ:IEEE, 2015:477- 485
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEGB2013CE9FF9C89BD39899797291CC77F&amp;v=MDI1NThmQ3BiUTM1TnhoeGJpOHdLbz1OaWZPYWNHNkh0RFAzUHBNRXAwR2YzUXd2V0lRNHpkMFFYanJxeEE4ZU1IblFyM3BDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Xu Shaoping, Hu Lingyan, Yang Xiaohui.Quality-aware features-based noise level estimator for block matching and three-dimensional filtering algorithm[J].Journal of Electronic Imaging, 2016, 25 (1) :013029
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multiple imagebased noise level estimation algorithm">

                                <b>[13]</b>Xu Shaoping, Zeng Xiaoxia, Jiang Yinnan, et al.A multiple image-based noise level estimation algorithm[J].IEEE Signal Processing Letters, 2017, 24 (11) :1701- 1705
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C] //Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2014) .Piscataway, NJ:IEEE, 2014:580- 587
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Web-scale training for face identification">

                                <b>[15]</b>Taigman Y, Yang Ming, Ranzato M, et al.Web-scale training for face identification[C] //Proc of the 18th IEEE Conf on Computer Vision and Pattern Recognition (CVPR 2015) .Piscataway, NJ:IEEE, 2015:2746- 2754
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[16]</b>Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640- 651
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201609002&amp;v=MDM3NDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnlubFVML0tLQ0xmWWJHNEg5Zk1wbzlGWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Chang Liang, Deng Xiaoming, Zhou Mingquan, et al.Convolutional neural networks in image understanding[J].Acta Automatica Sinica, 2016, 42 (9) :1300- 1312 (in Chinese) (常亮, 邓小明, 周明全, 等.图像理解中的卷积神经网络[J].自动化学报, 2016, 42 (9) :1300- 1312) 
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MzA1MTVxcUJ0R0ZyQ1VSTE9lWmVScUZ5bmxVTC9LTHo3QmRyRzRIOWJNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Zhou Feiyan, Jin Linpeng, Dong Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229- 1251 (in Chinese) (周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229- 1251) 
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rectified linear units improve restricted Boltzmann machines">

                                <b>[19]</b>Nair V, Hinton G E.Rectified linear units improve restricted Boltzmann machines[C] //Proc of the 27th Int Conf on Int Conf on Machine Learning.New York:ACM, 2010:807- 814
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK95C29D0CF3B7704D03904EAC08C95C8A&amp;v=MjM2NzI2VFo5VEFxVDN4STlDcnVSTnJMdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHhiaTh3S289TmlmSlpicTliZFBGMjQ4MkV1aDlDM3M1eTJJVA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>Rawat W, Wang Zenghui.Deep convolutional neural networks for image classification:A comprehensive review[J].Neural Computation, 2017, 29 (9) :2352- 2449
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES207B769A00307A2E5CBB3276FBC33EA0&amp;v=MDIxNjd2RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGh4Ymk4d0tvPU5pZk9mYkc0R2FQTHFZWTBaT3NNREh0SXpXTVdtVTBQUzMzbHFtUkhDckdYTU11ZkNPTg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Li Yanghao, Wang Naiyan, Shi Jianping, et al.Adaptive batch normalization for practical domain adaptation[J].Pattern Recognition, 2016, 80:109- 117
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Theory for Multiresolution Signal Decomposition: The Wavelet Representation">

                                <b>[22]</b>Mallat S G.A theory for multiresolution signal decomposition:The wavelet representation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1989, 11 (7) :674- 693
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A robust multi-class Adaboost algorithm for mislabeled noisy data">

                                <b>[23]</b>Sun Bo, Chen Songcan, Wang Jiandong, et al.A robust multi-class AdaBoost algorithm for mislabeled noisy data[J].Knowledge-Based Systems, 2016, 102 (5) :87- 102
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contour Detection and Hierarchical Image Segmentation">

                                <b>[24]</b>Arbelaez P, Maire M, Fowlkes C, et al.Contour detection and hierarchical image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898- 916
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image noise level estimation for blind denoising">

                                <b>[25]</b>Liu Xinhao, Tanaka M, Okutomi M.Single-image noise level estimation for blind denoising[J].IEEE Transactions on Image Processing, 2013, 22 (12) :5226- 5237
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201905016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905016&amp;v=MTQ4NDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnlubFVML0tMeXZTZExHNEg5ak1xbzlFWW9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dDdkNwbFRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

