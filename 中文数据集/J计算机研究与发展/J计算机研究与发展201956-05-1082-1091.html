

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128655751681250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201905018%26RESULT%3d1%26SIGN%3d%252f2h%252bXEm6fr4rIQm1w3udjaIOw7E%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905018&amp;v=MDU4OTZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeW5sVUx2TUx5dlNkTEc0SDlqTXFvOUU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#86" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="&lt;b&gt;1.1 基于全差异空间的语种识别系统&lt;/b&gt;"><b>1.1 基于全差异空间的语种识别系统</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;1.2 基于端对端神经网络的语种识别系统&lt;/b&gt;"><b>1.2 基于端对端神经网络的语种识别系统</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="&lt;b&gt;2 语种特征修复&lt;/b&gt; "><b>2 语种特征修复</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="&lt;b&gt;2.1 音素向量&lt;/b&gt;"><b>2.1 音素向量</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;2.2 基于降噪自动编码器的i-vector补偿方法&lt;/b&gt;"><b>2.2 基于降噪自动编码器的i-vector补偿方法</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;2.3 融合策略&lt;/b&gt;"><b>2.3 融合策略</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="&lt;b&gt;3 语种特征补偿实验&lt;/b&gt; "><b>3 语种特征补偿实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#135" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;3.2 实验结果&lt;/b&gt;"><b>3.2 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#158" data-title="&lt;b&gt;4 总 结&lt;/b&gt; "><b>4 总 结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="图1 GMM i-vector语种识别系统框图">图1 GMM i-vector语种识别系统框图</a></li>
                                                <li><a href="#106" data-title="图2 CNN-TAP端到端语种识别系统框图">图2 CNN-TAP端到端语种识别系统框图</a></li>
                                                <li><a href="#110" data-title="图3 基于DAE的i-vector补偿结构框图">图3 基于DAE的i-vector补偿结构框图</a></li>
                                                <li><a href="#121" data-title="图4 语种特征补偿方法图示">图4 语种特征补偿方法图示</a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表1 音素向量补偿性能对比&lt;/b&gt;"><b>表1 音素向量补偿性能对比</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表2 基线系统和补偿系统性能对比&lt;/b&gt;"><b>表2 基线系统和补偿系统性能对比</b></a></li>
                                                <li><a href="#153" data-title="图5 &lt;i&gt;C&lt;/i&gt;_&lt;i&gt;avg&lt;/i&gt;随得分融合系数变化图">图5 <i>C</i>_<i>avg</i>随得分融合系数变化图</a></li>
                                                <li><a href="#156" data-title="图6 分类混淆矩阵">图6 分类混淆矩阵</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="279">


                                    <a id="bibliography_1" title="Li Haizhou, Ma Bin, Lee K.Spoken language recognition:From fundamentals to practice[J].Proceedings of the IEEE, 2013, 101 (5) :1136-1159" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Spoken language recognition:from fundamentals to practice.&amp;quot;">
                                        <b>[1]</b>
                                        Li Haizhou, Ma Bin, Lee K.Spoken language recognition:From fundamentals to practice[J].Proceedings of the IEEE, 2013, 101 (5) :1136-1159
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_2" title="Zissman M A.Comparison of four approaches to automatic language identification of telephone speech[J].IEEETransactions on Speech and Audio Processing, 1996, 4 (1) :31" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparison of four approaches to automatic language identification of telephone speech">
                                        <b>[2]</b>
                                        Zissman M A.Comparison of four approaches to automatic language identification of telephone speech[J].IEEETransactions on Speech and Audio Processing, 1996, 4 (1) :31
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_3" title="Yan Yonghong, Barnard E.An approach to automatic language identification based on language-dependent phone recognition[C]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 1995:3511-3514" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An approach to automatic language identification based on language-dependent phone recognition">
                                        <b>[3]</b>
                                        Yan Yonghong, Barnard E.An approach to automatic language identification based on language-dependent phone recognition[C]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 1995:3511-3514
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_4" title="Yan Yonghong, Barnard E, Cole R A.Development of an approach to automatic language identification based on phone recognition[J].Computer Speech&amp;amp;Language, 1996, 10 (1) :37-54" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100176425&amp;v=MzE3ODQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SUlWNFhiaFk9TmlmT2ZiSzdIdERPcm85Rlpld0pDSDQ4b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Yan Yonghong, Barnard E, Cole R A.Development of an approach to automatic language identification based on phone recognition[J].Computer Speech&amp;amp;Language, 1996, 10 (1) :37-54
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_5" title="Li Haizhou, Ma Bin, Lee C.A vector space modeling approach to spoken language identification[J].IEEETransactions on Audio, Speech&amp;amp;Language Processing, 2007, 15 (1) :271-284" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Vector space modeling approach to spoken language identification">
                                        <b>[5]</b>
                                        Li Haizhou, Ma Bin, Lee C.A vector space modeling approach to spoken language identification[J].IEEETransactions on Audio, Speech&amp;amp;Language Processing, 2007, 15 (1) :271-284
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_6" title="Davis S, Mermelstein P.Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences[J].IEEE Transactions on Acoustics, Speech and Signal Processing, 1980, 28 (4) :65-74" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences">
                                        <b>[6]</b>
                                        Davis S, Mermelstein P.Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences[J].IEEE Transactions on Acoustics, Speech and Signal Processing, 1980, 28 (4) :65-74
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_7" title="Hermansk H.Perceptual linear predictive (PLP) analysis of speech[J].The Journal of the Acoustical Society of America, 1990, 87 (4) :1738-1752" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual linear predictive (PLP) analysis of speech">
                                        <b>[7]</b>
                                        Hermansk H.Perceptual linear predictive (PLP) analysis of speech[J].The Journal of the Acoustical Society of America, 1990, 87 (4) :1738-1752
                                    </a>
                                </li>
                                <li id="293">


                                    <a id="bibliography_8" title="Torres-Carrasquillo P A, Singer E, Kohler M A, et al.Approaches to language identification using Gaussian mixture models and shifted delta cepstral features[C]Proc of the7th Int Conf on Spoken Language Processing.Piscataway, NJ:IEEE, 2002:89-92" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Approachesto Language Identification Using Gaussian Mixture Models andShifted Delta Cepstral Features">
                                        <b>[8]</b>
                                        Torres-Carrasquillo P A, Singer E, Kohler M A, et al.Approaches to language identification using Gaussian mixture models and shifted delta cepstral features[C]Proc of the7th Int Conf on Spoken Language Processing.Piscataway, NJ:IEEE, 2002:89-92
                                    </a>
                                </li>
                                <li id="295">


                                    <a id="bibliography_9" title="Reynolds D A, Quatieri T F, Dunn R B.Speaker verification using adapted Gaussian mixture models[J].Digital Signal Processing, 2000, 10 (1?2?3) :19-41" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501235421&amp;v=MjYyMjNRVE1ud1plWnVIeWptVUx2SUlWNFhiaFk9TmlmT2ZiSzdIdEROcW85RVp1Z0tDSDQ0b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Reynolds D A, Quatieri T F, Dunn R B.Speaker verification using adapted Gaussian mixture models[J].Digital Signal Processing, 2000, 10 (1?2?3) :19-41
                                    </a>
                                </li>
                                <li id="297">


                                    <a id="bibliography_10" title="Campbell W M, Sturim D E, Reynolds D A.Support vector machines using GMM supervectors for speakers verification[J].IEEE Signal Processing Letters, 2006, 13 (5) :308-311" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector machines using GMM supervectors for speaker verification">
                                        <b>[10]</b>
                                        Campbell W M, Sturim D E, Reynolds D A.Support vector machines using GMM supervectors for speakers verification[J].IEEE Signal Processing Letters, 2006, 13 (5) :308-311
                                    </a>
                                </li>
                                <li id="299">


                                    <a id="bibliography_11" title="Dehak N, Torres-Carrasquillo P A, Reynolds D A, et al.Language recognition via i-vectors and dimensionality reduction[C]Proc of the 12th Annual Conf of the Int Speech Communication Association.Baixas, Florence:International Speech and Communication Association, 2011:857-860" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Language recognition via i-vectors and dimensionality reduction">
                                        <b>[11]</b>
                                        Dehak N, Torres-Carrasquillo P A, Reynolds D A, et al.Language recognition via i-vectors and dimensionality reduction[C]Proc of the 12th Annual Conf of the Int Speech Communication Association.Baixas, Florence:International Speech and Communication Association, 2011:857-860
                                    </a>
                                </li>
                                <li id="301">


                                    <a id="bibliography_12" title="Hinton G, Salakhutdinov R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reducing the dimensionality of data with neural networks">
                                        <b>[12]</b>
                                        Hinton G, Salakhutdinov R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507
                                    </a>
                                </li>
                                <li id="303">


                                    <a id="bibliography_13" title="Jiang Bing, Song Yan, Wei Si, et al.Performance evaluation of deep bottleneck features for spoken language identification[C]Proc of the 9th Int Symp on Chinese Spoken Language Processing.Piscataway, NJ:IEEE, 2014:143-147" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance evaluation of deep bottleneck features for spoken language identification">
                                        <b>[13]</b>
                                        Jiang Bing, Song Yan, Wei Si, et al.Performance evaluation of deep bottleneck features for spoken language identification[C]Proc of the 9th Int Symp on Chinese Spoken Language Processing.Piscataway, NJ:IEEE, 2014:143-147
                                    </a>
                                </li>
                                <li id="305">


                                    <a id="bibliography_14" title="Lei Yun, Scheffer N, Ferrer L, et al.A novel scheme for speaker recognition using aphonetically-aware deep neural network[C]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2014:1695-1699" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel scheme for speaker recognition using a phonetically-aware deep neural network">
                                        <b>[14]</b>
                                        Lei Yun, Scheffer N, Ferrer L, et al.A novel scheme for speaker recognition using aphonetically-aware deep neural network[C]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2014:1695-1699
                                    </a>
                                </li>
                                <li id="307">


                                    <a id="bibliography_15" title="Lopez-Moreno I, Gonzalez-Dominguez J, Plchot O, et al.Automatic language identification using deep neural networks[C]Proc of the 39th IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2014:5337-5341" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic language identification using deep neural networks">
                                        <b>[15]</b>
                                        Lopez-Moreno I, Gonzalez-Dominguez J, Plchot O, et al.Automatic language identification using deep neural networks[C]Proc of the 39th IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2014:5337-5341
                                    </a>
                                </li>
                                <li id="309">


                                    <a id="bibliography_16" title="Garcia-Romero D, McCree A.Stacked long-term TDNN for spoken language recognition[C]Proc of the 17th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2016:3226-3230" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked long-term TDNN for spoken language recognition">
                                        <b>[16]</b>
                                        Garcia-Romero D, McCree A.Stacked long-term TDNN for spoken language recognition[C]Proc of the 17th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2016:3226-3230
                                    </a>
                                </li>
                                <li id="311">


                                    <a id="bibliography_17" title="Gonzalez-Dominguez J, Lopez-Moreno I, Sak H, et al.Automatic language identification using long short-term memory recurrent neural networks[C]Proc of the 15th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2014:2155-2159" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic language identification using long short-term memory recurrent neural networks">
                                        <b>[17]</b>
                                        Gonzalez-Dominguez J, Lopez-Moreno I, Sak H, et al.Automatic language identification using long short-term memory recurrent neural networks[C]Proc of the 15th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2014:2155-2159
                                    </a>
                                </li>
                                <li id="313">


                                    <a id="bibliography_18" title="Geng Wang, Wang Wenfu, Zhao Yuanyuan, et al.End-toend language identification using attention-based recurrent neural networks[C]Proc of the 17th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2016:2944-2948" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end language identification using attention-based recurrent neural networks">
                                        <b>[18]</b>
                                        Geng Wang, Wang Wenfu, Zhao Yuanyuan, et al.End-toend language identification using attention-based recurrent neural networks[C]Proc of the 17th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2016:2944-2948
                                    </a>
                                </li>
                                <li id="315">


                                    <a id="bibliography_19" title="Jin Ma, Song Yan, McLoughlin I.LID-senones and their statistics for language identification[J].IEEE?ACMTransactions on Audio, Speech and Language Processing, 2018, 26 (1) :171-183" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2DA3BCEEB7D5B8EE7AD3B89C4AB43090&amp;v=MTgwMDRhQnVIWWZPR1FsZkNwYlEzNU54aHhiaTh4S3c9TmlmSVk3SE1iOUsrM1Bvd0Z1eDdDUTR4dW1NVW0wdCtPbmZyM3haRUM3YVhSYk9mQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        Jin Ma, Song Yan, McLoughlin I.LID-senones and their statistics for language identification[J].IEEE?ACMTransactions on Audio, Speech and Language Processing, 2018, 26 (1) :171-183
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_20" title="Cai Weicheng, Cai Zexin, Liu Wenbo, et al.Insights into end-to-end learning scheme for language identification[C?OL]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2018[2018-05-20].https:scholar.google.co.uk?scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Insights+into+end-to-end+learning+scheme+for+language+identification&amp;amp;btnG" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Insights into end-to-end learning scheme for language identification[C?OL]">
                                        <b>[20]</b>
                                        Cai Weicheng, Cai Zexin, Liu Wenbo, et al.Insights into end-to-end learning scheme for language identification[C?OL]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2018[2018-05-20].https:scholar.google.co.uk?scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Insights+into+end-to-end+learning+scheme+for+language+identification&amp;amp;btnG
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_21" title="Cai Weicheng, Cai Zexin, Zhang Xiang, et al.A novel learnable dictionary encoding layer for end-to-end language identification[C?OL]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2018[2018-05-20].https:scholar.google.co.uk?scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+novel+learnable+dictionary+encoding+layer+for+end-to-end+language+identification+&amp;amp;btnG" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel learnable dictionary encoding layer for end-to-end language identification[C?OL]">
                                        <b>[21]</b>
                                        Cai Weicheng, Cai Zexin, Zhang Xiang, et al.A novel learnable dictionary encoding layer for end-to-end language identification[C?OL]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2018[2018-05-20].https:scholar.google.co.uk?scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=A+novel+learnable+dictionary+encoding+layer+for+end-to-end+language+identification+&amp;amp;btnG
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_22" title="Cai Weicheng, Chen Jinkun, Li Ming.Exploring the encoding layer and loss function in end-to-end speaker and language recognition system[C?OL]Proc of Speaker Odyssey.2018[2018-06-01].https:scholar.google.co.uk?scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Exploring+the+Encoding+Layer+and+Loss+Function+in+End-toEnd+Speaker+and+Language+Recognition+System+&amp;amp;btnG" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring the encoding layer and loss function in end-to-end speaker and language recognition system[C?OL]">
                                        <b>[22]</b>
                                        Cai Weicheng, Chen Jinkun, Li Ming.Exploring the encoding layer and loss function in end-to-end speaker and language recognition system[C?OL]Proc of Speaker Odyssey.2018[2018-06-01].https:scholar.google.co.uk?scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=Exploring+the+Encoding+Layer+and+Loss+Function+in+End-toEnd+Speaker+and+Language+Recognition+System+&amp;amp;btnG
                                    </a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_23" title="Vincent P, Larochelle H, Bengio Y, et al.Extracting and composing robust features with denoising autoencoders[C]Proc of the 25th Int Conf on Machine learning.New York:ACM, 2008:1096-1103" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting and composing robust features with denoising autoencoders">
                                        <b>[23]</b>
                                        Vincent P, Larochelle H, Bengio Y, et al.Extracting and composing robust features with denoising autoencoders[C]Proc of the 25th Int Conf on Machine learning.New York:ACM, 2008:1096-1103
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_24" title="Lu Xugang, Tao Y, Matsuda S, et al.Speech enhancement based on deep denoising autoencoder[C]Proc of the 14th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2013:436-440" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech enhancement based on deep denoising autoencoder">
                                        <b>[24]</b>
                                        Lu Xugang, Tao Y, Matsuda S, et al.Speech enhancement based on deep denoising autoencoder[C]Proc of the 14th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2013:436-440
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_25" title="Ishii T, Komiyama H, Shinozaki T, et al.Reverberant speech recognition based on denoising autoencoder[C]Proc of the 14th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2013:3512-3516" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reverberant speech recognition based on denoising autoencoder">
                                        <b>[25]</b>
                                        Ishii T, Komiyama H, Shinozaki T, et al.Reverberant speech recognition based on denoising autoencoder[C]Proc of the 14th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2013:3512-3516
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_26" title="Yamamoto H, Koshinaka T.Denoising autoencoder-based speaker feature restoration for utterances of short duration[C]Proc of the 16th Annual Conf of the Int Speech Communication Association.Dresden, Germany:International Speech and Communication Association, 2015:1052-1056" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Denoising autoencoder-based speaker feature restoration for utterances of short duration">
                                        <b>[26]</b>
                                        Yamamoto H, Koshinaka T.Denoising autoencoder-based speaker feature restoration for utterances of short duration[C]Proc of the 16th Annual Conf of the Int Speech Communication Association.Dresden, Germany:International Speech and Communication Association, 2015:1052-1056
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_27" title="Yang I, Heo H, Yoon S, et al.Applying compensation techniques on i-vectors extracted from short-test utterances for speaker verification using deep neural network[C]Proc of the 42nd IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:5490-5494" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Applying compensation techniques on i-vectors extracted from short-test utterances for speaker verification using deep neural network">
                                        <b>[27]</b>
                                        Yang I, Heo H, Yoon S, et al.Applying compensation techniques on i-vectors extracted from short-test utterances for speaker verification using deep neural network[C]Proc of the 42nd IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:5490-5494
                                    </a>
                                </li>
                                <li id="333">


                                    <a id="bibliography_28" title="Muthusamy Y K.A segmental approach to automatic language identification[D].Portland, Oregon, USA:Oregon Health&amp;amp;Science University, 1993" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A segmental approach to automatic language identification">
                                        <b>[28]</b>
                                        Muthusamy Y K.A segmental approach to automatic language identification[D].Portland, Oregon, USA:Oregon Health&amp;amp;Science University, 1993
                                    </a>
                                </li>
                                <li id="335">


                                    <a id="bibliography_29" title="Hasan T, Saeidi R, Hansen J H L, et al.Duration mismatch compensation for i-vector based speaker recognition systems[C]Proc of the 38th IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:7663-7667" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Duration mismatch compensation for i-vector based speaker recognition systems">
                                        <b>[29]</b>
                                        Hasan T, Saeidi R, Hansen J H L, et al.Duration mismatch compensation for i-vector based speaker recognition systems[C]Proc of the 38th IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:7663-7667
                                    </a>
                                </li>
                                <li id="337">


                                    <a id="bibliography_30" title="Dehak N, Kenny P, Dehak R, et al.Frontend factor analysis for speaker verification[J].IEEE Transactions on Audio, Speech&amp;amp;Language Processing, 2011, 19 (4) :788-798" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Front-End Factor Analysis for Speaker Verification">
                                        <b>[30]</b>
                                        Dehak N, Kenny P, Dehak R, et al.Frontend factor analysis for speaker verification[J].IEEE Transactions on Audio, Speech&amp;amp;Language Processing, 2011, 19 (4) :788-798
                                    </a>
                                </li>
                                <li id="339">


                                    <a id="bibliography_31" title="Dehak N, Dehak R, Kenny P, et al.Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification[C]Proc of the10th Annual Conf of the Int Speech Communication Association.Brighton, United Kingdom:International Speech and Communication Association, 2009:1559-1562" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification">
                                        <b>[31]</b>
                                        Dehak N, Dehak R, Kenny P, et al.Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification[C]Proc of the10th Annual Conf of the Int Speech Communication Association.Brighton, United Kingdom:International Speech and Communication Association, 2009:1559-1562
                                    </a>
                                </li>
                                <li id="341">


                                    <a id="bibliography_32" title="Dehak N, Torres-Carrasquillo P A, Reynolds D A, et al.Language recognition via i-vectors and dimensionality reduction[C]Proc of the 12th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2011:857-860" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Language Recognition via Ivectors and Dimensionality Reduction">
                                        <b>[32]</b>
                                        Dehak N, Torres-Carrasquillo P A, Reynolds D A, et al.Language recognition via i-vectors and dimensionality reduction[C]Proc of the 12th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2011:857-860
                                    </a>
                                </li>
                                <li id="343">


                                    <a id="bibliography_33" title="Yang Jinchao, Zhang Xiang, Suo Hongbin, et al.Language recognition with language total variability[C]Proc of the2011 Int Conf on Innovative Computing and Cloud Computing.New York:ACM, 2011:6-9" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Language recognition with language total variability">
                                        <b>[33]</b>
                                        Yang Jinchao, Zhang Xiang, Suo Hongbin, et al.Language recognition with language total variability[C]Proc of the2011 Int Conf on Innovative Computing and Cloud Computing.New York:ACM, 2011:6-9
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_34" title="Martin A F, Le A N.NIST 2007 language recognition evaluation[S/OL].Gaithersburg, Maryland:National Institute of Standards and Technology.[2017-09-20].https://catalog.ldc.upenn.edu/docs/LDC2009S04/LRE07EvalPlan-v8b-1.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NIST 2007 language recognition evaluation[S/OL]">
                                        <b>[34]</b>
                                        Martin A F, Le A N.NIST 2007 language recognition evaluation[S/OL].Gaithersburg, Maryland:National Institute of Standards and Technology.[2017-09-20].https://catalog.ldc.upenn.edu/docs/LDC2009S04/LRE07EvalPlan-v8b-1.pdf
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_35" title="CallFriend Corpus.Linguistic data consortium[S/OL].[2017-09-20].http://www.ldc.upenn/ldc/about/callfriend.html" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linguistic data consortium[S/OL]">
                                        <b>[35]</b>
                                        CallFriend Corpus.Linguistic data consortium[S/OL].[2017-09-20].http://www.ldc.upenn/ldc/about/callfriend.html
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_36" title="National Institute of Standards and Technology.The NIST Year 2008 Speaker Recognition Evaluation Plan[S/OL].Gaithersburg, Maryland:National Institute of Standards and Technology.[2017-10-20].https://www.nist.gov/sites/default/files/documents/2017/09/26/sre08_evalplan_release4.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The NIST Year 2008 Speaker Recognition Evaluation Plan[S/OL]">
                                        <b>[36]</b>
                                        National Institute of Standards and Technology.The NIST Year 2008 Speaker Recognition Evaluation Plan[S/OL].Gaithersburg, Maryland:National Institute of Standards and Technology.[2017-10-20].https://www.nist.gov/sites/default/files/documents/2017/09/26/sre08_evalplan_release4.pdf
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(05),1082-1091 DOI:10.7544/issn1000-1239.2019.20180471            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于降噪自动编码器的语种特征补偿方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8B%97%E6%99%93%E6%99%93&amp;code=38524029&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苗晓晓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%8F%8A&amp;code=32204123&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐及</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%89%91&amp;code=41873590&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王剑</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A3%B0%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80%E8%AF%AD%E8%A8%80%E5%A3%B0%E5%AD%A6%E4%B8%8E%E5%86%85%E5%AE%B9%E7%90%86%E8%A7%A3%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0177578&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院声学研究所语言声学与内容理解重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在语种识别中, 当训练语音与测试语音长度失配时, 系统的识别性能会出现严重下降.基于降噪自动编码器 (denoising auto-encoder, DAE) 的方法对不同长度测试语音的语种特征进行补偿, 把不同长度的语音特征都映射为固定长度的语音特征, 一定程度上解决了长度失配和音素分配不平衡的问题.具体分为4个环节:1) 语音信号经过分帧、变换得到底层声学特征;2) 提取语音信号的原始i-vector, 同时计算其音素向量;3) 对原始i-vector和音素向量进行拼接, 送入基于DAE的语种特征补偿处理单元得到补偿后的i-vector;4) 将补偿后的i-vector和原始i-vector分别送入后端分类器得到2个分数向量, 并将其在得分域融合后进行判决.在NIST-LRE07上的实验结果表明:所提出的语种特征补偿算法在各种测试语音时长上的识别性能均有提升.相比传统的语种识别系统, 测试语音时长为30 s时性能相对提升3.16%, 测试语音时长为10 s时性能相对提升2.90%.相比端到端语种识别系统, 测试语音时长为3 s时性能相对提升3.21%.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E7%A7%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语种识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=i-vector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">i-vector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9F%B3%E7%B4%A0%E5%90%91%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">音素向量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%A1%A5%E5%81%BF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征补偿;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%99%8D%E5%99%AA%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">降噪自动编码器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *徐及, xuji@hccl.ioa.ac.cn;
                                </span>
                                <span>
                                    苗晓晓, miaoxiaoxiao@hccl.ioa.ac.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2016YFB0801203, 2016YFB0801200);</span>
                    </p>
            </div>
                    <h1><b>Denoising Autoencoder-Based Language Feature Compensation</b></h1>
                    <h2>
                    <span>Miao Xiaoxiao</span>
                    <span>Xu Ji</span>
                    <span>Wang Jian</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Speech Acoustics and Content Understanding, Institute of Acoustics, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Language identification (LID) accuracy is often significantly reduced when the duration of the test data and the training data are mismatched. This paper proposes a method to compensate language features using a denoising autoencoder (DAE) . Use of denoising autoencoder-based language feature compensation can map language features from variable length utterances into a fixed length representation. Therefore the problem of length mismatch and unbalanced phoneme distribution can be mitigated. The algorithm first converts the speech signal to low level acoustic features by framing and transforming, and then estimates its i-vector and phonetic vector. These two vectors are then concatenated and fed into the DAE-based language feature compensation processing unit. The compensated i-vector from the output of the DAE, and the original i-vector, are presented to the back-end classifier to obtain two score vectors. These two score vectors are finally fused at a score level to obtain a final result. Tests on NIST-LRE07 demonstrate that this feature compensation method improves identification performance over various test speech durations. Compared with traditional LID systems, the performance for 30 s test utterances improves by 3.16%, while the performance for 10 s test utterances improves by 2.90%. Compared with the end-to-end LID system, the performance on 3 s test utterances is increased by 3.21%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=language%20identification%20(LID)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">language identification (LID) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=i-vector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">i-vector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=phoneme%20vector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">phoneme vector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20compensation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature compensation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=denoising%20autoencoder%20(DAE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">denoising autoencoder (DAE) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Miao Xiaoxiao, born in 1994. PhD candidate. Her main research interests include language identification, speaker recognition and deep learning.<image id="202" type="formula" href="images/JFYZ201905018_20200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Xu Ji, born in 1986. PhD, associate professor. His main research interests include speech recognition, deep learning and ocean acoustics.<image id="204" type="formula" href="images/JFYZ201905018_20400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wang Jian, born in 1977. Master, associate professor. His main research interests include speech signal processing.<image id="206" type="formula" href="images/JFYZ201905018_20600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-06-27</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2016YFB0801203, 2016YFB0801200);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="78">语种识别 (language identification, LID) 是指自动判定给定语音段, 从该语音信号中提取各语种的差异信息, 判断语言种类的过程<citation id="351" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.人类的听觉系统是世界上最准确的语种识别系统.经过短时间的训练学习, 人们能够快速准确地判定语种类别.即使是不熟悉的语种, 人们也能对其语种与所知道的语种做出一个粗略的判断.语种识别的目标是将这种能力赋予计算机使语种分类自动化.根据测试集语种类别, 可以将语种识别任务分成闭集语种识别和开集语种识别.闭集的训练集语种类别包含所有测试集语种类别, 而开集的训练集并没有包含所有测试集语种类别<citation id="352" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.近年来, 语种识别技术在这2个任务上已取得了长足的进步.然而仍有很多不足, 尤其是面对短时语音段语种识别、高混淆度的语言识别、大量集外语种任务时.本文主要针对闭集语种识别问题展开研究.</p>
                </div>
                <div class="p1">
                    <p id="79">传统的语种识别技术可分为基于音素层特征和基于声学层特征.基于音素层特征的语种识别技术是将音素层特征作为识别依据, 主要考虑了不同语种有着不同的音素集合, 音节和音素出现的频率有很大差异, 以及音节和音素的组合大不相同等因素.一般通过音素识别器, 先将语音信号解码为音素序列或者音素网格.在建模阶段, 通常为每个语种建立<i>N</i>元文法 (Ngram) 模型或者<i>N</i>元文法统计量模型<citation id="353" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.常用的方法有音素识别后接<i>N</i>元文法模型 (phoneme recognizer followed by language model, PRLM) <citation id="354" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、并行音素识别器后接语言模型 (parallel phone recognition followed by language modeling, PPRLM) <citation id="362" type="reference"><link href="283" rel="bibliography" /><link href="285" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>和并行音素识别器后接向量空间模型 (parallel phoneme recognizer followed by vector space model, PPRVSM) <citation id="355" type="reference"><link href="287" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等.基于声学层特征的语种识别技术依赖于声学层特征.通过对语音信号分帧、变换提取声学层特征, 采用概率统计或鉴别性方法对其建模.常用的声学层特征有美尔频率倒谱系数 (Mel-frequency cepstral coefficient, MFCC) <citation id="356" type="reference"><link href="289" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、感知线性预测系数 (perceptual linear predictive, PLP) <citation id="357" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、滑动差分倒谱 (shifted delta cepstrum, SDC) <citation id="358" type="reference"><link href="293" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等.主流系统有混合高斯模型-全局背景模型 (Gaussian mixture model-universal back-ground model, GMM-UBM) <citation id="359" type="reference"><link href="295" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、高斯超向量-支持向量机 (GMM super vector-support vector machines, GSV-SVM) <citation id="360" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和基于全差异空间的 (total variability, TV) i-vector系统<citation id="361" type="reference"><link href="299" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等.</p>
                </div>
                <div class="p1">
                    <p id="80">近几年, 深度神经网络 (deep neural networks, DNNs) <citation id="363" type="reference"><link href="301" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>模型在语种识别任务上得到快速发展.一方面从前端特征提取层面, 蒋兵等人<citation id="364" type="reference"><link href="303" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>利用DNN强大的特征抽取能力, 提取了深度瓶颈特征 (deep bottleneck feature, DBF) ;另一方面从模型域出发, Lei等人<citation id="365" type="reference"><link href="305" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了基于DNN的TV建模策略.</p>
                </div>
                <div class="p1">
                    <p id="81">此外也出现了基于深度学习的端对端语种识别系统, 摒弃了传统的语种识别系统框架.2014年Google的研究人员<citation id="366" type="reference"><link href="307" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>将特征提取、特征变换和分类器融于一个神经网络模型中, 这是端对端系统首次被成功应用于语种识别任务.随后有研究人员在此基础上发掘了不同神经网络的优势, 包括延时神经网络 (time-delay neural network, TDNN) <citation id="367" type="reference"><link href="309" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、长短时记忆递归神经网络 (long short term memory-recurrent neural network, LSTM-RNN) <citation id="368" type="reference"><link href="311" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.2016年Geng等人<citation id="369" type="reference"><link href="313" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>利用注意力机制模型 (attention-based model) , 结合LSTM-RNN搭建了端对端语种识别系统, 也取得了不错的语种识别性能.2018年Jin等人<citation id="370" type="reference"><link href="315" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了基于LID-net、LID-bnet的端对端语种模型, 并利用LID-net模型提取统计量, 对比原有的DNN-TV系统, 从网络中间层中获取LID-senone特征, 证明了这个特征更具有语种区分性.同年Cai等人<citation id="371" type="reference"><link href="317" rel="bibliography" /><link href="319" rel="bibliography" /><link href="321" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>提出了一种基于可学习的字典编码层的端对端系统, 从底层声学特征直接学习语种类别信息, 摒弃了声学模型, 也取得了较优的识别性能.</p>
                </div>
                <div class="p1">
                    <p id="82">Vincent等人<citation id="372" type="reference"><link href="323" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>于2008 年提出降噪自动编码器 (denoising auto-encoder, DAE) .该神经网络用于抑制输入信号中的噪声因子, 能够有效加强系统的鲁棒性, 广泛应用于语音增强<citation id="373" type="reference"><link href="325" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、语音信号去混响<citation id="374" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>等领域.2015年Yamamoto等人<citation id="375" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>首次将DAE应用于短时说话人识别, 2017年Yang等人<citation id="376" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>在此基础上, 改进并提出了基于DAE的短时说话人i-vector补偿技术, 取得了较好的说话人识别性能.</p>
                </div>
                <div class="p1">
                    <p id="83">目前, 将降噪自动编码器用于语种识别领域的研究鲜见报道.本文基于国内外关于语种识别和降噪自动编码器的研究, 实现了基于降噪自动编码器的语种特征补偿方法.本文的主要贡献有2个方面:</p>
                </div>
                <div class="p1">
                    <p id="84">1) 目前的语种识别系统在训练语音与测试语音长度匹配的情况下具有较高的识别率, 而当长度失配时, 其性能也随之下降.为了解决这个问题, 本文提出一种基于降噪自动编码器的语种特征补偿方法, 将不同长度的语音特征都映射为固定长度的语音特征, 一定程度上解决了长度失配问题.</p>
                </div>
                <div class="p1">
                    <p id="85">2) 语音学的研究<citation id="377" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>表明, 在世界范围内, 几乎没有任何语音拥有相同的音素集合, 即使有些语言共用同一套音素体系, 音素出现的频率也有所差别.因此在语种识别中, 音素作为重要的特征, 可以被用来有效区分语种.而事实上, 测试语音小于10 s或者更短时, 语音的音素分布严重不均衡<citation id="378" type="reference"><link href="335" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>.在这种情况下语种特征的提取也是不可信的, 极大地影响了识别性能.基于DAE的语种特征补偿算法, 将短时语种特征映射到长时语种特征空间, 以得到音素分布更为平衡的短时语音段表示, 缓解了短时测试语音音素分布不平衡的问题.</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="87">本文搭建了目前国际主流的2个语种识别系统:基于全差异空间的语种识别系统和基于端对端神经网络的语种识别系统, 并以此作为实验的基线系统.二者建模方法有所差异, 前者是传统的生成式的建模方法, 将GMM均值超矢量映射成低维向量, 再利用语种标签信息训练得到后端分类器.而后者在整个训练过程中直接使用深度神经网络进行语种识别, 这也是当前语种识别方向的研究热点.</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>1.1 基于全差异空间的语种识别系统</b></h4>
                <div class="p1">
                    <p id="89">基于全差异空间的方法首先被成功应用于说话人识别任务<citation id="379" type="reference"><link href="337" rel="bibliography" /><link href="339" rel="bibliography" /><sup>[<a class="sup">30</a>,<a class="sup">31</a>]</sup></citation>, 随后被迁移到语种识别领域<citation id="380" type="reference"><link href="341" rel="bibliography" /><link href="343" rel="bibliography" /><sup>[<a class="sup">32</a>,<a class="sup">33</a>]</sup></citation>, 成为语种识别领域的主流方法之一.基本框架如图1所示:</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905018_090.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GMM i-vector语种识别系统框图" src="Detail/GetImg?filename=images/JFYZ201905018_090.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GMM i-vector语种识别系统框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905018_090.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 structure of the GMM i-vector LID system</p>

                </div>
                <div class="p1">
                    <p id="91">TV系统通过定义一个低维空间, 该空间不区分捕获到的差异信息是否与语种、说话人和信道信息相关, 而是将GMM超向量<citation id="381" type="reference"><link href="297" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>中的语种、说话人变化空间和信道变化空间合并为全差异空间来进行建模.因为强制分离空间的话有可能会因为分离的不正确而丢失重要的信息, 这些信息在后端建模中是无法弥补的.为避免有关语种的有效信息丢失, 假设所有的空间都被合并成一个统一的空间, 由全差异矩阵<b><i>T</i></b>来构建.直观上来讲, 全差异空间会将高维的GMM超向量<b><i>M</i></b>映射成一个低维向量.假设<b><i>M</i></b>能被分解为</p>
                </div>
                <div class="p1">
                    <p id="92"><b><i>M</i></b>=<b><i>m</i></b>+<b><i>Tw</i></b>, (1) </p>
                </div>
                <div class="p1">
                    <p id="93">其中, <b><i>m</i></b>是与语种和信道等无关的超向量, 即UBM的均值超向量.因为UBM是由背景数据训练得到的, 无任何先验信息, 因此可以认为<b><i>m</i></b>与语种、说话人、信道等无关.<b><i>T</i></b>为全差异矩阵, <b><i>w</i></b>是全差异因子, 也叫i-vector, 是对GMM超向量的低维表示.i-vector作为模型中的隐含变量, 满足高斯分布, 其后验分布:</p>
                </div>
                <div class="p1">
                    <p id="94"><b><i>w</i></b> (<i>u</i>) = (<b><i>I</i></b>+<b><i>T</i></b><sup>T</sup><i>Σ</i><sup>-1</sup><b><i>N</i></b> (<i>u</i>) <b><i>T</i></b>) <sup>-1</sup><b><i>T</i></b><sup>T</sup><i>Σ</i><sup>-1</sup><b><i>F</i></b> (<i>u</i>) , (2) </p>
                </div>
                <div class="p1">
                    <p id="95">其中, <i>Σ</i>是对角协方差矩阵, 定义了GMM超向量中未被TV空间描述的噪声部分.<b><i>N</i></b> (<i>u</i>) 和<b><i>F</i></b> (<i>u</i>) 每句训练语音在UBM上的Baum-Welch统计量:</p>
                </div>
                <div class="p1">
                    <p id="96"><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ν</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>c</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (3) </p>
                </div>
                <div class="p1">
                    <p id="98"><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>c</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (4) </p>
                </div>
                <div class="p1">
                    <p id="100">其中, <i>p</i> (<i>c</i>|·) 表示在UBM的第<i>c</i>个高斯上的后验概率;<b><i>u</i></b><sub><i>t</i></sub>表示训练语音<i>u</i>的第<i>t</i>帧特征, 共<i>L</i>帧;<b><i>m</i></b><sub><i>c</i></sub>表示在UBM的第<i>c</i>个高斯上的均值向量.</p>
                </div>
                <div class="p1">
                    <p id="101">换一种角度理解, TV方法是将每段语音当作独立的个体, 认为每段都属于不同的语种, 再根据各语音段与UBM均值超向量的差异性得到具有语种区分性的低维因子表示, 也可以被认为是一种将语音信号特征映射到一个低维空间的概率主成分分析.</p>
                </div>
                <div class="p1">
                    <p id="102">从以上的描述中, 可以看出在估计全差异矩阵和提取i-vector的过程中, 并没有用到语种类别信息, 因此还需要对i-vector进行区分性训练, 可以使用逻辑回归作为后端分类器, 从而得到包含更多语种信息的i-vector.</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>1.2 基于端对端神经网络的语种识别系统</b></h4>
                <div class="p1">
                    <p id="104">近年来, 深度学习在语音信号处理领域中得到了快速的发展, 基于端对端神经网络的语种识别系统也逐渐成为主流.它摒弃了传统的全差异空间建模的方法, 在训练过程中将特征提取、变换以及后端分类器融于一个神经网络中, 引入语种标号进行区分性训练.相比传统的TV系统, 端对端网络有2个优势.1) 运用了区分性的建模方法, 在整个网络的训练过程中, 企图寻找不同类别之间的最佳分类面, 并没有侧重去拟合数据的分布.2) 直接利用语种标签信息, 不断优化网络参数, 使得所提特征更具语种特性.而TV建模仅仅在后端分类器的训练过程中引入了语种标签信息.由于研究时间过短, 端对端网络还有许多可改进的空间, 是当前的研究热点之一.</p>
                </div>
                <div class="p1">
                    <p id="105">本文实现了文献<citation id="382" type="reference">[<a class="sup">20</a>,<a class="sup">21</a>]</citation>提出的一种新的端对端语种识别系统, 它结合了卷积神经网络 (convolu-tional neural network, CNN) 在帧级特征上强大的建模能力和时域平均池化层 (temporal average pooling, TAP) 将帧级特征转换到句级特征的池化能力.该系统被称为CNN-TAP, 基本框架如图2所示.</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905018_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN-TAP端到端语种识别系统框图" src="Detail/GetImg?filename=images/JFYZ201905018_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CNN-TAP端到端语种识别系统框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905018_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Structure of the CNN-TAP end-to-end LID system</p>

                </div>
                <div class="p1">
                    <p id="107">具体来说, CNN-TAP包含一个基于卷积层的前端特征提取器, 设语音信号的声学特征为<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>T</i></sub>, 共<i>T</i>帧.其中<b><i>x</i></b><sub><i>T</i></sub>表示第<i>T</i>帧特征经过非线性变换映射后, 得到的特征包含更多语种信息;接着利用池化层得到句级特征表示, 经过TAP层后, 不同长度的输入语音得到了固定维度的句级向量表示;最后得到类别后验概率<i>p</i> (<i>k</i><sub><i>i</i></sub>|<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>T</i></sub>) , <i>k</i><sub><i>i</i></sub>表示第<i>i</i>个类别.</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag"><b>2 语种特征修复</b></h3>
                <div class="p1">
                    <p id="109">本文所提出的语种特征补偿方法框架如图3所示.首先从语音信号分帧、变换得到底层声学特征;之后利用1.1节描述的基线系统提取原始i-vector;同时按照2.1节所提出的方法计算音素向量;将拼接后的i-vector和音素向量, 送入基于DAE的语种特征补偿处理单元映射得到补偿后的i-vector, 详见2.2节;最后将补偿后的i-vector和原始i-vector分别送入后端分类器得到分数向量, 并将其在得分域融合.</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于DAE的i-vector补偿结构框图" src="Detail/GetImg?filename=images/JFYZ201905018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于DAE的i-vector补偿结构框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Structure of DAE based i-vector feature  compensation</p>

                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>2.1 音素向量</b></h4>
                <div class="p1">
                    <p id="112">语言学研究表明:不同音素集、不同音素的组合及出现的频率表征了不同类别的语言<citation id="383" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>.显而易见, 音素信息在语种识别中有很重要的作用.本文尝试用较为简单的方法提取语音的音素信息作为辅助信息, 增加不同语种之间的区分度.</p>
                </div>
                <div class="p1">
                    <p id="113">音素信息的提取过程为:先利用高斯混合模型计算语音信号每帧的后验概率, 再将一句话所有帧的后验概率求和取平均作为新的特征, 这里称为音素向量, 计算为</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>c</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">其中, <i>p</i><sub><i>c</i></sub> (<i>u</i>) 是训练语音<i>u</i>在UBM的第<i>c</i>个高斯上的后验概率;<i>t</i>表示帧数, 共<i>L</i>帧.</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"><b>2.2 基于降噪自动编码器的i-vector补偿方法</b></h4>
                <div class="p1">
                    <p id="117">降噪自动编码器是自动编码器 (auto-encoder, AE) 的改良版, 旨在用被破坏的输入数据重构出原始未被破坏的数据, 从而提取、编码出具有鲁棒性的特征.在语音信号处理领域, DAE被用于加强语音信号, 包括去噪、去混响等.</p>
                </div>
                <div class="p1">
                    <p id="118">本文提出基于降噪自动编码器的i-vector补偿方法, 结构框图如图4所示.DAE补偿网络的特征向量由语音的i-vector <b><i>w</i></b> (<i>u</i>) 和音素向量<b><i>p</i></b> (<i>u</i>) 拼接组成.输入短时语音语种特征向量<b><i>x</i></b><sub>1</sub> (<i>u</i>) =[<b><i>w</i></b><sub>1</sub> (<i>u</i>) , <b><i>p</i></b><sub>1</sub> (<i>u</i>) ], 标签为长时语音语种特征向量<b><i>x</i></b><sub><i>L</i></sub> (<i>u</i>) =[<b><i>w</i></b><sub><i>L</i></sub> (<i>u</i>) , <b><i>p</i></b><sub><i>L</i></sub> (<i>u</i>) ].隐层之间的前向传递过程为</p>
                </div>
                <div class="p1">
                    <p id="119"><b><i>x</i></b><sub><i>i</i>+1</sub> (<i>u</i>) =<i>g</i> (<b><i>W</i></b><sub> (<i>i</i>, <i>i</i>+1) </sub><b><i>x</i></b><sub><i>i</i></sub> (<i>u</i>) +<b><i>b</i></b><sub> (<i>i</i>, <i>i</i>+1) </sub>) , (6) </p>
                </div>
                <div class="p1">
                    <p id="120">其中, <i>g</i> () 为非线性函数, <b><i>W</i></b><sub> (<i>i</i>, <i>i</i>+1) </sub>和<b><i>b</i></b><sub> (<i>i</i>, <i>i</i>+1) </sub>为前一隐层与后一隐层之间的权重参数和偏置参数, 最后输出层为补偿语种特征<b><i>x</i></b><sub><i>N</i></sub> (<i>u</i>) =[<b><i>w</i></b><sub><i>N</i></sub> (<i>u</i>) , <b><i>p</i></b><sub><i>N</i></sub> (<i>u</i>) ], 称为补偿向量.<b><i>w</i></b><sub><i>N</i></sub> (<i>u</i>) 为补偿后的i-vector, <b><i>p</i></b><sub><i>N</i></sub> (<i>u</i>) 为补偿后的音素向量.</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905018_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 语种特征补偿方法图示" src="Detail/GetImg?filename=images/JFYZ201905018_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 语种特征补偿方法图示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905018_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 The diagram for language feature compensation</p>

                </div>
                <div class="p1">
                    <p id="122">该网络的训练是一种有监督的训练.主要思想是输入被干扰的向量, 重构出原始输入.本文实验中输入的是短时语音语种特征, 目标向量为长时语音语种特征.训练数据的准备分3个步骤:</p>
                </div>
                <div class="p1">
                    <p id="123">1) 提取训练语音<i>S</i><sub><i>i</i></sub>的i-vector和音素向量, 拼接得到目标向量<b><i>x</i></b> (<i>S</i><sub><i>i</i></sub>) .训练语音的长度范围是0～2 min;</p>
                </div>
                <div class="p1">
                    <p id="124">2) 将训练语音<i>S</i><sub><i>i</i></sub>分别切成3 s, 10 s和30 s, 对每个语音段<i>s</i><sub><i>i</i>, <i>j</i></sub>提取i-vector和音素向量, 拼接构成短时向量<b><i>x</i></b> (<i>s</i><sub><i>i</i>, <i>j</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="125">3) 最终的训练数据对为 (<b><i>x</i></b> (<i>S</i><sub><i>i</i></sub>) , <b><i>x</i></b> (<i>s</i><sub><i>i</i>, <i>j</i></sub>) ) .通过最小化目标函数对DAE网络进行参数优化.目标函数可以是目标向量和补偿向量之间的均方误差.</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>2.3 融合策略</b></h4>
                <div class="p1">
                    <p id="127">每条语音经过语种特征补偿处理单元后得到2个i-vector:原始i-vector和补偿后的i-vector.为了充分利用这2个特征, 本文在模型后端釆用2种融合策略:i-vector融合和得分融合.</p>
                </div>
                <div class="p1">
                    <p id="128">i-vector融合方法是将语音信号的原始i-vector和补偿后的i-vector进行线性融合后, 再送入后端分类器得到最终的判决结果:</p>
                </div>
                <div class="p1">
                    <p id="129"><b><i>w</i></b><sup>f</sup> (<i>u</i>) = (1-<i>α</i>) <b><i>w</i></b> (<i>u</i>) +<i>α</i><b><i>w</i></b><sup>comp</sup> (<i>u</i>) , 0≤<i>α</i>≤1, (7) </p>
                </div>
                <div class="p1">
                    <p id="130">其中, <b><i>w</i></b> (<i>u</i>) 是原始i-vector, <b><i>w</i></b><sup>comp</sup> (<i>u</i>) 是它的补偿i-vector.</p>
                </div>
                <div class="p1">
                    <p id="131">得分融合方法是直接在TV模型输出的得分上进行线性融合:</p>
                </div>
                <div class="p1">
                    <p id="132"><b><i>s</i></b><sup>f</sup> (<i>u</i>) = (1-<i>α</i>) <i>s</i> (<b><i>w</i></b> (<i>u</i>) ) +<i>αs</i> (<b><i>w</i></b><sup>comp</sup> (<i>u</i>) ) , 0≤<i>α</i>≤1, (8) </p>
                </div>
                <div class="p1">
                    <p id="133">其中, <i>s</i> () 为分数后端计算得分的函数.</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag"><b>3 语种特征补偿实验</b></h3>
                <h4 class="anchor-tag" id="135" name="135"><b>3.1 实验设置</b></h4>
                <div class="p1">
                    <p id="136">1) 测试集.从20世纪90年代起, 美国国家标准技术研究院 (National Institute of Standards and Technology, NIST) 组织的语种识别评测比赛 (language recognition evaluation, LRE) , 为语种识别的研究提供了统一的、公共的数据集.本实验采用美国国家标准技术局在2007年闭集条件下的语种识别评测数据集<citation id="384" type="reference"><link href="345" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>.这个测试集包含14个语种:阿拉伯语 (Arabic, AR) 、孟加拉语 (Bengali, BE) 、英语 (English, EN) 、波斯语 (Farsi, FA) 、俄语 (Russian, RU) 、德语 (German, GE) 、印地语 (Hindustani, HI) 、日语 (Japanese, JA) 、韩语 (Korean, KO) 、中文 (Chinese, CH) 、西班牙语 (Spanish, SP) 、泰米尔语 (Tamil, TA) 、泰国语 (Thai, TH) 和越南语 (Vietnamese, VI) .其中中文除了包含普通话, 还有闽南语、吴方言和粤语.英语除了美国英语外, 还包括印度英语.而印地语包括了北印度语和乌尔都语.NIST07的测试数据按照语音的时长分为30 s, 10 s, 3 s这3种, 每种包括2 158条14个语种的语音.</p>
                </div>
                <div class="p1">
                    <p id="137">2) 训练集.训练语料主要使用CallFriend数据库<citation id="385" type="reference"><link href="347" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>.该数据库包含12个语种:阿拉伯语、英语、波斯语、法语、德语、印地语、日语、韩语、普通话、西班牙语、泰米尔语和越南语.LRE2007的测试数据还有2个CallFriend 数据库不包含的语种、方言, 因此, 训练数据在CallFriend数据库的基础上还添加了LRE2003, LRE2005, LRE2007开发集.除此之外还包含2008年说话人识别评测比赛 (speaker recognition evaluation, SRE) <citation id="386" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>的训练数据, 为该测试提供补充训练数据:孟加拉语, 俄语、泰国语、闽南语、吴方言、粤语、阿拉伯语和乌尔都语.训练集的数据分布严重不平衡, 英语、中文占所有语料的57.06%.</p>
                </div>
                <div class="p1">
                    <p id="138">语种识别的测试标准主要采用NIST-LRE07测试标准平均代价 (average cost, <i>C</i>_<i>avg</i>) <citation id="387" type="reference"><link href="345" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>和错误率 (error rate, ER) 来评价.另外本文在4.2节中还将提到虚警率、漏警率<citation id="388" type="reference"><link href="345" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>.这些指标从不同角度反映了语种识别系统性能的好坏, 它们都是越小越好.<i>C</i>_<i>avg</i>的定义为</p>
                </div>
                <div class="p1">
                    <p id="139" class="code-formula">
                        <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>C</mi><mo>_</mo><mi>a</mi><mi>v</mi><mi>g</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>L</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></munder><mi>C</mi></mstyle><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>i</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>i</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>a</mtext></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext><mo>-</mo><mtext>o</mtext><mtext>f</mtext><mo>-</mo><mtext>s</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo>, </mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>n</mtext><mo>-</mo><mtext>t</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>L</mi><msub><mrow></mrow><mtext>n</mtext></msub></mrow></munder><mi>C</mi></mstyle><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>a</mtext></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext><mo>-</mo><mtext>o</mtext><mtext>f</mtext><mo>-</mo><mtext>s</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo>, </mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>n</mtext><mo>-</mo><mtext>t</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="140"><i>P</i><sub>out-target</sub>= (1-<i>P</i><sub>target</sub>-<i>P</i><sub>out-of-set</sub>) / (<i>N</i><sub>1</sub>-1) (10) </p>
                </div>
                <div class="p1">
                    <p id="141">其中, <i>N</i><sub>l</sub>为集合中语种总数;<i>L</i><sub>t</sub>和<i>L</i><sub>n</sub>分别表示目标语种和非目标语种;<i>P</i><sub>miss</sub> (<i>L</i><sub>t</sub>) 表示目标语种为<i>L</i><sub>t</sub>时的漏检率;<i>P</i><sub>fa</sub> (<i>L</i><sub>t</sub>, <i>L</i><sub>n</sub>) 是目标语种为<i>L</i><sub>t</sub>时的虚警率;<i>C</i><sub>miss</sub>和<i>C</i><sub>fa</sub>分别是漏检和虚警的惩罚因子;<i>P</i><sub>target</sub>为目标语种的先验概率;<i>P</i><sub>non-target</sub>为非目标语种的先验概率;<i>P</i><sub>out-of-set</sub>为集外语种的先验概率.本论文实验只考虑闭集测试的情况, 因此<i>P</i><sub>out-of-set</sub>=0.NIST-LRE07设定<i>C</i><sub>miss</sub>=<i>C</i><sub>fa</sub>=1, <i>P</i><sub>target</sub>=0.5.</p>
                </div>
                <div class="p1">
                    <p id="142">本文采用2个基线系统:基线1是基于TV空间的i-vector系统, 分为前端声学特征提取和后端统计建模2部分.前端底层声学特征采用基于PLP线性扩展的SDC特征, 后端建模利用TV建模方法.TV建模中高斯混合数为512, i-vector维数为600, 采用逻辑回归分类器.基线2是基于端对端深度神经网络的语种识别系统.与文献<citation id="389" type="reference">[<a class="sup">20</a>,<a class="sup">21</a>]</citation>相似, 卷积神经网络采用深度残差网络resnet34, 采用交叉熵 (cross entropy) 准则进行训练, 采用随机梯度下降法 (stochastic gradient descent, SGD) 更新网络参数.Mini-batch大小设为128, 每个Mini-batch的帧长范围为[200, 1000].共60个epoch, 学习率为0.1.</p>
                </div>
                <div class="p1">
                    <p id="143">随机选择5 000条训练数据训练高斯混合数为32的UBM, 提取32维的音素向量.基于降噪自动编码器的网络输入是632维向量, 由600维i-vector和32维音素向量组成.网络共有4层, 隐层节点数是2048, 网络的结构是632-2048-2048-632.采用最小均方误差准则进行训练.经过对比2种融合策略, 实验均采用得分融合策略, <i>α</i>=0.1.关于融合系数<i>α</i>的选取, 将在4.2节实验结果中给出验证.</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>3.2 实验结果</b></h4>
                <div class="p1">
                    <p id="145">为了验证本文所提算法的有效性, 本节将描述多组实验.首先以音素向量作为唯一变量, 验证音素向量补偿的效果.表1列出了i-vector直接通过DAE网络映射以及音素向量和i-vector拼接后映射, 在不同时长测试语音下的评价指标<i>ER</i>和<i>C</i>_<i>avg</i>的变化趋势.表1中的Direct Mapping表示提取出的i-vector直接经过DAE网络映射.而Concatenate Mapping表示送入DAE网络的特征是音素向量和i-vector的拼接向量.2组实验的唯一变量是音素向量.DAE网络由3 s的短时语种特征和2 min的长时语种特征对训练得到.实验后端均采用逻辑回归分类器.映射后的i-vector和原始ivector按照0.1∶0.9在得分域融合.</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表1 音素向量补偿性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 The Results of Phonetic Vector Compensation</b></p>
                    <p class="img_note"></p>
                    <table id="146" border="1"><tr><td rowspan="3"><br />Different <br />Systems</td><td colspan="6"><br />Test Utterance Length/s</td></tr><tr><td colspan="2"><br />30</td><td colspan="2">10</td><td colspan="2">3</td></tr><tr><td><br /><i>ER</i></td><td><i>C</i>_<i>avg</i></td><td><i>ER</i></td><td><i>C</i>_<i>avg</i></td><td><i>ER</i></td><td><i>C</i>_<i>avg</i></td></tr><tr><td><br />Baseline 1</td><td>8.85</td><td>5.67</td><td>22.38</td><td>13.88</td><td>48.93</td><td>29.95</td></tr><tr><td><br />Baseline 2</td><td>8.53</td><td>5.31</td><td>21.59</td><td>12.94</td><td>50.32</td><td>29.98</td></tr><tr><td><br />Direct Mapping</td><td>8.80</td><td>5.65</td><td>22.15</td><td>13.83</td><td>48.84</td><td>29.91</td></tr><tr><td><br />Concatenate<br />Mapping</td><td>8.78</td><td>5.64</td><td>22.10</td><td>13.75</td><td>48.70</td><td>29.87</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="147">从表1可以看出, 相比基线1, i-vector直接映射后的识别性能在3种测试时长上均有提升, 尤其是在测试语音时长为10 s和3 s的短时情况, 因为本组实验的DAE网络训练数据由3 s和2 min语音对构成, 短时测试语音与模型更匹配, 性能提升更多.相比基线2, i-vector直接映射和拼接映射后识别性能在3 s的测试时长上也有所提升.音素向量和i-vector合并后, 语种识别性能进一步得到改善, 说明音素向量有一定的补偿效果.</p>
                </div>
                <div class="p1">
                    <p id="148">下面验证补偿网络的有效性.针对不同的测试条件, 在训练阶段, 长时的训练语料被切割成时长分别为30 s, 10 s和3 s的短时语音段, 并组成3种时长的短时语音语种训练集合, 分别学习对应的补偿网络.表2列出了针对不同测试时长的训练数据分别训练相应的补偿系统, 在不同时长测试语音下的评价指标<i>ER</i>和<i>C</i>_<i>avg</i>的变化情况.表2中的30 s compensation表示补偿网络的训练数据是30 s和2 min的训练对.</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表2 基线系统和补偿系统性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Performance Comparison on Baseline and Compensation System</b></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td rowspan="3"><br />Different <br />Systems</td><td colspan="6"><br />Test Utterance Length/s</td></tr><tr><td colspan="2"><br />30</td><td colspan="2">10</td><td colspan="2">3</td></tr><tr><td><br /><i>ER</i></td><td><i>C</i>_<i>avg</i></td><td><i>ER</i></td><td><i>C</i>_<i>avg</i></td><td><i>ER</i></td><td><i>C</i>_<i>avg</i></td></tr><tr><td><br />Baseline 1</td><td>8.85</td><td>5.67</td><td>22.38</td><td>13.88</td><td>48.93</td><td>29.95</td></tr><tr><td><br />Baseline 2</td><td>8.53</td><td>5.31</td><td>21.59</td><td>12.94</td><td>50.32</td><td>29.98</td></tr><tr><td><br />30 s compensation</td><td>8.57</td><td>5.59</td><td>22.15</td><td>13.77</td><td>48.84</td><td>29.90</td></tr><tr><td><br />10 s compensation</td><td>8.67</td><td>5.61</td><td>21.73</td><td>13.61</td><td>48.75</td><td>29.89</td></tr><tr><td><br />3 s compensation</td><td>8.78</td><td>5.64</td><td>22.10</td><td>13.75</td><td>48.70</td><td>29.87</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">从表2可以看出, 本文提出的补偿算法在各种测试时长上的识别性能都有提高.尤其是在测试语音时长和训练语音时长完全匹配的情况下, 性能提升最大.相比基线1, 测试语音时长为30 s, 经过30 s补偿网络映射后, 错误率相对降低了3.16%;测试语音时长为10 s的错误率经过10 s补偿网络映射后相对降低了2.90%;测试语音时长为3 s的错误率, 经过3 s补偿网络映射后相较基线提升不大.这是因为使用3 s时长的短时训练样本由于语音段过短使其在提取i-vector时包含的语种相关信息非常容易受到影响, 直接用于网络训练可能导致模型的估计不准确.相比基线2, 补偿网络在测试语音时长为30 s和10 s的性能没有提升, 但基本可以与基线持平;测试语音时长为3 s的错误率经过3 s补偿网络映射后相对降低了3.21%.总的来说, 针对不同测试时长, 利用时长互相匹配的训练数据分别训练相应的补偿系统能进一步提升补偿网络系统的性能.</p>
                </div>
                <div class="p1">
                    <p id="151">Vincent等人<citation id="390" type="reference"><link href="323" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>证明了DAE网络的可以对加噪数据进行降噪, 得到更加鲁棒的不变性特征, 获得输入的更有效表达.文献<citation id="391" type="reference">[<a class="sup">22</a>]</citation>提到一般情况下, 高维的数据都处于一个较低维的流形曲面上, 被干扰向量的分布稀疏, 远离低维曲面.而DAE网络可以将稀疏的特征映射到更为紧实的低维曲面上.这与本文所描述的基于DAE的补偿网络的主要思想是一致的, 短时特征可以被当做是长时特征经过噪声干扰后的数据.补偿网络得输入是短时语音的i-vector, 通过学习隐含特征, 可以有效重构出长时语音的i-vector.</p>
                </div>
                <div class="p1">
                    <p id="152">图5显示出30 s, 10 s, 3 s测试时长的<i>C</i>_<i>avg</i>随补偿i-vector权重<i>α</i>变化的情况, 不同测试时长语音分别经过对应时长的补偿网络, 采用式 (8) 的得分融合策略.</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905018_153.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 C_avg随得分融合系数变化图" src="Detail/GetImg?filename=images/JFYZ201905018_153.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>C</i>_<i>avg</i>随得分融合系数变化图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905018_153.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The variation of <i>C</i>_<i>avg</i> with respect to  score fusion coefficient</p>

                </div>
                <div class="p1">
                    <p id="154">从图5中可以看出, <i>α</i>取0.1～0.2时融合系统的性能稍好于基线1, <i>α</i>=0.1时在3种测试时长下性能最优.这说明基于DAE的补偿网络提供了额外的语言信息, 与原始i-vector语言信息互补.</p>
                </div>
                <div class="p1">
                    <p id="155">混淆矩阵可以很直观地反映出各语种的虚警率、漏警率以及各语种间的混淆程度.本文统计了测试语音在3 s补偿系统上的得分所构成的混淆矩阵.如图6所示, 纵轴代表真实类别, 横轴代表预测类别.ACC代表该语种识别精度.从图6可以看出, 各语种大部分样本都能保证分类正确, 彼此间不存在较大的干扰.但是某些类别之间混淆程度较大导致系统的虚警率和漏警率升高.3.1节中提到训练数据分布不均衡, 其中MA所占比例最大, 它们的识别精度分别为71.94%, 84.50%, 虚警率为31.02%和22.97%.这是因为这部分语料比重大, 在训练阶段系统会侧重优化这些语种的模型, 使得最终模型的预测结果有偏差.还有一些语种即使训练语料所占比例不大, 却仍存在很高的虚警率, 这说明简单的依赖底层声学特征是无法准确地给出易混语种的判别结果.从分析结果可以看出本文提出的模型虽然较基线系统有一定的性能提升, 但还存在很大的提升空间.</p>
                </div>
                <div class="area_img" id="156">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905018_156.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 分类混淆矩阵" src="Detail/GetImg?filename=images/JFYZ201905018_156.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 分类混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905018_156.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Classification confusion matrix</p>

                </div>
                <h3 id="158" name="158" class="anchor-tag"><b>4 总 结</b></h3>
                <div class="p1">
                    <p id="159">语种识别系统在训练语音和测试语音长度不匹配时, 性能会出现大幅下滑, 严重制约语种识别技术在实际中的应用.本文提出基于降噪自动编码器的语种特征补偿方法来解决这个问题.在NIST-LRE07上的实验结果表明, 所提出的语种特征补偿算法在30 s, 10 s和3 s这3种测试条件下均可以获得不同程度的性能提升.对实验细节的进一步分析表明:目前音素向量提取过程较为简单, 并没有充分挖掘出语音的音素分布信息, 未能起到较大作用, 后期仍有进一步改良的空间.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="279">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Spoken language recognition:from fundamentals to practice.&amp;quot;">

                                <b>[1]</b>Li Haizhou, Ma Bin, Lee K.Spoken language recognition:From fundamentals to practice[J].Proceedings of the IEEE, 2013, 101 (5) :1136-1159
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparison of four approaches to automatic language identification of telephone speech">

                                <b>[2]</b>Zissman M A.Comparison of four approaches to automatic language identification of telephone speech[J].IEEETransactions on Speech and Audio Processing, 1996, 4 (1) :31
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An approach to automatic language identification based on language-dependent phone recognition">

                                <b>[3]</b>Yan Yonghong, Barnard E.An approach to automatic language identification based on language-dependent phone recognition[C]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 1995:3511-3514
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100176425&amp;v=MTgyNzd5am1VTHZJSVY0WGJoWT1OaWZPZmJLN0h0RE9ybzlGWmV3SkNINDhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Yan Yonghong, Barnard E, Cole R A.Development of an approach to automatic language identification based on phone recognition[J].Computer Speech&amp;Language, 1996, 10 (1) :37-54
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Vector space modeling approach to spoken language identification">

                                <b>[5]</b>Li Haizhou, Ma Bin, Lee C.A vector space modeling approach to spoken language identification[J].IEEETransactions on Audio, Speech&amp;Language Processing, 2007, 15 (1) :271-284
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences">

                                <b>[6]</b>Davis S, Mermelstein P.Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences[J].IEEE Transactions on Acoustics, Speech and Signal Processing, 1980, 28 (4) :65-74
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual linear predictive (PLP) analysis of speech">

                                <b>[7]</b>Hermansk H.Perceptual linear predictive (PLP) analysis of speech[J].The Journal of the Acoustical Society of America, 1990, 87 (4) :1738-1752
                            </a>
                        </p>
                        <p id="293">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Approachesto Language Identification Using Gaussian Mixture Models andShifted Delta Cepstral Features">

                                <b>[8]</b>Torres-Carrasquillo P A, Singer E, Kohler M A, et al.Approaches to language identification using Gaussian mixture models and shifted delta cepstral features[C]Proc of the7th Int Conf on Spoken Language Processing.Piscataway, NJ:IEEE, 2002:89-92
                            </a>
                        </p>
                        <p id="295">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501235421&amp;v=MTEwNDFCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SUlWNFhiaFk9TmlmT2ZiSzdIdEROcW85RVp1Z0tDSDQ0bw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Reynolds D A, Quatieri T F, Dunn R B.Speaker verification using adapted Gaussian mixture models[J].Digital Signal Processing, 2000, 10 (1?2?3) :19-41
                            </a>
                        </p>
                        <p id="297">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector machines using GMM supervectors for speaker verification">

                                <b>[10]</b>Campbell W M, Sturim D E, Reynolds D A.Support vector machines using GMM supervectors for speakers verification[J].IEEE Signal Processing Letters, 2006, 13 (5) :308-311
                            </a>
                        </p>
                        <p id="299">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Language recognition via i-vectors and dimensionality reduction">

                                <b>[11]</b>Dehak N, Torres-Carrasquillo P A, Reynolds D A, et al.Language recognition via i-vectors and dimensionality reduction[C]Proc of the 12th Annual Conf of the Int Speech Communication Association.Baixas, Florence:International Speech and Communication Association, 2011:857-860
                            </a>
                        </p>
                        <p id="301">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reducing the dimensionality of data with neural networks">

                                <b>[12]</b>Hinton G, Salakhutdinov R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507
                            </a>
                        </p>
                        <p id="303">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance evaluation of deep bottleneck features for spoken language identification">

                                <b>[13]</b>Jiang Bing, Song Yan, Wei Si, et al.Performance evaluation of deep bottleneck features for spoken language identification[C]Proc of the 9th Int Symp on Chinese Spoken Language Processing.Piscataway, NJ:IEEE, 2014:143-147
                            </a>
                        </p>
                        <p id="305">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel scheme for speaker recognition using a phonetically-aware deep neural network">

                                <b>[14]</b>Lei Yun, Scheffer N, Ferrer L, et al.A novel scheme for speaker recognition using aphonetically-aware deep neural network[C]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2014:1695-1699
                            </a>
                        </p>
                        <p id="307">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic language identification using deep neural networks">

                                <b>[15]</b>Lopez-Moreno I, Gonzalez-Dominguez J, Plchot O, et al.Automatic language identification using deep neural networks[C]Proc of the 39th IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2014:5337-5341
                            </a>
                        </p>
                        <p id="309">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked long-term TDNN for spoken language recognition">

                                <b>[16]</b>Garcia-Romero D, McCree A.Stacked long-term TDNN for spoken language recognition[C]Proc of the 17th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2016:3226-3230
                            </a>
                        </p>
                        <p id="311">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic language identification using long short-term memory recurrent neural networks">

                                <b>[17]</b>Gonzalez-Dominguez J, Lopez-Moreno I, Sak H, et al.Automatic language identification using long short-term memory recurrent neural networks[C]Proc of the 15th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2014:2155-2159
                            </a>
                        </p>
                        <p id="313">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end language identification using attention-based recurrent neural networks">

                                <b>[18]</b>Geng Wang, Wang Wenfu, Zhao Yuanyuan, et al.End-toend language identification using attention-based recurrent neural networks[C]Proc of the 17th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2016:2944-2948
                            </a>
                        </p>
                        <p id="315">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2DA3BCEEB7D5B8EE7AD3B89C4AB43090&amp;v=MTIxNzhmcjN4WkVDN2FYUmJPZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHhiaTh4S3c9TmlmSVk3SE1iOUsrM1Bvd0Z1eDdDUTR4dW1NVW0wdCtPbg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>Jin Ma, Song Yan, McLoughlin I.LID-senones and their statistics for language identification[J].IEEE?ACMTransactions on Audio, Speech and Language Processing, 2018, 26 (1) :171-183
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Insights into end-to-end learning scheme for language identification[C?OL]">

                                <b>[20]</b>Cai Weicheng, Cai Zexin, Liu Wenbo, et al.Insights into end-to-end learning scheme for language identification[C?OL]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2018[2018-05-20].https:scholar.google.co.uk?scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Insights+into+end-to-end+learning+scheme+for+language+identification&amp;btnG
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel learnable dictionary encoding layer for end-to-end language identification[C?OL]">

                                <b>[21]</b>Cai Weicheng, Cai Zexin, Zhang Xiang, et al.A novel learnable dictionary encoding layer for end-to-end language identification[C?OL]Proc of IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2018[2018-05-20].https:scholar.google.co.uk?scholar?hl=en&amp;as_sdt=0%2C5&amp;q=A+novel+learnable+dictionary+encoding+layer+for+end-to-end+language+identification+&amp;btnG
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring the encoding layer and loss function in end-to-end speaker and language recognition system[C?OL]">

                                <b>[22]</b>Cai Weicheng, Chen Jinkun, Li Ming.Exploring the encoding layer and loss function in end-to-end speaker and language recognition system[C?OL]Proc of Speaker Odyssey.2018[2018-06-01].https:scholar.google.co.uk?scholar?hl=en&amp;as_sdt=0%2C5&amp;q=Exploring+the+Encoding+Layer+and+Loss+Function+in+End-toEnd+Speaker+and+Language+Recognition+System+&amp;btnG
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting and composing robust features with denoising autoencoders">

                                <b>[23]</b>Vincent P, Larochelle H, Bengio Y, et al.Extracting and composing robust features with denoising autoencoders[C]Proc of the 25th Int Conf on Machine learning.New York:ACM, 2008:1096-1103
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech enhancement based on deep denoising autoencoder">

                                <b>[24]</b>Lu Xugang, Tao Y, Matsuda S, et al.Speech enhancement based on deep denoising autoencoder[C]Proc of the 14th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2013:436-440
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reverberant speech recognition based on denoising autoencoder">

                                <b>[25]</b>Ishii T, Komiyama H, Shinozaki T, et al.Reverberant speech recognition based on denoising autoencoder[C]Proc of the 14th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2013:3512-3516
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Denoising autoencoder-based speaker feature restoration for utterances of short duration">

                                <b>[26]</b>Yamamoto H, Koshinaka T.Denoising autoencoder-based speaker feature restoration for utterances of short duration[C]Proc of the 16th Annual Conf of the Int Speech Communication Association.Dresden, Germany:International Speech and Communication Association, 2015:1052-1056
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Applying compensation techniques on i-vectors extracted from short-test utterances for speaker verification using deep neural network">

                                <b>[27]</b>Yang I, Heo H, Yoon S, et al.Applying compensation techniques on i-vectors extracted from short-test utterances for speaker verification using deep neural network[C]Proc of the 42nd IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2017:5490-5494
                            </a>
                        </p>
                        <p id="333">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A segmental approach to automatic language identification">

                                <b>[28]</b>Muthusamy Y K.A segmental approach to automatic language identification[D].Portland, Oregon, USA:Oregon Health&amp;Science University, 1993
                            </a>
                        </p>
                        <p id="335">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Duration mismatch compensation for i-vector based speaker recognition systems">

                                <b>[29]</b>Hasan T, Saeidi R, Hansen J H L, et al.Duration mismatch compensation for i-vector based speaker recognition systems[C]Proc of the 38th IEEE Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:7663-7667
                            </a>
                        </p>
                        <p id="337">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Front-End Factor Analysis for Speaker Verification">

                                <b>[30]</b>Dehak N, Kenny P, Dehak R, et al.Frontend factor analysis for speaker verification[J].IEEE Transactions on Audio, Speech&amp;Language Processing, 2011, 19 (4) :788-798
                            </a>
                        </p>
                        <p id="339">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification">

                                <b>[31]</b>Dehak N, Dehak R, Kenny P, et al.Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification[C]Proc of the10th Annual Conf of the Int Speech Communication Association.Brighton, United Kingdom:International Speech and Communication Association, 2009:1559-1562
                            </a>
                        </p>
                        <p id="341">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Language Recognition via Ivectors and Dimensionality Reduction">

                                <b>[32]</b>Dehak N, Torres-Carrasquillo P A, Reynolds D A, et al.Language recognition via i-vectors and dimensionality reduction[C]Proc of the 12th Annual Conf of the Int Speech Communication Association.Baixas, France:International Speech and Communication Association, 2011:857-860
                            </a>
                        </p>
                        <p id="343">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Language recognition with language total variability">

                                <b>[33]</b>Yang Jinchao, Zhang Xiang, Suo Hongbin, et al.Language recognition with language total variability[C]Proc of the2011 Int Conf on Innovative Computing and Cloud Computing.New York:ACM, 2011:6-9
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NIST 2007 language recognition evaluation[S/OL]">

                                <b>[34]</b>Martin A F, Le A N.NIST 2007 language recognition evaluation[S/OL].Gaithersburg, Maryland:National Institute of Standards and Technology.[2017-09-20].https://catalog.ldc.upenn.edu/docs/LDC2009S04/LRE07EvalPlan-v8b-1.pdf
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linguistic data consortium[S/OL]">

                                <b>[35]</b>CallFriend Corpus.Linguistic data consortium[S/OL].[2017-09-20].http://www.ldc.upenn/ldc/about/callfriend.html
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The NIST Year 2008 Speaker Recognition Evaluation Plan[S/OL]">

                                <b>[36]</b>National Institute of Standards and Technology.The NIST Year 2008 Speaker Recognition Evaluation Plan[S/OL].Gaithersburg, Maryland:National Institute of Standards and Technology.[2017-10-20].https://www.nist.gov/sites/default/files/documents/2017/09/26/sre08_evalplan_release4.pdf
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201905018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905018&amp;v=MDU4OTZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeW5sVUx2TUx5dlNkTEc0SDlqTXFvOUU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMlkzU0N3Q0FNM2dHMmNzQVhNcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

