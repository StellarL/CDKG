

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128632453868750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201906019%26RESULT%3d1%26SIGN%3djv1m8SMY3NwQ9bR5hRN3BnW4dz8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906019&amp;v=Mjc5NjNTZExHNEg5ak1xWTlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEblZyN01MeXY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;2 方法描述&lt;/b&gt; "><b>2 方法描述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="&lt;b&gt;2.1 视觉模态特征提取网络&lt;/b&gt;"><b>2.1 视觉模态特征提取网络</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;2.2 文本模态特征提取网络&lt;/b&gt;"><b>2.2 文本模态特征提取网络</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;2.3 多模态深度多重判别性相关分析&lt;/b&gt;"><b>2.3 多模态深度多重判别性相关分析</b></a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;2.4 多模态注意力融合网络的情感分类&lt;/b&gt;"><b>2.4 多模态注意力融合网络的情感分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#202" data-title="&lt;b&gt;3 实验分析&lt;/b&gt; "><b>3 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#204" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#212" data-title="&lt;b&gt;3.2 实验设置&lt;/b&gt;"><b>3.2 实验设置</b></a></li>
                                                <li><a href="#216" data-title="&lt;b&gt;3.3 实验1:情感分类准确性评估&lt;/b&gt;"><b>3.3 实验1:情感分类准确性评估</b></a></li>
                                                <li><a href="#230" data-title="&lt;b&gt;3.4 实验2: 局部模型效用评估&lt;/b&gt;"><b>3.4 实验2: 局部模型效用评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#251" data-title="&lt;b&gt;4 总  结&lt;/b&gt; "><b>4 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="图1 基于层次化深度关联融合网络的社交媒体多模态情感分类框架图">图1 基于层次化深度关联融合网络的社交媒体多模态情感分类框架图</a></li>
                                                <li><a href="#86" data-title="图2 视觉和文本的多模态深度多重判别性相关分析图解">图2 视觉和文本的多模态深度多重判别性相关分析图解</a></li>
                                                <li><a href="#177" data-title="图3 多模态注意力融合网络的情感分类图解">图3 多模态注意力融合网络的情感分类图解</a></li>
                                                <li><a href="#211" data-title="&lt;b&gt;表1 实验使用数据集统计&lt;/b&gt;"><b>表1 实验使用数据集统计</b></a></li>
                                                <li><a href="#225" data-title="&lt;b&gt;表2 在VCGI和VCGII数据集上不同方法的准确率&lt;/b&gt;"><b>表2 在VCGI和VCGII数据集上不同方法的准确率</b></a></li>
                                                <li><a href="#228" data-title="&lt;b&gt;表3 在MVSO-EN和Flickr数据集上不同方法的准确率&lt;/b&gt;"><b>表3 在MVSO-EN和Flickr数据集上不同方法的准确率</b></a></li>
                                                <li><a href="#247" data-title="图4 在5个数据集上评估深度判别性相关分析的性能">图4 在5个数据集上评估深度判别性相关分析的性能</a></li>
                                                <li><a href="#250" data-title="图5 在5个数据集上评估co-attention设置的性能">图5 在5个数据集上评估co-attention设置的性能</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="294">


                                    <a id="bibliography_1" title="Wang Min, Cao Donglin, Li Lingxiao, et al.Microblog sentiment analysis based on cross-media bag-of-words model[C] //Proc of the 2014 Int Conf on Multimedia Computing and Service.New York:ACM, 2014:76- 80" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microblog sentiment analysis based on cross-media bag-of-words model">
                                        <b>[1]</b>
                                        Wang Min, Cao Donglin, Li Lingxiao, et al.Microblog sentiment analysis based on cross-media bag-of-words model[C] //Proc of the 2014 Int Conf on Multimedia Computing and Service.New York:ACM, 2014:76- 80
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_2" title="Cao Donglin, Ji Rongrong, Lin Dazhen, et al.A cross-media public sentiment analysis system for microblog[J].Multimedia Systems, 2016, 22 (4) :479- 486" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A cross-media public sentiment analysis system for microblog">
                                        <b>[2]</b>
                                        Cao Donglin, Ji Rongrong, Lin Dazhen, et al.A cross-media public sentiment analysis system for microblog[J].Multimedia Systems, 2016, 22 (4) :479- 486
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_3" title="Poria S, Cambria E, Howard N, et al.Fusing audio, visual and textual clues for sentiment analysis from multimodal content[J].Neurocomputing, 2016, 174:50- 59" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEF472DCBE40D3F9A56DBC2D63226A020&amp;v=MDIyMzA5UHhtY1c3RXNQTzMyV3FoRTNlN1RsUmJpZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54Z3pMcTZ3YXc9TmlmT2ZjYk9HdGJPMi93M0VlOFBlSA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Poria S, Cambria E, Howard N, et al.Fusing audio, visual and textual clues for sentiment analysis from multimodal content[J].Neurocomputing, 2016, 174:50- 59
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_4" title="Katsurai M, Satoh S.Image sentiment analysis using latent correlations among visual, textual, and sentiment views[C] //Proc of the 2016 Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2016:2837- 2841" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image sentiment analysis using latent correlations among visual,textual,and sentiment views">
                                        <b>[4]</b>
                                        Katsurai M, Satoh S.Image sentiment analysis using latent correlations among visual, textual, and sentiment views[C] //Proc of the 2016 Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2016:2837- 2841
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_5" title="Cai Guoyong, Xia Binbin.Convolutional neural networks for multimedia sentiment analysis[C] //Proc of the 2015 Conf on Natural Language Processing and Chinese Computing.Berlin:Springer, 2015:159- 167" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for multimedia sentiment analysis">
                                        <b>[5]</b>
                                        Cai Guoyong, Xia Binbin.Convolutional neural networks for multimedia sentiment analysis[C] //Proc of the 2015 Conf on Natural Language Processing and Chinese Computing.Berlin:Springer, 2015:159- 167
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_6" title="Yu Yuhai, Lin Hongfei, Meng Jiana, et al.Visual and textual sentiment analysis of a microblog using deep convolutional neural networks[J].Algorithms, 2016, 9 (2) :41- 51" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual and Textual Sentiment Analysis of a Microblog Using Deep Convolutional Neural Networks">
                                        <b>[6]</b>
                                        Yu Yuhai, Lin Hongfei, Meng Jiana, et al.Visual and textual sentiment analysis of a microblog using deep convolutional neural networks[J].Algorithms, 2016, 9 (2) :41- 51
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_7" title="Baecchi C, Uricchio T, Bertini M, et al.A multimodal feature learning approach for sentiment analysis of social network multimedia[J].Multimedia Tools and Applications, 2016, 75 (5) :2507- 2525" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multimodal feature learning approach for sentiment analysis of social network multimedia">
                                        <b>[7]</b>
                                        Baecchi C, Uricchio T, Bertini M, et al.A multimodal feature learning approach for sentiment analysis of social network multimedia[J].Multimedia Tools and Applications, 2016, 75 (5) :2507- 2525
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_8" title="You Quanzeng, Luo Jiebo, Jin Hailin, et al.Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia[C] //Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016:13- 22" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-modality consistent regression for joint visual-textual sentiment analysis">
                                        <b>[8]</b>
                                        You Quanzeng, Luo Jiebo, Jin Hailin, et al.Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia[C] //Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016:13- 22
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_9" title="Xu Nan, Mao Wenji.A residual merged neutral network for multimodal sentiment analysis[C] //Proc of the 2nd Int Conf on Big Data Analysis (ICBDA) .New York:IEEE, 2017:6- 10" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A residual merged neutral network for multimodal sentiment analysis">
                                        <b>[9]</b>
                                        Xu Nan, Mao Wenji.A residual merged neutral network for multimodal sentiment analysis[C] //Proc of the 2nd Int Conf on Big Data Analysis (ICBDA) .New York:IEEE, 2017:6- 10
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_10" title="Dos Santos C, Gatti M.Deep convolutional neural networks for sentiment analysis of short texts[C] //Proc of the 25th Int Conf on Computational Linguistics (COLING 2014) .Stroudsburg, PA:ACL, 2014:69- 78" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts">
                                        <b>[10]</b>
                                        Dos Santos C, Gatti M.Deep convolutional neural networks for sentiment analysis of short texts[C] //Proc of the 25th Int Conf on Computational Linguistics (COLING 2014) .Stroudsburg, PA:ACL, 2014:69- 78
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_11" title="Liang Bin, Liu Quan, Xu Jin, et al.Aspect-based sentiment analysis based on multi-attention CNN[J].Journal of Computer Research and Development, 2017, 54 (8) :1724- 1735 (in Chinese) (梁斌, 刘全, 徐进, 等.基于多注意力卷积神经网络的特定目标情感分析[J].计算机研究与发展, 2017, 54 (8) :1724- 1735) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708009&amp;v=MDIzODllUnFGaURuVnI3TUx5dlNkTEc0SDliTXA0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Liang Bin, Liu Quan, Xu Jin, et al.Aspect-based sentiment analysis based on multi-attention CNN[J].Journal of Computer Research and Development, 2017, 54 (8) :1724- 1735 (in Chinese) (梁斌, 刘全, 徐进, 等.基于多注意力卷积神经网络的特定目标情感分析[J].计算机研究与发展, 2017, 54 (8) :1724- 1735) 
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_12" title="Chen Ke, Liang Bin, Ke Wende, et al.Chinese micro-blog sentiment analysis based on multi-channels convolutional neural networks[J].Journal of Computer Research and Development, 2018, 55 (5) :945- 957 (in Chinese) (陈珂, 梁斌, 柯文德, 等.基于多通道卷积神经网络的中文微博情感分析[J].计算机研究与发展, 2018, 55 (5) :945- 957) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201805006&amp;v=MDg2OTJGaURuVnI3TUx5dlNkTEc0SDluTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Chen Ke, Liang Bin, Ke Wende, et al.Chinese micro-blog sentiment analysis based on multi-channels convolutional neural networks[J].Journal of Computer Research and Development, 2018, 55 (5) :945- 957 (in Chinese) (陈珂, 梁斌, 柯文德, 等.基于多通道卷积神经网络的中文微博情感分析[J].计算机研究与发展, 2018, 55 (5) :945- 957) 
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_13" title="Poria S, Cambria E, Gelbukh A.Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis[C] //Proc of the 2015 Conf on Empirical Methods in Natural Language Processing (EMNLP) .Stroudsburg, PA:ACL, 2015:2539- 2544" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis">
                                        <b>[13]</b>
                                        Poria S, Cambria E, Gelbukh A.Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis[C] //Proc of the 2015 Conf on Empirical Methods in Natural Language Processing (EMNLP) .Stroudsburg, PA:ACL, 2015:2539- 2544
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_14" title="You Quanzeng, Luo Jiebo, Jin Hailin, et al.Robust image sentiment analysis using progressively trained and domain transferred deep networks[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park:AAAI, 2015:381- 388" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust image sentiment analysis using progressively trained and domain transferred deep networks">
                                        <b>[14]</b>
                                        You Quanzeng, Luo Jiebo, Jin Hailin, et al.Robust image sentiment analysis using progressively trained and domain transferred deep networks[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park:AAAI, 2015:381- 388
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_15" title="Campos V, Salvador A, Giro-I-Nieto X, et al.Diving deep into sentiment:Understanding fine-tuned CNNs for visual sentiment prediction[C] //Proc of the 1st Int Workshop on Affect &amp;amp; Sentiment in Multimedia.New York:ACM, 2015:57- 62" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diving deep into sentiment:Understanding fine-tuned cnns for visual sentiment prediction">
                                        <b>[15]</b>
                                        Campos V, Salvador A, Giro-I-Nieto X, et al.Diving deep into sentiment:Understanding fine-tuned CNNs for visual sentiment prediction[C] //Proc of the 1st Int Workshop on Affect &amp;amp; Sentiment in Multimedia.New York:ACM, 2015:57- 62
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_16" title="Campos V, Jou B, Giro-i-Nieto X.From pixels to sentiment:Fine-tuning cnns for visual sentiment prediction[J].Image and Vision Computing, 2017, 65:15- 22" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES948D849EDE4B66128654B3F384B0824D&amp;v=MDM0NzNmT0dRbGZDcGJRMzVOeGd6THE2d2F3PU5pZk9mYnE4RnFYRXE0WXdFSjRMZm5vL3poUWI3RHA1T255VXJ4b3hDN0tjUjc3ckNPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Campos V, Jou B, Giro-i-Nieto X.From pixels to sentiment:Fine-tuning cnns for visual sentiment prediction[J].Image and Vision Computing, 2017, 65:15- 22
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_17" title="Andrew G, Arora R, Bilmes J, et al.Deep canonical correlation analysis[C] //Proc of the 2013 Int Conf on Machine Learning.Atlant, GA:Machine Learning Society, 2013:1247- 1255" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep canonical correlation analysis">
                                        <b>[17]</b>
                                        Andrew G, Arora R, Bilmes J, et al.Deep canonical correlation analysis[C] //Proc of the 2013 Int Conf on Machine Learning.Atlant, GA:Machine Learning Society, 2013:1247- 1255
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_18" title="Dorfer M, Kelz R, Widmer G.Deep linear discriminant analysis[J].arXiv preprint, arXiv:1511.04707, 2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep linear discriminant analysis">
                                        <b>[18]</b>
                                        Dorfer M, Kelz R, Widmer G.Deep linear discriminant analysis[J].arXiv preprint, arXiv:1511.04707, 2015
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_19" title="Borth D, Ji Rongrong, Chen Tao, et al.Large-scale visual sentiment ontology and detectors using adjective noun pairs[C] //Proc of the 21st ACM Int Conf on Multimedia.New York:ACM, 2013:223- 232" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale visual sentiment ontology and detectors using adjective noun pairs">
                                        <b>[19]</b>
                                        Borth D, Ji Rongrong, Chen Tao, et al.Large-scale visual sentiment ontology and detectors using adjective noun pairs[C] //Proc of the 21st ACM Int Conf on Multimedia.New York:ACM, 2013:223- 232
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_20" title="Atrey P K, Hossain M A, El Saddik A, et al.Multimodal fusion for multimedia analysis:A survey[J].Multimedia Systems, 2010, 16 (6) :345- 379" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003726909&amp;v=MjM0MTgzTklWbz1OajdCYXJPNEh0SFBxSTFEYmVzR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkN6a1c3&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        Atrey P K, Hossain M A, El Saddik A, et al.Multimodal fusion for multimedia analysis:A survey[J].Multimedia Systems, 2010, 16 (6) :345- 379
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_21" title="Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate [J].arXiv preprint, arXiv:1409.0473, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">
                                        <b>[21]</b>
                                        Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate [J].arXiv preprint, arXiv:1409.0473, 2014
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_22" title="Yang Zicao, He Xiaodong, Gao Jianfeng, et al.Stacked attention networks for image question answering[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:21- 29" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked attention networks for image question answering">
                                        <b>[22]</b>
                                        Yang Zicao, He Xiaodong, Gao Jianfeng, et al.Stacked attention networks for image question answering[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:21- 29
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_23" title="Vinyals O, Toshev A, Bengio S, et al.Show and tell:A neural image caption generator[C] //Proc of the 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ :IEEE, 2015:3156- 3164" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show and tell:A neural image caption generator">
                                        <b>[23]</b>
                                        Vinyals O, Toshev A, Bengio S, et al.Show and tell:A neural image caption generator[C] //Proc of the 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ :IEEE, 2015:3156- 3164
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_24" title="Siersdorfer S, Minack E, Deng F, et al.Analyzing and predicting sentiment of images on the social Web[C] //Proc of the 18th ACM Int Conf on Multimedia.New York:ACM, 2010:715- 718" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analyzing and predicting sentiment of images on the social web">
                                        <b>[24]</b>
                                        Siersdorfer S, Minack E, Deng F, et al.Analyzing and predicting sentiment of images on the social Web[C] //Proc of the 18th ACM Int Conf on Multimedia.New York:ACM, 2010:715- 718
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_25" title="Jou B, Chen Tao, Pappas N, et al.Visual affect around the world:A large-scale multilingual visual sentiment ontology[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:159- 168" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual affect around the world:A large-scale multilingual visual sentiment ontology">
                                        <b>[25]</b>
                                        Jou B, Chen Tao, Pappas N, et al.Visual affect around the world:A large-scale multilingual visual sentiment ontology[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:159- 168
                                    </a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_26" title="Li Zuhe, Fan Yangyu, Liu Weihua, et al.Image sentiment prediction based on textual descriptions with adjective noun pairs[J].Multimedia Tools and Applications, 2018, 77 (1) :1115- 1132" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image sentiment prediction based on textual descriptions with adjective noun pairs">
                                        <b>[26]</b>
                                        Li Zuhe, Fan Yangyu, Liu Weihua, et al.Image sentiment prediction based on textual descriptions with adjective noun pairs[J].Multimedia Tools and Applications, 2018, 77 (1) :1115- 1132
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_27" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition [J].arXiv preprint, arXiv:1409.1556, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[27]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition [J].arXiv preprint, arXiv:1409.1556, 2014
                                    </a>
                                </li>
                                <li id="348">


                                    <a id="bibliography_28" title="Kim Y.Convolutional neural networks for sentence classification [J].arXiv preprint, arXiv:1408.5882, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">
                                        <b>[28]</b>
                                        Kim Y.Convolutional neural networks for sentence classification [J].arXiv preprint, arXiv:1408.5882, 2014
                                    </a>
                                </li>
                                <li id="350">


                                    <a id="bibliography_29" title="Ruder S.An overview of gradient descent optimization algorithms [J].arXiv preprint, arXiv:1609.04747, 2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An overview of gradient descent optimization algorithms">
                                        <b>[29]</b>
                                        Ruder S.An overview of gradient descent optimization algorithms [J].arXiv preprint, arXiv:1609.04747, 2016
                                    </a>
                                </li>
                                <li id="352">


                                    <a id="bibliography_30" title="Islam J, Zhang Yangqing.Visual sentiment analysis for social images using transfer learning approach [C] //Proc of the 2016 IEEE Int Conf on Big Data and Cloud Computing.Piscataway, NJ:IEEE, 2016:124- 130" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual sentiment analysis for social images using transfer learning approach">
                                        <b>[30]</b>
                                        Islam J, Zhang Yangqing.Visual sentiment analysis for social images using transfer learning approach [C] //Proc of the 2016 IEEE Int Conf on Big Data and Cloud Computing.Piscataway, NJ:IEEE, 2016:124- 130
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(06),1312-1324 DOI:10.7544/issn1000-1239.2019.20180341            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于层次化深度关联融合网络的社交媒体情感分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%94%A1%E5%9B%BD%E6%B0%B8&amp;code=10801134&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蔡国永</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E5%85%89%E7%91%9E&amp;code=38732407&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕光瑞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E6%99%BA&amp;code=37730361&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐智</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E8%A5%BF%E5%8F%AF%E4%BF%A1%E8%BD%AF%E4%BB%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6)&amp;code=0269119&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广西可信软件重点实验室(桂林电子科技大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有的多数情感分析研究都是基于单一文本或视觉数据, 效果还不够理想, 多模态数据由于能够提供更丰富的信息, 因此多模态情感分析正受到越来越多的关注.社交媒体上视觉数据常常和与之共现的文本数据存在较强的语义关联, 因此混合图文的多模态情感分类为社交媒体情感分析提供了新的视角.为了解决图文之间的精细语义配准问题, 提出了一种基于层次化深度关联融合网络的多媒体数据情感分类模型.该模型不仅利用图像的中层语义特征, 还利用多模态深度多重判别性相关分析来学习最大相关的图像视觉特征表示和文本语义特征表示, 而且使形成的视觉特征表示和语义特征表示均具有线性判别性.在此基础上, 提出合并图像视觉特征表示和文本语义特征表示的多模态注意力融合网络, 以进一步改进情感分类器.最后, 在来自于社交网络的真实数据集上的大量实验结果表明, 通过层次化捕获视觉情感特征和文本情感特征之间的内部关联, 可以更准确地实现对图文融合社交媒体的情感分类预测.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A4%BE%E4%BA%A4%E5%AA%92%E4%BD%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">社交媒体;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%85%B3%E8%81%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度关联;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%A4%E5%88%AB%E6%80%A7%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">判别性相关分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A8%A1%E6%80%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多模态注意力融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    蔡国永, ccgycai@guet.edu.cn, born in 1971. PhD, professor.Senior member of CCF.His main research interests include social media mining, sentiment analysis, machine learning.;
                                </span>
                                <span>
                                    *吕光瑞, lvguangrui_rio@163.com, born in 1989.PhD candidate.His main research interests include natural language processing, multimodal sentiment analysis, and deep learning.;
                                </span>
                                <span>
                                    Xu Zhi, born in 1977.PhD, associate professor.Member of CCF.His main research interests include pattern recognition, image classification, and machine learning.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61763007, 66162014);</span>
                                <span>广西自然科学基金重点项目 (2017JJD160017);</span>
                                <span>广西可信软件重点实验室项目 (201503);</span>
                    </p>
            </div>
                    <h1><b>A Hierarchical Deep Correlative Fusion Network for Sentiment Classification in Social Media</b></h1>
                    <h2>
                    <span>Cai Guoyong</span>
                    <span>Lü Guangrui</span>
                    <span>Xu Zhi</span>
            </h2>
                    <h2>
                    <span>Guangxi Key Laboratory of Trusted Software (Guilin University of Electronic Technology)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Most existing research of sentiment analysis are based on either textual or visual data and can not achieve satisfied results. As multi-modal data can provide richer information, multi-modal sentiment analysis is attracting more and more attentions and has become a hot research topic. Due to the strong semantic correlation between visual data and the co-occurrence textual data in social media, mixed data of texts and images provides a new view to learn better classifier for social media sentiment classification. A hierarchical deep correlative fusion network framework is proposed to jointly learn textual and visual sentiment representations from training samples for sentiment classification. In order to alleviate the problem of fine-grained semantic matching between image and text, both the middle level semantic features of images and the deep multi-modal discriminative correlation analysis are applied to learn the most relevant visual feature representation and semantic feature representation, meanwhile, keeping both the visual and semantic feature representations to be linear discriminable. Motivated by the successful use of attention mechanisms, we further propose a multi-modal attention fusion network by incorporating visual and semantic feature representations to train sentiment classifier. Experiments on the real-world datasets which come from social networks show that, the proposed method gets more accurate prediction on multi-media sentiment analysis by capturing the internal relations between text and image hierarchically.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=social%20media&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">social media;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sentiment%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sentiment analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20correlation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep correlation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=discriminant%20correlation%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">discriminant correlation analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-modal%20attention%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-modal attention fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61763007, 66162014);</span>
                                <span>the Natural Science Foundation of Guangxi Province of China (2017JJD160017);</span>
                                <span>the Project of the Guangxi Key Laboratory of Trusted Software (201503);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="66">随着移动互联网和智能终端的快速发展, 社交媒体上的用户生成内容变得越来越多样化, 社交媒体数据已不再仅限于单一的文本形式, 例如越来越多的社交用户倾向于使用图像和短文本这种多模态内容的形式来表达他们的观点和在社交媒体上相互交流.这些大量社交用户分享的多模态数据为人们提供了探索众多话题的情感和观点的宝库, 因此多模态情感分析已经成为一个重要的研究热点<citation id="354" type="reference"><link href="294" rel="bibliography" /><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><link href="300" rel="bibliography" /><link href="302" rel="bibliography" /><link href="304" rel="bibliography" /><link href="306" rel="bibliography" /><link href="308" rel="bibliography" /><link href="310" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>, 但是大规模多模态社交媒体数据的情感分析还是一个充满挑战的任务.</p>
                </div>
                <div class="p1">
                    <p id="67">早期的情感研究较多关注单一的文本或图像, 且采用传统的机器学习分类算法.近年来, 鉴于深度学习技术的优异表现, 越来越多的研究人员倾向于使用深度神经网络来学习文本的分布式和稳健的特征表示用于情感分类<citation id="355" type="reference"><link href="312" rel="bibliography" /><link href="314" rel="bibliography" /><link href="316" rel="bibliography" /><link href="318" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>.与此同时, 卷积神经网络 (convolutional neural network, CNN) 能够自动地从大规模图像数据中学习稳健的特征且展示了优异的性能, 一些研究者开始探索基于CNN的图像情感分析<citation id="356" type="reference"><link href="320" rel="bibliography" /><link href="322" rel="bibliography" /><link href="324" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>.最近, 在多模态情感分析的研究中<citation id="357" type="reference"><link href="294" rel="bibliography" /><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><link href="300" rel="bibliography" /><link href="302" rel="bibliography" /><link href="304" rel="bibliography" /><link href="306" rel="bibliography" /><link href="308" rel="bibliography" /><link href="310" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>, 利用深度神经网络的方法在性能上也更优异.多模态情感分析是融合多种模态的信息进行统一的分类预测任务, 其关键的问题是多模态样本特征的融合.由于不同模态的异质性, 模态之间特征的融合是较困难的.尽管基于深度网络相关的模型已经取得了不错的进展, 但是基于深度网络的融合模型仍需要进一步深入研究.</p>
                </div>
                <div class="p1">
                    <p id="68">为了克服已有的图像-文本的多媒体情感分析研究中存在的异构模态的特征融合方式相对简单以及单一图像处理上仅从图像自身提取特征等不足, 本文的主要贡献有4个方面:</p>
                </div>
                <div class="p1">
                    <p id="69">1) 在图像的处理上利用迁移学习策略和图像中层语义特征相结合的方法来构建具有一定语义的视觉情感特征表示.</p>
                </div>
                <div class="p1">
                    <p id="70">2) 结合深度典型相关分析 (deep canonical cor-relation analysis, DCCA) <citation id="358" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和深度线性判别分析 (deep linear discriminant analysis, DeepLDA) <citation id="359" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>的思想提出多模态深度多重判别性相关分析的联合优化目标, 通过优化生成最大相关的判别性视觉特征和判别性语义特征以构建图像和文本在特征层次上的语义相关, 且使特征具有判别性的能力, 从而提升语义配准.</p>
                </div>
                <div class="p1">
                    <p id="71">3) 提出基于多模态协同注意力网络的融合方法, 能进一步序列化地交互图像的视觉特征和文本的语义特征, 从而更好地匹配融合多模态特征.</p>
                </div>
                <div class="p1">
                    <p id="72">4) 在多个数据集上的对比实验表明, 本文提出的层次化深度关联融合的网络模型在情感分类任务中能取得更好的分类效果.</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="74">多模态情感分析的研究尚且处于初期阶段, 大致可以分为2类.较早的研究以特征选择模型为主, 最近开始基于深度神经网络模型展开研究.</p>
                </div>
                <div class="p1">
                    <p id="75">Wang等人<citation id="360" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>利用统一的跨媒体词袋模型来表示文本特征和图像特征, 且利用机器学习的方法来预测融合后的情感, 结果表明跨模态情感分类结果要略优于单模态的情感分类结果.Cao等人<citation id="361" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>融合来自于形容词名词对 (adjective noun pairs, ANPs) <citation id="362" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的图像中层视觉特征的预测结果和由情感词、情感标签和句子结构规则组成的文本特征的预测结果, 其中图像和文本的融合权重是通过参数来控制, 最后用于微博的公共情感分析.Poria等人<citation id="363" type="reference"><link href="298" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>通过使用特征级的和决策级的融合方法合并来自于多模态的情感信息.Katsurai等人<citation id="364" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>首先构筑视觉特征、文本特征和情感特征, 然后利用映射矩阵映射视觉、文本、情感这3个模态的数据到一个共同的潜在嵌入空间中, 认为潜在空间中的映射特征是来自于不同模态的互补信息从而被用于训练情感分类器.</p>
                </div>
                <div class="p1">
                    <p id="76">最近深度学习方法应用于多模态情感预测也备受关注.如Cai等人<citation id="365" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>利用2个单独的CNN结构分别学习文本特征表示和图像特征表示, 将其合并后输入另外的CNN结构以进行多媒体的情感分析.Yu等人<citation id="366" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>也利用2个CNN结构分别提取文本和图像的特征表示, 使用逻辑回归对文本的和图像的特征表示进行情感预测, 最后使用平均策略和加权的方法融合概率结果.Baecchi等人<citation id="367" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出基于连续词袋模型和降噪自动编码的多模态特征的学习模型以进行Twitter数据情感分析, 当然该模型也可应用到其他的社交媒体数据上.You等人<citation id="368" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出跨模态一致回归的方法用于结合视觉和文本的情感分析, 该方法利用深度视觉的和文本的特征构建回归模型.而Xu等人<citation id="369" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>利用卷积网络的结构来提取图像和文本的特征表示, 然后利用残差的模型来合并图像和文本的多模态特征用于情感分析.</p>
                </div>
                <div class="p1">
                    <p id="77">尽管这些模型都是有效的, 但是大多都独立地使用视觉和文本的信息, 且在融合过程中往往忽略了图像和文本之间的内在关联.通常, 组合不同模态数据的多模态融合方法可以分为早融合、后融合、混合融合<citation id="370" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.其中, 后融合涉及为每种模态数据构建相应的分类器, 然后结合这些决策进行预测;而早融合需要将不同模态的特征融合到单个分类器中.本文的研究仍属于特征层的融合, 但是不同于已有的研究方法, 本文工作的关注点有2个方面:1) 同时处理图像和与之共现的文本信息;2) 在多模态深度网络的结构中, 利用层次化深度关联融合的方法来探究图像和文本之间的语义关联.首先, 本文整合DCCA<citation id="371" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和DeepLDA<citation id="372" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>到一个统一的联合多模态优化目标中, 以此构建图像和与之共现的文本在特征层次上的语义关联, 且使各自生成的特征具有较好的判别性.此外, 最近注意力模块已经成为应用于各种任务的现代神经系统的组成部分, 比如机器翻译<citation id="373" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、图像问答任务<citation id="374" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和图像标题生成<citation id="375" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>等, 然而很少的研究工作已经利用注意力机制进行融合, 本文提出基于协同注意力 (co-attention) 机制的多模态融合策略, 用于训练情感分类器.</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>2 方法描述</b></h3>
                <div class="p1">
                    <p id="79">本节介绍提出的用于多模态情感分析任务的层次化深度关联融合的网络模型, 整体结构如图1所示, 总共由5个部分构成:①视觉模态特征提取网络;②文本模态特征提取网络;③多模态深度多重判别性相关分析;④co-attention网络的多模态注意力融合模型;⑤分类网络.</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906019_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于层次化深度关联融合网络的社交媒体多模态情感分类框架图" src="Detail/GetImg?filename=images/JFYZ201906019_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于层次化深度关联融合网络的社交媒体多模态情感分类框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906019_080.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Framework of hierarchical deep correlative fusion network for multi-modal sentiment classification</p>

                </div>
                <div class="p1">
                    <p id="81">基于层次化深度关联融合网络的多模态情感分类模型首先利用图1中①②的多模态特征提取网络逐层提取视觉模态和文本模态的特征, 得到相对应的顶层特征表示, 然后通过图1中③进一步生成最大相关的判别性特征表示, 最后使用图1中④的co-attention网络来交互合并这2种特征表示并传递到图1中⑤的全连接神经网络 (fully connected neural network, FCNN) 中进一步深层融合后再用于训练情感分类器.下面阐述模型的细节.</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>2.1 视觉模态特征提取网络</b></h4>
                <div class="p1">
                    <p id="83">尽管已有学者在情感分析相关研究上探测过图像视觉特征<citation id="376" type="reference"><link href="320" rel="bibliography" /><link href="322" rel="bibliography" /><link href="324" rel="bibliography" /><link href="340" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">24</a>]</sup></citation>或者图像中层语义特征<citation id="377" type="reference"><link href="330" rel="bibliography" /><link href="342" rel="bibliography" /><link href="344" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>, 但是仅从单一视觉特征或中层语义特征的角度来构筑视觉情感特征, 并不能构筑完整的且易于理解的图像视觉特征.本文同时从图像特征提取和图像中层语义特征提取的角度来学习高层次的视觉情感表示, 如图2中①所示.</p>
                </div>
                <div class="p1">
                    <p id="84">图像的特征提取是基于VGG<citation id="378" type="reference"><link href="346" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>展开的, 其由5个卷积块和3个全连接层组成, 且已经在1 000个目标分类的ImageNet数据集上表现出了极好的性能.本文利用迁移学习的策略来克服ImageNet数据集和图像情感数据集的不同差异.首先, VGG16模型在ImageNet的数据集上训练好, 然后迁移已经学习好的参数到情感分析的目标中.在提出的模型中, 修改最后用于目标分类的全连接层为特征映射层, 然后提取该全连接层的特征输出, 如图2中① (a-1) 所示.</p>
                </div>
                <div class="p1">
                    <p id="85">为了提取更全面的图像中层语义特征, 首先划分每一个图像对应的中层语义特征 (ANP) 为形容词和名词, 然后通过CNN来分别提取图像的形容词描述性特征和名词客观性特征.针对形容词和名词的特征提取网络, CNN采用的是二维卷积, 每一个形容词或名词的样本像单通道图像一样被调整为50×50的大小, 利用2个平行的子网络, 即图2中① (a-2) 中A-net和N-net, 其分别由同样的卷积层和全连接层组成.</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906019_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 视觉和文本的多模态深度多重判别性相关分析图解" src="Detail/GetImg?filename=images/JFYZ201906019_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 视觉和文本的多模态深度多重判别性相关分析图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906019_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Schematic sketch of deep multi-modal multi-discriminative correlation analysis to learn the visual and textual content</p>

                </div>
                <div class="p1">
                    <p id="87">总之, 在视觉模态特征提取上, 本文提出联合学习图像ANP的形容词和名词以及图像特征以构筑具有一定语义的视觉情感特征表示, 以此缓解图像视觉特征和文本语义特征之间的语义鸿沟.后文中将称视觉模态特征提取网络为<i>f</i>.</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>2.2 文本模态特征提取网络</b></h4>
                <div class="p1">
                    <p id="89">文本模态特征提取网络是由词向量输入层、卷积层、双向长短时记忆网络 (Bi-LSTM) 层和全连接层组成, 如图2中②所示.</p>
                </div>
                <div class="p1">
                    <p id="90">假设<b><i>x</i></b><sub><i>i</i></sub>∈R<sup><i>k</i></sup>是句子中第<i>i</i>个词对应的<i>k</i>维词向量, 则一个长度为<i>n</i>的句子表示为</p>
                </div>
                <div class="p1">
                    <p id="91"><b><i>x</i></b><sub>1:<i>n</i></sub>=<b><i>x</i></b><sub>1</sub>♁<b><i>x</i></b><sub>2</sub>♁…♁<b><i>x</i></b><sub><i>n</i></sub>,      (1) </p>
                </div>
                <div class="p1">
                    <p id="92">其中, ♁表示连接操作, 在句子矩阵<b><i>x</i></b><sub>1:<i>n</i></sub>上利用一个单层的CNN<citation id="379" type="reference"><link href="348" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, 它的卷积层包含高度分别为<i>h</i><sub>1</sub>, <i>h</i><sub>2</sub>, <i>h</i><sub>3</sub>的3个滤波器<b><i>F</i></b><sub>1</sub>∈R<sup><i>h</i><sub>1</sub>×<i>k</i></sup>, <b><i>F</i></b><sub>2</sub>∈R<sup><i>h</i><sub>2</sub>×<i>k</i></sup>, <b><i>F</i></b><sub>3</sub>∈R<sup><i>h</i><sub>3</sub>×<i>k</i></sup>.每个滤波器<b><i>F</i></b><sub><i>i</i></sub>在输入的句子序列上进行滑动, 当<b><i>F</i></b><sub><i>i</i></sub>应用到整个句子矩阵中每一个可能的<i>h</i><sub><i>i</i></sub>窗口的词上时, 就会产生一个特征映射<b><i>c</i></b><sub><i>i</i></sub>∈R<sup><i>n</i>-<i>h</i><sub><i>i</i></sub>+1</sup>, 其中某一项窗口的词的特征映射<b><i>c</i></b><sub><i>i</i>, <i>j</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="93"><b><i>c</i></b><sub><i>i</i>, <i>j</i></sub>=<i>δ</i> (<b><i>F</i></b><sub><i>i</i></sub>*<b><i>x</i></b><sub>[<i>j</i>:<i>j</i>+<i>h</i><sub><i>i</i></sub>-1]</sub>+<i>b</i><sub><i>i</i></sub>) ,      (2) </p>
                </div>
                <div class="p1">
                    <p id="94">这里*是卷积操作, <i>j</i>=1, 2, …, <i>n</i>-<i>h</i><sub><i>i</i></sub>+1, <i>b</i><sub><i>i</i></sub>∈R是一个偏置项, <i>δ</i> (·) 是一个非线性激活函数.每一个滤波器<b><i>F</i></b><sub><i>i</i></sub>能够生成<i>M</i>个这样的特征映射, 因此总共获得了3<i>M</i>个特征映射.然后, 在滤波器<b><i>F</i></b><sub><i>i</i></sub>的<i>M</i>个特征映射向量的每一个长度上应用最大池化操作, 则结果输出向量为<b><i>o</i></b><sub><i>i</i></sub>∈R<sup><i>M</i></sup>, 具体表示为</p>
                </div>
                <div class="p1">
                    <p id="95"><b><i>o</i></b><sub><i>i</i></sub>= (max (<b><i>c</i></b><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></math></mathml>) , max (<b><i>c</i></b><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>) , …, max (<b><i>c</i></b><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>Μ</mi></msubsup></mrow></math></mathml>) ) .      (3) </p>
                </div>
                <div class="p1">
                    <p id="99">通过♁连接每一个<b><i>o</i></b><sub><i>i</i></sub>得到<b><i>o</i></b>= (<b><i>o</i></b><sub>1</sub>♁<b><i>o</i></b><sub>2</sub>♁<b><i>o</i></b><sub>3</sub>) ∈R<sup>3<i>M</i></sup>.然后将<b><i>o</i></b>输入Bi-LSTM网络, 从正向和反向的角度来使用已提取的特征从而更好地学习输入的文本序列.最后, 经过对文本的序列建模后, 将Bi-LSTM的输出传递给全连接的神经网络以更好地融合时序特征以形成更容易被区分的高层特征表示.后文中称文本模态特征提取网络为<i>g</i>.</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>2.3 多模态深度多重判别性相关分析</b></h4>
                <div class="p1">
                    <p id="101">本文提出的多模态深度多重判别性相关分析是基于典型相关分析 (canonical correlation analysis, CCA) 和线性判别分析 (linear discriminant analysis, LDA) 展开的.两者都来自经典的多元统计, 都依赖于各自输入特征分布的协方差结构.不同之处在于, CCA是一种适用于多模态数据的分析方法, 但是它既没有考虑标签信息也不能对各自模态的内部信息进行分析;而LDA是一种利用标签信息的且适用于单模态数据的分析方法, 但是它不能直接地应用到多模态数据分析上, 因此可将两者结合起来, 以充分发掘各自模型的优势, 从而形成一个在多模态学习过程中既探究不同模态之间的最大相关性又兼顾各自模态最大判别性的多模态数据处理方法.</p>
                </div>
                <div class="p1">
                    <p id="102">多模态的样本数据往往来自于异构特征空间, 不同模态数据的特征分布差异较大, 此时如果将异构特征融合后再进行LDA, 较难取得好的效果.例如, 那些来自于社交网站的图像和文本, 如果直接将图像和文本的特征融合后再用于LDA, 这既没有考虑图像和文本的对应关联也没有考虑图像和文本各自特征分布的差异.因此本文将在考虑不同模态之间相关性的同时, 也尽量考虑不同模态之间的特征分布的差异, 即在寻求视觉模态和文本模态最大相关性的同时, 兼顾视觉模态和文本模态各自的线性判别性.多模态深度多重判别性相关分析方法包含2部分:相关性分析部分和判别性分析部分.</p>
                </div>
                <div class="p1">
                    <p id="103">在描述方法之前, 首先给出相关变量的数学定义形式.令{ (<b><i>x</i></b><sup><i>p</i></sup><sub><i>i</i></sub>, <b><i>x</i></b><sup><i>q</i></sup><sub><i>i</i></sub>) }<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>表示一系列<i>N</i>对的图像-文本观察值, 其中{<b><i>x</i></b><sup><i>p</i></sup><sub><i>i</i></sub>}<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>=<b><i>X</i></b><sup><i>p</i></sup>∈R<sup><i>N</i>×<i>d</i><sub><i>p</i></sub></sup>, {<b><i>x</i></b><sup><i>q</i></sup><sub><i>i</i></sub>}<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>=<b><i>X</i></b><sup><i>q</i></sup>∈R<sup><i>N</i>×<i>d</i><sub><i>q</i></sub></sup>, 且属于<i>C</i>个不同的类<i>c</i>∈{<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, …, <i>k</i><sub><i>C</i></sub>}, 这里上标<i>p</i>和<i>q</i>分别表示图像和文本, 即视觉特征向量<b><i>x</i></b><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow></math></mathml>表示第<i>i</i>个对中的图像, 同理<b><i>x</i></b><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup></mrow></math></mathml>表示对应于图像<b><i>x</i></b><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow></math></mathml>的文本特征向量.此外, 当在视觉模态上同时利用图像特征和图像的中层语义特征共同学习时, 视觉模态上是3个输入, 这里不再形式化表示, 可依照图像-文本对的形式化表示类推.利用2.1节设计的视觉模态特征提取网络<i>f</i>和2.2节设计的文本模态特征提取网络<i>g</i>作为非线性特征映射来处理输入的数据{ (<b><i>x</i></b><sup><i>p</i></sup><sub><i>i</i></sub>, <b><i>x</i></b><sup><i>q</i></sup><sub><i>i</i></sub>) }<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>.图像和文本分别通过<i>f</i>和<i>g</i>这2个不同的神经网络生成各自的顶层特征表示<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) ∈R<sup><i>N</i>×<i>L</i></sup>和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ∈R<sup><i>N</i>×<i>L</i></sup>.设<i>f</i>和<i>g</i>这2个不同神经网络的学习参数 (<b><i>W</i></b><sup><i>p</i></sup>;<b><i>b</i></b><sup><i>p</i></sup>) 和 (<b><i>W</i></b><sup><i>q</i></sup>;<b><i>b</i></b><sup><i>q</i></sup>) 的集合分别表示为<i>θ</i><sup><i>p</i></sup>和<i>θ</i><sup><i>q</i></sup>, 且固定<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 的维度是相同的, 记为<i>L</i>.则多模态深度多重判别性相关分析的模型框架简单描述为</p>
                </div>
                <div class="p1">
                    <p id="111"><i>J</i> (<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) , <i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ) =<i>C</i> (<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) , <i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ) +[<i>D</i> (<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) ) +<i>D</i> (<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ) ],      (4) </p>
                </div>
                <div class="p1">
                    <p id="113">其中, <i>J</i> (<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) , <i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ) 表示<i>p</i>, <i>q</i>模态间的多重判别性相关分析的目标函数, <i>C</i> (<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) , <i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ) 表示两者模态间的相关性分析项, <i>D</i> (<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) ) 和<i>D</i> (<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) ) 分别表示各自模态内部的判别性分析项.</p>
                </div>
                <div class="p1">
                    <p id="114">本文以式 (4) 为基准来设计模型, 即从不同模态之间来考虑多重判别性的相关分析, 下面分别对模型中的各项进行阐述.</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">2.3.1 多模态深度相关性分析</h4>
                <div class="p1">
                    <p id="116">Andrew等人<citation id="380" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出基于CCA的端到端的深度神经网络的解释方法DCCA, 其优化目标是推动多模态网络学习高度关联的特征表示.受到DCCA方法的启发, 本文在自定义的多模态深度网络结构<i>f</i>和<i>g</i>下来学习视觉模态和文本模态间的相关性, 称为Multi-DCCA.</p>
                </div>
                <div class="p1">
                    <p id="117">在CCA中, 首先通过预处理的操作, 分别使<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 变成中心数据矩阵, 表示为</p>
                </div>
                <div class="p1">
                    <p id="118"><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mn>1</mn></mrow></math></mathml>,      (5) </p>
                </div>
                <div class="p1">
                    <p id="120"><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>g</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo><mn>1</mn></mrow></math></mathml>,      (6) </p>
                </div>
                <div class="p1">
                    <p id="122">其中, <i>N</i>表示数据的总数, <b>1</b>∈R<sup><i>N</i>×<i>N</i></sup>表示元素全为1的矩阵.</p>
                </div>
                <div class="p1">
                    <p id="123">视觉模态和文本模态的顶层特征表示的正则化自协方差矩阵, 分别表示为</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>p</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mi>r</mi><msub><mrow></mrow><mi>p</mi></msub><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>q</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mover><mstyle mathsize="140%" displaystyle="true"><mi>g</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>g</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mi>r</mi><msub><mrow></mrow><mi>q</mi></msub><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">其中, <i>r</i><sub><i>p</i></sub>, <i>r</i><sub><i>q</i></sub>是正则化参数, 是为了确保协方差有积极的定义;<b><i>I</i></b>是单位矩阵.</p>
                </div>
                <div class="p1">
                    <p id="126">除了领域自身的方差外, 不同领域学习到的特征表示的交叉协方差矩阵为</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>g</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">基于CCA中介绍的协方差矩阵<i>Σ</i><sub><i>pp</i></sub>, <i>Σ</i><sub><i>pq</i></sub>, <i>Σ</i><sub><i>qq</i></sub>, 定义矩阵<b><i>T</i></b>=<i>Σ</i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>p</mi><mi>p</mi></mrow><mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup></mrow></math></mathml><i>Σ</i><sub><i>pq</i></sub><i>Σ</i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>q</mi><mi>q</mi></mrow><mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msubsup></mrow></math></mathml>.然后<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 的总体关联通过相对应的奇异值问题<b><i>T</i></b>=<b><i>U</i></b><sub><i>p</i></sub><i>Λ</i><b><i>U</i></b><sub><i>q</i></sub>和<i>Λ</i>=diag (<i>d</i>) 中的奇异值<i>d</i>的求和来计算.<b><i>U</i></b><sub><i>p</i></sub>和<b><i>U</i></b><sub><i>q</i></sub>分别是转化视觉模态和文本模态到线性CCA子空间的映射矩阵.故Multi-DCCA的相关分析是在<i>f</i>和<i>g</i>相对应的网络参数<i>θ</i><sup><i>p</i></sup>和<i>θ</i><sup><i>q</i></sup>下最大化奇异值<i>d</i>的和, 即:</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mo>, </mo><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>p</mi></msup><mo>, </mo><mspace width="0.25em" /><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>q</mi></msup></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">在网络<i>f</i>和<i>g</i>有相同的特征维度<i>L</i>时, 也可以通过最大化矩阵迹的范数<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">Τ</mi><mo>|</mo></mrow><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mtext>t</mtext><mtext>r</mtext></mrow></math></mathml> ( (<b><i>T</i></b><sup>T</sup><b><i>T</i></b>) <sup>1/2</sup>) 来优化典型关联.</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">2.3.2 多模态深度判别性分析</h4>
                <div class="p1">
                    <p id="135">Dorfer等人<citation id="381" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出基于LDA的端到端的深度神经网络的解释方法DeepLDA, 其优化目标是推动网络在顶层表示上学习线性可分的潜在空间.受到 DeepLDA的启发, 本文在视觉模态特征提取网络<i>f</i>的顶层和文本模态特征提取网络<i>g</i>的顶层同时学习可以最大化<i>C</i>个不同的多模态数据类别之间区分的潜在表示, 称为Multi-DeepLDA.</p>
                </div>
                <div class="p1">
                    <p id="136">对于LDA而言, <i>Σ</i><sub><i>pp</i></sub>可作为视觉模态的总体离散度矩阵, 同理<i>Σ</i><sub><i>qq</i></sub>可作为文本模态的总体离散度矩阵.此外, 由于图像-文本对的标签属于<i>C</i>个不同的类<i>c</i>∈{<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, …, <i>k</i><sub><i>C</i></sub>}, 则LDA还需要<i>C</i>个不同类别中每个类别的视觉模态和文本模态的协方差矩阵<i>Σ</i><sub><i>pc</i></sub>, <i>Σ</i><sub><i>qc</i></sub>, 以及视觉模态和文本模态中所有不同类协方差矩阵的均值<i>Σ</i><sub><i>p</i>w</sub>, <i>Σ</i><sub><i>q</i>w</sub>, 即类内离散度矩阵, 分别表示为</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>c</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>c</mi></msub><mo>-</mo><mn>1</mn></mrow></mfrac><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>c</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>c</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mi>r</mi><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>c</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>c</mi></msub><mo>-</mo><mn>1</mn></mrow></mfrac><mover><mstyle mathsize="140%" displaystyle="true"><mi>g</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>c</mi><mi>q</mi></msubsup><mo stretchy="false">) </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>g</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>c</mi><mi>q</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mi>r</mi><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mtext>w</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>C</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>c</mi></munder><mi mathvariant="bold-italic">Σ</mi></mstyle><msub><mrow></mrow><mrow><mi>p</mi><mi>c</mi></mrow></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>q</mi><mtext>w</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>C</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>c</mi></munder><mi mathvariant="bold-italic">Σ</mi></mstyle><msub><mrow></mrow><mrow><mi>q</mi><mi>c</mi></mrow></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">其中, <i>r</i>是正则化参数, 是为了确保协方差有积极的定义.</p>
                </div>
                <div class="p1">
                    <p id="139">最后, 通过总体离散度矩阵<i>Σ</i><sub><i>pp</i></sub>, <i>Σ</i><sub><i>qq</i></sub>和类内离散度矩阵<i>Σ</i><sub><i>p</i>w</sub>, <i>Σ</i><sub><i>q</i>w</sub>来定义视觉模态和文本模态的各自类间离散度矩阵<i>Σ</i><sub><i>p</i>b</sub>, <i>Σ</i><sub><i>q</i>b</sub>:</p>
                </div>
                <div class="p1">
                    <p id="140"><i>Σ</i><sub><i>p</i>b</sub>=<i>Σ</i><sub><i>pp</i></sub>-<i>Σ</i><sub><i>p</i>w</sub>;  <i>Σ</i><sub><i>q</i>b</sub>=<i>Σ</i><sub><i>qq</i></sub>-<i>Σ</i><sub><i>q</i>w</sub>,      (15) </p>
                </div>
                <div class="p1">
                    <p id="141">则Multi-DeepLDA是通过找到视觉模态和文本模态内部的映射矩阵<b><i>A</i></b><sub>1</sub>和<b><i>A</i></b><sub>2</sub>, 使得在相同标签下各自模态内的类间离散度矩阵和类内离散度矩阵的比值最大化, 具体表述为</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>p</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mtext>b</mtext></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>1</mn><mtext>Τ</mtext></msubsup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mtext>w</mtext></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>1</mn><mtext>Τ</mtext></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munder><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>q</mi><mtext>b</mtext></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>q</mi><mtext>w</mtext></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">其中, 映射矩阵<b><i>A</i></b><sub>1</sub>和<b><i>A</i></b><sub>2</sub>分别转化各自模态的数据到一个<i>C</i>-1维的空间中, 在各自空间中的映射特征变得线性可区分.</p>
                </div>
                <div class="p1">
                    <p id="144">下面仅对<b><i>A</i></b><sub>1</sub>的求解做简要阐述, <b><i>A</i></b><sub>2</sub>的求解以此类推.具体来讲, 最大化类别间隔的线性组合<b><i>A</i></b><sub>1</sub>是通过求解<i>Σ</i><sub><i>p</i>b</sub><b><i>e</i></b><sub><i>i</i></sub>=<i>v</i><sub><i>i</i></sub> (<i>Σ</i><sub><i>p</i>w</sub>+<i>λ</i><b><i>I</i></b>) <b><i>e</i></b><sub><i>i</i></sub>确定的, 而映射矩阵<b><i>A</i></b><sub>1</sub>是与特征值问题有关的特征向量<b><i>e</i></b>的集合.最后得到的特征值<i>v</i><sub><i>i</i></sub>量化了在对应特征向量方向<b><i>e</i></b><sub><i>i</i></sub>上的判别性方差, 最终的优化目标关注的是最大化<i>k</i><sub><i>p</i></sub>个最小的特征值{<i>v</i><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>p</mi></msubsup></mrow></math></mathml>, <i>v</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>p</mi></msubsup></mrow></math></mathml>, …, <i>v</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>p</mi></msubsup></mrow></math></mathml>}, 即:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>p</mi></msup></mrow></munder><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></munderover><mi>v</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">其中, {<i>v</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>p</mi></msubsup></mrow></math></mathml>, <i>v</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>p</mi></msubsup></mrow></math></mathml>, …, <i>v</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>p</mi></msubsup></mrow></math></mathml>}={<i>v</i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>|<i>v</i><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>&lt;min{<i>v</i><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>p</mi></msubsup></mrow></math></mathml>, <i>v</i><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>p</mi></msubsup></mrow></math></mathml>, …, <i>v</i><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>C</mi><mo>-</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow></math></mathml>}+<i>ε</i>}, 该目标函数的设计是为了推动网络能学习判别性的能力到特征空间中所有可用的维度中<citation id="382" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="158" name="158">2.3.3 相关分析与判别分析的融合</h4>
                <div class="p1">
                    <p id="159">综合2.3.1节和2.3.2节可看出, Multi-DCCA和Multi-DeepLDA都是基于相对应的特征值问题的特征结构优化的.其中, Multi-DCCA的优化是把最大化视觉模态特征提取网络<i>f</i>和文本模态特征提取网络<i>g</i>的隐层输出的相关性作为目标来求解矩阵<b><i>T</i></b>的奇异值;而Multi-DeepLDA的优化是在相同的多模态类别下最大化视觉的和文本的各自模态内类别的区分, 其由相对应的广义特征值问题的特征值大小进行量化.尽管两者的优化有差异, 但是这2种方法有相同之处, 即它们都反向传播一个由特征值问题引起的误差来调整深度神经网络的参数.</p>
                </div>
                <div class="p1">
                    <p id="160">故多模态深度多重判别性相关分析是同时使用Multi-DCCA和Multi-DeepLDA的模型和优化理论, 即同时优化2个不同模态之间隐层表示的相关性以及使各自模态学到表示具有判别性能力的联合优化目标的形式化表示为</p>
                </div>
                <div class="p1">
                    <p id="161" class="code-formula">
                        <mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>p</mi></msup><mo>, </mo><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>q</mi></msup></mrow></munder><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></munderover><mi>v</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo>+</mo><mfrac><mn>1</mn><mrow><mi>k</mi><msub><mrow></mrow><mi>q</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi>q</mi></msub></mrow></munderover><mi>v</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="162">其中, 第1项是为了优化视觉模态和文本模态之间的相关性, 其中用<i>L</i>来泛化典型相关;而第2项和第3项分别是为了优化视觉模态和文本模态的判别性.</p>
                </div>
                <div class="p1">
                    <p id="163">多模态深度多重判别性的优化目标式 (19) 是个端到端的优化过程, 首先需要计算相关性的优化目标分别对<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 的梯度, 以及各自判别性的优化目标对<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 的梯度, 然后沿着多模态网络的2个分支并通过标准的反向传播的方法计算针对<i>θ</i><sup><i>p</i></sup>和<i>θ</i><sup><i>q</i></sup>的梯度.</p>
                </div>
                <div class="p1">
                    <p id="164">经过式 (19) 这种多模态深度多重判别性相关分析的优化, 最后经CCA的特征映射可将图2中①②这2个网络的顶层输出<i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 转化成最大相关的且各自具有判别性的特征表示<b><i>U</i></b><sup>T</sup><sub><i>p</i></sub><i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<b><i>U</i></b><sup>T</sup><sub><i>q</i></sub><i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) , 如图2中③所示.如果使用图2中②提取的文本语义特征和仅用图2中①的VGG-net提取图像视觉特征, 通过图2中③生成新的特征表示, 后文称该模型为DDC;若使用图2中②提取的文本语义特征和图2中①的3个子网络共同提取含有一定语义的图像视觉特征通过图2中③生成新的特征表示, 则后文称该模型为DANDC.式 (19) 的表述是从优化求解的角度展开的, 如果从理论层次上来形式化表述整个过程, 则表示为</p>
                </div>
                <div class="p1">
                    <p id="165" class="code-formula">
                        <mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtable><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>p</mi></msup><mo>, </mo><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mi>q</mi></msup><mo>, </mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mi>p</mi></msup><mo>, </mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mi>q</mi></msup></mrow></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mtext>Ν</mtext><msub><mrow></mrow><mo>+</mo></msub></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>p</mi><mtext>Τ</mtext></msubsup><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>q</mi><mtext>Τ</mtext></msubsup><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mtext>Ν</mtext><msub><mrow></mrow><mo>+</mo></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>c</mi></mrow></munder><mi>L</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo><mo>, </mo><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>p</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>p</mi></mrow></msub><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mi mathvariant="bold-italic">Ι</mi><mo>, </mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>q</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>q</mi><mi>q</mi></mrow></msub><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>q</mi></msub><mo>=</mo><mi mathvariant="bold-italic">Ι</mi><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">Σ</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mrow><mi>q</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mtext>Τ</mtext></msubsup><mo>=</mo><mn>0</mn><mo>, </mo><mo>∀</mo><mi>i</mi><mo>≠</mo><mi>j</mi><mo>, </mo><mspace width="0.25em" /></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="166">其中, 式 (20) 中的第1项是在无监督的情况下, 致力于使2个不同模态之间具有最大相关性, 即两者的距离最小;而第2项是在相同标签的有监督情况下, 致力于使2个模态能够各自产生具有可区分性的特征表示.</p>
                </div>
                <div class="p1">
                    <p id="167">那些来自于社交网站上的图像-文本的共现数据, 在人类概念理解层面上两者之间是存在语义相关性的, 但是在特征层面上两者之间并没有关系, 且属于异构模态特征, 存在较大的语义鸿沟.经过上述系列操作, 将存在语义相关的成对的图像-文本数据转化成在具体特征形式上的最大相关, 即在特征层次上将图像数据和对应的文本数据建立起关联, 从而使两者之间差异更小, 如式 (20) 所示, 这一定程度上缓解了异构模态特征之间的鸿沟, 且使各个模态具有优异的判别能力.</p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>2.4 多模态注意力融合网络的情感分类</b></h4>
                <div class="p1">
                    <p id="169">受人类视觉注意力启发的注意力模块提供了一种机制来推断局部特征对于整体特征的相对重要性.鉴于它能够提供完整的可微性和可解释性来发掘网络关注的重点, 目前已经在许多神经网络的应用中作为默认的组成部分.注意力模块可以是只关注整体特征中某一特定部分的硬性注意力机制, 也可以是通过重要性的概率分布来分配给所有特征的软性注意力机制.本文主要选择软性注意力机制来展开后续的研究.</p>
                </div>
                <div class="p1">
                    <p id="170">尽管2.3节中获得的判别性视觉特征表示<b><i>U</i></b><sup>T</sup><sub><i>p</i></sub><i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和判别性语义特征表示<b><i>U</i></b><sup>T</sup><sub><i>q</i></sub><i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 两者在整体特征层面上是最大相关的且具有判别性的, 即两者之间在特征层次上的语义差距最小, 而在多模态情感分析任务中图像视觉特征和文本语义特征的贡献度不是同等重要的, 因此, 需要进一步探测视觉特征和语义特征之间更深层次的内部联系.于是提出一种先后序列化生成语义注意力和视觉注意力的co-attention网络, 进一步发掘最大相关的判别性视觉特征和判别性语义特征之间更深层次的内部情感关系.由于视觉特征和人类高层情感概念理解之间存在情感鸿沟, 为了形成更好的易于理解的视觉特征表示, 本文首先关注基于语义的视觉注意力.</p>
                </div>
                <div class="p1">
                    <p id="171">为了便于形式化表示, 分别简写<b><i>U</i></b><sup>T</sup><sub><i>p</i></sub><i>f</i> (<b><i>X</i></b><sup><i>p</i></sup>) 和<b><i>U</i></b><sup>T</sup><sub><i>q</i></sub><i>g</i> (<b><i>X</i></b><sup><i>q</i></sup>) 为<b><i>v</i></b><sub>I</sub>和<b><i>v</i></b><sub>S</sub>.如图3中①所示, 首先同时输入<b><i>v</i></b><sub>I</sub>和<b><i>v</i></b><sub>S</sub>到相同构造的2个全连接神经网络<i>f</i><sub>I</sub>和<i>f</i><sub>S</sub>中进一步提取成对的视觉和语义特征表示, 其中<i>f</i><sub>I</sub>为学习视觉特征的网络, 而<i>f</i><sub>S</sub>为学习语义特征的网络.在<b><i>v</i></b><sub>I</sub>和<b><i>v</i></b><sub>S</sub>共同逐层学习的过程中, 通过使用一个单层的神经网络来结合视觉特征和语义特征, 然后使用<i>softmax</i>层来生成一个视觉注意力分布, 相关操作为</p>
                </div>
                <div class="p1">
                    <p id="172"><b><i>h</i></b><sub>I</sub>=tanh (<b><i>W</i></b><sub><b><i>v</i></b><sub>I</sub></sub><i>f</i><sub>I</sub> (<b><i>v</i></b><sub>I</sub>) ⊙<b><i>W</i></b><sub><b><i>v</i></b><sub>S</sub></sub><i>f</i><sub>S</sub> (<b><i>v</i></b><sub>S</sub>) ) ,      (21) </p>
                </div>
                <div class="p1">
                    <p id="173"><i>α</i>=<i>softmax</i> (<b><i>W</i></b><sub><b><i>h</i></b><sub>I</sub></sub><b><i>h</i></b><sub>I</sub>+<b><i>b</i></b><sub><b><i>h</i></b><sub>I</sub></sub>) ,      (22) </p>
                </div>
                <div class="p1">
                    <p id="174">其中, <b><i>W</i></b><sub><b><i>v</i></b><sub>I</sub></sub>, <b><i>W</i></b><sub><b><i>v</i></b><sub>S</sub></sub>, <b><i>W</i></b><sub><b><i>h</i></b><sub>I</sub></sub>, <b><i>b</i></b><sub><b><i>h</i></b><sub>I</sub></sub>是参数, 使用⊙表示视觉特征表示和语义特征表示的结合, 其中视觉特征<i>f</i><sub>I</sub> (<b><i>v</i></b><sub>I</sub>) ∈R<sup><i>d</i></sup>和语义特征<i>f</i><sub>S</sub> (<b><i>v</i></b><sub>S</sub>) ∈R<sup><i>d</i></sup>具有相同的特征维度<i>d</i>, 通过对应交互视觉特征<i>f</i><sub>I</sub> (<b><i>v</i></b><sub>I</sub>) 和语义特征<i>f</i><sub>S</sub> (<b><i>v</i></b><sub>S</sub>) 从而形成视觉语义特征<i>f</i><sub>IS</sub> (<b><i>v</i></b>) , 为了更加频繁地深入交互特征元素, 继续学习<i>f</i><sub>IS</sub> (<b><i>v</i></b>) 使其特征元素全部关联到<i>d</i>维特征空间中, 从而形成具有特征之间内部关联的新的视觉语义特征<b><i>h</i></b><sub>I</sub>, 因此可得对应于<b><i>h</i></b><sub>I</sub>中特征的注意力概率<i>α</i>∈R<sup><i>d</i></sup>, 是一个<i>d</i>维向量.</p>
                </div>
                <div class="p1">
                    <p id="175">基于每一个特征<i>i</i>的视觉注意力概率<i>α</i><sub><i>i</i></sub>, 新的判别性视觉特征表示通过视觉特征的权重和来构造, 即:</p>
                </div>
                <div class="p1">
                    <p id="176" class="code-formula">
                        <mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover><msub><mrow></mrow><mtext>Ι</mtext></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>f</mi><msub><mrow></mrow><mtext>Ι</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="177">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906019_177.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 多模态注意力融合网络的情感分类图解" src="Detail/GetImg?filename=images/JFYZ201906019_177.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 多模态注意力融合网络的情感分类图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906019_177.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Schematic sketch of multi-modal attention fusion network for sentiment classification</p>

                </div>
                <div class="p1">
                    <p id="178">然后使用新的判别性视觉特征表示<mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>I</sub>来形成基于视觉的语义注意力.</p>
                </div>
                <div class="p1">
                    <p id="180">为了构建更好的信息表示, 除了用文本语义特征来引导图像视觉特征的注意力, 还进一步探讨基于视觉的语义注意力来研究视觉和语义的相互影响.基于视觉的语义注意力与基于语义的视觉注意力操作过程相似.为了获得语义的注意力分布, 首先利用<mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>I</sub>结合相对应的语义特征, 然后基于概率分布形成新的判别性语义特征表示<mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>S</sub>, 详细的操作为</p>
                </div>
                <div class="p1">
                    <p id="183"><mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover><msub><mrow></mrow><mtext>Ι</mtext></msub></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover><msub><mrow></mrow><mtext>Ι</mtext></msub><mo>⊙</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mtext>S</mtext></msub></mrow></msub><mi>f</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>,      (24) </p>
                </div>
                <div class="p1">
                    <p id="185"><i>β</i>=<i>softmax</i> (<b><i>W</i></b><sub><b><i>h</i></b><sub>S</sub></sub><b><i>h</i></b><sub>S</sub>+<b><i>b</i></b><sub><b><i>h</i></b><sub>S</sub></sub>) ,      (25) </p>
                </div>
                <div class="p1">
                    <p id="186" class="code-formula">
                        <mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover><msub><mrow></mrow><mtext>S</mtext></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>f</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="187">同理, 式 (24) ～ (26) 中的参数设置与基于语义的视觉注意力的等式设置相同.</p>
                </div>
                <div class="p1">
                    <p id="188">总之, 基于语义的视觉注意力和基于视觉的语义注意力是一个交互影响的过程, 通过交互来形成更好的有利于图像和文本进行深层融合的特征表示.为了探索图像和文本之间更深层次的内部关联, 可以尝试多次序列化地迭代交互视觉特征和语义特征, 即形成嵌套的co-attention网络.</p>
                </div>
                <div class="p1">
                    <p id="189">通过合并<mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>I</sub>和<mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>S</sub>生成图像视觉和文本语义的融合表示<b><i>v</i></b><sub>m</sub>, 即:</p>
                </div>
                <div class="p1">
                    <p id="192"><mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mtext>m</mtext></msub><mo>=</mo><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover><msub><mrow></mrow><mtext>Ι</mtext></msub><mo>⊕</mo><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover><msub><mrow></mrow><mtext>S</mtext></msub></mrow></math></mathml>,      (27) </p>
                </div>
                <div class="p1">
                    <p id="194">其中, ♁是连接操作.在网络学习的过程中, 隐藏层可以自动地结合视觉的和文本的情感表示.</p>
                </div>
                <div class="p1">
                    <p id="195">在获得了融合特征<b><i>v</i></b><sub>m</sub>之后, 通过2层全连接神经网络<i>f</i><sub>m</sub>进一步捕获更深层次的内部关联, 将最后一个全连接层的输出通过<i>softmax</i>层产生分类标签的分布, 如图3中②所示, 该过程简要描述为</p>
                </div>
                <div class="p1">
                    <p id="196"><mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">y</mi><mo>^</mo></mover><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mtext>m</mtext></msub></mrow></msub><mo stretchy="false">) </mo><mi>f</mi><msub><mrow></mrow><mtext>m</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mtext>m</mtext></msub><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mtext>m</mtext></msub></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>,      (28) </p>
                </div>
                <div class="p1">
                    <p id="198">其中, <b><i>W</i></b><sub><i>f</i><sub>m</sub></sub>∈R<sup><i>C</i>×<i>d</i></sup>和<b><i>b</i></b><sub><i>f</i><sub>m</sub></sub>∈R<sup><i>C</i></sup>是参数, <i>C</i>是标签的数量, 在多模态注意力融合网络模型的设置中, <b><i>v</i></b><sub>I</sub>和<b><i>v</i></b><sub>S</sub>的输入到最后的分类是一个端到端的过程, 该模型使用分类交叉熵计算基于反向传播的训练的批量损失.</p>
                </div>
                <div class="p1">
                    <p id="199" class="code-formula">
                        <mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mfrac><mrow><mo>-</mo><mn>1</mn></mrow><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>y</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="200">其中, <i>N</i>是一批中的总共样本数量, <i>C</i>是情感类别的数量.<b><i>y</i></b><sub><i>i</i></sub>表示来自于训练批次中第<i>i</i>个样本的独热的真实向量, <mathml id="201"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub></mrow></math></mathml>表示对于相同样本中类别<i>j</i>的预测概率.</p>
                </div>
                <h3 id="202" name="202" class="anchor-tag"><b>3 实验分析</b></h3>
                <div class="p1">
                    <p id="203">本节首先介绍实验中要用到的5个数据集, 其中3个是根据ANP<citation id="384" type="reference"><link href="330" rel="bibliography" /><link href="342" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">25</a>]</sup></citation>从不同的社交网络上爬取的, 另外2个是来自于公开的数据集<citation id="383" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>;然后介绍了本文实验中的一些设置;最后通过实验来评估本文提出方法的性能, 大致包括2部分内容:1) 从整体情感分类性能的角度来比较本文提出方法和其他对比方法的实验结果的差异;2) 从局部模型设置合理性的角度来确定整体模型中的2个关键部分对情感分类结果的影响.</p>
                </div>
                <h4 class="anchor-tag" id="204" name="204"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="205">在目前的多模态情感分析中, 由于存在一些可以构建的具有英文描述的图像-文本对的多模态情感数据集, 而缺乏公开的具有中文描述的多模态情感数据集, 故在本文后续的实验中主要讨论英文描述的图文多媒体情感数据集.但是本文提出的模型同样也适用于具有中文描述的多模态情感数据集, 这是因为本文提出的模型主要关注的是构建视觉语义和文本语义之间的深层关联交互, 与文本语言的表现形式关系不大.语言形式对模型的影响将在今后进一步的工作中验证.</p>
                </div>
                <div class="p1">
                    <p id="206">由此, 首先利用不同的情感关键词查询视觉中国官网的搜索引擎来构筑数据集.具体而言, 利用视觉情感本体库 (VSO) 中3 244个ANP<citation id="385" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>作为情感关键词从视觉中国网站上的Getty专区爬取38 363条图像-文本对, 称其为VCGI数据集;此外, 从3 244个ANP<citation id="386" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>中随机选出300个ANP作为情感关键词, 又从相同的网站上爬取37 158条图像-文本对, 称其为VCGII数据集.</p>
                </div>
                <div class="p1">
                    <p id="207">此外, 多语言视觉情感本体库MVSO是由来自于12种语言 (例如中文、英文等) 的15 600个概念构成, 这些概念和图像中表达的情感和情绪密切相关.类似于VSO数据集, 这些概念也以ANP的形式定义.与VCG数据获取的方式相同, 利用MVSO<citation id="387" type="reference"><link href="342" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>中提供的英文语言ANP, 选取其中情感分数绝对值大于1的ANP作为关键词从社交网站Flickr上爬取75 516条图像与其相对应的标题、标签、描述, 称其为MVSO-EN数据集.</p>
                </div>
                <div class="p1">
                    <p id="208">文献<citation id="388" type="reference">[<a class="sup">4</a>]</citation>中公开了带有3个标注 (积极、中性、消极) 的Flickr图像ID, 幸运的是Flickr提供了API, 其能通过提供的唯一ID获得1张图像的元数据 (描述、上传日期、标签 (tags) 等) , 因此利用公开的所有ID从Flickr网站上爬取了6万余张图像以及相对应的标题、标签、描述, 称其为Flickr数据集.</p>
                </div>
                <div class="p1">
                    <p id="209">对于来自于Getty图像的2个数据集, 由于存在极少量中文描述的数据集, 则删除那些描述是中文的图像-文本对, 同时为了获得更丰富的文本语义信息, 则删除那些英文描述少于20个字符的图像-文本对;对于MVSO-EN数据集和Flickr数据集, 选择那些标签和描述至少有1个存在的数据, 将筛选过后的数据集中存在的标签、描述、标题组合成文本信息 (这里并不是所有的数据都是三者都有, 但至少有1个) .由于文本中存在一些不是词汇的内容, 而是以链接、符号等明显不含语义信息的内容形式展示, 则利用wordnet删除文本信息中不在wordnet中的词汇以生成最终的文本.</p>
                </div>
                <div class="p1">
                    <p id="210">VCG数据集和MVSO-EN数据集中图像的情感极性标签来自于ANP的情感分数值, 而Flickr数据集中图像的情感标签来自于人工标注, 将至少2个人标注为积极的图像的极性标签认为是积极, 至少2个人标注为中性的图像的极性标签认为是中性, 至少2个人标注为消极的图像的极性标签认为是消极.此外, 处理后的Flickr数据集有3万多张积极标签的图像, 明显高于消极的和中性的数量.为了人工构造一个较平衡的数据集, 从积极的图像中随机取样一些与消极或中性大致数量相等的数据.因此得到了本文在实验中使用的5个数据集, 分别为VCGI, VCGII, MVSO-EN, Flickr-2, Flickr-3, 其具体信息统计如表1所示:</p>
                </div>
                <div class="area_img" id="211">
                    <p class="img_tit"><b>表1 实验使用数据集统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Statistic of The Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="211" border="1"><tr><td><br />Dataset</td><td>Positive</td><td>Neutral</td><td>Negative</td><td>Total</td></tr><tr><td><br />VCGI</td><td>18 847</td><td>0</td><td>15 837</td><td>34 684</td></tr><tr><td><br />VCGII</td><td>18 134</td><td>0</td><td>16 184</td><td>34 318</td></tr><tr><td><br />MVSO-EN</td><td>35 295</td><td>0</td><td>24 363</td><td>59 658</td></tr><tr><td><br />Flickr-2</td><td>12 773</td><td>0</td><td>10 070</td><td>22 843</td></tr><tr><td><br />Flickr-3</td><td>12 773</td><td>13 518</td><td>10 070</td><td>36 361</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="212" name="212"><b>3.2 实验设置</b></h4>
                <div class="p1">
                    <p id="213">VCG数据来自于视觉中国网站的Getty专区, 其图像的文本描述相对正式和简洁.由于其文本长度普遍较短且长短不一, 则选取所用训练集中最长的文本长度为最大长度, 不足最大长度的文本用零向量填充.MVSO-EN数据集和Flickr数据集均来自社交网站Flickr, 不同的是获取数据的方式以及图像标签 (label) 的方法不同.由于不是所有的图像共现的文本信息中都含有标签 (tags) 、描述和标题, 则文本长度长短不一且差别较大, 故截取最大文本长度为300, 不足最大长度的文本以零向量填充.</p>
                </div>
                <div class="p1">
                    <p id="214">每一个词向量的维度设置为300, 在训练过程中微调词向量来适应本文使用的情感数据集.文本模态特征提取网络的卷积核在实验中使用了3个不同的卷积核尺寸, 分别为3, 4, 5, 且针对每一个卷积核尺寸采用了20个滤波器.此外, 针对所有的图像都调整其为相同的大小224×224.在实验中总共有2个端到端的优化过程:1) 多模态深度多重判别性相关分析的优化, 除了在最后关联层上采用线性 (linear) 激活函数, 其他网络层的输出均连接到ReLU激活函数;2) 多模态注意力融合网络的分类交叉熵的优化, 每一个全连接层 (除最后一个) 的输出均连接到ReLU激活函数, 最后一个全连接层的输出采用<i>softmax</i>进行分类.但是这2个优化的过程均使用小批量的RMSprop方法<citation id="389" type="reference"><link href="350" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>来优化网络.为了防止过拟合, 实验中整体模型上均采用Dropout策略, 具体设定Dropout的值为0.5.</p>
                </div>
                <div class="p1">
                    <p id="215">本文实验主要评估提出的方法在二分类 (积极、消极) 目标和三分类 (积极、消极、中性) 目标上的效果.针对情感分类准确性评估和局部模型效用评估的所有实验中, 每个实验均从各自对应数据集中随机选取80%用于训练, 20%用于测试.</p>
                </div>
                <h4 class="anchor-tag" id="216" name="216"><b>3.3 实验1:情感分类准确性评估</b></h4>
                <h4 class="anchor-tag" id="217" name="217">3.3.1 对比方法</h4>
                <div class="p1">
                    <p id="218">为了证明提出方法的有效性, 首先比较其与仅用图像和仅用文本进行情感分析的方法, 然后进一步比较其与其他相关的图文融合情感分类方法的性能.对比方法说明有4种:</p>
                </div>
                <div class="p1">
                    <p id="219">1) S -Visual. 利用文献<citation id="390" type="reference">[<a class="sup">30</a>]</citation>中提出的基于迁移学习的视觉情感分析方法, 不同的是本文实验利用VGG-16net网络模型.</p>
                </div>
                <div class="p1">
                    <p id="220">2) S -Text. 利用本文提出的文本模态特征提取网络, 并通过<i>softmax</i>层对文本进行情感分类.</p>
                </div>
                <div class="p1">
                    <p id="221">3) CNN-Multi. 由3个CNN组成.预训练的文本CNN和图像CNN分别抽取文本和图像的特征表示, 然后拼接2个特征向量输入到另一个仅有4个全连接层的multi-CNN结构.文本CNN中的卷积层用的是二维卷积, 每一个文本样本的维度像单通道图像一样被调整为50×50的大小<citation id="391" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="222">4) DNN-Multi. 方法同CNN-Multi, 不同的是利用本文提出的视觉模态特征提取网络和文本模态特征提取网络分别抽取图像和文本的特征表示, 然后拼接2个特征向量输入到另一个有4个全连接层的结构中.</p>
                </div>
                <h4 class="anchor-tag" id="223" name="223">3.3.2 结果与讨论</h4>
                <div class="p1">
                    <p id="224">表2展示了本文方法和对比方法在2个VCG数据集上的比较结果.如表2所示, 本文提出的层次化深度关联融合网络的方法DDC+co-attention和DANDC+co-attention的分类效果明显优于单模态图像S -Visual和单模态文本S -Text的分类效果, 说明学习图文多媒体内容的特征能更好地理解用户的情感.此外, 尽管CNN-Multi在多模态情感分析的任务上取得了一定的效果, 然而其特征提取的网络模型比较简单, 故修改CNN-Multi网络结构的DNN-Multi方法取得了更优异的效果, 这一定程度上说明设计合适的网络结构有益于学习好的特征表示以更好地服务于情感分类.</p>
                </div>
                <div class="area_img" id="225">
                    <p class="img_tit"><b>表2 在VCGI和VCGII数据集上不同方法的准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Accuracy of Different Methods on VCGI and VCGII Dataset</b></p>
                    <p class="img_note"> %</p>
                    <table id="225" border="1"><tr><td><br />Methods</td><td>VCGI</td><td>VCGII</td></tr><tr><td><br />S -Visual</td><td>65.82</td><td>75.71</td></tr><tr><td><br />S -Text</td><td>67.58</td><td>76.92</td></tr><tr><td><br />CNN-Multi</td><td>71.56</td><td>77.86</td></tr><tr><td><br />DNN-Multi</td><td>71.86</td><td>79.63</td></tr><tr><td><br />DDC+co-attention</td><td><b>73.24</b></td><td><b>83.86</b></td></tr><tr><td><br />DANDC+co-attention</td><td><b>74.42</b></td><td><b>85.52</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Notes: The bold values are the accuracy obtained by our method.</p>
                </div>
                <div class="p1">
                    <p id="226">然而CNN-Multi和DNN-Multi都是首先分别提取图像和文本的特征然后再进行融合, 不是共同地学习成对的图像-文本数据, 而社交媒体上共现的图像-文本数据往往是存在语义概念相关的, 若分别提取图像特征和文本特征后再进行特征融合, 这会割裂图像与文本之间对应的语义关联.本文提出的方法是同时共同地学习图像-文本的共现数据, 且效果也优于CNN-Multi和DNN-Multi, 这表明在多模态情感分析任务上同时处理成对的图像-文本的共现数据是必要的.如表2所示, 提出的方法在VCGI和VCGII数据集上相较对比方法均展示出更好的性能, 说明提出的方法在相同领域不同背景的数据集下具有领域适应能力.</p>
                </div>
                <div class="p1">
                    <p id="227">表3分别展示了本文方法和对比方法在MVSO-EN数据集和Flickr数据集上的实验结果.尽管MVSO-EN数据集和Flickr数据集都是来自于Flickr社交网站, 但是它们数据集的构造方式略有不同, 其中MVSO-EN数据集和VCG数据集的构造方式相同, 则针对MVSO-EN数据集的实验评估, 采取了与表2中VCG数据集同样的对比方式, 且本文的方法DDC+co-attention和DANDC+co-attention都展示了优异的性能.</p>
                </div>
                <div class="area_img" id="228">
                    <p class="img_tit"><b>表3 在MVSO-EN和Flickr数据集上不同方法的准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Accuracy of Different Methods on MVSO-EN and Flickr Dataset</b></p>
                    <p class="img_note">%</p>
                    <table id="228" border="1"><tr><td><br />Methods</td><td>MVSO-EN</td><td>Flickr-2</td><td>Flickr-3</td></tr><tr><td><br />S -Visual</td><td>66.06</td><td>79.36</td><td>59.17</td></tr><tr><td><br />S -Text</td><td>63.24</td><td>73.24</td><td>56.53</td></tr><tr><td><br />CNN-Multi</td><td>70.68</td><td>81.22</td><td>61.69</td></tr><tr><td><br />DNN-Multi</td><td>72.23</td><td>82.18</td><td>62.13</td></tr><tr><td><br />DDC+co-attention</td><td><b>84.55</b></td><td><b>85.92</b></td><td><b>63.97</b></td></tr><tr><td><br />DANDC+co-attention</td><td><b>83.46</b></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Notes: The bold values are the accuracy obtained by our method.</p>
                </div>
                <div class="p1">
                    <p id="229">此外, 由于Flickr数据集的标签来自于人工标注, 故没有图像的ANP信息, 则在Flickr数据集上不能评估DANDC+co-attention的性能, 表3中空白表示无实验数据.但是由于本文使用的Flickr数据集来自于人工标注, 其标签相比更准确, 同时为了证明本文提出的DDC+co-attention同样适用于三分类的目标, 故针对Flickr数据集, 在二分类目标和三分类目标上都进行了分类性能评估, 其中在Flickr-2数据集上是为了评估二分类目标, 而在Flickr-3数据集上是为了评估三分类目标, 且在Flickr-2和Flickr-3这2个数据集上DDC+co-attention均较对比方法展示了更好的性能.</p>
                </div>
                <h4 class="anchor-tag" id="230" name="230"><b>3.4 实验2: 局部模型效用评估</b></h4>
                <div class="p1">
                    <p id="231">尽管表2和表3的实验已经展示了本文提出的方法可以达到更好的情感分类效果, 但是在本文提出的层次化深度关联融合网络的模型中, 不仅考虑了经过多模态深度多重判别性相关分析的优化而生成的最大相关的判别性视觉特征表示和判别性语义特征表示, 还在多模态注意力的融合网络中序列化地研究了图像视觉特征和文本语义特征之间的协同关注 (co-attention) .为了探讨这2部分模型的设置对图像和文本融合的情感分类结果的贡献度以及合理性, 则分别做实验来评估这2个部分的性能.</p>
                </div>
                <h4 class="anchor-tag" id="232" name="232">3.4.1 对比方法</h4>
                <div class="p1">
                    <p id="233">首先, 通过设定实验来评估多模态深度多重判别性相关分析的合理性.对比方法设置为:</p>
                </div>
                <div class="p1">
                    <p id="234">1) DNN-S.利用DNN-Multi方法中的DNN网络结构分别提取图像和文本的特征, 然后拼接特征向量输入<i>softmax</i>层进行情感分类.</p>
                </div>
                <div class="p1">
                    <p id="235">2) DC-S.利用文献<citation id="392" type="reference">[<a class="sup">17</a>]</citation>中提出的深度相关性分析的方法, 不同于文献<citation id="393" type="reference">[<a class="sup">17</a>]</citation>中的网络结构, 而是利用本文提出的视觉模态特征提取网络和文本模态特征提取网络来共同提取图像和文本的最大相关的视觉和语义的映射特征, 将图文映射特征融合后通过<i>softmax</i>层进行情感分类.</p>
                </div>
                <div class="p1">
                    <p id="236">3) DDC-S.利用本文DDC的方法共同地提取图像和文本的最大相关的判别性视觉和语义的映射特征, 将视觉和语义映射特征融合后通过<i>softmax</i>层进行情感分类.</p>
                </div>
                <div class="p1">
                    <p id="237">4) DANDC-S.利用本文DANDC的方法共同地提取图像和文本的最大相关的判别性视觉和语义的映射特征, 将视觉和语义映射特征融合后通过<i>softmax</i>层进行情感分类.</p>
                </div>
                <div class="p1">
                    <p id="238">总之, 前3组实验设置是为了评估简单的特征融合 (DNN-S) 、具有深度相关分析的特征映射 (DC-S) 、具有深度多重判别性相关分析的特征映射 (DDC-S) 这三者在情感分类上的性能差异, 而DANDC-S是为了评估在深度多重判别性相关分析阶段, 融入图像中层语义特征对分类结果的影响.</p>
                </div>
                <div class="p1">
                    <p id="239">其次, 通过设定实验来评估多模态协同注意力 (co-attention) 设置的合理性, 对比方法设置为:</p>
                </div>
                <div class="p1">
                    <p id="240">1) co-attention-2.若称2.4节中提出的co-attention模型中基于语义的视觉注意力和基于视觉的语义注意力为1层co-attention, 则co-attention-2方法大致同co-attention模型, 不同之处在于基于2.4节1层co-attention中形成的<mathml id="241"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>I</sub>和<mathml id="242"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>S</sub>再序列化地进行一次基于语义的视觉注意力和基于视觉的语义注意力来建模图像视觉和文本语义, 称其为嵌套的2层co-attention.</p>
                </div>
                <div class="p1">
                    <p id="243">2) same-co-attention. 2.4节中提出的co-attention模型是先后序列化地生成视觉的注意力和语义的注意力, 不同于co-attention模型的序列化操作, 而same-co-attention是同时平行地生成视觉的注意力和语义的注意力.具体而言, 基于2.4节中视觉语义特征<b><i>h</i></b><sub>I</sub>产生的注意力概率向量<i>α</i>, 共同构筑新的判别性视觉特征表示<mathml id="244"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>I</sub>和判别性语义特征表示<mathml id="245"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">v</mi><mo>˜</mo></mover></math></mathml><sub>S</sub>.</p>
                </div>
                <h4 class="anchor-tag" id="246" name="246">3.4.2 结果与讨论</h4>
                <div class="area_img" id="247">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906019_247.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在5个数据集上评估深度判别性相关分析的性能" src="Detail/GetImg?filename=images/JFYZ201906019_247.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 在5个数据集上评估深度判别性相关分析的性能  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906019_247.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Evaluate the performance of deep and  discriminative correlation analysis on five datasets</p>

                </div>
                <div class="p1">
                    <p id="248">图4的实验结果展示了在5个数据集上利用多模态深度多重判别性相关分析 (DDC-S和DANDC-S) 的分类性能均优于DNN-S和DC-S, 这说明利用多重深度判别性相关分析来学习最大相关的判别性特征表示是可行且必要的.此外, 在视觉模态上共同学习图像视觉特征和图像中层语义特征的DANDC-S在除了VCGI数据集外的所有数据集上的分类结果上均优于仅利用视觉特征的DDC-S.然而, 在VCGI数据集上DANDC+co-attention的情感分类性能要优于DDC+co-attention, 如表2所示.此外, 在表3中的MVSO-EN数据集上, DANDC+co-attention的性能次优于DDC+co-attention, 但是在多重深度判别性相关分析阶段DANDC-S的分类性能要优于DDC-S, 如图4所示.这表明融入图像的中层语义特征 (ANP) 在一定程度上对多模态情感分类的性能是起积极作用的.</p>
                </div>
                <div class="p1">
                    <p id="249">然后, 进一步评估co-attention方法设置的合理性, 本实验仅利用提出的DDC模型生成的最大相关的判别性视觉特征和判别性语义特征做基准, 比较其与same-co-attention和co-attention-2的性能差异.如图5所示, 在5个数据集上的对比实验均显示序列化的co-attention相比于非序列化的same-co-attention都取得了略好的情感分类效果, 这说明先后序列化生成视觉的注意力和语义的注意力的设置有益于探测图像视觉和文本语义之间的深层内部关联.另外, 为了探讨嵌套co-attention网络的性能, 在5个数据集上也相应做了实验评估.如图5所示, 在Flickr-2和Flickr-3数据集上的分类结果co-attention-2略优于co-attention, 但在其他数据集上效果反而不如co-attention的性能.由于增加co-attention网络的迭代交互的次数, 不仅会使模型变得更复杂, 而且在实验中需要更多的训练时间.很显然, 嵌套序列交互后的效果没有明显的提升甚至在几个数据集上反而下降, 因此, 实验设置中没有必要去设置更多嵌套co-attention层的模型.</p>
                </div>
                <div class="area_img" id="250">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906019_250.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 在5个数据集上评估co-attention设置的性能" src="Detail/GetImg?filename=images/JFYZ201906019_250.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 在5个数据集上评估co-attention设置的性能  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906019_250.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Evaluate the performance of co-attention  settings on five datasets</p>

                </div>
                <h3 id="251" name="251" class="anchor-tag"><b>4 总  结</b></h3>
                <div class="p1">
                    <p id="252">近年来, 多模态情感分析已经成为一个日益重要的研究热点, 尤其在社交媒体大数据的环境下.本文提出一个新颖的层次化深度关联融合网络结构用于多模态情感分析.在提出的方法中, 首先依赖提出的多模态深度多重判别性相关分析的模型共同学习最大相关的判别性视觉特征表示和判别性语义特征表示.基于这2种特征表示, 进一步提出多模态注意力融合网络的情感分类模型, 首先, 序列化地生成语义的视觉注意力和视觉的语义注意力来交互视觉和语义, 从而获得图像的和文本的更深层和更判别性的特征表示;然后, 合并最新的图像视觉特征和文本语义特征后并通过全连接神经网络学习后再用于训练情感分类器.在5个真实数据集上已经评估了提出方法的有效性, 且实验结果表明本文提出的层次化深度关联融合网络的图文媒体情感分析方法要优于其他相关的方法.</p>
                </div>
                <div class="p1">
                    <p id="253">在未来的工作中将考虑不同的文本语言类型、图像的区域化语义, 设计更好的多模态网络提取结构以及更合理的注意力网络模型用于情感分析, 此外, 还将研究更好的特征融合策略以进一步提高异构多模态特征融合的性能.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="397" type="formula" href="images/JFYZ201906019_39700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">蔡国永</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="398" type="formula" href="images/JFYZ201906019_39800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">吕光瑞</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="399" type="formula" href="images/JFYZ201906019_39900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">徐智</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="294">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microblog sentiment analysis based on cross-media bag-of-words model">

                                <b>[1]</b>Wang Min, Cao Donglin, Li Lingxiao, et al.Microblog sentiment analysis based on cross-media bag-of-words model[C] //Proc of the 2014 Int Conf on Multimedia Computing and Service.New York:ACM, 2014:76- 80
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A cross-media public sentiment analysis system for microblog">

                                <b>[2]</b>Cao Donglin, Ji Rongrong, Lin Dazhen, et al.A cross-media public sentiment analysis system for microblog[J].Multimedia Systems, 2016, 22 (4) :479- 486
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEF472DCBE40D3F9A56DBC2D63226A020&amp;v=Mjc2MzBzUE8zMldxaEUzZTdUbFJiaWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGd6THE2d2F3PU5pZk9mY2JPR3RiTzIvdzNFZThQZUg5UHhtY1c3RQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Poria S, Cambria E, Howard N, et al.Fusing audio, visual and textual clues for sentiment analysis from multimodal content[J].Neurocomputing, 2016, 174:50- 59
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image sentiment analysis using latent correlations among visual,textual,and sentiment views">

                                <b>[4]</b>Katsurai M, Satoh S.Image sentiment analysis using latent correlations among visual, textual, and sentiment views[C] //Proc of the 2016 Int Conf on Acoustics, Speech and Signal Processing (ICASSP) .Piscataway, NJ:IEEE, 2016:2837- 2841
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for multimedia sentiment analysis">

                                <b>[5]</b>Cai Guoyong, Xia Binbin.Convolutional neural networks for multimedia sentiment analysis[C] //Proc of the 2015 Conf on Natural Language Processing and Chinese Computing.Berlin:Springer, 2015:159- 167
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual and Textual Sentiment Analysis of a Microblog Using Deep Convolutional Neural Networks">

                                <b>[6]</b>Yu Yuhai, Lin Hongfei, Meng Jiana, et al.Visual and textual sentiment analysis of a microblog using deep convolutional neural networks[J].Algorithms, 2016, 9 (2) :41- 51
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multimodal feature learning approach for sentiment analysis of social network multimedia">

                                <b>[7]</b>Baecchi C, Uricchio T, Bertini M, et al.A multimodal feature learning approach for sentiment analysis of social network multimedia[J].Multimedia Tools and Applications, 2016, 75 (5) :2507- 2525
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-modality consistent regression for joint visual-textual sentiment analysis">

                                <b>[8]</b>You Quanzeng, Luo Jiebo, Jin Hailin, et al.Cross-modality consistent regression for joint visual-textual sentiment analysis of social multimedia[C] //Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016:13- 22
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A residual merged neutral network for multimodal sentiment analysis">

                                <b>[9]</b>Xu Nan, Mao Wenji.A residual merged neutral network for multimodal sentiment analysis[C] //Proc of the 2nd Int Conf on Big Data Analysis (ICBDA) .New York:IEEE, 2017:6- 10
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts">

                                <b>[10]</b>Dos Santos C, Gatti M.Deep convolutional neural networks for sentiment analysis of short texts[C] //Proc of the 25th Int Conf on Computational Linguistics (COLING 2014) .Stroudsburg, PA:ACL, 2014:69- 78
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708009&amp;v=MDE3MTh0R0ZyQ1VSTE9lWmVScUZpRG5WcjdNTHl2U2RMRzRIOWJNcDQ5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Liang Bin, Liu Quan, Xu Jin, et al.Aspect-based sentiment analysis based on multi-attention CNN[J].Journal of Computer Research and Development, 2017, 54 (8) :1724- 1735 (in Chinese) (梁斌, 刘全, 徐进, 等.基于多注意力卷积神经网络的特定目标情感分析[J].计算机研究与发展, 2017, 54 (8) :1724- 1735) 
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201805006&amp;v=MjUwNjFyQ1VSTE9lWmVScUZpRG5WcjdNTHl2U2RMRzRIOW5NcW85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Chen Ke, Liang Bin, Ke Wende, et al.Chinese micro-blog sentiment analysis based on multi-channels convolutional neural networks[J].Journal of Computer Research and Development, 2018, 55 (5) :945- 957 (in Chinese) (陈珂, 梁斌, 柯文德, 等.基于多通道卷积神经网络的中文微博情感分析[J].计算机研究与发展, 2018, 55 (5) :945- 957) 
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis">

                                <b>[13]</b>Poria S, Cambria E, Gelbukh A.Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis[C] //Proc of the 2015 Conf on Empirical Methods in Natural Language Processing (EMNLP) .Stroudsburg, PA:ACL, 2015:2539- 2544
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust image sentiment analysis using progressively trained and domain transferred deep networks">

                                <b>[14]</b>You Quanzeng, Luo Jiebo, Jin Hailin, et al.Robust image sentiment analysis using progressively trained and domain transferred deep networks[C] //Proc of the 29th AAAI Conf on Artificial Intelligence.Menlo Park:AAAI, 2015:381- 388
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diving deep into sentiment:Understanding fine-tuned cnns for visual sentiment prediction">

                                <b>[15]</b>Campos V, Salvador A, Giro-I-Nieto X, et al.Diving deep into sentiment:Understanding fine-tuned CNNs for visual sentiment prediction[C] //Proc of the 1st Int Workshop on Affect &amp; Sentiment in Multimedia.New York:ACM, 2015:57- 62
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES948D849EDE4B66128654B3F384B0824D&amp;v=MjQwNTM3S2NSNzdyQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhnekxxNndhdz1OaWZPZmJxOEZxWEVxNFl3RUo0TGZuby96aFFiN0RwNU9ueVVyeG94Qw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Campos V, Jou B, Giro-i-Nieto X.From pixels to sentiment:Fine-tuning cnns for visual sentiment prediction[J].Image and Vision Computing, 2017, 65:15- 22
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep canonical correlation analysis">

                                <b>[17]</b>Andrew G, Arora R, Bilmes J, et al.Deep canonical correlation analysis[C] //Proc of the 2013 Int Conf on Machine Learning.Atlant, GA:Machine Learning Society, 2013:1247- 1255
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep linear discriminant analysis">

                                <b>[18]</b>Dorfer M, Kelz R, Widmer G.Deep linear discriminant analysis[J].arXiv preprint, arXiv:1511.04707, 2015
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale visual sentiment ontology and detectors using adjective noun pairs">

                                <b>[19]</b>Borth D, Ji Rongrong, Chen Tao, et al.Large-scale visual sentiment ontology and detectors using adjective noun pairs[C] //Proc of the 21st ACM Int Conf on Multimedia.New York:ACM, 2013:223- 232
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003726909&amp;v=MjA1NzU5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ3prVzczTklWbz1OajdCYXJPNEh0SFBxSTFEYmVzR1kzazV6QmRoNGo5&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>Atrey P K, Hossain M A, El Saddik A, et al.Multimodal fusion for multimedia analysis:A survey[J].Multimedia Systems, 2010, 16 (6) :345- 379
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">

                                <b>[21]</b>Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate [J].arXiv preprint, arXiv:1409.0473, 2014
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked attention networks for image question answering">

                                <b>[22]</b>Yang Zicao, He Xiaodong, Gao Jianfeng, et al.Stacked attention networks for image question answering[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:21- 29
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show and tell:A neural image caption generator">

                                <b>[23]</b>Vinyals O, Toshev A, Bengio S, et al.Show and tell:A neural image caption generator[C] //Proc of the 2015 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ :IEEE, 2015:3156- 3164
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analyzing and predicting sentiment of images on the social web">

                                <b>[24]</b>Siersdorfer S, Minack E, Deng F, et al.Analyzing and predicting sentiment of images on the social Web[C] //Proc of the 18th ACM Int Conf on Multimedia.New York:ACM, 2010:715- 718
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual affect around the world:A large-scale multilingual visual sentiment ontology">

                                <b>[25]</b>Jou B, Chen Tao, Pappas N, et al.Visual affect around the world:A large-scale multilingual visual sentiment ontology[C] //Proc of the 23rd ACM Int Conf on Multimedia.New York:ACM, 2015:159- 168
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image sentiment prediction based on textual descriptions with adjective noun pairs">

                                <b>[26]</b>Li Zuhe, Fan Yangyu, Liu Weihua, et al.Image sentiment prediction based on textual descriptions with adjective noun pairs[J].Multimedia Tools and Applications, 2018, 77 (1) :1115- 1132
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[27]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition [J].arXiv preprint, arXiv:1409.1556, 2014
                            </a>
                        </p>
                        <p id="348">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">

                                <b>[28]</b>Kim Y.Convolutional neural networks for sentence classification [J].arXiv preprint, arXiv:1408.5882, 2014
                            </a>
                        </p>
                        <p id="350">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An overview of gradient descent optimization algorithms">

                                <b>[29]</b>Ruder S.An overview of gradient descent optimization algorithms [J].arXiv preprint, arXiv:1609.04747, 2016
                            </a>
                        </p>
                        <p id="352">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual sentiment analysis for social images using transfer learning approach">

                                <b>[30]</b>Islam J, Zhang Yangqing.Visual sentiment analysis for social images using transfer learning approach [C] //Proc of the 2016 IEEE Int Conf on Big Data and Cloud Computing.Piscataway, NJ:IEEE, 2016:124- 130
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201906019" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906019&amp;v=Mjc5NjNTZExHNEg5ak1xWTlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEblZyN01MeXY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4azl3cmZ3Qmw3VDdOOWZuST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

