

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128626430275000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201906008%26RESULT%3d1%26SIGN%3dTPOiZLHz7qlYhevyKnAERPXWClg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906008&amp;v=MDMwMzVxcUJ0R0ZyQ1VSTE9lWmVScUZpRG1WcjdLTHl2U2RMRzRIOWpNcVk5RmJJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="&lt;b&gt;1.1 通用细粒度数据流加速器&lt;/b&gt;"><b>1.1 通用细粒度数据流加速器</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;1.2 基于数据流思想的专用DNN加速器&lt;/b&gt;"><b>1.2 基于数据流思想的专用DNN加速器</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="&lt;b&gt;2 DNN全连接层背景&lt;/b&gt; "><b>2 DNN全连接层背景</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="&lt;b&gt;3 FDPU结构&lt;/b&gt; "><b>3 FDPU结构</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="&lt;b&gt;3.1 细粒度数据流体系结构概述&lt;/b&gt;"><b>3.1 细粒度数据流体系结构概述</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;3.2 并行模式&lt;/b&gt;"><b>3.2 并行模式</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.3 数据流指令集&lt;/b&gt;"><b>3.3 数据流指令集</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="&lt;b&gt;4 基于稀疏的全连接层加速方案&lt;/b&gt; "><b>4 基于稀疏的全连接层加速方案</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#126" data-title="&lt;b&gt;4.1 细粒度数据流分支指令设计&lt;/b&gt;"><b>4.1 细粒度数据流分支指令设计</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;4.2 面向细粒度数据流的稀疏存储格式&lt;/b&gt;"><b>4.2 面向细粒度数据流的稀疏存储格式</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;4.3 读取稀疏存储格式的数据流图设计&lt;/b&gt;"><b>4.3 读取稀疏存储格式的数据流图设计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="&lt;b&gt;5 实验与结果&lt;/b&gt; "><b>5 实验与结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#143" data-title="&lt;b&gt;5.1 度量标准&lt;/b&gt;"><b>5.1 度量标准</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;5.2 实验平台&lt;/b&gt;"><b>5.2 实验平台</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;5.3 数据集&lt;/b&gt;"><b>5.3 数据集</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;5.4 实验和结果&lt;/b&gt;"><b>5.4 实验和结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#165" data-title="&lt;b&gt;6 关于卷积层的讨论&lt;/b&gt; "><b>6 关于卷积层的讨论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#167" data-title="&lt;b&gt;7 总  结&lt;/b&gt; "><b>7 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;表1 剪枝后AlexNet与VGG-16中全连接层剩余的有效权重比率&lt;/b&gt;"><b>表1 剪枝后AlexNet与VGG-16中全连接层剩余的有效权重比率</b></a></li>
                                                <li><a href="#72" data-title="图1 FDPU相比于GPU在运行AlexNet各层时的加速比">图1 FDPU相比于GPU在运行AlexNet各层时的加速比</a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表2 全连接层的参数&lt;/b&gt;"><b>表2 全连接层的参数</b></a></li>
                                                <li><a href="#98" data-title="图2 当&lt;i&gt;N&lt;/i&gt;=1时全连接层的计算形式">图2 当<i>N</i>=1时全连接层的计算形式</a></li>
                                                <li><a href="#101" data-title="图3 FDPU体系结构框图">图3 FDPU体系结构框图</a></li>
                                                <li><a href="#111" data-title="图4 数据流执行过程">图4 数据流执行过程</a></li>
                                                <li><a href="#122" data-title="图5 FDPU的指令格式">图5 FDPU的指令格式</a></li>
                                                <li><a href="#123" data-title="图6 使用SWITCH指令的数据流图例子">图6 使用SWITCH指令的数据流图例子</a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表3 数据流指令描述&lt;/b&gt;"><b>表3 数据流指令描述</b></a></li>
                                                <li><a href="#130" data-title="图7 SWITCH指令的反馈依赖边的功能说明">图7 SWITCH指令的反馈依赖边的功能说明</a></li>
                                                <li><a href="#136" data-title="图8 FD-CSR稀疏矩阵存储格式">图8 FD-CSR稀疏矩阵存储格式</a></li>
                                                <li><a href="#134" data-title="图9 全连接层的基本操作">图9 全连接层的基本操作</a></li>
                                                <li><a href="#148" data-title="图10 FDPU运行稠密全连接层和稀疏全连接层时的计算部件利用率">图10 FDPU运行稠密全连接层和稀疏全连接层时的计算部件利用率</a></li>
                                                <li><a href="#160" data-title="图11 稀疏FC层相较于稠密FC层对带宽需求的减少比">图11 稀疏FC层相较于稠密FC层对带宽需求的减少比</a></li>
                                                <li><a href="#163" data-title="&lt;b&gt;表4 FDPU与其他平台运行稀疏FC层时的计算部件利用率对比&lt;/b&gt; %"><b>表4 FDPU与其他平台运行稀疏FC层时的计算部件利用率对比</b> %</a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;表5 FDPU与CPU, GPU和mGPU的面积效率和能效对比&lt;/b&gt;"><b>表5 FDPU与CPU, GPU和mGPU的面积效率和能效对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="204">


                                    <a id="bibliography_1" title="Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C] //Proc of NIPS 2012.Cambridge, MA:MIT Press, 2012:1097- 1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[1]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C] //Proc of NIPS 2012.Cambridge, MA:MIT Press, 2012:1097- 1105
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_2" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint, arXiv:1409.1556, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[2]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint, arXiv:1409.1556, 2014
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_3" title="Han Song, Mao Huizi, Dally W J.Deep compression:Compressing deep neural networks with pruning, trained quantization and huffman coding[J].arXiv preprint, arXiv:1510.00149, 2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep compression:Compressing deep neural networks with pruning,trained quantization and huffman coding">
                                        <b>[3]</b>
                                        Han Song, Mao Huizi, Dally W J.Deep compression:Compressing deep neural networks with pruning, trained quantization and huffman coding[J].arXiv preprint, arXiv:1510.00149, 2015
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_4" title="Chen Yunji, Chen Tianshi, Du Zidong, et al.DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning[C] //Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2014:269- 284" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning">
                                        <b>[4]</b>
                                        Chen Yunji, Chen Tianshi, Du Zidong, et al.DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning[C] //Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2014:269- 284
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_5" title="Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C] //Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1- 17" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indatacenter performance analysis of a tensor processing unit">
                                        <b>[5]</b>
                                        Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C] //Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1- 17
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_6" title="Chen Yu-Hsin, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:367- 379" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks">
                                        <b>[6]</b>
                                        Chen Yu-Hsin, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:367- 379
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_7" title="Han Song, Liu Xingyu, Mao Huizi, et al.EIE:Efficient inference engine on compressed deep neural network[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:243- 254" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EIE:Efficient Inference Engine on Compressed Deep Neural Network">
                                        <b>[7]</b>
                                        Han Song, Liu Xingyu, Mao Huizi, et al.EIE:Efficient inference engine on compressed deep neural network[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:243- 254
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_8" title="Zhang Shijin, Du Zidong, Zhang Lei, et al.Cambricon-X:An accelerator for sparse neural networks[C] //Proc of the 49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.20" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cambricon-x:An accelerator for sparse neural networks">
                                        <b>[8]</b>
                                        Zhang Shijin, Du Zidong, Zhang Lei, et al.Cambricon-X:An accelerator for sparse neural networks[C] //Proc of the 49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.20
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_9" title="Verdoscia L, Vaccaro R, Giorgi R.A matrix multiplier case study for an evaluation of a configurable dataflow-machine[C] //Proc of the 12th ACM Int Conf on Computing Frontiers.New York:ACM, 2015:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A matrix multiplier case study for an evaluation of a configurable dataflow-machine">
                                        <b>[9]</b>
                                        Verdoscia L, Vaccaro R, Giorgi R.A matrix multiplier case study for an evaluation of a configurable dataflow-machine[C] //Proc of the 12th ACM Int Conf on Computing Frontiers.New York:ACM, 2015:1- 6
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_10" title="Milutinovic M, Salom J, Trifunovic N, et al.Guide to Dataflow Supercomputing[M].Berlin:Springer, 2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Guide to Dataflow Supercomputing">
                                        <b>[10]</b>
                                        Milutinovic M, Salom J, Trifunovic N, et al.Guide to Dataflow Supercomputing[M].Berlin:Springer, 2015
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_11" title="Shen Xiaowe, Ye Xiaochun, Tan Xu, et al.An efficient network-on-chip router for dataflow architecture[J].Journal of Computer Science and Technology, 2017, 32 (1) :11- 25" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An efficient network-onchip router for dataflow architecture">
                                        <b>[11]</b>
                                        Shen Xiaowe, Ye Xiaochun, Tan Xu, et al.An efficient network-on-chip router for dataflow architecture[J].Journal of Computer Science and Technology, 2017, 32 (1) :11- 25
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_12" title="Tan Xu, Shen Xiaowe, Ye Xiaochun, et al.A non-stop double buffering mechanism for dataflow architecture[J].Journal of Computer Science and Technology, 2018, 33 (1) :145- 157" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-stop double buffering mechanism for dataflow architecture">
                                        <b>[12]</b>
                                        Tan Xu, Shen Xiaowe, Ye Xiaochun, et al.A non-stop double buffering mechanism for dataflow architecture[J].Journal of Computer Science and Technology, 2018, 33 (1) :145- 157
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_13" title="Xiang Taoran, Feng Yujing, Ye Xiaochun, et al.Accelerating CNN algorithm with fine-grained dataflow architectures[C] //Proc of 2018 IEEE 20th Int Conf on High Performance Computing and Communications.Los Alamitos, CA:IEEE Computer Society, 2018:243- 251" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating CNN algorithm with fine-grained dataflow architectures">
                                        <b>[13]</b>
                                        Xiang Taoran, Feng Yujing, Ye Xiaochun, et al.Accelerating CNN algorithm with fine-grained dataflow architectures[C] //Proc of 2018 IEEE 20th Int Conf on High Performance Computing and Communications.Los Alamitos, CA:IEEE Computer Society, 2018:243- 251
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_14" title="Dennis J B.First version of data flow procedure language[C] //Proc of Programming Symp, Proceedings Colloque Sur La Programmation.Berlin:Springer, 1974:362- 376" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=First version of a data flow procedure language">
                                        <b>[14]</b>
                                        Dennis J B.First version of data flow procedure language[C] //Proc of Programming Symp, Proceedings Colloque Sur La Programmation.Berlin:Springer, 1974:362- 376
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_15" title="Govindan M S S, Burger D.TRIPS:A distributed explicit data graph execution (EDGE) microprocessor[C] //Proc of 2007 IEEE Hot Chips 19 Symp (HCS) .Piscataway, NJ:IEEE, 2013:1- 13" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TRIPS:A distributed explicit data graph execution (EDGE) microprocessor">
                                        <b>[15]</b>
                                        Govindan M S S, Burger D.TRIPS:A distributed explicit data graph execution (EDGE) microprocessor[C] //Proc of 2007 IEEE Hot Chips 19 Symp (HCS) .Piscataway, NJ:IEEE, 2013:1- 13
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_16" title="Swanson S.The WaveScalar Architecture[M].Saint Louis, Missouri:University of Washington, 2006" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The WaveScalar Architecture">
                                        <b>[16]</b>
                                        Swanson S.The WaveScalar Architecture[M].Saint Louis, Missouri:University of Washington, 2006
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_17" title="Chen Guilin, Ma Sheng, Guo Yang.Survey on accelerating neural network with hardware[J].Journal of Computer Research and Development, 2019, 56 (2) :240- 253 (in Chinese) (陈桂林, 马胜, 郭阳.硬件加速神经网络综述[J].计算机研究与发展, 2019, 56 (2) :240- 253) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902002&amp;v=MDg0NTg3S0x5dlNkTEc0SDlqTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGaURtVnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Chen Guilin, Ma Sheng, Guo Yang.Survey on accelerating neural network with hardware[J].Journal of Computer Research and Development, 2019, 56 (2) :240- 253 (in Chinese) (陈桂林, 马胜, 郭阳.硬件加速神经网络综述[J].计算机研究与发展, 2019, 56 (2) :240- 253) 
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_18" title="Ji Rongrong, Lin Shaohui, Chao Fei, et al.Deep neural network compression and acceleration:A review[J].Journal of Computer Research and Development, 2018, 55 (9) :1871- 1888 (in Chinese) (纪荣嵘, 林绍辉, 晁飞, 等.深度神经网络压缩与加速综述[J].计算机研究与发展, 2018, 55 (9) :1871- 1888) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201809005&amp;v=MjQ2MzMzenFxQnRHRnJDVVJMT2VaZVJxRmlEbVZyN0tMeXZTZExHNEg5bk1wbzlGWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Ji Rongrong, Lin Shaohui, Chao Fei, et al.Deep neural network compression and acceleration:A review[J].Journal of Computer Research and Development, 2018, 55 (9) :1871- 1888 (in Chinese) (纪荣嵘, 林绍辉, 晁飞, 等.深度神经网络压缩与加速综述[J].计算机研究与发展, 2018, 55 (9) :1871- 1888) 
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_19" title="Lu Liqiang, Liang Yun, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[C] //Proc of 2017 IEEE 25th Annual Int Symp on Field-Programmable Custom Computing Machines (FCCM) .Los Alamitos, CA:IEEE Computer Society, 2017:101- 108" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating fast algorithms for convolutional neural networks on FPGAs">
                                        <b>[19]</b>
                                        Lu Liqiang, Liang Yun, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[C] //Proc of 2017 IEEE 25th Annual Int Symp on Field-Programmable Custom Computing Machines (FCCM) .Los Alamitos, CA:IEEE Computer Society, 2017:101- 108
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_20" title="Lu Liqiang, Liang Yun.SpWA:An efficient sparse winograd convolutional neural networks accelerator on FPGAs[C] //Proc of 2018 55th ACM/ESDA/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2018:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SpWA:An efficient sparse winograd convolutional neural networks accelerator on FPGAs">
                                        <b>[20]</b>
                                        Lu Liqiang, Liang Yun.SpWA:An efficient sparse winograd convolutional neural networks accelerator on FPGAs[C] //Proc of 2018 55th ACM/ESDA/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2018:1- 6
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_21" title="Lu Liqiang, Liang Yun, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019:1- 1" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating fast algorithms for convolutional neural networks on FPGAs">
                                        <b>[21]</b>
                                        Lu Liqiang, Liang Yun, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019:1- 1
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_22" title="Farabet C, Martini B, Corda B, et al.NeuFlow:A runtime reconfigurable dataflow processor for vision[C] //Proc of 2011 IEEE Computer Society Conf on Computer Vision and Pattern Recognition Workshops (CVPR Workshops 2011) .Los Alamitos, CA:IEEE Computer Society, 2011:109- 116" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Neuflow:A runtime reconfigurable dataflow processor for vision.&amp;quot;">
                                        <b>[22]</b>
                                        Farabet C, Martini B, Corda B, et al.NeuFlow:A runtime reconfigurable dataflow processor for vision[C] //Proc of 2011 IEEE Computer Society Conf on Computer Vision and Pattern Recognition Workshops (CVPR Workshops 2011) .Los Alamitos, CA:IEEE Computer Society, 2011:109- 116
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_23" title="Lu Wenyan, Yan Guihai, Li Jiajun, et al.FlexFlow:A flexible dataflow accelerator architecture for convolutional neural networks[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2017:553- 564" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Flexflow:A flexible dataflow accelerator architecture for convolutional neural networks">
                                        <b>[23]</b>
                                        Lu Wenyan, Yan Guihai, Li Jiajun, et al.FlexFlow:A flexible dataflow accelerator architecture for convolutional neural networks[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2017:553- 564
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_24" title="Fan Dongrui, Li Wenming, Ye Xiaochun, et al.SmarCo:An efficient many-core processor for high-throughput applica-tions in datacenters[C] //Proc of 2018 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2018:596- 607" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SmarCo:An efficient many-core processor for high-throughput applica-tions in datacenters">
                                        <b>[24]</b>
                                        Fan Dongrui, Li Wenming, Ye Xiaochun, et al.SmarCo:An efficient many-core processor for high-throughput applica-tions in datacenters[C] //Proc of 2018 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2018:596- 607
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_25" title="Shen Xiaowei, Ye Xiaochun, Tan Xu, et al.POSTER:An optimization of dataflow architectures for scientific applica-tions[C] //Proc of the 2016 Int Conf on Parallel Architectures &amp;amp; Compilation Techniques (PACT) .Piscataway, NJ:IEEE, 2016:441- 442" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=POSTER:An optimization of dataflow architectures for scientific applica-tions">
                                        <b>[25]</b>
                                        Shen Xiaowei, Ye Xiaochun, Tan Xu, et al.POSTER:An optimization of dataflow architectures for scientific applica-tions[C] //Proc of the 2016 Int Conf on Parallel Architectures &amp;amp; Compilation Techniques (PACT) .Piscataway, NJ:IEEE, 2016:441- 442
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_26" title="Shen Xiaowei, Ye Xiaochun, Wang Da, et al.Optimizing dataflow architecture for scientific applications[J].Chinese Journal of Computers, 2017, 40 (9) :223- 238 (in Chinese) (申小伟, 叶笑春, 王达, 等.一种面向科学计算的数据流优化方法[J].计算机学报, 2017, 40 (9) :223- 238) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201709014&amp;v=MTg0NzFxRmlEbVZyN0tMejdCZHJHNEg5Yk1wbzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        Shen Xiaowei, Ye Xiaochun, Wang Da, et al.Optimizing dataflow architecture for scientific applications[J].Chinese Journal of Computers, 2017, 40 (9) :223- 238 (in Chinese) (申小伟, 叶笑春, 王达, 等.一种面向科学计算的数据流优化方法[J].计算机学报, 2017, 40 (9) :223- 238) 
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_27" title="Tan Xu, Ye Xiaochun, Shen Xiaowei, et al.A pipelining loop optimization method for dataflow architecture[J].Journal of Computer Science and Technology, 2018, 33 (1) :116- 130" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A pipelining loop optimization method for dataflow architecture">
                                        <b>[27]</b>
                                        Tan Xu, Ye Xiaochun, Shen Xiaowei, et al.A pipelining loop optimization method for dataflow architecture[J].Journal of Computer Science and Technology, 2018, 33 (1) :116- 130
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_28" title="Ye Xiaochun, Fan Dongrui, Sun Ninghui, et al.SimICT:A fast and flexible framework for performance and power evaluation of large-scale architecture[C] //Proc of IEEE Int Symp on Low Power Electronics &amp;amp; Design.Piscataway, NJ:IEEE, 2013:273- 278" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SimICT:A fast and flexible framework for performance and power evaluation of large-scale architecture">
                                        <b>[28]</b>
                                        Ye Xiaochun, Fan Dongrui, Sun Ninghui, et al.SimICT:A fast and flexible framework for performance and power evaluation of large-scale architecture[C] //Proc of IEEE Int Symp on Low Power Electronics &amp;amp; Design.Piscataway, NJ:IEEE, 2013:273- 278
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(06),1192-1204 DOI:10.7544/issn1000-1239.2019.20190117            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于细粒度数据流架构的稀疏神经网络全连接层加速</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%91%E9%99%B6%E7%84%B6&amp;code=42095399&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">向陶然</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E7%AC%91%E6%98%A5&amp;code=17769045&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶笑春</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%96%87%E6%98%8E&amp;code=28099695&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李文明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E7%85%9C%E6%99%B6&amp;code=39802654&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯煜晶</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E6%97%AD&amp;code=30468184&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭旭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B5%A9&amp;code=09597026&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8C%83%E4%B8%9C%E7%9D%BF&amp;code=09559507&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">范东睿</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E6%89%80)&amp;code=0142480&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机体系结构国家重点实验室(中国科学院计算技术研究所)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>深度神经网络 (deep neural network, DNN) 是目前最先进的图像识别算法, 被广泛应用于人脸识别、图像识别、文字识别等领域.DNN具有极高的计算复杂性, 为解决这个问题, 近年来涌出了大量可以并行运算神经网络的硬件加速器.但是, DNN中的全连接层有大量的权重参数, 对加速器的带宽提出了很高的要求.为了减轻加速器的带宽压力, 一些DNN压缩算法被提出.然而基于FPGA和ASIC的DNN专用加速器, 通常是通过牺牲硬件的灵活性获得更高的加速比和更低的能耗, 很难实现稀疏神经网络的加速.而另一类基于CPU, GPU的CNN加速方案虽然较为灵活, 但是带来很高的能耗.细粒度数据流体系结构打破了传统的控制流结构的限制, 展示出了加速DNN的天然优势, 它在提供高性能的运算能力的同时也保持了一定的灵活性.为此, 提出了一种在基于细粒度数据流体系结构的硬件加速器上加速稀疏的DNN全连接层的方案.该方案相较于原有稠密的全连接层的计算减少了2.44×～6.17×的峰值带宽需求.此外细粒度数据流加速器在运行稀疏全连接层时的计算部件利用率远超过其他硬件平台对稀疏全连接层的实现, 平均比CPU, GPU和mGPU分别高了43.15%, 34.57%和44.24%.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%86%E7%B2%92%E5%BA%A6%E6%95%B0%E6%8D%AE%E6%B5%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">细粒度数据流;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%9A%E7%94%A8%E5%8A%A0%E9%80%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">通用加速器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E9%87%8D%E7%94%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据重用;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%B9%B6%E8%A1%8C%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高并行性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    向陶然, xiangtaoran@ict.ac.cn, born in 1992.PhD candidate.Her main research interests include computer architecture, dataflow architecture.;
                                </span>
                                <span>
                                    *叶笑春, yexiaochun@ict.ac.cn, born in 1981.PhD, associate professor.Member of CCF.His main research interests include algorithm paralleling and optimizing, software simulation, and architecture for high-performance computing.;
                                </span>
                                <span>
                                    Li Wenming, born in 1988.PhD, associate professor. His main research interests include high throughput computing architecture and software simulation.;
                                </span>
                                <span>
                                    Feng Yujing, born in 1984.PhD candidate.Her main research interests include computer architecture, heterogeneous system, dataflow architecture.;
                                </span>
                                <span>
                                    Tan Xu, born in 1991.PhD candidate.His main research interests include high throughput computing architecture and dataflow architecture.;
                                </span>
                                <span>
                                    Zhang Hao, born in 1981.PhD, associate professor.Member of CCF.Associate chief architect of the Godson-T many core processor.His main research interests include high throught CPU micro-architectures and application analysis.;
                                </span>
                                <span>
                                    Fan Dongrui, born in 1979. PhD, professor, PhD supervisor.Senior member of CCF. His main research interests include many-core processor design, high throughput processor design and lowpower micro-architecture.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2018YFB1003501);</span>
                                <span>国家自然科学基金项目 (61732018, 61872335, 61802367);</span>
                                <span>中国科学院国际伙伴计划 (171111KYSB20170032);</span>
                                <span>计算机体系结构国家重点实验室创新项目 (CARCH3303, CARCH3407, CARCH3502, CARCH3505);</span>
                    </p>
            </div>
                    <h1><b>Accelerating Fully Connected Layers of Sparse Neural Networks with Fine-Grained Dataflow Architectures</b></h1>
                    <h2>
                    <span>Xiang Taoran</span>
                    <span>Ye Xiaochun</span>
                    <span>Li Wenming</span>
                    <span>Feng Yujing</span>
                    <span>Tan Xu</span>
                    <span>Zhang Hao</span>
                    <span>Fan Dongrui</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory of Computer Architecture (Institute of Computing Technology, Chinese Academy of Sciences)</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep neural network (DNN) is a hot and state-of-the-art algorithm which is widely used in applications such as face recognition, intelligent monitoring, image recognition and text recognition. Because of its high computational complexity, many efficient hardware accelerators have been proposed to exploit high degree of parallel processing for DNN. However, the fully connected layers in DNN have a large number of weight parameters, which imposes high requirements on the bandwidth of the accelerator. In order to reduce the bandwidth pressure of the accelerator, some DNN compression algorithms are proposed. But accelerators which are implemented on FPGAs and ASICs usually sacrifice generality for higher performance and lower power consumption, making it difficult to accelerate sparse neural networks. Other accelerators, such as GPUs, are general enough, but they lead to higher power consumption. Fine-grained dataflow architectures, which break conventional Von Neumann architectures, show natural advantages in processing DNN-like algorithms with high computational efficiency and low power consumption. At the same time, it remains broadly applicable and adaptable. In this paper, we propose a scheme to accelerate the sparse DNN fully connected layers on a hardware accelerator based on fine-grained dataflow architecture. Compared with the original dense fully connected layers, the scheme reduces the peak bandwidth requirement of 2.44×～ 6.17×. In addition, the utilization of the computational resource of the fine-grained dataflow accelerator running the sparse fully-connected layers far exceeds the implementation by other hardware platforms, which is 43.15%, 34.57%, and 44.24% higher than the CPU, GPU, and mGPU, respectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fine-grained%20dataflow&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fine-grained dataflow;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparse%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparse neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=general%20accelerator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">general accelerator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20reuse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data reuse;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high%20parallel&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high parallel;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Plan of China (2018YFB1003501);</span>
                                <span>the National Natural Science Foundation of China (61732018, 61872335, 61802367);</span>
                                <span>the International Partnership Program of Chinese Academy of Sciences (171111KYSB20170032);</span>
                                <span>the Innovation Project of the State Key Laboratory of Computer Architecture (CARCH3303, CARCH3407, CARCH3502, CARCH3505);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="66">深度神经网络近几年在飞速发展, 它们在人脸识别、智能监控、图像识别、文字识别等领域有着非常出色的表现.特别是在2012年多伦多大学的Alex Krizhevsky团队凭借他们提出的深度神经网络分类模型AlexNet<citation id="260" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 获得了ImageNet挑战赛冠军.他们把分类误差记录从26%降到了15%.自此之后, 大量公司和学者都投入了深度学习的研究中.</p>
                </div>
                <div class="p1">
                    <p id="67">目前常用的DNN算法都具有大量的权重, 其中全连接层 (fully connected layers, FC layers) 的权重比例非常高.例如, AlexNet有61×10<sup>6</sup>的权重参数, 而这其中全连接层的参数数量有59×10<sup>6</sup>.VGG-16<citation id="261" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>有138×10<sup>6</sup>的权重参数, 其中全连接层的参数数量有124×10<sup>6</sup>.为减少DNN算法的权重数量, 文献<citation id="262" type="reference">[<a class="sup">3</a>]</citation>提出了一种使用剪枝 (pruning) 、权值量化与共享 (weight quanti-zation and shared) 和哈夫曼编码 (Huffman coding) 压缩权重的方法.如表1所示, 经过剪枝后AlexNet和VGG-16在全连接层的权重数量大量减少.</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表1 剪枝后AlexNet与VGG-16中全连接层剩余的有效权重比率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Compression Statistics for FC Layers in AlexNet and VGG-16 After Pruning</b></p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td><br />DNN Model</td><td>Layer</td><td>#Weights</td><td>Weights Rate/%</td></tr><tr><td><br /></td><td>FC6</td><td>38×10<sup>6</sup></td><td>9</td></tr><tr><td><br />AlexNet</td><td>FC7</td><td>17×10<sup>6</sup></td><td>9</td></tr><tr><td><br /></td><td>FC8</td><td>4×10<sup>6</sup></td><td>25</td></tr><tr><td><br /></td><td>FC6</td><td>103×10<sup>6</sup></td><td>4</td></tr><tr><td><br />VGG-16</td><td>FC7</td><td>17×10<sup>6</sup></td><td>4</td></tr><tr><td><br /></td><td>FC8</td><td>4×10<sup>6</sup></td><td>23</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">虽然压缩DNN可以减少存储权重的空间, 然而目前的DNN加速器并不能很好支持这样的算法执行.</p>
                </div>
                <div class="p1">
                    <p id="70">近年来涌现出的许多DNN硬件加速器, 例如DianNao<citation id="263" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, TPU<citation id="264" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, Eyeriss<citation id="265" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等, 充分利用了DNN的并行性和数据重用的特征, 提出了低功耗高性能的DNN硬件加速方法.但是这些专用加速器只能运行稠密的DNN, 甚至部分加速器不能支持DNN中一些常见的层, 如激活层和局部响应归一化层.只有少量加速器, 如EIE<citation id="266" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和Cambricon-X<citation id="267" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 可以运行稀疏的神经网络.但是同样地在这些不灵活的硬件上很难实现一些新的算法.</p>
                </div>
                <div class="p1">
                    <p id="71">细粒度数据流加速器已经在科学计算领域<citation id="268" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和大数据领域<citation id="269" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>广泛应用, 它在具有高性能的同时也保持着很高的通用性.本文构建的一个细粒度数据流加速器 (fine-grained dataflow processing units, FDPU) , 它可以加速Stencil、FFT和矩阵乘等高性能应用<citation id="271" type="reference"><link href="224" rel="bibliography" /><link href="226" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>, 此外文献<citation id="270" type="reference">[<a class="sup">13</a>]</citation>还证明了DNN在细粒度数据流结构上的实现可以获得非常好的效率和能效比.如图1所示, FDPU (16tiles, 8.192Tops) 在运行AlexNet各层时, 相较于GPU (NVIDIA Tesla K80, 8.73TFLOPS) 有1.48× ～ 26×的加速.</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_072.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 FDPU相比于GPU在运行AlexNet各层时的加速比" src="Detail/GetImg?filename=images/JFYZ201906008_072.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 FDPU相比于GPU在运行AlexNet各层时的加速比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_072.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Speedup of FDPU over GPU running layers of AlexNet</p>

                </div>
                <div class="p1">
                    <p id="73">为了更高效地在细粒度数据流加速器上实现DNN的加速, 我们提出了一种在细粒度数据流加速器上加速稀疏的全连接层的方法.这种方法可以减少加速器对内存数据的访问量, 并且基本不增加硬件设计, 同时对加速器的性能影响也较小.</p>
                </div>
                <div class="p1">
                    <p id="74">本文的贡献主要有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="75">1) 提出了一种适合细粒度数据流加速器的压缩数据格式, 并提出了读取该压缩数据格式的数据流图.</p>
                </div>
                <div class="p1">
                    <p id="76">2) 分析了全连接层的特点, 为稀疏的神经网络提出了在FDPU上的加速方案.</p>
                </div>
                <div class="p1">
                    <p id="77">3) 对比了FDPU与CPU, GPU和专用数据流加速器的实验结果.从实验结果可知, 本文提出的加速稀疏的全连接层的方法相较于原有稠密的全连接层运算减少了2.44×～ 6.17×的峰值带宽需求.在输入图像的批次大小 (batch size) 为64时, FDPU运行稀疏全连接层的计算部件利用率远超过其他硬件平台, 平均比CPU, GPU和mGPU分别高了43.15%, 34.57%和44.24%.</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="79" name="79"><b>1.1 通用细粒度数据流加速器</b></h4>
                <div class="p1">
                    <p id="80">细粒度数据流架构由Dennis<citation id="272" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出, 其结构与传统的控制流架构完全不同.细粒度数据流结构具有较好的指令并行性、数据复用性和低功耗等特点.细粒度数据流架构具有3种特性:</p>
                </div>
                <div class="p1">
                    <p id="81">1) 数据流结构中的指令只要其操作数准备好了即可被执行, 指令的执行不受程序计数器和指令窗口的限制, 可以充分挖掘指令级并行和数据级并行;</p>
                </div>
                <div class="p1">
                    <p id="82">2) 数据在执行单元 (PE) 间直接通信, 避免了频繁的数据存取, 减少访存开销;</p>
                </div>
                <div class="p1">
                    <p id="83">3) 执行单元不需要如乱序执行、分支预测、深度流水等复杂的逻辑控制, 简化了执行单元的设计.</p>
                </div>
                <div class="p1">
                    <p id="84">TRIPS<citation id="273" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是由德克萨斯大学的Burger等人提出的细粒度数据流体系结构.TRIPS可以同时支持8个程序块 (frame) 并行运行, 用以掩盖指令之间传输操作数的延迟.程序块的指令被映射到4×4执行单元 (PE) 阵列上, 每个PE执行被分配到的部分指令, 并将执行结果传递给目的指令.程序块执行完成后, TRIPS调度下一个程序块, 将下一个程序块的指令映射到执行阵列上执行.WaveScalar<citation id="274" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>是华盛顿大学的Swanson提出的基于簇 (cluster) 的可扩展数据流体系结构.每个簇包含4个域 (domains) , 每个域包含8个PE.数据流程序中的每条指令被映射到PE中.在程序执行期间, WaveScalar不断替换无用指令并将新的未执行指令加载到PE中, 就如同波浪一样一波一波地执行指令.2种数据流体系结构都是通用处理器, 无法充分利用DNN中的并行性来掩盖操作数传输的延迟, 导致功能单元的利用率较低.</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>1.2 基于数据流思想的专用DNN加速器</b></h4>
                <div class="p1">
                    <p id="86">相比于功耗较高的通用计算引擎, 例如图形处理单元 (GPU) , 越来越多的研究人员提出了在功耗和性能上更具优势的基于ASIC<citation id="275" type="reference"><link href="236" rel="bibliography" /><link href="238" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>或基于FPGA<citation id="276" type="reference"><link href="240" rel="bibliography" /><link href="242" rel="bibliography" /><link href="244" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>的专用DNN加速器.其中基于脉动阵列的DNN加速器或者是简单的控制数据流入流出的DNN专用加速器都可以视为数据流思想延伸的产物.</p>
                </div>
                <div class="p1">
                    <p id="87">纽约大学的Farabet等人在2011年提出了一款面向卷积神经网络的数据流加速器NeuFlow<citation id="277" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, 它是一个运行时可重配置的数据流结构.DianNao<citation id="278" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>是由中国科学院计算技术研究所的陈云霁于2014年提出的深度学习专用加速器.它采用了基于分时复用的加速器设计结构, 该结构由神经功能部件 (neural functional unit, NFU) 和片上存储构成.DianNao可以说开启了专用DNN加速芯片研究的风潮.2016年麻省理工大学的Sze提出了卷积神经网络加速器Eyeriss<citation id="279" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.Eyeriss使用了一种最小化数据传输功耗开销的数据流模型, 可以利用DNN具有的所有数据重用类型.TPU<citation id="280" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>是谷歌提出的机器学习专用芯片, 它使用了256×256的脉动阵列加速矩阵乘和卷积, 实现了非常高的吞吐量和极短的响应时间.FlexFlow<citation id="281" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation> 提出了基于多种并行类型互补的卷积实现, 提高了计算资源的利用率.此外, 它可以为不同的卷积层提供最优的并行混合方案.EIE<citation id="282" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是2016年斯坦福大学提出的用于加速稀疏的全连接层和RNN等神经网络的专用加速器.Cambricon-X<citation id="283" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一款既可以加速稠密神经网络, 也可以加速稀疏神经网络的硬件加速器.但是这些专用数据流加速器为了获得更高的性能牺牲了硬件的灵活性, 有些加速器甚至不能支持DNN的全部层.相较于这些加速器, 细粒度数据流体系结构可以提供更高的灵活性和更广的应用范围, 针对不同应用自身的特点挖掘其并行性.如本文提出的架构可以用于数据中心<citation id="284" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>加速矩阵乘、FFT、Stencil等高性能应用, 同时也能用于加速神经网络等应用.</p>
                </div>
                <h3 id="88" name="88" class="anchor-tag"><b>2 DNN全连接层背景</b></h3>
                <div class="p1">
                    <p id="89">DNN在各个领域和各种应用中具有不同的形状和尺寸, 比较著名的DNN模型有AlexNet<citation id="285" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, VGG-16<citation id="286" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等, 这些模型独特的结构决定了它们能达到的精度和效率.DNN是由一系列层 (layer) 构成的, 主要有卷积层 (convolutional layers, CONV) 、池化层 (pooling layers, POOL) 、全连接层 (fully connected layers, FC) 、激活层 (activation layers, ACT) 和局部响应归一化层 (local response normali-zation layers, LRN) 等.DNN的输入是一组需要被网络分析的信息, 这些值可以是图像的像素、音频的采样幅度等数值表示.这些输入通过DNN的推断后, 就会得到相应的分析结果.DNN的精度还与它各个层中权重的值有关, 可以通过训练对网络中的权值进行调整.</p>
                </div>
                <div class="p1">
                    <p id="90">全连接层是DNN的重要组成部分, 是DNN所有层中参数最多的层.全连接层通过使用滤波器权重从输入数据中提取特征.全连接层的参数如表2所示.</p>
                </div>
                <div class="p1">
                    <p id="91">全连接层的输入是由从多张输入图像提取出的一组2-D输入特征图 (input feature maps) 构成.一张输入图像提取出的一组输入特征图称为一个通道 (channel) , 每次输入网络的一组图片称为一个批次 (batch) .所以输入集是一个四维张量, 其参数分别是一个batch内的图像个数<i>N</i>、每张图像的输入特征图数量<i>C</i>、输入特征图的行数<i>H</i>、输入特征图的列数<i>W</i>, 本文用<b><i>I</i></b>∈R<sup><i>NCHW</i></sup>表示.全连接层的权重 (filters) 也是一个四维张量, 其参数分别是每张图像的输入特征图数量<i>C</i>、每张图像的输出特征图 (output feature maps) 数量<i>K</i>、权重的行数<i>R</i>、权重的列数<i>S</i>, 本文中用<b><i>F</i></b>∈R<sup><i>KCRS</i></sup>表示.在全连接层中, 每一个结点都与上一层的所有结点相连.因此输入特征图与权重大小相同, 即<i>H</i>=<i>R</i>, <i>W</i>=<i>S</i>.多个2-D filters会组成一个filter channel, 分别与一个channel内对应的2-D输入特征图卷积, 所得结果累加获得一个输出特征图.每个输入channel都会和<i>K</i>个filter channel卷积, 从而获得输出特征图的四维张量, 本文中用<b><i>O</i></b>∈R<sup><i>NKPQ</i></sup>表示 (其中<i>P</i>是输出特征图的行数, <i>Q</i>是输出特征图的列数, <i>P</i>=<i>Q</i>=1) .</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表2 全连接层的参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Parameters of FC Layers</b></p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td><br />Parameter</td><td>Description</td></tr><tr><td><br /><i>N</i></td><td>Number of images in batch</td></tr><tr><td><br /><i>C</i></td><td>Number of ifmaps per image/number of <br />filters per channel</td></tr><tr><td><br /><i>H</i></td><td>Height of ifmaps</td></tr><tr><td><br /><i>W</i></td><td>Width of ifmaps</td></tr><tr><td><br /><i>K</i></td><td>Number of ofmaps per image/number of <br />filter channels</td></tr><tr><td><br /><i>R</i></td><td>Height of filters</td></tr><tr><td><br /><i>S</i></td><td>Width of filters</td></tr><tr><td><br /><i>P</i></td><td>Height of ofmaps</td></tr><tr><td><br /><i>Q</i></td><td>Width of ofmaps</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="93">可以得到全连接层的计算为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ο</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>p</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>q</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>C</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>R</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>F</mi></mstyle></mrow></mstyle></mrow></mstyle><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>r</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>s</mi><mo stretchy="false">]</mo><mo>×</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>r</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>s</mi><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>≤</mo><mi>n</mi><mo>&lt;</mo><mi>Ν</mi><mo>, </mo><mspace width="0.25em" /><mn>0</mn><mo>≤</mo><mi>k</mi><mo>&lt;</mo><mi>Κ</mi><mo>, </mo><mspace width="0.25em" /><mi>p</mi><mo>=</mo><mi>q</mi><mo>=</mo><mn>0</mn><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">从式 (1) 可以看出, 全连接层可以看作是输入特征图与权重大小相同的特殊卷积层.根据全连接的运算特点, 当<i>N</i>=1时, 通常将其转换为矩阵向量乘计算, 其形式如图2所示.<i>N</i>&gt;1时, 则可以转化为矩阵乘.</p>
                </div>
                <div class="p1">
                    <p id="96"><i>N</i>=1时, 得到全连接层的公式为</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ο</mi><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>C</mi><mi>R</mi><mi>S</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>F</mi></mstyle><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo><mo>×</mo><mi>Ι</mi><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>≤</mo><mi>n</mi><mo>&lt;</mo><mi>Ν</mi><mo>, </mo><mspace width="0.25em" /><mn>0</mn><mo>≤</mo><mi>k</mi><mo>&lt;</mo><mi>Κ</mi><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 当N=1时全连接层的计算形式" src="Detail/GetImg?filename=images/JFYZ201906008_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 当<i>N</i>=1时全连接层的计算形式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Computation of FC layers when <i>N</i>=1</p>

                </div>
                <h3 id="99" name="99" class="anchor-tag"><b>3 FDPU结构</b></h3>
                <h4 class="anchor-tag" id="100" name="100"><b>3.1 细粒度数据流体系结构概述</b></h4>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 FDPU体系结构框图" src="Detail/GetImg?filename=images/JFYZ201906008_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 FDPU体系结构框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 FDPU architecture diagram</p>

                </div>
                <div class="p1">
                    <p id="102">FDPU是面向计算密集型的、具有简单的访存模式和数据重用特征应用的细粒度数据流加速芯片.FDPU是基于ASIC实现的, 它支持细粒度数据流指令集, 通过最大化数据流指令级并行性提高其性能.FDPU的总体结构如图3所示.FDPU是由执行单元 (processing element, PE) 、数据缓存 (data buffer, Dbuf) 、指令缓存 (command buffer, Cbuf) 、一个微控制器 (micro controller, MicC) 和一个直接存储器存取 (direct memory access, DMA) 组成.PE阵列呈二维结构排列.Dbuf和Cbuf是用便签式存储 (scratch pad memory, SPM) 的存储访问形式实现的.Dbuf分布在PE阵列的周围, Cbuf则位于PE阵列的左侧.MicC用于控制指令在PE上的执行.PE, Dbuf, Cbuf, MicC通过2-D mesh网络相互通信.</p>
                </div>
                <div class="p1">
                    <p id="103">1) PE.PE的内部结构如图3的下半部分所示.每个PE包含可流水的执行单元、指令缓冲、操作数缓冲、指令发射控制器和本地寄存器单元.</p>
                </div>
                <div class="p1">
                    <p id="104">2) Dbuf.用于存储数据, 被所有PE共享.</p>
                </div>
                <div class="p1">
                    <p id="105">3) Cbuf.用于存储要映射到PE中的指令和PE的本地寄存器值.</p>
                </div>
                <div class="p1">
                    <p id="106">4) MicC.负责控制加速器的运行过程.执行开始时, CPU会向MicC发送请求启动加速器.然后MicC会通过Mesh网络把Cbuf中存储的指令和本地寄存器的值送到PE内.PE初始化完成后, MicC会向PE阵列流水地发送上下文启动指令和上下文ID, 并收集PE发来的上下文结束消息.当PE阵列将所有上下文都执行完后, MicC会向CPU发送结束消息.</p>
                </div>
                <div class="p1">
                    <p id="107">5) NoC.Router之间采用静态<i>XY</i> (<i>X</i>方向优先) 的确定性路由策略.</p>
                </div>
                <div class="p1">
                    <p id="108">6) DMA.负责Cbuf、Dbuf和内存的数据交换.</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>3.2 并行模式</b></h4>
                <div class="p1">
                    <p id="110">传统细粒度数据流架构 (如TRIPS<citation id="287" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>) , 将应用的程序划分为多个块, 程序块之间必须串行执行.只有前一个程序块执行完后, 才会载入和运行新的程序块, 这造成计算部件的利用率不高.因此, FDPU不只是简单地采用了数据流模式, 还采用了循环流水<citation id="288" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>的优化方法提高计算部件的利用率.</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_111.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 数据流执行过程" src="Detail/GetImg?filename=images/JFYZ201906008_111.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 数据流执行过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_111.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Process of dataflow executing</p>

                </div>
                <h4 class="anchor-tag" id="112" name="112">1) 数据流模式</h4>
                <div class="p1">
                    <p id="113">数据流模式是一种与传统控制流完全不同的计算模式.在传统控制流处理器中, 指令按照程序计数器 (program counter, PC) 的顺序执行.但是, 在数据流模式中, 只要指令的操作数准备好了, 那么这条指令即可被执行.</p>
                </div>
                <div class="p1">
                    <p id="114">在数据流计算中, 程序是以数据流图表示的, 每条指令的执行结果直接传递到另外一条指令, 指令与指令之间通过依赖边来建立依赖关系, 从而形成数据流图.数据流图中的每个节点表示一条指令, 每条边表示一条指令与另一条指令之间的依赖关系.如图4 (a) 所示, 把数送入数据流图, 指令就会依据依赖关系依次被发射.例如当Inst3收到Inst0和Inst1输出的结果后, Inst3就可以执行了.</p>
                </div>
                <div class="p1">
                    <p id="115">在PE中, 操作数缓冲中的每一个条目是被一条指令私有的, 存储属于这条指令的所有操作数.指令发射控制器会监视指令所需的所有操作数是否都已就位.当指令满足发射条件, 指令发射控制器会将指令和其操作数送入译码器中.译码完成后, opcode和操作数被送入可流水的计算部件中.最终, 指令的计算结果会通过Mesh网络传输到Dbuf中或依赖于这条指令的其他指令的数据缓冲条目中.</p>
                </div>
                <div class="p1">
                    <p id="116">数据流图的指令会被映射到PE中, 图4 (b) 给出了图4 (a) 中的一种可能的映射结果.文献<citation id="289" type="reference">[<a class="sup">26</a>]</citation>提出了一种在细粒度数据流体系结构中使用的指令映射算法——基于负载均衡 (load balance centric, LBC) 的指令映射算法.LBC算法按照深度的优先顺序依次映射数据流图中的所有指令, 对每条指令分别计算执行单元阵列中所有位置的代价, 取最小代价的位置作为最佳映射位置.在本文中, 为了获得尽可能更优的性能, 数据流图的映射则是采用了手动映射的方法.</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">2) 循环流水模式</h4>
                <div class="p1">
                    <p id="118">数据流程序可以在加速器上被多次执行, 一个数据流程序的一次完整执行称为一个上下文.循环流水的优化方法面向具有可分块和并行性特征的应用, 进一步利用上下文间的并行处理特征修改数据流结构中上下文切换逻辑, 使每一个上下文的开始不需要等待上一个上下文的结束.这样循环流水化结构上的多个上下文以流水线的方式进入执行阵列, 数据流图中的每一条指令在执行一次后会立刻接收到下一个上下文的操作数, 使执行单元的利用率得到极大的提高.</p>
                </div>
                <div class="p1">
                    <p id="119">如图4 (c) 所示, 多个上下文以流水线的方式流过整个数据图, 结果以流的方式从数据流图中流出.Inst5执行完上下文1的数据操作后, 立刻接收到上下文2的操作数, 可以再次执行运算.用户只需要配置MicC, 设置好需要计算的上下文数量.FDPU开始计算后, MicC将上下文流水送入执行阵列, 并统计已经完成的上下文数量, 确定当前计算是否完成.在上下文之间完全没有数据依赖时, 所有上下文可以在加速器上充分流水;若上下文间有数据依赖, 可通过MicC中的上下文控制逻辑控制上下文进入执行阵列的时间, 维持有数据依赖的上下文之间的顺序.</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.3 数据流指令集</b></h4>
                <div class="p1">
                    <p id="121">FDPU的指令格式如图5所示.每个指令由指令码、指令依赖的源操作数个数、立即数、结果的目的地址组成.目的地址指向某个PE中的操作数缓冲的地址.FDPU的指令集包含了基础的运算指令、访存指令和循环指令<citation id="290" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>等.表3列出了本文中用到的数据流指令和指令对应的功能.</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_122.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 FDPU的指令格式" src="Detail/GetImg?filename=images/JFYZ201906008_122.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 FDPU的指令格式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_122.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Instruction format of FDPU</p>

                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_123.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 使用SWITCH指令的数据流图例子" src="Detail/GetImg?filename=images/JFYZ201906008_123.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 使用SWITCH指令的数据流图例子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_123.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 A dataflow diagram with SWITCH instruction</p>

                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表3 数据流指令描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Descriptions of Dataflow Instructions</b></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td><br />Inst</td><td>Type</td><td>Function Descriptions</td></tr><tr><td><br />LD</td><td>Mix</td><td>Load data</td></tr><tr><td><br />ST</td><td>Mix</td><td>Store data</td></tr><tr><td><br />COPY</td><td>Mix</td><td>Transmit data to multiple destination</td></tr><tr><td><br />IMM</td><td>Integer</td><td>Generate integer constant</td></tr><tr><td><br />ADD</td><td>Integer</td><td>Addition</td></tr><tr><td><br />SUB</td><td>Integer</td><td>Subtraction</td></tr><tr><td><br />MUL</td><td>Integer</td><td>Multiplication</td></tr><tr><td><br />EQ</td><td>Integer</td><td>When two operands are equal, <br />return 1, else return 0</td></tr><tr><td><br />LSR</td><td>Integer</td><td>Right shift</td></tr><tr><td><br />AND</td><td>Integer</td><td>Bitwise AND</td></tr><tr><td><br />FIMM</td><td>Floating-point</td><td>Generate floating-point constant</td></tr><tr><td><br />FMUL</td><td>Floating-point</td><td>Multiplication</td></tr><tr><td><br />FMADD</td><td>Floating-point</td><td>Multiply-accumulate</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="125" name="125" class="anchor-tag"><b>4 基于稀疏的全连接层加速方案</b></h3>
                <h4 class="anchor-tag" id="126" name="126"><b>4.1 细粒度数据流分支指令设计</b></h4>
                <div class="p1">
                    <p id="127">为了增加数据流程序的控制能力、更好地支持稀疏的全连接层, 本文设计了用于细粒度数据流结构的分支指令——SWITCH指令.SWITCH语句会根据上游给它的第2个操作数的值与0比大小的结果判断将数据发送给哪些下游指令.图6是一个使用SWITCH指令的数据流程序例子.图6 (a) 是这个数据流程序实现功能的伪代码.如果EQ发送给SWITCH0的值大于0, 则SWITCH0会沿着点划线发送从LD0接收来的数据, 接着会执行IMM1, MUL0指令;如果EQ发送给SWITCH0的值等于0, 则SWITCH0会沿着粗实线发送从LD0接收来数据, 接着会执行IMM3指令.点划线和粗实线最终都会将数据传输到同一个指令的相同操作数位置, 保证不论走哪个分支下游指令均可以发射.</p>
                </div>
                <div class="p1">
                    <p id="128">可以看到图6中ADD0指令有一条虚线指向SWITCH0, 这条依赖边是为了保证在循环流水模式下, 上下文流过SWITCH0的数据顺序与流过ADD0的数据顺序是一致的.如图7 (a) 所示, 在没有ADD0指向SWITCH0的数据依赖边时, 当2个上下文的数据在不同的分支流动, 后运行的上下文的数据有可能会先于前面的上下文到达后续的指令.这是因为指令执行 (数据流指令的发射没有固定的顺序) 和数据在片上网络传输的延迟是不确定的, 这种情况会造成运算结果的错误.</p>
                </div>
                <div class="p1">
                    <p id="129">在汇合指令ADD0增加一条指向SWITCH0的依赖边可以解决这样的问题, 这条边被称为反馈依赖边.SWITCH指令被增加了一个源操作数用于接收反馈信号, 在执行初始状态下, 这个操作数被初始化为已经收到;运行过程中, 每次SWITCH执行后都需要等到分支后的某个汇合指令给它发送的操作数, 才能进行下一次运算.如图7 (b) 中, ADD0担任了汇合指令的作用.汇合指令可以是任意种类的指令, 但它必须是SWITCH指令的2个分支汇合后的数据流指令.这样的方式可以在不增加硬件开销的情况下保证SWITCH和其后的指令的数据流动顺序一致.</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_130.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 SWITCH指令的反馈依赖边的功能说明" src="Detail/GetImg?filename=images/JFYZ201906008_130.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 SWITCH指令的反馈依赖边的功能说明  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_130.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Function description of the feedback data dependence edge of SWITCH</p>

                </div>
                <h4 class="anchor-tag" id="131" name="131"><b>4.2 面向细粒度数据流的稀疏存储格式</b></h4>
                <div class="p1">
                    <p id="132">常用的稀疏矩阵存储格式有很多, 比如坐标格式 (coordinate format, COO) 、压缩稀疏行格式 (compressed sparse row, CSR) 等.但是由于数据流程序的控制性较弱, 无法灵活地控制循环次数等, 在这些格式很难用数据流程序高效解析.比如COO, 它用一个三元组表示, 分别是 (行号, 列号, 数值) .读取到当前数据的行号与列号后, 才能知道数据的位置, 很难将数据划分为多个上下文并行处理.而CSR格式, 虽然其格式可以方便地推算得到每行数据的数量和数据的起始位置, 便于划分数据, 但是由于每行数据的数量不确定, 数据流图无法确定.对于细粒度数据流加速器, 需要一种更具有“确定性”的稀疏矩阵存储格式.</p>
                </div>
                <div class="p1">
                    <p id="133">本文提出了一种类似于CSR的存储格式, 称为面向细粒度数据流的压缩稀疏行格式 (fine-grained dataflow CSR, FD-CSR) .FD-CSR与CSR相同, 也由3部分组成:数值 (values) 、列号 (column indices) 以及行偏移 (row offsets) .数值用于存储数组中所有的非零元素;列号的每一个比特标志了一行数据中每个元素是否为0;行偏移则表示每行的第1个非零元素在value中的偏移位置.通过这样的设计, 可以使列号和行偏移都成为固定长度的数组, 方便数据流程序进行读取.</p>
                </div>
                <div class="p1">
                    <p id="135">如图8所示, 一个任意长度的一维数组会被重新排列为每行16个元素的二维数组.这是因为每个列号的数据位宽是16 bit, 只能表示最多16个元素的有效位.列号的每一个比特代表其对应的数据是否为0, 如第1个列号为6339, 其二进制数值为0001100011000011 (最低位对应第1行的第1个数据, 最高位对应第1行的最后一个数据) .该数组压缩后的列号和行偏移的数据个数均为二维数组的行数, 数值的数据个数则为数组中非零元素的个数.</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_136.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 FD-CSR稀疏矩阵存储格式" src="Detail/GetImg?filename=images/JFYZ201906008_136.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 FD-CSR稀疏矩阵存储格式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_136.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Format of FD-CSR</p>

                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_134.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 全连接层的基本操作" src="Detail/GetImg?filename=images/JFYZ201906008_134.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 全连接层的基本操作  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_134.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Basic operation of FC layers</p>

                </div>
                <div class="p1">
                    <p id="137">这样的稀疏矩阵存储格式的设计可以将列号和行偏移的个数固定.本文设计的数据流图会通过对列号和行偏移的分析, 同时使用SWITCH语句, 实现对不定个数的数值的解析访问.</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138"><b>4.3 读取稀疏存储格式的数据流图设计</b></h4>
                <div class="p1">
                    <p id="139">全连接层的计算通常被转化为矩阵向量乘.矩阵向量乘是由大量的向量点积组成的, 其操作如图9 (a) 所示, 其中<b><i>A</i></b>代表权重的一行, <b><i>B</i></b>代表输入特征图, <i>C</i>代表输出特征图的一个元素.<b><i>A</i></b>的每个元素与对应的<b><i>B</i></b>元素相乘, 其结果进行累加, 最终得到<i>C</i>.在稠密的矩阵向量乘中, 数据流图被写为图9 (b) 的形式.在稀疏的全连接层中, 权重使用了稀疏矩阵存储格式进行存储.为了读取稀疏的权重, 本文提出了如图9 (c) 所示的数据流图.</p>
                </div>
                <div class="p1">
                    <p id="140">数据流图首先获取了本行的列号和行偏移.列号被共享给了多条指令, AND0将列号与0x1按位与, 获得了本行第1个数是否为非零数的结果.同样AND1, AND2, AND3分别将右移了1位、2位、3位的值与0x1按位与, 分别获得了对应的数据是否为非零数的结果.行偏移首先被减一, 然后与AND0输出的结果相加.如果AND0输出1, 那么ADD0算出的就是本行第1个数在数值向量中的偏移.ADD0的结果会传给下一个ADD, 只有AND输出的值为1时, 即对应数据是非零数, ADD的输出结果才是对应数据在数值向量中的地址偏移.AND与ADD的结果均会被传给SWITCH指令.当AND输出1时, SWITCH会把ADD算出的偏移传给LD指令, LD指令从对应的存储地址取数并把获取到的数发送给FMUL或FMADD.当AND输出0时, SWITCH则会传输数据给FIMM, FIMM则将0值发送给下游.FMUL指令和FMADD指令会从2条分支中的一条获得向量<b><i>A</i></b>中的一个元素, 并与从LD指令传输来的向量<b><i>B</i></b>的元素相乘.FMADD指令将乘积与上游送来的<i>C</i>的部分和进行累加, 最终得到<i>C</i>.</p>
                </div>
                <h3 id="141" name="141" class="anchor-tag"><b>5 实验与结果</b></h3>
                <div class="p1">
                    <p id="142">在本节中, 我们使用本文提出的方法, 在FDPU上加速运算了经过剪枝后的AlexNet和VGG-16中的稀疏全连接层.</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143"><b>5.1 度量标准</b></h4>
                <div class="p1">
                    <p id="144">本文使用了计算部件利用率 (computing resource utilization) 来评估稀疏神经网络在不同峰值性能的硬件加速器的优化效果.</p>
                </div>
                <div class="area_img" id="146">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201906008_14600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_148.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 FDPU运行稠密全连接层和稀疏全连接层时的计算部件利用率" src="Detail/GetImg?filename=images/JFYZ201906008_148.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 FDPU运行稠密全连接层和稀疏全连接层时的计算部件利用率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_148.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Computing resource utilization of dense FC layers and sparse FC layers for FDPU</p>

                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>5.2 实验平台</b></h4>
                <div class="p1">
                    <p id="150">我们以中国科学院计算技术研究所自主研发的大规模并行模拟框架SimICT<citation id="291" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>为平台, 实现了一个C语言的精确于时钟的模拟器.实验环境具体配置如表1所示.FDPU模拟器的结构如图3所示, 主要包含ARM处理器、DMA控制器、内存和FDPU等模拟组件.</p>
                </div>
                <div class="p1">
                    <p id="151">FDPU加速器由8×8个PE、32个Dbuf、8个Cbuf、1个微控制器 (MicC) 和1个DMA控制器组成.所有Dbuf的大小均为2 MB, 每个PE内的操作数缓冲为6 KB.浮点数据均使用16-bit表示.每个PE有2个32-bit的定点乘加器、用于计算load/store的地址索引以及16-bit的SIMD4的乘加器.FDPU运行时的频率为1 GHz, 其峰值性能为512 Gops.</p>
                </div>
                <div class="p1">
                    <p id="152">我们用verilog对FDPU设计进行了实现和rtl级仿真, 并对模拟器进行了时钟级的校准.然后用45 nm的工艺进行了综合, 最终得到的面积约为44 mm<sup>2</sup>, 功耗约为3.27 W.</p>
                </div>
                <div class="p1">
                    <p id="153">实验中的对比基准我们使用了CPU (Intel Core i-7 5930k) , GPU (NVIDIA GeForce GTX Titan X) 和Mobile GPU (NVIDIA Tegra K1) .其性能数据均来自文献<citation id="292" type="reference">[<a class="sup">7</a>]</citation>.</p>
                </div>
                <h4 class="anchor-tag" id="154" name="154"><b>5.3 数据集</b></h4>
                <div class="p1">
                    <p id="155">我们选择了AlexNet和VGG-16中稀疏程度差异较大的3个层进行性能对比, 这3个层分别是AlexNet-FC6, AlexNet-FC8和VGG16-FC7, 它们的稀疏性分别为9%, 25%, 4%.我们对比了运行稠密的全连接层和稀疏的全连接层的性能.其中稠密的全连接层来自Caffe model zone, 稀疏的全连接层使用了文献<citation id="293" type="reference">[<a class="sup">3</a>]</citation>中剪枝的算法生成.</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156"><b>5.4 实验和结果</b></h4>
                <div class="p1">
                    <p id="157">图10展示了FDPU在加速稀疏全连接层与稠密全连接层所实现的计算部件利用率.我们可以看出, 本文提出的稀疏全连接层的加速方法虽然相较于稠密的全连接层增加了大量的指令, 但是仅有少量的性能损失.其中在batch size为1和4时, 两者的性能差距最高为1.4%;在batch size为64时, 稀疏全连接层的计算部件利用率平均比稠密的低11.7%.其主要原因在于batch size较小时, 全连接层的数据复用程度很低, 此时不论是稠密的还是稀疏的全连接层数据流图中计算指令的比例较低, 访存和计算访存地址的指令比例较高.这使得因读取稀疏矩阵数据所增加的指令在访存指令的占比很低, 导致稀疏与稠密的全连接层运行时计算部件的利用率相近.而batch size为64时, 因数据在全连接层的复用程度增高, 全连接层数据流图中计算指令的比例增加, 使得因读取稀疏矩阵数据所增加的指令在访存指令的占比增高, 导致了稀疏全连接层的性能较稠密的全连接层有较大的性能下降.</p>
                </div>
                <div class="p1">
                    <p id="158">从图10中我们还可以发现本文提出的稀疏全连接层的加速方法在不同的稀疏比例的全连接层中计算部件利用率基本相等.这是因为我们提出的方案为了适应细粒度数据流结构, 采取了比较稳定的数据流图设计.对于不同的全连接层, 其数据流图的流动方式、执行的指令条数相差不大.对于每个权重, 不论其是否为非零值, 我们都会计算出其对应的地址索引.因此在不同的稀疏比例的全连接层中, FDPU获得的计算部件利用率非常接近.</p>
                </div>
                <div class="p1">
                    <p id="159">图11展示了FDPU运行稀疏全连接层相较于稠密全连接层对带宽需求的减少比.稀疏全连接层相较于稠密全连接层可以减少2.44×～ 6.17×的峰值带宽需求.</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906008_160.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 稀疏FC层相较于稠密FC层对带宽需求的减少比" src="Detail/GetImg?filename=images/JFYZ201906008_160.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 稀疏FC层相较于稠密FC层对带宽需求的减少比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906008_160.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Bandwidtion requirement reduction of sparse  FC layers over dense FC layers</p>

                </div>
                <div class="p1">
                    <p id="161">表4列出了FDPU与CPU, GPU和mGPU在运行batch size为64的稀疏全连接层时的计算部件利用率对比.可以看到, FDPU的计算部件利用率远超过其他硬件平台, 平均比CPU, GPU和mGPU分别高了43.15%, 34.57%和44.24%.</p>
                </div>
                <div class="p1">
                    <p id="162">表5列出了FDPU与CPU, GPU和mGPU在运行稀疏全连接层时的峰值性能、面积效率和能效等的对比.其中FDPU的面积效率是CPU和GPU的17.9倍和2.7倍, 能效是CPU, GPU和mGPU的50倍、9.7倍和7.9倍.</p>
                </div>
                <div class="area_img" id="163">
                    <p class="img_tit"><b>表4 FDPU与其他平台运行稀疏FC层时的计算部件利用率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Computing Resource Utilization Comparison of FDPU and Other Platforms for Running Sparse FC Layers</b></p>
                    <p class="img_note"></p>
                    <table id="163" border="1"><tr><td><br />Platform</td><td>AlexNet-FC6</td><td>AlexNet-FC8</td><td>VGG16-FC7</td></tr><tr><td>CPU (Core i7-5930k) </td><td>7.92</td><td>2.99</td><td>18.16</td></tr><tr><td><br />GPU (Titan X) </td><td>17.32</td><td>7.66</td><td>29.84</td></tr><tr><td><br />mGPU (Tegra K1) </td><td>5.80</td><td>4.37</td><td>15.64</td></tr><tr><td><br />FDPU</td><td>53.26</td><td>52.01</td><td>53.26</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="164">
                    <p class="img_tit"><b>表5 FDPU与CPU, GPU和mGPU的面积效率和能效对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Area Efficiency and Energy Efficiency Comparison of CPU, GPU, mGPU, and FDPU</b></p>
                    <p class="img_note"></p>
                    <table id="164" border="1"><tr><td><br />Parameter</td><td>CPU (Core <br />i7-5930k) </td><td>GPU<br /> (Titan X) </td><td>mGPU<br /> (Tegra K1) </td><td>FDPU</td></tr><tr><td><br />Technology/nm</td><td>22</td><td>28</td><td>28</td><td>45</td></tr><tr><td><br />Hardware<br />Area/mm<sup>2</sup></td><td>356</td><td>601</td><td></td><td>44</td></tr><tr><td><br />Power/W</td><td>73</td><td>159</td><td>5.1</td><td>3.27</td></tr><tr><td><br />Peak Sparse M×V <br />Throughput/Gops</td><td>122</td><td>1 375</td><td>54</td><td>273</td></tr><tr><td><br />Area Efficiency/<br /> (Gops/mm<sup>2</sup>) </td><td>0.34</td><td>2.29</td><td></td><td>6.11</td></tr><tr><td><br />Energy Efficiency/<br /> (Gops·W<sup>-1</sup>) </td><td>1.67</td><td>8.65</td><td>10.59</td><td>83.49</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="165" name="165" class="anchor-tag"><b>6 关于卷积层的讨论</b></h3>
                <div class="p1">
                    <p id="166">本文提出的方法主要利用了权重的稀疏性.卷积层在FDPU上实现是通过循环展开成矩阵乘的方式实现.因此, 卷积层也可以利用这种方法减少存储空间和访存带宽需求.但是在卷积层中权重占输入数据的比重相较于全连接层较小, 例如在batch size为64时, AlexNet中的5个卷积层中权重分别占输入数据的0.35%, 9.43%, 19.35%, 19.35%和13.79%.本文提出的方法对于卷积层的存储空间和带宽需求的改善很小, 但是会造成一定的性能下降.因此在FDPU上目前卷积层还是用稠密的方式存储权重和计算.未来我们会进一步研究如何避免零值权重的计算, 从而提高FDPU加速卷积层和全连接层的性能.</p>
                </div>
                <h3 id="167" name="167" class="anchor-tag"><b>7 总  结</b></h3>
                <div class="p1">
                    <p id="168">为了更高效地在细粒度数据流加速器上实现DNN的加速, 我们提出了一种在细粒度数据流加速器上加速稀疏的全连接层的方法.这种方法可以减少加速器对内存数据的访问量, 并且基本不增加硬件设计, 同时对加速器的性能影响也较小.本文提出的加速稀疏的全连接层的方法相较于原有稠密的全连接层运算减少了2.44×～ 6.17×的峰值带宽需求.在batch size为64时, FDPU运行稀疏全连接层的计算部件利用率远超过其他硬件平台, 平均比CPU, GPU和mGPU分别高了43.15%, 34.57%和44.24%.但是本文中提出的方法无法利用全连接层的稀疏性减少运行的指令条数.未来工作中, 我们会继续研究稀疏全连接层的加速, 目标是设计出可以利用权重的稀疏性减少计算的数据流图.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="301" type="formula" href="images/JFYZ201906008_30100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">向陶然</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="302" type="formula" href="images/JFYZ201906008_30200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">叶笑春</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="303" type="formula" href="images/JFYZ201906008_30300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李文明</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="304" type="formula" href="images/JFYZ201906008_30400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">冯煜晶</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="305" type="formula" href="images/JFYZ201906008_30500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">谭旭</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="306" type="formula" href="images/JFYZ201906008_30600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张浩</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="307" type="formula" href="images/JFYZ201906008_30700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">范东睿</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="204">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[1]</b>Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C] //Proc of NIPS 2012.Cambridge, MA:MIT Press, 2012:1097- 1105
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[2]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint, arXiv:1409.1556, 2014
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep compression:Compressing deep neural networks with pruning,trained quantization and huffman coding">

                                <b>[3]</b>Han Song, Mao Huizi, Dally W J.Deep compression:Compressing deep neural networks with pruning, trained quantization and huffman coding[J].arXiv preprint, arXiv:1510.00149, 2015
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning">

                                <b>[4]</b>Chen Yunji, Chen Tianshi, Du Zidong, et al.DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning[C] //Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2014:269- 284
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indatacenter performance analysis of a tensor processing unit">

                                <b>[5]</b>Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C] //Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1- 17
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks">

                                <b>[6]</b>Chen Yu-Hsin, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:367- 379
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EIE:Efficient Inference Engine on Compressed Deep Neural Network">

                                <b>[7]</b>Han Song, Liu Xingyu, Mao Huizi, et al.EIE:Efficient inference engine on compressed deep neural network[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:243- 254
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cambricon-x:An accelerator for sparse neural networks">

                                <b>[8]</b>Zhang Shijin, Du Zidong, Zhang Lei, et al.Cambricon-X:An accelerator for sparse neural networks[C] //Proc of the 49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.20
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A matrix multiplier case study for an evaluation of a configurable dataflow-machine">

                                <b>[9]</b>Verdoscia L, Vaccaro R, Giorgi R.A matrix multiplier case study for an evaluation of a configurable dataflow-machine[C] //Proc of the 12th ACM Int Conf on Computing Frontiers.New York:ACM, 2015:1- 6
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Guide to Dataflow Supercomputing">

                                <b>[10]</b>Milutinovic M, Salom J, Trifunovic N, et al.Guide to Dataflow Supercomputing[M].Berlin:Springer, 2015
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An efficient network-onchip router for dataflow architecture">

                                <b>[11]</b>Shen Xiaowe, Ye Xiaochun, Tan Xu, et al.An efficient network-on-chip router for dataflow architecture[J].Journal of Computer Science and Technology, 2017, 32 (1) :11- 25
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-stop double buffering mechanism for dataflow architecture">

                                <b>[12]</b>Tan Xu, Shen Xiaowe, Ye Xiaochun, et al.A non-stop double buffering mechanism for dataflow architecture[J].Journal of Computer Science and Technology, 2018, 33 (1) :145- 157
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating CNN algorithm with fine-grained dataflow architectures">

                                <b>[13]</b>Xiang Taoran, Feng Yujing, Ye Xiaochun, et al.Accelerating CNN algorithm with fine-grained dataflow architectures[C] //Proc of 2018 IEEE 20th Int Conf on High Performance Computing and Communications.Los Alamitos, CA:IEEE Computer Society, 2018:243- 251
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=First version of a data flow procedure language">

                                <b>[14]</b>Dennis J B.First version of data flow procedure language[C] //Proc of Programming Symp, Proceedings Colloque Sur La Programmation.Berlin:Springer, 1974:362- 376
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TRIPS:A distributed explicit data graph execution (EDGE) microprocessor">

                                <b>[15]</b>Govindan M S S, Burger D.TRIPS:A distributed explicit data graph execution (EDGE) microprocessor[C] //Proc of 2007 IEEE Hot Chips 19 Symp (HCS) .Piscataway, NJ:IEEE, 2013:1- 13
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The WaveScalar Architecture">

                                <b>[16]</b>Swanson S.The WaveScalar Architecture[M].Saint Louis, Missouri:University of Washington, 2006
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902002&amp;v=MzE0NTVGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEbVZyN0tMeXZTZExHNEg5ak1yWTk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Chen Guilin, Ma Sheng, Guo Yang.Survey on accelerating neural network with hardware[J].Journal of Computer Research and Development, 2019, 56 (2) :240- 253 (in Chinese) (陈桂林, 马胜, 郭阳.硬件加速神经网络综述[J].计算机研究与发展, 2019, 56 (2) :240- 253) 
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201809005&amp;v=MjEwODdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEbVZyN0tMeXZTZExHNEg5bk1wbzlGWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Ji Rongrong, Lin Shaohui, Chao Fei, et al.Deep neural network compression and acceleration:A review[J].Journal of Computer Research and Development, 2018, 55 (9) :1871- 1888 (in Chinese) (纪荣嵘, 林绍辉, 晁飞, 等.深度神经网络压缩与加速综述[J].计算机研究与发展, 2018, 55 (9) :1871- 1888) 
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating fast algorithms for convolutional neural networks on FPGAs">

                                <b>[19]</b>Lu Liqiang, Liang Yun, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[C] //Proc of 2017 IEEE 25th Annual Int Symp on Field-Programmable Custom Computing Machines (FCCM) .Los Alamitos, CA:IEEE Computer Society, 2017:101- 108
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SpWA:An efficient sparse winograd convolutional neural networks accelerator on FPGAs">

                                <b>[20]</b>Lu Liqiang, Liang Yun.SpWA:An efficient sparse winograd convolutional neural networks accelerator on FPGAs[C] //Proc of 2018 55th ACM/ESDA/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2018:1- 6
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating fast algorithms for convolutional neural networks on FPGAs">

                                <b>[21]</b>Lu Liqiang, Liang Yun, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019:1- 1
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Neuflow:A runtime reconfigurable dataflow processor for vision.&amp;quot;">

                                <b>[22]</b>Farabet C, Martini B, Corda B, et al.NeuFlow:A runtime reconfigurable dataflow processor for vision[C] //Proc of 2011 IEEE Computer Society Conf on Computer Vision and Pattern Recognition Workshops (CVPR Workshops 2011) .Los Alamitos, CA:IEEE Computer Society, 2011:109- 116
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Flexflow:A flexible dataflow accelerator architecture for convolutional neural networks">

                                <b>[23]</b>Lu Wenyan, Yan Guihai, Li Jiajun, et al.FlexFlow:A flexible dataflow accelerator architecture for convolutional neural networks[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2017:553- 564
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SmarCo:An efficient many-core processor for high-throughput applica-tions in datacenters">

                                <b>[24]</b>Fan Dongrui, Li Wenming, Ye Xiaochun, et al.SmarCo:An efficient many-core processor for high-throughput applica-tions in datacenters[C] //Proc of 2018 IEEE Int Symp on High Performance Computer Architecture (HPCA) .Piscataway, NJ:IEEE, 2018:596- 607
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=POSTER:An optimization of dataflow architectures for scientific applica-tions">

                                <b>[25]</b>Shen Xiaowei, Ye Xiaochun, Tan Xu, et al.POSTER:An optimization of dataflow architectures for scientific applica-tions[C] //Proc of the 2016 Int Conf on Parallel Architectures &amp; Compilation Techniques (PACT) .Piscataway, NJ:IEEE, 2016:441- 442
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201709014&amp;v=MTIxMTVtVnI3S0x6N0Jkckc0SDliTXBvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGaUQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>Shen Xiaowei, Ye Xiaochun, Wang Da, et al.Optimizing dataflow architecture for scientific applications[J].Chinese Journal of Computers, 2017, 40 (9) :223- 238 (in Chinese) (申小伟, 叶笑春, 王达, 等.一种面向科学计算的数据流优化方法[J].计算机学报, 2017, 40 (9) :223- 238) 
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A pipelining loop optimization method for dataflow architecture">

                                <b>[27]</b>Tan Xu, Ye Xiaochun, Shen Xiaowei, et al.A pipelining loop optimization method for dataflow architecture[J].Journal of Computer Science and Technology, 2018, 33 (1) :116- 130
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SimICT:A fast and flexible framework for performance and power evaluation of large-scale architecture">

                                <b>[28]</b>Ye Xiaochun, Fan Dongrui, Sun Ninghui, et al.SimICT:A fast and flexible framework for performance and power evaluation of large-scale architecture[C] //Proc of IEEE Int Symp on Low Power Electronics &amp; Design.Piscataway, NJ:IEEE, 2013:273- 278
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201906008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906008&amp;v=MDMwMzVxcUJ0R0ZyQ1VSTE9lWmVScUZpRG1WcjdLTHl2U2RMRzRIOWpNcVk5RmJJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGVUaGl2a0RiaVg1d1FwND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

