

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127881150431250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908001%26RESULT%3d1%26SIGN%3ddN5n3skyb11812lXaAEa%252bkCgQ%252bs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908001&amp;v=MjUxNzdCdEdGckNVUkxPZVplUnJGeXZnVTd2S0x5dlNkTEc0SDlqTXA0OUZaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#189" data-title="&lt;b&gt;1 数据中心能效分析&lt;/b&gt; "><b>1 数据中心能效分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#191" data-title="&lt;b&gt;1.1 数据中心能耗分析&lt;/b&gt;"><b>1.1 数据中心能耗分析</b></a></li>
                                                <li><a href="#195" data-title="&lt;b&gt;1.2 数据中心能效衡量标准&lt;/b&gt;"><b>1.2 数据中心能效衡量标准</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#198" data-title="&lt;b&gt;2 数据中心能耗模型和节能机制&lt;/b&gt; "><b>2 数据中心能耗模型和节能机制</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#201" data-title="&lt;b&gt;2.1 数据中心能耗模型&lt;/b&gt;"><b>2.1 数据中心能耗模型</b></a></li>
                                                <li><a href="#221" data-title="&lt;b&gt;2.2 数据中心节能机制&lt;/b&gt;"><b>2.2 数据中心节能机制</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#229" data-title="&lt;b&gt;3 数据中心节能研究的分类比较&lt;/b&gt; "><b>3 数据中心节能研究的分类比较</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#237" data-title="&lt;b&gt;4 数据中心能耗研究进展&lt;/b&gt; "><b>4 数据中心能耗研究进展</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#239" data-title="&lt;b&gt;4.1 服务器系统节能&lt;/b&gt;"><b>4.1 服务器系统节能</b></a></li>
                                                <li><a href="#304" data-title="&lt;b&gt;4.2 网络设备节能&lt;/b&gt;"><b>4.2 网络设备节能</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#364" data-title="&lt;b&gt;5 总结和展望&lt;/b&gt; "><b>5 总结和展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#193" data-title="图1 数据中心能耗来源分解">图1 数据中心能耗来源分解</a></li>
                                                <li><a href="#226" data-title="图2 数据中心节能策略分类标准">图2 数据中心节能策略分类标准</a></li>
                                                <li><a href="#249" data-title="&lt;b&gt;表1 节能资源调度策略分类&lt;/b&gt;"><b>表1 节能资源调度策略分类</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="427">


                                    <a id="bibliography_1" title="Glanz J.Power, Pollution and the Internet[N/OL].New York:The New York Times, 2012-09-22 (2012-09-23) [2018-05-20].https:www.nytimes.com/2012/09/23/technology/datacenters-waste-vast-amounts-of-energy-belying-industry-image.html" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Power,pollution and the Internet">
                                        <b>[1]</b>
                                        Glanz J.Power, Pollution and the Internet[N/OL].New York:The New York Times, 2012-09-22 (2012-09-23) [2018-05-20].https:www.nytimes.com/2012/09/23/technology/datacenters-waste-vast-amounts-of-energy-belying-industry-image.html
                                    </a>
                                </li>
                                <li id="429">


                                    <a id="bibliography_2" title="Shehabi A, Smith S, Sartor D, et al.United states data center energy usage report, LBNL-1005775[R].Berkeley:Lawrence Berkeley National Laboratory, 2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=United states data center energy usage report">
                                        <b>[2]</b>
                                        Shehabi A, Smith S, Sartor D, et al.United states data center energy usage report, LBNL-1005775[R].Berkeley:Lawrence Berkeley National Laboratory, 2016
                                    </a>
                                </li>
                                <li id="431">


                                    <a id="bibliography_3" title="Koomey J.Growth in data center electricity use 2005to 2010[R/OL].New York:Analytical Press, (2011-08-01) [2018-05-20].https:alejandrobarros.com/wp-content/uploads/old/Growth_in_Data_Center_Electricity_use_2005_to_2010.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Growth in data center electricity use 2005to 2010[R/OL]">
                                        <b>[3]</b>
                                        Koomey J.Growth in data center electricity use 2005to 2010[R/OL].New York:Analytical Press, (2011-08-01) [2018-05-20].https:alejandrobarros.com/wp-content/uploads/old/Growth_in_Data_Center_Electricity_use_2005_to_2010.pdf
                                    </a>
                                </li>
                                <li id="433">


                                    <a id="bibliography_4" title="Anderson D, Morris P, Cader T, et al.A framework for data center energy productivity, Rev 2008-0[R].Oregon:The Green Grid, 2008" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A framework for data center energy productivity,Rev 2008-0">
                                        <b>[4]</b>
                                        Anderson D, Morris P, Cader T, et al.A framework for data center energy productivity, Rev 2008-0[R].Oregon:The Green Grid, 2008
                                    </a>
                                </li>
                                <li id="435">


                                    <a id="bibliography_5" title="Canalys.Data center infrastructure market will be worth$152 billion by 2016[OL]. (2012-07-11) [2018-05-20].https:www.canalys.com/newsroom/data-center-infrastructuremarket-will-be-worth-152-billion-2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data center infrastructure market will be worth$152 billion by 2016[OL]">
                                        <b>[5]</b>
                                        Canalys.Data center infrastructure market will be worth$152 billion by 2016[OL]. (2012-07-11) [2018-05-20].https:www.canalys.com/newsroom/data-center-infrastructuremarket-will-be-worth-152-billion-2016
                                    </a>
                                </li>
                                <li id="437">


                                    <a id="bibliography_6" title="Van Heddeghem W, Lambert S, Lannoo B, et al.Trends in worldwide ICT electricity consumption from 2007to 2012[J].Computer Communications, 2014, 50:64-76" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300032411&amp;v=MDA5NDRJMXNVYmhBPU5pZk9mYks4SHRMT3JJOUZaT2dOQ0gwNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Van Heddeghem W, Lambert S, Lannoo B, et al.Trends in worldwide ICT electricity consumption from 2007to 2012[J].Computer Communications, 2014, 50:64-76
                                    </a>
                                </li>
                                <li id="439">


                                    <a id="bibliography_7" title="Mittal S.Power management techniques for data centers:Asurvey[OL]. (2014-05-07) [2018-05-20].https:arxiv.org/abs/1404.6681" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Power management techniques for data centers:Asurvey[OL]">
                                        <b>[7]</b>
                                        Mittal S.Power management techniques for data centers:Asurvey[OL]. (2014-05-07) [2018-05-20].https:arxiv.org/abs/1404.6681
                                    </a>
                                </li>
                                <li id="441">


                                    <a id="bibliography_8" title="Barroso L A, Clidaras J, H9lzle U.The Datacenter as a Computer:An Introduction to the Design of Warehouse-scale Machines[M]San Rafael:Morgan&amp;amp;Claypool Publishers, 2013" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Datacenter as a Computer:An Introduction to the Design of Warehouse-scale Machines">
                                        <b>[8]</b>
                                        Barroso L A, Clidaras J, H9lzle U.The Datacenter as a Computer:An Introduction to the Design of Warehouse-scale Machines[M]San Rafael:Morgan&amp;amp;Claypool Publishers, 2013
                                    </a>
                                </li>
                                <li id="443">


                                    <a id="bibliography_9" title="Pelley S, Meisner D, Wenisch T F, et al.Understanding and abstracting total data center power[C/OL]Proc of WEED.2009[2018-05-20].http:seelab.ucsd.edu/mobile/related_papers/22_weed09.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding and abstracting total data center power[C/OL]">
                                        <b>[9]</b>
                                        Pelley S, Meisner D, Wenisch T F, et al.Understanding and abstracting total data center power[C/OL]Proc of WEED.2009[2018-05-20].http:seelab.ucsd.edu/mobile/related_papers/22_weed09.pdf
                                    </a>
                                </li>
                                <li id="445">


                                    <a id="bibliography_10" title="Belady C, Rawson A, Pfleuger J, et al.Green grid data center power efficiency metrics:PUE and DCIE, Rev 2007-0[R].Oregon:The Green Grid, 2007" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Green grid data center power efficiency metrics:PUE and DCIE,Rev 2007-0">
                                        <b>[10]</b>
                                        Belady C, Rawson A, Pfleuger J, et al.Green grid data center power efficiency metrics:PUE and DCIE, Rev 2007-0[R].Oregon:The Green Grid, 2007
                                    </a>
                                </li>
                                <li id="447">


                                    <a id="bibliography_11" title="Mathew P, Ganguly S, Greenberg S, et al.Selfbenchmarking guide for data centers:Metrics, benchmarks, actions[OL].Berkeley:The Lawrence Berkeley National Laboratory, (2010-07-13) [2018-05-20].https:eetd.lbl.gov/sites/all/files/publications/lbnl-3393e.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Selfbenchmarking guide for data centers:Metrics,benchmarks,actions[OL]">
                                        <b>[11]</b>
                                        Mathew P, Ganguly S, Greenberg S, et al.Selfbenchmarking guide for data centers:Metrics, benchmarks, actions[OL].Berkeley:The Lawrence Berkeley National Laboratory, (2010-07-13) [2018-05-20].https:eetd.lbl.gov/sites/all/files/publications/lbnl-3393e.pdf
                                    </a>
                                </li>
                                <li id="449">


                                    <a id="bibliography_12" title="Roy S, Rudra A, Verma A.An energy complexity model for algorithms[C]Proc of the 4th Conf on Innovations in Theoretical Computer Science.New York:ACM, 2013:283-304" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An energy complexity model for algorithms">
                                        <b>[12]</b>
                                        Roy S, Rudra A, Verma A.An energy complexity model for algorithms[C]Proc of the 4th Conf on Innovations in Theoretical Computer Science.New York:ACM, 2013:283-304
                                    </a>
                                </li>
                                <li id="451">


                                    <a id="bibliography_13" title="Tudor B M, Teo Y M.On understanding the energy consumption of arm-based multicore servers[C]Proc of ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2013:267-278" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000104672&amp;v=MzE2MDdtVUxySUkxc1ViaEE9TmlmSVk3SzdIdGpOcjQ5Rlplc0xDbnM3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Tudor B M, Teo Y M.On understanding the energy consumption of arm-based multicore servers[C]Proc of ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2013:267-278
                                    </a>
                                </li>
                                <li id="453">


                                    <a id="bibliography_14" title="Ge Rong, Feng Xizhou, Cameron K W.Modeling and evaluating energy-performance efficiency of parallel processing on multicore based power aware systems[C]Proc of IEEE Int Parallel&amp;amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2009:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling and evaluating energy-performance efficiency of parallel processing on multicore based power aware systems">
                                        <b>[14]</b>
                                        Ge Rong, Feng Xizhou, Cameron K W.Modeling and evaluating energy-performance efficiency of parallel processing on multicore based power aware systems[C]Proc of IEEE Int Parallel&amp;amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2009:1-8
                                    </a>
                                </li>
                                <li id="455">


                                    <a id="bibliography_15" title="Song S L, Barker K, Kerbyson D.Unified performance and power modeling of scientific workloads[C]Proc of the 1st Int Workshop on Energy Efficient Supercomputing.New York:ACM, 2013:No.4" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unified performance and power modeling of scientific workloads">
                                        <b>[15]</b>
                                        Song S L, Barker K, Kerbyson D.Unified performance and power modeling of scientific workloads[C]Proc of the 1st Int Workshop on Energy Efficient Supercomputing.New York:ACM, 2013:No.4
                                    </a>
                                </li>
                                <li id="457">


                                    <a id="bibliography_16" title="Lewis A, Simon J, Tzeng N F.Chaotic attractor prediction for server run-time energy consumption[C]Proc of the Int Conf on Power Aware Computing and Systems.Berkeley, CA:USENIX Association, 2010:1-16" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chaotic attractor prediction for server run-time energy consumption">
                                        <b>[16]</b>
                                        Lewis A, Simon J, Tzeng N F.Chaotic attractor prediction for server run-time energy consumption[C]Proc of the Int Conf on Power Aware Computing and Systems.Berkeley, CA:USENIX Association, 2010:1-16
                                    </a>
                                </li>
                                <li id="459">


                                    <a id="bibliography_17" title="Lewis A W, Tzeng N F, Ghosh S.Runtime energy consumption estimation for server workloads based on chaotic time-series approximation[J]ACM Transactions on Architecture and Code Optimization, 2012, 9 (3) :15:1-15:26" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007305&amp;v=MDMxMTdMcklJMXNVYmhBPU5pZklZN0s3SHRqTnI0OUZaT3NJRDN3OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Lewis A W, Tzeng N F, Ghosh S.Runtime energy consumption estimation for server workloads based on chaotic time-series approximation[J]ACM Transactions on Architecture and Code Optimization, 2012, 9 (3) :15:1-15:26
                                    </a>
                                </li>
                                <li id="461">


                                    <a id="bibliography_18" title="Perumal V, Subbiah S.Power-conservative server consolidation based resource management in cloud[J].International Journal of Network Management, 2014, 24 (6) :415-432" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Power-conservative Server Consolidation Based Resource Management in Cloud">
                                        <b>[18]</b>
                                        Perumal V, Subbiah S.Power-conservative server consolidation based resource management in cloud[J].International Journal of Network Management, 2014, 24 (6) :415-432
                                    </a>
                                </li>
                                <li id="463">


                                    <a id="bibliography_19" title="Chatzipapas A, Pediaditakis D, Rotsos C, et al.Challenge:Resolving data center power bill disputes:The energyperformance trade-offs of consolidation[C]Proc of the 6th ACM Int Conf on Future Energy Systems.New York:ACM, 2015:89-94" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Challenge:Resolving data center power bill disputes:The energyperformance trade-offs of consolidation">
                                        <b>[19]</b>
                                        Chatzipapas A, Pediaditakis D, Rotsos C, et al.Challenge:Resolving data center power bill disputes:The energyperformance trade-offs of consolidation[C]Proc of the 6th ACM Int Conf on Future Energy Systems.New York:ACM, 2015:89-94
                                    </a>
                                </li>
                                <li id="465">


                                    <a id="bibliography_20" title="Kansal A, Zhao Feng, Liu Jie, et al.Virtual machine power metering and provisioning[C]Proc of the 1st ACM Symp on Cloud Computing.New York:ACM, 2010:39-50" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Virtual machine power metering and provisioning">
                                        <b>[20]</b>
                                        Kansal A, Zhao Feng, Liu Jie, et al.Virtual machine power metering and provisioning[C]Proc of the 1st ACM Symp on Cloud Computing.New York:ACM, 2010:39-50
                                    </a>
                                </li>
                                <li id="467">


                                    <a id="bibliography_21" title="Chen Yiyu, Amitayu D, Qin Wubi, et al.Managing server energy and operational costs in hosting centers[C]Proc of the 26th ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2005:303-314" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000057009&amp;v=MDM5OTNlWnVIeWptVUxySUkxc1ViaEE9TmlmSVk3SzdIdGpOcjQ5RlpPNElESHd3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Chen Yiyu, Amitayu D, Qin Wubi, et al.Managing server energy and operational costs in hosting centers[C]Proc of the 26th ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2005:303-314
                                    </a>
                                </li>
                                <li id="469">


                                    <a id="bibliography_22" title="Elnozahy E M, Kistler M, Rajamony R.Energy-efficient server clusters[C]Proc of the 2nd Int Workshop on PowerAware Computer Systems.Berlin:Springer, 2002:179-197" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-efficient server clusters">
                                        <b>[22]</b>
                                        Elnozahy E M, Kistler M, Rajamony R.Energy-efficient server clusters[C]Proc of the 2nd Int Workshop on PowerAware Computer Systems.Berlin:Springer, 2002:179-197
                                    </a>
                                </li>
                                <li id="471">


                                    <a id="bibliography_23" title="Ge Rong, Feng Xizhou, Kirk W Cameron.Performanceconstrained distributed DVS scheduling for scientific applications on power-aware clusters[C]Proc of the 18th ACM/IEEE Conf on Supercomputing.Piscataway, NJ:IEEE, 2005:34-45" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance Constrained DistributedDVS Scheduling for Scientific Applications on Poweraware Clusters">
                                        <b>[23]</b>
                                        Ge Rong, Feng Xizhou, Kirk W Cameron.Performanceconstrained distributed DVS scheduling for scientific applications on power-aware clusters[C]Proc of the 18th ACM/IEEE Conf on Supercomputing.Piscataway, NJ:IEEE, 2005:34-45
                                    </a>
                                </li>
                                <li id="473">


                                    <a id="bibliography_24" title="Yeo S, Lee H.Peeling the Power Onion of Data Centers[M].Berlin:Springer, 2012:137-168" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Peeling the Power Onion of Data Centers">
                                        <b>[24]</b>
                                        Yeo S, Lee H.Peeling the Power Onion of Data Centers[M].Berlin:Springer, 2012:137-168
                                    </a>
                                </li>
                                <li id="475">


                                    <a id="bibliography_25" title="Yao F, Demers A, Shenker S.A scheduling model for reduced CPU energy[C]Proc of the 36th Annual Symp on Foundations of Computer Science.Piscataway, NJ:IEEE, 1995:374-382" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Scheduling Model for Reduced CPU Energy">
                                        <b>[25]</b>
                                        Yao F, Demers A, Shenker S.A scheduling model for reduced CPU energy[C]Proc of the 36th Annual Symp on Foundations of Computer Science.Piscataway, NJ:IEEE, 1995:374-382
                                    </a>
                                </li>
                                <li id="477">


                                    <a id="bibliography_26" title="Albers S.Energy-efficient algorithms[J].Communications of the ACM, 2010, 53 (5) :86-96" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000031340&amp;v=MTkzNThud1plWnVIeWptVUxySUkxc1ViaEE9TmlmSVk3SzdIdGpOcjQ5RlpPZ09EM2c1b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        Albers S.Energy-efficient algorithms[J].Communications of the ACM, 2010, 53 (5) :86-96
                                    </a>
                                </li>
                                <li id="479">


                                    <a id="bibliography_27" title="Economou D, Rivoire S, Kozyrakis C, et al.Full-system power analysis and modeling for server environments[C]Proc of the 27th Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2006:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Full-system power analysis and modeling for server environments">
                                        <b>[27]</b>
                                        Economou D, Rivoire S, Kozyrakis C, et al.Full-system power analysis and modeling for server environments[C]Proc of the 27th Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2006:1-8
                                    </a>
                                </li>
                                <li id="481">


                                    <a id="bibliography_28" title="Beloglazov A, Abawajy J, Buyya R.Energy-aware resource allocation heuristics for efficient management of data centers for cloud computing[J]Future Generation Computer Systems, 2012, 28 (5) :755-768" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300442579&amp;v=MjA0NTJIeWptVUxySUkxc1ViaEE9TmlmT2ZiSzdIdERPckk5RllPOE5DWHN3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                        Beloglazov A, Abawajy J, Buyya R.Energy-aware resource allocation heuristics for efficient management of data centers for cloud computing[J]Future Generation Computer Systems, 2012, 28 (5) :755-768
                                    </a>
                                </li>
                                <li id="483">


                                    <a id="bibliography_29" title="Lewis A W, Ghosh S, Tzeng N F.Run-time energy consumption estimation based on workload in server systems[C]Proc of HotPower.Berkeley, CA:USENIX Association, 2008:17-21" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Run-time energy consumption estimation based on workload in server systems&amp;quot;">
                                        <b>[29]</b>
                                        Lewis A W, Ghosh S, Tzeng N F.Run-time energy consumption estimation based on workload in server systems[C]Proc of HotPower.Berkeley, CA:USENIX Association, 2008:17-21
                                    </a>
                                </li>
                                <li id="485">


                                    <a id="bibliography_30" title="Kliazovich D, Bouvry P, Khan S U.GreenCloud:A packetlevel simulator of energy-aware cloud computing data centers[J].The Journal of Supercomputing, 2012, 62 (3) :1263-1283" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD121106000609&amp;v=MTQ0NjdCYXJLNkg5RE1xWTlGWk8wUEJSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3ek5JRm9XTmo3&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                        Kliazovich D, Bouvry P, Khan S U.GreenCloud:A packetlevel simulator of energy-aware cloud computing data centers[J].The Journal of Supercomputing, 2012, 62 (3) :1263-1283
                                    </a>
                                </li>
                                <li id="487">


                                    <a id="bibliography_31" title="Gao Yongqiang, Guan Haibing, Qi Zhengwei, et al.Quality of service aware power management for virtualized data centers[J].Journal of Systems Architecture, 2013, 59 (4) :245-259" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080100029604&amp;v=MDA3NjhDbnc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSTFzVWJoQT1OaWZPZmJLN0h0bk1ybzlGWk9rRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                        Gao Yongqiang, Guan Haibing, Qi Zhengwei, et al.Quality of service aware power management for virtualized data centers[J].Journal of Systems Architecture, 2013, 59 (4) :245-259
                                    </a>
                                </li>
                                <li id="489">


                                    <a id="bibliography_32" title="Fan Xiaobo, Weber W, Barroso L.Power provisioning for a warehouse-sized computer[J].ACM SIGARCH Computer Architecture News, 2007, 35 (2) :13-23" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000044731&amp;v=MTAzNDJLN0h0ak5yNDlGWk84TEMzODRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYmhBPU5pZklZNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                        Fan Xiaobo, Weber W, Barroso L.Power provisioning for a warehouse-sized computer[J].ACM SIGARCH Computer Architecture News, 2007, 35 (2) :13-23
                                    </a>
                                </li>
                                <li id="491">


                                    <a id="bibliography_33" title="Dayarathna M, Wen Yonggang, Fan Rui.Data center energy consumption modeling:A survey[J].IEEE Communications Surveys&amp;amp;Tutorials, 2016, 18 (1) :732-794" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data Center Energy Consumption Modeling:A Survey">
                                        <b>[33]</b>
                                        Dayarathna M, Wen Yonggang, Fan Rui.Data center energy consumption modeling:A survey[J].IEEE Communications Surveys&amp;amp;Tutorials, 2016, 18 (1) :732-794
                                    </a>
                                </li>
                                <li id="493">


                                    <a id="bibliography_34" title="Heller B, Seetharaman S, Mahadevan P, et al.ElasticTree:Saving energy in data center networks[C]Proc of Symp on Networked Systems Design and Implementation.Berkeley, CA:USENIX Association, 2010:249-264" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ElasticTree:Saving energy in data center networks">
                                        <b>[34]</b>
                                        Heller B, Seetharaman S, Mahadevan P, et al.ElasticTree:Saving energy in data center networks[C]Proc of Symp on Networked Systems Design and Implementation.Berkeley, CA:USENIX Association, 2010:249-264
                                    </a>
                                </li>
                                <li id="495">


                                    <a id="bibliography_35" title="Widjaja I, Walid A, Luo Yanbin, et al.Small versus large:Switch sizing in topology design of energy-efficient data centers[C]Proc of the 21st IEEE/ACM Int Symp on Quality of Service (IWQoS) .Piscataway, NJ:IEEE, 2013:51-56" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Small versus large:Switch sizing in topology design of energy-efficient data centers">
                                        <b>[35]</b>
                                        Widjaja I, Walid A, Luo Yanbin, et al.Small versus large:Switch sizing in topology design of energy-efficient data centers[C]Proc of the 21st IEEE/ACM Int Symp on Quality of Service (IWQoS) .Piscataway, NJ:IEEE, 2013:51-56
                                    </a>
                                </li>
                                <li id="497">


                                    <a id="bibliography_36" title="Zhang Yan, Ansari N.Hero:Hierarchical energy optimization for data center networks[C]Proc of IEEE Int Conf on Communications.Piscataway, NJ:IEEE, 2012:2924-2928" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hero:Hierarchical energy optimization for data center networks">
                                        <b>[36]</b>
                                        Zhang Yan, Ansari N.Hero:Hierarchical energy optimization for data center networks[C]Proc of IEEE Int Conf on Communications.Piscataway, NJ:IEEE, 2012:2924-2928
                                    </a>
                                </li>
                                <li id="499">


                                    <a id="bibliography_37" title="Jin Hao, Cheocherngngarn T, Levy D, et al.Joint hostnetwork optimization for energy-efficient data center networking[C]Proc of the 27th IEEE Int Symp on Parallel&amp;amp;Distributed Processing.Piscataway, NJ:IEEE, 2013:623-634" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint hostnetwork optimization for energy-efficient data center networking">
                                        <b>[37]</b>
                                        Jin Hao, Cheocherngngarn T, Levy D, et al.Joint hostnetwork optimization for energy-efficient data center networking[C]Proc of the 27th IEEE Int Symp on Parallel&amp;amp;Distributed Processing.Piscataway, NJ:IEEE, 2013:623-634
                                    </a>
                                </li>
                                <li id="501">


                                    <a id="bibliography_38" title="Li Dan, Shang Yunfei, Chen Congjie.Software defined green data center network with exclusive routing[C]Proc of IEEE Conf on Computer Communications.Piscataway, NJ:IEEE, 2014:1743-1751" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Software Defined Green Data Center Network with Exclusive Routing">
                                        <b>[38]</b>
                                        Li Dan, Shang Yunfei, Chen Congjie.Software defined green data center network with exclusive routing[C]Proc of IEEE Conf on Computer Communications.Piscataway, NJ:IEEE, 2014:1743-1751
                                    </a>
                                </li>
                                <li id="503">


                                    <a id="bibliography_39" title="Li Minming, Andrew C, et al.Discrete and continuous minenergy schedules for variable voltage processors[J].National Academy of Sciences, 2006, 103 (11) :3983-3987" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discrete and continuous min-energy schedules for variable voltage processors">
                                        <b>[39]</b>
                                        Li Minming, Andrew C, et al.Discrete and continuous minenergy schedules for variable voltage processors[J].National Academy of Sciences, 2006, 103 (11) :3983-3987
                                    </a>
                                </li>
                                <li id="505">


                                    <a id="bibliography_40" title="Pruhs K, Van Stee R, Uthaisombut P.Speed scaling of tasks with precedence constraints[J].Theory of Computing Systems, 2008, 43 (1) :67-80" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000255596&amp;v=MTMyNjNBWWVJSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFVMcklKRnc9Tmo3QmFyTzRIdEhNcllw&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[40]</b>
                                        Pruhs K, Van Stee R, Uthaisombut P.Speed scaling of tasks with precedence constraints[J].Theory of Computing Systems, 2008, 43 (1) :67-80
                                    </a>
                                </li>
                                <li id="507">


                                    <a id="bibliography_41" title="Bunde D P.Power-aware scheduling for makespan and flow[C]Proc of the 8th Annual ACM Symp on Parallelism in Algorithms and Architectures.New York:ACM, 2006:190-196" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Power-aware scheduling for makespan and flow">
                                        <b>[41]</b>
                                        Bunde D P.Power-aware scheduling for makespan and flow[C]Proc of the 8th Annual ACM Symp on Parallelism in Algorithms and Architectures.New York:ACM, 2006:190-196
                                    </a>
                                </li>
                                <li id="509">


                                    <a id="bibliography_42" title="Angel E, Bampis E, Chau V.Throughput maximization in the speed-scaling setting[C]Proc of the 31st Int Symp on Theoretical Aspects of Computer Science.Berlin:Springer, 2014:53-62" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Throughput maximization in the speed-scaling setting">
                                        <b>[42]</b>
                                        Angel E, Bampis E, Chau V.Throughput maximization in the speed-scaling setting[C]Proc of the 31st Int Symp on Theoretical Aspects of Computer Science.Berlin:Springer, 2014:53-62
                                    </a>
                                </li>
                                <li id="511">


                                    <a id="bibliography_43" title="Bansal N, Kimbrel T, Pruhs K.Speed scaling to manage energy and temperature[J].Journal of the ACM, 2007, 54 (1) :3:1-3:39" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000020238&amp;v=MTY0Mzl1SHlqbVVMcklJMXNVYmhBPU5pZklZN0s3SHRqTnI0OUZaT2tQRG44eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[43]</b>
                                        Bansal N, Kimbrel T, Pruhs K.Speed scaling to manage energy and temperature[J].Journal of the ACM, 2007, 54 (1) :3:1-3:39
                                    </a>
                                </li>
                                <li id="513">


                                    <a id="bibliography_44" title="Albers S, M&#252;ller F, Schmelzer S.Speed scaling on parallel processors[J].Algorithmica, 2014, 68 (2) :404-425" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300061233&amp;v=MjczMzRBPU5qN0Jhcks4SHRMTXJJOUZaTzBPRG44Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc1ViaA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[44]</b>
                                        Albers S, M&#252;ller F, Schmelzer S.Speed scaling on parallel processors[J].Algorithmica, 2014, 68 (2) :404-425
                                    </a>
                                </li>
                                <li id="515">


                                    <a id="bibliography_45" title="Greiner G, Nonner T, Souza A.The bell is ringing in speedscaled multiprocessor scheduling[C]Proc of the 21st Annual Symp on Parallelism in Algorithms and Architectures.New York:ACM, 2009:11-18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The bell is ringing in speedscaled multiprocessor scheduling">
                                        <b>[45]</b>
                                        Greiner G, Nonner T, Souza A.The bell is ringing in speedscaled multiprocessor scheduling[C]Proc of the 21st Annual Symp on Parallelism in Algorithms and Architectures.New York:ACM, 2009:11-18
                                    </a>
                                </li>
                                <li id="517">


                                    <a id="bibliography_46" title="Wong D.Peak efficiency aware scheduling for highly energy proportional servers[C]Proc of the 43rd ACM/IEEEAnnual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:481-492" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Peak efficiency aware scheduling for highly energy proportional servers">
                                        <b>[46]</b>
                                        Wong D.Peak efficiency aware scheduling for highly energy proportional servers[C]Proc of the 43rd ACM/IEEEAnnual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:481-492
                                    </a>
                                </li>
                                <li id="519">


                                    <a id="bibliography_47" title="Jiang Congfeng, Wang Yumei, Ou Dongyang, et al.Energy proportional servers:Where are we in 2016/[C]Proc of the 37th IEEE Int Conf on Distributed Computing Systems.Piscataway, NJ:IEEE, 2017:1649-1660" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy proportional servers:Where are we in 2016/">
                                        <b>[47]</b>
                                        Jiang Congfeng, Wang Yumei, Ou Dongyang, et al.Energy proportional servers:Where are we in 2016/[C]Proc of the 37th IEEE Int Conf on Distributed Computing Systems.Piscataway, NJ:IEEE, 2017:1649-1660
                                    </a>
                                </li>
                                <li id="521">


                                    <a id="bibliography_48" title="Antoniadis A, Huang C.Non-preemptive speed scaling[J].Journal of Scheduling, 2013, 16 (4) :385-394" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13070800006013&amp;v=MTA0MjJzSkRIMDZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYmhBPU5qN0Jhcks3SHRiTXA0OUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[48]</b>
                                        Antoniadis A, Huang C.Non-preemptive speed scaling[J].Journal of Scheduling, 2013, 16 (4) :385-394
                                    </a>
                                </li>
                                <li id="523">


                                    <a id="bibliography_49" title="Bampis E, Kononov A, Letsios D, et al.From preemptive to non-preemptive speed-scaling scheduling[C]Proc of the19th Int Computing and Combinatorics Conf.Berlin:Springer, 2013:134-146" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From preemptive to non-preemptive speed-scaling scheduling">
                                        <b>[49]</b>
                                        Bampis E, Kononov A, Letsios D, et al.From preemptive to non-preemptive speed-scaling scheduling[C]Proc of the19th Int Computing and Combinatorics Conf.Berlin:Springer, 2013:134-146
                                    </a>
                                </li>
                                <li id="525">


                                    <a id="bibliography_50" title="Leung J, Chung L.Scheduling with processing set restrictions:A survey[J].International Journal of Production Economics, 2008, 116 (2) :251-262" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501881825&amp;v=Mjg0ODBVTHJJSTFzVWJoQT1OaWZPZmJLN0h0RE5xbzlFYk9NT0JINDhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[50]</b>
                                        Leung J, Chung L.Scheduling with processing set restrictions:A survey[J].International Journal of Production Economics, 2008, 116 (2) :251-262
                                    </a>
                                </li>
                                <li id="527">


                                    <a id="bibliography_51" title="Qiang Wang, Chu Xiaowen.GPGPU power estimation with core and memory frequency scaling[J].ACM SIGMETRICSPerformance Evaluation Review, 2017, 45 (2) :73-78" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMC8FD3F9EFE1A67796C5C54BAEBCD0718&amp;v=MTIwMzZwYlEzNU4xaHg3Mi94S289TmlmSVk4Q3dhS1hQMllZd0VwNE9mWG8reUI4Vm1Ub09UWHVRM1dkSENzYVVRcnVYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[51]</b>
                                        Qiang Wang, Chu Xiaowen.GPGPU power estimation with core and memory frequency scaling[J].ACM SIGMETRICSPerformance Evaluation Review, 2017, 45 (2) :73-78
                                    </a>
                                </li>
                                <li id="529">


                                    <a id="bibliography_52" title="Mei Xinxin, Chu Xiaowen, Liu Hai, et al.Energy efficient real-time task scheduling on CPU-GPU hybrid clusters[C]Proc of IEEE Conf on Computer Communications.Piscataway, NJ:IEEE, 2017:1-9" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy efficient real-time task scheduling on CPU-GPU hybrid clusters">
                                        <b>[52]</b>
                                        Mei Xinxin, Chu Xiaowen, Liu Hai, et al.Energy efficient real-time task scheduling on CPU-GPU hybrid clusters[C]Proc of IEEE Conf on Computer Communications.Piscataway, NJ:IEEE, 2017:1-9
                                    </a>
                                </li>
                                <li id="531">


                                    <a id="bibliography_53" title="Chau Vincent, Chu Xiaowen, Liu Hai, et al.Energy efficient job scheduling with DVFS for CPU-GPU heterogeneous systems[C]Proc of the 8th Int Conf on Future Energy Systems.New York:ACM, 2017:1-11" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy efficient job scheduling with DVFS for CPU-GPU heterogeneous systems">
                                        <b>[53]</b>
                                        Chau Vincent, Chu Xiaowen, Liu Hai, et al.Energy efficient job scheduling with DVFS for CPU-GPU heterogeneous systems[C]Proc of the 8th Int Conf on Future Energy Systems.New York:ACM, 2017:1-11
                                    </a>
                                </li>
                                <li id="533">


                                    <a id="bibliography_54" title="Srikantaiah S, Kansal A, Zhao Feng.Energy aware consolidation for cloud computing[C]Proc of the Conf on Power Aware Computing and Systems.Berkeley, CA:USENIX Association, 2008:10-15" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy aware consolidation for cloud computing">
                                        <b>[54]</b>
                                        Srikantaiah S, Kansal A, Zhao Feng.Energy aware consolidation for cloud computing[C]Proc of the Conf on Power Aware Computing and Systems.Berkeley, CA:USENIX Association, 2008:10-15
                                    </a>
                                </li>
                                <li id="535">


                                    <a id="bibliography_55" title="Gupta A, Im S, Krishnaswamy R, et al.Scheduling heterogeneous processors isn&#39;t as easy as you think[C]Proc of the 23rd Annual ACM-SIAM Symp on Discrete Algorithms.Philadelphia:SIAM, 2012:1242-1253" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scheduling heterogeneous processors isn&amp;#39;&amp;#39;t as easy as you think">
                                        <b>[55]</b>
                                        Gupta A, Im S, Krishnaswamy R, et al.Scheduling heterogeneous processors isn&#39;t as easy as you think[C]Proc of the 23rd Annual ACM-SIAM Symp on Discrete Algorithms.Philadelphia:SIAM, 2012:1242-1253
                                    </a>
                                </li>
                                <li id="537">


                                    <a id="bibliography_56" title="Wang Weina, Zhu Kai, Ying Lei, et al.A throughput optimal algorithm for map task scheduling in mapreduce with data locality[J].ACM SIGMETRICS Performance Evaluation Review, 2013, 40 (4) :33-42" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000011389&amp;v=MTQ0MDVMcklJMXNVYmhBPU5pZklZN0s3SHRqTnI0OUZaT29PRDNRd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[56]</b>
                                        Wang Weina, Zhu Kai, Ying Lei, et al.A throughput optimal algorithm for map task scheduling in mapreduce with data locality[J].ACM SIGMETRICS Performance Evaluation Review, 2013, 40 (4) :33-42
                                    </a>
                                </li>
                                <li id="539">


                                    <a id="bibliography_57" title="Jin Xibo, Zhang Fa, Song Ying, et al.Energy-Efficient scheduling with time and processors eligibility restrictions[C]Proc of the 19th European Conf on Parallel Processing.Berlin:Springer, 2013:66-77" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-Efficient scheduling with time and processors eligibility restrictions">
                                        <b>[57]</b>
                                        Jin Xibo, Zhang Fa, Song Ying, et al.Energy-Efficient scheduling with time and processors eligibility restrictions[C]Proc of the 19th European Conf on Parallel Processing.Berlin:Springer, 2013:66-77
                                    </a>
                                </li>
                                <li id="541">


                                    <a id="bibliography_58" title="Li Dawei, Wu Jie.Minimizing energy consumption for framebased tasks on heterogeneous multiprocessor platforms[J].IEEE Transaction on Parallel and Distributed Systems, 2015, 26 (3) :810-823" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Minimizing energy consumption for frame-based tasks on heterogeneous multiprocessor platforms">
                                        <b>[58]</b>
                                        Li Dawei, Wu Jie.Minimizing energy consumption for framebased tasks on heterogeneous multiprocessor platforms[J].IEEE Transaction on Parallel and Distributed Systems, 2015, 26 (3) :810-823
                                    </a>
                                </li>
                                <li id="543">


                                    <a id="bibliography_59" title="Wang Qiang, Xu Pengfei, Zhang Yatao, et al.EPPMiner:An extended benchmark suite for energy, power and performance characterization of heterogeneous architecture[C]Proc of the 8th Int Conf on Future Energy Systems.New York:ACM, 2017:23-33" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EPPMiner:An extended benchmark suite for energy,power and performance characterization of heterogeneous architecture">
                                        <b>[59]</b>
                                        Wang Qiang, Xu Pengfei, Zhang Yatao, et al.EPPMiner:An extended benchmark suite for energy, power and performance characterization of heterogeneous architecture[C]Proc of the 8th Int Conf on Future Energy Systems.New York:ACM, 2017:23-33
                                    </a>
                                </li>
                                <li id="545">


                                    <a id="bibliography_60" title="Bobroff N, Kochut A, Beaty K.Dynamic placement of virtual machines for managing sla violations[C]Proc of the10th IFIP/IEEE Int Symp on Integrated Network Management.Piscataway, NJ:IEEE, 2007:119-128" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic placement of virtual machines for managing SLA violations">
                                        <b>[60]</b>
                                        Bobroff N, Kochut A, Beaty K.Dynamic placement of virtual machines for managing sla violations[C]Proc of the10th IFIP/IEEE Int Symp on Integrated Network Management.Piscataway, NJ:IEEE, 2007:119-128
                                    </a>
                                </li>
                                <li id="547">


                                    <a id="bibliography_61" title="Kusic D, Kephart J O, Hanson J E, et al.Power and performance management of virtualized computing environments via lookahead control[C]Proc of the 5th Int Conf on Autonomic Computing.Piscataway, NJ:IEEE, 2008:3-12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Power and Performance Management of Virtualized Computing Environments Via Lookahead Control">
                                        <b>[61]</b>
                                        Kusic D, Kephart J O, Hanson J E, et al.Power and performance management of virtualized computing environments via lookahead control[C]Proc of the 5th Int Conf on Autonomic Computing.Piscataway, NJ:IEEE, 2008:3-12
                                    </a>
                                </li>
                                <li id="549">


                                    <a id="bibliography_62" >
                                        <b>[62]</b>
                                    Van H, Tran F, Menaud J M.SLA-aware virtual resource management for cloud infrastructures[C]Proc of the 9th IEEE Int Conf on Computer and Information Technology.Piscataway, NJ:IEEE, 2009:357-362</a>
                                </li>
                                <li id="551">


                                    <a id="bibliography_63" title="Gmach D, Rolia J, Cherkasova L et al.Resource pool management:Reactive versus proactive or let’s be friends[J].Computer Networks, 2009, 53 (17) :2905-2922" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300561145&amp;v=MzE0MzNlME9EWGc4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSTFzVWJoQT1OaWZPZmJLN0h0RE5ySTlGWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[63]</b>
                                        Gmach D, Rolia J, Cherkasova L et al.Resource pool management:Reactive versus proactive or let’s be friends[J].Computer Networks, 2009, 53 (17) :2905-2922
                                    </a>
                                </li>
                                <li id="553">


                                    <a id="bibliography_64" title="Hermenier F, Lorca X, Menaud J M, et al.Entropy:Aconsolidation manager for clusters[C]Proc of the ACMSIGPLAN/SIGOPS Int Conf on Virtual Execution Environments.New York:ACM, 2009:41-50" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Entropy:a consolidation manager for clusters">
                                        <b>[64]</b>
                                        Hermenier F, Lorca X, Menaud J M, et al.Entropy:Aconsolidation manager for clusters[C]Proc of the ACMSIGPLAN/SIGOPS Int Conf on Virtual Execution Environments.New York:ACM, 2009:41-50
                                    </a>
                                </li>
                                <li id="555">


                                    <a id="bibliography_65" title="Beloglazov A, Buyya R.Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in cloud data centers[J].Concurrency and Computation:Practice and Experience, 2012, 24 (13) :1397-1420" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD15121501504362&amp;v=MTI0Mzcxc1ViaEE9TmlmY2FySzlIOVBOcW85RVllc0xEM283b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[65]</b>
                                        Beloglazov A, Buyya R.Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in cloud data centers[J].Concurrency and Computation:Practice and Experience, 2012, 24 (13) :1397-1420
                                    </a>
                                </li>
                                <li id="557">


                                    <a id="bibliography_66" title="Stolyar A L, Zhong Yuan.A large-scale service system with packing constraints:Minimizing the number of occupied servers[C]Proc of ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2013:41-52" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000104654&amp;v=MDM3NDBOaWZJWTdLN0h0ak5yNDlGWmVzTENuazlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYmhBPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[66]</b>
                                        Stolyar A L, Zhong Yuan.A large-scale service system with packing constraints:Minimizing the number of occupied servers[C]Proc of ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2013:41-52
                                    </a>
                                </li>
                                <li id="559">


                                    <a id="bibliography_67" title="Jin Xibo, Zhang Fa, Hu Songlin, et al.Risk management for virtual machines consolidation in data centers[C]Proc of IEEE Global Communications Conf.Piscataway, NJ:IEEE, 2013:2872-2878" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Risk management for virtual machines consolidation in data centers">
                                        <b>[67]</b>
                                        Jin Xibo, Zhang Fa, Hu Songlin, et al.Risk management for virtual machines consolidation in data centers[C]Proc of IEEE Global Communications Conf.Piscataway, NJ:IEEE, 2013:2872-2878
                                    </a>
                                </li>
                                <li id="561">


                                    <a id="bibliography_68" title="Van H, Tran F, Menaud J.Sla-aware virtual resource management for cloud infrastructures[C]Proc of the 9th IEEE Int Conf on Computer and Information Technology.Piscataway, NJ:IEEE, 2009:357-362" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLA Aware Virtual Resource Management for Cloud Infrastructures">
                                        <b>[68]</b>
                                        Van H, Tran F, Menaud J.Sla-aware virtual resource management for cloud infrastructures[C]Proc of the 9th IEEE Int Conf on Computer and Information Technology.Piscataway, NJ:IEEE, 2009:357-362
                                    </a>
                                </li>
                                <li id="563">


                                    <a id="bibliography_69" title="Levy M, Raviv D.A framework for data center site risk metric[C]Proc of Ubiquitous Computing, Electronics and Mobile Communication Conf.Piscataway, NJ:IEEE, 2017:9-15" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A framework for data center site risk metric">
                                        <b>[69]</b>
                                        Levy M, Raviv D.A framework for data center site risk metric[C]Proc of Ubiquitous Computing, Electronics and Mobile Communication Conf.Piscataway, NJ:IEEE, 2017:9-15
                                    </a>
                                </li>
                                <li id="565">


                                    <a id="bibliography_70" title="Chauhan N, Rakesh N, Matam R.Assessment on VMplacement and VM selection strategies[C]Proc of Nature Inspired Computing.Berlin:Springer.2018:157-163" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Assessment on VMplacement and VM selection strategies">
                                        <b>[70]</b>
                                        Chauhan N, Rakesh N, Matam R.Assessment on VMplacement and VM selection strategies[C]Proc of Nature Inspired Computing.Berlin:Springer.2018:157-163
                                    </a>
                                </li>
                                <li id="567">


                                    <a id="bibliography_71" title="Wang Youshi, Zhang Fa, Liu Zhiyong.Truthful strategy and resource integration for multi-tenant data center demand response[C]Proc of the 5th Int Workshop on Frontiers in Algorithmics.Berlin:Springer, 2015:259-270" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Truthful strategy and resource integration for multi-tenant data center demand response">
                                        <b>[71]</b>
                                        Wang Youshi, Zhang Fa, Liu Zhiyong.Truthful strategy and resource integration for multi-tenant data center demand response[C]Proc of the 5th Int Workshop on Frontiers in Algorithmics.Berlin:Springer, 2015:259-270
                                    </a>
                                </li>
                                <li id="569">


                                    <a id="bibliography_72" title="Dabbagh M, Hamdaoui B, Guizani M, et al.Online assignment and placement of cloud task requests with heterogeneous requirements[C]Proc of Global Communications Conf.Piscataway, NJ:IEEE, 2015:1-6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online assignment and placement of cloud task requests with heterogeneous requirements">
                                        <b>[72]</b>
                                        Dabbagh M, Hamdaoui B, Guizani M, et al.Online assignment and placement of cloud task requests with heterogeneous requirements[C]Proc of Global Communications Conf.Piscataway, NJ:IEEE, 2015:1-6
                                    </a>
                                </li>
                                <li id="571">


                                    <a id="bibliography_73" title="Jin Xibo, Zhang Fa, Wang Lin, et al.Joint optimization of operational cost and performance interference in cloud data centers[J].IEEE Transaction on Cloud Computing, 2017, 5 (4) :697-711" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Optimization of Operational Cost and Performance Interference in Cloud Data Centers">
                                        <b>[73]</b>
                                        Jin Xibo, Zhang Fa, Wang Lin, et al.Joint optimization of operational cost and performance interference in cloud data centers[J].IEEE Transaction on Cloud Computing, 2017, 5 (4) :697-711
                                    </a>
                                </li>
                                <li id="573">


                                    <a id="bibliography_74" title="Chiang R, Huang H.TRACON:Interference-aware scheduling for data-intensive applications in virtualized environments[C]Proc of Int Conf for High Performance Computing, Networking, Storage and Analysis.New York:ACM, 2011:47:1-47:12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TRACON:Interference-aware scheduling for data-intensive applications in virtualized environments">
                                        <b>[74]</b>
                                        Chiang R, Huang H.TRACON:Interference-aware scheduling for data-intensive applications in virtualized environments[C]Proc of Int Conf for High Performance Computing, Networking, Storage and Analysis.New York:ACM, 2011:47:1-47:12
                                    </a>
                                </li>
                                <li id="575">


                                    <a id="bibliography_75" title="Moreno I S, Yang Renyu, Xu Jie, et al.Improved energyefficiency in cloud datacenters with interference-aware virtual machine placement[C]Proc of the 7th IEEE Int Symp on Autonomous Decentralized Systems.Piscataway, NJ:IEEE, 2013:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved energyefficiency in cloud datacenters with interference-aware virtual machine placement">
                                        <b>[75]</b>
                                        Moreno I S, Yang Renyu, Xu Jie, et al.Improved energyefficiency in cloud datacenters with interference-aware virtual machine placement[C]Proc of the 7th IEEE Int Symp on Autonomous Decentralized Systems.Piscataway, NJ:IEEE, 2013:1-8
                                    </a>
                                </li>
                                <li id="577">


                                    <a id="bibliography_76" title="Sharifi L, Freitag F, Veiga L.Virtual machine power modelling in multi-tenant ecosystems:Challenges and pitfalls[N].ICT-Energy Letters, 2015-07-15" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Virtual machine power modelling in multi-tenant ecosystems:Challenges and pitfalls">
                                        <b>[76]</b>
                                        Sharifi L, Freitag F, Veiga L.Virtual machine power modelling in multi-tenant ecosystems:Challenges and pitfalls[N].ICT-Energy Letters, 2015-07-15
                                    </a>
                                </li>
                                <li id="579">


                                    <a id="bibliography_77" title="Pascual F, Rzadca K.Optimizing egalitarian performance in the side-effects model of colocation for data center resource management[C]Proc of the 23rd European Conf on Parallel Processing.Berlin:Springer, 2017:206-219" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing egalitarian performance in the side-effects model of colocation for data center resource management">
                                        <b>[77]</b>
                                        Pascual F, Rzadca K.Optimizing egalitarian performance in the side-effects model of colocation for data center resource management[C]Proc of the 23rd European Conf on Parallel Processing.Berlin:Springer, 2017:206-219
                                    </a>
                                </li>
                                <li id="581">


                                    <a id="bibliography_78" title="Wang Lin, Zhang Fa, Zheng Kai, et al.Energy-efficient flow scheduling and routing with hard deadlines in data center networks[C]Proc of the 34th IEEE Int Conf on Distributed Computing Systems.Piscataway, NJ:IEEE, 2014:248-257" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-efficient flow scheduling and routing with hard deadlines in data center networks">
                                        <b>[78]</b>
                                        Wang Lin, Zhang Fa, Zheng Kai, et al.Energy-efficient flow scheduling and routing with hard deadlines in data center networks[C]Proc of the 34th IEEE Int Conf on Distributed Computing Systems.Piscataway, NJ:IEEE, 2014:248-257
                                    </a>
                                </li>
                                <li id="583">


                                    <a id="bibliography_79" title="Zhou Biyu, Wu Jie, Wang Lin, et al.Online flow scheduling with deadline for energy conservation in data center networks[C]Proc of the 23rd IEEE Int Conf on Parallel and Distributed Systems.Piscataway, NJ:IEEE, 2017:578-585" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online flow scheduling with deadline for energy conservation in data center networks">
                                        <b>[79]</b>
                                        Zhou Biyu, Wu Jie, Wang Lin, et al.Online flow scheduling with deadline for energy conservation in data center networks[C]Proc of the 23rd IEEE Int Conf on Parallel and Distributed Systems.Piscataway, NJ:IEEE, 2017:578-585
                                    </a>
                                </li>
                                <li id="585">


                                    <a id="bibliography_80" title="Xu Guan, Dai Bin, Huang Benxiong, et al.Bandwidth-aware energy efficient flow scheduling with SDN in data center networks[J].Future Generation Computer Systems, 2017, 68 (c) :163-174" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bandwidth-aware energy efficient flow scheduling with SDN in data center networks">
                                        <b>[80]</b>
                                        Xu Guan, Dai Bin, Huang Benxiong, et al.Bandwidth-aware energy efficient flow scheduling with SDN in data center networks[J].Future Generation Computer Systems, 2017, 68 (c) :163-174
                                    </a>
                                </li>
                                <li id="587">


                                    <a id="bibliography_81" title="Khosravi A, Buyya R.Energy and carbon footprint-aware management of geo-distributed cloud data centers:Ataxonomy, state of the art, and future directions[M]Advancing Cloud Database Systems and Capacity Planning with Dynamic Applications.Hershey, PA:IGI, 2016:1456-1475" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy and carbon footprint-aware management of geo-distributed cloud data centers:Ataxonomy,state of the art,and future directions">
                                        <b>[81]</b>
                                        Khosravi A, Buyya R.Energy and carbon footprint-aware management of geo-distributed cloud data centers:Ataxonomy, state of the art, and future directions[M]Advancing Cloud Database Systems and Capacity Planning with Dynamic Applications.Hershey, PA:IGI, 2016:1456-1475
                                    </a>
                                </li>
                                <li id="589">


                                    <a id="bibliography_82" title="Gunaratne C, Christensen K, Nordman B.Managing energy consumption costs in desktop PCs and LAN switches with proxying, split TCP connections, and scaling of link speed[J].International Journal of Network Management, 2005, 15 (5) :297-310" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001081318&amp;v=MjA0NjlDM2xVTHJJSkZ3PU5pZmNhck80SHRITnI0ZEVaK29IWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRG&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[82]</b>
                                        Gunaratne C, Christensen K, Nordman B.Managing energy consumption costs in desktop PCs and LAN switches with proxying, split TCP connections, and scaling of link speed[J].International Journal of Network Management, 2005, 15 (5) :297-310
                                    </a>
                                </li>
                                <li id="591">


                                    <a id="bibliography_83" title="Bruce N.IEEE 802.3Energy efficient ethernet study group[OL]. (2007-09-21) [2019-03-02]http:www.ieee 802.org/3/eee_study/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy efficient ethernet study group[OL]">
                                        <b>[83]</b>
                                        Bruce N.IEEE 802.3Energy efficient ethernet study group[OL]. (2007-09-21) [2019-03-02]http:www.ieee 802.org/3/eee_study/
                                    </a>
                                </li>
                                <li id="593">


                                    <a id="bibliography_84" title="Wang Xiaodong, Yao Yanjun, Wang Xiaorui, et al.Carpo:Correlation-aware power optimization in data center networks[C]Proc of the 31st IEEE Int Conf on Computer Communications.Piscataway, NJ:IEEE, 2012:1125-1133" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Carpo:Correlation-aware power optimization in data center networks">
                                        <b>[84]</b>
                                        Wang Xiaodong, Yao Yanjun, Wang Xiaorui, et al.Carpo:Correlation-aware power optimization in data center networks[C]Proc of the 31st IEEE Int Conf on Computer Communications.Piscataway, NJ:IEEE, 2012:1125-1133
                                    </a>
                                </li>
                                <li id="595">


                                    <a id="bibliography_85" title="Shang Yunfei, Li Dan, Xu Mingwei.Energy-aware routing in data center network[C]Proc of the 1st ACM SIGCOMMWorkshop on Green Networking.New York:ACM, 2010:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-aware routing in data center network">
                                        <b>[85]</b>
                                        Shang Yunfei, Li Dan, Xu Mingwei.Energy-aware routing in data center network[C]Proc of the 1st ACM SIGCOMMWorkshop on Green Networking.New York:ACM, 2010:1-8
                                    </a>
                                </li>
                                <li id="597">


                                    <a id="bibliography_86" title="Si Weisheng, Taheri J, Zomaya A.A distributed energy saving approach for Ethernet switches in data centers[C]Proc of the 37th IEEE Conf on Local Computer Networks.Piscataway, NJ:IEEE, 2012:505-512" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A distributed energy saving approach for Ethernet switches in data centers">
                                        <b>[86]</b>
                                        Si Weisheng, Taheri J, Zomaya A.A distributed energy saving approach for Ethernet switches in data centers[C]Proc of the 37th IEEE Conf on Local Computer Networks.Piscataway, NJ:IEEE, 2012:505-512
                                    </a>
                                </li>
                                <li id="599">


                                    <a id="bibliography_87" title="Wang Lin, Fern A, Zhang Fa, et al.Multi-resource energyefficient routing in cloud data centers with network-as-aservice[C]Proc of the 20th IEEE Symp on Computers and Communication.Piscataway, NJ:IEEE, 2015:694-699" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-resource energyefficient routing in cloud data centers with network-as-aservice">
                                        <b>[87]</b>
                                        Wang Lin, Fern A, Zhang Fa, et al.Multi-resource energyefficient routing in cloud data centers with network-as-aservice[C]Proc of the 20th IEEE Symp on Computers and Communication.Piscataway, NJ:IEEE, 2015:694-699
                                    </a>
                                </li>
                                <li id="601">


                                    <a id="bibliography_88" title="Wang Lin, Zhang Fa, Aroca J, et al.GreenDCN:A general framework for achieving energy efficiency in data center networks[J].IEEE Journal on Selected Areas in Communications, 2014, 32 (1) :4-15" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GreenDCN:A general framework for achieving energy efficiency in data center networks">
                                        <b>[88]</b>
                                        Wang Lin, Zhang Fa, Aroca J, et al.GreenDCN:A general framework for achieving energy efficiency in data center networks[J].IEEE Journal on Selected Areas in Communications, 2014, 32 (1) :4-15
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1587-1603 DOI:10.7544/issn1000-1239.2019.20180574            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>数据中心能耗模型及能效算法综述</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%BB%A7%E4%B8%9A&amp;code=24164003&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王继业</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E7%A2%A7%E7%8E%89&amp;code=42315259&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周碧玉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B3%95&amp;code=03574055&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张法</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%9F%B3%E7%BF%94&amp;code=42588546&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">石翔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%BE%E6%A5%A0&amp;code=35756075&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曾楠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%BF%97%E5%8B%87&amp;code=10348425&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘志勇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E5%AE%B6%E7%94%B5%E7%BD%91%E5%85%AC%E5%8F%B8&amp;code=0108958&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国家电网公司</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1698670&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院信息工程研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0142480&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院计算技术研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>近年来, 云计算技术发展迅猛.作为云计算的物理平台和重要基础设施, 数据中心的数量和规模都得到了前所未有的发展.与此同时, 数据中心极低的资源利用率和巨大的能耗问题日益突出, 数据中心能效的研究已经成为了近年来学术界与工业界关注的热点.针对数据中心能效的基本问题, 研究了基于资源和任务调度的数据中心节能关键技术, 从能效模型与能效算法的角度总结了数据中心服务器系统与网络系统的节能研究进展和最新成果, 涵盖能效分析、能耗模型、分类标准和策略算法4个方面, 并且展望了数据中心能效优化研究的发展趋势.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%BD%E8%80%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">能耗;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据中心;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B3%BB%E7%BB%9F%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">系统模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%BD%E6%95%88%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">能效算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E4%BC%98%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最优化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘志勇, zyliu@ict.ac.cn;
                                </span>
                                <span>
                                    王继业, jiyewang@sgcc.com.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2017YFB1010001);</span>
                                <span>国家自然科学基金项目 (61520106005, 61761136014);</span>
                    </p>
            </div>
                    <h1><b>Data Center Energy Consumption Models and Energy Efficient Algorithms</b></h1>
                    <h2>
                    <span>Wang Jiye</span>
                    <span>Zhou Biyu</span>
                    <span>Zhang Fa</span>
                    <span>Shi Xiang</span>
                    <span>Zeng Nan</span>
                    <span>Liu Zhiyong</span>
            </h2>
                    <h2>
                    <span>State Grid Corporation of China</span>
                    <span>Institute of Information Engineering, Chinese Academy of Sciences</span>
                    <span>Institute of Computing Technology, Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the rapid development of cloud computing and virtualization technology, the number as well as the scale of data centers are growing rapidly around the world, which results in huge energy consumption of data centers. At the same time, however, the resource utilization of data centers are very low, leading to a lot of waste of energy. Due to the contradiction between huge energy consumption and extreme low resource utilization in data centers, optimizing the energy efficiency of data centers has become a hot topic in both academic and industrial fields in recent years. Aiming at the basic problem of energy efficiency in data center, we study the key technology of energy saving in data center based on resource and task scheduling. From the view of energy efficiency model and energy efficiency algorithm, the research progress and the latest achievement of energy efficiency in data center are summarized, mainly on server system and network system. This paper firstly decomposes and analyzes the data center energy consumption sources. Then the typical energy consumption models and classification standard are introduced. Based on the models and the standard, the strategy algorithms are summarized. The development trend of energy efficiency optimization in data center is also prospected.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high-efficiency&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high-efficiency;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20center&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data center;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=system%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">system model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=energy%20efficient%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">energy efficient algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optimization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optimization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Wang Jiye, born in 1964.PhD, professor-level senior engineer.His main research interests include information, management of power system, smart grid and the next generation energy system.<image id="659" type="" href="images/JFYZ201908001_65900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhou Biyu, born in 1990.PhD, assistant professor.Her main research interests include green computing, cloud computing, and machine learning.<image id="661" type="" href="images/JFYZ201908001_66100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhang Fa, born in 1974.PhD, professor, PhD supervisor.His main research interests include energy-aware algorithms and high performance computing.<image id="663" type="" href="images/JFYZ201908001_66300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Shi Xiang, born in 1992.PhD candidate. Her main  research  interests  include networks, cloud computing, and algorithms.<image id="665" type="" href="images/JFYZ201908001_66500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zeng Nan, born in 1978.Master, senior engineer. His main research interests include electric power information manage-ment and technology.<image id="667" type="" href="images/JFYZ201908001_66700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Zhiyong, born in 1946.PhD, professor, PhD supervisor.His main research interests include high performance algorithms and architectures, parallel  processing  and networks.<image id="669" type="" href="images/JFYZ201908001_66900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2017YFB1010001);</span>
                                <span>the National Natural Science Foundation of China (61520106005, 61761136014);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="185">近年来, 云计算技术发展迅猛.作为云计算的物理平台和重要基础设施, 数据中心的数量和规模都得到了前所未有的发展.据报道<citation id="603" type="reference"><link href="427" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 截止2017年, 世界上正在运营的各种类型的数据中心总数已经高达860万.迅速增长的数据中心数目带来了2方面问题:1) 给运营商带来日益庞大的能耗开销.统计结果表明<citation id="604" type="reference"><link href="429" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 2014年美国的数据中心电能消耗总量占据全美全年总耗电量的1.8%;预计到2020年, 全球的数据中心总的耗电量将占到当年全球总的耗电量的8%<citation id="605" type="reference"><link href="431" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.2) 给社会来了沉重环保的压力.全球环保组织 (Global e-Sustainability Initiative, GeSI) 估计, 到 2020年数据中心的温室气体的排放将占到信息技术领域的18%<citation id="606" type="reference"><link href="433" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="186">与巨大的能耗相对应的是极低的资源利用率, 统计报告称<citation id="607" type="reference"><link href="435" rel="bibliography" /><link href="437" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 典型数据中心的资源利用率通常在5%～25%之间.产生这一现象的主要原因是数据中心的运营者通常会为了追求高性能、保证服务质量和可靠性, 而采用冗余资源部署的策略.因此, 不论当前负载如何, 所有的服务器都将处于最高速率运行状态.这些多维度资源 (CPU、存储、内存和网络带宽) 的低利用率直接导致了巨大的资源浪费.而计算、存储、通讯等资源的无效使用, 也加重了其他如冷却系统、配电装置等配套设备的投入.</p>
                </div>
                <div class="p1">
                    <p id="187">数据中心巨大的能源消耗, 严重阻碍了数据中心本身的发展以及节能型社会的创建, 已经成为一个对技术、经济、环境发展具有重要影响的重大社会问题, 急需解决.结合数据中心巨大的能耗和极低的资源利用率两大问题, 目前已经有很多研究者利用提高数据中心资源利用率的方法来降低数据中心能耗.然而, 不恰当的提高资源利用率也可能带来应用的性能和服务质量的损失.因此, 在进行数据中心能耗优化研究的同时, 针对数据中心的特点, 提出在保障服务质量和应用性能的前提下可以优化数据中心能耗的策略和算法, 对于提高数据中心能效研究具有重大意义.</p>
                </div>
                <div class="p1">
                    <p id="188">本文从资源和任务的分配调度的角度研究数据中心的节能问题, 从能效分析、能耗模型、分类标准和策略算法4个方面对近年来数据中心能效优化领域的研究进展和最新成果进行全面的综述.</p>
                </div>
                <h3 id="189" name="189" class="anchor-tag"><b>1 数据中心能效分析</b></h3>
                <div class="p1">
                    <p id="190">数据中心是一整套用于承载计算机系统和相关组件的复杂设施.它不仅仅包括计算机系统 (例如服务器、通信和存储系统等) , 还包含与之配套的数据通信连接、环境控制设备 (例如空调系统和消防系统) 、监控设备以及各种安全装置<citation id="608" type="reference"><link href="439" rel="bibliography" /><link href="441" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>.计算机系统和其他与之配套的设备是数据中心的重要组成部分, 同时也是数据中心能量消耗的主要来源.</p>
                </div>
                <h4 class="anchor-tag" id="191" name="191"><b>1.1 数据中心能耗分析</b></h4>
                <div class="p1">
                    <p id="192">如图1所示, 数据中心的能量消耗来源可以大致分为2部分<citation id="609" type="reference"><link href="435" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:IT设施能耗和基础设施能耗.IT设施指的服务器系统 (包含服务器和存储) 、网络系统等信息技术设施, 而配套基础设施主要包含冷却系统和供电系统 (包含供电系统和照明系统) .</p>
                </div>
                <div class="area_img" id="193">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908001_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 数据中心能耗来源分解" src="Detail/GetImg?filename=images/JFYZ201908001_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 数据中心能耗来源分解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908001_193.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Data center energy source decomposition</p>

                </div>
                <div class="p1">
                    <p id="194">IT设备和配套基础设施的能耗比重由数据中心的具体设计和设备本身的能耗效率决定.通常, 数据中心能耗最大系统是服务器系统, 其能耗占据数据中心总能耗的40%以上, 其次是制冷系统<citation id="610" type="reference"><link href="437" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.IT设备的总能耗占据数据中心总能耗的50%左右, 而在Pelley等人<citation id="611" type="reference"><link href="443" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的报告中, 该比重甚至高达61%.</p>
                </div>
                <h4 class="anchor-tag" id="195" name="195"><b>1.2 数据中心能效衡量标准</b></h4>
                <div class="p1">
                    <p id="196">制定合理的数据中心能效衡量标准是提高数据中心能效需要解决的第1个问题.目前, 工业界广泛使用的数据中心能效衡量标准主要是电能使用效率 (power usage efficiency, PUE) <citation id="612" type="reference"><link href="445" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 其定义为数据中心的总输入能耗占输入到IT设备子系统的总能耗的比值.PUE是基于数据中心有效能耗这个概念来设计的, 其原理是数据中心输入的总能耗中只有一部分用于数据中心计算功能实现上, 剩下的能量消耗在支撑计算功能运行的环节, 即供电系统和冷却系统等.类似的衡量标准还有PUE的倒数, 即数据中心基础设施效率 (datacenter infrastructure efficiency, DCiE) <citation id="613" type="reference"><link href="445" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.然而, PUE可用性很差, 因为数据中心的总输入能耗在实际中不好测量, 因此GreenGrid建议将数据中心的总输入能耗分解为总制冷能耗、总电源供电系统能耗以及IT总能耗3个部分分别进行测量.此外, PUE衡量的只是输入到IT设备的在总能耗与数据中心总输入能耗之间的关系, 用来描述数据中心能效并不十分恰当, 更合理的能效评估标准应该是比较系统的有效输出与系统的总输入之间的关系.然而, 对系统的有效输出的定义和测量尚未有令人信服的研究.</p>
                </div>
                <div class="p1">
                    <p id="197">除上面2个常用衡量指标外, 数据中心的能效指标还有衡量数据中心计算效率的数据中心有效能耗比 (data center energy productive, DCeP) <citation id="614" type="reference"><link href="433" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、衡量数据中心绿色环保程度的碳使用效率 (carbon usage effectiveness, CUE) <citation id="615" type="reference"><link href="433" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、衡量数据中心IT能耗的碳排放效率的冷却系统效率 (cooling system efficiency, CSE) <citation id="616" type="reference"><link href="447" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、衡量冷却系统效率的气流节省器效率 (air economizer utilization, AEU) <citation id="617" type="reference"><link href="447" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、水流节省器效率 (water economizer utilization, WEU) <citation id="618" type="reference"><link href="433" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等.上述指标均针对特定系统和目标而设, 研发更加实用的数据中心性能指标仍旧是目前一大难题.</p>
                </div>
                <h3 id="198" name="198" class="anchor-tag"><b>2 数据中心能耗模型和节能机制</b></h3>
                <div class="p1">
                    <p id="199">研究和分析数据中心能耗模型对提高数据中心能效有着重要的意义.一方面, 数据中心是一个非常复杂的设施, 找出影响数据中心能效的瓶颈因素才能有针对性地提高数据中心总体能效;另一方面, 通过对数据中心能耗的形式化描述, 把数据中心能效优化问题转化为一些经典的优化问题, 通过对优化问题的求解可以为数据中心能效优化提供方针和策略.</p>
                </div>
                <div class="p1">
                    <p id="200">与IT设备相比, 基础设施的能耗行为不仅取决于设备本身的能耗效率、负载水平, 还取决于输入能源的种类、时间和地理等复杂因素.此外, 数据中心IT设备能耗占据数据中心总能耗的一半多, 该比例随着数据中心高效制冷和智能供电技术的提高逐年增加, 而降低IT设备的能耗也能降低冷却系统和配电系统等配套设备的能耗.因此, 本文主要研究数据中心IT设备的节能问题.本节将总结近年来使用广泛的服务器系统和网络系统的能耗模型, 为后续高能效策略和算法的设计和构建提供基础.</p>
                </div>
                <h4 class="anchor-tag" id="201" name="201"><b>2.1 数据中心能耗模型</b></h4>
                <h4 class="anchor-tag" id="202" name="202">2.1.1 服务器设备的能耗模型</h4>
                <div class="p1">
                    <p id="203">数据中心的服务器设备主要包括通用的计算型服务器和存储型服务器.目前, 数据中心应用最广的服务器设备的能耗模型主要有加性模型和基于系统利用率模型.</p>
                </div>
                <div class="p1">
                    <p id="204">加性模型指的是将整个服务器的能耗形式化成服务器子结构的能耗之和.核心思想是将拟合后的局部非参量函数组合在一起以建立目标模型, 因此加性模型可以简单地看作是一种线性回归的改良版本.Roy等人<citation id="619" type="reference"><link href="449" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>利用这种思想提出了一种简单的服务器加性模型, 该模型考虑了CPU和内存的能耗, 其模型为</p>
                </div>
                <div class="p1">
                    <p id="205"><i>E</i> (<i>A</i>) =<i>E</i><sub>CPU</sub> (<i>A</i>) +<i>E</i><sub>memory</sub> (<i>A</i>) , </p>
                </div>
                <div class="p1">
                    <p id="206">其中, <i>E</i><sub>CPU</sub> (<i>A</i>) 和<i>E</i><sub>memory</sub> (<i>A</i>) 分别表示运行算法<i>A</i>时CPU和内存消耗的能量, 具体计算细节详见文献<citation id="620" type="reference">[<a class="sup">12</a>]</citation>.此后, 很多研究者围绕该模型进行了细化和完善, 这些工作<citation id="623" type="reference"><link href="451" rel="bibliography" /><link href="453" rel="bibliography" /><link href="455" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>主要着眼于将更多的服务器能耗部件考虑进模型, 比如磁盘、I/O设备、网卡等.此外, 还有一些研究<citation id="624" type="reference"><link href="457" rel="bibliography" /><link href="459" rel="bibliography" /><link href="461" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>将服务器主板的能耗考虑进去, 或者干脆直接将这部分能耗看作一个常量加到模型中<citation id="621" type="reference"><link href="463" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.然而, 这些模型的通病在于现有的平台只可以测量出服务器总能耗, 而这些子系统的精确能耗值尚不能独立地测量出来<citation id="622" type="reference"><link href="465" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="207">除了加性模型之外, 另一类最常用的服务器能耗模型是基于系统利用率的模型.人们观察到服务器系统能耗由静态能耗与动态能耗2部分组成, 而系统的动态能耗与各个子系统的资源利用率相关, 因此将子系统资源利用率作为变量纳入服务器能耗模型之内.考虑到CPU是服务器各个子系统中能耗最大的部件, 通常将CPU的利用率作为服务器系统能耗模型的变量.这类模型最早是将CPU的运行时钟频率作为变量纳入能耗模型中进行计算<citation id="625" type="reference"><link href="467" rel="bibliography" /><link href="469" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>.这类研究基本上可以看作基本数字电路级功率模型<citation id="626" type="reference"><link href="471" rel="bibliography" /><link href="473" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>的一个扩展, 将CPU的能耗形式化为</p>
                </div>
                <div class="p1">
                    <p id="208"><i>P</i>=<i>c</i><sub>0</sub>+<i>ACV</i><sup>2</sup><i>f</i>, </p>
                </div>
                <div class="p1">
                    <p id="209">其中, <i>c</i><sub>0</sub>为CPU的静态功率, <i>ACV</i><sup>2</sup><i>f</i>是其动态功率.<i>A</i>为转换系数, <i>C</i>为电容, <i>V</i>为电压, <i>f</i>为时钟频率.对于特定的硬件, <i>c</i><sub>0</sub>, <i>A</i>, <i>C</i>均为常数.而<i>V</i>与<i>f</i>成正比, 所以CPU的动态能耗可以认为是与其时钟频率成3次方关系.又因为<i>f</i>与系统运行速度<i>s</i>成正比例关系, 因此, 相关研究<citation id="627" type="reference"><link href="475" rel="bibliography" /><link href="477" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>建立了动态功率<i>P</i>与运行速率<i>s</i> (<i>s</i>&gt;0) 之间的关系为</p>
                </div>
                <div class="p1">
                    <p id="210"><i>P</i> (<i>s</i>) =<i>σ</i>+<i>μs</i><sup><i>α</i></sup>, </p>
                </div>
                <div class="p1">
                    <p id="211">其中, <i>σ</i>为静态功率, <i>μ</i>和<i>α</i>为常数, 与具体的硬件设备有关, <i>α</i>&gt;1.此类模型另一个常用的方式是通过预估系统各个部件的功率情况, 采用线性回归的手段得到服务器与各种资源利用率的函数关系<citation id="628" type="reference"><link href="479" rel="bibliography" /><link href="481" rel="bibliography" /><link href="483" rel="bibliography" /><link href="485" rel="bibliography" /><sup>[<a class="sup">27</a>,<a class="sup">28</a>,<a class="sup">29</a>,<a class="sup">30</a>]</sup></citation>.对系统各个部件的功率预估可以采用入侵式和非入侵式2种策略.然而, 这种基于归回分析的方法需要针对特定的服务器做大量的实验, 以得到相应服务器的能耗参数.除上面2种模型之外, 还有一种使用广泛的基于利用率的功率模型, 由Fan等人<citation id="629" type="reference"><link href="487" rel="bibliography" /><link href="489" rel="bibliography" /><sup>[<a class="sup">31</a>,<a class="sup">32</a>]</sup></citation>提出.该模型自提出后, 一直被用来对数据中心能耗进行建模, 在数据中心能耗研究领域影响甚是深远.模型提出者证明, 线性功耗模型可以更加精确地追踪服务器系统的功率使用情况.在假设服务器处于关闭状态下功率近似为0的条件下, 可以将任何一台服务器在任意CPU利用率<i>u</i>情况下的全系统功率形式化为表达式:</p>
                </div>
                <div class="p1">
                    <p id="212"><i>P</i><sub><i>u</i></sub>= (<i>P</i><sub>max</sub>-<i>P</i><sub>idle</sub>) <i>u</i>+<i>P</i><sub>idle</sub>, </p>
                </div>
                <div class="p1">
                    <p id="213">其中, <i>P</i><sub>max</sub>和<i>P</i><sub>idle</sub>分别代表服务器在全速率工作和空闲状态的平均功率.</p>
                </div>
                <div class="p1">
                    <p id="214">加性模型和基于系统利用率的模型是当前数据中心能耗研究中应用最广的服务器能耗模型.除此之外, 学术界针对服务器的能耗还提出了一些其他模型<citation id="630" type="reference"><link href="491" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>, 在此不一一列举.</p>
                </div>
                <h4 class="anchor-tag" id="215" name="215">2.1.2 网络系统的能耗模型</h4>
                <div class="p1">
                    <p id="216">本文中我们全面考虑数据中心网络的能耗行为, 具体来说, 包括连接数据中心内部各个服务器设备以及连接不同地域分布的数据中心站点之间的网络系统的能耗模型.</p>
                </div>
                <div class="p1">
                    <p id="217">从全局角度对数据中心网络能耗行为进行建模的研究主要采用的是加性模型, 将网络全局能耗拆分为3个部分能耗之和, 即网络链路、网络设备和网络接口3个部分.通常情况下, 普通网络接口能耗所占比例很小, 因此一些研究在建模时直接将该部分能耗忽视, 或者将网络接口能耗整合进网络链路或者是网络设备能耗中, 不再单独考虑, 该类模型的典型表达式<citation id="631" type="reference"><link href="493" rel="bibliography" /><link href="495" rel="bibliography" /><link href="497" rel="bibliography" /><link href="499" rel="bibliography" /><link href="501" rel="bibliography" /><sup>[<a class="sup">34</a>,<a class="sup">35</a>,<a class="sup">36</a>,<a class="sup">37</a>,<a class="sup">38</a>]</sup></citation>为</p>
                </div>
                <div class="p1">
                    <p id="218" class="code-formula">
                        <mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>E</mi></mrow></munder><mi>X</mi></mstyle><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mi>a</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>V</mi></mrow></munder><mi>Y</mi></mstyle><msub><mrow></mrow><mi>u</mi></msub><mi>b</mi><mo stretchy="false"> (</mo><mi>u</mi><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="219">其中, <i>a</i> (<i>u</i>, <i>v</i>) 是链路 (<i>u</i>, <i>v</i>) 的能耗, <i>b</i> (<i>u</i>) 是网络设备 (交换机或者路由器等) <i>u</i>的能耗, <i>X</i><sub><i>u</i>, <i>v</i></sub>为二元变量, 代表链路 (<i>u</i>, <i>v</i>) 的状态是开启或者关闭, <i>Y</i><sub><i>u</i></sub>也是二元变量, 代表网络设备<i>u</i>的状态是开启或者关闭.对于给定的网络场景, 链路的能耗与网络设备的能耗均是固定不变的常量.</p>
                </div>
                <div class="p1">
                    <p id="220">另一些研究侧重于研究数据中心网络设备, 尤其是交换机和路由器的能耗模型.这一类研究主要借鉴于对服务器能耗的研究<citation id="632" type="reference"><link href="491" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>, 也主要分为2种:加性模型和基于利用率的模型.具体的模型刻画过程在此不再赘述.</p>
                </div>
                <h4 class="anchor-tag" id="221" name="221"><b>2.2 数据中心节能机制</b></h4>
                <div class="p1">
                    <p id="222">为了保证对用户提供稳定和高质量的计算服务, 不管当前负载水平如何, 传统数据中心内的IT设备都是以最大运行速率运行, 造成了大量的能耗浪费.对上述数据中心服务器系统和网络系统的能耗模型进行分析可以很容易得到节省数据中心能耗的基本思路.最直观的方式是让处于空闲状态的IT设备进入休眠或者关闭状态;其次是使得设备可以根据当前负载水平动态调整设备电压和CPU运行的时钟频率.总结起来有2个方面的内容:1) 在构建数据中心时, 淘汰落后的设备和仪器, 选用更先进和能效比更高的设备节点等.该方法注重于提高单个设备本身的能量利用效率;2) 通过优化数据中心IT设备的资源和任务调度和分配方式来提高整体资源的能耗利用效率.对具有节能功能的IT设备的研究工作主要集中于设备本身硬件的升级换代, 这方面的研究超出了本文的讨论范围.优化数据中心IT设备的资源和任务调度依赖于采用的调度策略和算法的优劣, 本文将在第4节重点对这部分内容进行总结.</p>
                </div>
                <div class="p1">
                    <p id="223">目前对数据中心进行节能的研究方法主要是将上述2种方法进行结合, 即在部署具有节能功能的IT设备的数据中心内, 通过优化数据中心IT设备的资源和任务调度和分配方式来提高数据中心整体资源的能耗利用效率.该方法包含2个核心要素:首先数据中心设备具有节能功能, 其次设计有效的资源调度和任务分配策略和算法指导资源的优化配置, 使得更多的设备可以最大程度地节省能耗.根据不同的应用场景和需求差异, 目前最主流的数据中心IT设备节能机制可以分为3种:</p>
                </div>
                <div class="p1">
                    <p id="224">1) 休眠机制.在IT设备运行的过程中, 系统不可避免会在不同时间段内处于空闲状态, 该状态下设备消耗额定功耗的能量却未进行任何有效的计算, 此时如果将设备状态切换至休眠或者关闭状态, 可以减少空闲状态下的能耗浪费.事实上, 目前大部分设备在工作和休眠之间还存在多个对应不同能耗水平的休眠状态.然而, 在不同的休眠状态下将设备唤醒至工作状态需要花费不同的时间.一般来说, 随着休眠深度的增加, 设备消耗的能量也相应减少, 而从休眠状态下唤醒该设备需要的时间也相应的延长.实现通过休眠的方式使全局设备节省能耗最多且对服务性能损失最小化这个目标需要通过设备间的配合和合理的资源调度策略来实现.通常的做法是采用资源聚合的思路将任务合并到尽可能少的物理机器上运行, 或者用尽可能少的数据链路传输给定的数据传输任务.该做法的难点在于如何在保证各种服务质量的情况下使得可节省的能耗最大化.</p>
                </div>
                <div class="p1">
                    <p id="225">2) 速率缩放.在运行过程中, 不管当前负载水平如何, 传统的设备都是以最大运行速度运行.而设备的能耗与其运行速度成正相关, 因此在负载水平低时大量能耗被浪费.调整设备运行的电压或者时钟频率、速率等, 使之适应不同的负载水平, 以节省设备动态能耗的方法被称作速率缩放节能机制.通常而言, 设备的能耗是设备运行速率的超线性幂函数 (例如<i>y</i>=<i>a</i>+<i>bx</i><sup><i>n</i></sup>, <i>n</i>&gt;1) , 因此, 降低设备能耗往往意味着调低设备的运行速率.然而, 低速运行的设备需要花费更长的运行时间来完成给定的计算任务, 由此将导致对服务质量的影响.因此, 利用速率缩放机制进行设备节能的关键问题在于如何权衡能耗与服务性能之间的关系.</p>
                </div>
                <div class="area_img" id="226">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908001_226.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数据中心节能策略分类标准" src="Detail/GetImg?filename=images/JFYZ201908001_226.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 数据中心节能策略分类标准  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908001_226.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Data center energy saving strategy classification</p>

                </div>
                <div class="p1">
                    <p id="227">3) 混合策略.除上述2种节能策略外, 目前研究比较多的还有混合策略.顾名思义, 该策略就是将休眠机制和速率缩放机制结合起来考虑, 两者的结合给设备的节能提供了更多的潜在节能空间.然而, 采用速率缩放机制进行节能的策略会尽可能降低设备运行速率, 与之相反, 采用休眠机制的节能策略会尽可能提高设备的运行速率以产生更多的设备空闲时间片供设备休眠省能.如何去寻找2种机制的平衡点和最优折中点是混合策略机制的最大挑战.</p>
                </div>
                <div class="p1">
                    <p id="228">在数据中心内, IT设备的节能机制主要配合虚拟化技术进行.近年来, 数据中心虚拟化技术的发展给数据中心节能带来了新的活力.虚拟化技术最初用于在1个宿主操作系统上运行多个客户操作系统 (称为虚拟机) , 通过1个管理程序来生成模拟真实计算环境的虚拟环境, 然后在这些虚拟机上分发真实资源.这项技术使得服务软件不再需要固定占用某些硬件资源, 而是能够以非常灵活的方式部署, 使得各种软件可以更加灵活利用硬件平台;此外, 数据中心的计算能力不再由物理机的数量决定, 而是随着资源的动态变化而增减, 也就是所谓的弹性计算.弹性计算环境中, 虚拟机可以在不同物理计算位置间迁移与合并, 还可以随时被快速启动和关闭删除, 这样的灵活性使得人们可以从优化数据中心能效的角度来对数据中心资源进行合理分配与调度以提高资源使用效率.例如, 通过将适量虚拟机合并到尽可能少的物理设备中, 以最大化可关闭的物理设备, 从而最大化能耗节省;再例如, 可以通过合理分配虚拟机在不同物理位置上设备的分布, 使得全局设备能耗最小.虚拟化技术的引入, 使得数据中心资源和任务的分配可以更加的灵活, 从而提高了数据中心节能的潜在空间.随着云计算和大数据的蓬勃发展, 虚拟化技术在全球范围内的数据中心内均有广泛的应用, 因此, 针对虚拟化资源的调度与分配研究逐渐成为数据中心节能研究的热点之一.</p>
                </div>
                <h3 id="229" name="229" class="anchor-tag"><b>3 数据中心节能研究的分类比较</b></h3>
                <div class="p1">
                    <p id="230">面对数据中心的高能耗与低利用率, 近几年很多研究者纷纷提出了降低能量消耗、提高能效的方案, 这些方案基本上都是围绕上节中总结的节能机制来展开的.本节首先总结现有的优化数据中心能效策略的分类标准, 然后依据上述总结的分类标准对现有节能策略进行对比分析.</p>
                </div>
                <div class="p1">
                    <p id="231">针对数据中心IT设备能效优化策略的分类方法有很多, 已有的分类标准可以总结如图2所示.最直观的分类标准是按照优化部件划分的.依据数据中心IT设备的能耗情况可以将节能策略分为两大类:针对服务器设备的节能策略与针对网络设备的节能策略.其中, 2种节能策略有时候需要彼此的相互配合, 例如, 为了达到最佳的网络节能水平, 需要联合优化服务器的部署产生有利于网络节能的流量分布.</p>
                </div>
                <div class="p1">
                    <p id="232">节能策略可以根据实施对象的节点数目进行分类, 例如单处理器节点、多处理器节点、单个数据中心以及跨数据中心.不同场景下需要考虑的因素不同, 因此针对不同节点数目设计的节能策略也不尽相同.针对单节点的节能策略主要通过单个节点的资源分配使得能耗与负载成正比例;对于多处理器的节点就要涉及到节点内部不同处理器之间的任务分配, 这时候已经上升成全局视角的能耗优化;对于数据中心内部的节能机制也是全局视角的能耗优化, 不同的是数据中心内部能耗优化不仅涉及服务器, 还涉及数据中心内网能耗优化;至于跨数据中心, 需要在更高的层次考虑整个分布式系统的资源调度问题.</p>
                </div>
                <div class="p1">
                    <p id="233">按照实现方法可以分为动态速率缩放、休眠机制、混合策略以及虚拟化4个类别.这个分类标准主要是基于节能机制的不同划分的, 各个实现方法可以相互配合.例如虚拟化技术通常都需要配合另外3种机制才可以发挥最大的效果, 而混合策略本身就是动态速率缩放与休眠机制共同配合的产物.</p>
                </div>
                <div class="p1">
                    <p id="234">不同的节能策略需要的信息输入也不尽相同.例如一些策略需要提前知道系统的历史数据, 将历史数据进行分析处理, 从中总结出一些规律和特征, 然后根据这些特征提出特定的节能策略;另一些策略不需要提前知道系统的历史数据, 策略本身可以根据实时数据进行自适应迭代, 或者策略本身就是固定不变的.2种方式各有利弊, 前1种可能有比较好的节能效果, 但是历史数据的获得本身是困难甚至是不可能完成的任务;后者不需要历史数据输入, 但是如何做到实时对系统状态的变化做出高效的反应也是个非常具有挑战性的问题.</p>
                </div>
                <div class="p1">
                    <p id="235">数据中心的负载大体可以分为3类:预测数据、及时交互型以及批量处理型.针对不同的负载提出节能策略对时间敏感程度不同.针对即时交互型负载的事件敏感度最高, 其次是批量处理型, 对于预测数据的事件敏感程度最低.</p>
                </div>
                <div class="p1">
                    <p id="236">针对上述各种的分类方法涌现了很多的研究成果, 在此不再一一详细描述.接下来主要介绍数据中心能耗系统模型的构建和算法设计问题.</p>
                </div>
                <h3 id="237" name="237" class="anchor-tag"><b>4 数据中心能耗研究进展</b></h3>
                <div class="p1">
                    <p id="238">在本节中, 我们将从节能部件的角度对近年来数据中心能耗系统的模型构建方法与算法设计进行总结, 分为对服务器端节能和对网络端节能2类.</p>
                </div>
                <h4 class="anchor-tag" id="239" name="239"><b>4.1 服务器系统节能</b></h4>
                <div class="p1">
                    <p id="240">采用速率缩放机制进行服务器节能的研究主要的思想是降低服务器的运行速率.然而, 降低运行速率会引起任务完工时间延长, 因此, 采用该种机制进行节能时, 必须要考虑到对完工时间的影响.首先将采用速率缩放机制的服务器节能问题描述如下:</p>
                </div>
                <div class="p1">
                    <p id="241">假设处理速度为<i>s</i>时的服务器能耗函数为<i>p</i>=<i>s</i><sup><i>α</i></sup>, <i>α</i>&gt;1.给定<i>n</i>个相互独立的任务集<i>J</i>={<i>J</i><sub>1</sub>, <i>J</i><sub>2</sub>, …, <i>J</i><sub><i>n</i></sub>}和<i>m</i>个同构或者异构的服务器节点集合<i>P</i>={<i>P</i><sub>1</sub>, <i>P</i><sub>2</sub>, …, <i>P</i><sub><i>m</i></sub>}.每个任务<i>J</i><sub><i>j</i></sub>的需要执行的周期数为<i>w</i><sub><i>j</i></sub>.假设任务具有抢占性, 即1个服务器节点1个时刻只能处理1个任务.所有任务在调度之前已经到达, 并且有相同的截止时间<i>C</i>.在时刻<i>t</i>, 服务器节点执行任务<i>J</i><sub><i>j</i></sub>的处理速度为<i>s</i><sub><i>jt</i></sub>, 相应的功耗为 (<i>s</i><sub><i>jt</i></sub>) <sup><i>α</i></sup>.记<i>c</i><sub><i>j</i></sub>为完成任务<i>J</i><sub><i>j</i></sub>的时间, <i>x</i><sub><i>ij</i></sub> (<i>i</i>=1, 2, …, <i>m</i>;<i>j</i>=1, 2, …, <i>n</i>) 为0或1的指示变量, 表示<i>J</i><sub><i>j</i></sub>是否分配给服务器<i>P</i><sub><i>i</i></sub>处理.问题是如何为每1个任务分配1个服务器节点, 并且为每个任务设置处理速度, 使得在满足任务截止时间限制条件下, 所有服务器节点消耗的总能耗最小.该问题可以被形式化为约束的优化问题:</p>
                </div>
                <div class="p1">
                    <p id="242">min<mathml id="243"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mo stretchy="false"> (</mo></mrow></mstyle></mrow></mstyle><mi>s</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>t</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mi>α</mi></msup><mspace width="0.25em" /><mtext>d</mtext><mi>t</mi></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="244">s.t. <i>c</i><sub><i>j</i></sub>≤<i>C</i>, ∀<i>J</i><sub><i>j</i></sub>, (1) </p>
                </div>
                <div class="p1">
                    <p id="245"><mathml id="246"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mo>∀</mo><mi>J</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>, (2) </p>
                </div>
                <div class="p1">
                    <p id="247"><i>x</i><sub><i>ij</i></sub>∈{0, 1}, ∀<i>J</i><sub><i>j</i></sub>, <i>P</i><sub><i>i</i></sub>∈<i>M</i>, (3) </p>
                </div>
                <div class="p1">
                    <p id="248">其中:式 (1) 限制所有任务均在截止时间之前完成;式 (2) 限制每个任务只能部署在1台服务器上;式 (2) 为0-1二元变量约束.该问题的求解可以借鉴多处理器上最小完工时间的调度问题.Yao等人<citation id="633" type="reference"><link href="475" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>最先使用了速率缩放机制来研究单服务器节点上的最优能耗调度问题.此后, 针对不同的限定性约束和不同的计算环境平台设置, 研究者们纷纷提出了不同的节能资源调度方法以及得到了相应的结果, 归纳如表1所示:</p>
                </div>
                <div class="area_img" id="249">
                    <p class="img_tit"><b>表1 节能资源调度策略分类</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 The Classification of Energy Efficient Resource Scheduling Strategies</b></p>
                    <p class="img_note"></p>
                    <table id="249" border="1"><tr><td><br />Classification</td><td>Strategy</td><td>Source</td></tr><tr><td><br />Constraints</td><td>Time/Throughput/Sequence<br />of Tasks</td><td>Ref [39-42]</td></tr><tr><td><br />Granularity</td><td>Single Processor/Multiple<br />Processors/Multiple DCs</td><td>Ref [43-47]</td></tr><tr><td><br />Task Type</td><td>Preemptive/Non-preemptive</td><td>Ref [48-49]</td></tr><tr><td><br />Model</td><td>Continuous/Discrete</td><td>Ref [42, 48]</td></tr><tr><td><br />Platform</td><td>Homogenous/Heterogeneous</td><td>Ref [12, 50-53]</td></tr><tr><td><br />Time Scale</td><td>Online/Offline</td><td>Ref [12, 42, 48, 50]</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="250">然而这些研究都假设每个任务能在所有的服务器节点上运行, 但是在当前的数据中心计算系统中任务与服务器节点间存在选择关系, 即任务只能在部分的服务器节点上运行.这个特性主要源于3个原因:</p>
                </div>
                <div class="p1">
                    <p id="251">1) 由于计算系统中的服务节点会在不同的时间点采购, 所以表现出不同的计算能力和差异配置<citation id="634" type="reference"><link href="525" rel="bibliography" /><sup>[<a class="sup">50</a>]</sup></citation>.比如需要GPU辅助来执行的任务必须在有GPU的服务器节点上执行<citation id="635" type="reference"><link href="533" rel="bibliography" /><sup>[<a class="sup">54</a>]</sup></citation>;</p>
                </div>
                <div class="p1">
                    <p id="252">2) 有些服务器节点与某类任务之间存在定制的关系, 即这些服务器是专门针对这类任务设计的, 所以这类任务应该安排在最适合它们运行的那些服务器节点上<citation id="636" type="reference"><link href="535" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>;</p>
                </div>
                <div class="p1">
                    <p id="253">3) 当考虑输入端的数据时, 任务需要安排在包含有它的输入数据的服务器节点上运行, 比如Hadoop应用需要考虑数据的局部性<citation id="637" type="reference"><link href="537" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="254">针对这个问题, 文献<citation id="638" type="reference">[<a class="sup">57</a>]</citation>在最基本的节能调度问题的基础上增加1条限制:限定可以运行任务<i>J</i><sub><i>j</i></sub>的服务器集合为<i>M</i><sub><i>j</i></sub>⊆<i>P</i> (<i>M</i><sub><i>j</i></sub>≠∅) , 即限制任务只能部署在指定的服务器集合中, 以满足任务与服务器之间具有选择关系的条件.在此模型下, 文中分别讨论了在连续速率调节模型中和在离散速率调节模型中2种不同速率调节策略下的能耗最小化的资源调度, 并且针对任务量相同与任务量不同2种情况分别给出复杂度为<i>O</i> (<i>mn</i><sup>3</sup> log <i>n</i>) 的最优调度算法和近似比为<mathml id="255"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><msup><mrow></mrow><mrow><mi>α</mi><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mn>2</mn><mo>-</mo><mfrac><mn>1</mn><mrow><mi>p</mi><msup><mrow></mrow><mi>α</mi></msup></mrow></mfrac><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>p</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder></mrow></math></mathml>|<i>M</i><sub><i>j</i></sub>|≤<i>m</i>) 的近似算法.实验运行结果表明:文中提出的算法要优于目前的主流算法, 如最少适应任务优先 (least flexible tasks, LFJ) 和最少适应服务器节点优先 (least flexible machine, FM) 算法.类似考虑任务与服务器存在选择关系的研究还有<citation id="639" type="reference"><link href="541" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="256">伴随着人工智能的热潮, GPU被大量用于数据中心应用加速.在混合CPU-GPU集群中, 每个服务器同时配备有多个CPU-GPU对.每个CPU-GPU对可以处于3种状态, 即关闭、工作和空闲, 其中CPU与GPU处于相同的状态.然而, 由于GPU具有与CPU不同的能耗特性, 因此, 现有的CPU节能方案不能直接用于CPU-GPU混合框架的节能.文献<citation id="640" type="reference">[<a class="sup">52</a>]</citation>研究了在CPU-GPU混合数据中心内的节能问题.由于在通常情况下, 1个GPU处理器的能耗远大于1个CPU处理器, 因此, 该研究假设CPU功率为定值, 并将CPU的功率看作是GPU卡的静止功率的一部分, CPU的运行时间也与GPU的运行时间保持一致.1个GPU卡包含1个多核GPU模块和1个GPU存储模块.GPU核以及GPU存储均可以独立调整对应的电压和频率, 从而改变整个GPU卡的运行速率和对应的能耗.因此, 一个给定的任务在特定的CPU-GPU服务器中运行的功耗和运行时间可以建模为</p>
                </div>
                <div class="p1">
                    <p id="257"><i>P</i> (<i>V</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>m</sub></sup>) =<i>P</i><sup><i>G</i><sub>0</sub></sup>+<i>γf</i><sup><i>G</i><sub>m</sub></sup>+<i>c</i><sup><i>G</i></sup> (<sup><i>VG</i><sub>c</sub></sup>) 2<i>f</i><sup><i>G</i><sub>c</sub></sup>, </p>
                </div>
                <div class="p1">
                    <p id="258"><i>T</i> (<i>V</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>m</sub></sup>) =<mathml id="259"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mfrac><mi>δ</mi><mrow><mi>f</mi><msup><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mtext>c</mtext></msub></mrow></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>δ</mi></mrow><mrow><mi>f</mi><msup><mrow></mrow><mrow><mi>G</mi><msub><mrow></mrow><mtext>m</mtext></msub></mrow></msup></mrow></mfrac><mo stretchy="false">) </mo><mo>+</mo><mi>t</mi><msup><mrow></mrow><mn>0</mn></msup></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="260">其中, <i>P</i> (<i>V</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>m</sub></sup>) 和<i>T</i> (<i>V</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>m</sub></sup>) 分别表示CPU-GPU的功率以及运行时间, <i>P</i><sup><i>G</i><sub>0</sub></sup>表示GPU卡的静态功耗 (包含CPU功率) , <i>f</i><sup><i>G</i><sub>m</sub></sup>表示CPU存储的频率, <i>V</i><sup><i>G</i><sub>c</sub></sup>和<i>f</i><sup><i>G</i><sub>c</sub></sup>分别表示GPU核的电压和频率, <i>t</i><sup>0</sup>表示除GPU卡外其他部件的运行时间 (包含CPU运行时) , 而<i>γ</i>, <i>c</i><sup><i>G</i></sup>, <i>δ</i>, <i>D</i>均为参数.在该模型下, 处理单个任务的能耗可以计算为</p>
                </div>
                <div class="p1">
                    <p id="261"><i>E</i><sub><i>J</i></sub>=<i>P</i> (<i>V</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>m</sub></sup>) ×<i>T</i> (<i>V</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>c</sub></sup>, <i>f</i><sup><i>G</i><sub>m</sub></sup>) .</p>
                </div>
                <div class="p1">
                    <p id="262">优化目标是最小化执行所有任务所消耗的总能量, 并且保证每个任务在其完工时间前完成处理任务.该问题的求解仍旧是NP难的, 而求解的关键点在于合理运用<i>V</i><sup><i>G</i><sub>c</sub></sup>和<i>f</i><sup><i>G</i><sub>c</sub></sup>之间的超线性关系.文献<citation id="641" type="reference">[<a class="sup">52</a>]</citation>首先针对单个任务在放松完工时间的约束以及固定GPU存储频率的前提下求解最优的GPU核电压和频率.在该基础上利用求导的方式得到最佳的GPU存储频率设定.随后对完工时间超额的任务进行缩放, 得到最优的设定值.基于单任务最优方案可以由截止时间最短优先的贪心思路得出多任务的启发式方案.类似的CPU-GPU混合节能方案还有文献<citation id="642" type="reference">[<a class="sup">53</a>,<a class="sup">59</a>]</citation>.随着深度学习类应用在数据中心的占比不断攀升, 针对深度学习框架下的CPU-GPU混合架构节能优化研究逐渐引起人们的关注, 然而, 目前尚缺乏该领域的研究工作.</p>
                </div>
                <div class="p1">
                    <p id="263">除速率缩放机制外, 休眠机制也广泛应用于数据中心服务器系统的节能领域.休眠机制的基本思想是将处于空转状态的服务器节点调至低功耗状态, 从而节约能量消耗.因此, 研究的问题一般可以概括为如何在保证完成任务的前提下, 确定什么时候切换低功耗状态以及保持低功耗状态多长时间, 使得总能耗最低.最基本的采用休眠机制的服务器节能模型可以构造为</p>
                </div>
                <div class="p1">
                    <p id="264" class="code-formula">
                        <mathml id="264"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>c</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>e</mtext></mrow></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>;</mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo>;</mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="265">其中<i>y</i><sub><i>i</i></sub>为0-1二元变量, 代表服务器<i>i</i>的开关状态.休眠机制需要将任务集中合并到尽可能少的服务器上运行, 使得处于空闲状态的、可关闭的服务器尽可能多, 从而使得能耗最小化.由于真实情况下物理服务器计算资源有限, 因此不能够无限度地将所有任务集中到几台服务器中运行.假设所有服务器<i>i</i>的资源容量为<i>C</i><sub><i>i</i></sub>, 而每个任务需要的资源为<i>c</i><sub><i>j</i></sub>.考虑与前面描述的速率缩放机制下同样的数据中心服务器系统场景, 基于休眠机制的服务器系统节能问题可以形式化为优化问题:</p>
                </div>
                <div class="p1">
                    <p id="266">min<mathml id="267"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>p</i><sub><i>i</i></sub><i>y</i><sub><i>i</i></sub></p>
                </div>
                <div class="p1">
                    <p id="268">s.t.<mathml id="269"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>c</i><sub><i>j</i></sub><i>x</i><sub><i>ij</i></sub>≤<i>C</i><sub><i>i</i></sub>,  ∀<i>P</i><sub><i>i</i></sub>; (4) </p>
                </div>
                <div class="p1">
                    <p id="270"><mathml id="271"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>x</i><sub><i>ij</i></sub>=1,  ∀<i>J</i><sub><i>j</i></sub>; (5) </p>
                </div>
                <div class="p1">
                    <p id="272"><i>x</i><sub><i>ij</i></sub>≤<i>y</i><sub><i>i</i></sub>,  ∀<i>J</i><sub><i>j</i></sub>; (6) </p>
                </div>
                <div class="p1">
                    <p id="273"><i>x</i><sub><i>ij</i></sub>, <i>y</i><sub><i>i</i></sub>∈{0, 1},  ∀<i>J</i><sub><i>j</i></sub>, <i>P</i><sub><i>i</i></sub>; (7) </p>
                </div>
                <div class="p1">
                    <p id="274">其中, 目标函数是处于开启状态的服务器总能耗.式 (4) 限制合并到服务器上运行的任务总资源需求量不能超过该服务器的资源容量;式 (5) 限制每个任务只能部署在1台服务器上;式 (6) 限制只要有任务运行, 服务器必须处于开启状态;式 (7) 为0-1二元变量约束.</p>
                </div>
                <div class="p1">
                    <p id="275">传统的计算集群中1个服务器节点通常只运行1个应用, 在应用的资源利用率很低时会造成资源的浪费和多余的能量消耗.而现代数据中心内的虚拟化技术通过资源复用, 可以让1个服务器节点运行多个部属在虚拟机上的应用, 从而提高了服务器节点的资源利用率.虚拟化的这个特性为数据中心提高能效带来了新的思路:当多个服务器节点的资源利用率都很低时, 可以聚合部署在它们上面的虚拟机, 从而减少开启的服务器节点数量以节约能量消耗.目前已有一些研究通过让资源利用率低的应用部署到不同的虚拟机上, 然后共用同1个物理服务器节点, 从而提高服务器节点资源利用率和降低能耗<citation id="643" type="reference"><link href="545" rel="bibliography" /><link href="547" rel="bibliography" /><link href="549" rel="bibliography" /><link href="551" rel="bibliography" /><link href="553" rel="bibliography" /><link href="555" rel="bibliography" /><link href="557" rel="bibliography" /><sup>[<a class="sup">60</a>,<a class="sup">61</a>,<a class="sup">62</a>,<a class="sup">63</a>,<a class="sup">64</a>,<a class="sup">65</a>,<a class="sup">66</a>]</sup></citation>.这些研究大都利用虚拟机的历史使用资源情况确定聚合时给虚拟机分配的资源, 然后转化为各种背包问题, 以解决虚拟机的部署和调度.</p>
                </div>
                <div class="p1">
                    <p id="276">然而, 上述虚拟化聚合技术存在2点问题:1) 在动态虚拟机聚合的过程中, 为满足峰值计算而设计的静态虚拟机资源分配方法将导致资源利用率低;2) 虚拟机聚合过程中未考虑因为虚拟机的迁移而导致的服务质量 (service level agreement, SLA) 损失问题.这些问题严重影响了上述虚拟化聚合技术在实际数据中心内的应用.</p>
                </div>
                <div class="p1">
                    <p id="277">为了解决2个问题, 文献<citation id="644" type="reference">[<a class="sup">67</a>]</citation>研究了虚拟机聚合过程中服务质量的优化问题, 其中包括了为虚拟机分配资源的随机规划以及运行时的虚拟机动态整合算法这2方面的内容.主要解决3个问题:1) 设计随机规划方法解决虚拟机在动态运行中的资源分配问题;2) 根据虚拟机在时间片上的资源需求, 设计虚拟机动态聚合算法;3) 采用PlanetLab提供的真实负载, 设计实验来评估算法在节约能量消耗与保障服务质量之间的权衡作用.</p>
                </div>
                <div class="p1">
                    <p id="278">考虑1个包含<i>m</i>个同构或者异构的服务器节点<i>M</i>={1, 2, …, <i>m</i>}.在时刻<i>t</i>有<i>n</i><sub><i>t</i></sub>个虚拟机在运行, 每个虚拟机装载1个应用实例.求不同时刻运行的服务器节点数<i>M</i><sub><i>t</i></sub>⊆<i>M</i>, 使得将空转服务器关闭后数据中心总能耗最低.首先建立物理服务器的功耗模型与服务器节点CPU利用率<i>u</i>之间的关系:</p>
                </div>
                <div class="p1">
                    <p id="279"><i>P</i> (<i>u</i>) =<i>P</i><sub>idle</sub>+ (<i>P</i><sub>busy</sub>-<i>P</i><sub>idle</sub>) ×<i>u</i>, </p>
                </div>
                <div class="p1">
                    <p id="280">其中, <i>P</i><sub>idle</sub>和<i>P</i><sub>busy</sub>分别表示服务器节点在CPU空转和满负荷运行状态下的功耗.当分配给虚拟机的资源少于完成应用请求所需要的资源时, 平均响应时间会超过请求定义的服务质量要求, 因而产生服务质量损失.时刻<i>t</i>的CPU利用率<i>u</i><sub><i>t</i></sub>对服务质量损失代价建模为</p>
                </div>
                <div class="p1">
                    <p id="281" class="code-formula">
                        <mathml id="281"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>u</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>λ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mi>Τ</mi><mo stretchy="false"> (</mo><mi>u</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>λ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>≤</mo><mi>t</mi><msub><mrow></mrow><mi>a</mi></msub><mo>;</mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>u</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>λ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>t</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>Τ</mi><mo stretchy="false"> (</mo><mi>u</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>λ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>&gt;</mo><mi>t</mi><msub><mrow></mrow><mi>a</mi></msub><mo>;</mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="282">其中, <i>λ</i><sub><i>t</i></sub>定义为时刻<i>t</i>的请求数目, 要求的响应时间为<i>t</i><sub><i>a</i></sub>.<i>T</i> (<i>u</i><sub><i>t</i></sub>, <i>λ</i><sub><i>t</i></sub>) 为通过经验学习得到的该时刻的真实响应时间.<i>g</i> (·) 为罚函数, 表示对返回的响应时间大于要求的应用时间<i>t</i><sub><i>a</i></sub>部分的惩罚.定义时间表<i>t</i><sub><i>k</i></sub>为虚拟机触发迁移的时间, <i>τ</i>为迁移间的持续时间.在每个调度时间表<i>t</i><sub><i>k</i></sub>=<i>kτ</i>, <i>k</i>=1, 2, …, <i>T</i>, 虚拟机的资源需求<i>u</i><sub><i>k</i></sub>都将进行重分配.定义<i>T</i> (<i>u</i><sub><i>k</i></sub>, <i>λ</i><sub><i>t</i></sub>) 为请求返回的响应时间.文献<citation id="645" type="reference">[<a class="sup">68</a>]</citation>的研究表明CPU利用率<i>u</i><sub><i>t</i></sub>与请求数目<i>λ</i><sub><i>t</i></sub>成正比, 因此, 通过经验学习的方法可以得到请求返回的响应时间可以用计算为</p>
                </div>
                <div class="p1">
                    <p id="283"><i>T</i> (<i>u</i><sub><i>k</i></sub>, <i>λ</i><sub><i>t</i></sub>) = (<i>a</i>/<i>u</i><sub><i>k</i></sub>+<i>b</i>) <i>u</i><sub><i>t</i></sub>, </p>
                </div>
                <div class="p1">
                    <p id="284">其中, (<i>a</i>, <i>b</i>) 为2个调优参数.在各个时间间隔段上CPU资源利用率<i>u</i><sub><i>t</i></sub>已知的情况下, 最优的CPU分配需求<i>u</i><sub><i>k</i></sub>的值可以通过最小化问题得到:</p>
                </div>
                <div class="p1">
                    <p id="285">min <i>C</i> (<i>u</i><sub><i>k</i></sub>) =∫<mathml id="286"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mi>τ</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mi>τ</mi></mrow></msubsup></mrow></math></mathml>[<i>P</i><sub>idle</sub><i>u</i><sub><i>k</i></sub>+ (<i>P</i><sub>busy</sub>-</p>
                </div>
                <div class="p1">
                    <p id="287"><i>P</i><sub>idle</sub>) <i>u</i><sub><i>t</i></sub>]d<i>t</i>+<mathml id="288"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><mrow><msubsup><mo>∫</mo><mrow><mi>k</mi><mi>τ</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mi>τ</mi></mrow></msubsup><mi>f</mi></mrow></mstyle><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mfrac><mi>a</mi><mrow><mi>u</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>+</mo><mi>b</mi><mo stretchy="false">) </mo><mi>u</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mtext>d</mtext><mi>t</mi></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="289">上述模型完成了最小化能量消耗和最小化服务质量损失双目标的虚拟机聚合任务, 其中, <i>P</i><sub>idle</sub><i>u</i><sub><i>k</i></sub>表示分配的CPU资源为<i>u</i><sub><i>k</i></sub>时, 该虚拟机消耗的CPU静态功耗.算法求解过程中, 为了降低求解的时间复杂度, 文中首先使用装箱问题中的FFD算法估算应该处于开启状态的服务器节点数目;然后计算各个虚拟机之间的喜好程度;最后使用稳定匹配算法, 求得虚拟机在服务器节点上的部署和调度, 实现优化能耗与应用性能的目标.在真实数据集上的评测结果表明, 文中提出的算法可以大幅度地节省数据中心能耗, 与此同时亦能降低服务质量的性能损失, 此外, 文中的动态虚拟机聚合算法并不需要繁重的计算开销, 尤其适合于含有大量虚拟机的现代数据中心.类似的研究还有文献<citation id="646" type="reference">[<a class="sup">69</a>,<a class="sup">70</a>,<a class="sup">71</a>,<a class="sup">72</a>]</citation>.</p>
                </div>
                <div class="p1">
                    <p id="290">在优化数据中心能耗的同时, 另一个需要考虑的是如何处理聚合在物理服务器节点上的虚拟机之间的性能干扰问题.这种性能干扰主要是由于服务器节点上的虚拟机对共用资源的竞争而产生的, 常见的如最后一级共用寄存器、内存总线、数据和网络带宽等的竞争.虚拟机间性能干扰将会严重影响部署在虚拟机中运行的应用程序的性能.</p>
                </div>
                <div class="p1">
                    <p id="291">一般来说, 虚拟机间的性能干扰主要由于2个原因:1) 虚拟机间资源需求重合;2) 虚拟机间生命周期重合.如果把虚拟机间不同组合与虚拟机生命周期均考虑在虚拟机的节能部署内, 那么虚拟机的部署和调度问题将更加具有挑战性.这是因为:一方面, 聚合运行生命周期相同的虚拟机到同一物理服务器节点上能提高资源利用率, 从而降低边际成本;而另一方面, 由于性能干扰的作用, 降低虚拟机间生命周期的重合时间能降低应用性能的下降比率.也就是说, 能加快部署在虚拟机上的应用的完工时间.</p>
                </div>
                <div class="p1">
                    <p id="292">目前关于探索能量消耗与性能干扰之间量化权衡关系的研究还比较少, 文献<citation id="647" type="reference">[<a class="sup">73</a>]</citation>对能耗与性能干扰进行了统一的建模, 并且在虚拟机部署和调度中同时考虑了虚拟机间的组合以及不同虚拟机生命周期的重合, 以通过虚拟机部署和调度来降低数据中心能量消耗, 同时降低虚拟机之间的性能干扰.</p>
                </div>
                <div class="p1">
                    <p id="293">用户为提交给云数据中心的每个虚拟机请求<i>vm</i><sub><i>j</i></sub>都指定了1组参数<b><i>I</i></b><sub><i>j</i></sub>=[<i>a</i><sub><i>j</i></sub>, <i>p</i><sub><i>j</i></sub>, <b><i>R</i></b><sub><i>j</i></sub>], 其中<i>a</i><sub><i>j</i></sub>表示资源请求到达的时间, <i>p</i><sub><i>j</i></sub>为虚拟机独占1个服务器节点时的执行时间, 该值可以从事例信息管理器中得到.资源向量<b><i>R</i></b><sub><i>j</i></sub>=[<i>R</i><sub><i>jk</i></sub>] (<i>k</i>=1, 2, …, <i>s</i>) 表示虚拟机运行时所需要的计算资源情况.当每个虚拟机到达时, 需要对其进行部署和调度.把时间划分成离散的时间片, <i>t</i>=1, 2, …, <i>T</i>, 记<i>x</i><sub><i>ij</i></sub>为0或者1的指示变量, <i>x</i><sub><i>ij</i></sub> (<i>t</i>) =1表示在时间片<i>t</i>, 虚拟机<i>vm</i><sub><i>j</i></sub>部署在服务器<i>server</i><sub><i>i</i></sub>上面.定义<i>c</i><sub><i>i</i></sub> (<i>t</i>) 为服务器节点<i>server</i><sub><i>i</i></sub>在时间片<i>t</i>上消耗的能量, 因此有<i>c</i><sub><i>i</i></sub> (<i>t</i>) =<i>P</i> (<i>u</i> (<i>t</i>) ) <i>τ</i>.定义<i>Q</i> (<i>t</i>) 为时间片<i>t</i>上运行的虚拟机集合, <i>D</i> (<i>t</i>) 为在时间片<i>t</i>上刚好完成任务的虚拟机集合.因此, <i>vm</i><sub><i>j</i></sub> (<i>j</i>∈<i>D</i> (<i>t</i>) ) 实际执行时间为:<i>t</i><sub><i>j</i></sub>=<i>t</i>-<i>a</i><sub><i>j</i></sub>.定义虚拟机性能下降的惩罚函数为凸函数<i>f</i> (·) , 并且用<i>f</i> ( (<i>t</i><sub><i>j</i></sub>-<i>p</i><sub><i>j</i></sub>) /<i>p</i><sub><i>j</i></sub>) =<i>α</i><sup> (<i>t</i><sub><i>j</i></sub>-<i>p</i><sub><i>j</i></sub>) /<i>p</i><sub><i>j</i></sub></sup>-1 (<i>t</i><sub><i>j</i></sub>≥<i>p</i><sub><i>j</i></sub>) 作为一个适用模型对超出独占服务器节点运行时间<i>p</i><sub><i>j</i></sub>的惩罚.由上述定义可以将降低能量消耗与减少性能损失这两者之间均衡的虚拟机部署和调度问题形式化为优化问题:</p>
                </div>
                <div class="p1">
                    <p id="294">min<mathml id="295"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>c</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></munder><mi>f</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>p</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo stretchy="false">) </mo><msup><mrow></mrow><mo>+</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="296">s.t.<mathml id="297"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>Q</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml><i>x</i><sub><i>ij</i></sub> (<i>t</i>) <i>R</i><sub><i>jk</i></sub>≤<i>R</i><sub><i>ik</i></sub>, ∀<i>i</i>, ∀<i>j</i>, ∀<i>k</i>; (8) </p>
                </div>
                <div class="p1">
                    <p id="298"><mathml id="299"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>x</i><sub><i>ij</i></sub> (<i>t</i>) =1, ∀<i>j</i>, ∀<i>t</i>; (9) </p>
                </div>
                <div class="p1">
                    <p id="300"><i>x</i><sub><i>ij</i></sub> (<i>t</i>) ≤<i>x</i><sub><i>ij</i></sub> (<i>t</i>+1) , ∀<i>t</i>, <i>j</i>∉<i>D</i> (<i>t</i>) ; (10) </p>
                </div>
                <div class="p1">
                    <p id="301"><i>x</i><sub><i>ij</i></sub> (<i>t</i>) ∈{0, 1}, ∀<i>i</i>, ∀<i>j</i>, ∀<i>k</i>; (11) </p>
                </div>
                <div class="p1">
                    <p id="302">其中, 目标函数表示最小化数据中心能量消耗与性能损失代价之和, 调节参数<i>β</i>&gt;0表示能量消耗与性能损失代价之间的权重.式 (8) 约束了服务器节点上部署的虚拟机资源需求总量不超过服务器节点的资源容量;式 (9) 限制在同一时刻1个虚拟机只能在1个服务器上运行;式 (10) 限制1个虚拟机只能部署在1个服务器上.然后根据不同虚拟机间的组合 (即哪些虚拟机分配到同一个物理服务器节点上运行) 以及虚拟机间生命周期的重合提出和实现了离线与在线的部署和调度方法.实际运行结果表明:本文提出的方法能大幅度地改进能耗和性能下降两者的总和, 改进能量消耗和降低由于竞争产生的性能损失.关注数据中心虚拟机间干扰的节能研究还有文献<citation id="648" type="reference">[<a class="sup">74</a>,<a class="sup">75</a>,<a class="sup">76</a>,<a class="sup">77</a>]</citation>.</p>
                </div>
                <div class="p1">
                    <p id="303">本节主要总结了近年来针对数据中心服务器系统的能效优化研究, 包含模型的构建以及对应的能效方案.总而言之, 在现代数据中心内进行服务器能效优化是非常复杂的问题, 需要考虑到应用与服务器之间的选择关系、虚拟机的动态资源分配、虚拟机迁移造成的性能损失、虚拟机之间的干扰等多方面因素.</p>
                </div>
                <h4 class="anchor-tag" id="304" name="304"><b>4.2 网络设备节能</b></h4>
                <div class="p1">
                    <p id="305">网络设备的节能机制主要借鉴与服务器系统的节能研究, 网络设备的能耗函数可以刻画为</p>
                </div>
                <div class="p1">
                    <p id="306" class="code-formula">
                        <mathml id="306"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mi>x</mi><mo>=</mo><mn>0</mn><mo>;</mo></mtd></mtr><mtr><mtd><mi>σ</mi><mo>+</mo><mi>μ</mi><mi>x</mi><msup><mrow></mrow><mi>a</mi></msup><mo>, </mo><mspace width="0.25em" /><mi>x</mi><mo>&gt;</mo><mn>0</mn><mo>;</mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="307">其中, 函数<i>f</i> (·) 描述了链路传输速度<i>x</i>与该链路能耗之间的对应关系.<i>σ</i>, <i>μ</i>和<i>α</i>均为由链路性质决定的常数.通常情况下, 有<i>α</i>&gt;1.当<i>σ</i>=0时, 该函数为速率缩放机制的能耗函数;当<i>μ</i>=0时, 该函数为休眠机制的能耗函数;当<i>σ</i>&gt;0且<i>μ</i>&gt;0时, 为混合机制的能耗函数.</p>
                </div>
                <div class="p1">
                    <p id="308">速度缩放机制的基本原理是计算设备能够根据实时负载的变化动态连续地调整其运行速率.该机制最先被应用到计算机处理器的节能上.近年来, 速度缩放机制也被引入到了网络设备的节能中并且受到研究者的广泛关注, 目前研究的重点在于从网络全局的角度对网络能效进行优化.我们首先介绍网络流守恒约束的形式化表达.令<i>s</i>, <i>d</i>分别表示给定的网络流的源节点和目的节点, <i>T</i><sup><i>s</i>, <i>d</i></sup>表示该网络流的数据量, <i>f</i><mathml id="309"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>s</mi><mo>, </mo><mi>d</mi></mrow></msubsup></mrow></math></mathml>, <i>f</i><mathml id="310"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mi>i</mi></mrow><mrow><mi>s</mi><mo>, </mo><mi>d</mi></mrow></msubsup></mrow></math></mathml>分别表示由节点<i>i</i>发往相邻的节点<i>j</i>的数据量和由节点<i>j</i>发往相邻节点<i>i</i>的该网络流的数据量, </p>
                </div>
                <div class="p1">
                    <p id="311" class="code-formula">
                        <mathml id="311"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow></munder><mi>f</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>s</mi><mo>, </mo><mi>d</mi></mrow></msubsup><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>V</mi></mrow></munder><mi>f</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mi>i</mi></mrow><mrow><mi>s</mi><mo>, </mo><mi>d</mi></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo>-</mo><mi>Τ</mi><msup><mrow></mrow><mrow><mi>s</mi><mo>, </mo><mi>d</mi></mrow></msup><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mi>s</mi><mo>;</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><msup><mrow></mrow><mrow><mi>s</mi><mo>, </mo><mi>d</mi></mrow></msup><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mi>d</mi><mo>;</mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mi>j</mi><mo>≠</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo>.</mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="312">可以描述网络中的流量守恒约束.</p>
                </div>
                <div class="p1">
                    <p id="313">常用的基于速度缩放机制的能耗模型可以被形式化为:用无向图<i>G</i>= (<i>V</i>, <i>E</i>) 描述网络, 用<i>f</i> (<i>x</i><sub><i>e</i></sub>) 表示当链路传输速度为<i>x</i><sub><i>e</i></sub>时链路的能耗 (此处的能耗不仅仅包括链路本身的能耗, 还包括与链路链接的网络设备接口所消耗的能耗) .给定连续流请求<i>D</i>={<i>d</i><sub>1</sub>, <i>d</i><sub>2</sub>, …, <i>d</i><sub><i>k</i></sub>}, 即假设有连续的、数据量大小为<i>d</i><sub><i>i</i></sub>的请求<i>i</i>沿着单一路径从源点<i>s</i><sub><i>i</i></sub>路由至终点<i>t</i><sub><i>i</i></sub>.每条链路<i>e</i>都有容量值<i>c</i><sub><i>e</i></sub>.此能耗最小路由问题寻找满足请求<i>i</i>的路径, 使得<mathml id="314"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></munder><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>e</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>的值最小.因此, 能耗最小路由问题可以形式化为优化问题:</p>
                </div>
                <div class="p1">
                    <p id="315">min<mathml id="316"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></munder><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>e</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="317">s.t.<mathml id="318"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mi>e</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>y</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>e</mi></mrow></msub><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>,  ∀<i>e</i>; (12) </p>
                </div>
                <div class="p1">
                    <p id="319"><i>x</i><sub><i>e</i></sub>≤<i>c</i><sub><i>e</i></sub>, ∀<i>e</i>; (13) </p>
                </div>
                <div class="p1">
                    <p id="320"><i>y</i><sub><i>i</i>, <i>e</i></sub>={0, 1}, ∀<i>e</i>; (14) </p>
                </div>
                <div class="p1">
                    <p id="321"><i>y</i><sub><i>i</i>, <i>e</i></sub>: 流守恒; (15) </p>
                </div>
                <div class="p1">
                    <p id="322">其中:式 (12) 计算链路总流量;式 (13) 限制链路<i>e</i>的链路速度不可以超过容量限制<i>c</i><sub><i>e</i></sub>;式 (14) 二元变量<i>y</i><sub><i>i</i>, <i>e</i></sub>表示请求<i>i</i>是否通过边<i>e</i>;式 (15) 为流量守恒约束.该模型是个整数规划模型, 对于大多数能耗函数而言, 这是NP难问题, 通过对该问题进行适当的约束, 利用启发式算法可以得到不同形式能耗函数的近似解.</p>
                </div>
                <div class="p1">
                    <p id="323">现有的数据中心网络节能方案仅仅考虑如何最大限度地降低网络能耗, 并没有协同优化网络性能.然而, 网络能耗和性能是2个互相矛盾的优化目标, 降低网络能耗通常需要以牺牲网络性能为代价.无法保障网络性能在很大程度上局限了这些节能优化方案在实际系统中的部署和使用.通常网络的性能可以用吞吐量或网络延迟来描述.然而, 数据中心内各种网络流对性能的要求各不相同, 例如来自实时应用的网络流需要低延迟传输以提高服务效率, 而后台备份类应用的网络流一般对延迟没有太高的要求, 但需要较大的网络吞吐量.文献<citation id="649" type="reference">[<a class="sup">78</a>]</citation>对网络能耗与性能间的权衡问题进行了研究.文中指出, 不论来自何种应用的网络流, 最重要的性能指标可以概括为:网络流能否在规定的时间内完成传输.因此提出以网络流完成时间作为最重要的统一性能指标对网络的性能进行保障.</p>
                </div>
                <div class="p1">
                    <p id="324">数据中心内的每个应用程序建模成一系列具有完成时间限制的网路流的集合, 其中每条网络流包含了一定量的需要在一定时间内在网络上从某个给定源点路由到给定的目的地点的数据.对于时间区间[<i>T</i><sub>0</sub>, <i>T</i><sub>1</sub>], 给定网络流<i>J</i>={<i>j</i><sub>1</sub>, <i>j</i><sub>2</sub>, …, <i>j</i><sub><i>n</i></sub>}, <i>j</i><sub><i>i</i></sub>=&lt;<i>w</i><sub><i>i</i></sub>, (<i>r</i><sub><i>i</i></sub>, <i>d</i><sub><i>i</i></sub>) , (<i>p</i><sub><i>i</i></sub>, <i>q</i><sub><i>i</i></sub>) &gt;, 其中<i>w</i><sub><i>i</i></sub>为数据包的数据量, (<i>r</i><sub><i>i</i></sub>, <i>d</i><sub><i>i</i></sub>) 为网络流到达时间和完成时间, (<i>p</i><sub><i>i</i></sub>, <i>q</i><sub><i>i</i></sub>) 为网络流的源点和目的点.定义<i>S</i><sub><i>i</i></sub>=[<i>r</i><sub><i>i</i></sub>, <i>d</i><sub><i>i</i></sub>]为网络<i>j</i><sub><i>i</i></sub>的跨度, 定义每条网络流的密度为<i>D</i><sub><i>i</i></sub>=<i>w</i><sub><i>i</i></sub>/ (<i>d</i><sub><i>i</i></sub>-<i>r</i><sub><i>i</i></sub>) .因此调度被定义为集合:</p>
                </div>
                <div class="p1">
                    <p id="325"><i>S</i>={ (<i>s</i><sub><i>i</i></sub> (<i>t</i>) , <i>P</i><sub><i>i</i></sub>) |∀<i>j</i><sub><i>i</i></sub>∈<i>J</i>, ∀<i>t</i>∈[<i>r</i><sub><i>i</i></sub>, <i>d</i><sub><i>i</i></sub>]}, </p>
                </div>
                <div class="p1">
                    <p id="326">其中, <i>s</i><sub><i>i</i></sub> (<i>t</i>) 表示网路流<i>j</i><sub><i>i</i></sub>在时刻<i>t</i>的传输速率, <i>P</i><sub><i>i</i></sub>是为网络流<i>j</i><sub><i>i</i></sub>选择的路由路径上所有链路的集合.定义<i>J</i><sub><i>e</i></sub>∈<i>J</i>为链路<i>e</i>上路由的网络流集合, 则约束</p>
                </div>
                <div class="p1">
                    <p id="327" class="code-formula">
                        <mathml id="327"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><msup><mi>J</mi><mo>′</mo></msup></mrow></munder><mrow><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></mstyle><mo>-</mo><mo stretchy="false"> (</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>j</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><msup><mi>J</mi><mo>′</mo></msup></mrow></munder><mspace width="0.25em" /><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>j</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><msup><mi>J</mi><mo>′</mo></msup></mrow></munder><mspace width="0.25em" /><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>≤</mo><mn>0</mn><mo>, </mo><mspace width="0.25em" /><msup><mi>J</mi><mo>′</mo></msup><mo>⊆</mo><mi>J</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="328">限制了对于任意链路<i>e</i>, <i>J</i><sub><i>e</i></sub>的任意子集内的网络流都可以在该子集中所有网络的最迟完成期限前处理完毕.</p>
                </div>
                <div class="p1">
                    <p id="329">文中对2个决策过程从网络流的层面进行优化, 即网络流的调度和路由.研究如何合理地调度网络流并对网络流分配适当的路由路径, 以达到网络能耗的最优化, 同时满足所有网络流的完成时间限制.类似的考虑网络流截止时间的还包括文献<citation id="650" type="reference">[<a class="sup">79</a>,<a class="sup">80</a>,<a class="sup">81</a>]</citation>.</p>
                </div>
                <div class="p1">
                    <p id="330">如果对速率缩放机制中设备速率数值的选择做出限制, 就可以得到速率自适应机制, 该机制在实际应用中更加广泛.速率自适应机制的核心思想是通过实时调整网络设备的接口速率来达到节能的目的.与速度缩放机制不同的是, 速率自适应机制要求网络设备具备多个候选的运行速率, 在实际运行中网络设备根据自身负载从这些候选速率中选择合适的速率运行.由于较低的速率通常具有更低的功耗, 因此该技术可以在很大程度上消除网络冗余, 降低网络能耗.由定义可以很容易得出在该机制下的链路容量约束可以建模为</p>
                </div>
                <div class="p1">
                    <p id="331"><i>x</i><sub><i>e</i></sub>≤<i>z</i><sub><i>e</i></sub>, <i>z</i><sub><i>e</i></sub>∈{<i>R</i><sub>1</sub>, <i>R</i><sub>2</sub>, …, <i>R</i><sub><i>k</i></sub>}, ∀<i>e</i>, </p>
                </div>
                <div class="p1">
                    <p id="332">其中, <i>z</i><sub><i>e</i></sub>代表设备所选择的有效运行速率, <i>R</i><sub>1</sub>, <i>R</i><sub>2</sub>, …, <i>R</i><sub><i>k</i></sub>代表<i>k</i>种可供选择的运行速率.Gunaratn等人<citation id="651" type="reference"><link href="589" rel="bibliography" /><sup>[<a class="sup">82</a>]</sup></citation>最早研究这一类问题, 他们提出了一种链路速率调整机制, 该机制综合考虑了当前的链路负载、缓存队列的长度以及链路利用率等信息, 以确定是否调整链路速率.当链路的负载较低并且缓存队列长度和链路利用率均低于预设的阈值时, 网络链路将降低其传输速率;而当链路负载较大并且缓存队列长度和链路利用率高于预设的阈值时, 网络链路将提高其传输速率.当网络流量状况发生改变导致链路的速率需要调整时, 链路的一端会向对端发送请求, 并在该请求中携带发送端所期望的链路速率信息.当另一端接收到该请求时, 判断并确定是否接受该请求以进行速率调整.以上机制能够使链路速率根据负载动态变化, 因此可以有效地降低网络链路的能耗.然而, 频繁的速率调整会带来一定的额外能耗开销, 并且会因为调整期间的延迟对网络的稳定性造成不利影响.目前ALR技术已经被一些通用网络设备所支持, 例如 InfiniBand.</p>
                </div>
                <div class="p1">
                    <p id="333">设备休眠也被广泛应用在网络节能研究中.IEEE已经专门成立了针对以太网网络设备节能技术的讨论组EEE (Energy Efficient Ethernet Task Force) , 并且已经将以太网链路休眠技术标准化为 IEEE 802.3az<citation id="652" type="reference"><link href="591" rel="bibliography" /><sup>[<a class="sup">83</a>]</sup></citation>.基本的基于休眠机制节能模型的网络能耗优化问题可以按如下方式建模.假设给定网络<i>G</i>= (<i>V</i>, <i>E</i>) , 其中, <i>V</i>代表节点集合, <i>E</i>代表链路集合, 假设每条链路<i>e</i>都有容量限制值<i>C</i><sub><i>e</i></sub>, 链路均为双向链路, 而且彼此相互独立.在时刻<i>t</i>, 给定网络流集合, 定义网络流为一个序列的数据包, 这些数据包包含同样的头域信息 (例如源目的IP地址, 目的节点的端口) .为了避免数据包乱序重排, 假设每个数据流中的所有数据包都只走同一条路径.假设每条链接<i>e</i>在开启状态消耗的功率为<i>P</i><sub><i>e</i></sub>, 处于关闭状态的链接不产生能耗.定义<i>x</i><sub><i>e</i></sub> (<i>t</i>) ∈{0, 1}为链接<i>e</i>在时刻<i>t</i>的状态, <i>x</i><sub><i>e</i></sub> (<i>t</i>) =1代表链接<i>e</i>在时刻<i>t</i>处于开启状态, 反之表示处于关闭状态.定义<i>f</i><sub><i>e</i></sub> (<i>t</i>) 为所有源节点对经过链接<i>e</i>的总流量.给定时间限制[<i>t</i><sub>0</sub>, <i>t</i><sub>1</sub>], 在该时间段内的网络总能耗最小化的问题可以形式化为优化问题:</p>
                </div>
                <div class="p1">
                    <p id="334">min<mathml id="335"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><mrow><msubsup><mo>∫</mo><mrow><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msubsup><mrow></mrow></mrow></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml><i>x</i><sub><i>e</i></sub> (<i>t</i>) <i>P</i><sub><i>e</i></sub> d<i>t</i></p>
                </div>
                <div class="p1">
                    <p id="336">s.t.  0≤<i>f</i><sub><i>e</i></sub> (<i>t</i>) ≤<i>x</i><sub><i>e</i></sub> (<i>t</i>) <i>C</i><sub><i>e</i></sub>,  ∀<i>e</i>∈<i>E</i>; (16) </p>
                </div>
                <div class="p1">
                    <p id="337"><i>x</i><sub><i>e</i></sub> (<i>t</i>) ={0, 1}, ∀<i>e</i>∈<i>E</i>; (17) </p>
                </div>
                <div class="p1">
                    <p id="338"><i>f</i><sub><i>e</i></sub> (<i>t</i>) :  流守恒, ∀<i>e</i>∈<i>E</i>; (18) </p>
                </div>
                <div class="p1">
                    <p id="339">其中:式 (16) 限制网络中每条链接上的流量不能超过流量的容量限制;式 (17) 是二元变量限制;式 (18) 是流量守恒限制.对该问题的求解意味着为每个网络流分配合适的路径, 然后为不同链接选择合适的开启或者闭合状态, 使得在给定时间区间[<i>t</i><sub>0</sub>, <i>t</i><sub>1</sub>]内网络中总能耗最小.显而易见, 上述问题是个整数规划问题, 因此, 对上述问题的求解是NP难的, 目前的研究普遍寻求近似算法或者启发式算法进行求解<citation id="653" type="reference"><link href="493" rel="bibliography" /><link href="593" rel="bibliography" /><link href="595" rel="bibliography" /><link href="597" rel="bibliography" /><sup>[<a class="sup">34</a>,<a class="sup">84</a>,<a class="sup">85</a>,<a class="sup">86</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="340">传统网络模型中网络设备的主要功能是负责数据包的转发.然而, 为了丰富网络的功能, 越来越多的中间件被引入到网络中.这些中间件负责诸如防火墙、深度包检测或代理等各种附加网络功能.由于这些中间件一般为特殊硬件设施, 其部署和管理通常需要人工参与.随着网络功能的不断复杂化和网络规模的不断扩大, 在网络中正确高效地进行中间件的布置及协调已经成为网络管理的一大挑战.为了解决这个问题, 近年来研究人员提出了新的网络模型——网络即服务 (network-as-a-service, NaaS) .相比于传统模型, 该模型下的网络节点由通用服务器构建并基于虚拟化技术运行网络功能, 并且由1个基于软件定义网络的逻辑集中式的控制单元负责网络功能的部署与协调.在传统数据包转发网络中, 链路利用率是网络流聚合的首要标准.然而这在 NaaS 模型下不仅要考虑数据传输的拥塞, 还需要考虑其他资源例如处理单元或者内存等的过载问题.文献<citation id="654" type="reference">[<a class="sup">87</a>]</citation>研究如何在多重资源维度环境内进行网络的优化, 并以网络能耗优化问题为例进行详细的阐述.</p>
                </div>
                <div class="p1">
                    <p id="341">给定网络<i>G</i>= (<i>V</i>, <i>E</i>) , 其中<i>V</i>为顶点集合, 每个顶点代表1个用于数据包处理的通用服务器, <i>E</i>为网络中链路的集合.每个节点<i>v</i>∈<i>V</i>定义<i>K</i>种不同硬件资源, 例如CPU、内存、存储、网络带宽等.每种资源<i>k</i>∈{1, 2, …, <i>K</i>}定义1个最大容量<i>C</i><sub><i>v</i>, <i>k</i></sub>.由于软件数据包处理网络一般由通用服务器构建, 因此我们可以合理地假设所有的顶点同质, 故有<i>C</i><sub><i>v</i>, <i>k</i></sub>=<i>C</i><sub><i>k</i></sub> (∀<i>v</i>∈<i>V</i>, <i>k</i>∈{1, 2, …, <i>K</i>}) .给定<i>M</i>条网络流的集合<i>D</i>={<i>d</i><sub>1</sub>, <i>d</i><sub>2</sub>, …, <i>d</i><sub><i>M</i></sub>}, 每条网络流<i>d</i><sub><i>m</i></sub>定义1个三元组 (<i>v</i><mathml id="342"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>s</mi></msubsup></mrow></math></mathml>, <i>v</i><mathml id="343"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>t</mi></msubsup></mrow></math></mathml>, <b><i>R</i></b><sub><i>m</i></sub>) , 其中<i>v</i><mathml id="344"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>s</mi></msubsup></mrow></math></mathml>和<i>v</i><mathml id="345"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mi>t</mi></msubsup></mrow></math></mathml>分别表示网络流的源和目的地址, <b><i>R</i></b><sub><i>m</i></sub>为1个<i>K</i>维向量 (<i>r</i><sub><i>m</i>, 1</sub>, <i>r</i><sub><i>m</i>, 2</sub>, …, <i>r</i><sub><i>m</i>, <i>K</i></sub>) , 表示网络流<i>d</i><sub><i>m</i></sub>的数据包在通用服务器上被处理时所需要预留的资源.该资源通常是已知的.为了描述方便, 假设对所有<i>m</i>∈{1, 2, …, <i>M</i>}和所有<i>k</i>∈{1, 2, …, <i>K</i>}, <i>r</i><sub><i>m</i>, <i>k</i></sub>都以<i>C</i><sub><i>k</i></sub>为基准进行了归一化.基于上述定义, 可以通过要引入下列约束来形式化多重资源节能路由问题:</p>
                </div>
                <div class="p1">
                    <p id="346"><mathml id="347"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi mathvariant="bold-italic">R</mi></mstyle><msub><mrow></mrow><mi>m</mi></msub><mo>⋅</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>v</mi></mrow></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mi>∞</mi></msub><mo>≤</mo><mn>1</mn></mrow></math></mathml>,  <i>v</i>∈<i>V</i>.</p>
                </div>
                <div class="p1">
                    <p id="348">该约束限制了所有流经相同节点的网络流的总资源请求不超过该节点的总资源.当<i>K</i>=1时, 上述优化问题对应于一般的容量限制的网络设计问题, 该问题已经被广泛研究.</p>
                </div>
                <div class="p1">
                    <p id="349">上述多重资源节能路由问题被证明是不存在渐过性多项式时间近似方案, 除非<i>P</i>=<i>NP</i>.文献<citation id="655" type="reference">[<a class="sup">87</a>]</citation>中提供了一种基于迭代的路由方案, 该方案通过不断选择网路流来最大化占用已经开启的网络节点的剩余资源, 并且基于网络流的资源请求向量和网络节点的剩余资源向量的分布情况来为网络流选择合适的路由路径.通过对网络拓扑的规律性加以利用, 文中又提出了一种拓扑感知的启发式多重资源节能路由算法.该算法可以在提供相当水平的节能效果的同时显著地降低运行时间.</p>
                </div>
                <div class="p1">
                    <p id="350">针对数据中心网络的能耗或性能的优化通常基于流量工程方法, 通过聚合网络流来调整网络链路的负载.流量工程方法需要基于网络流量矩阵来设计, 而实时获取数据中心网络上的流量信息很难实现.因此, 该类方法通常需要分析网络流量在不同时间的重复性特征, 并根据历史流量数据来对网络流量进行预测.然而, 文献<citation id="656" type="reference">[<a class="sup">88</a>]</citation>指出数据中心网络的流量具有高突发性, 并且不具备细粒度特征信息, 很难基于历史流量信息进行精确预测.因此, 基于流量预测的网络流量工程优化方法在实际运行中的效果也就很难预知.</p>
                </div>
                <div class="p1">
                    <p id="351">不同于传统网络, 数据中心网络具有一些独特的特征可以被利用以辅助优化方案的设计.这些特征包括:1) 拓扑的规律性.相比于传统网络, 数据中心网络拓扑例如fat-tree, BCube, DCell等通常具有非常高的对称性与规律性;2) 虚拟机布置.得益于虚拟化技术, 在数据中心环境中, 网络流的端点可以通过虚拟机布置来进行确定;3) 上层应用特征.云计算数据中心内运行的大部分应用都是基于MapReduce框架构建, 这些应用会产生非常规律的通信模式.对这些通信模式信息加以利用, 将可以构造出更加有效的网络优化方案.</p>
                </div>
                <div class="p1">
                    <p id="352">文献<citation id="657" type="reference">[<a class="sup">88</a>]</citation>考虑数据中心内影响网络流量特征的4个关键因素:上层应用通信特征、虚拟机布置、网络拓扑和流量工程.结合这些因素, 文中提出一种针对数据中心网络能耗或性能优化的一般性框架.基于该框架, 利用时域相关的模型来对网络节能问题进行建模, 并且对该问题的求解复杂度进行了理论分析.首先对文中的问题进行描述:</p>
                </div>
                <div class="p1">
                    <p id="353">假设1组作业<i>J</i>需要在时间范围[<i>t</i><sub>1</sub>, <i>t</i><sub><i>r</i></sub>]内被执行, 每个作业<i>j</i>∈<i>J</i>由<i>n</i><sub><i>j</i></sub>个任务组成, 对于每个作业<i>j</i>, 其任务之间的流量矩阵为<b><i>T</i></b><sub><i>j</i></sub> (<i>t</i>) , 其中<i>t</i>为指定时间范围内的1个时间片.每个任务由1个虚拟机处理, 所有虚拟机的集合为<i>M</i>.假设所有的虚拟机一旦被部署后将不会再次被迁移, 此时网络中总能耗可以表示为</p>
                </div>
                <div class="p1">
                    <p id="354" class="code-formula">
                        <mathml id="354"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></munderover><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mo>∈</mo><mi>V</mi></mrow></munder><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow></mrow></mstyle><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="355">其中, <i>x</i><sub><i>v</i></sub> (<i>t</i>) 表示交换机<i>v</i>在时间片<i>t</i>内的流量负载.目标是将所有虚拟机分配到物理服务器上, 并且为所有的数据流选择合适的路径使得上述目标函数值最小.</p>
                </div>
                <div class="p1">
                    <p id="356">不难发现:上述问题可以分为2个相互依赖的过程:虚拟机的部署与流量工程.给定虚拟机的部署方案, 上述问题就转化为普通的网络能耗最小化问题.因此, 首先假设给定算法<i>A</i>来求解该路由优化问题, 在此基础上, 该节能问题转化为一个经典的虚拟机部署问题, 即在物理服务器资源约束的前提下最小化<mathml id="357"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>A</i> (<b><i>D</i></b> (<i>t</i>) ) .其中<b><i>D</i></b> (<i>t</i>) 为时间片<i>t</i>下虚拟机的特定部署所产生的流量矩阵.</p>
                </div>
                <div class="p1">
                    <p id="358">一旦<b><i>D</i></b> (<i>t</i>) 确定下来, 网络总能耗最小化的问题就退化成最基本的网络能耗优化问题.在此之前, 考虑到网络应用环境为数据中心网络, 假设该网络按照目前数据中心应用最广泛的FatTree来构建.因此, 对于每个交换机<i>v</i>∈<i>V</i>, 其承载的流量负载可以表示为</p>
                </div>
                <div class="p1">
                    <p id="359"><i>x</i><sub><i>v</i></sub>=<mathml id="360"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">{</mo><mi>e</mi><mo>∈</mo><mi>E</mi><mo>:</mo><mi>e</mi><mspace width="0.25em" /><mtext>i</mtext><mtext>s</mtext><mspace width="0.25em" /><mtext>i</mtext><mtext>n</mtext><mtext>c</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext><mspace width="0.25em" /><mtext>t</mtext><mtext>o</mtext><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">}</mo></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml><i>y</i><sub><i>e</i></sub>, </p>
                </div>
                <div class="p1">
                    <p id="361">其中, <i>x</i><sub><i>e</i></sub>为经过交换机<i>v</i>的总流量, <i>y</i><sub><i>e</i></sub>为链路承载的总流量.因此, 该网络能耗最小化问题可以形式化为整数规划问题.</p>
                </div>
                <div class="p1">
                    <p id="362">该问题的求解也分为2个阶段.首先, 基于文中提出的3个针对虚拟机布置的原则, 设计了有针对性的虚拟机布置方案, 在网络上生成了有利于流量工程优化的网络流量分布.然后, 分析了路由和节能之间的关系, 并设计了可以获得近似最优节能效果的网络路由优化方案.最后利用模拟实验对提出的方法的有效性进行了全面的验证, 并与其他常见的方法进行了比较.实验结果表明:文中提出的优化框架能够有效地提升网络的能效.</p>
                </div>
                <div class="p1">
                    <p id="363">本节主要总结了近年来针对数据中心网络系统的能效优化研究, 包含模型的构建以及对应的能效方案.现代数据中心已经成为各种新型网络技术的应用主场, 因此, 进行数据中心网络能效优化需要充分考虑到新型网络拓扑特性、新型网络模式特性、新型网络应用特征等各方面的因素.</p>
                </div>
                <h3 id="364" name="364" class="anchor-tag"><b>5 总结和展望</b></h3>
                <div class="p1">
                    <p id="365">大数据和云计算的蓬勃发展促进了数据中心在全球范围内的广泛部署, 与之同时带来的能耗问题引起了学术界和工业界的广泛关注.本文主要研究了基于资源和任务调度的数据中心节能机制和算法, 重点对数据中心服务器系统与网络系统2个能耗重点系统的能耗问题展开研究, 首先分析2个系统的能耗模型, 针对其共性与差异性分别总结对应的节能机制, 在此基础上结合目前相关领域最新的研究成果总结构建出2种系统的能耗模型, 最后对模型的求解进行分析总结.</p>
                </div>
                <div class="p1">
                    <p id="366">目前, 数据中心能耗问题依旧是阻碍数据中心发展的巨大障碍, 随着世界范围内能源问题的进一步恶化, 数据中心的巨大能耗问题将变得越来越突出.近几年学术界与工业界都在该领域进行了研究并取得了一定的成果, 然而, 还是存在很多问题和难题需要进一步的研究, 具体总结为5点:</p>
                </div>
                <div class="p1">
                    <p id="367">1) 缺乏针对数据中心多维资源的利用率、服务质量和能耗的综合考虑.当前大部分研究集中通过资源调度来降低能耗或提高服务质量等方面, 例如从节能架构的角度改造数据中心, 通过关闭低负载设备、将负载合并到少数物理机器上来提高资源利用率等等.但是, 在综合考虑数据中心多维资源 (计算、存储、带宽等) 的利用率、保证服务质量和降低能耗方面的理论和技术的研究较缺乏;如何综合考虑多维或多种资源的利用率和服务质量 (包含用户体验) 为系统建模是一个挑战性的科学问题.</p>
                </div>
                <div class="p1">
                    <p id="368">2) 缺乏针对多租户数据中心资源协同、成本和能耗优化的理论与算法研究.多租户数据中心具有租户可以集中于自身业务而不必考虑信息基础设施的管理和运作的优势.但现在多租户数据中心内的服务器由租户控制, 供应商难以干预, 而整个数据中心难以协同运作, 导致传统数据中心的提高资源利用率和节能策略无法直接移植到多租户数据中心.目前针对多租户数据中心的研究较少, 而针对多租户数据中心资源利用率、能效成本优化的理论与技术研究尤为缺乏.</p>
                </div>
                <div class="p1">
                    <p id="369">3) 缺乏跨域数据中心节点间资源调度和能效优化模型与算法的研究.处于不同地域的数据中心节点其运营成本不同、与客户的物理距离不一样, 导致同一用户的请求在不同的节点得到不同的服务质量;同时, 不同节点处理同一个请求将花费不同的运营成本.因此, 跨域数据中心面临的最基本的问题是如何在多个不同地域分布的节点之间合理进行资源/任务的调度.目前已知该方面的研究缺乏数据中心节点间资源/任务调度、能效优化模型方面的研究, 难以对提出的优化策略进行理论方面的分析与验证, 而且此类方法难以从跨不同域的整体范围上得出优化的解决方法.</p>
                </div>
                <div class="p1">
                    <p id="370">4) 缺乏跨域数据中心节点间新能源应用于负载调度的联合优化研究.目前针对跨域数据中心节点间的多路能源的互补优化、兼顾新能源应用与负载调度、统一权衡节能与服务延迟等关键问题, 缺乏模型和算法方面的基础理论和技术;另外该方面的研究大多集中在解决依赖传统能源的数据中心节能减排问题上, 而缺乏对于多种新能源统一设计协调和调度的理论技术, 例如尚未考虑到燃料电池这种有着截然不同的供电与成本特性的新兴能源在数据中心环境下的应用.</p>
                </div>
                <div class="p1">
                    <p id="371">5) 缺乏高效能的数据中心内/间全网互联与传输机制.目前传统TCP协议在当前数据中心应用中会造成性能瓶颈;云计算服务对网络传输的使用特征使得传统TCP 协议在低延迟高吞吐的数据中心网络中存在严重的性能问题, 如TCP incast、网络流完成时间的长尾效应等.基于网络即服务模型下, 如何对数据中心网络资源进行管理和优化 (包括传输协议、数据的调度及路由等) 以达到提高资源利用率和降低能耗的目的, 是一个亟待解决的重要问题.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="427">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Power,pollution and the Internet">

                                <b>[1]</b>Glanz J.Power, Pollution and the Internet[N/OL].New York:The New York Times, 2012-09-22 (2012-09-23) [2018-05-20].https:www.nytimes.com/2012/09/23/technology/datacenters-waste-vast-amounts-of-energy-belying-industry-image.html
                            </a>
                        </p>
                        <p id="429">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=United states data center energy usage report">

                                <b>[2]</b>Shehabi A, Smith S, Sartor D, et al.United states data center energy usage report, LBNL-1005775[R].Berkeley:Lawrence Berkeley National Laboratory, 2016
                            </a>
                        </p>
                        <p id="431">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Growth in data center electricity use 2005to 2010[R/OL]">

                                <b>[3]</b>Koomey J.Growth in data center electricity use 2005to 2010[R/OL].New York:Analytical Press, (2011-08-01) [2018-05-20].https:alejandrobarros.com/wp-content/uploads/old/Growth_in_Data_Center_Electricity_use_2005_to_2010.pdf
                            </a>
                        </p>
                        <p id="433">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A framework for data center energy productivity,Rev 2008-0">

                                <b>[4]</b>Anderson D, Morris P, Cader T, et al.A framework for data center energy productivity, Rev 2008-0[R].Oregon:The Green Grid, 2008
                            </a>
                        </p>
                        <p id="435">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data center infrastructure market will be worth$152 billion by 2016[OL]">

                                <b>[5]</b>Canalys.Data center infrastructure market will be worth$152 billion by 2016[OL]. (2012-07-11) [2018-05-20].https:www.canalys.com/newsroom/data-center-infrastructuremarket-will-be-worth-152-billion-2016
                            </a>
                        </p>
                        <p id="437">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300032411&amp;v=MTE0MDN3WmVadUh5am1VTHJJSTFzVWJoQT1OaWZPZmJLOEh0TE9ySTlGWk9nTkNIMDRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Van Heddeghem W, Lambert S, Lannoo B, et al.Trends in worldwide ICT electricity consumption from 2007to 2012[J].Computer Communications, 2014, 50:64-76
                            </a>
                        </p>
                        <p id="439">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Power management techniques for data centers:Asurvey[OL]">

                                <b>[7]</b>Mittal S.Power management techniques for data centers:Asurvey[OL]. (2014-05-07) [2018-05-20].https:arxiv.org/abs/1404.6681
                            </a>
                        </p>
                        <p id="441">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Datacenter as a Computer:An Introduction to the Design of Warehouse-scale Machines">

                                <b>[8]</b>Barroso L A, Clidaras J, H9lzle U.The Datacenter as a Computer:An Introduction to the Design of Warehouse-scale Machines[M]San Rafael:Morgan&amp;Claypool Publishers, 2013
                            </a>
                        </p>
                        <p id="443">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding and abstracting total data center power[C/OL]">

                                <b>[9]</b>Pelley S, Meisner D, Wenisch T F, et al.Understanding and abstracting total data center power[C/OL]Proc of WEED.2009[2018-05-20].http:seelab.ucsd.edu/mobile/related_papers/22_weed09.pdf
                            </a>
                        </p>
                        <p id="445">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Green grid data center power efficiency metrics:PUE and DCIE,Rev 2007-0">

                                <b>[10]</b>Belady C, Rawson A, Pfleuger J, et al.Green grid data center power efficiency metrics:PUE and DCIE, Rev 2007-0[R].Oregon:The Green Grid, 2007
                            </a>
                        </p>
                        <p id="447">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Selfbenchmarking guide for data centers:Metrics,benchmarks,actions[OL]">

                                <b>[11]</b>Mathew P, Ganguly S, Greenberg S, et al.Selfbenchmarking guide for data centers:Metrics, benchmarks, actions[OL].Berkeley:The Lawrence Berkeley National Laboratory, (2010-07-13) [2018-05-20].https:eetd.lbl.gov/sites/all/files/publications/lbnl-3393e.pdf
                            </a>
                        </p>
                        <p id="449">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An energy complexity model for algorithms">

                                <b>[12]</b>Roy S, Rudra A, Verma A.An energy complexity model for algorithms[C]Proc of the 4th Conf on Innovations in Theoretical Computer Science.New York:ACM, 2013:283-304
                            </a>
                        </p>
                        <p id="451">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000104672&amp;v=MjM1NzdlWnVIeWptVUxySUkxc1ViaEE9TmlmSVk3SzdIdGpOcjQ5Rlplc0xDbnM3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Tudor B M, Teo Y M.On understanding the energy consumption of arm-based multicore servers[C]Proc of ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2013:267-278
                            </a>
                        </p>
                        <p id="453">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling and evaluating energy-performance efficiency of parallel processing on multicore based power aware systems">

                                <b>[14]</b>Ge Rong, Feng Xizhou, Cameron K W.Modeling and evaluating energy-performance efficiency of parallel processing on multicore based power aware systems[C]Proc of IEEE Int Parallel&amp;Distributed Processing Symp.Piscataway, NJ:IEEE, 2009:1-8
                            </a>
                        </p>
                        <p id="455">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unified performance and power modeling of scientific workloads">

                                <b>[15]</b>Song S L, Barker K, Kerbyson D.Unified performance and power modeling of scientific workloads[C]Proc of the 1st Int Workshop on Energy Efficient Supercomputing.New York:ACM, 2013:No.4
                            </a>
                        </p>
                        <p id="457">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chaotic attractor prediction for server run-time energy consumption">

                                <b>[16]</b>Lewis A, Simon J, Tzeng N F.Chaotic attractor prediction for server run-time energy consumption[C]Proc of the Int Conf on Power Aware Computing and Systems.Berkeley, CA:USENIX Association, 2010:1-16
                            </a>
                        </p>
                        <p id="459">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007305&amp;v=MTg0MTZVTHJJSTFzVWJoQT1OaWZJWTdLN0h0ak5yNDlGWk9zSUQzdzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Lewis A W, Tzeng N F, Ghosh S.Runtime energy consumption estimation for server workloads based on chaotic time-series approximation[J]ACM Transactions on Architecture and Code Optimization, 2012, 9 (3) :15:1-15:26
                            </a>
                        </p>
                        <p id="461">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Power-conservative Server Consolidation Based Resource Management in Cloud">

                                <b>[18]</b>Perumal V, Subbiah S.Power-conservative server consolidation based resource management in cloud[J].International Journal of Network Management, 2014, 24 (6) :415-432
                            </a>
                        </p>
                        <p id="463">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Challenge:Resolving data center power bill disputes:The energyperformance trade-offs of consolidation">

                                <b>[19]</b>Chatzipapas A, Pediaditakis D, Rotsos C, et al.Challenge:Resolving data center power bill disputes:The energyperformance trade-offs of consolidation[C]Proc of the 6th ACM Int Conf on Future Energy Systems.New York:ACM, 2015:89-94
                            </a>
                        </p>
                        <p id="465">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Virtual machine power metering and provisioning">

                                <b>[20]</b>Kansal A, Zhao Feng, Liu Jie, et al.Virtual machine power metering and provisioning[C]Proc of the 1st ACM Symp on Cloud Computing.New York:ACM, 2010:39-50
                            </a>
                        </p>
                        <p id="467">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000057009&amp;v=MzAxMDZadUh5am1VTHJJSTFzVWJoQT1OaWZJWTdLN0h0ak5yNDlGWk80SURId3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Chen Yiyu, Amitayu D, Qin Wubi, et al.Managing server energy and operational costs in hosting centers[C]Proc of the 26th ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2005:303-314
                            </a>
                        </p>
                        <p id="469">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-efficient server clusters">

                                <b>[22]</b>Elnozahy E M, Kistler M, Rajamony R.Energy-efficient server clusters[C]Proc of the 2nd Int Workshop on PowerAware Computer Systems.Berlin:Springer, 2002:179-197
                            </a>
                        </p>
                        <p id="471">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance Constrained DistributedDVS Scheduling for Scientific Applications on Poweraware Clusters">

                                <b>[23]</b>Ge Rong, Feng Xizhou, Kirk W Cameron.Performanceconstrained distributed DVS scheduling for scientific applications on power-aware clusters[C]Proc of the 18th ACM/IEEE Conf on Supercomputing.Piscataway, NJ:IEEE, 2005:34-45
                            </a>
                        </p>
                        <p id="473">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Peeling the Power Onion of Data Centers">

                                <b>[24]</b>Yeo S, Lee H.Peeling the Power Onion of Data Centers[M].Berlin:Springer, 2012:137-168
                            </a>
                        </p>
                        <p id="475">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Scheduling Model for Reduced CPU Energy">

                                <b>[25]</b>Yao F, Demers A, Shenker S.A scheduling model for reduced CPU energy[C]Proc of the 36th Annual Symp on Foundations of Computer Science.Piscataway, NJ:IEEE, 1995:374-382
                            </a>
                        </p>
                        <p id="477">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000031340&amp;v=MTQwNzhmSVk3SzdIdGpOcjQ5RlpPZ09EM2c1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSTFzVWJoQT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>Albers S.Energy-efficient algorithms[J].Communications of the ACM, 2010, 53 (5) :86-96
                            </a>
                        </p>
                        <p id="479">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Full-system power analysis and modeling for server environments">

                                <b>[27]</b>Economou D, Rivoire S, Kozyrakis C, et al.Full-system power analysis and modeling for server environments[C]Proc of the 27th Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2006:1-8
                            </a>
                        </p>
                        <p id="481">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300442579&amp;v=MzExOTJaZVp1SHlqbVVMcklJMXNVYmhBPU5pZk9mYks3SHRET3JJOUZZTzhOQ1hzd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b>Beloglazov A, Abawajy J, Buyya R.Energy-aware resource allocation heuristics for efficient management of data centers for cloud computing[J]Future Generation Computer Systems, 2012, 28 (5) :755-768
                            </a>
                        </p>
                        <p id="483">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Run-time energy consumption estimation based on workload in server systems&amp;quot;">

                                <b>[29]</b>Lewis A W, Ghosh S, Tzeng N F.Run-time energy consumption estimation based on workload in server systems[C]Proc of HotPower.Berkeley, CA:USENIX Association, 2008:17-21
                            </a>
                        </p>
                        <p id="485">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD121106000609&amp;v=MTA2NjduS3JpZlp1OXVGQ3ZnVTd6TklGb1dOajdCYXJLNkg5RE1xWTlGWk8wUEJSTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b>Kliazovich D, Bouvry P, Khan S U.GreenCloud:A packetlevel simulator of energy-aware cloud computing data centers[J].The Journal of Supercomputing, 2012, 62 (3) :1263-1283
                            </a>
                        </p>
                        <p id="487">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080100029604&amp;v=MTg2MjBVTHJJSTFzVWJoQT1OaWZPZmJLN0h0bk1ybzlGWk9rR0NudzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b>Gao Yongqiang, Guan Haibing, Qi Zhengwei, et al.Quality of service aware power management for virtualized data centers[J].Journal of Systems Architecture, 2013, 59 (4) :245-259
                            </a>
                        </p>
                        <p id="489">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000044731&amp;v=MjI3NDhyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYmhBPU5pZklZN0s3SHRqTnI0OUZaTzhMQzM4NG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b>Fan Xiaobo, Weber W, Barroso L.Power provisioning for a warehouse-sized computer[J].ACM SIGARCH Computer Architecture News, 2007, 35 (2) :13-23
                            </a>
                        </p>
                        <p id="491">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data Center Energy Consumption Modeling:A Survey">

                                <b>[33]</b>Dayarathna M, Wen Yonggang, Fan Rui.Data center energy consumption modeling:A survey[J].IEEE Communications Surveys&amp;Tutorials, 2016, 18 (1) :732-794
                            </a>
                        </p>
                        <p id="493">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ElasticTree:Saving energy in data center networks">

                                <b>[34]</b>Heller B, Seetharaman S, Mahadevan P, et al.ElasticTree:Saving energy in data center networks[C]Proc of Symp on Networked Systems Design and Implementation.Berkeley, CA:USENIX Association, 2010:249-264
                            </a>
                        </p>
                        <p id="495">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Small versus large:Switch sizing in topology design of energy-efficient data centers">

                                <b>[35]</b>Widjaja I, Walid A, Luo Yanbin, et al.Small versus large:Switch sizing in topology design of energy-efficient data centers[C]Proc of the 21st IEEE/ACM Int Symp on Quality of Service (IWQoS) .Piscataway, NJ:IEEE, 2013:51-56
                            </a>
                        </p>
                        <p id="497">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hero:Hierarchical energy optimization for data center networks">

                                <b>[36]</b>Zhang Yan, Ansari N.Hero:Hierarchical energy optimization for data center networks[C]Proc of IEEE Int Conf on Communications.Piscataway, NJ:IEEE, 2012:2924-2928
                            </a>
                        </p>
                        <p id="499">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint hostnetwork optimization for energy-efficient data center networking">

                                <b>[37]</b>Jin Hao, Cheocherngngarn T, Levy D, et al.Joint hostnetwork optimization for energy-efficient data center networking[C]Proc of the 27th IEEE Int Symp on Parallel&amp;Distributed Processing.Piscataway, NJ:IEEE, 2013:623-634
                            </a>
                        </p>
                        <p id="501">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Software Defined Green Data Center Network with Exclusive Routing">

                                <b>[38]</b>Li Dan, Shang Yunfei, Chen Congjie.Software defined green data center network with exclusive routing[C]Proc of IEEE Conf on Computer Communications.Piscataway, NJ:IEEE, 2014:1743-1751
                            </a>
                        </p>
                        <p id="503">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discrete and continuous min-energy schedules for variable voltage processors">

                                <b>[39]</b>Li Minming, Andrew C, et al.Discrete and continuous minenergy schedules for variable voltage processors[J].National Academy of Sciences, 2006, 103 (11) :3983-3987
                            </a>
                        </p>
                        <p id="505">
                            <a id="bibliography_40" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000255596&amp;v=MTM3NDZkdEZDM2xVTHJJSkZ3PU5qN0Jhck80SHRITXJZcEFZZUlKWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[40]</b>Pruhs K, Van Stee R, Uthaisombut P.Speed scaling of tasks with precedence constraints[J].Theory of Computing Systems, 2008, 43 (1) :67-80
                            </a>
                        </p>
                        <p id="507">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Power-aware scheduling for makespan and flow">

                                <b>[41]</b>Bunde D P.Power-aware scheduling for makespan and flow[C]Proc of the 8th Annual ACM Symp on Parallelism in Algorithms and Architectures.New York:ACM, 2006:190-196
                            </a>
                        </p>
                        <p id="509">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Throughput maximization in the speed-scaling setting">

                                <b>[42]</b>Angel E, Bampis E, Chau V.Throughput maximization in the speed-scaling setting[C]Proc of the 31st Int Symp on Theoretical Aspects of Computer Science.Berlin:Springer, 2014:53-62
                            </a>
                        </p>
                        <p id="511">
                            <a id="bibliography_43" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000020238&amp;v=MTQzMzVtVUxySUkxc1ViaEE9TmlmSVk3SzdIdGpOcjQ5RlpPa1BEbjh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[43]</b>Bansal N, Kimbrel T, Pruhs K.Speed scaling to manage energy and temperature[J].Journal of the ACM, 2007, 54 (1) :3:1-3:39
                            </a>
                        </p>
                        <p id="513">
                            <a id="bibliography_44" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300061233&amp;v=MjU5NDRyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYmhBPU5qN0Jhcks4SHRMTXJJOUZaTzBPRG44Nm9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[44]</b>Albers S, Müller F, Schmelzer S.Speed scaling on parallel processors[J].Algorithmica, 2014, 68 (2) :404-425
                            </a>
                        </p>
                        <p id="515">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The bell is ringing in speedscaled multiprocessor scheduling">

                                <b>[45]</b>Greiner G, Nonner T, Souza A.The bell is ringing in speedscaled multiprocessor scheduling[C]Proc of the 21st Annual Symp on Parallelism in Algorithms and Architectures.New York:ACM, 2009:11-18
                            </a>
                        </p>
                        <p id="517">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Peak efficiency aware scheduling for highly energy proportional servers">

                                <b>[46]</b>Wong D.Peak efficiency aware scheduling for highly energy proportional servers[C]Proc of the 43rd ACM/IEEEAnnual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:481-492
                            </a>
                        </p>
                        <p id="519">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy proportional servers:Where are we in 2016/">

                                <b>[47]</b>Jiang Congfeng, Wang Yumei, Ou Dongyang, et al.Energy proportional servers:Where are we in 2016/[C]Proc of the 37th IEEE Int Conf on Distributed Computing Systems.Piscataway, NJ:IEEE, 2017:1649-1660
                            </a>
                        </p>
                        <p id="521">
                            <a id="bibliography_48" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13070800006013&amp;v=MDExNTBzVWJoQT1OajdCYXJLN0h0Yk1wNDlGWk9zSkRIMDZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[48]</b>Antoniadis A, Huang C.Non-preemptive speed scaling[J].Journal of Scheduling, 2013, 16 (4) :385-394
                            </a>
                        </p>
                        <p id="523">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From preemptive to non-preemptive speed-scaling scheduling">

                                <b>[49]</b>Bampis E, Kononov A, Letsios D, et al.From preemptive to non-preemptive speed-scaling scheduling[C]Proc of the19th Int Computing and Combinatorics Conf.Berlin:Springer, 2013:134-146
                            </a>
                        </p>
                        <p id="525">
                            <a id="bibliography_50" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501881825&amp;v=MDk2NDdJSTFzVWJoQT1OaWZPZmJLN0h0RE5xbzlFYk9NT0JINDhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[50]</b>Leung J, Chung L.Scheduling with processing set restrictions:A survey[J].International Journal of Production Economics, 2008, 116 (2) :251-262
                            </a>
                        </p>
                        <p id="527">
                            <a id="bibliography_51" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMC8FD3F9EFE1A67796C5C54BAEBCD0718&amp;v=MTA2NjFOMWh4NzIveEtvPU5pZklZOEN3YUtYUDJZWXdFcDRPZlhvK3lCOFZtVG9PVFh1UTNXZEhDc2FVUXJ1WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[51]</b>Qiang Wang, Chu Xiaowen.GPGPU power estimation with core and memory frequency scaling[J].ACM SIGMETRICSPerformance Evaluation Review, 2017, 45 (2) :73-78
                            </a>
                        </p>
                        <p id="529">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy efficient real-time task scheduling on CPU-GPU hybrid clusters">

                                <b>[52]</b>Mei Xinxin, Chu Xiaowen, Liu Hai, et al.Energy efficient real-time task scheduling on CPU-GPU hybrid clusters[C]Proc of IEEE Conf on Computer Communications.Piscataway, NJ:IEEE, 2017:1-9
                            </a>
                        </p>
                        <p id="531">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy efficient job scheduling with DVFS for CPU-GPU heterogeneous systems">

                                <b>[53]</b>Chau Vincent, Chu Xiaowen, Liu Hai, et al.Energy efficient job scheduling with DVFS for CPU-GPU heterogeneous systems[C]Proc of the 8th Int Conf on Future Energy Systems.New York:ACM, 2017:1-11
                            </a>
                        </p>
                        <p id="533">
                            <a id="bibliography_54" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy aware consolidation for cloud computing">

                                <b>[54]</b>Srikantaiah S, Kansal A, Zhao Feng.Energy aware consolidation for cloud computing[C]Proc of the Conf on Power Aware Computing and Systems.Berkeley, CA:USENIX Association, 2008:10-15
                            </a>
                        </p>
                        <p id="535">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scheduling heterogeneous processors isn&amp;#39;&amp;#39;t as easy as you think">

                                <b>[55]</b>Gupta A, Im S, Krishnaswamy R, et al.Scheduling heterogeneous processors isn't as easy as you think[C]Proc of the 23rd Annual ACM-SIAM Symp on Discrete Algorithms.Philadelphia:SIAM, 2012:1242-1253
                            </a>
                        </p>
                        <p id="537">
                            <a id="bibliography_56" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000011389&amp;v=MzE5NTdoQT1OaWZJWTdLN0h0ak5yNDlGWk9vT0QzUXdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[56]</b>Wang Weina, Zhu Kai, Ying Lei, et al.A throughput optimal algorithm for map task scheduling in mapreduce with data locality[J].ACM SIGMETRICS Performance Evaluation Review, 2013, 40 (4) :33-42
                            </a>
                        </p>
                        <p id="539">
                            <a id="bibliography_57" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-Efficient scheduling with time and processors eligibility restrictions">

                                <b>[57]</b>Jin Xibo, Zhang Fa, Song Ying, et al.Energy-Efficient scheduling with time and processors eligibility restrictions[C]Proc of the 19th European Conf on Parallel Processing.Berlin:Springer, 2013:66-77
                            </a>
                        </p>
                        <p id="541">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Minimizing energy consumption for frame-based tasks on heterogeneous multiprocessor platforms">

                                <b>[58]</b>Li Dawei, Wu Jie.Minimizing energy consumption for framebased tasks on heterogeneous multiprocessor platforms[J].IEEE Transaction on Parallel and Distributed Systems, 2015, 26 (3) :810-823
                            </a>
                        </p>
                        <p id="543">
                            <a id="bibliography_59" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EPPMiner:An extended benchmark suite for energy,power and performance characterization of heterogeneous architecture">

                                <b>[59]</b>Wang Qiang, Xu Pengfei, Zhang Yatao, et al.EPPMiner:An extended benchmark suite for energy, power and performance characterization of heterogeneous architecture[C]Proc of the 8th Int Conf on Future Energy Systems.New York:ACM, 2017:23-33
                            </a>
                        </p>
                        <p id="545">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic placement of virtual machines for managing SLA violations">

                                <b>[60]</b>Bobroff N, Kochut A, Beaty K.Dynamic placement of virtual machines for managing sla violations[C]Proc of the10th IFIP/IEEE Int Symp on Integrated Network Management.Piscataway, NJ:IEEE, 2007:119-128
                            </a>
                        </p>
                        <p id="547">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Power and Performance Management of Virtualized Computing Environments Via Lookahead Control">

                                <b>[61]</b>Kusic D, Kephart J O, Hanson J E, et al.Power and performance management of virtualized computing environments via lookahead control[C]Proc of the 5th Int Conf on Autonomic Computing.Piscataway, NJ:IEEE, 2008:3-12
                            </a>
                        </p>
                        <p id="549">
                            <a id="bibliography_62" >
                                    <b>[62]</b>
                                Van H, Tran F, Menaud J M.SLA-aware virtual resource management for cloud infrastructures[C]Proc of the 9th IEEE Int Conf on Computer and Information Technology.Piscataway, NJ:IEEE, 2009:357-362
                            </a>
                        </p>
                        <p id="551">
                            <a id="bibliography_63" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300561145&amp;v=MjU4NTJySTlGWWUwT0RYZzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNVYmhBPU5pZk9mYks3SHRETg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[63]</b>Gmach D, Rolia J, Cherkasova L et al.Resource pool management:Reactive versus proactive or let’s be friends[J].Computer Networks, 2009, 53 (17) :2905-2922
                            </a>
                        </p>
                        <p id="553">
                            <a id="bibliography_64" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Entropy:a consolidation manager for clusters">

                                <b>[64]</b>Hermenier F, Lorca X, Menaud J M, et al.Entropy:Aconsolidation manager for clusters[C]Proc of the ACMSIGPLAN/SIGOPS Int Conf on Virtual Execution Environments.New York:ACM, 2009:41-50
                            </a>
                        </p>
                        <p id="555">
                            <a id="bibliography_65" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD15121501504362&amp;v=MTkwOTBadUh5am1VTHJJSTFzVWJoQT1OaWZjYXJLOUg5UE5xbzlFWWVzTEQzbzdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[65]</b>Beloglazov A, Buyya R.Optimal online deterministic algorithms and adaptive heuristics for energy and performance efficient dynamic consolidation of virtual machines in cloud data centers[J].Concurrency and Computation:Practice and Experience, 2012, 24 (13) :1397-1420
                            </a>
                        </p>
                        <p id="557">
                            <a id="bibliography_66" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000104654&amp;v=MTM4NTV1SHlqbVVMcklJMXNVYmhBPU5pZklZN0s3SHRqTnI0OUZaZXNMQ25rOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[66]</b>Stolyar A L, Zhong Yuan.A large-scale service system with packing constraints:Minimizing the number of occupied servers[C]Proc of ACM SIGMETRICS Performance Evaluation Review.New York:ACM, 2013:41-52
                            </a>
                        </p>
                        <p id="559">
                            <a id="bibliography_67" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Risk management for virtual machines consolidation in data centers">

                                <b>[67]</b>Jin Xibo, Zhang Fa, Hu Songlin, et al.Risk management for virtual machines consolidation in data centers[C]Proc of IEEE Global Communications Conf.Piscataway, NJ:IEEE, 2013:2872-2878
                            </a>
                        </p>
                        <p id="561">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLA Aware Virtual Resource Management for Cloud Infrastructures">

                                <b>[68]</b>Van H, Tran F, Menaud J.Sla-aware virtual resource management for cloud infrastructures[C]Proc of the 9th IEEE Int Conf on Computer and Information Technology.Piscataway, NJ:IEEE, 2009:357-362
                            </a>
                        </p>
                        <p id="563">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A framework for data center site risk metric">

                                <b>[69]</b>Levy M, Raviv D.A framework for data center site risk metric[C]Proc of Ubiquitous Computing, Electronics and Mobile Communication Conf.Piscataway, NJ:IEEE, 2017:9-15
                            </a>
                        </p>
                        <p id="565">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Assessment on VMplacement and VM selection strategies">

                                <b>[70]</b>Chauhan N, Rakesh N, Matam R.Assessment on VMplacement and VM selection strategies[C]Proc of Nature Inspired Computing.Berlin:Springer.2018:157-163
                            </a>
                        </p>
                        <p id="567">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Truthful strategy and resource integration for multi-tenant data center demand response">

                                <b>[71]</b>Wang Youshi, Zhang Fa, Liu Zhiyong.Truthful strategy and resource integration for multi-tenant data center demand response[C]Proc of the 5th Int Workshop on Frontiers in Algorithmics.Berlin:Springer, 2015:259-270
                            </a>
                        </p>
                        <p id="569">
                            <a id="bibliography_72" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online assignment and placement of cloud task requests with heterogeneous requirements">

                                <b>[72]</b>Dabbagh M, Hamdaoui B, Guizani M, et al.Online assignment and placement of cloud task requests with heterogeneous requirements[C]Proc of Global Communications Conf.Piscataway, NJ:IEEE, 2015:1-6
                            </a>
                        </p>
                        <p id="571">
                            <a id="bibliography_73" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Optimization of Operational Cost and Performance Interference in Cloud Data Centers">

                                <b>[73]</b>Jin Xibo, Zhang Fa, Wang Lin, et al.Joint optimization of operational cost and performance interference in cloud data centers[J].IEEE Transaction on Cloud Computing, 2017, 5 (4) :697-711
                            </a>
                        </p>
                        <p id="573">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TRACON:Interference-aware scheduling for data-intensive applications in virtualized environments">

                                <b>[74]</b>Chiang R, Huang H.TRACON:Interference-aware scheduling for data-intensive applications in virtualized environments[C]Proc of Int Conf for High Performance Computing, Networking, Storage and Analysis.New York:ACM, 2011:47:1-47:12
                            </a>
                        </p>
                        <p id="575">
                            <a id="bibliography_75" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved energyefficiency in cloud datacenters with interference-aware virtual machine placement">

                                <b>[75]</b>Moreno I S, Yang Renyu, Xu Jie, et al.Improved energyefficiency in cloud datacenters with interference-aware virtual machine placement[C]Proc of the 7th IEEE Int Symp on Autonomous Decentralized Systems.Piscataway, NJ:IEEE, 2013:1-8
                            </a>
                        </p>
                        <p id="577">
                            <a id="bibliography_76" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Virtual machine power modelling in multi-tenant ecosystems:Challenges and pitfalls">

                                <b>[76]</b>Sharifi L, Freitag F, Veiga L.Virtual machine power modelling in multi-tenant ecosystems:Challenges and pitfalls[N].ICT-Energy Letters, 2015-07-15
                            </a>
                        </p>
                        <p id="579">
                            <a id="bibliography_77" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing egalitarian performance in the side-effects model of colocation for data center resource management">

                                <b>[77]</b>Pascual F, Rzadca K.Optimizing egalitarian performance in the side-effects model of colocation for data center resource management[C]Proc of the 23rd European Conf on Parallel Processing.Berlin:Springer, 2017:206-219
                            </a>
                        </p>
                        <p id="581">
                            <a id="bibliography_78" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-efficient flow scheduling and routing with hard deadlines in data center networks">

                                <b>[78]</b>Wang Lin, Zhang Fa, Zheng Kai, et al.Energy-efficient flow scheduling and routing with hard deadlines in data center networks[C]Proc of the 34th IEEE Int Conf on Distributed Computing Systems.Piscataway, NJ:IEEE, 2014:248-257
                            </a>
                        </p>
                        <p id="583">
                            <a id="bibliography_79" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online flow scheduling with deadline for energy conservation in data center networks">

                                <b>[79]</b>Zhou Biyu, Wu Jie, Wang Lin, et al.Online flow scheduling with deadline for energy conservation in data center networks[C]Proc of the 23rd IEEE Int Conf on Parallel and Distributed Systems.Piscataway, NJ:IEEE, 2017:578-585
                            </a>
                        </p>
                        <p id="585">
                            <a id="bibliography_80" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bandwidth-aware energy efficient flow scheduling with SDN in data center networks">

                                <b>[80]</b>Xu Guan, Dai Bin, Huang Benxiong, et al.Bandwidth-aware energy efficient flow scheduling with SDN in data center networks[J].Future Generation Computer Systems, 2017, 68 (c) :163-174
                            </a>
                        </p>
                        <p id="587">
                            <a id="bibliography_81" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy and carbon footprint-aware management of geo-distributed cloud data centers:Ataxonomy,state of the art,and future directions">

                                <b>[81]</b>Khosravi A, Buyya R.Energy and carbon footprint-aware management of geo-distributed cloud data centers:Ataxonomy, state of the art, and future directions[M]Advancing Cloud Database Systems and Capacity Planning with Dynamic Applications.Hershey, PA:IGI, 2016:1456-1475
                            </a>
                        </p>
                        <p id="589">
                            <a id="bibliography_82" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001081318&amp;v=MzE1NDVMcklKRnc9TmlmY2FyTzRIdEhOcjRkRVorb0hZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDM2xV&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[82]</b>Gunaratne C, Christensen K, Nordman B.Managing energy consumption costs in desktop PCs and LAN switches with proxying, split TCP connections, and scaling of link speed[J].International Journal of Network Management, 2005, 15 (5) :297-310
                            </a>
                        </p>
                        <p id="591">
                            <a id="bibliography_83" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy efficient ethernet study group[OL]">

                                <b>[83]</b>Bruce N.IEEE 802.3Energy efficient ethernet study group[OL]. (2007-09-21) [2019-03-02]http:www.ieee 802.org/3/eee_study/
                            </a>
                        </p>
                        <p id="593">
                            <a id="bibliography_84" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Carpo:Correlation-aware power optimization in data center networks">

                                <b>[84]</b>Wang Xiaodong, Yao Yanjun, Wang Xiaorui, et al.Carpo:Correlation-aware power optimization in data center networks[C]Proc of the 31st IEEE Int Conf on Computer Communications.Piscataway, NJ:IEEE, 2012:1125-1133
                            </a>
                        </p>
                        <p id="595">
                            <a id="bibliography_85" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-aware routing in data center network">

                                <b>[85]</b>Shang Yunfei, Li Dan, Xu Mingwei.Energy-aware routing in data center network[C]Proc of the 1st ACM SIGCOMMWorkshop on Green Networking.New York:ACM, 2010:1-8
                            </a>
                        </p>
                        <p id="597">
                            <a id="bibliography_86" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A distributed energy saving approach for Ethernet switches in data centers">

                                <b>[86]</b>Si Weisheng, Taheri J, Zomaya A.A distributed energy saving approach for Ethernet switches in data centers[C]Proc of the 37th IEEE Conf on Local Computer Networks.Piscataway, NJ:IEEE, 2012:505-512
                            </a>
                        </p>
                        <p id="599">
                            <a id="bibliography_87" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-resource energyefficient routing in cloud data centers with network-as-aservice">

                                <b>[87]</b>Wang Lin, Fern A, Zhang Fa, et al.Multi-resource energyefficient routing in cloud data centers with network-as-aservice[C]Proc of the 20th IEEE Symp on Computers and Communication.Piscataway, NJ:IEEE, 2015:694-699
                            </a>
                        </p>
                        <p id="601">
                            <a id="bibliography_88" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GreenDCN:A general framework for achieving energy efficiency in data center networks">

                                <b>[88]</b>Wang Lin, Zhang Fa, Aroca J, et al.GreenDCN:A general framework for achieving energy efficiency in data center networks[J].IEEE Journal on Selected Areas in Communications, 2014, 32 (1) :4-15
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908001&amp;v=MjUxNzdCdEdGckNVUkxPZVplUnJGeXZnVTd2S0x5dlNkTEc0SDlqTXA0OUZaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3B0WkU5b0REbytEMTRrUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

