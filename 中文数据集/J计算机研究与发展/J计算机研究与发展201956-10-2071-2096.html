

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127126970613750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201910004%26RESULT%3d1%26SIGN%3dpHiAbL4HL%252fhtPSN4u4R3Zpb8jL4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201910004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201910004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201910004&amp;v=MzI3MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJzRnl6Z1Zydk9MeXZTZExHNEg5ak5yNDlGWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#317" data-title="&lt;b&gt;1 机器学习可解释性问题&lt;/b&gt; "><b>1 机器学习可解释性问题</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#321" data-title="&lt;b&gt;2 ante&lt;/b&gt; -&lt;b&gt;hoc可解释性&lt;/b&gt; "><b>2 ante</b> -<b>hoc可解释性</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#323" data-title="&lt;b&gt;2.1 自解释模型&lt;/b&gt;"><b>2.1 自解释模型</b></a></li>
                                                <li><a href="#328" data-title="&lt;b&gt;2.2 广义加性模型&lt;/b&gt;"><b>2.2 广义加性模型</b></a></li>
                                                <li><a href="#333" data-title="&lt;b&gt;2.3 注意力机制&lt;/b&gt;"><b>2.3 注意力机制</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#339" data-title="&lt;b&gt;3 post-hoc可解释性&lt;/b&gt; "><b>3 post-hoc可解释性</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#346" data-title="&lt;b&gt;3.1 全局解释&lt;/b&gt;"><b>3.1 全局解释</b></a></li>
                                                <li><a href="#372" data-title="&lt;b&gt;3.2 局部解释&lt;/b&gt;"><b>3.2 局部解释</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#412" data-title="&lt;b&gt;4 可解释性应用&lt;/b&gt; "><b>4 可解释性应用</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#414" data-title="&lt;b&gt;4.1 模型验证&lt;/b&gt;"><b>4.1 模型验证</b></a></li>
                                                <li><a href="#420" data-title="&lt;b&gt;4.2 模型诊断&lt;/b&gt;"><b>4.2 模型诊断</b></a></li>
                                                <li><a href="#424" data-title="&lt;b&gt;4.3 辅助分析&lt;/b&gt;"><b>4.3 辅助分析</b></a></li>
                                                <li><a href="#430" data-title="&lt;b&gt;4.4 知识发现&lt;/b&gt;"><b>4.4 知识发现</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#435" data-title="&lt;b&gt;5 可解释性与安全性分析&lt;/b&gt; "><b>5 可解释性与安全性分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#437" data-title="&lt;b&gt;5.1 安全隐患消除&lt;/b&gt;"><b>5.1 安全隐患消除</b></a></li>
                                                <li><a href="#441" data-title="&lt;b&gt;5.2 安全威胁&lt;/b&gt;"><b>5.2 安全威胁</b></a></li>
                                                <li><a href="#444" data-title="&lt;b&gt;5.3 自身安全问题&lt;/b&gt;"><b>5.3 自身安全问题</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#451" data-title="&lt;b&gt;6 当前挑战与未来方向&lt;/b&gt; "><b>6 当前挑战与未来方向</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#453" data-title="&lt;b&gt;6.1 解释方法设计&lt;/b&gt;"><b>6.1 解释方法设计</b></a></li>
                                                <li><a href="#457" data-title="&lt;b&gt;6.2 解释方法评估&lt;/b&gt;"><b>6.2 解释方法评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#461" data-title="&lt;b&gt;7 结束语&lt;/b&gt; "><b>7 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#337" data-title="图1 自然语言处理应用中的注意力权重可视化">图1 自然语言处理应用中的注意力权重可视化</a></li>
                                                <li><a href="#338" data-title="图2 看图说话任务中注意力实现单词与图片的对齐">图2 看图说话任务中注意力实现单词与图片的对齐</a></li>
                                                <li><a href="#342" data-title="&lt;b&gt;表1 经典的post-hoc解释方法总结&lt;/b&gt;"><b>表1 经典的post-hoc解释方法总结</b></a></li>
                                                <li><a href="#370" data-title="图3 利用生成模型与激活最大化相结合生成的类别对应原型样本">图3 利用生成模型与激活最大化相结合生成的类别对应原型样本</a></li>
                                                <li><a href="#382" data-title="图4 通过图像模糊的方式最小化分类概率来学习显著性掩码">图4 通过图像模糊的方式最小化分类概率来学习显著性掩码</a></li>
                                                <li><a href="#394" data-title="图5 4种梯度反向传播解释方法解释效果对比">图5 4种梯度反向传播解释方法解释效果对比</a></li>
                                                <li><a href="#400" data-title="图6 导向特征反演方法解释示例">图6 导向特征反演方法解释示例</a></li>
                                                <li><a href="#404" data-title="图7 Grad-CAM与Guided Grad-CAM方法解释结果可视化">图7 Grad-CAM与Guided Grad-CAM方法解释结果可视化</a></li>
                                                <li><a href="#418" data-title="图8 基于可解释性的模型验证示例">图8 基于可解释性的模型验证示例</a></li>
                                                <li><a href="#427" data-title="图9 可解释方法在医疗诊断中的应用">图9 可解释方法在医疗诊断中的应用</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="1239">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                    Schroff F,Kalenichenko D,Philbin J.Facenet:A unified embedding for face recognition and clustering[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:815- 823</a>
                                </li>
                                <li id="1241">


                                    <a id="bibliography_2" title="Sun Yi,Liang Ding,Wang Xiaogang,et al.Deepid3:Face recognition with very deep neural networks[J].arXiv preprint arXiv:1502.00873,2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepid3:Face recognition with very deep neural networks">
                                        <b>[2]</b>
                                        Sun Yi,Liang Ding,Wang Xiaogang,et al.Deepid3:Face recognition with very deep neural networks[J].arXiv preprint arXiv:1502.00873,2015
                                    </a>
                                </li>
                                <li id="1243">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                    Taigman Y,Yang Ming,Ranzato M A,et al.Deepface:Closing the gap to human-level performance in face verification[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:1701- 1708</a>
                                </li>
                                <li id="1245">


                                    <a id="bibliography_4" title="Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?the kitti vision benchmark suite[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:3354- 3361" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Are We Ready For Autonomous Driving? TheKITTI Vision Benchmark Suite">
                                        <b>[4]</b>
                                        Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?the kitti vision benchmark suite[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:3354- 3361
                                    </a>
                                </li>
                                <li id="1247">


                                    <a id="bibliography_5" title="Tobiyama S,Yamaguchi Y,Shimada H,et al.Malware detection with deep neural network using process behavior[C] //Proc of the 40th Annual Computer Software and Applications Conf.Piscataway,NJ:IEEE,2016:577- 582" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Malware detection with deep neural network using process behavior">
                                        <b>[5]</b>
                                        Tobiyama S,Yamaguchi Y,Shimada H,et al.Malware detection with deep neural network using process behavior[C] //Proc of the 40th Annual Computer Software and Applications Conf.Piscataway,NJ:IEEE,2016:577- 582
                                    </a>
                                </li>
                                <li id="1249">


                                    <a id="bibliography_6" title="Rajpurkar P,Irvin J,Zhu K,et al.Chexnet:Radiologist-level pneumonia detection on chest x-rays with deep learning[J].arXiv preprint arXiv:1711.05225,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chexnet:Radiologist-level pneumonia detection on chest x-rays with deep learning">
                                        <b>[6]</b>
                                        Rajpurkar P,Irvin J,Zhu K,et al.Chexnet:Radiologist-level pneumonia detection on chest x-rays with deep learning[J].arXiv preprint arXiv:1711.05225,2017
                                    </a>
                                </li>
                                <li id="1251">


                                    <a id="bibliography_7" title="Ibrahim M,Louie M,Modarres C,et al.Global Explanations of Neural Networks:Mapping the Landscape of Predictions[J].arXiv preprint arXiv:1902.02384,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global Explanations of Neural Networks:Mapping the Landscape of Predictions">
                                        <b>[7]</b>
                                        Ibrahim M,Louie M,Modarres C,et al.Global Explanations of Neural Networks:Mapping the Landscape of Predictions[J].arXiv preprint arXiv:1902.02384,2019
                                    </a>
                                </li>
                                <li id="1253">


                                    <a id="bibliography_8" title="Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[J].arXiv preprint arXiv:1302.6199,2013" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks">
                                        <b>[8]</b>
                                        Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[J].arXiv preprint arXiv:1302.6199,2013
                                    </a>
                                </li>
                                <li id="1255">


                                    <a id="bibliography_9" title="Li Jinfeng,Ji Shouling,Du Tianyu,et al.TextBugger:Generating adversarial text against real-world applications[C] //Proc of the 26th Annual Network and Distributed Systems Security Symp.Reston,VA:ISOC,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TextBugger:Generating adversarial text against real-world applications">
                                        <b>[9]</b>
                                        Li Jinfeng,Ji Shouling,Du Tianyu,et al.TextBugger:Generating adversarial text against real-world applications[C] //Proc of the 26th Annual Network and Distributed Systems Security Symp.Reston,VA:ISOC,2019
                                    </a>
                                </li>
                                <li id="1257">


                                    <a id="bibliography_10" title="Du Tianyu,Ji Shouling,Li Jinfeng,et al.SirenAttack:Generating adversarial audio for end-to-end acoustic systems[J].arXiv preprint arXiv:1901.07846,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SirenAttack:Generating adversarial audio for end-to-end acoustic systems">
                                        <b>[10]</b>
                                        Du Tianyu,Ji Shouling,Li Jinfeng,et al.SirenAttack:Generating adversarial audio for end-to-end acoustic systems[J].arXiv preprint arXiv:1901.07846,2019
                                    </a>
                                </li>
                                <li id="1259">


                                    <a id="bibliography_11" title="Doshi-Velez F,Kim B.Towards a rigorous science of interpretable machine learning[J].arXiv preprint arXiv:1702.08608,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards a rigorous science of interpretable machine learning">
                                        <b>[11]</b>
                                        Doshi-Velez F,Kim B.Towards a rigorous science of interpretable machine learning[J].arXiv preprint arXiv:1702.08608,2017
                                    </a>
                                </li>
                                <li id="1261">


                                    <a id="bibliography_12" title="Guidotti R,Monreale A,Ruggieri S,et al.A survey of methods for explaining black box models[J].ACM Computing Surveys,2018,51(5):No.93" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM54EF9F7B61C0881B34CDC725ABD161FB&amp;v=MDcxNDBXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnBod0wyNnhLND1OaWZJWTdhOGE2ZkYyWWczWXVwOERIUXh6bVFRN2t3Sk8zamdxV05IRGJPU1JNenRDT052RlNpVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Guidotti R,Monreale A,Ruggieri S,et al.A survey of methods for explaining black box models[J].ACM Computing Surveys,2018,51(5):No.93
                                    </a>
                                </li>
                                <li id="1263">


                                    <a id="bibliography_13" title="Ribeiro M T,Singh S,Guestrin C.Why should I trust you?Explaining the predictions of any classifier[C] //Proc of the 22nd ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2016:1135- 1144" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining the predictions of any classifier">
                                        <b>[13]</b>
                                        Ribeiro M T,Singh S,Guestrin C.Why should I trust you?Explaining the predictions of any classifier[C] //Proc of the 22nd ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2016:1135- 1144
                                    </a>
                                </li>
                                <li id="1265">


                                    <a id="bibliography_14" title="Baehrens D,Schroeter T,Harmeling S,et al.How to explain individual classification decisions[J].Journal of Machine Learning Research,2010,11(6):1803- 1831" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=How to Explain Individual Classification Decisions">
                                        <b>[14]</b>
                                        Baehrens D,Schroeter T,Harmeling S,et al.How to explain individual classification decisions[J].Journal of Machine Learning Research,2010,11(6):1803- 1831
                                    </a>
                                </li>
                                <li id="1267">


                                    <a id="bibliography_15" title="Melis D A,Jaakkola T.Towards robust interpretability with self-explaining neural networks[C] //Proc of the 32nd Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2018:7775- 7784" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards robust interpretability with self-explaining neural networks">
                                        <b>[15]</b>
                                        Melis D A,Jaakkola T.Towards robust interpretability with self-explaining neural networks[C] //Proc of the 32nd Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2018:7775- 7784
                                    </a>
                                </li>
                                <li id="1269">


                                    <a id="bibliography_16" title="Poulin B,Eisner R,Szafron D,et al.Visual explanation of evidence with additive classifiers[C] //Proc of the 18th Conf on Innovative Applications of Artificial Intelligence.Palo Alto,CA:AAAI,2006:1822- 1829" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual explanation of evidence with additive classifiers">
                                        <b>[16]</b>
                                        Poulin B,Eisner R,Szafron D,et al.Visual explanation of evidence with additive classifiers[C] //Proc of the 18th Conf on Innovative Applications of Artificial Intelligence.Palo Alto,CA:AAAI,2006:1822- 1829
                                    </a>
                                </li>
                                <li id="1271">


                                    <a id="bibliography_17" title="Kononenko I.An efficient explanation of individual classifications using game theory[J].Journal of Machine Learning Research,2010,11(1):1- 18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Efficient Explanation of Individual Classifications using Game Theory">
                                        <b>[17]</b>
                                        Kononenko I.An efficient explanation of individual classifications using game theory[J].Journal of Machine Learning Research,2010,11(1):1- 18
                                    </a>
                                </li>
                                <li id="1273">


                                    <a id="bibliography_18" title="Haufe S,Meinecke F,G&#246;rgen K,et al.On the interpretation of weight vectors of linear models in multivariate neuroimaging[J].NeuroImage,2014,87:96- 110" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300124656&amp;v=MjM1Nzl1SHlqbVVMM0lKRnNSYmhRPU5pZk9mYks4SHRMT3JJOUZaZWtMQ25rL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Haufe S,Meinecke F,G&#246;rgen K,et al.On the interpretation of weight vectors of linear models in multivariate neuroimaging[J].NeuroImage,2014,87:96- 110
                                    </a>
                                </li>
                                <li id="1275">


                                    <a id="bibliography_19" title="Huysmans J,Dejaeger K,Mues C,et al.An empirical evaluation of the comprehensibility of decision table,tree and rule based predictive models[J].Decision Support Systems,2011,51(1):141- 154" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300403312&amp;v=MTU1OThNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZzUmJoUT1OaWZPZmJLN0h0RE9ySTlGWU9zTUQzMDdvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        Huysmans J,Dejaeger K,Mues C,et al.An empirical evaluation of the comprehensibility of decision table,tree and rule based predictive models[J].Decision Support Systems,2011,51(1):141- 154
                                    </a>
                                </li>
                                <li id="1277">


                                    <a id="bibliography_20" title="Breslow L A,Aha D W.Simplifying decision trees:A survey[J].The Knowledge Engineering Review,1997,12(1):1- 40" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simplifying decision trees: A survey">
                                        <b>[20]</b>
                                        Breslow L A,Aha D W.Simplifying decision trees:A survey[J].The Knowledge Engineering Review,1997,12(1):1- 40
                                    </a>
                                </li>
                                <li id="1279">


                                    <a id="bibliography_21" title="Frank E,Witten I H.Generating accurate rule sets without global optimization[C] //Proc of the 15th Int Conf on Machine Learning.San Francisco:Morgan Kaufmann,1998:144- 151" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Generating accurate rule sets without global optimization&amp;quot;">
                                        <b>[21]</b>
                                        Frank E,Witten I H.Generating accurate rule sets without global optimization[C] //Proc of the 15th Int Conf on Machine Learning.San Francisco:Morgan Kaufmann,1998:144- 151
                                    </a>
                                </li>
                                <li id="1281">


                                    <a id="bibliography_22" title="Quinlan J R.Generating production rules from decision trees[C] //Proc of the 10th Int Joint Conf on Artificial Intelligence.San Francisco:Morgan Kaufmann,1987:304- 307" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating production rules from decision trees">
                                        <b>[22]</b>
                                        Quinlan J R.Generating production rules from decision trees[C] //Proc of the 10th Int Joint Conf on Artificial Intelligence.San Francisco:Morgan Kaufmann,1987:304- 307
                                    </a>
                                </li>
                                <li id="1283">


                                    <a id="bibliography_23" title="Deng Houtao.Interpreting tree ensembles with intrees[J].International Journal of Data Science and Analytics,2019,7(4):277- 287" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpreting tree ensembles with intrees">
                                        <b>[23]</b>
                                        Deng Houtao.Interpreting tree ensembles with intrees[J].International Journal of Data Science and Analytics,2019,7(4):277- 287
                                    </a>
                                </li>
                                <li id="1285">


                                    <a id="bibliography_24" title="Lou Yin,Caruana R,Gehrke J.Intelligible models for classification and regression[C] //Proc of the 18th ACM SIGKDD Int Conf on Knowledge Discovery and Data mining.New York:ACM,2012:150- 158" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intelligible models for classification and regression">
                                        <b>[24]</b>
                                        Lou Yin,Caruana R,Gehrke J.Intelligible models for classification and regression[C] //Proc of the 18th ACM SIGKDD Int Conf on Knowledge Discovery and Data mining.New York:ACM,2012:150- 158
                                    </a>
                                </li>
                                <li id="1287">


                                    <a id="bibliography_25" title="Henson R K.The logic and interpretation of structure coefficients in multivariate general linear model analyses,No.ED467381[R].New Orleans,LA:ERIC Document Reproduction Service,2002" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The logic and interpretation of structure coefficients in multivariate general linear model analyses,No.ED467381">
                                        <b>[25]</b>
                                        Henson R K.The logic and interpretation of structure coefficients in multivariate general linear model analyses,No.ED467381[R].New Orleans,LA:ERIC Document Reproduction Service,2002
                                    </a>
                                </li>
                                <li id="1289">


                                    <a id="bibliography_26" title="Hastie T J.Statistical Models in S[M].New York:Routledge,2017:249- 307" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statistical Models in S">
                                        <b>[26]</b>
                                        Hastie T J.Statistical Models in S[M].New York:Routledge,2017:249- 307
                                    </a>
                                </li>
                                <li id="1291">


                                    <a id="bibliography_27" title="Wood S N.Generalized additive models:An introduction with R[M].New York:Chapman and Hall/CRC,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized additive models:An introduction with R">
                                        <b>[27]</b>
                                        Wood S N.Generalized additive models:An introduction with R[M].New York:Chapman and Hall/CRC,2017
                                    </a>
                                </li>
                                <li id="1293">


                                    <a id="bibliography_28" title="Ravikumar P,Lafferty J,Liu Han,et al.Sparse additive models[J].Journal of the Royal Statistical Society:Series B (Statistical Methodology),2009,71(5):1009- 1030" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse additive models">
                                        <b>[28]</b>
                                        Ravikumar P,Lafferty J,Liu Han,et al.Sparse additive models[J].Journal of the Royal Statistical Society:Series B (Statistical Methodology),2009,71(5):1009- 1030
                                    </a>
                                </li>
                                <li id="1295">


                                    <a id="bibliography_29" title="Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[J].arXiv preprint arXiv:1409.0473,2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">
                                        <b>[29]</b>
                                        Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[J].arXiv preprint arXiv:1409.0473,2014
                                    </a>
                                </li>
                                <li id="1297">


                                    <a id="bibliography_30" title="Vaswani A,Shazeer N,Parmar N,et al.Attention is all you need[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2017:6000- 6010" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention is all you need">
                                        <b>[30]</b>
                                        Vaswani A,Shazeer N,Parmar N,et al.Attention is all you need[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2017:6000- 6010
                                    </a>
                                </li>
                                <li id="1299">


                                    <a id="bibliography_31" title="Choi E,Bahadori M T,Sun Jimeng,et al.Retain:An interpretable predictive model for healthcare using reverse time attention mechanism[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2016:3504- 3512" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Retain:An interpretable predictive model for healthcare using reverse time attention mechanism">
                                        <b>[31]</b>
                                        Choi E,Bahadori M T,Sun Jimeng,et al.Retain:An interpretable predictive model for healthcare using reverse time attention mechanism[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2016:3504- 3512
                                    </a>
                                </li>
                                <li id="1301">


                                    <a id="bibliography_32" title="Xu K,Ba J,Kiros R,et al.Show,attend and tell:Neural image caption generation with visual attention[C] //Proc of the 32nd Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,2015:2048- 2057" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">
                                        <b>[32]</b>
                                        Xu K,Ba J,Kiros R,et al.Show,attend and tell:Neural image caption generation with visual attention[C] //Proc of the 32nd Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,2015:2048- 2057
                                    </a>
                                </li>
                                <li id="1303">


                                    <a id="bibliography_33" title="Chaudhari S,Polatkan G,Ramanath R,et al.An Attentive survey of attention models[J].arXiv:1904.02874,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Attentive survey of attention models">
                                        <b>[33]</b>
                                        Chaudhari S,Polatkan G,Ramanath R,et al.An Attentive survey of attention models[J].arXiv:1904.02874,2019
                                    </a>
                                </li>
                                <li id="1305">


                                    <a id="bibliography_34" title="Yang Zichao,Yang Diyi,Dyer C,et al.Hierarchical attention networks for document classification[C] //Proc of the 2016 Conf of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies.Stroudsburg,PA:Association for Computational Linguistics,2016:1480- 1489" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical attention networks for document classification">
                                        <b>[34]</b>
                                        Yang Zichao,Yang Diyi,Dyer C,et al.Hierarchical attention networks for document classification[C] //Proc of the 2016 Conf of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies.Stroudsburg,PA:Association for Computational Linguistics,2016:1480- 1489
                                    </a>
                                </li>
                                <li id="1307">


                                    <a id="bibliography_35" title="He Xiangnan,He Zhankui,Song Jingkuan,et al.NAIS:Neural attentive item similarity model for recommendation[J].IEEE Transactions on Knowledge and Data Engineering,2018,30(12):2354- 2366" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NAIS:Neural attentive item similarity model for recommendation">
                                        <b>[35]</b>
                                        He Xiangnan,He Zhankui,Song Jingkuan,et al.NAIS:Neural attentive item similarity model for recommendation[J].IEEE Transactions on Knowledge and Data Engineering,2018,30(12):2354- 2366
                                    </a>
                                </li>
                                <li id="1309">


                                    <a id="bibliography_36" title="Ying Haochao,Zhuang Fuzheng,Zhang Fuzheng,et al.Sequential recommender system based on hierarchical attention networks[C] //Proc of the 27th Int Joint Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:3926- 3932" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequential recommender system based on hierarchical attention networks">
                                        <b>[36]</b>
                                        Ying Haochao,Zhuang Fuzheng,Zhang Fuzheng,et al.Sequential recommender system based on hierarchical attention networks[C] //Proc of the 27th Int Joint Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:3926- 3932
                                    </a>
                                </li>
                                <li id="1311">


                                    <a id="bibliography_37" title="Zhou Chang,Bai Jinze,Song Junshuai,et al.ATRank:An attention-based user behavior modeling framework for recommendation[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:4564- 4571" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ATRank:An attention-based user behavior modeling framework for recommendation">
                                        <b>[37]</b>
                                        Zhou Chang,Bai Jinze,Song Junshuai,et al.ATRank:An attention-based user behavior modeling framework for recommendation[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:4564- 4571
                                    </a>
                                </li>
                                <li id="1313">


                                    <a id="bibliography_38" title="Yu Shuai,Wang Yongbo,Yang Min,et al.NAIRS:A Neural Attentive Interpretable Recommendation System[C] //Proc of the 12th ACM Int Conf on Web Search and Data Mining.New York:ACM,2019:790- 793" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NAIRS:A Neural Attentive Interpretable Recommendation System">
                                        <b>[38]</b>
                                        Yu Shuai,Wang Yongbo,Yang Min,et al.NAIRS:A Neural Attentive Interpretable Recommendation System[C] //Proc of the 12th ACM Int Conf on Web Search and Data Mining.New York:ACM,2019:790- 793
                                    </a>
                                </li>
                                <li id="1315">


                                    <a id="bibliography_39" title="Seo S,Huang Jing,Yang Hao,et al.Interpretable convolutional neural networks with dual local and global attention for review rating prediction[C] //Proc of the 11th ACM Conf on Recommender Systems.New York:ACM,2017:297- 305" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretable convolutional neural networks with dual local and global attention for review rating prediction">
                                        <b>[39]</b>
                                        Seo S,Huang Jing,Yang Hao,et al.Interpretable convolutional neural networks with dual local and global attention for review rating prediction[C] //Proc of the 11th ACM Conf on Recommender Systems.New York:ACM,2017:297- 305
                                    </a>
                                </li>
                                <li id="1317">


                                    <a id="bibliography_40" title="Mashayekhi M,Gras R.Rule extraction from decision trees ensembles:New algorithms based on heuristic search and sparse group lasso methods[J].International Journal of Information Technology &amp;amp; Decision Making,2017,16(6):1707- 1727" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rule extraction from decision trees ensembles:New algorithms based on heuristic search and sparse group lasso methods">
                                        <b>[40]</b>
                                        Mashayekhi M,Gras R.Rule extraction from decision trees ensembles:New algorithms based on heuristic search and sparse group lasso methods[J].International Journal of Information Technology &amp;amp; Decision Making,2017,16(6):1707- 1727
                                    </a>
                                </li>
                                <li id="1319">


                                    <a id="bibliography_41" title="Yang Chengliang,Rangarajan A,Ranka S.Global model interpretation via recursive partitioning[C] //Proc of the 4th IEEE Int Conf on Data Science and Systems.Piscataway,NJ:IEEE,2018:1563- 1570" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global model interpretation via recursive partitioning">
                                        <b>[41]</b>
                                        Yang Chengliang,Rangarajan A,Ranka S.Global model interpretation via recursive partitioning[C] //Proc of the 4th IEEE Int Conf on Data Science and Systems.Piscataway,NJ:IEEE,2018:1563- 1570
                                    </a>
                                </li>
                                <li id="1321">


                                    <a id="bibliography_42" title="Puri N,Gupta P,Agarwal P,et al.MAGIX:Model agnostic globally interpretable explanations[J].arXiv preprint arXiv:1706.07160,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MAGIX:Model agnostic globally interpretable explanations">
                                        <b>[42]</b>
                                        Puri N,Gupta P,Agarwal P,et al.MAGIX:Model agnostic globally interpretable explanations[J].arXiv preprint arXiv:1706.07160,2017
                                    </a>
                                </li>
                                <li id="1323">


                                    <a id="bibliography_43" title="Wang Junpeng,Gou Liang,Zhang Wei,et al.DeepVID:Deep visual interpretation and diagnosis for image classifiers via knowledge distillation[J].IEEE Transactions on Visualization and Computer Graphics,2019,25(6):2168- 2180" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepVID:Deep visual interpretation and diagnosis for image classifiers via knowledge distillation">
                                        <b>[43]</b>
                                        Wang Junpeng,Gou Liang,Zhang Wei,et al.DeepVID:Deep visual interpretation and diagnosis for image classifiers via knowledge distillation[J].IEEE Transactions on Visualization and Computer Graphics,2019,25(6):2168- 2180
                                    </a>
                                </li>
                                <li id="1325">


                                    <a id="bibliography_44" title="Erhan D,Bengio Y,Courville A,et al.Visualizing higher-layer features of a deep network[J].University of Montreal,2009,1341(3):1- 13" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing higher-layer features of a deep network">
                                        <b>[44]</b>
                                        Erhan D,Bengio Y,Courville A,et al.Visualizing higher-layer features of a deep network[J].University of Montreal,2009,1341(3):1- 13
                                    </a>
                                </li>
                                <li id="1327">


                                    <a id="bibliography_45" title="Nguyen A,Dosovitskiy A,Yosinski J,et al.Synthesizing the preferred inputs for neurons in neural networks via deep generator networks[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2016:3387- 3395" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synthesizing the preferred inputs for neurons in neural networks via deep generator networks">
                                        <b>[45]</b>
                                        Nguyen A,Dosovitskiy A,Yosinski J,et al.Synthesizing the preferred inputs for neurons in neural networks via deep generator networks[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2016:3387- 3395
                                    </a>
                                </li>
                                <li id="1329">


                                    <a id="bibliography_46" title="Yuan Hao,Chen Yongjun,Hu Xia,et al.Interpreting deep models for text analysis via optimization and regularization methods[C] //Proc of the 33rd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2019:5717- 5724" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpreting deep models for text analysis via optimization and regularization methods">
                                        <b>[46]</b>
                                        Yuan Hao,Chen Yongjun,Hu Xia,et al.Interpreting deep models for text analysis via optimization and regularization methods[C] //Proc of the 33rd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2019:5717- 5724
                                    </a>
                                </li>
                                <li id="1331">


                                    <a id="bibliography_47" title="Fong R C,Vedaldi A.Interpretable explanations of black boxes by meaningful perturbation[C] //Proc of the 16th IEEE Int Conf on Computer Vision.Piscataway,NJ:IEEE,2017:3429- 3437" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretable explanations of black boxes by meaningful perturbation">
                                        <b>[47]</b>
                                        Fong R C,Vedaldi A.Interpretable explanations of black boxes by meaningful perturbation[C] //Proc of the 16th IEEE Int Conf on Computer Vision.Piscataway,NJ:IEEE,2017:3429- 3437
                                    </a>
                                </li>
                                <li id="1333">


                                    <a id="bibliography_48" title="Liu Lingqiao,Wang Lei.What has my classifier learned?visualizing the classification rules of bag-of-feature model by support region detection[C] //Proc of the 24th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:3586- 3593" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What has my classifier learned?Visualizing the classification rules of bag-of-feature model by support region detection">
                                        <b>[48]</b>
                                        Liu Lingqiao,Wang Lei.What has my classifier learned?visualizing the classification rules of bag-of-feature model by support region detection[C] //Proc of the 24th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:3586- 3593
                                    </a>
                                </li>
                                <li id="1335">


                                    <a id="bibliography_49" title="Guidotti R,Monreale A,Ruggieri S,et al.Local rule-based explanations of black box decision systems[J].arXiv preprint arXiv:1805.10820,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local rule-based explanations of black box decision systems">
                                        <b>[49]</b>
                                        Guidotti R,Monreale A,Ruggieri S,et al.Local rule-based explanations of black box decision systems[J].arXiv preprint arXiv:1805.10820,2018
                                    </a>
                                </li>
                                <li id="1337">


                                    <a id="bibliography_50" title="Ribeiro M T,Singh S,Guestrin C.Anchors:High-precision model-agnostic explanations[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:1527- 1535" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchors:High-precision model-agnostic explanations">
                                        <b>[50]</b>
                                        Ribeiro M T,Singh S,Guestrin C.Anchors:High-precision model-agnostic explanations[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:1527- 1535
                                    </a>
                                </li>
                                <li id="1339">


                                    <a id="bibliography_51" title="Guo Wenbo,Mu Dongliang,Xu Jun,et al.Lemna:Explaining deep learning based security applications[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:364- 379" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lemna:Explaining deep learning based security applications">
                                        <b>[51]</b>
                                        Guo Wenbo,Mu Dongliang,Xu Jun,et al.Lemna:Explaining deep learning based security applications[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:364- 379
                                    </a>
                                </li>
                                <li id="1341">


                                    <a id="bibliography_52" title="Simonyan K,Vedaldi A,Zisserman A.Deep inside convolutional networks:Visualising image classification models and saliency maps[J].arXiv preprint arXiv:1312.6034,2013" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep inside convolutional networks:Visualising image classification models and saliency maps">
                                        <b>[52]</b>
                                        Simonyan K,Vedaldi A,Zisserman A.Deep inside convolutional networks:Visualising image classification models and saliency maps[J].arXiv preprint arXiv:1312.6034,2013
                                    </a>
                                </li>
                                <li id="1343">


                                    <a id="bibliography_53" title="Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C] //Proc of the 13th European Conf on Computer Vision.Berlin:Springer,2014:818- 833" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[53]</b>
                                        Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C] //Proc of the 13th European Conf on Computer Vision.Berlin:Springer,2014:818- 833
                                    </a>
                                </li>
                                <li id="1345">


                                    <a id="bibliography_54" title="Springenberg J T,Dosovitskiy A,Brox T,et al.Striving for simplicity:The all convolutional net[J].arXiv preprint arXiv:1412.6806,2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Striving for simplicity:The all convolutional net">
                                        <b>[54]</b>
                                        Springenberg J T,Dosovitskiy A,Brox T,et al.Striving for simplicity:The all convolutional net[J].arXiv preprint arXiv:1412.6806,2014
                                    </a>
                                </li>
                                <li id="1347">


                                    <a id="bibliography_55" title="Sundararajan M,Taly A,Yan Qiqi.Gradients of counterfactuals[J].arXiv preprint arXiv:1611.02639,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradients of counterfactuals">
                                        <b>[55]</b>
                                        Sundararajan M,Taly A,Yan Qiqi.Gradients of counterfactuals[J].arXiv preprint arXiv:1611.02639,2016
                                    </a>
                                </li>
                                <li id="1349">


                                    <a id="bibliography_56" title="Smilkov D,Thorat N,Kim B,et al.Smoothgrad:Removing noise by adding noise[J].arXiv preprint arXiv:1706.03825,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Smoothgrad:Removing noise by adding noise">
                                        <b>[56]</b>
                                        Smilkov D,Thorat N,Kim B,et al.Smoothgrad:Removing noise by adding noise[J].arXiv preprint arXiv:1706.03825,2017
                                    </a>
                                </li>
                                <li id="1351">


                                    <a id="bibliography_57" title="Bach S,Binder A,Montavon G,et al.On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation[J].PloS One,2015,10(7):e0130140" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation">
                                        <b>[57]</b>
                                        Bach S,Binder A,Montavon G,et al.On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation[J].PloS One,2015,10(7):e0130140
                                    </a>
                                </li>
                                <li id="1353">


                                    <a id="bibliography_58" title="Shrikumar A,Greenside P,Shcherbina A,et al.Not just a black box:Learning important features through propagating activation differences[J].arXiv preprint arXiv:1605.01713,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Not just a black box:Learning important features through propagating activation differences">
                                        <b>[58]</b>
                                        Shrikumar A,Greenside P,Shcherbina A,et al.Not just a black box:Learning important features through propagating activation differences[J].arXiv preprint arXiv:1605.01713,2016
                                    </a>
                                </li>
                                <li id="1355">


                                    <a id="bibliography_59" title="Du Mengnan,Liu Ninghao,Song Qingquan,et al.Towards explanation of DNN-based prediction with guided feature inversion[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM,2018:1358- 1367" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards explanation of DNN-based prediction with guided feature inversion">
                                        <b>[59]</b>
                                        Du Mengnan,Liu Ninghao,Song Qingquan,et al.Towards explanation of DNN-based prediction with guided feature inversion[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM,2018:1358- 1367
                                    </a>
                                </li>
                                <li id="1357">


                                    <a id="bibliography_60" title="Zhou Bolei,Khosla A,Lapedriza A,et al.Learning deep features for discriminative localization[C] //Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2921- 2929" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep features for discriminative localization">
                                        <b>[60]</b>
                                        Zhou Bolei,Khosla A,Lapedriza A,et al.Learning deep features for discriminative localization[C] //Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2921- 2929
                                    </a>
                                </li>
                                <li id="1359">


                                    <a id="bibliography_61" title="Selvaraju R R,Cogswell M,Das A,et al.Grad-CAM:Visual explanations from deep networks via gradient-based localization[C] //Proc of the IEEE Intel Conf on Computer Vision.Piscataway,NJ:IEEE,2017:618- 626" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grad-CAM:Visual explanations from deep networks via gradient-based localization">
                                        <b>[61]</b>
                                        Selvaraju R R,Cogswell M,Das A,et al.Grad-CAM:Visual explanations from deep networks via gradient-based localization[C] //Proc of the IEEE Intel Conf on Computer Vision.Piscataway,NJ:IEEE,2017:618- 626
                                    </a>
                                </li>
                                <li id="1361">


                                    <a id="bibliography_62" title="Gehr T,Mirman M,Drachsler-Cohen D,et al.Ai2:Safety and robustness certification of neural networks with abstract interpretation[C] //Proc of the 2018 IEEE Symposium on Security and Privacy.Piscataway,NJ:IEEE,2018:3- 18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AI 2:Safety and robustness certification of neural networks with abstract interpretation">
                                        <b>[62]</b>
                                        Gehr T,Mirman M,Drachsler-Cohen D,et al.Ai2:Safety and robustness certification of neural networks with abstract interpretation[C] //Proc of the 2018 IEEE Symposium on Security and Privacy.Piscataway,NJ:IEEE,2018:3- 18
                                    </a>
                                </li>
                                <li id="1363">


                                    <a id="bibliography_63" title="Chu Lingyang,Hu Xia,Hu Juhua,et al.Exact and consistent interpretation for piecewise linear neural networks:A closed form solution[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM,2018:1244- 1253" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exact and consistent interpretation for piecewise linear neural networks:A closed form solution">
                                        <b>[63]</b>
                                        Chu Lingyang,Hu Xia,Hu Juhua,et al.Exact and consistent interpretation for piecewise linear neural networks:A closed form solution[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM,2018:1244- 1253
                                    </a>
                                </li>
                                <li id="1365">


                                    <a id="bibliography_64" title="Andrews R,Diederich J,Tickle A B.Survey and critique of techniques for extracting rules from trained artificial neural networks[J].Knowledge-Based Systems,1995,8(6):373- 389" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501720365&amp;v=MjMwMzd5am1VTDNJSkZzUmJoUT1OaWZPZmJLN0h0RE5xbzlFWStrUEQzbzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[64]</b>
                                        Andrews R,Diederich J,Tickle A B.Survey and critique of techniques for extracting rules from trained artificial neural networks[J].Knowledge-Based Systems,1995,8(6):373- 389
                                    </a>
                                </li>
                                <li id="1367">


                                    <a id="bibliography_65" title="Tickle A B,Andrews R,Golea M,et al.The truth will come to light:Directions and challenges in extracting the knowledge embedded within trained artificial neural networks[J].IEEE Transactions on Neural Networks,1998,9(6):1057- 1068" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The truth will come to light: directions and challenges in extracting the knowledge embedded within trained artificial neural networks">
                                        <b>[65]</b>
                                        Tickle A B,Andrews R,Golea M,et al.The truth will come to light:Directions and challenges in extracting the knowledge embedded within trained artificial neural networks[J].IEEE Transactions on Neural Networks,1998,9(6):1057- 1068
                                    </a>
                                </li>
                                <li id="1369">


                                    <a id="bibliography_66" title="Tickle A B,Orlowski M,Diederich J.DEDEC:A methodology for extracting rules from trained artificial neural networks[C] //Proc of the AISB&#39;96 Workshop on Rule Extraction from Trained Neural Networks.Amsterdam,The Netherlands:IOS Press,1996:90- 102" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DEDEC: A Methodology for extracting rules from trained artificial neural networks">
                                        <b>[66]</b>
                                        Tickle A B,Orlowski M,Diederich J.DEDEC:A methodology for extracting rules from trained artificial neural networks[C] //Proc of the AISB&#39;96 Workshop on Rule Extraction from Trained Neural Networks.Amsterdam,The Netherlands:IOS Press,1996:90- 102
                                    </a>
                                </li>
                                <li id="1371">


                                    <a id="bibliography_67" title="Fu Limin.Rule generation from neural networks[J].IEEE Transactions on Systems,Man,and Cybernetics,1994,24(8):1114- 1124" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rule generation from neural networks">
                                        <b>[67]</b>
                                        Fu Limin.Rule generation from neural networks[J].IEEE Transactions on Systems,Man,and Cybernetics,1994,24(8):1114- 1124
                                    </a>
                                </li>
                                <li id="1373">


                                    <a id="bibliography_68" title="Bastani O,Kim C,Bastani H.Interpreting blackbox models via model extraction[J].arXiv preprint arXiv:1705.08504,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpreting blackbox models via model extraction">
                                        <b>[68]</b>
                                        Bastani O,Kim C,Bastani H.Interpreting blackbox models via model extraction[J].arXiv preprint arXiv:1705.08504,2017
                                    </a>
                                </li>
                                <li id="1375">


                                    <a id="bibliography_69" title="Craven M W.Extracting comprehensible models from trained neural networks[D].Madison,WI:Department of Computer Sciences,University of Wisconsin-Madison,1996" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting comprehensible models from trained neural networks">
                                        <b>[69]</b>
                                        Craven M W.Extracting comprehensible models from trained neural networks[D].Madison,WI:Department of Computer Sciences,University of Wisconsin-Madison,1996
                                    </a>
                                </li>
                                <li id="1377">


                                    <a id="bibliography_70" title="Boz O.Extracting decision trees from trained neural networks[C] //Proc of the 8th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2002:456- 461" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting decision trees from trained neural networks">
                                        <b>[70]</b>
                                        Boz O.Extracting decision trees from trained neural networks[C] //Proc of the 8th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2002:456- 461
                                    </a>
                                </li>
                                <li id="1379">


                                    <a id="bibliography_71" title="Mashayekhi M,Gras R.Rule extraction from random forest:The RF+HC methods[G] //LNCS 9091:Proc of the 28th Canadian Conf on Artificial Intelligence.Berlin:Springer,2015:223- 237" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rule extraction from random forest:The RF+HC methods">
                                        <b>[71]</b>
                                        Mashayekhi M,Gras R.Rule extraction from random forest:The RF+HC methods[G] //LNCS 9091:Proc of the 28th Canadian Conf on Artificial Intelligence.Berlin:Springer,2015:223- 237
                                    </a>
                                </li>
                                <li id="1381">


                                    <a id="bibliography_72" title="Hara S,Hayashi K.Making tree ensembles interpretable:A Bayesian model selection approach[J].arXiv preprint arXiv:1606.09066,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making tree ensembles interpretable:A Bayesian model selection approach">
                                        <b>[72]</b>
                                        Hara S,Hayashi K.Making tree ensembles interpretable:A Bayesian model selection approach[J].arXiv preprint arXiv:1606.09066,2016
                                    </a>
                                </li>
                                <li id="1383">


                                    <a id="bibliography_73" title="Hara S,Hayashi K.Making tree ensembles interpretable[J].arXiv preprint arXiv:1606.05390,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making tree ensembles interpretable">
                                        <b>[73]</b>
                                        Hara S,Hayashi K.Making tree ensembles interpretable[J].arXiv preprint arXiv:1606.05390,2016
                                    </a>
                                </li>
                                <li id="1385">


                                    <a id="bibliography_74" title="Krishnan R.A systematic method for decompositional rule extraction from neural networks[C] //Proc of the 10th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,1996" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A systematic method for decompositional rule extraction from neural networks">
                                        <b>[74]</b>
                                        Krishnan R.A systematic method for decompositional rule extraction from neural networks[C] //Proc of the 10th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,1996
                                    </a>
                                </li>
                                <li id="1387">


                                    <a id="bibliography_75" title="Bondarenko A,Zmanovska T,Borisov A.Decompositional rules extraction methods from neural networks[C] //Proc of the 16th Int Conf on Soft Computing.Berlin:Springer,2010:256- 262" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decompositional rules extraction methods from neural networks">
                                        <b>[75]</b>
                                        Bondarenko A,Zmanovska T,Borisov A.Decompositional rules extraction methods from neural networks[C] //Proc of the 16th Int Conf on Soft Computing.Berlin:Springer,2010:256- 262
                                    </a>
                                </li>
                                <li id="1389">


                                    <a id="bibliography_76" title="Craven M W,Shavlik J W.Using sampling and queries to extract rules from trained neural networks[C] //Proc of the 8th Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,1994:37- 45" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using sampling and queries to extract rules from trained neural networks">
                                        <b>[76]</b>
                                        Craven M W,Shavlik J W.Using sampling and queries to extract rules from trained neural networks[C] //Proc of the 8th Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,1994:37- 45
                                    </a>
                                </li>
                                <li id="1391">


                                    <a id="bibliography_77" title="Zhou Zhihua,Jiang Yuan,Chen Shifu.Extracting symbolic rules from trained neural network ensembles[J].AI Communications,2003,16(1):3- 15" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD00000000637&amp;v=MDYyNTBIN1I3cWVidWR0RkNybFY3ck5KRmc9TmlUQmFyTzRIdEhNcjQ5Rll1Z0lZM2s1ekJkaDRqOTlTWHFScnhveGNN&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[77]</b>
                                        Zhou Zhihua,Jiang Yuan,Chen Shifu.Extracting symbolic rules from trained neural network ensembles[J].AI Communications,2003,16(1):3- 15
                                    </a>
                                </li>
                                <li id="1393">


                                    <a id="bibliography_78" title="De Fortuny E J,Martens D.Active learning-based pedagogical rule extraction[J].IEEE Transactions on Neural Networks and Learning Systems,2015,26(11):2664- 2677" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active learning-based pedagogical rule extraction">
                                        <b>[78]</b>
                                        De Fortuny E J,Martens D.Active learning-based pedagogical rule extraction[J].IEEE Transactions on Neural Networks and Learning Systems,2015,26(11):2664- 2677
                                    </a>
                                </li>
                                <li id="1395">


                                    <a id="bibliography_79" title="Lakkaraju H,Kamar E,Caruana R,et al.Interpretable &amp;amp; explorable approximations of black box models[J].arXiv preprint arXiv:1707.01154,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretable &amp;amp; explorable approximations of black box models">
                                        <b>[79]</b>
                                        Lakkaraju H,Kamar E,Caruana R,et al.Interpretable &amp;amp; explorable approximations of black box models[J].arXiv preprint arXiv:1707.01154,2017
                                    </a>
                                </li>
                                <li id="1397">


                                    <a id="bibliography_80" title="Liu Xuan,Wang Xiaoguang,Matwin S.Improving the interpretability of deep neural networks with knowledge distillation[C] //Proc of the 18th IEEE Int Conf on Data Mining Workshops.Piscataway,NJ:IEEE,2018:905- 912" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving the interpretability of deep neural networks with knowledge distillation">
                                        <b>[80]</b>
                                        Liu Xuan,Wang Xiaoguang,Matwin S.Improving the interpretability of deep neural networks with knowledge distillation[C] //Proc of the 18th IEEE Int Conf on Data Mining Workshops.Piscataway,NJ:IEEE,2018:905- 912
                                    </a>
                                </li>
                                <li id="1399">


                                    <a id="bibliography_81" title="Buciluǎ C,Caruana R,Niculescu-Mizil A.Model compression[C] //Proc of the 12th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2006:535- 541" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model compression">
                                        <b>[81]</b>
                                        Buciluǎ C,Caruana R,Niculescu-Mizil A.Model compression[C] //Proc of the 12th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2006:535- 541
                                    </a>
                                </li>
                                <li id="1401">


                                    <a id="bibliography_82" title="Hinton G,Vinyals O,Dean J.Distilling the knowledge in a neural network[J].arXiv preprint arXiv:1503.02531,2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distilling the knowledge in a neural network">
                                        <b>[82]</b>
                                        Hinton G,Vinyals O,Dean J.Distilling the knowledge in a neural network[J].arXiv preprint arXiv:1503.02531,2015
                                    </a>
                                </li>
                                <li id="1403">


                                    <a id="bibliography_83" title="Craven M,Shavlik J W.Extracting tree-structured representations of trained networks[C] //Proc of the 10th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,1996:24- 30" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting tree-structured representations of trained networks">
                                        <b>[83]</b>
                                        Craven M,Shavlik J W.Extracting tree-structured representations of trained networks[C] //Proc of the 10th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,1996:24- 30
                                    </a>
                                </li>
                                <li id="1405">


                                    <a id="bibliography_84" title="Frosst N,Hinton G.Distilling a neural network into a soft decision tree[J].arXiv preprint arXiv:1711.09784,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distilling a neural network into a soft decision tree">
                                        <b>[84]</b>
                                        Frosst N,Hinton G.Distilling a neural network into a soft decision tree[J].arXiv preprint arXiv:1711.09784,2017
                                    </a>
                                </li>
                                <li id="1407">


                                    <a id="bibliography_85" title="Tan S,Caruana R,Hooker G,et al.Learning global additive explanations for neural nets using model distillation[J].arXiv preprint arXiv:1801.08640,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning global additive explanations for neural nets using model distillation">
                                        <b>[85]</b>
                                        Tan S,Caruana R,Hooker G,et al.Learning global additive explanations for neural nets using model distillation[J].arXiv preprint arXiv:1801.08640,2018
                                    </a>
                                </li>
                                <li id="1409">


                                    <a id="bibliography_86" title="Che Zhengping,Purushotham S,Khemani R,et al.Interpretable deep models for ICU outcome prediction[C] //Proc of the AMIA Annual Symp.Bethesda,MD:American Medical Informatics Association,2016:371- 380" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretable deep models for ICU outcome prediction">
                                        <b>[86]</b>
                                        Che Zhengping,Purushotham S,Khemani R,et al.Interpretable deep models for ICU outcome prediction[C] //Proc of the AMIA Annual Symp.Bethesda,MD:American Medical Informatics Association,2016:371- 380
                                    </a>
                                </li>
                                <li id="1411">


                                    <a id="bibliography_87" title="Ding Tao,Hasan F,Bickel W K,et al.Interpreting social media-based substance use prediction models with knowledge distillation[C] //Proc of the 30th IEEE Int Conf on Tools with Artificial Intelligence.Piscataway,NJ:IEEE,2018:623- 630" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpreting social media-based substance use prediction models with knowledge distillation">
                                        <b>[87]</b>
                                        Ding Tao,Hasan F,Bickel W K,et al.Interpreting social media-based substance use prediction models with knowledge distillation[C] //Proc of the 30th IEEE Int Conf on Tools with Artificial Intelligence.Piscataway,NJ:IEEE,2018:623- 630
                                    </a>
                                </li>
                                <li id="1413">


                                    <a id="bibliography_88" title="Xu Kai,Park D H,Yi Chang,et al.Interpreting deep classifier by visual distillation of dark knowledge[J].arXiv preprint arXiv:1803.04042,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpreting deep classifier by visual distillation of dark knowledge">
                                        <b>[88]</b>
                                        Xu Kai,Park D H,Yi Chang,et al.Interpreting deep classifier by visual distillation of dark knowledge[J].arXiv preprint arXiv:1803.04042,2018
                                    </a>
                                </li>
                                <li id="1415">


                                    <a id="bibliography_89" title="Tan S,Caruana R,Hooker G,et al.Distill-and-compare:Auditing black-box models using transparent model distillation[C] //Proc of the 2018 AAAI/ACM Conf on AI,Ethics,and Society.New York:ACM,2018:303- 310" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distill-and-compare:Auditing black-box models using transparent model distillation">
                                        <b>[89]</b>
                                        Tan S,Caruana R,Hooker G,et al.Distill-and-compare:Auditing black-box models using transparent model distillation[C] //Proc of the 2018 AAAI/ACM Conf on AI,Ethics,and Society.New York:ACM,2018:303- 310
                                    </a>
                                </li>
                                <li id="1417">


                                    <a id="bibliography_90" title="Tan S,Caruana R,Hooker G,et al.Detecting bias in black-box models using transparent model distillation[J].arXiv preprint arXiv:1710.06169,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting bias in black-box models using transparent model distillation">
                                        <b>[90]</b>
                                        Tan S,Caruana R,Hooker G,et al.Detecting bias in black-box models using transparent model distillation[J].arXiv preprint arXiv:1710.06169,2017
                                    </a>
                                </li>
                                <li id="1419">


                                    <a id="bibliography_91" title="Zhang Quanshi,Wang Wenguan,Zhu Songchun.Examining CNN representations with respect to dataset bias[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:4464- 4473" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Examining CNN representations with respect to dataset bias">
                                        <b>[91]</b>
                                        Zhang Quanshi,Wang Wenguan,Zhu Songchun.Examining CNN representations with respect to dataset bias[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:4464- 4473
                                    </a>
                                </li>
                                <li id="1421">


                                    <a id="bibliography_92" title="Zhang Quanshi,Wu Yingnian,Zhu Songchun.Interpretable convolutional neural networks[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:8827- 8836" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretable convolutional neural networks">
                                        <b>[92]</b>
                                        Zhang Quanshi,Wu Yingnian,Zhu Songchun.Interpretable convolutional neural networks[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:8827- 8836
                                    </a>
                                </li>
                                <li id="1423">


                                    <a id="bibliography_93" title="Berkes P,Wiskott L.On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields[J].Neural Computation,2006,18(8):1868- 1895" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012549&amp;v=MjQ1MTdSZEdlcnFRVE1ud1plWnVIeWptVUwzSUpGc1JiaFE9TmlmSlpiSzlIdGpNcW85RlpPb05DWGd3b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[93]</b>
                                        Berkes P,Wiskott L.On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields[J].Neural Computation,2006,18(8):1868- 1895
                                    </a>
                                </li>
                                <li id="1425">


                                    <a id="bibliography_94" title="Montavon G,Samek W,M&#252;ller K-R.Methods for interpreting and understanding deep neural networks[J].Digital Signal Processing,2018,73:1- 15" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES999A72AA8421C0DBBDAD577B3F8639B4&amp;v=MzA1OTJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOcGh3TDI2eEs0PU5pZk9mYnF4RjZETHJmNDBiTzhORFE4NXUyUmhuazRKVFhqbDNoRkRjYlNYVE1pYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[94]</b>
                                        Montavon G,Samek W,M&#252;ller K-R.Methods for interpreting and understanding deep neural networks[J].Digital Signal Processing,2018,73:1- 15
                                    </a>
                                </li>
                                <li id="1427">


                                    <a id="bibliography_95" title="Mahendran A,Vedaldi A.Understanding deep image representations by inverting them[C] //Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:5188- 5196" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding deep image representations by inverting them">
                                        <b>[95]</b>
                                        Mahendran A,Vedaldi A.Understanding deep image representations by inverting them[C] //Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:5188- 5196
                                    </a>
                                </li>
                                <li id="1429">


                                    <a id="bibliography_96" title="Nguyen A,Clune J,Bengio Y,et al.Plug &amp;amp; play generative networks:Conditional iterative generation of images in latent space[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:4467- 4477" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Plug.&amp;amp;.play generative networks:conditional iterative generation of images in latent space">
                                        <b>[96]</b>
                                        Nguyen A,Clune J,Bengio Y,et al.Plug &amp;amp; play generative networks:Conditional iterative generation of images in latent space[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:4467- 4477
                                    </a>
                                </li>
                                <li id="1431">


                                    <a id="bibliography_97" title="Nguyen A,Yosinski J,Clune J.Multifaceted feature visualization:Uncovering the different types of features learned by each neuron in deep neural networks[J].arXiv preprint arXiv:1602.03616,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multifaceted feature visualization:Uncovering the different types of features learned by each neuron in deep neural networks">
                                        <b>[97]</b>
                                        Nguyen A,Yosinski J,Clune J.Multifaceted feature visualization:Uncovering the different types of features learned by each neuron in deep neural networks[J].arXiv preprint arXiv:1602.03616,2016
                                    </a>
                                </li>
                                <li id="1433">


                                    <a id="bibliography_98" title="Saltelli A,Tarantola S,Campolongo F,et al.Sensitivity Analysis in Practice:A Guide to Assessing Scientific Models[M].New York:John Wiley &amp;amp; Sons,2004" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sensitivity Analysis in Practice">
                                        <b>[98]</b>
                                        Saltelli A,Tarantola S,Campolongo F,et al.Sensitivity Analysis in Practice:A Guide to Assessing Scientific Models[M].New York:John Wiley &amp;amp; Sons,2004
                                    </a>
                                </li>
                                <li id="1435">


                                    <a id="bibliography_99" title="Chatterjee S,Hadi A S.Sensitivity Analysis in Linear Regression[M].New York:John Wiley &amp;amp; Sons,2009" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sensitivity Analysis in Linear Regression">
                                        <b>[99]</b>
                                        Chatterjee S,Hadi A S.Sensitivity Analysis in Linear Regression[M].New York:John Wiley &amp;amp; Sons,2009
                                    </a>
                                </li>
                                <li id="1437">


                                    <a id="bibliography_100" title="Homma T,Saltelli A.Importance measures in global sensitivity analysis of nonlinear models[J].Reliability Engineering &amp;amp; System Safety,1996,52(1):1- 17" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501718640&amp;v=MTc4ODZmT2ZiSzdIdEROcW85RVkrb0hDbmc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZzUmJoUT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[100]</b>
                                        Homma T,Saltelli A.Importance measures in global sensitivity analysis of nonlinear models[J].Reliability Engineering &amp;amp; System Safety,1996,52(1):1- 17
                                    </a>
                                </li>
                                <li id="1439">


                                    <a id="bibliography_101" title="Saltelli A,Tarantola S,Chan K-S.A quantitative model-independent method for global sensitivity analysis of model output[J].Technometrics,1999,41(1):39- 56" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quantitative model-independent method for global sensitivity analysis of model output">
                                        <b>[101]</b>
                                        Saltelli A,Tarantola S,Chan K-S.A quantitative model-independent method for global sensitivity analysis of model output[J].Technometrics,1999,41(1):39- 56
                                    </a>
                                </li>
                                <li id="1441">


                                    <a id="bibliography_102" title="Gevrey M,Dimopoulos I,Lek S.Review and comparison of methods to study the contribution of variables in artificial neural network models[J].Ecological Modelling,2003,160(3):249- 264" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012101044279&amp;v=MTU2ODd3WmVadUh5am1VTDNJSkZzUmJoUT1OaWZPZmJLN0h0RE9ybzlFWk84TERuc3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[102]</b>
                                        Gevrey M,Dimopoulos I,Lek S.Review and comparison of methods to study the contribution of variables in artificial neural network models[J].Ecological Modelling,2003,160(3):249- 264
                                    </a>
                                </li>
                                <li id="1443">


                                    <a id="bibliography_103" title="Engelbrecht A P,Cloete I,Zurada J M.Determining the significance of input parameters using sensitivity analysis[C] //Proc of the 3rd Int Workshop on Artificial Neural Networks.Berlin:Springer,1995:382- 388" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Determining the significance of input parameters using sensitivity analysis">
                                        <b>[103]</b>
                                        Engelbrecht A P,Cloete I,Zurada J M.Determining the significance of input parameters using sensitivity analysis[C] //Proc of the 3rd Int Workshop on Artificial Neural Networks.Berlin:Springer,1995:382- 388
                                    </a>
                                </li>
                                <li id="1445">


                                    <a id="bibliography_104" title="Harrington P D B,Wan C.Sensitivity analysis applied to artificial neural networks:What has my neural network actually learned?[J].Analytical Chemistry,1998,70:2983- 2990" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sensitivity analysis applied to artificial neural networks:What has my neural network actually learned?">
                                        <b>[104]</b>
                                        Harrington P D B,Wan C.Sensitivity analysis applied to artificial neural networks:What has my neural network actually learned?[J].Analytical Chemistry,1998,70:2983- 2990
                                    </a>
                                </li>
                                <li id="1447">


                                    <a id="bibliography_105" title="Sung A.Ranking importance of input parameters of neural networks[J].Expert Systems with Applications,1998,15(3/4):405- 411" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501644719&amp;v=MDE1MjFCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUwzSUpGc1JiaFE9TmlmT2ZiSzdIdEROcW85RVl1OExDMzB3bw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[105]</b>
                                        Sung A.Ranking importance of input parameters of neural networks[J].Expert Systems with Applications,1998,15(3/4):405- 411
                                    </a>
                                </li>
                                <li id="1449">


                                    <a id="bibliography_106" title="Robnik-Šikonja M,Kononenko I.Explaining classifications for individual instances[J].IEEE Transactions on Knowledge and Data Engineering,2008,20(5):589- 600" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining classifications for individual instances">
                                        <b>[106]</b>
                                        Robnik-Šikonja M,Kononenko I.Explaining classifications for individual instances[J].IEEE Transactions on Knowledge and Data Engineering,2008,20(5):589- 600
                                    </a>
                                </li>
                                <li id="1451">


                                    <a id="bibliography_107" title="Li Jiwei,Monroe W,Jurafsky D.Understanding neural networks through representation erasure[J].arXiv preprint arXiv:1612.08220,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding neural networks through representation erasure">
                                        <b>[107]</b>
                                        Li Jiwei,Monroe W,Jurafsky D.Understanding neural networks through representation erasure[J].arXiv preprint arXiv:1612.08220,2016
                                    </a>
                                </li>
                                <li id="1453">


                                    <a id="bibliography_108" title="Ribeiro M T,Singh S,Guestrin C.Nothing else matters:Model-agnostic explanations by identifying prediction invariance[J].arXiv preprint arXiv:1611.05817,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nothing else matters:Model-agnostic explanations by identifying prediction invariance">
                                        <b>[108]</b>
                                        Ribeiro M T,Singh S,Guestrin C.Nothing else matters:Model-agnostic explanations by identifying prediction invariance[J].arXiv preprint arXiv:1611.05817,2016
                                    </a>
                                </li>
                                <li id="1455">


                                    <a id="bibliography_109" title="Landecker W,Thomure M D,Bettencourt L M,et al.Interpreting individual classifications of hierarchical networks[C] //Proc of the IEEE Symp on Computational Intelligence and Data Mining.Piscataway,NJ:IEEE,2013:32- 38" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpreting individual classifications of hierarchical networks">
                                        <b>[109]</b>
                                        Landecker W,Thomure M D,Bettencourt L M,et al.Interpreting individual classifications of hierarchical networks[C] //Proc of the IEEE Symp on Computational Intelligence and Data Mining.Piscataway,NJ:IEEE,2013:32- 38
                                    </a>
                                </li>
                                <li id="1457">


                                    <a id="bibliography_110" title="Ding Yanzhuo,Liu Yang,Luan Huanbo,et al.Visualizing and understanding neural machine translation[C] //Proc of the 55th Annual Meeting of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2017:1150- 1159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding neural machine translation">
                                        <b>[110]</b>
                                        Ding Yanzhuo,Liu Yang,Luan Huanbo,et al.Visualizing and understanding neural machine translation[C] //Proc of the 55th Annual Meeting of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2017:1150- 1159.
                                    </a>
                                </li>
                                <li id="1459">


                                    <a id="bibliography_111" title="Arras L,Horn F,Montavon G,et al.What is relevant in a text document?:An interpretable machine learning approach[J].PloS One,2017,12(8):e0181142" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What is relevant in a text document?an interpretable machine learning approach">
                                        <b>[111]</b>
                                        Arras L,Horn F,Montavon G,et al.What is relevant in a text document?:An interpretable machine learning approach[J].PloS One,2017,12(8):e0181142
                                    </a>
                                </li>
                                <li id="1461">


                                    <a id="bibliography_112" title="Carter S.Exploring neural networks with activation atlases[OL].[2019-06-11].https://ai.googleblog.com/2019/03/exploring-neural-networks.html" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring neural networks with activation atlases[OL]">
                                        <b>[112]</b>
                                        Carter S.Exploring neural networks with activation atlases[OL].[2019-06-11].https://ai.googleblog.com/2019/03/exploring-neural-networks.html
                                    </a>
                                </li>
                                <li id="1463">


                                    <a id="bibliography_113" title="Dosovitskiy A,Brox T.Inverting visual representations with convolutional networks[C] //Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:4829- 4837" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverting Visual Representations with Convolutional Networks">
                                        <b>[113]</b>
                                        Dosovitskiy A,Brox T.Inverting visual representations with convolutional networks[C] //Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:4829- 4837
                                    </a>
                                </li>
                                <li id="1465">


                                    <a id="bibliography_114" title="Zhou Bolei,Khosla A,Lapedriza A,et al.Object detectors emerge in deep scene CNNs[J].arXiv preprint arXiv:1412.6856,2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object detectors emerge in deep scene CNNs">
                                        <b>[114]</b>
                                        Zhou Bolei,Khosla A,Lapedriza A,et al.Object detectors emerge in deep scene CNNs[J].arXiv preprint arXiv:1412.6856,2014
                                    </a>
                                </li>
                                <li id="1467">


                                    <a id="bibliography_115" title="Shrikumar A,Greenside P,Kundaje A.Learning important features through propagating activation differences[C] //Proc of the 34th Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,2017:3145- 3153" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning important features through propagating activation differences">
                                        <b>[115]</b>
                                        Shrikumar A,Greenside P,Kundaje A.Learning important features through propagating activation differences[C] //Proc of the 34th Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,2017:3145- 3153
                                    </a>
                                </li>
                                <li id="1469">


                                    <a id="bibliography_116" title="Lapuschkin S,Binder A,Montavon G,et al.Analyzing classifiers:Fisher vectors and deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2912- 2920" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analyzing classifiers:fisher vectors and deep neural networks">
                                        <b>[116]</b>
                                        Lapuschkin S,Binder A,Montavon G,et al.Analyzing classifiers:Fisher vectors and deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2912- 2920
                                    </a>
                                </li>
                                <li id="1471">


                                    <a id="bibliography_117" title="Cadamuro G,Gilad-Bachrach R,Zhu X.Debugging machine learning models[C] //Proc of the 33rd ICML Workshop on Reliable Machine Learning in the Wild.Tahoe,CA:International Machine Learning Society,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Debugging machine learning models">
                                        <b>[117]</b>
                                        Cadamuro G,Gilad-Bachrach R,Zhu X.Debugging machine learning models[C] //Proc of the 33rd ICML Workshop on Reliable Machine Learning in the Wild.Tahoe,CA:International Machine Learning Society,2016
                                    </a>
                                </li>
                                <li id="1473">


                                    <a id="bibliography_118" title="Kulesza T,Burnett M,Wong W-K,et al.Principles of explanatory debugging to personalize interactive machine learning[C] //Proc of the 20th ACM Int Conf on Intelligent User Interfaces.New York:ACM,2015:126- 137" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Principles of explanatory debugging to personalize interactive machine learning">
                                        <b>[118]</b>
                                        Kulesza T,Burnett M,Wong W-K,et al.Principles of explanatory debugging to personalize interactive machine learning[C] //Proc of the 20th ACM Int Conf on Intelligent User Interfaces.New York:ACM,2015:126- 137
                                    </a>
                                </li>
                                <li id="1475">


                                    <a id="bibliography_119" title="Kulesza T,Stumpf S,Burnett M,et al.Explanatory debugging:Supporting end-user debugging of machine-learned programs[C] //Proc of the IEEE Symp on Visual Languages and Human-Centric Computing.Piscataway,NJ:IEEE,2010:41- 48" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explanatory debugging:Supporting end-user debugging of machine-learned programs">
                                        <b>[119]</b>
                                        Kulesza T,Stumpf S,Burnett M,et al.Explanatory debugging:Supporting end-user debugging of machine-learned programs[C] //Proc of the IEEE Symp on Visual Languages and Human-Centric Computing.Piscataway,NJ:IEEE,2010:41- 48
                                    </a>
                                </li>
                                <li id="1477">


                                    <a id="bibliography_120" title="Bastani O,Kim C,Bastani H.Interpretability via model extraction[J].arXiv preprint arXiv:1706.09773,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretability via model extraction">
                                        <b>[120]</b>
                                        Bastani O,Kim C,Bastani H.Interpretability via model extraction[J].arXiv preprint arXiv:1706.09773,2017
                                    </a>
                                </li>
                                <li id="1479">


                                    <a id="bibliography_121" title="Kahng M,Andrews P Y,Kalro A,et al.Activis:Visual exploration of industry-scale deep neural network models[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):88- 97" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Activis:Visual exploration of industry-scale deep neural network models">
                                        <b>[121]</b>
                                        Kahng M,Andrews P Y,Kalro A,et al.Activis:Visual exploration of industry-scale deep neural network models[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):88- 97
                                    </a>
                                </li>
                                <li id="1481">


                                    <a id="bibliography_122" title="Strobelt H,Gehrmann S,Pfister H,et al.Lstmvis:A tool for visual analysis of hidden state dynamics in recurrent neural networks[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):667- 676" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lstmvis:A tool for visual analysis of hidden state dynamics in recurrent neural networks">
                                        <b>[122]</b>
                                        Strobelt H,Gehrmann S,Pfister H,et al.Lstmvis:A tool for visual analysis of hidden state dynamics in recurrent neural networks[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):667- 676
                                    </a>
                                </li>
                                <li id="1483">


                                    <a id="bibliography_123" title="Wongsuphasawat K,Smilkov D,Wexler J,et al.Visualizing dataflow graphs of deep learning models in tensorflow[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):1- 12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing dataflow graphs of deep learning models in TensorFlow">
                                        <b>[123]</b>
                                        Wongsuphasawat K,Smilkov D,Wexler J,et al.Visualizing dataflow graphs of deep learning models in tensorflow[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):1- 12
                                    </a>
                                </li>
                                <li id="1485">


                                    <a id="bibliography_124" title="Zhang Jiawei,Wang Yang,Molino P,et al.Manifold:A model-agnostic framework for interpretation and diagnosis of machine learning models[J].IEEE Transactions on Visualization and Computer Graphics,2019,25(1):364- 373" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manifold:A model-agnostic framework for interpretation and diagnosis of machine learning models">
                                        <b>[124]</b>
                                        Zhang Jiawei,Wang Yang,Molino P,et al.Manifold:A model-agnostic framework for interpretation and diagnosis of machine learning models[J].IEEE Transactions on Visualization and Computer Graphics,2019,25(1):364- 373
                                    </a>
                                </li>
                                <li id="1487">


                                    <a id="bibliography_125" title="Krause J,Perer A,Ng K.Interacting with predictions:Visual inspection of black-box machine learning models[C] //Proc of the ACM CHI Conf on Human Factors in Computing Systems.New York:ACM,2016:5686- 5697" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interacting with predictions:Visual inspection of black-box machine learning models">
                                        <b>[125]</b>
                                        Krause J,Perer A,Ng K.Interacting with predictions:Visual inspection of black-box machine learning models[C] //Proc of the ACM CHI Conf on Human Factors in Computing Systems.New York:ACM,2016:5686- 5697
                                    </a>
                                </li>
                                <li id="1489">


                                    <a id="bibliography_126" title="Krause J,Dasgupta A,Swartz J,et al.A workflow for visual diagnostics of binary classifiers using instance-level explanations[C] //Proc of the IEEE Conf on Visual Analytics Science and Technology.Piscataway,NJ:IEEE,2017:162- 172" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A workflow for visual diagnostics of binary classifiers using instance-level explanations">
                                        <b>[126]</b>
                                        Krause J,Dasgupta A,Swartz J,et al.A workflow for visual diagnostics of binary classifiers using instance-level explanations[C] //Proc of the IEEE Conf on Visual Analytics Science and Technology.Piscataway,NJ:IEEE,2017:162- 172
                                    </a>
                                </li>
                                <li id="1491">


                                    <a id="bibliography_127" title="Paiva J G S,Schwartz W R,Pedrini H,et al.An approach to supporting incremental visual data classification[J].IEEE Transactions on Visualization and Computer Graphics,2015,21(1):4- 17" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An approach to supporting incremental visual data classification">
                                        <b>[127]</b>
                                        Paiva J G S,Schwartz W R,Pedrini H,et al.An approach to supporting incremental visual data classification[J].IEEE Transactions on Visualization and Computer Graphics,2015,21(1):4- 17
                                    </a>
                                </li>
                                <li id="1493">


                                    <a id="bibliography_128" title="Brooks M,Amershi S,Lee B,et al.FeatureInsight:Visual support for error-driven feature ideation in text classification[C] //Proc of the IEEE Conf on Visual Analytics Science and Technology.Piscataway,NJ:IEEE,2015:105- 112" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature insight:Visual support for error-driven feature ideation in text classification">
                                        <b>[128]</b>
                                        Brooks M,Amershi S,Lee B,et al.FeatureInsight:Visual support for error-driven feature ideation in text classification[C] //Proc of the IEEE Conf on Visual Analytics Science and Technology.Piscataway,NJ:IEEE,2015:105- 112
                                    </a>
                                </li>
                                <li id="1495">


                                    <a id="bibliography_129" title="Arvaniti E,Fricker K S,Moret M,et al.Automated Gleason grading of prostate cancer tissue microarrays via deep learning[J].Scientific Reports,2018,8(1):12054- 12054" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated Gleason grading of prostate cancer tissue microarrays via deep learning">
                                        <b>[129]</b>
                                        Arvaniti E,Fricker K S,Moret M,et al.Automated Gleason grading of prostate cancer tissue microarrays via deep learning[J].Scientific Reports,2018,8(1):12054- 12054
                                    </a>
                                </li>
                                <li id="1497">


                                    <a id="bibliography_130" title="Aspuru-Guzik A,Lindh R,Reiher M.The matter simulation (r) evolution[J].ACS Central Science,2018,4(2):144- 152" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The matter simulation (r) evolution">
                                        <b>[130]</b>
                                        Aspuru-Guzik A,Lindh R,Reiher M.The matter simulation (r) evolution[J].ACS Central Science,2018,4(2):144- 152
                                    </a>
                                </li>
                                <li id="1499">


                                    <a id="bibliography_131" title="Boukouvalas Z,Elton D C,Chung P W,et al.Independent vector analysis for data fusion prior to molecular property prediction with machine learning[J].arXiv preprint arXiv:1811.00628,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Independent vector analysis for data fusion prior to molecular property prediction with machine learning">
                                        <b>[131]</b>
                                        Boukouvalas Z,Elton D C,Chung P W,et al.Independent vector analysis for data fusion prior to molecular property prediction with machine learning[J].arXiv preprint arXiv:1811.00628,2018
                                    </a>
                                </li>
                                <li id="1501">


                                    <a id="bibliography_132" title="H&#228;se F,Galv&#225;n I F,Aspuru-Guzik A,et al.How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry[J].Chemical Science,2019,10(8):2298- 2307" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry">
                                        <b>[132]</b>
                                        H&#228;se F,Galv&#225;n I F,Aspuru-Guzik A,et al.How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry[J].Chemical Science,2019,10(8):2298- 2307
                                    </a>
                                </li>
                                <li id="1503">


                                    <a id="bibliography_133" title="Sch&#252;tt K T,Arbabzadah F,Chmiela S,et al.Quantum-chemical insights from deep tensor neural networks[J].arXiv preprint arXiv:1609.08529,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quantum-chemical insights from deep tensor neural networks">
                                        <b>[133]</b>
                                        Sch&#252;tt K T,Arbabzadah F,Chmiela S,et al.Quantum-chemical insights from deep tensor neural networks[J].arXiv preprint arXiv:1609.08529,2016
                                    </a>
                                </li>
                                <li id="1505">


                                    <a id="bibliography_134" title="Yue Tianwei,Wang Haohan.Deep learning for genomics:A concise overview[J].arXiv preprint arXiv:1802.00810,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for genomics:A concise overview">
                                        <b>[134]</b>
                                        Yue Tianwei,Wang Haohan.Deep learning for genomics:A concise overview[J].arXiv preprint arXiv:1802.00810,2018
                                    </a>
                                </li>
                                <li id="1507">


                                    <a id="bibliography_135" title="Angermueller C,Lee H J,Reik W,et al.DeepCpG:Accurate prediction of single-cell DNA methylation states using deep learning[J].Genome Biology,2017,18(1):67" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD2923078DA560F32A2CC739EA38FCD5E5&amp;v=MjM1MzNsZkNwYlEzNU5waHdMMjZ4SzQ9Tmo3QmFyR3hITkxNcUljeEZlNEpEQW82eldjUm1VeDZTM2FYM1JFOUQ4SGdRTSthQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[135]</b>
                                        Angermueller C,Lee H J,Reik W,et al.DeepCpG:Accurate prediction of single-cell DNA methylation states using deep learning[J].Genome Biology,2017,18(1):67
                                    </a>
                                </li>
                                <li id="1509">


                                    <a id="bibliography_136" title="Quang D,Xie Xiaohui.DanQ:A hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences[J].Nucleic Acids Research,2016,44(11):107" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dan Q a hybrid convolutional and recurren deep neural network for quantifying the function of DNAsequences">
                                        <b>[136]</b>
                                        Quang D,Xie Xiaohui.DanQ:A hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences[J].Nucleic Acids Research,2016,44(11):107
                                    </a>
                                </li>
                                <li id="1511">


                                    <a id="bibliography_137" title="Lanchantin J,Singh R,Wang B,et al.Deep motif dashboard:Visualizing and understanding genomic sequences using deep neural networks[C] //Proc of the 22nd Pacific Symp on Biocomputing.Singapore:World Scientific Publishing Co,2017:254- 265" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep motif dashboard:Visualizing and understanding genomic sequences using deep neural networks">
                                        <b>[137]</b>
                                        Lanchantin J,Singh R,Wang B,et al.Deep motif dashboard:Visualizing and understanding genomic sequences using deep neural networks[C] //Proc of the 22nd Pacific Symp on Biocomputing.Singapore:World Scientific Publishing Co,2017:254- 265
                                    </a>
                                </li>
                                <li id="1513">


                                    <a id="bibliography_138" title="Alipanahi B,Delong A,Weirauch M T,et al.Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning[J].Nature Biotechnology,2015,33(8):831- 838" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning">
                                        <b>[138]</b>
                                        Alipanahi B,Delong A,Weirauch M T,et al.Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning[J].Nature Biotechnology,2015,33(8):831- 838
                                    </a>
                                </li>
                                <li id="1515">


                                    <a id="bibliography_139" title="Gulia N,Singh S,Sapra L.A study on different classification models for knowledge discovery[J].International Journal of Computer Science Mob Computer,2015,4(6):241- 248" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A study on different classification models for knowledge discovery">
                                        <b>[139]</b>
                                        Gulia N,Singh S,Sapra L.A study on different classification models for knowledge discovery[J].International Journal of Computer Science Mob Computer,2015,4(6):241- 248
                                    </a>
                                </li>
                                <li id="1517">


                                    <a id="bibliography_140" title="Helma C.Data mining and knowledge discovery in predictive toxicology[J].SAR and QSAR in Environmental Research,2004,15(5-6):367- 383" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD79E10CDA92B669B1067B43E6FB474986&amp;v=MjE1MDBhQnVIWWZPR1FsZkNwYlEzNU5waHdMMjZ4SzQ9TmpuQmFyU3hhOURNM1BzMGJlbDlDbm93dlJjVDdEZ1BUSHlYcW1SSGZiV1FUTEtaQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[140]</b>
                                        Helma C.Data mining and knowledge discovery in predictive toxicology[J].SAR and QSAR in Environmental Research,2004,15(5-6):367- 383
                                    </a>
                                </li>
                                <li id="1519">


                                    <a id="bibliography_141" title="Bertini E,Lalanne D.Investigating and reflecting on the integration of automatic data analysis and visualization in knowledge discovery[J].ACM SIGKDD Explorations Newsletter,2010,11(2):9- 18" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000085573&amp;v=MDgxODRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMM0lKRnNSYmhRPU5pZklZN0s3SHRqTnI0OUZaT01LQ1hzNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[141]</b>
                                        Bertini E,Lalanne D.Investigating and reflecting on the integration of automatic data analysis and visualization in knowledge discovery[J].ACM SIGKDD Explorations Newsletter,2010,11(2):9- 18
                                    </a>
                                </li>
                                <li id="1521">


                                    <a id="bibliography_142" title="Fayyad U,Piatetsky-Shapiro G,Smyth P.From data mining to knowledge discovery in databases[J].AI Magazine,1996,17(3):37- 37" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From data mining to knowledge discovery in databases">
                                        <b>[142]</b>
                                        Fayyad U,Piatetsky-Shapiro G,Smyth P.From data mining to knowledge discovery in databases[J].AI Magazine,1996,17(3):37- 37
                                    </a>
                                </li>
                                <li id="1523">


                                    <a id="bibliography_143" title="Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[J].arXiv preprint arXiv:1412.6572,2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining and harnessing adversarial examples">
                                        <b>[143]</b>
                                        Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[J].arXiv preprint arXiv:1412.6572,2014
                                    </a>
                                </li>
                                <li id="1525">


                                    <a id="bibliography_144" title="Carlini N,Wagner D.Towards evaluating the robustness of neural networks[C] //Proc of the 38th IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:39- 57" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards evaluating the robustness of neural networks">
                                        <b>[144]</b>
                                        Carlini N,Wagner D.Towards evaluating the robustness of neural networks[C] //Proc of the 38th IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:39- 57
                                    </a>
                                </li>
                                <li id="1527">


                                    <a id="bibliography_145" title="Tao Guanhong,Ma Shiqing,Liu Yingqi,et al.Attacks meet interpretability:Attribute-steered detection of adversarial samples[C] //Proc of the 32nd Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2018:7717- 7728" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attacks meet interpretability:Attribute-steered detection of adversarial samples">
                                        <b>[145]</b>
                                        Tao Guanhong,Ma Shiqing,Liu Yingqi,et al.Attacks meet interpretability:Attribute-steered detection of adversarial samples[C] //Proc of the 32nd Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2018:7717- 7728
                                    </a>
                                </li>
                                <li id="1529">


                                    <a id="bibliography_146" title="Liu Ninghao,Yang Hongxia,Hu Xia.Adversarial detection with model interpretation[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM,2018:1803- 1811" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial detection with model interpretation">
                                        <b>[146]</b>
                                        Liu Ninghao,Yang Hongxia,Hu Xia.Adversarial detection with model interpretation[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM,2018:1803- 1811
                                    </a>
                                </li>
                                <li id="1531">


                                    <a id="bibliography_147" title="Shi Chenghui,Xu Xiaogang,Ji Shouling,et al.Adversarial CAPTCHAs[J].arXiv preprint arXiv:1901.01107,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial CAPTCHAs">
                                        <b>[147]</b>
                                        Shi Chenghui,Xu Xiaogang,Ji Shouling,et al.Adversarial CAPTCHAs[J].arXiv preprint arXiv:1901.01107,2019
                                    </a>
                                </li>
                                <li id="1533">


                                    <a id="bibliography_148" title="Papernot N,Mcdaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the 1st IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2016:372- 387" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">
                                        <b>[148]</b>
                                        Papernot N,Mcdaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the 1st IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2016:372- 387
                                    </a>
                                </li>
                                <li id="1535">


                                    <a id="bibliography_149" title="Li Xurong,Ji Shouling,Han Meng,et al.Adversarial examples versus cloud-based detectors:A black-box empirical study[J].arXiv preprint arXiv:1901.01223,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial examples versus cloud-based detectors:A black-box empirical study">
                                        <b>[149]</b>
                                        Li Xurong,Ji Shouling,Han Meng,et al.Adversarial examples versus cloud-based detectors:A black-box empirical study[J].arXiv preprint arXiv:1901.01223,2019
                                    </a>
                                </li>
                                <li id="1537">


                                    <a id="bibliography_150" title="Papernot N,Mcdaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 12th ACM Asia Conf on Computer and Communications Security.New York:ACM,2017:506- 519" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Practical black-box attacks against machine learning">
                                        <b>[150]</b>
                                        Papernot N,Mcdaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 12th ACM Asia Conf on Computer and Communications Security.New York:ACM,2017:506- 519
                                    </a>
                                </li>
                                <li id="1539">


                                    <a id="bibliography_151" title="Ghorbani A,Abid A,Zou J.Interpretation of neural networks is fragile[J].arXiv preprint arXiv:1710.10547,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretation of neural networks is fragile">
                                        <b>[151]</b>
                                        Ghorbani A,Abid A,Zou J.Interpretation of neural networks is fragile[J].arXiv preprint arXiv:1710.10547,2017
                                    </a>
                                </li>
                                <li id="1541">


                                    <a id="bibliography_152" title="Zhang Xinyang,Wang Ningfei,Ji Shouling,et al.Interpretable deep learning under fire[C] //Proc of the 29th USENIX Security Symp.Berkeley,CA:USENIX Association,2020" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interpretable deep learning under fire">
                                        <b>[152]</b>
                                        Zhang Xinyang,Wang Ningfei,Ji Shouling,et al.Interpretable deep learning under fire[C] //Proc of the 29th USENIX Security Symp.Berkeley,CA:USENIX Association,2020
                                    </a>
                                </li>
                                <li id="1543">


                                    <a id="bibliography_153" >
                                        <b>[153]</b>
                                    Dabkowski P,Gal Y.Real time image saliency for black box classifiers[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2017:6967- 6976■Ji Shouling,born in 1986.Received a PhD in electrical and computer engineering from Georgia Institute of Technology and a PhD in computer science from Georgia State University.Currently ZJU 100-Young Professor in the College of Computer Science and Technology at Zhejiang University and research faculty in the School of Electrical and Computer Engineering at Georgia Institute of Technology.Member of IEEE and ACM.His main research interests include big data security and privacy,big data driven decurity and privacy,adversarial learning,graph theory and algorithms,and wireless networks.■Li Jinfeng,born in 1994.MSc candidate in the College of Computer Science and Technology at Zhejiang University.His main research interests include big data driven security,AI security and deep learning interpretability.■Du Tianyu,born in 1996.PhD candidate in the College of Computer Science and Technology at Zhejiang University.Her main research interests include big data driven security,adversarial learning and AI security.■Li Bo,born in 1989.Assistant professor in the Department of Computer Science at University of Illinois at Urbana-Champaign,and recipient of the Symantec Research Labs Fellowship.Member of IEEE and ACM.Her main research interests include theoretical and practical aspects of security,machine learning,privacy,game theory,and blockchain.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-10-28 10:35</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(10),2071-2096 DOI:10.7544/issn1000-1239.2019.20190540            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>机器学习模型可解释性方法、应用与安全研究综述</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BA%AA%E5%AE%88%E9%A2%86&amp;code=39798184&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">纪守领</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%BF%9B%E9%94%8B&amp;code=40227235&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李进锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%9C%E5%A4%A9%E5%AE%87&amp;code=40224833&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杜天宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%8D%9A&amp;code=43050186&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李博</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E5%AE%89%E5%85%A8%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0157820&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江大学计算机科学与技术学院网络空间安全研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BC%8A%E5%88%A9%E8%AF%BA%E4%BC%8A%E5%A4%A7%E5%AD%A6%E9%A6%99%E6%A7%9F%E5%88%86%E6%A0%A1%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0839666&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">伊利诺伊大学香槟分校计算机科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>尽管机器学习在许多领域取得了巨大的成功,但缺乏可解释性严重限制了其在现实任务尤其是安全敏感任务中的广泛应用.为了克服这一弱点,许多学者对如何提高机器学习模型可解释性进行了深入的研究,并提出了大量的解释方法以帮助用户理解模型内部的工作机制.然而,可解释性研究还处于初级阶段,依然还有大量的科学问题尚待解决.并且,不同的学者解决问题的角度不同,对可解释性赋予的含义也不同,所提出的解释方法也各有侧重.迄今为止,学术界对模型可解释性仍缺乏统一的认识,可解释性研究的体系结构尚不明确.在综述中,回顾了机器学习中的可解释性问题,并对现有的研究工作进行了系统的总结和科学的归类.同时,讨论了可解释性相关技术的潜在应用,分析了可解释性与可解释机器学习的安全性之间的关系,并且探讨了可解释性研究当前面临的挑战和未来潜在的研究方向,以期进一步推动可解释性研究的发展和应用.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可解释性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">解释方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可解释机器学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%89%E5%85%A8%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安全性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Ji Shouling,born in 1986.Received a PhDin electrical and computer engineering fromGeorgia Institute of Technology and a PhDin computer science from Georgia StateUniversity. Currently ZJU 100-YoungProfessor in the College of Computer Scienceand Technology at Zhejiang University andresearch faculty in the School of Electricaland Computer Engineering at GeorgiaInstitute of Technology.Member of IEEEand ACM.His main research interestsinclude big data security and privacy,bigdata driven decurity and privacy,adversarial learning, graph theory andalgorithms,and wireless networks.&lt;image id="619" type="formula" href="images/JFYZ201910004_61900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Li Jinfeng,born in 1994.MSc candidate inthe College of Computer Science andTechnology at Zhejiang University.Hismain research interests include big datadriven security, AI security and deeplearning interpretability.lijinfeng0713@zju.edu.cn&lt;image id="621" type="formula" href="images/JFYZ201910004_62100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Du Tianyu,born in 1996.PhD candidate inthe College of Computer Science andTechnology at Zhejiang University.Hermain research interests include big datadriven security,adversarial learning andAI security.&lt;image id="623" type="formula" href="images/JFYZ201910004_62300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Li Bo,born in 1989.Assistant professor inthe Department of Computer Science atUniversity of Illinois at Urbana-Champaign,and recipient of the SymantecResearch Labs Fellowship. Member ofIEEE and ACM. Her main researchinterests include theoretical and practicalaspects of security, machine learning,privacy,game theory,and blockchain.&lt;image id="625" type="formula" href="images/JFYZ201910004_62500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61772466,U1836202);</span>
                                <span>浙江省自然科学基金杰出青年项目(LR19F020003);</span>
                                <span>浙江省科技计划项目(2017C01055);</span>
                    </p>
            </div>
                    <h1><b>Survey on Techniques, Applications and Security of Machine Learning Interpretability</b></h1>
                    <h2>
                    <span>Ji Shouling</span>
                    <span>Li Jinfeng</span>
                    <span>Du Tianyu</span>
                    <span>Li Bo</span>
            </h2>
                    <h2>
                    <span>Institute of Cyberspace Research and College of Computer Science and Technology, Zhejiang University</span>
                    <span>Department of Computer Science, University of Illinois at Urbana-Champaign</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>While machine learning has achieved great success in various domains, the lack of interpretability has limited its widespread applications in real-world tasks, especially security-critical tasks. To overcome this crucial weakness, intensive research on improving the interpretability of machine learning models has emerged, and a plethora of interpretation methods have been proposed to help end users understand its inner working mechanism. However, the research on model interpretation is still in its infancy, and there are a large amount of scientific issues to be resolved. Furthermore, different researchers have different perspectives on solving the interpretation problem and give different definitions for interpretability, and the proposed interpretation methods also have different emphasis. Till now, the research community still lacks a comprehensive understanding of interpretability as well as a scientific guide for the research on model interpretation. In this survey, we review the explanatory problems in machine learning, and make a systematic summary and scientific classification of the existing research works. At the same time, we discuss the potential applications of interpretation related technologies, analyze the relationship between interpretability and the security of interpretable machine learning, and discuss the current research challenges and potential future research directions, aiming at providing necessary help for future researchers to facilitate the research and application of model interpretability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=interpretability&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">interpretability;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=interpretation%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">interpretation method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=interpretable%20machine%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">interpretable machine learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=security&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">security;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-06-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61772466,U1836202);</span>
                                <span>the Natural Science Foundation for Distinguished Young Scholars of Zhejiang Province(LR19F020003);</span>
                                <span>the Provincial Key Research and Development Program of Zhejiang(2017C01055);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="313">近年来,机器学习相关技术在计算机视觉、自然语言处理、语音识别等多个领域取得了巨大的成功,机器学习模型也被广泛地应用到一些重要的现实任务中,如人脸识别<citation id="1548" type="reference"><link href="1239" rel="bibliography" /><link href="1241" rel="bibliography" /><link href="1243" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>、自动驾驶<citation id="1545" type="reference"><link href="1245" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、恶意软件检测<citation id="1546" type="reference"><link href="1247" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和智慧医疗分析<citation id="1547" type="reference"><link href="1249" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等.在某些场景中,机器学习模型的表现甚至超过了人类.</p>
                </div>
                <div class="p1">
                    <p id="314">尽管机器学习在许多有意义的任务中胜过人类,但由于缺乏可解释性,其表现和应用也饱受质疑<citation id="1549" type="reference"><link href="1251" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.对于普通用户而言机器学习模型尤其是深度神经网络(deep neural networks, DNN)模型如同黑盒一般,给它一个输入,其反馈一个决策结果,没人能确切地知道它背后的决策依据以及它做出的决策是否可靠.而缺乏可解释性将有可能给实际任务中尤其是安全敏感任务中的许多基于DNN的现实应用带来严重的威胁.比如说,缺乏可解释性的自动医疗诊断模型可能给患者带来错误的治疗方案,甚至严重威胁患者的生命安全.此外,最近的研究表明,DNN本身也面临着多种安全威胁——恶意构造的对抗性样本可以轻易让DNN模型分类出错<citation id="1550" type="reference"><link href="1253" rel="bibliography" /><link href="1255" rel="bibliography" /><link href="1257" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>,而他们针对对抗样本的脆弱性同样也缺乏可解释性.因此,缺乏可解释性已经成为机器学习在现实任务中的进一步发展和应用的主要障碍之一.</p>
                </div>
                <div class="p1">
                    <p id="315">为了提高机器学习模型的可解释性和透明性,建立用户与决策模型之间的信任关系,消除模型在实际部署应用中的潜在威胁,近年来学术界和工业界进行了广泛和深入的研究并且提出了一系列的机器学习模型可解释性方法.然而,由于不同的研究者解决问题的角度不同,因而给“可解释性”赋予的含义也不同,所提出的可解释性方法也各有侧重.因此,亟需对现有工作进行系统的整理和科学的总结、归类,以促进该领域的研究.</p>
                </div>
                <div class="p1">
                    <p id="316">在本文中,我们首先详细地阐述可解释性的定义和所解决的问题.然后,我们对现有的可解释性方法进行系统的总结和归类,并讨论相关方法的局限性.接着,我们简单地介绍模型可解释性相关技术的实际应用场景,同时详细地分析可解释性中的安全问题.最后,我们讨论模型可解释性相关研究所面临的挑战以及未来可行的研究方向.</p>
                </div>
                <h3 id="317" name="317" class="anchor-tag"><b>1 机器学习可解释性问题</b></h3>
                <div class="p1">
                    <p id="318">在介绍具体的可解释问题与相应的解决方法之前,我们先简单地介绍什么是可解释性以及为什么需要可解释性.在数据挖掘和机器学习场景中,可解释性被定义为向人类解释或以呈现可理解的术语的能力<citation id="1551" type="reference"><link href="1259" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>.从本质上讲,可解释性是人类与决策模型之间的接口,它既是决策模型的准确代理,又是人类所可以理解的<citation id="1552" type="reference"><link href="1261" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.在自上而下的机器学习任务中,模型通常建立在一组统计规则和假设之上,因而可解释性至关重要,因为它是所定义的规则和假设的基石.此外,模型可解释性是验证假设是否稳健,以及所定义的规则是否完全适合任务的重要手段.与自上而下的任务不同,自下而上的机器学习通常对应于手动和繁重任务的自动化,即给定一批训练数据,通过最小化学习误差,让模型自动地学习输入数据与输出类别之间的映射关系.在自下而上的学习任务中,由于模型是自动构建的,我们不清楚其学习过程,也不清楚其工作机制,因此,可解释性旨在帮助人们理解机器学习模型是如何学习的,它从数据中学到了什么,针对每一个输入它为什么会做出如此决策以及它所做的决策是否可靠.</p>
                </div>
                <div class="p1">
                    <p id="319">在机器学习任务中,除了可解释性,常常会提到另外2个概念:模型准确性(accuracy)和模型复杂度(model complexity).准确性反映了模型的拟合能力以及在某种程度上准确预测未知样本的能力.模型复杂度反映了模型结构上的复杂性,只与模型本身有关,与模型训练数据无关.在线性模型中,模型的复杂度由非零权重的个数来体现;在决策树模型中,模型的复杂度由树的深度体现;在神经网络模型中,模型复杂度则由神经网络的深度、宽度、模型的参数量以及模型的计算量来体现<citation id="1553" type="reference"><link href="1263" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.模型的复杂度与模型准确性相关联,又与模型的可解释性相对立.通常情况下,结构简单的模型可解释性好,但拟合能力差,往往准确率不高.结构复杂的模型,拟合能力强,准确性高,但由于模型参数量大、工作机制复杂、透明性低,因而可解释性又相对较差.</p>
                </div>
                <div class="p1">
                    <p id="320">那么,在实际的学习任务中,我们是选择结构简单易于解释的模型然后训练它,还是训练复杂的最优模型然后开发可解释性技术解释它呢?基于这2种不同的选择,机器学习模型可解释性总体上可分为2类:事前(ante -hoc)可解释性和事后(post-hoc)可解释性.其中,ante -hoc可解释性指通过训练结构简单、可解释性好的模型或将可解释性结合到具体的模型结构中的自解释模型使模型本身具备可解释能力.post-hoc可解释性指通过开发可解释性技术解释已训练好的机器学习模型.根据解释目标和解释对象的不同,post-hoc可解释性又可分为全局可解释性(global interpretability)和局部可解释性(local interpreta-bility).全局可解释性旨在帮助人们理解复杂模型背后的整体逻辑以及内部的工作机制<citation id="1554" type="reference"><link href="1261" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,局部可解释性旨在帮助人们理解机器学习模型针对每一个输入样本的决策过程和决策依据<citation id="1555" type="reference"><link href="1265" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.</p>
                </div>
                <h3 id="321" name="321" class="anchor-tag"><b>2 ante</b> -<b>hoc可解释性</b></h3>
                <div class="p1">
                    <p id="322">ante -hoc可解释性指模型本身内置可解释性,即对于一个已训练好的学习模型,无需额外的信息就可以理解模型的决策过程或决策依据.模型的ante -hoc可解释性发生在模型训练之前,因而也称为事前可解释性.在学习任务中,我们通常采用结构简单、易于理解的自解释模型来实现ante -hoc可解释性,如朴素贝叶斯、线性回归、决策树、基于规则的模型.此外,我们也可以通过构建将可解释性直接结合到具体的模型结构中的学习模型来实现模型的内置可解释性<citation id="1556" type="reference"><link href="1267" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="323" name="323"><b>2.1 自解释模型</b></h4>
                <div class="p1">
                    <p id="324">对于自解释模型,我们从2个角度考虑模型的可解释性和透明性,即模型整体的可模拟性(simulatabi-lity)和模型单个组件的可分解性(decomposability).</p>
                </div>
                <div class="p1">
                    <p id="325">严格意义上来讲,如果我们认为某个模型是透明的,那么我们一定能从整体上完全理解一个模型,也应该能够将输入数据连同模型的参数一起,在合理的时间步骤内完成产生预测所需的每一个计算(即整体上的可模拟性).比如在朴素贝叶斯模型中,由于条件独立性的假设,我们可以将模型的决策过程转化为概率运算<citation id="1557" type="reference"><link href="1269" rel="bibliography" /><link href="1271" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>.在线性模型中,我们可以基于模型权重,通过矩阵运算线性组合样本的特征值,复现线性模型的决策过程,其中模型权重体现了特征之间的相关关系<citation id="1558" type="reference"><link href="1263" rel="bibliography" /><link href="1271" rel="bibliography" /><link href="1273" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>.而在决策树模型中,每一棵决策树都由表示特征或者属性的内部节点和表示类别的叶子节点组成,树的每一个分支代表一种可能的决策结果<citation id="1559" type="reference"><link href="1275" rel="bibliography" /><link href="1277" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>.决策树中每一条从根节点到不同叶子节点的路径都代表着一条不同的决策规则,因而每一棵决策树都可以被线性化为一系列由if-then形式组成的决策规则<citation id="1560" type="reference"><link href="1277" rel="bibliography" /><link href="1279" rel="bibliography" /><link href="1281" rel="bibliography" /><link href="1283" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>.因此,对于新的观测样本,我们可以通过从上到下遍历决策树,结合内部节点中的条件测试,基于if-then决策规则判定样本是否必须遵循左或右分支来模拟决策树的决策过程.</p>
                </div>
                <div class="p1">
                    <p id="326">自解释模型的可分解性要求模型的每个部分,包括模型结构、模型参数,模型的每一个输入以及每一维特征都允许直观的解释<citation id="1561" type="reference"><link href="1285" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>.在朴素贝叶斯模型中,由于条件独立性的假设,模型的预测可以很容易地转化为单个特征值的贡献——特征向量,特征向量的每一维表示每个特征值对最终分类结果的贡献程度<citation id="1562" type="reference"><link href="1271" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.在线性模型中,模型的权重直接反映了样本特征重要性,既包括重要性大小也包括相关性方向<citation id="1563" type="reference"><link href="1287" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.权重绝对值越大,则该特征对最终预测结果的贡献越大,反之则越小.如果权重值为正,则该特征与最终的预测类别正相关,反之则负相关.在决策树模型中,每个节点包含了特征值的条件测试,判定样本属于哪一分支以及使用哪一条规则,同时,每一条规则也为最终的分类结果提供了解释.此外,决策树模型自带的基于信息理论的筛选变量标准也有助于理解在模型决策过程中哪些变量起到了显著的作用.</p>
                </div>
                <div class="p1">
                    <p id="327">然而,由于人类认知的局限性,自解释模型的内置可解释性受模型的复杂度制约,这要求自解释模型结构一定不能过于复杂.因此,上述模型只有具有合理的规模才能具有有效的可解释性.例如对于高维的线性模型,其内置可解释性未必优于DNN.此外,对于决策树模型和基于规则的模型,如果树深度太深或者模型的规则太复杂,人类也未必能理解<citation id="1565" type="reference"><link href="1261" rel="bibliography" /><link href="1277" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">20</a>]</sup></citation>.但如果模型结构太简单,模型的拟合能力必然受限,因此模型可能会学习错误的特征来最小化在训练集上的经验误差,而这些特征可能与人类认知相违背,对于人类而言同样也很难解释.因此,自解释模型的内置可解释性与模型准确性之间始终存在一个平衡<citation id="1564" type="reference"><link href="1263" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="328" name="328"><b>2.2 广义加性模型</b></h4>
                <div class="p1">
                    <p id="329">在实际学习任务中,简单模型(如线性模型)因为准确率低而无法满足需要,而复杂模型的高准确率又通常是牺牲自身可解释性为代价的.作为一种折中,广义加性模型既能提高简单线性模型的准确率,又能保留线性模型良好的内置可解释性<citation id="1566" type="reference"><link href="1285" rel="bibliography" /><link href="1289" rel="bibliography" /><link href="1291" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">26</a>,<a class="sup">27</a>]</sup></citation>.广义加性模型一般形式为</p>
                </div>
                <div class="p1">
                    <p id="330"><i>g</i>(<i>y</i>)=<i>f</i><sub>1</sub>(<i>x</i><sub>1</sub>)+<i>f</i><sub>2</sub>(<i>x</i><sub>2</sub>)+…+<i>f</i><sub><i>n</i></sub>(<i>x</i><sub><i>n</i></sub>),</p>
                </div>
                <div class="p1">
                    <p id="331">其中,<i>f</i><sub><i>i</i></sub>(·)为单特征(single-feature)模型,也称为特征<i>x</i><sub><i>i</i></sub>对应的形函数(shape function).广义加性模型通过线性函数组合每一单特征模型得到最终的决策形式.在广义加性模型中,形函数本身可能是非线性的,每一个单特征模型可能采用一个非常复杂的形函数<i>f</i><sub><i>i</i></sub>(<i>x</i><sub><i>i</i></sub>)来量化每一个特征<i>x</i><sub><i>i</i></sub>与最终决策目标之间的关系,因而可以捕获到每一个特征与最终决策目标之间的非线性关系,因此广义加性模型准确率高于简单线性模型.又因为广义加性模型通过简单的线性函数组合每一个单特征模型得到最终的决策形式,消除了特征之间的相互作用,因此可以保留简单线性模型良好的可解释性,从而解决了复杂模型因为特征之间复杂的相关关系而削弱自身可解释性的问题.</p>
                </div>
                <div class="p1">
                    <p id="332">Lou等人<citation id="1567" type="reference"><link href="1285" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提出了一种基于有限大小的梯度提升树加性模型方法,该方法在回归和分类问题上精度显著优于传统方法,同时还保持了GAM模型的可解释性.Ravikumar等人<citation id="1568" type="reference"><link href="1293" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>结合稀疏线性建模和加性非参数回归的思想,提出了一种称之为稀疏加性模型的高维非参数回归分类方法,解决了高维空间中加性模型的拟合问题,同时基于ℓ<sub>1</sub>正则的稀疏性,可实现特征的有效选择. Poulin等人<citation id="1569" type="reference"><link href="1269" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>开发了一个图形化解释框架,提供了对加性模型的图形化解释,包括对模型整体的理解以及决策特征的可视化,以帮助建立用户与决策系统之间的信任关系.</p>
                </div>
                <h4 class="anchor-tag" id="333" name="333"><b>2.3 注意力机制</b></h4>
                <div class="p1">
                    <p id="334">神经网络模型由于模型结构复杂,算法透明性低,因而模型本身的可解释性差.因此,神经网络模型的自身可解释性只能通过额外引入可解释性模块来实现,一种有效的方法就是引入注意力机制(atten-tion mechanism)<citation id="1570" type="reference"><link href="1295" rel="bibliography" /><link href="1297" rel="bibliography" /><link href="1299" rel="bibliography" /><sup>[<a class="sup">29</a>,<a class="sup">30</a>,<a class="sup">31</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="335">注意力机制源于对人类认知神经学的研究.在认知科学中,由于信息处理的瓶颈,人脑可以有意或无意地从大量输入信息中选择小部分有用信息来重点处理,同时忽略其他可见的信息,这就是人脑的注意力机制<citation id="1571" type="reference"><link href="1301" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>.在计算能力有限的情况下,注意力机制是解决信息超载问题的一种有效手段,通过决定需要关注的输入部分,将有限的信息处理资源分配给更重要的任务.此外,注意力机制具有良好的可解释性,注意力权重矩阵直接体现了模型在决策过程中感兴趣的区域.</p>
                </div>
                <div class="p1">
                    <p id="336">近年来,基于注意力机制的神经网络已成为神经网络研究的一大热点,并在自然语言处理、计算机视觉、推荐系统等领域有着大量的应用<citation id="1572" type="reference"><link href="1303" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>.在自然语言处理领域,Bahdanau等人<citation id="1573" type="reference"><link href="1295" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>将注意力机制引入到基于编码器-解码器架构的机器翻译中,有效地提高了“英语-法语”翻译的性能.在编码阶段,机器翻译模型采用双向循环神经网络(Bi-RNN)将源语言编码到向量空间中;在解码阶段,注意力机制为解码器的隐藏状态分配不同的权重,从而允许解码器在生成法语翻译的每个步骤选择性地处理输入句子的不同部分.最后通过可视化注意力权重(如图1(a)所示),用户可以清楚地理解一种语言中的单词是如何依赖另一种语言中的单词进行正确翻译的.Yang等人<citation id="1574" type="reference"><link href="1305" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>将分层注意力机制引入到文本分类任务中,显著提高了情感分析任务的性能,同时注意力权重量化了每一个词的重要性,可帮助人们清晰地理解每一个词对最终情感分类结果的贡献(如图1(b)所示).在计算机视觉领域,Xu等人<citation id="1575" type="reference"><link href="1301" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>将注意力机制应用于看图说话(image caption)任务中以产生对图片的描述.首先利用卷积神经网络(CNN)提取图片特征,然后基于提取的特征,利用带注意力机制的循环神经网络(RNN)生成描述.在这个过程中,注意力实现了单词与图片之间的对齐,因此,通过可视化注意力权重矩阵,人们可以清楚地了解到模型在生成每一个单词时所对应的感兴趣的图片区域(如图2所示).此外,注意力机制还被广泛地应用于推荐系统中,以研究可解释的推荐系统<citation id="1576" type="reference"><link href="1307" rel="bibliography" /><link href="1309" rel="bibliography" /><link href="1311" rel="bibliography" /><link href="1313" rel="bibliography" /><link href="1315" rel="bibliography" /><sup>[<a class="sup">35</a>,<a class="sup">36</a>,<a class="sup">37</a>,<a class="sup">38</a>,<a class="sup">39</a>]</sup></citation>.具体地,这些方法首先基于历史记录,利用注意力机制计算针对每一条记录的注意力分值,从而给不同的偏好设置不同的权重,或者通过注意力机制对用户行为、用户表征进行建模来学习用户的长期偏好,以推荐用户可能感兴趣的下一个项目;最后,通过可视化用户历史记录列表中每一条记录的注意力分值来提供对推荐结果的解释,以增强推荐系统自身的可解释性.</p>
                </div>
                <div class="area_img" id="337">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_337.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 自然语言处理应用中的注意力权重可视化" src="Detail/GetImg?filename=images/JFYZ201910004_337.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 自然语言处理应用中的注意力权重可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_337.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Visualization of attention weight in natural language processing applications</p>

                </div>
                <div class="area_img" id="338">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_338.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 看图说话任务中注意力实现单词与图片的对齐[32]" src="Detail/GetImg?filename=images/JFYZ201910004_338.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 看图说话任务中注意力实现单词与图片的对齐<citation id="1577" type="reference"><link href="1301" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_338.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Alignment of words and images by attention in image caption task</p>

                </div>
                <h3 id="339" name="339" class="anchor-tag"><b>3 post-hoc可解释性</b></h3>
                <div class="p1">
                    <p id="340">post-hoc可解释性也称事后可解释性,发生在模型训练之后.对于一个给定的训练好的学习模型,post-hoc可解释性旨在利用解释方法或构建解释模型,解释学习模型的工作机制、决策行为和决策依据.因此,post-hoc可解释性的重点在于设计高保真的解释方法或构建高精度的解释模型.</p>
                </div>
                <div class="p1">
                    <p id="341">根据解释目的和解释对象的不同,post-hoc可解释性又分为全局可解释性和局部可解释性,所对应的方法分别称为全局解释方法和局部解释方法.经典的post-hoc解释方法及其满足的属性如表1所示:</p>
                </div>
                <div class="area_img" id="342">
                    <p class="img_tit"><b>表1 经典的post-hoc解释方法总结</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Summary of Classic post-hoc Interpretation Methods</b></p>
                    <p class="img_note"></p>
                    <table id="342" border="1"><tr><td>Method</td><td>G/L</td><td>MA/MS</td><td>TML</td><td>FCN</td><td>CNN</td><td>RNN</td><td>Fidelity</td><td>Security</td><td>Domain</td></tr><tr><td><br />inTree<sup>[23]</sup></td><td>G</td><td>MS</td><td>√</td><td>×</td><td>×</td><td>×</td><td>○</td><td></td><td></td></tr><tr><td><br />SGL<sup>[40]</sup></td><td>G</td><td>MS</td><td>√</td><td>×</td><td>×</td><td>×</td><td>○</td><td></td><td></td></tr><tr><td><br />GIRP<sup>[51]</sup></td><td>G</td><td>MA</td><td>√</td><td>√</td><td>√</td><td>√</td><td>○</td><td>×</td><td>CV/NLP</td></tr><tr><td><br />MAGIX<sup>[42]</sup></td><td>G</td><td>MA</td><td>√</td><td>√</td><td>√</td><td>√</td><td>○</td><td></td><td></td></tr><tr><td><br />DeepVID<sup>[43]</sup></td><td>G</td><td>MA</td><td>×</td><td>×</td><td>√</td><td>×</td><td>○</td><td>×</td><td>CV</td></tr><tr><td><br />AM<sup>[44]</sup></td><td>G</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1694.jpg" /></td><td>×</td><td>CV</td></tr><tr><td><br />Nguyen et al.<sup>[45]</sup></td><td>G</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1694.jpg" /></td><td>×</td><td>CV</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="343"><b>Continued (Table 1</b>)</p>
                </div>
                <div class="area_img" id="344">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="344" border="1"><tr><td>Method</td><td>G/L</td><td>MA/MS</td><td>TML</td><td>FCN</td><td>CNN</td><td>RNN</td><td>Fidelity</td><td>Security</td><td>Domain</td></tr><tr><td><br />Yuan et al.<sup>[46]</sup></td><td>G</td><td>MS</td><td>×</td><td>×</td><td>×</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>NLP</td></tr><tr><td><br />Saliency Mask<sup>[47]</sup></td><td>L</td><td>MA</td><td>×</td><td>√</td><td>√</td><td>×</td><td>○</td><td>×</td><td>CV</td></tr><tr><td><br />RSRS<sup>[48]</sup></td><td>L</td><td>MA</td><td>×</td><td>√</td><td>√</td><td>×</td><td>○</td><td>×</td><td>CV</td></tr><tr><td><br />LIME<sup>[13]</sup></td><td>L</td><td>MA</td><td>√</td><td>√</td><td>√</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV/NLP</td></tr><tr><td><br />LORE<sup>[49]</sup></td><td>L</td><td>MA</td><td>√</td><td>√</td><td>√</td><td>√</td><td>○</td><td>×</td><td></td></tr><tr><td><br />Anchor<sup>[50]</sup></td><td>L</td><td>MA</td><td>√</td><td>√</td><td>√</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV/NLP</td></tr><tr><td><br />LEMNA<sup>[51]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>×</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>NLP/Malware</td></tr><tr><td><br />Grad<sup>[52]</sup></td><td>L</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>√</td><td>○</td><td>×</td><td>CV/NLP</td></tr><tr><td><br />DeconvNet<sup>[53]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV</td></tr><tr><td><br />GuidedBP<sup>[54]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV</td></tr><tr><td><br />Integrated<sup>[55]</sup></td><td>L</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>√</td><td>○</td><td>×</td><td>CV/NLP</td></tr><tr><td><br />SmoothGrad<sup>[56]</sup></td><td>L</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV/NLP</td></tr><tr><td><br />LRP<sup>[57]</sup></td><td>L</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV/NLP</td></tr><tr><td><br />DeepLIFT<sup>[58]</sup></td><td>L</td><td>MS</td><td>×</td><td>√</td><td>√</td><td>√</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV/Genomics</td></tr><tr><td><br />Guided Inversion<sup>[59]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>√</td><td>CV</td></tr><tr><td><br />CAM<sup>[60]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV</td></tr><tr><td><br />Grad-CAM<sup>[61]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>√</td><td>×</td><td><img src="images\JFYZ201910004_1695.jpg" /></td><td>×</td><td>CV</td></tr><tr><td><br />AI<sup>2[62]</sup></td><td>L</td><td>MS</td><td>×</td><td>×</td><td>√</td><td>×</td><td>○</td><td>√</td><td>CV</td></tr><tr><td><br />OpenBox<sup>[63]</sup></td><td>G, L</td><td>MS</td><td>×</td><td>√</td><td>×</td><td>×</td><td>●</td><td>√</td><td>CV</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: G=global, L=local, MA=model-agnostic, MS=model-specific, TML=traditional machine learning, √=secure, ×=not secure, ○=low,○=low,<image id="626" type="formula" href="images/JFYZ201910004_62600.jpg" display="inline" placement="inline"><alt></alt></image>=middle，●=high,CV=computer vision,NLP=natural language processing.</p>
                </div>
                <h4 class="anchor-tag" id="346" name="346"><b>3.1 全局解释</b></h4>
                <div class="p1">
                    <p id="347">机器学习模型的全局可解释性旨在帮助人们从整体上理解模型背后的复杂逻辑以及内部的工作机制,例如模型是如何学习的、模型从训练数据中学到了什么、模型是如何进行决策的等,这要求我们能以人类可理解的方式来表示一个训练好的复杂学习模型.典型的全局解释方法包括解释模型/规则提取、模型蒸馏、激活最大化解释等.</p>
                </div>
                <h4 class="anchor-tag" id="348" name="348">3.1.1 规则提取</h4>
                <div class="p1">
                    <p id="349">早期针对模型可解释性的研究主要集中于解释规则或解释模型提取,即通过从受训模型中提取解释规则的方式,提供对复杂模型尤其是黑盒模型整体决策逻辑的理解<citation id="1578" type="reference"><link href="1365" rel="bibliography" /><link href="1367" rel="bibliography" /><link href="1369" rel="bibliography" /><link href="1371" rel="bibliography" /><sup>[<a class="sup">64</a>,<a class="sup">65</a>,<a class="sup">66</a>,<a class="sup">67</a>]</sup></citation>.规则提取技术以难以理解的复杂模型或黑盒模型作为入手点,利用可理解的规则集合生成可解释的符号描述,或从中提取可解释模型(如决策树、基于规则的模型等)<citation id="1579" type="reference"><link href="1373" rel="bibliography" /><link href="1375" rel="bibliography" /><link href="1377" rel="bibliography" /><sup>[<a class="sup">68</a>,<a class="sup">69</a>,<a class="sup">70</a>]</sup></citation>,使之具有与原模型相当的决策能力.解释模型或规则提取是一种有效的开箱技术,有效地提供了对复杂模型或黑盒模型内部工作机制的深入理解.根据解释对象不同,规则提取方法可分为针对树融合(tree ensemble)模型的规则提取<citation id="1580" type="reference"><link href="1283" rel="bibliography" /><link href="1317" rel="bibliography" /><link href="1379" rel="bibliography" /><link href="1381" rel="bibliography" /><link href="1383" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">40</a>,<a class="sup">71</a>,<a class="sup">72</a>,<a class="sup">73</a>]</sup></citation>和针对神经网络的规则提取.</p>
                </div>
                <div class="p1">
                    <p id="350">针对复杂的树融合模型(例如随机森林、提升树等)的规则提取方法通常包含4个部分:1)从树融合模型中提取规则,一个集成的树模型通常由多个决策树构成,每棵树的根节点到叶子节点的每一条路径都表示一条决策规则,将从每一棵决策树中提取的规则进行组合即可得到从树融合模型中提取的规则;2)基于规则长度、规则频率、误差等指标对提取的规则进行排序,其中规则长度反映了规则的复杂度,规则频率反映满足规则的数据实例的比例,误差则反映了规则的决策能力;3)基于排序结果,对规则中的无关项和冗余项进行剪枝并选择一组相关的非冗余规则;4)基于挑选的规则构建一个可解释的规则学习器,用于决策和解释.</p>
                </div>
                <div class="p1">
                    <p id="351">针对神经网络的规则提取方法可以分为2类:分解法(decompositional method)<citation id="1582" type="reference"><link href="1319" rel="bibliography" /><link href="1385" rel="bibliography" /><link href="1387" rel="bibliography" /><sup>[<a class="sup">41</a>,<a class="sup">74</a>,<a class="sup">75</a>]</sup></citation>和教学法(pedagogical method)<citation id="1583" type="reference"><link href="1389" rel="bibliography" /><link href="1391" rel="bibliography" /><link href="1393" rel="bibliography" /><sup>[<a class="sup">76</a>,<a class="sup">77</a>,<a class="sup">78</a>]</sup></citation>.分解法的显著特点是注重从受训神经网络中提取单个单元(如隐含单元、输出单元)层次上规则,这要求神经网络是“透明”的,即我们可以接触到模型的具体架构和参数.分解法要求受训神经网络中的每一个隐含单元和输出单元的计算结果都能映射成一个对应于一条规则的二进制结果.因此,每一个隐含单元或输出单元都可以被解释为一个阶跃函数或一条布尔规则.分解法通过聚合在单个单元级别提取的规则,形成整个受训神经网络的复合规则库,最后基于复合规则库提供对神经网络的整体解释.与分解法不同,教学法将受训神经网络模型当作是一个黑盒,即神经网络是“不透明”的,我们无法利用其结构和参数信息,只能操纵模型的输入和输出<citation id="1584" type="reference"><link href="1321" rel="bibliography" /><link href="1395" rel="bibliography" /><sup>[<a class="sup">42</a>,<a class="sup">79</a>]</sup></citation>.因此,教学法旨在提取将输入直接映射到输出的规则,基本思想是结合符号学习算法,利用受训神经网络来为学习算法生成样本,最后从生成的样例中提取规则<citation id="1581" type="reference"><link href="1391" rel="bibliography" /><sup>[<a class="sup">77</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="352">然而,规则提取方法提取的规则往往不够精确,因而只能提供近似解释,不一定能反映待解释模型的真实行为.此外,规则提取方法提供的可解释性的质量受规则本身复杂度的制约,如果从待解释模型中提取的规则很复杂或者提取的决策树模型深度很深,那么提取的规则本身就不具备良好的可解释性,因而无法为待解释模型提供有效的解释.</p>
                </div>
                <h4 class="anchor-tag" id="353" name="353">3.1.2 模型蒸馏</h4>
                <div class="p1">
                    <p id="354">当模型的结构过于复杂时,要想从整体上理解受训模型的决策逻辑通常是很困难的.解决该问题的一个有效途径是降低待解释模型的复杂度,而模型蒸馏(model distillation)则是降低模型复杂度的一个最典型的方法<citation id="1585" type="reference"><link href="1397" rel="bibliography" /><sup>[<a class="sup">80</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="355">模型蒸馏,也称知识蒸馏或模型模拟学习,是一种经典的模型压缩方法,其目的在于将复杂模型学习的函数压缩为具有可比性能更小、更快的模型<citation id="1586" type="reference"><link href="1399" rel="bibliography" /><sup>[<a class="sup">81</a>]</sup></citation>.模型蒸馏的核心思想是利用结构紧凑的学生模型(student model)来模拟结构复杂的教师模型(teacher model),从而完成从教师模型到学生模型的知识迁移过程,实现对复杂教师模型的知识“蒸馏”.蒸馏的难点在于压缩模型结构的同时如何保留教师模型从海量数据中学习到的知识和模型的泛化能力.一种有效的解决办法是利用软目标来辅助硬目标一起训练学生模型,其中硬目标为原始数据的类别信息,软目标为教师模型的分类概率值,包含的信息量大,体现了不同类别之间相关关系的信息<citation id="1587" type="reference"><link href="1401" rel="bibliography" /><sup>[<a class="sup">82</a>]</sup></citation>.给定一个复杂的教师模型和一批训练数据,模型蒸馏方法首先利用教师模型生成软目标,然后通过最小化软目标和硬目标的联合损失函数来训练学生模型,损失函数定义为</p>
                </div>
                <div class="p1">
                    <p id="356"><i>L</i><sub>stdudent</sub>=<i>αL</i><sup>(soft)</sup>+(1-<i>α</i>)<i>L</i><sup>(hard)</sup>,</p>
                </div>
                <div class="p1">
                    <p id="357">其中,<i>L</i><sup>(soft)</sup>为软目标损失,要求学生模型生成的软目标与教师模型生成的软目标要尽可能的接近,保证学生模型能有效地学习教师模型中的暗知识(dark knowledge);<i>L</i><sup>(hard)</sup>为硬目标损失,要求学生模型能够保留教师模型良好的决策性能.</p>
                </div>
                <div class="p1">
                    <p id="358">由于模型蒸馏可以完成从教师模型到学生模型的知识迁移,因而学生模型可以看作是教师模型的全局近似,在一定程度上反映了教师模型的整体逻辑,因此我们可以基于学生模型,提供对教师模型的全局解释.在利用模型蒸馏作为全局解释方法时,学生模型通常采用可解释性好的模型来实现,如线性模型、决策树、广义加性模型以及浅层神经网络等<citation id="1594" type="reference"><link href="1403" rel="bibliography" /><link href="1405" rel="bibliography" /><link href="1407" rel="bibliography" /><sup>[<a class="sup">83</a>,<a class="sup">84</a>,<a class="sup">85</a>]</sup></citation>.Hinton等人<citation id="1588" type="reference"><link href="1401" rel="bibliography" /><sup>[<a class="sup">82</a>]</sup></citation>提出了一种知识蒸馏方法,通过训练单一的相对较小的网络来模拟原始复杂网络或集成网络模型的预测概率来提炼复杂网络的知识,以模拟原始复杂网络的决策过程,并且证明单一网络能达到复杂网络几乎同样的性能.为了进一步提升蒸馏知识的可解释性,Frosst等人<citation id="1589" type="reference"><link href="1405" rel="bibliography" /><sup>[<a class="sup">84</a>]</sup></citation>扩展了Hinton提出的知识蒸馏方法,提出利用决策树来模拟复杂深度神经网络模型的决策.Tan等人<citation id="1590" type="reference"><link href="1407" rel="bibliography" /><sup>[<a class="sup">85</a>]</sup></citation>基于广义加性模型的良好可解释性,提出利用模型蒸馏的方法来学习描述输入特征与复杂模型的预测之间关系的全局加性模型,并基于加性模型对复杂模型进行全局解释.Che等人<citation id="1591" type="reference"><link href="1409" rel="bibliography" /><sup>[<a class="sup">86</a>]</sup></citation>将基于模型蒸馏的可解释方法应用于医疗诊断模型的可解释性研究中,提出利用梯度提升树进行知识蒸馏的方式来学习可解释模型,不仅在急性肺损伤病人无呼吸机天数预测任务中取得了优异的性能,而且还可以为临床医生提供良好的可解释性.Ding等人<citation id="1592" type="reference"><link href="1411" rel="bibliography" /><sup>[<a class="sup">87</a>]</sup></citation>利用知识蒸馏解释基于社交媒体的物质使用预测模型,通过运用知识蒸馏框架来构建解释模型,取得了与最先进的预测模型相当的性能,而且还可以提供对用户的社交媒体行为与物质使用之间的关系深入理解.Xu等人<citation id="1593" type="reference"><link href="1413" rel="bibliography" /><sup>[<a class="sup">88</a>]</sup></citation>开发了DarkSight可解释方法,通过利用模型蒸馏的方式从黑盒模型中提取暗知识,并以可视化的形式对提取的暗知识进行呈现,以帮助分析师直观地了解模型决策逻辑.</p>
                </div>
                <div class="p1">
                    <p id="359">此外,基于模型蒸馏的解释方法还被广泛地应用于模型诊断与验证<citation id="1597" type="reference"><link href="1323" rel="bibliography" /><link href="1415" rel="bibliography" /><link href="1417" rel="bibliography" /><sup>[<a class="sup">43</a>,<a class="sup">89</a>,<a class="sup">90</a>]</sup></citation>.Tan等人<citation id="1595" type="reference"><link href="1415" rel="bibliography" /><sup>[<a class="sup">89</a>]</sup></citation>提出了一种针对黑盒风险评分模型的2阶段模型审计方法,对于一个给定的黑盒风险评分模型和一批审计数据,该方法首先利用模型蒸馏的方法得到一个解释模型,同时基于审计数据和其真实标签训练一个透明的结果预测模型,并通过比较解释模型和结果预测模型来理解特征与风险评分之间的相关关系;最后,通过使用统计测试的方式来确定黑盒模型是否使用了审计数据中不存在的其他特征.同时,通过评估受保护特征对风险评分的贡献与其对实际结果的贡献的差异,可以检测黑盒风险评分模型中是否存在偏差<citation id="1596" type="reference"><link href="1417" rel="bibliography" /><sup>[<a class="sup">90</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="360">模型蒸馏解释方法实现简单,易于理解,且不依赖待解释模型的具体结构信息,因而作为一种模型无关的解释方法,常被用于解释黑盒机器学习模型.然而,蒸馏模型只是对原始复杂模型的一种全局近似,它们之间始终存在差距.因此,基于蒸馏模型所做出的解释不一定能反映待解释模型的真实行为.此外,知识蒸馏过程通常不可控,无法保障待解释模型从海量数据中学到的知识有效地迁移到蒸馏模型中,因而导致解释结果质量较低无法满足精确解释的需要.</p>
                </div>
                <h4 class="anchor-tag" id="361" name="361">3.1.3 激活最大化</h4>
                <div class="p1">
                    <p id="362">在自下而上的深度学习任务中,给定一批训练数据,DNN不仅可以自动地学习输入数据与输出类别之间的映射关系,同时也可以从数据中学到特定的特征表示(feature representation).然而,考虑到数据集中存在偏差,我们无法通过模型精度来保证模型表征的可靠性,也无法确定DNN用于预测的内部工作模式<citation id="1598" type="reference"><link href="1419" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>.因此,深入理解并呈现DNN中每一个隐含层的神经元所捕获的表征,有助于从语义上、视觉上帮助人们理解DNN内部的工作逻辑<citation id="1599" type="reference"><link href="1421" rel="bibliography" /><sup>[<a class="sup">92</a>]</sup></citation>.为此,许多研究者探索如何在输入空间实现对DNN任意层神经单元计算内容的可视化,并使其尽可能通用,以便能够深入了解神经网络不同单元代表的特定含义.其中,最有效和使用最广泛的一种方法是通过在特定的层上找到神经元的首选输入最大化神经元激活,因此该方法也称为激活最大化(activation maximization,  AM)方法<citation id="1600" type="reference"><link href="1341" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="363">激活最大化方法思想较为简单,即通过寻找有界范数的输入模式,最大限度地激活给定的隐藏单元,而一个单元最大限度地响应的输入模式可能是一个单元正在做什么的良好的一阶表示<citation id="1601" type="reference"><link href="1325" rel="bibliography" /><link href="1423" rel="bibliography" /><link href="1425" rel="bibliography" /><sup>[<a class="sup">44</a>,<a class="sup">93</a>,<a class="sup">94</a>]</sup></citation>.给定一个DNN模型,寻找最大化神经元激活的原型样本<i>x</i><sup>*</sup>的问题可以被定义成一个优化问题,其形式化定义为</p>
                </div>
                <div class="p1">
                    <p id="364" class="code-formula">
                        <mathml id="364"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>x</mi></munder><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>-</mo><mi>λ</mi><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="365">其中,优化目标第一项<i>f</i><sub><i>l</i></sub>(<i>x</i>)为DNN第<i>l</i>层某一个神经元在当前输入<i>x</i>下的激活值;第2项为ℓ<sub>2</sub>正则,用于保证优化得到的原型样本(prototype)与原样本尽可能地接近.整个优化过程可以通过梯度上升来求解.最后,通过可视化生成的原型样本<i>x</i><sup>*</sup>,可以帮助我们理解该神经元在其感受野中所捕获到的内容.当然,我们可以分析任意层的神经元,以理解DNN不同层所编码的不同表示内容.当我们分析输出层神经元的最大激活时,可以找到某一类别所对应的最具代表性的原型样本.</p>
                </div>
                <div class="p1">
                    <p id="366">激活最大化方法虽然原理简单,但如何使其正常工作同样面临着一些挑战.由于样本搜索空间很大,优化过程可能产生含有噪声和高频模式的不现实图像,导致原型样本虽能最大化神经元激活却难以理解.为了获取更有意义、更自然的原型样本,优化过程必须采用自然图像先验约束,为此,一些研究者创造性地提出了人工构造先验,包括<i>α</i>范数、高斯模糊等<citation id="1603" type="reference"><link href="1427" rel="bibliography" /><link href="1429" rel="bibliography" /><sup>[<a class="sup">95</a>,<a class="sup">96</a>]</sup></citation>.此外,一些研究者将激活最大化框架与生成模型相结合,利用生成模型产生的更强的自然图像先验正则化优化过程.Nguyen等人<citation id="1602" type="reference"><link href="1327" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>提出利用生成对抗网络与激活最大化优化相结合的方法来生成原型样本,优化问题被重定义为</p>
                </div>
                <div class="p1">
                    <p id="367" class="code-formula">
                        <mathml id="367"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>z</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mtext>z</mtext><mo>∈</mo><mtext>Ζ</mtext></mrow></munder><mspace width="0.25em" /><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>-</mo><mi>λ</mi><mrow><mo>|</mo><mi>z</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="368">其中,第1项为解码器与原神经元激活值的结合,第2项为代码空间中的ℓ<sub>2</sub>正则.该方法不直接优化图像,转而优化代码空间以找到可以最大化神经元激活的解<i>z</i><sup>*</sup>,一旦最优解<i>z</i><sup>*</sup>找到,则可以通过解码得到原型样本<i>z</i><sup>*</sup>,即<i>x</i><sup>*</sup>=<i>g</i>(<i>z</i><sup>*</sup>).实验结果表明(如图3所示),将激活最大化与生成模型相结合的方法可以产生更真实、更具有可解释性的原型样本.从图3可以看出:模型成功捕获了与类别相对应的特征表示.</p>
                </div>
                <div class="p1">
                    <p id="369">对不同层生成的原型样本的可视化结果表明,DNN在若干抽象层次上进行表示学习,从模型的第一层到最后一层,模型学习到的特征表征由局部过渡到整体,由一般任务过渡到特定任务.以图像分类任务中的CNN为例,低层神经元通常可以捕获到图片中的颜色、边缘等信息;中间层神经元有更复杂的不变性,可以捕获相似的纹理;中高层神经元可以捕获图片中的显著变化,并可以聚焦到特定类别对应的局部特征,如狗的脸部、鸟的脚部等;最后,高层神经元则通过组合局部特征表征,从而学习到整个分类目标的整体表征<citation id="1604" type="reference"><link href="1343" rel="bibliography" /><sup>[<a class="sup">53</a>]</sup></citation>.此外,神经元具有多面性,可以对与同一语义概念相关的不同图像做出反应,例如,人脸检测神经元可以同时对人脸和动物面孔做出反应<citation id="1605" type="reference"><link href="1431" rel="bibliography" /><sup>[<a class="sup">97</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="370">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_370.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 利用生成模型与激活最大化相结合生成的类别对应原型样本[45]" src="Detail/GetImg?filename=images/JFYZ201910004_370.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 利用生成模型与激活最大化相结合生成的类别对应原型样本<citation id="1606" type="reference"><link href="1327" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_370.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Class-discriminative prototypes generated by combining generative model with activation maximization</p>

                </div>
                <div class="p1">
                    <p id="371">激活最大化解释方法是一种模型相关的解释方法,相比规则提取解释和模型蒸馏解释,其解释结果更准确,更能反映待解释模型的真实行为.同时,利用激活最大化解释方法,可从语义上、视觉上帮助人们理解模型是如何从数据中进行学习的以及模型从数据中学到了什么.然而,激活最大化本身是一个优化问题,在通过激活最大化寻找原型样本的过程中,优化过程中的噪音和不确定性可能导致产生的原型样本难以解释.尽管可以通过构造自然图像先验约束优化过程来解决这一问题,但如何构造更好的自然图像先验本身就是一大难题.此外,激活最大化方法只能用于优化连续性数据,无法直接应用于诸如文本、图数据等离散型数据<citation id="1607" type="reference"><link href="1329" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>,因而该方法难以直接用于解释自然语言处理模型和图神经网络模型.</p>
                </div>
                <h4 class="anchor-tag" id="372" name="372"><b>3.2 局部解释</b></h4>
                <div class="p1">
                    <p id="373">机器学习模型的局部可解释性旨在帮助人们理解学习模型针对每一个特定输入样本的决策过程和决策依据.与全局可解释性不同,模型的局部可解释性以输入样本为导向,通常可以通过分析输入样本的每一维特征对模型最终决策结果的贡献来实现.在实际应用中,由于模型算法的不透明性、模型结构的复杂性以及应用场景的多元性,提供对机器学习模型的全局解释通常比提供局部解释更困难,因而针对模型局部可解释性的研究更加广泛,局部解释方法相对于全局解释方法也更常见.经典的局部解释方法包括敏感性分析解释、局部近似解释、梯度反向传播解释、特征反演解释以及类激活映射解释等.</p>
                </div>
                <h4 class="anchor-tag" id="374" name="374">3.2.1 敏感性分析</h4>
                <div class="p1">
                    <p id="375">敏感性分析(sensitivity analysis)是指在给定的一组假设下,从定量分析的角度研究相关自变量发生某种变化对某一特定的因变量影响程度的一种不确定分析技术<citation id="1608" type="reference"><link href="1433" rel="bibliography" /><sup>[<a class="sup">98</a>]</sup></citation>,其核心思想是通过逐一改变自变量的值来解释因变量受自变量变化影响大小的规律.敏感性分析被广泛地应用于机器学习及其应用中,如机器学习模型分析<citation id="1610" type="reference"><link href="1435" rel="bibliography" /><link href="1437" rel="bibliography" /><link href="1439" rel="bibliography" /><sup>[<a class="sup">99</a>,<a class="sup">100</a>,<a class="sup">101</a>]</sup></citation>、生态建模<citation id="1609" type="reference"><link href="1441" rel="bibliography" /><sup>[<a class="sup">102</a>]</sup></citation>等.近年来,敏感性分析作为一种模型局部解释方法,被用于分析待解释样本的每一维特征对模型最终分类结果的影响<citation id="1611" type="reference"><link href="1443" rel="bibliography" /><link href="1445" rel="bibliography" /><link href="1447" rel="bibliography" /><sup>[<a class="sup">103</a>,<a class="sup">104</a>,<a class="sup">105</a>]</sup></citation>,以提供对某一个特定决策结果的解释.根据是否需要利用模型的梯度信息,敏感性分析方法可分为模型相关方法和模型无关方法.</p>
                </div>
                <div class="p1">
                    <p id="376">模型相关方法利用模型的局部梯度信息评估特征与决策结果的相关性,常见的相关性定义为</p>
                </div>
                <div class="p1">
                    <p id="377" class="code-formula">
                        <mathml id="377"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="378">其中,<i>f</i>(<i>x</i>)为模型的决策函数,<i>x</i><sub><i>i</i></sub>为待解释样本<i>x</i>的第<i>i</i>维特征.直观地,相关性分数<i>R</i><sub><i>i</i></sub>(<i>x</i>)可以看作是模型梯度的ℓ<sub>2</sub>范数的分解,即<mathml id="464"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mi>R</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>|</mo><mrow><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>.在模型相关方法中,相关性分数<i>R</i><sub><i>i</i></sub>(<i>x</i>)可通过梯度反向传播来求解.最后,通过以热力图的形式可视化相关性分数可以直观地理解输入的每一维特征对决策结果的影响程度.</p>
                </div>
                <div class="p1">
                    <p id="379">在模型无关敏感性分析方法中,待解释模型可以看作是黑盒,我们无需利用模型的梯度信息,只关注待解释样本特征值变化对模型最终决策结果的影响.Robnik-Šikonja等人<citation id="1612" type="reference"><link href="1449" rel="bibliography" /><sup>[<a class="sup">106</a>]</sup></citation>提出通过对输入样本单个属性值的预测进行分解的方式来观察属性值对该样本预测结果的影响.具体地,该方法通过观察去掉某一特定属性前后模型预测结果的变化来确定该属性对预测结果的重要性,即:</p>
                </div>
                <div class="p1">
                    <p id="380"><i>R</i><sub><i>i</i></sub>(<i>x</i>)=<i>f</i>(<i>x</i>)-<i>f</i>(<i>x</i>＼<i>x</i><sub><i>i</i></sub>).</p>
                </div>
                <div class="p1">
                    <p id="381">类似地,Liu等人<citation id="1613" type="reference"><link href="1333" rel="bibliography" /><sup>[<a class="sup">48</a>]</sup></citation>提出了“限制支持域集”的概念,它被定义为一组受大小限制且不重叠的区域,并且满足如下属性:删除任何一个区域将会导致模型分类出错.其本质思想是,如果某个特定区域的缺失导致模型分类结果发生反转,则该区域必定为模型正确决策提供支持.因此,最终可通过分析特定图像区域是否存在与模型决策结果之间的依赖关系来可视化模型决策规则.Fong等人<citation id="1614" type="reference"><link href="1331" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>提出了一种基于有意义扰动的敏感性分析方法,通过添加扰动或删除待解释图片的不同区域来最小化模型目标类别分类概率的方式学习一个显著性掩码,以识别对模型决策结果影响最大的图像部分,并可视化显著性掩码作为对该决策结果的解释,如图4所示.Li等人<citation id="1615" type="reference"><link href="1451" rel="bibliography" /><sup>[<a class="sup">107</a>]</sup></citation>则提出通过观察修改或删除特征子集前后模型决策结果的相应变化的方式来推断待解释样本的决策特征.</p>
                </div>
                <div class="area_img" id="382">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_382.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 通过图像模糊的方式最小化分类概率来学习显著性掩码[47]" src="Detail/GetImg?filename=images/JFYZ201910004_382.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 通过图像模糊的方式最小化分类概率来学习显著性掩码<citation id="1616" type="reference"><link href="1331" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_382.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Learn a saliency mask by blurring an image to minimize the probability of its target class</p>

                </div>
                <div class="p1">
                    <p id="383">然而,敏感性分析方法解释的是决策函数<i>f</i>(<i>x</i>)局部变化对决策结果的影响,而不是解释决策函数本身,只能捕获到单个特征对最终决策结果的影响程度,而不一定关注实际的决策相关特征,因而相关性分值<i>R</i><sub><i>i</i></sub>(<i>x</i>)对应的热力图在空间上是分散而不连续的.因此,敏感性分析方法提供的解释结果通常相对粗糙且难以理解.此外,敏感性分析方法无法解释特征之间的相关关系对最终决策结果的影响.</p>
                </div>
                <h4 class="anchor-tag" id="384" name="384">3.2.2 局部近似</h4>
                <div class="p1">
                    <p id="385">局部近似解释方法的核心思想是利用结构简单的可解释模型拟合待解释模型针对某一输入实例的决策结果,然后基于解释模型对该决策结果进行解释.该方法通常基于如下假设:给定一个输入实例,模型针对该实例以及该实例邻域内样本的决策边界可以通过可解释的白盒模型来近似.在整个数据空间中,待解释模型的决策边界可以任意的复杂,但模型针对某一特定实例的决策边界通常是简单的,甚至是近线性的<citation id="1617" type="reference"><link href="1263" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.我们通常很难也不需要对待解释模型的整体决策边界进行全局近似,但可在给定的实例及其邻域内利用可解释模型对待解释模型的局部决策边界进行近似,然后基于可解释模型提供对待解释模型的决策依据的解释.</p>
                </div>
                <div class="p1">
                    <p id="386">Ribeiro等人<citation id="1618" type="reference"><link href="1263" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>基于神经网络的局部线性假设,提出了一种模型无关局部可解释方法(LIME).具体地,对于每一个输入实例,LIME首先利用该实例以及该实例的一组近邻训练一个易于解释的线性回归模型来拟合待解释模型的局部边界,然后基于该线性模型解释待解释模型针对该实例的决策依据,其中,线性模型的权重系数直接体现了当前决策中该实例的每一维特征重要性.Guidotti等人<citation id="1619" type="reference"><link href="1335" rel="bibliography" /><sup>[<a class="sup">49</a>]</sup></citation>提出了一种适用于关系表数据的基于局部规则的黑盒模型决策结果解释方法(LORE).给定一个二分类模型<i>f</i>及一个由<i>f</i> 标记的特定实例 <i>x</i>,LORE首先利用ad-hoc遗传算法生成给定实例<i>x</i>的一组平衡邻居实例来构建一个简单的、可解释的预测模型,以逼近二分类模型<i>f</i>针对实例<i>x</i>的决策边界;然后,基于该解释模型,从生成的实例集合中提取一个决策树模型;最后,从决策树模型中提取决策规则作为对实例<i>x</i>的分类结果的局部解释.Ribeiro等人<citation id="1620" type="reference"><link href="1337" rel="bibliography" /><link href="1453" rel="bibliography" /><sup>[<a class="sup">50</a>,<a class="sup">108</a>]</sup></citation>提出了一种称之为锚点解释(anchor)的局部解释方法,针对每一个输入实例,该方法利用被称之为“锚点”的if-then规则来逼近待解释模型的局部边界.Anchor方法充分地结合了模型无关局部解释方法的优点和规则的良好可解释性,在Anchor方法中用于解释的“锚点”通常是直观、易于理解的,而且解释覆盖范围非常清晰.通过构造,“锚点”不仅可以与待解释模型保持一致,而且还可以以确保正确理解和高保真的方式将待解释模型的决策行为传达给用户.</p>
                </div>
                <div class="p1">
                    <p id="387">然而,LIME,LORE以及Anchor等解释方法均假设输入样本的特征相互独立,因而无法准确地解释诸如RNN等专门对序列数据中的依赖关系进行建模的模型.为此,Guo等人<citation id="1621" type="reference"><link href="1339" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>提出了LEMNA,一种专用于安全应用场景中的RNN模型的高保真解释方法,其核心思想与LIME等方法相似,即利用可解释模型来近似RNN的局部决策边界,并针对每一个输入实例,产生一组可解释的特征以解释针对该实例的决策依据.与LIME不同的是,LEMNA假设待解释模型的局部边界是非线性的,为了保证解释的保真度,LEMNA通过训练混合回归模型来近似RNN针对每个输入实例的局部决策边界.此外,LEMNA引入了融合Lasso正则来处理RNN模型中的特征依赖问题,有效地弥补了LIME等方法的不足.</p>
                </div>
                <div class="p1">
                    <p id="388">基于局部近似的解释方法实现简单,易于理解且不依赖待解释模型的具体结构,适于解释黑盒机器学习模型.但解释模型只是待解释模型的局部近似,因而只能捕获模型的局部特征,无法解释模型的整体决策行为.针对每一个输入实例,局部近似解释方法均需要重新训练一个解释模型来拟合待解释模型针对该实例的决策结果,因而此类方法的解释效率通常不高.此外,大多数的局部近似解释方法假设待解释实例的特征相互独立,因此无法解释特征之间的相关关系对决策结果的影响.</p>
                </div>
                <h4 class="anchor-tag" id="389" name="389">3.2.3 反向传播</h4>
                <div class="p1">
                    <p id="390">基于反向传播(back propagation)的解释方法的核心思想是利用DNN的反向传播机制将模型的决策重要性信号从模型的输出层神经元逐层传播到模型的输入以推导输入样本的特征重要性.</p>
                </div>
                <div class="p1">
                    <p id="391">Simonyan等人<citation id="1622" type="reference"><link href="1341" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>最先提出了利用反向传播推断特征重要性的解释方法(Grad),通过利用反向传播算法计算模型的输出相对于输入图片的梯度来求解该输入图片所对应的分类显著图(Saliency Map).与Grad方法类似,Zeiler等人<citation id="1623" type="reference"><link href="1343" rel="bibliography" /><sup>[<a class="sup">53</a>]</sup></citation>提出了反卷积网络(DeconvNet),通过将DNN的高层激活反向传播到模型的输入以识别输入图片中负责激活的重要部分.不同的是,在处理线性整流单元(ReLU)过程中,当使用Grad方法反向传播重要性时,如果正向传播过程中ReLU的输入为负,则反向传播过程中传入ReLU的梯度值为零.而在反卷积网络中反向传播一个重要信号时,当且仅当信号值为负,进入ReLU的重要信号被置零,而不考虑前向传播过程中输入到ReLU的信号的符号.Springenberg等人<citation id="1624" type="reference"><link href="1345" rel="bibliography" /><sup>[<a class="sup">54</a>]</sup></citation>将Grad方法与反卷积网络相结合提出了导向反向传播方法(GuidedBP),通过在反向传播过程中丢弃负值来修改ReLU函数的梯度.与只计算输出针对当前输入的梯度不同,Sundararajan等人<citation id="1625" type="reference"><link href="1347" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>提出了一种集成梯度方法(Integrated),该方法通过计算输入从某些起始值按比例放大到当前值的梯度的积分代替单一梯度,有效地解决了DNN中神经元饱和问题导致无法利用梯度信息反映特征重要性的问题.</p>
                </div>
                <div class="p1">
                    <p id="392">然而,Grad,GuidedBP以及Integrated等方法通过反向传播所得到的显著图通常包含很多视觉可见的噪音,如图5所示,而我们无法确定这种噪音是否真实地反映了模型在分类过程中的决策依据.为此,Smilkov等人<citation id="1626" type="reference"><link href="1349" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>提出了一种平滑梯度的反向传播解释方法(SmoothGrad),该方法通过向输入样本中引入噪声解决了Grad等方法中存在的视觉噪音问题.SmoothGrad方法的核心思想是通过向待解释样本中添加噪声对相似的样本进行采样,然后利用反向传播方法求解每个采样样本的决策显著图,最后将所有求解得到的显著图进行平均并将其作为对模型针对该样本的决策结果的解释.</p>
                </div>
                <div class="p1">
                    <p id="393">尽管上述基于梯度反向传播的方法可以定位输入样本中决策特征,但却无法量化每个特征对模型决策结果的贡献程度.因此,Landecker等人<citation id="1627" type="reference"><link href="1455" rel="bibliography" /><sup>[<a class="sup">109</a>]</sup></citation>提出一种贡献传播方法,该方法首先利用加性模型计算DNN高层特征对模型分类结果的贡献,然后通过反向传播将高层特征的贡献逐层传递到模型的输入,以确定每一层的每一个神经元节点对其下一层神经元节点的相对贡献.给定一个待解释样本,该方法不仅可以定位样本中的重要特征,而且还能量化每一个特征对于分类结果的重要性.Bach等人<citation id="1628" type="reference"><link href="1351" rel="bibliography" /><sup>[<a class="sup">57</a>]</sup></citation>则提出了一种分层相关性传播方法(LRP),用于计算单个像素对图像分类器预测结果的贡献.一般形式的LRP方法假设分类器可以被分解为多个计算层,每一层都可以被建模为一个多维向量并且该多维向量的每一维都对应一个相关性分值,LRP的核心则是利用反向传播将高层的相关性分值递归地传播到低层直至传播到输入层.Shrikumar等人<citation id="1629" type="reference"><link href="1353" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>对LRP方法进行了改进(DeepLIFT),通过在输入空间中定义参考点并参考神经元激活的变化按比例传播相关分数.其研究结果表明,在不进行数值稳定性修正的情况下,原始LRP方法的输出结果等价于Grad方法所求显著图与输入之间的乘积.与梯度反向传播方法不同的是,LRP方法不要求DNN神经元的激活是可微的或平滑的.基于此优点,Ding等人首次将LRP方法应用于基于注意力机制的编码器-解码器框架,以度量神经网络中任意2个神经元之间关联程度的相关性.在汉英翻译案例中的研究表明,该方法有助于解释神经机器翻译系统的内部工作机制并分析翻译错误.类似地,Arras等人将LRP方法引入到自然语言处理任务中,并且从定性和定量的角度证明LRP方法既可以用于文档级别的细粒度分析,也可以作为跨文档的数据集级别的分析,以识别对分类器决策很重要的单词.</p>
                </div>
                <div class="area_img" id="394">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_394.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 4种梯度反向传播解释方法解释效果对比[59]" src="Detail/GetImg?filename=images/JFYZ201910004_394.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 4种梯度反向传播解释方法解释效果对比<citation id="1630" type="reference"><link href="1355" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_394.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Comparison of interpretation quality of four gradient back-propagation based interpretation methods</p>

                </div>
                <div class="p1">
                    <p id="396">基于反向传播的解释方法通常实现简单、计算效率高且充分利用了模型的结构特性.然而,从理论上易知,如果预测函数在输入附近变得平坦,那么预测函数相对于输入的梯度在该输入附近将变得很小,进而导致无法利用梯度信息定位样本的决策特征.尽管Integrated方法在一定程度上解决了该问题,但同时也增加了计算开销,并且Integrated方法的解释结果中依然存在许多人类无法理解的噪音.此外,梯度信息只能用于定位重要特征,而无法量化特征对决策结果的重要程度,利用基于重要性或相关性反向传播的解释方法则可以解决该问题.</p>
                </div>
                <h4 class="anchor-tag" id="397" name="397">3.2.4 特征反演</h4>
                <div class="p1">
                    <p id="398">尽管敏感性分析、局部近似以及梯度反向传播等方法在一定程度上可以提供对待解释模型决策结果的局部解释,但它们通常忽略了待解释模型的中间层,因而遗漏了大量的中间信息.而利用模型的中间层信息,我们能更容易地表征模型在正常工作条件下的决策行为,进而可提供更准确的解释结果.特征反演(feature inversion)作为一种可视化和理解DNN中间特征表征的技术,可以充分利用模型的中间层信息,以提供对模型整体行为及模型决策结果的解释.</p>
                </div>
                <div class="p1">
                    <p id="399">特征反演解释方法可分为模型级(model-level)解释方法和实例级(instance-level)解释方法.模型级解释方法旨在从输入空间中寻找可以表示DNN神经元所学到的抽象概念的解释原型(如激活最大化方法),并通过可视化和理解DNN每一层特征表示的方式,提供对DNN每一层所提取信息的理解<citation id="1632" type="reference"><link href="1341" rel="bibliography" /><link href="1427" rel="bibliography" /><link href="1461" rel="bibliography" /><link href="1463" rel="bibliography" /><sup>[<a class="sup">52</a>,<a class="sup">95</a>,<a class="sup">112</a>,<a class="sup">113</a>]</sup></citation>.然而,模型级解释方法的反演结果通常相对粗糙且难以理解,此外,如何从输入样本中自动化提取用于模型决策的重要特征仍然面临着巨大的挑战.针对模型级方法的不足,实例级特征反演方法试图回答输入样本的哪些特征被用于激活DNN的神经元以做出特定的决策.其中,最具代表性的是Du等人<citation id="1631" type="reference"><link href="1355" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>提出的一个实例级特征反演解释框架,该框架通过在执行导向特征反演过程中加入类别依赖约束,不仅可以准确地定位待输入实例中的用于模型决策的重要特征(如图6所示),还可以提供对DNN模型决策过程的深入理解.</p>
                </div>
                <div class="area_img" id="400">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 导向特征反演方法解释示例[59]" src="Detail/GetImg?filename=images/JFYZ201910004_400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 导向特征反演方法解释示例<citation id="1633" type="reference"><link href="1355" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Interpretation example of guided feature inversion method</p>

                </div>
                <h4 class="anchor-tag" id="401" name="401">3.2.5 类激活映射</h4>
                <div class="p1">
                    <p id="402">最新研究表明:CNN不同层次的卷积单元包含大量的位置信息,使其具有良好的定位能力<citation id="1634" type="reference"><link href="1465" rel="bibliography" /><sup>[<a class="sup">114</a>]</sup></citation>.基于卷积单元的定位能力,我们可以定位出输入样本中用于CNN决策的核心区域,如分类任务中的决策特征、目标检测任务中的物体位置等.然而,传统CNN模型通常在卷积和池化之后采用全连接层对卷积层提取的特征图进行组合用于最终决策,因而导致网络的定位能力丧失.</p>
                </div>
                <div class="p1">
                    <p id="403">为解决这一问题,Zhou等人<citation id="1635" type="reference"><link href="1357" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>提出了类激活映射(class activation mapping, CAM)解释方法,该方法利用全局平均池化(global average pooling)层来替代传统CNN模型中除softmax层以外的所有全连接层,并通过将输出层的权重投影到卷积特征图来识别图像中的重要区域.具体地,CAM首先利用全局平均池化操作输出CNN最后一个卷积层每个单元的特征图的空间平均值,并通过对空间平均值进行加权求和得到CNN的最终决策结果.同时,CAM通过计算最后一个卷积层的特征图的加权和,得到CNN模型的类激活图,而一个特定类别所对应的类激活图则反映了CNN用来识别该类别的核心图像区域.最后,通过以热力图的形式可视化类激活图得到最终的解释结果.研究结果表明,全局平均池化层的优势远不止于作为一个正则器来防止网络过拟合,事实上,通过稍加调整,全局平均池化还可以将CNN良好的定位能力保留到网络的最后一层.</p>
                </div>
                <div class="area_img" id="404">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_404.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 Grad-CAM与Guided Grad-CAM方法解释结果可视化[61]" src="Detail/GetImg?filename=images/JFYZ201910004_404.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 Grad-CAM与Guided Grad-CAM方法解释结果可视化<citation id="1636" type="reference"><link href="1359" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_404.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Visualization of interpretation results of Grad-CAM and Guided Grad-CAM methods</p>

                </div>
                <div class="p1">
                    <p id="406">然而,CAM方法需要修改网络结构并重训练模型,因而在实际应用中并不实用.因此,Selvaraju等人<citation id="1637" type="reference"><link href="1359" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>对CAM方法进行了改进,提出了一种将梯度信息与特征映射相结合的梯度加权类激活映射方法(Grad-CAM).给定一个输入样本,Grad-CAM首先计算目标类别相对于最后一个卷积层中每一个特征图的梯度并对梯度进行全局平均池化,以获得每个特征图的重要性权重;然后,基于重要性权重计算特征图的加权激活,以获得一个粗粒度的梯度加权类激活图,用于定位输入样本中具有类判别性的重要区域,如图7(c)所示.与CAM相比,Grad-CAM无需修改网络架构或重训练模型,避免了模型的可解释性与准确性之间的权衡,因而可适用于多种任务以及任何基于CNN结构的模型,对于全卷积神经网络,Grad-CAM退化为CAM方法.尽管Grad-CAM具有良好的类别判别能力并能很好地定位相关图像区域,但缺乏诸如DeconvNet<citation id="1638" type="reference"><link href="1343" rel="bibliography" /><sup>[<a class="sup">53</a>]</sup></citation>和GuidedBP<citation id="1639" type="reference"><link href="1345" rel="bibliography" /><sup>[<a class="sup">54</a>]</sup></citation>等像素级别梯度可视化解释方法显示细粒度特征重要性的能力<citation id="1640" type="reference"><link href="1359" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>.为获得更细粒度的特征重要性,作者将Grad-CAM与GuidedBP方法相结合提出了导向梯度加权类激活映射方法(Guided Grad-CAM),该方法首先利用双线性插值将梯度加权类激活图上采样到输入图片分辨率大小,然后点乘GuidedBP方法的输出结果,得到细粒度的类判别性特征定位图,如图7(d)所示.研究结果表明,Guided Grad-CAM方法解释效果优于GuidedBP和Grad-CAM.</p>
                </div>
                <div class="p1">
                    <p id="407">类激活映射解释方法实现简单、计算效率高,解释结果视觉效果好且易于理解,但这类方法只适用于解释CNN模型,很难扩展到全连接神经网络(FCN)以及RNN等模型.此外,CAM方法需要修改网络结构并重训练模型,模型的准确性与可解释性之间始终存在一个权衡,且针对重训练模型做出的解释结果与原待解释模型的真实行为之间存在一定的不一致性,因而在真实应用场景中很难适用.Grad-CAM虽然解决了CAM需要进行网络修改和模型重训练的问题,但仍然与CAM方法一样只能提供粗粒度的解释结果,无法满足安全敏感应用场景(如自动驾驶、医疗诊断等)中对精细化解释的需要.Guided Grad-CAM方法作为CAM和Grad-CAM的加强版,既不需要修改网络结构或重训练模型,又能提供更细粒度的解释结果,但由于引入了导向反向传播方法,因而该方法同样存在由于负梯度归零导致无法定位与模型决策结果呈负相关的样本特征的局限性<citation id="1641" type="reference"><link href="1467" rel="bibliography" /><sup>[<a class="sup">115</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="408" name="408">3.2.6 其他方法</h4>
                <div class="p1">
                    <p id="409">除了上述5种典型的局部可解释方法外,其他研究者从不同的角度对模型可解释性进行了深入研究,并提出了一些新的局部解释方法,包括抽象解释<citation id="1642" type="reference"><link href="1361" rel="bibliography" /><sup>[<a class="sup">62</a>]</sup></citation>和准确一致解释<citation id="1643" type="reference"><link href="1363" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>等.</p>
                </div>
                <div class="p1">
                    <p id="410">针对DNN系统的可靠分析技术所面临的主要挑战是如何在解释神经网络某些特性的同时将其扩展到大规模的DNN分类器,因此,分析方法必须考虑到任何经过大量中间神经元处理的大规模输入集上所有可能的模型输出结果.由于模型的输入空间通常是巨大的,因而通过在所有可能的输入样本上运行模型来检查它们是否满足某一特性是不可行的.为解决这一挑战,避免状态空间爆炸,Gehr等人<citation id="1644" type="reference"><link href="1361" rel="bibliography" /><sup>[<a class="sup">62</a>]</sup></citation>将程序分析中的经典抽象解释框架应用于DNN分析,首次提出了可扩展的、可用于验证和分析DNN安全性和鲁棒性的抽象解释系统(AI<sup>2</sup>).具体地,AI<sup>2</sup>首先构造一个包含一系列逻辑约束和抽象元素的数值抽象域;由于DNN的每一层处理的是具体的数值,因而抽象元素无法在网络中传播.为解决此问题,AI<sup>2</sup>通过定义一个被称之为抽象转换器(abstract transformer)的函数将DNN的每一层转换为对应的抽象层,并基于抽象元素过近似(over-approximation)原神经网络每一层的处理函数以捕获其真实行为;最后,AI<sup>2</sup>基于抽象转换器返回的抽象结果,分析并验证神经网络的鲁棒性和安全性.AI<sup>2</sup>不用真正运行DNN模型即可验证DNN的某些特定属性,因而计算效率高,可扩展到大规模、更复杂的DNN网络.但由于采用了过近似处理,尽管AI<sup>2</sup>能提供可靠的解释但无法保证解释的准确性.</p>
                </div>
                <div class="p1">
                    <p id="411">现有局部解释方法包括抽象解释都很难保证解释结果的准确性和一致性,为此,许多学者开始研究针对DNN模型的精确解释方法.Chu等人<citation id="1645" type="reference"><link href="1363" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>提出了一种准确一致的解释方法(OpenBox),可为分段线性神经网络(PLNN)家族模型提供精确一致的解释.作者研究证明,PLNN在数学上等价于一系列的局部线性分类器,其中每一个线性分类器负责分类输入空间中的一组样本.因此,给定一个待解释PLNN模型,OpenBox首先利用神经网络的前向传播机制和矩阵运算将给定的PLNN模型表示成数学上与之等价的、由一系列数据依赖的局部线性分类器组成的线性解释模型;然后,针对每一个待解释样本,OpenBox基于该样本所对应的局部线性分类器提供对PLNN分类结果的解释.研究结果表明,由于线性解释模型数学上与待解释PLNN等价,因此基于线性解释模型给出的解释结果能精确地反映PLNN的真实决策行为,并且线性解释模型针对每一个输入的决策结果与待解释PLNN的决策结果完全一致,从而解决了模型的可解释性与准确性之间的权衡难题.此外,针对近似的样本,OpenBox可以给出一致的解释,保证了解释结果的一致性.然而,OpenBox作为针对PLNN家族的特定解释方法,只能解释线性神经网络模型,无法用于解释非线性神经网络模型.此外,如何将其扩展到CNN,RNN等更复杂的神经网络模型同样面临着巨大的挑战.</p>
                </div>
                <h3 id="412" name="412" class="anchor-tag"><b>4 可解释性应用</b></h3>
                <div class="p1">
                    <p id="413">机器学习模型可解释性相关技术潜在应用非常广泛,具体包括模型验证、模型诊断、辅助分析以及知识发现等.</p>
                </div>
                <h4 class="anchor-tag" id="414" name="414"><b>4.1 模型验证</b></h4>
                <div class="p1">
                    <p id="415">传统的模型验证方法通常是通过构造一个与训练集不相交的验证集,然后基于模型在验证集上的误差来评估模型的泛化性能,从而提供对模型好坏的一个粗粒度的验证.然而,由于数据集中可能存在偏差,并且验证集也可能与训练集同分布,我们很难简单地通过评估模型在验证集上的泛化能力来验证模型的可靠性,也很难验证模型是否从训练数据中学到了真正的决策知识.以冰原狼与哈士奇的分类为例,由于训练集中所有冰原狼样本图片的背景均为雪地,导致分类模型可能从训练集中学到数据偏差从而将雪作为冰原狼的分类特征,又由于验证集与训练集同分布,模型在验证集上的分类性能与在训练集上的性能同样优异,因而导致传统的模型验证方法将该模型识别为一个好的分类模型<citation id="1646" type="reference"><link href="1263" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.很显然,这样的模型通常是不可靠的,一旦模型在推理阶段遇到背景为雪地的哈士奇样本图片,分类模型会做出错误的决策,而模型的这种行为将会给实际场景尤其是风险敏感场景中的真实应用带来潜在的威胁.</p>
                </div>
                <div class="p1">
                    <p id="416">针对传统模型验证方法的不足,我们可以利用模型的可解释性及相关解释方法对模型可靠性进行更细粒度的评估和验证,从而消除模型在实际部署应用中的潜在风险.基于可解释性的模型验证方法一般思路如下:首先构造一个可信验证集,消除验证集中可能存在的数据偏差,保证验证数据的可靠性;然后,基于可信验证集,利用相关解释方法提供对模型整体决策行为(全局解释)或模型决策结果(局部解释)的解释;最后,基于解释方法给出的解释结果并结合人类认知,对模型决策行为和决策结果的可靠性进行验证,以检查模型是否在以符合人类认知的形式正常工作.</p>
                </div>
                <div class="p1">
                    <p id="417">在冰原狼与哈士奇分类的例子中,Ribeiro等人<citation id="1647" type="reference"><link href="1263" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>利用局部解释方法LIME解释分类模型针对一个背景为雪的哈士奇图片的分类结果,发现分类模型将该图片错误地分类为冰原狼,而解释方法给出的解释结果表明模型做出决策的依据是图片背景中的雪,如图8(a)所示.很显然,该解释结果与人类的认知相违背,表明模型在学习的过程中错误地将雪作为冰原狼的决策特征,从而证明该模型是不可靠的.类似地,Lapuschkin等人<citation id="1648" type="reference"><link href="1469" rel="bibliography" /><sup>[<a class="sup">116</a>]</sup></citation>利用LRP解释方法定性地分析一个从ImageNet中迁移训练得到的CNN模型和一个在PASCAL VOC 2007数据集上训练得到的Fisher向量(FV)分类器的决策结果,以检测训练数据中的潜在缺陷和偏差.研究结果表明,尽管2个模型具有相似的分类精度,但在对输入样本进行分类时却采用了完全不同的分类策略.从LRP解释方法给出的解释结果可以看出,如图8(b)所示,在对轮船图片进行分类时,FV分类器依据的是海水特征,而CNN模型则能正确地捕获到轮船的轮廓信息.与此同时,如果将位于水外的轮船作为测试样本,FV分类器的分类性能将大幅下降,而CNN模型则几乎不受影响.这一验证结果表明,FV分类器的决策行为存在偏差而CNN模型表现正常.因此,我们认为CNN模型比FV分类器更可靠,在进行模型选择时,我们将会选择CNN模型作为最终的分类模型.</p>
                </div>
                <div class="area_img" id="418">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_418.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 基于可解释性的模型验证示例" src="Detail/GetImg?filename=images/JFYZ201910004_418.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 基于可解释性的模型验证示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_418.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Examples of interpretation-based model validation</p>

                </div>
                <div class="p1">
                    <p id="419">而对于可解释方法所识别出的不可靠的模型,我们可以采取相应的对策来进行改进.比如说,我们可以通过在训练模型时引入归纳偏置,提高模型在预测阶段的泛化能力,从而使其能对未知样本做出正确的决策.我们也可以通过修正训练集分布,消除数据中存在的偏差,并利用修正后的数据集重训练模型达到消除模型决策偏差的目的.</p>
                </div>
                <h4 class="anchor-tag" id="420" name="420"><b>4.2 模型诊断</b></h4>
                <div class="p1">
                    <p id="421">由于机器学习模型内部工作机制复杂、透明性低,模型开发人员往往缺乏可靠的推理或依据来辅助他们进行模型开发和调试,因而使得模型开发迭代过程变得更加耗时且容易出错.而模型可解释性相关技术作为一种细粒度分析和解释模型的有效手段,可用于分析和调试模型的错误决策行为,以“诊断”模型中存在的缺陷,并为修复模型中的缺陷提供有力的支撑.近年来,随着模型可解释性研究不断取得新的突破,基于可解释性的机器学习模型诊断相关研究也吸引了越来越多的关注<citation id="1649" type="reference"><link href="1471" rel="bibliography" /><link href="1473" rel="bibliography" /><link href="1475" rel="bibliography" /><link href="1477" rel="bibliography" /><sup>[<a class="sup">117</a>,<a class="sup">118</a>,<a class="sup">119</a>,<a class="sup">120</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="422">研究表明:基于模型特征表示可视化以及中间层分析的解释方法(如激活最大化、特征反演等)可以有效地用于解释和诊断复杂模型.典型的解决方案包括可视化模型的中间激活状态或内部特征表示以及可视化模型中的数据流图<citation id="1654" type="reference"><link href="1479" rel="bibliography" /><link href="1481" rel="bibliography" /><link href="1483" rel="bibliography" /><sup>[<a class="sup">121</a>,<a class="sup">122</a>,<a class="sup">123</a>]</sup></citation>,以增强对复杂模型的解释和理解,同时分析和评估模型或算法的性能,为在模型开发的不同阶段(如前期特征工程、中期超参调整以及后期模型微调等)交互式改进模型提供有效的指导<citation id="1650" type="reference"><link href="1485" rel="bibliography" /><sup>[<a class="sup">124</a>]</sup></citation>.此外,一些其他的研究方法则通过识别与模型“漏洞”相关的重要特征或实例来进行模型诊断和调试.Krause等人<citation id="1651" type="reference"><link href="1487" rel="bibliography" /><sup>[<a class="sup">125</a>]</sup></citation> 基于敏感性分析解释方法的思想,设计了一个名为Prospector的系统,通过修改特征值并检查预测结果的相应变化来确定敏感性特征.Cadamuro等人<citation id="1652" type="reference"><link href="1471" rel="bibliography" /><sup>[<a class="sup">117</a>]</sup></citation>提出了一种概念分析和诊断循环的模型诊断方法,允许终端用户迭代地检测模型“漏洞”,以找到对模型“漏洞”贡献最大的训练实例,从而确定模型出错的根本原因.Krause等人<citation id="1653" type="reference"><link href="1489" rel="bibliography" /><sup>[<a class="sup">126</a>]</sup></citation>提出了一个可视化模型诊断工作流,通过利用局部解释方法度量输入实例中的局部特征相关性,以帮助数据科学家和领域专家理解和诊断模型所做出的决策.具体地,该工作流首先利用聚合统计查看数据在正确决策和错误决策之间的分布;然后,基于解释方法理解用于做出这些决策的特征;最后基于原始数据,对影响模型决策的潜在根本原因进行深入分析.</p>
                </div>
                <div class="p1">
                    <p id="423">针对已发现的模型“漏洞”,我们可以基于模型诊断方法给出的推理结果,采取相应的措施对模型进行“治疗”,如提高训练数据的质量、选择可靠特征以及调整模型超参等.Paiva等人<citation id="1655" type="reference"><link href="1491" rel="bibliography" /><sup>[<a class="sup">127</a>]</sup></citation>提出了一种可视化数据分类方法,该方法通过点布局策略实现数据集的可视化,允许用户选择并指定用于模型学习过程的训练数据,从而提高训练集的整体质量.Brooks等人<citation id="1656" type="reference"><link href="1493" rel="bibliography" /><sup>[<a class="sup">128</a>]</sup></citation>提出了一个用于改进特征工程的交互式可视化分析系统,该系统支持错误驱动的特征构思过程并为误分类样本提供交互式可视化摘要,允许在误分类样本和正确分类样本之间进行特征级别的比较,以选择能减小模型预测错误率的特征,从而提高模型性能并修复模型中的“漏洞”.</p>
                </div>
                <h4 class="anchor-tag" id="424" name="424"><b>4.3 辅助分析</b></h4>
                <div class="p1">
                    <p id="425">除了用于模型验证与模型诊断之外,可解释性相关技术还可用于辅助分析与决策,以提高人工分析和决策的效率.相关研究表明,基于可解释性的辅助分析技术在医疗数据分析、分子模拟以及基因分析等多个领域取得了巨大的成功,有效地解决了人工分析耗时费力的难题.</p>
                </div>
                <div class="p1">
                    <p id="426">在智慧医疗领域,许多学者尝试将深度学习及可解释性技术应用于构建自动化智能诊断系统,以辅助医护人员分析病人的医疗诊断数据,从而提高人工诊断的效率<citation id="1659" type="reference"><link href="1249" rel="bibliography" /><link href="1495" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">129</a>]</sup></citation>.Rajpurkar等人<citation id="1657" type="reference"><link href="1249" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>基于大规模病人胸片数据开发了基于深度学习的肺炎检测系统(CheXNet),其检测性能甚至超过了放射科医师的诊断水平,该系统通过将可解释方法CAM应用于解释检测系统的决策依据并可视化对应的解释结果(如图9所示),可以为医师分析病人医疗影像数据以快速定位病人的病灶提供大量的辅助信息.Arvaniti等人<citation id="1658" type="reference"><link href="1495" rel="bibliography" /><sup>[<a class="sup">129</a>]</sup></citation>研究结果表明,在给定一个良好标注的数据集的前提下,可以利用CNN模型成功地实现对前列腺癌组织微阵列的自动格里森分级.同时,利用解释方法给出自动分级系统的分级依据,可实现病理专家级的分级效果,从而为简化相对繁琐的分级任务提供了支撑.</p>
                </div>
                <div class="area_img" id="427">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910004_427.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 可解释方法在医疗诊断中的应用[6]" src="Detail/GetImg?filename=images/JFYZ201910004_427.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 可解释方法在医疗诊断中的应用<citation id="1660" type="reference"><link href="1249" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910004_427.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Application of interpretation in medical diagnosis</p>

                </div>
                <div class="p1">
                    <p id="428">在量子化学领域,分子动力学模拟是理解化学反应机理、速率和产率的关键,然而由于分子的完整波函数相对复杂,且难以计算和近似,导致人们通常难以理解,因而如何创建人类可解释的分子表示成为21世纪物质模拟的一大挑战<citation id="1661" type="reference"><link href="1497" rel="bibliography" /><sup>[<a class="sup">130</a>]</sup></citation>.为解决这一难题,许多学者将机器学习及可解释性技术引入到分子模拟任务中,用于辅助分析分子结构与分子性质之间的关系<citation id="1664" type="reference"><link href="1499" rel="bibliography" /><link href="1501" rel="bibliography" /><link href="1503" rel="bibliography" /><sup>[<a class="sup">131</a>,<a class="sup">132</a>,<a class="sup">133</a>]</sup></citation>.其中,Schütt等人<citation id="1662" type="reference"><link href="1503" rel="bibliography" /><sup>[<a class="sup">133</a>]</sup></citation>提出一种通过结合强大的结构和表示能力以实现较高预测性能和良好可解释性的深度张量神经网络(DTNN),用于预测分子结构与电子性质之间的关系.同时,作者利用基于测试电荷扰动的敏感性分析方法测量在给定的位置插入电荷对DTNN输出结果的影响,从而找到与解释分子结构与性质关系最相关的每个单独的分子空间结构.Häse等人<citation id="1663" type="reference"><link href="1501" rel="bibliography" /><sup>[<a class="sup">132</a>]</sup></citation>提出一种利用机器学习来辅助分子动力学模拟的方法,该方法利用模拟产生的大量数据训练贝叶斯神经网络(BNN)来预测1,2-二氧杂环丁烷从初始核位置的离解时间.为了构建一个可解释的BNN模型,作者将模型的权重和偏置分布参数化为拉普拉斯分布,以确定与准确预测离解时间以及实际的物理过程相关的输入特征.研究结果表明,该方法不仅可以准确地再现化合物的离解过程,而且能自动地从模拟数据中提取相关信息,而不需要预先了解相关化学反应.同时,通过解释BNN所捕获的特征与实际物理过程之间的相关关系,可以在不了解电子结构的情况下,确定核坐标与离解时间之间的物理相关性,从而为人们在化学领域取得概念性的突破提供灵感.</p>
                </div>
                <div class="p1">
                    <p id="429">在基因组分析领域,由基因组学研究不断进步而产生的数据爆炸,给传统的基因组分析方法带来了巨大的挑战,同时也给数据驱动的深度学习技术在基因组分析研究中的发展和应用带来了机遇<citation id="1665" type="reference"><link href="1505" rel="bibliography" /><sup>[<a class="sup">134</a>]</sup></citation>.相关研究表明,深度学习在基因组分析中的应用已突显出了其强大的优势<citation id="1669" type="reference"><link href="1507" rel="bibliography" /><link href="1509" rel="bibliography" /><link href="1511" rel="bibliography" /><link href="1513" rel="bibliography" /><sup>[<a class="sup">135</a>,<a class="sup">136</a>,<a class="sup">137</a>,<a class="sup">138</a>]</sup></citation>.然而,人们期望深度学习模型不仅能成功地预测结果,还能识别有意义的基因序列,并对所研究的科学问题(如基因与疾病、药物之间的关系)提供进一步的见解,因而模型的可解释性在应用中显得至关重要.Lanchantin等人<citation id="1666" type="reference"><link href="1511" rel="bibliography" /><sup>[<a class="sup">137</a>]</sup></citation>将3种DNN模型(即CNN,RNN以及CNN-RNN)应用于预测给定的DNA序列中某一特定的转录因子是否有结合位点,并且提出了一套基于解释方法的可视化策略,用于解释对应的预测模型并从中提取隐含的序列模式.其中,作者基于反向传播解释方法,通过计算预测概率相对于输入DNA序列的梯度来构建显著图<citation id="1667" type="reference"><link href="1341" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>,用于度量并显示核苷酸的重要性.同时,作者利用时间域输出分值来识别DNN序列中与特定转录因子结合位点相关的关键序列位置,并利用类激活最大化方法生成与特定预测结果相关的Motif模式.实验结果证明,这一系列的可视化策略可为研究人员分析DNA序列结构、组成成分与特定转录因子结合位点之间的关系提供大量的辅助信息.类似地,Alipanahi等人<citation id="1668" type="reference"><link href="1513" rel="bibliography" /><sup>[<a class="sup">138</a>]</sup></citation>构建了一个名为DeepBind的系统,通过训练一个CNN模型将DNA和RNA序列映射到蛋白质结合位点上,以了解DNA和RNA结合蛋白的序列特异性.为了进一步探索遗传变异对蛋白质结合位点的影响,作者采用了基于扰动的敏感性分析方法,通过计算突变对DeepBind预测结果的影响生成“突变图”,以解释序列中每个可能的点突破对结合亲和力的影响.作者表明,DeepBind可用于揭示RNA结合蛋白质在选择性剪接中的调节作用,并辅助研究人员分析、识别、分组及可视化可影响转录因子结合和基因表达的疾病相关遗传变异,从而有望实现精准医学.</p>
                </div>
                <h4 class="anchor-tag" id="430" name="430"><b>4.4 知识发现</b></h4>
                <div class="p1">
                    <p id="431">近年来,随着人工智能相关技术的发展,基于机器学习的自动决策系统被广泛地应用到各个领域,如恶意程序分析、自动化医疗诊断以及量化交易等.然而,由于实际任务的复杂性以及人类认知和领域知识的局限性,人们可能无法理解决策系统给出的结果,因而缺乏对相关领域问题更深入的理解,进而导致许多科学问题难以得到有效的解决.最新研究成果表明,通过将可解释性相关技术与基于机器学习的自动决策系统相结合,可有效地挖掘出自动决策系统从数据中学到的新知识,以提供对所研究科学问题的深入理解,从而弥补人类认知与领域知识的局限性.</p>
                </div>
                <div class="p1">
                    <p id="432">在二进制分析领域,许多潜在的启发式方法都是针对某一个特定的函数的,而挖掘这些潜在的方法通常需要丰富的领域知识,因而很难通过人工的方式对所有的启发式方法进行汇总.Guo等人<citation id="1670" type="reference"><link href="1339" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>将可解释方法LEMNA应用于一个基于LSTM的二进制函数入口检测器,以提供对LSTM检测结果的解释.通过分析解释结果,作者发现检测模型确实从训练数据中学到了用于识别函数入口的潜在特征,这表明利用LEMNA解释方法可以挖掘出检测模型从数据中学到的新知识,从而对总结针对某个特殊函数的所有潜在的启发式方法提供帮助.</p>
                </div>
                <div class="p1">
                    <p id="433">在医疗保健领域,由于病人病理错综复杂且因人而异,医护人员往往无法通过有限的医疗诊断知识挖掘潜在的致病因素及其之间的相互作用,而对潜在因素的忽视极其可能带来致命的威胁.Yang等人<citation id="1671" type="reference"><link href="1319" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>基于重症监护室(ICU)治疗记录数据构建了一个带注意力机制的RNN模型,用于分析医疗条件与ICU死亡率之间的关系,而这些关系在以往的医疗实践中往往没有得到很好的研究.作者研究结果表明,利用可解释性技术有助于发现与医疗保健中某些结果相关的潜在影响因素或相互作用,从而使得从自动化医疗诊断模型中学习新的诊断知识成为可能.</p>
                </div>
                <div class="p1">
                    <p id="434">此外,作为知识发现的重要手段,模型可解释性及其相关解释方法还被广泛地应用到了数据挖掘领域,以从海量数据中自动地挖掘隐含的新知识<citation id="1672" type="reference"><link href="1515" rel="bibliography" /><link href="1517" rel="bibliography" /><link href="1519" rel="bibliography" /><link href="1521" rel="bibliography" /><sup>[<a class="sup">139</a>,<a class="sup">140</a>,<a class="sup">141</a>,<a class="sup">142</a>]</sup></citation>.这类研究核心思想是基于所研究的领域及科学目标构建海量数据集,然后对构建的数据集进行清洗并利用机器学习模型从清洗后的数据中提取数据映射模式,最后利用解释方法从挖掘到的数据模式识别代表新知识的模式并利用可视化技术将新知识呈现给用户.</p>
                </div>
                <h3 id="435" name="435" class="anchor-tag"><b>5 可解释性与安全性分析</b></h3>
                <div class="p1">
                    <p id="436">模型可解释性研究的初衷是通过构建可解释的模型或设计解释方法提高模型的透明性,同时验证和评估模型决策行为和决策结果的可靠性和安全性,消除模型在实际部署应用中的安全隐患.然而,模型可解释性相关技术同样可以被攻击者利用以探测机器学习模型中的“漏洞”,因而会给机器学习模型以及真实应用场景中尤其是风险敏感场景中的机器学习应用带来威胁.此外,由于解释方法与待解释模型之间可能存在不一致性,因而可解释系统或可解释方法本身就存在一定的安全风险.</p>
                </div>
                <h4 class="anchor-tag" id="437" name="437"><b>5.1 安全隐患消除</b></h4>
                <div class="p1">
                    <p id="438">如第4节中所述,模型可解释性及相关解释方法不仅可以用于评估和验证机器学习模型,以弥补传统模型验证方法的不足,保证模型决策行为和决策结果的可靠性和安全性,还可用于辅助模型开发人员和安全分析师诊断和调试模型以检测模型中的缺陷,并为安全分析师修复模型“漏洞”提供指导,从而消除模型在实际部署应用中的安全隐患.并且,通过同时向终端用户提供模型的预测结果及对应的解释结果,可提高模型决策的透明性,进而有助于建立终端用户与决策系统之间的信任关系.</p>
                </div>
                <div class="p1">
                    <p id="439">除了用于消除上述内在安全隐患之外,模型可解释性相关技术还可以帮助抵御外在安全风险.人工智能安全领域相关研究表明即使决策“可靠”的机器学习模型也同样容易受到对抗样本攻击,只需要在输入样本中添加精心构造的、人眼不可察觉的扰动就可以轻松地让模型决策出错<citation id="1673" type="reference"><link href="1253" rel="bibliography" /><link href="1523" rel="bibliography" /><link href="1525" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">143</a>,<a class="sup">144</a>]</sup></citation>.这种攻击危害性大、隐蔽性强、变种多且难以防御,严重地威胁着人工智能系统的安全.而现存防御方法大多数是针对某一个特定的对抗样本攻击设计的静态的经验性防御,因而防御能力极其有限.然而,不管是哪种攻击方法,其本质思想都是通过向输入中添加扰动以转移模型的决策注意力,最终使模型决策出错.由于这种攻击使得模型决策依据发生变化,因而解释方法针对对抗样本的解释结果必然与其针对对应的正常样本的解释结果不同.因此,我们可以通过对比并利用这种解释结果的反差来检测对抗样本,而这种方法并不特定于某一种对抗攻击,因而可以弥补传统经验性防御的不足.</p>
                </div>
                <div class="p1">
                    <p id="440">除上述防御方法外,很多学者从不同的角度提出了一些新的基于可解释性技术的对抗防御方法.其中,Tao等人<citation id="1674" type="reference"><link href="1527" rel="bibliography" /><sup>[<a class="sup">145</a>]</sup></citation>认为对抗攻击与模型的可解释性密切相关,即对于正常样本的决策结果,可以基于人类可感知的特征或属性来进行推理,而对于对抗样本的决策结果我们则通常无法解释.基于这一认知,作者提出一种针对人脸识别模型的对抗样本检测方法,该方法首先利用敏感性分析解释方法识别与人类可感知属性相对应的神经元,称之为“属性见证”神经元;然后,通过加强见证神经元同时削弱其他神经元将原始模型转换为属性导向模型,对于正常样本,属性导向模型的预测结果与原始模型一致,对于对抗样本二者预测结果则不一致;最后,利用2个模型预测结果的不一致性来检测对抗样本,实现对对抗攻击的防御.Liu等人<citation id="1675" type="reference"><link href="1529" rel="bibliography" /><sup>[<a class="sup">146</a>]</sup></citation>则基于对分类模型的解释,提出了一种新的对抗样本检测框架.给定一个恶意样本检测器,该框架首先选择一个以确定为恶意样本的样本子集作为种子样本,然后构建一个局部解释器解释种子样本被分类器视为恶意样本的原因,并通过朝着解释器确定的规避方向来扰动每一个种子样本的方式产生对抗样本.最后,通过利用原始数据和生成的对抗样本对检测器进行对抗训练,以提高检测器对对抗样本的鲁棒性,从而降低模型的外在安全风险.</p>
                </div>
                <h4 class="anchor-tag" id="441" name="441"><b>5.2 安全威胁</b></h4>
                <div class="p1">
                    <p id="442">尽管可解释性技术是为保证模型可靠性和安全性而设计的,但其同样可以被恶意用户滥用而给实际部署应用的机器学习系统带来安全威胁.比如说,攻击者可以利用解释方法探测能触发模型崩溃的模型漏洞,在对抗攻击中,攻击者还可以利用可解释方法探测模型的决策弱点或决策逻辑,从而为设计更强大的攻击提供详细的信息.在本文中,我们将以对抗攻击为例,阐述可解释性技术可能带来的安全风险.</p>
                </div>
                <div class="p1">
                    <p id="443">在白盒对抗攻击中,攻击者可以获取目标模型的结构、参数信息,因而可以利用反向传播解释方法的思想来探测模型的弱点<citation id="1676" type="reference"><link href="1531" rel="bibliography" /><sup>[<a class="sup">147</a>]</sup></citation>.其中,Goodfellow等人<citation id="1677" type="reference"><link href="1523" rel="bibliography" /><sup>[<a class="sup">143</a>]</sup></citation>提出了快速梯度符号攻击方法(FGSM),通过计算模型输出相对于输入样本的梯度信息来探测模型的敏感性,并通过朝着敏感方向添加一个固定规模的噪音来生成对抗样本.Papernot等人<citation id="1678" type="reference"><link href="1533" rel="bibliography" /><sup>[<a class="sup">148</a>]</sup></citation>基于Grad<citation id="1679" type="reference"><link href="1341" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>解释方法提出了雅可比显著图攻击(JSMA),该攻击方法首先利用Grad解释方法生成显著图,然后基于选择图来选择最重要的特征进行攻击.利用Grad方法提供的特征重要性信息,JMSA攻击只需要扰动少量的特征就能达到很高的攻击成功率,因而攻击的隐蔽性更强.对于黑盒对抗攻击,由于无法获取模型的结构信息,只能操纵模型的输入和输出<citation id="1680" type="reference"><link href="1535" rel="bibliography" /><sup>[<a class="sup">149</a>]</sup></citation>,因而攻击者可以利用模型无关解释方法的思想来设计攻击方法.其中,Papernot等人<citation id="1681" type="reference"><link href="1537" rel="bibliography" /><sup>[<a class="sup">150</a>]</sup></citation>提出了一种针对黑盒机器学习模型的替代模型攻击方法.该方法首先利用模型蒸馏解释方法的思想训练一个替代模型来拟合目标黑盒模型的决策结果,以完成从黑盒模型到替代模型的知识迁移过程;然后,利用已有的攻击方法针对替代模型生成对抗样本;最后,利用生成的对抗样本对黑盒模型进行迁移攻击.Li等人<citation id="1682" type="reference"><link href="1255" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了一种基于敏感性分析解释方法的文本对抗攻击方法(TextBugger),用于攻击真实场景中的情感分析模型和垃圾文本检测器.该方法首先通过观察去掉某个词前后模型决策结果的变化来定位文本中的重要单词,然后通过利用符合人类感知的噪音逐个扰动重要的单词直到达到攻击目标.该研究表明,利用TextBugger攻击方法可以轻松的攻破Google Cloud,Microsoft Azure,Amazon AWS,IBM Watson,Facebook fastText等平台提供的商业自然语言处理机器学习服务,并且攻击成功率高、隐蔽性强.</p>
                </div>
                <h4 class="anchor-tag" id="444" name="444"><b>5.3 自身安全问题</b></h4>
                <div class="p1">
                    <p id="445">由于采用了近似处理或是基于优化手段,大多数解释方法只能提供近似的解释,因而解释结果与模型的真实行为之间存在一定的不一致性.而最新研究表明,攻击者可以利用解释方法与待解释模型之间的这种不一致性设计针对可解释系统的新型对抗样本攻击,因而严重的威胁着可解释系统的自身安全.</p>
                </div>
                <div class="p1">
                    <p id="446">根据攻击目的不同,现存针对可解释系统的新型对抗样本攻击可以分为2类:1)在不改变模型的决策结果的前提下,使解释方法解释出错<citation id="1683" type="reference"><link href="1539" rel="bibliography" /><sup>[<a class="sup">151</a>]</sup></citation>;2)使模型决策出错而不改变解释方法的解释结果<citation id="1684" type="reference"><link href="1541" rel="bibliography" /><sup>[<a class="sup">152</a>]</sup></citation>.其中,Ghorbani等人<citation id="1685" type="reference"><link href="1539" rel="bibliography" /><sup>[<a class="sup">151</a>]</sup></citation>首次将对抗攻击的概念引入到了神经网络的可解释性中并且提出了模型解释脆弱性的概念.具体地,他们将针对解释方法的对抗攻击定义为优化问题:</p>
                </div>
                <div class="p1">
                    <p id="447"><mathml id="465"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>δ</mi></munder><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>;<i>N</i>),<i>I</i>(<i>x</i><sub><i>t</i></sub>+<i>δ</i>;<i>N</i>))</p>
                </div>
                <div class="p1">
                    <p id="448">s.t. ‖<i>δ</i>‖<sub>∞</sub>≤<i>ε</i>,<i>f</i>(<i>x</i><sub><i>t</i></sub>+<i>δ</i>)=<i>f</i>(<i>x</i><sub><i>t</i></sub>),</p>
                </div>
                <div class="p1">
                    <p id="449">其中,<i>I</i>(<i>x</i><sub><i>t</i></sub>;<i>N</i>)为解释系统对神经网络<i>N</i>针对样本<i>x</i><sub><i>t</i></sub>决策结果<i>f</i>(<i>x</i><sub><i>t</i></sub>)的解释,<i>δ</i>为样本中所需添加的扰动,<i>D</i>(·)用于度量扰动前后解释结果的变化.通过优化上述目标函数,可以在不改变模型决策结果的前提下,生成能让解释方法产生截然不同的解释结果的对抗样本.针对Grad<citation id="1686" type="reference"><link href="1341" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>,Integrated<citation id="1687" type="reference"><link href="1347" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>以及DeepLIFT<citation id="1688" type="reference"><link href="1353" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>等反向传播解释方法的对抗攻击实验证明,上述解释方法均容易受到对抗样本攻击,因而只能提供脆弱的模型解释.与Ghorbani等人研究相反,Zhang等人<citation id="1689" type="reference"><link href="1541" rel="bibliography" /><sup>[<a class="sup">152</a>]</sup></citation>提出了Acid攻击,旨在生成能让模型分类出错而不改变解释方法解释结果的对抗样本.通过对表示导向的(如激活最大化、特征反演等)、模型导向的(如基于掩码模型的显著性检测等<citation id="1690" type="reference"><link href="1543" rel="bibliography" /><sup>[<a class="sup">153</a>]</sup></citation>)以及扰动导向的(如敏感性分析等)三大类解释方法进行Acid攻击和经验性评估,作者发现生成欺骗分类器及其解释方法的对抗样本实际上并不比生成仅能欺骗分类器的对抗样本更困难.因此,这几类解释方法同样是脆弱的,在对抗的环境下,其提供的解释结果未必可靠.此外,这种攻击还会使基于对比攻击前后解释结果的防御方法失效,导致对抗攻击更难防御.</p>
                </div>
                <div class="p1">
                    <p id="450">上述研究表明:现存解释方法大多数是脆弱的,因此只能提供有限的安全保证.但由于可解释性技术潜在应用广泛,因而其自身安全问题不容忽视.以医疗诊断中的可解释系统为例,在临床治疗中,医生会根据可解释系统提供的解释结果对病人进行相应的诊断和治疗,一旦解释系统被新型对抗攻击方法攻击,那么提供的解释结果必然会影响医生的诊断过程,甚至是误导医生的诊断而给病人带来致命的威胁.因此,仅有解释是不够的,为保证机器学习及可解释性技术在实际部署应用中的安全,解释方法本身必须是安全的,而设计更精确的解释方法以消除解释方法与决策系统之间的不一致性则是提高解释方法鲁棒性进而消除其外在安全隐患的重要途径.</p>
                </div>
                <h3 id="451" name="451" class="anchor-tag"><b>6 当前挑战与未来方向</b></h3>
                <div class="p1">
                    <p id="452">尽管模型可解释性研究已取得一系列瞩目的研究成果,但其研究还处于初级阶段,依然面临着许多的挑战且存在许多的关键问题尚待解决.其中,可解释性研究当前面临的一个挑战是如何设计更精确、更友好的解释方法,消除解释结果与模型真实行为之间的不一致;第2个挑战是如何设计更科学、更统一的可解释性评估指标,以评估可解释方法解释性能和安全性.</p>
                </div>
                <h4 class="anchor-tag" id="453" name="453"><b>6.1 解释方法设计</b></h4>
                <div class="p1">
                    <p id="454">精确地理解机器学习的工作原理,研究透明的、可解释且可证明机器学习技术,有助于推动机器学习研究的进一步发展,同时有助于促进人工智能相关技术的落地应用.这要求机器学习可解释性研究必须具备能精确地揭示模型内部工作逻辑同时向人类提供可以足够准确理解模型决策的信息的能力.因此,无论是ante -hoc可解释性还是post-hoc可解释性,我们所设计的解释方法都必须是精确的,我们的解释方法提供的解释结果都必须忠实于模型的真实决策行为.</p>
                </div>
                <div class="p1">
                    <p id="455">由于模型的决策准确性与模型自身可解释性之间存在一个权衡,现有关于ante -hoc可解释性的研究多局限于诸如线性回归、决策树等算法透明、结构简单的模型,对于复杂的DNN模型则只能依赖于注意力机制提供一个粗粒度的解释.因此,如何设计可解释的机器学习模型以消除模型准确性与可解释性之间的制约是ante -hoc可解释性研究所面临的一大挑战,也是未来可解释性研究发展的一个重要趋势.其中,一种直观的方法是将机器学习与因果模型相结合,让机器学习系统具备从观察数据中发现事物间的因果结构和定量推断的能力.同时,我们还可以将机器学习与常识推理和类比计算等技术相结合,形成可解释的、能自动推理的学习系统.未来我们还可以考虑利用仿生学知识并结合更先进的认知理论对人类认知建模,以设计具备人类自我解释能力的机器学习模型,实现具有一定思维能力并且能自我推理自我解释的强人工智能系统.</p>
                </div>
                <div class="p1">
                    <p id="456">对于post-hoc可解释性而言,大多数的研究都在尝试采用近似的方法来模拟模型的决策行为,以从全局的角度解释模型的整体决策逻辑或者从局部的角度解释模型的单个决策结果.然而,由于近似过程往往不够精确,解释方法给出的解释结果无法正确地反映待解释模型的实际运行状态和真实决策行为,而解释方法与决策模型之间的这种不一致性甚至严重地威胁着可解释系统自身的安全.因此,当前post-hoc可解释性相关研究面临的巨大挑战是如何设计忠实于决策模型的安全可保障的精确解释方法,以消除解释结果与模型真实行为之间的不一致性,从而保证解释结果的可靠性和安全性.未来一个有前景的潜在研究方向是设计数学上与待解释模型等价的解释方法或解释模型.对于全连接神经网络,Chu等人<citation id="1691" type="reference"><link href="1363" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>已经给出了相应的研究方法并取得了一定的研究成果,我们则可以基于具体模型的内部机理和神经网络的前向传播机制,将Chu等人提出的研究方法扩展到CNN,RNN等更复杂神经网络模型,从而实现对复杂模型的精确解释.</p>
                </div>
                <h4 class="anchor-tag" id="457" name="457"><b>6.2 解释方法评估</b></h4>
                <div class="p1">
                    <p id="458">目前,可解释性研究领域缺乏一个用于评估解释方法的科学评估体系,尤其是在计算机视觉领域,许多解释方法的评估还依赖于人类的认知,因而只能定性评估,无法对解释方法的性能进行量化,也无法对同类型的研究工作进行精确地比较.并且,由于人类认知的局限性,人们只能理解解释结果中揭示的显性知识,而通常无法理解其隐性知识,因而无法保证基于认知的评估方法的可靠性.</p>
                </div>
                <div class="p1">
                    <p id="459">对于ante -hoc可解释性而言,其评估挑战在于如何量化模型的内在解释能力.对于同一应用场景,我们可能会采用不同的模型,同一模型也可能会应用到不同的场景中,而对于如何衡量和比较这些模型的可解释性目前仍没有达成共识.由于模型自身可解释性受实际应用场景、模型算法本身以及人类理解能力的制约,未来我们可以从应用场景、算法功能、人类认知这3个角度来设计评估指标.这些指标虽各有利弊但相互补充,可以实现多层次、细粒度的可解释性评估,以弥补单一评估指标的不足.</p>
                </div>
                <div class="p1">
                    <p id="460">对于post-hoc可解释性而言,其评估挑战在于如何量化解释结果的保真度和一致性.如前所述,由于人类认知的局限性,解释方法针对机器学习模型给出的解释结果并不总是“合理”的,而我们很难判断这种与人类认知相违背的解释结果到底是由于模型自身的错误行为还是解释方法的局限性,抑或是人类认知的局限性造成的.因此,我们需要设计可靠的评估指标对解释方法进行定量的评估.Guo等人<citation id="1692" type="reference"><link href="1339" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>提出利用解释方法给出的预测结果与待解释模型预测结果之间的均方根误差(RMSE)来评估解释方法的保真度,然而这种评估指标无法用于评估激活最大化、敏感性分析、反向传播以及特征反演等不提供预测结果的解释方法.Chu等人<citation id="1693" type="reference"><link href="1363" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>提出利用输入样本及其邻近样本的解释结果的余弦相似性来评估解释方法,然而这种方法无法用于评估解释结果的保真度.此外,目前还缺乏用于评估针对同一模型的不同解释方法的评估指标.因此,未来我们需要从解释结果的保真度、一致性以及不同解释方法的差异性等角度设计评价指标,对解释方法进行综合评估.</p>
                </div>
                <h3 id="461" name="461" class="anchor-tag"><b>7 结束语</b></h3>
                <div class="p1">
                    <p id="462">机器学习可解释性是一个非常有前景的研究领域,该领域已经成为了国内外学者的研究热点,并且取得了许多瞩目的研究成果.但到目前为止,机器学习可解释性研究还处于初级阶段,依然存在许多关键问题尚待解决.为了总结现有研究成果的优势与不足,探讨未来研究方向,本文从可解释性相关技术、潜在应用、安全性分析等方面对现有研究成果进行了归类、总结和分析,同时讨论了当前研究面临的挑战和未来潜在的研究方向,旨在为推动模型可解释性研究的进一步发展和应用提供一定帮助.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="1239">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                Schroff F,Kalenichenko D,Philbin J.Facenet:A unified embedding for face recognition and clustering[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:815- 823
                            </a>
                        </p>
                        <p id="1241">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepid3:Face recognition with very deep neural networks">

                                <b>[2]</b>Sun Yi,Liang Ding,Wang Xiaogang,et al.Deepid3:Face recognition with very deep neural networks[J].arXiv preprint arXiv:1502.00873,2015
                            </a>
                        </p>
                        <p id="1243">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                Taigman Y,Yang Ming,Ranzato M A,et al.Deepface:Closing the gap to human-level performance in face verification[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2014:1701- 1708
                            </a>
                        </p>
                        <p id="1245">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Are We Ready For Autonomous Driving? TheKITTI Vision Benchmark Suite">

                                <b>[4]</b>Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?the kitti vision benchmark suite[C] //Proc of IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:3354- 3361
                            </a>
                        </p>
                        <p id="1247">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Malware detection with deep neural network using process behavior">

                                <b>[5]</b>Tobiyama S,Yamaguchi Y,Shimada H,et al.Malware detection with deep neural network using process behavior[C] //Proc of the 40th Annual Computer Software and Applications Conf.Piscataway,NJ:IEEE,2016:577- 582
                            </a>
                        </p>
                        <p id="1249">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chexnet:Radiologist-level pneumonia detection on chest x-rays with deep learning">

                                <b>[6]</b>Rajpurkar P,Irvin J,Zhu K,et al.Chexnet:Radiologist-level pneumonia detection on chest x-rays with deep learning[J].arXiv preprint arXiv:1711.05225,2017
                            </a>
                        </p>
                        <p id="1251">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global Explanations of Neural Networks:Mapping the Landscape of Predictions">

                                <b>[7]</b>Ibrahim M,Louie M,Modarres C,et al.Global Explanations of Neural Networks:Mapping the Landscape of Predictions[J].arXiv preprint arXiv:1902.02384,2019
                            </a>
                        </p>
                        <p id="1253">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks">

                                <b>[8]</b>Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[J].arXiv preprint arXiv:1302.6199,2013
                            </a>
                        </p>
                        <p id="1255">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TextBugger:Generating adversarial text against real-world applications">

                                <b>[9]</b>Li Jinfeng,Ji Shouling,Du Tianyu,et al.TextBugger:Generating adversarial text against real-world applications[C] //Proc of the 26th Annual Network and Distributed Systems Security Symp.Reston,VA:ISOC,2019
                            </a>
                        </p>
                        <p id="1257">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SirenAttack:Generating adversarial audio for end-to-end acoustic systems">

                                <b>[10]</b>Du Tianyu,Ji Shouling,Li Jinfeng,et al.SirenAttack:Generating adversarial audio for end-to-end acoustic systems[J].arXiv preprint arXiv:1901.07846,2019
                            </a>
                        </p>
                        <p id="1259">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards a rigorous science of interpretable machine learning">

                                <b>[11]</b>Doshi-Velez F,Kim B.Towards a rigorous science of interpretable machine learning[J].arXiv preprint arXiv:1702.08608,2017
                            </a>
                        </p>
                        <p id="1261">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM54EF9F7B61C0881B34CDC725ABD161FB&amp;v=MzA5NzFKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnBod0wyNnhLND1OaWZJWTdhOGE2ZkYyWWczWXVwOERIUXh6bVFRN2t3Sk8zamdxV05IRGJPU1JNenRDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Guidotti R,Monreale A,Ruggieri S,et al.A survey of methods for explaining black box models[J].ACM Computing Surveys,2018,51(5):No.93
                            </a>
                        </p>
                        <p id="1263">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining the predictions of any classifier">

                                <b>[13]</b>Ribeiro M T,Singh S,Guestrin C.Why should I trust you?Explaining the predictions of any classifier[C] //Proc of the 22nd ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2016:1135- 1144
                            </a>
                        </p>
                        <p id="1265">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=How to Explain Individual Classification Decisions">

                                <b>[14]</b>Baehrens D,Schroeter T,Harmeling S,et al.How to explain individual classification decisions[J].Journal of Machine Learning Research,2010,11(6):1803- 1831
                            </a>
                        </p>
                        <p id="1267">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards robust interpretability with self-explaining neural networks">

                                <b>[15]</b>Melis D A,Jaakkola T.Towards robust interpretability with self-explaining neural networks[C] //Proc of the 32nd Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2018:7775- 7784
                            </a>
                        </p>
                        <p id="1269">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual explanation of evidence with additive classifiers">

                                <b>[16]</b>Poulin B,Eisner R,Szafron D,et al.Visual explanation of evidence with additive classifiers[C] //Proc of the 18th Conf on Innovative Applications of Artificial Intelligence.Palo Alto,CA:AAAI,2006:1822- 1829
                            </a>
                        </p>
                        <p id="1271">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Efficient Explanation of Individual Classifications using Game Theory">

                                <b>[17]</b>Kononenko I.An efficient explanation of individual classifications using game theory[J].Journal of Machine Learning Research,2010,11(1):1- 18
                            </a>
                        </p>
                        <p id="1273">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300124656&amp;v=MjczMTl1SHlqbVVMM0lKRnNSYmhRPU5pZk9mYks4SHRMT3JJOUZaZWtMQ25rL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Haufe S,Meinecke F,Görgen K,et al.On the interpretation of weight vectors of linear models in multivariate neuroimaging[J].NeuroImage,2014,87:96- 110
                            </a>
                        </p>
                        <p id="1275">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300403312&amp;v=MTc0NDdHZXJxUVRNbndaZVp1SHlqbVVMM0lKRnNSYmhRPU5pZk9mYks3SHRET3JJOUZZT3NNRDMwN29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>Huysmans J,Dejaeger K,Mues C,et al.An empirical evaluation of the comprehensibility of decision table,tree and rule based predictive models[J].Decision Support Systems,2011,51(1):141- 154
                            </a>
                        </p>
                        <p id="1277">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simplifying decision trees: A survey">

                                <b>[20]</b>Breslow L A,Aha D W.Simplifying decision trees:A survey[J].The Knowledge Engineering Review,1997,12(1):1- 40
                            </a>
                        </p>
                        <p id="1279">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Generating accurate rule sets without global optimization&amp;quot;">

                                <b>[21]</b>Frank E,Witten I H.Generating accurate rule sets without global optimization[C] //Proc of the 15th Int Conf on Machine Learning.San Francisco:Morgan Kaufmann,1998:144- 151
                            </a>
                        </p>
                        <p id="1281">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating production rules from decision trees">

                                <b>[22]</b>Quinlan J R.Generating production rules from decision trees[C] //Proc of the 10th Int Joint Conf on Artificial Intelligence.San Francisco:Morgan Kaufmann,1987:304- 307
                            </a>
                        </p>
                        <p id="1283">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpreting tree ensembles with intrees">

                                <b>[23]</b>Deng Houtao.Interpreting tree ensembles with intrees[J].International Journal of Data Science and Analytics,2019,7(4):277- 287
                            </a>
                        </p>
                        <p id="1285">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intelligible models for classification and regression">

                                <b>[24]</b>Lou Yin,Caruana R,Gehrke J.Intelligible models for classification and regression[C] //Proc of the 18th ACM SIGKDD Int Conf on Knowledge Discovery and Data mining.New York:ACM,2012:150- 158
                            </a>
                        </p>
                        <p id="1287">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The logic and interpretation of structure coefficients in multivariate general linear model analyses,No.ED467381">

                                <b>[25]</b>Henson R K.The logic and interpretation of structure coefficients in multivariate general linear model analyses,No.ED467381[R].New Orleans,LA:ERIC Document Reproduction Service,2002
                            </a>
                        </p>
                        <p id="1289">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statistical Models in S">

                                <b>[26]</b>Hastie T J.Statistical Models in S[M].New York:Routledge,2017:249- 307
                            </a>
                        </p>
                        <p id="1291">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized additive models:An introduction with R">

                                <b>[27]</b>Wood S N.Generalized additive models:An introduction with R[M].New York:Chapman and Hall/CRC,2017
                            </a>
                        </p>
                        <p id="1293">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse additive models">

                                <b>[28]</b>Ravikumar P,Lafferty J,Liu Han,et al.Sparse additive models[J].Journal of the Royal Statistical Society:Series B (Statistical Methodology),2009,71(5):1009- 1030
                            </a>
                        </p>
                        <p id="1295">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">

                                <b>[29]</b>Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[J].arXiv preprint arXiv:1409.0473,2014
                            </a>
                        </p>
                        <p id="1297">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention is all you need">

                                <b>[30]</b>Vaswani A,Shazeer N,Parmar N,et al.Attention is all you need[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2017:6000- 6010
                            </a>
                        </p>
                        <p id="1299">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Retain:An interpretable predictive model for healthcare using reverse time attention mechanism">

                                <b>[31]</b>Choi E,Bahadori M T,Sun Jimeng,et al.Retain:An interpretable predictive model for healthcare using reverse time attention mechanism[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2016:3504- 3512
                            </a>
                        </p>
                        <p id="1301">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention">

                                <b>[32]</b>Xu K,Ba J,Kiros R,et al.Show,attend and tell:Neural image caption generation with visual attention[C] //Proc of the 32nd Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,2015:2048- 2057
                            </a>
                        </p>
                        <p id="1303">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Attentive survey of attention models">

                                <b>[33]</b>Chaudhari S,Polatkan G,Ramanath R,et al.An Attentive survey of attention models[J].arXiv:1904.02874,2019
                            </a>
                        </p>
                        <p id="1305">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical attention networks for document classification">

                                <b>[34]</b>Yang Zichao,Yang Diyi,Dyer C,et al.Hierarchical attention networks for document classification[C] //Proc of the 2016 Conf of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies.Stroudsburg,PA:Association for Computational Linguistics,2016:1480- 1489
                            </a>
                        </p>
                        <p id="1307">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NAIS:Neural attentive item similarity model for recommendation">

                                <b>[35]</b>He Xiangnan,He Zhankui,Song Jingkuan,et al.NAIS:Neural attentive item similarity model for recommendation[J].IEEE Transactions on Knowledge and Data Engineering,2018,30(12):2354- 2366
                            </a>
                        </p>
                        <p id="1309">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequential recommender system based on hierarchical attention networks">

                                <b>[36]</b>Ying Haochao,Zhuang Fuzheng,Zhang Fuzheng,et al.Sequential recommender system based on hierarchical attention networks[C] //Proc of the 27th Int Joint Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:3926- 3932
                            </a>
                        </p>
                        <p id="1311">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ATRank:An attention-based user behavior modeling framework for recommendation">

                                <b>[37]</b>Zhou Chang,Bai Jinze,Song Junshuai,et al.ATRank:An attention-based user behavior modeling framework for recommendation[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:4564- 4571
                            </a>
                        </p>
                        <p id="1313">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NAIRS:A Neural Attentive Interpretable Recommendation System">

                                <b>[38]</b>Yu Shuai,Wang Yongbo,Yang Min,et al.NAIRS:A Neural Attentive Interpretable Recommendation System[C] //Proc of the 12th ACM Int Conf on Web Search and Data Mining.New York:ACM,2019:790- 793
                            </a>
                        </p>
                        <p id="1315">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretable convolutional neural networks with dual local and global attention for review rating prediction">

                                <b>[39]</b>Seo S,Huang Jing,Yang Hao,et al.Interpretable convolutional neural networks with dual local and global attention for review rating prediction[C] //Proc of the 11th ACM Conf on Recommender Systems.New York:ACM,2017:297- 305
                            </a>
                        </p>
                        <p id="1317">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rule extraction from decision trees ensembles:New algorithms based on heuristic search and sparse group lasso methods">

                                <b>[40]</b>Mashayekhi M,Gras R.Rule extraction from decision trees ensembles:New algorithms based on heuristic search and sparse group lasso methods[J].International Journal of Information Technology &amp; Decision Making,2017,16(6):1707- 1727
                            </a>
                        </p>
                        <p id="1319">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global model interpretation via recursive partitioning">

                                <b>[41]</b>Yang Chengliang,Rangarajan A,Ranka S.Global model interpretation via recursive partitioning[C] //Proc of the 4th IEEE Int Conf on Data Science and Systems.Piscataway,NJ:IEEE,2018:1563- 1570
                            </a>
                        </p>
                        <p id="1321">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MAGIX:Model agnostic globally interpretable explanations">

                                <b>[42]</b>Puri N,Gupta P,Agarwal P,et al.MAGIX:Model agnostic globally interpretable explanations[J].arXiv preprint arXiv:1706.07160,2017
                            </a>
                        </p>
                        <p id="1323">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepVID:Deep visual interpretation and diagnosis for image classifiers via knowledge distillation">

                                <b>[43]</b>Wang Junpeng,Gou Liang,Zhang Wei,et al.DeepVID:Deep visual interpretation and diagnosis for image classifiers via knowledge distillation[J].IEEE Transactions on Visualization and Computer Graphics,2019,25(6):2168- 2180
                            </a>
                        </p>
                        <p id="1325">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing higher-layer features of a deep network">

                                <b>[44]</b>Erhan D,Bengio Y,Courville A,et al.Visualizing higher-layer features of a deep network[J].University of Montreal,2009,1341(3):1- 13
                            </a>
                        </p>
                        <p id="1327">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synthesizing the preferred inputs for neurons in neural networks via deep generator networks">

                                <b>[45]</b>Nguyen A,Dosovitskiy A,Yosinski J,et al.Synthesizing the preferred inputs for neurons in neural networks via deep generator networks[C] //Proc of the 30th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2016:3387- 3395
                            </a>
                        </p>
                        <p id="1329">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpreting deep models for text analysis via optimization and regularization methods">

                                <b>[46]</b>Yuan Hao,Chen Yongjun,Hu Xia,et al.Interpreting deep models for text analysis via optimization and regularization methods[C] //Proc of the 33rd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2019:5717- 5724
                            </a>
                        </p>
                        <p id="1331">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretable explanations of black boxes by meaningful perturbation">

                                <b>[47]</b>Fong R C,Vedaldi A.Interpretable explanations of black boxes by meaningful perturbation[C] //Proc of the 16th IEEE Int Conf on Computer Vision.Piscataway,NJ:IEEE,2017:3429- 3437
                            </a>
                        </p>
                        <p id="1333">
                            <a id="bibliography_48" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What has my classifier learned?Visualizing the classification rules of bag-of-feature model by support region detection">

                                <b>[48]</b>Liu Lingqiao,Wang Lei.What has my classifier learned?visualizing the classification rules of bag-of-feature model by support region detection[C] //Proc of the 24th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2012:3586- 3593
                            </a>
                        </p>
                        <p id="1335">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local rule-based explanations of black box decision systems">

                                <b>[49]</b>Guidotti R,Monreale A,Ruggieri S,et al.Local rule-based explanations of black box decision systems[J].arXiv preprint arXiv:1805.10820,2018
                            </a>
                        </p>
                        <p id="1337">
                            <a id="bibliography_50" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchors:High-precision model-agnostic explanations">

                                <b>[50]</b>Ribeiro M T,Singh S,Guestrin C.Anchors:High-precision model-agnostic explanations[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:1527- 1535
                            </a>
                        </p>
                        <p id="1339">
                            <a id="bibliography_51" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lemna:Explaining deep learning based security applications">

                                <b>[51]</b>Guo Wenbo,Mu Dongliang,Xu Jun,et al.Lemna:Explaining deep learning based security applications[C] //Proc of the 2018 ACM SIGSAC Conf on Computer and Communications Security.New York:ACM,2018:364- 379
                            </a>
                        </p>
                        <p id="1341">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep inside convolutional networks:Visualising image classification models and saliency maps">

                                <b>[52]</b>Simonyan K,Vedaldi A,Zisserman A.Deep inside convolutional networks:Visualising image classification models and saliency maps[J].arXiv preprint arXiv:1312.6034,2013
                            </a>
                        </p>
                        <p id="1343">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[53]</b>Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C] //Proc of the 13th European Conf on Computer Vision.Berlin:Springer,2014:818- 833
                            </a>
                        </p>
                        <p id="1345">
                            <a id="bibliography_54" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Striving for simplicity:The all convolutional net">

                                <b>[54]</b>Springenberg J T,Dosovitskiy A,Brox T,et al.Striving for simplicity:The all convolutional net[J].arXiv preprint arXiv:1412.6806,2014
                            </a>
                        </p>
                        <p id="1347">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradients of counterfactuals">

                                <b>[55]</b>Sundararajan M,Taly A,Yan Qiqi.Gradients of counterfactuals[J].arXiv preprint arXiv:1611.02639,2016
                            </a>
                        </p>
                        <p id="1349">
                            <a id="bibliography_56" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Smoothgrad:Removing noise by adding noise">

                                <b>[56]</b>Smilkov D,Thorat N,Kim B,et al.Smoothgrad:Removing noise by adding noise[J].arXiv preprint arXiv:1706.03825,2017
                            </a>
                        </p>
                        <p id="1351">
                            <a id="bibliography_57" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation">

                                <b>[57]</b>Bach S,Binder A,Montavon G,et al.On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation[J].PloS One,2015,10(7):e0130140
                            </a>
                        </p>
                        <p id="1353">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Not just a black box:Learning important features through propagating activation differences">

                                <b>[58]</b>Shrikumar A,Greenside P,Shcherbina A,et al.Not just a black box:Learning important features through propagating activation differences[J].arXiv preprint arXiv:1605.01713,2016
                            </a>
                        </p>
                        <p id="1355">
                            <a id="bibliography_59" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards explanation of DNN-based prediction with guided feature inversion">

                                <b>[59]</b>Du Mengnan,Liu Ninghao,Song Qingquan,et al.Towards explanation of DNN-based prediction with guided feature inversion[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp; Data Mining.New York:ACM,2018:1358- 1367
                            </a>
                        </p>
                        <p id="1357">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep features for discriminative localization">

                                <b>[60]</b>Zhou Bolei,Khosla A,Lapedriza A,et al.Learning deep features for discriminative localization[C] //Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2921- 2929
                            </a>
                        </p>
                        <p id="1359">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grad-CAM:Visual explanations from deep networks via gradient-based localization">

                                <b>[61]</b>Selvaraju R R,Cogswell M,Das A,et al.Grad-CAM:Visual explanations from deep networks via gradient-based localization[C] //Proc of the IEEE Intel Conf on Computer Vision.Piscataway,NJ:IEEE,2017:618- 626
                            </a>
                        </p>
                        <p id="1361">
                            <a id="bibliography_62" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AI 2:Safety and robustness certification of neural networks with abstract interpretation">

                                <b>[62]</b>Gehr T,Mirman M,Drachsler-Cohen D,et al.Ai2:Safety and robustness certification of neural networks with abstract interpretation[C] //Proc of the 2018 IEEE Symposium on Security and Privacy.Piscataway,NJ:IEEE,2018:3- 18
                            </a>
                        </p>
                        <p id="1363">
                            <a id="bibliography_63" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exact and consistent interpretation for piecewise linear neural networks:A closed form solution">

                                <b>[63]</b>Chu Lingyang,Hu Xia,Hu Juhua,et al.Exact and consistent interpretation for piecewise linear neural networks:A closed form solution[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp; Data Mining.New York:ACM,2018:1244- 1253
                            </a>
                        </p>
                        <p id="1365">
                            <a id="bibliography_64" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501720365&amp;v=MzIwMzBmT2ZiSzdIdEROcW85RVkra1BEM284b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZzUmJoUT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[64]</b>Andrews R,Diederich J,Tickle A B.Survey and critique of techniques for extracting rules from trained artificial neural networks[J].Knowledge-Based Systems,1995,8(6):373- 389
                            </a>
                        </p>
                        <p id="1367">
                            <a id="bibliography_65" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The truth will come to light: directions and challenges in extracting the knowledge embedded within trained artificial neural networks">

                                <b>[65]</b>Tickle A B,Andrews R,Golea M,et al.The truth will come to light:Directions and challenges in extracting the knowledge embedded within trained artificial neural networks[J].IEEE Transactions on Neural Networks,1998,9(6):1057- 1068
                            </a>
                        </p>
                        <p id="1369">
                            <a id="bibliography_66" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DEDEC: A Methodology for extracting rules from trained artificial neural networks">

                                <b>[66]</b>Tickle A B,Orlowski M,Diederich J.DEDEC:A methodology for extracting rules from trained artificial neural networks[C] //Proc of the AISB'96 Workshop on Rule Extraction from Trained Neural Networks.Amsterdam,The Netherlands:IOS Press,1996:90- 102
                            </a>
                        </p>
                        <p id="1371">
                            <a id="bibliography_67" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rule generation from neural networks">

                                <b>[67]</b>Fu Limin.Rule generation from neural networks[J].IEEE Transactions on Systems,Man,and Cybernetics,1994,24(8):1114- 1124
                            </a>
                        </p>
                        <p id="1373">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpreting blackbox models via model extraction">

                                <b>[68]</b>Bastani O,Kim C,Bastani H.Interpreting blackbox models via model extraction[J].arXiv preprint arXiv:1705.08504,2017
                            </a>
                        </p>
                        <p id="1375">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting comprehensible models from trained neural networks">

                                <b>[69]</b>Craven M W.Extracting comprehensible models from trained neural networks[D].Madison,WI:Department of Computer Sciences,University of Wisconsin-Madison,1996
                            </a>
                        </p>
                        <p id="1377">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting decision trees from trained neural networks">

                                <b>[70]</b>Boz O.Extracting decision trees from trained neural networks[C] //Proc of the 8th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2002:456- 461
                            </a>
                        </p>
                        <p id="1379">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rule extraction from random forest:The RF+HC methods">

                                <b>[71]</b>Mashayekhi M,Gras R.Rule extraction from random forest:The RF+HC methods[G] //LNCS 9091:Proc of the 28th Canadian Conf on Artificial Intelligence.Berlin:Springer,2015:223- 237
                            </a>
                        </p>
                        <p id="1381">
                            <a id="bibliography_72" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making tree ensembles interpretable:A Bayesian model selection approach">

                                <b>[72]</b>Hara S,Hayashi K.Making tree ensembles interpretable:A Bayesian model selection approach[J].arXiv preprint arXiv:1606.09066,2016
                            </a>
                        </p>
                        <p id="1383">
                            <a id="bibliography_73" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making tree ensembles interpretable">

                                <b>[73]</b>Hara S,Hayashi K.Making tree ensembles interpretable[J].arXiv preprint arXiv:1606.05390,2016
                            </a>
                        </p>
                        <p id="1385">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A systematic method for decompositional rule extraction from neural networks">

                                <b>[74]</b>Krishnan R.A systematic method for decompositional rule extraction from neural networks[C] //Proc of the 10th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,1996
                            </a>
                        </p>
                        <p id="1387">
                            <a id="bibliography_75" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decompositional rules extraction methods from neural networks">

                                <b>[75]</b>Bondarenko A,Zmanovska T,Borisov A.Decompositional rules extraction methods from neural networks[C] //Proc of the 16th Int Conf on Soft Computing.Berlin:Springer,2010:256- 262
                            </a>
                        </p>
                        <p id="1389">
                            <a id="bibliography_76" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using sampling and queries to extract rules from trained neural networks">

                                <b>[76]</b>Craven M W,Shavlik J W.Using sampling and queries to extract rules from trained neural networks[C] //Proc of the 8th Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,1994:37- 45
                            </a>
                        </p>
                        <p id="1391">
                            <a id="bibliography_77" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD00000000637&amp;v=MjAxNzd1Z0lZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDcmxWN3JOSkZnPU5pVEJhck80SHRITXI0OUZZ&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[77]</b>Zhou Zhihua,Jiang Yuan,Chen Shifu.Extracting symbolic rules from trained neural network ensembles[J].AI Communications,2003,16(1):3- 15
                            </a>
                        </p>
                        <p id="1393">
                            <a id="bibliography_78" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active learning-based pedagogical rule extraction">

                                <b>[78]</b>De Fortuny E J,Martens D.Active learning-based pedagogical rule extraction[J].IEEE Transactions on Neural Networks and Learning Systems,2015,26(11):2664- 2677
                            </a>
                        </p>
                        <p id="1395">
                            <a id="bibliography_79" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretable &amp;amp; explorable approximations of black box models">

                                <b>[79]</b>Lakkaraju H,Kamar E,Caruana R,et al.Interpretable &amp; explorable approximations of black box models[J].arXiv preprint arXiv:1707.01154,2017
                            </a>
                        </p>
                        <p id="1397">
                            <a id="bibliography_80" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving the interpretability of deep neural networks with knowledge distillation">

                                <b>[80]</b>Liu Xuan,Wang Xiaoguang,Matwin S.Improving the interpretability of deep neural networks with knowledge distillation[C] //Proc of the 18th IEEE Int Conf on Data Mining Workshops.Piscataway,NJ:IEEE,2018:905- 912
                            </a>
                        </p>
                        <p id="1399">
                            <a id="bibliography_81" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model compression">

                                <b>[81]</b>Buciluǎ C,Caruana R,Niculescu-Mizil A.Model compression[C] //Proc of the 12th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2006:535- 541
                            </a>
                        </p>
                        <p id="1401">
                            <a id="bibliography_82" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distilling the knowledge in a neural network">

                                <b>[82]</b>Hinton G,Vinyals O,Dean J.Distilling the knowledge in a neural network[J].arXiv preprint arXiv:1503.02531,2015
                            </a>
                        </p>
                        <p id="1403">
                            <a id="bibliography_83" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting tree-structured representations of trained networks">

                                <b>[83]</b>Craven M,Shavlik J W.Extracting tree-structured representations of trained networks[C] //Proc of the 10th Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,1996:24- 30
                            </a>
                        </p>
                        <p id="1405">
                            <a id="bibliography_84" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distilling a neural network into a soft decision tree">

                                <b>[84]</b>Frosst N,Hinton G.Distilling a neural network into a soft decision tree[J].arXiv preprint arXiv:1711.09784,2017
                            </a>
                        </p>
                        <p id="1407">
                            <a id="bibliography_85" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning global additive explanations for neural nets using model distillation">

                                <b>[85]</b>Tan S,Caruana R,Hooker G,et al.Learning global additive explanations for neural nets using model distillation[J].arXiv preprint arXiv:1801.08640,2018
                            </a>
                        </p>
                        <p id="1409">
                            <a id="bibliography_86" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretable deep models for ICU outcome prediction">

                                <b>[86]</b>Che Zhengping,Purushotham S,Khemani R,et al.Interpretable deep models for ICU outcome prediction[C] //Proc of the AMIA Annual Symp.Bethesda,MD:American Medical Informatics Association,2016:371- 380
                            </a>
                        </p>
                        <p id="1411">
                            <a id="bibliography_87" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpreting social media-based substance use prediction models with knowledge distillation">

                                <b>[87]</b>Ding Tao,Hasan F,Bickel W K,et al.Interpreting social media-based substance use prediction models with knowledge distillation[C] //Proc of the 30th IEEE Int Conf on Tools with Artificial Intelligence.Piscataway,NJ:IEEE,2018:623- 630
                            </a>
                        </p>
                        <p id="1413">
                            <a id="bibliography_88" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpreting deep classifier by visual distillation of dark knowledge">

                                <b>[88]</b>Xu Kai,Park D H,Yi Chang,et al.Interpreting deep classifier by visual distillation of dark knowledge[J].arXiv preprint arXiv:1803.04042,2018
                            </a>
                        </p>
                        <p id="1415">
                            <a id="bibliography_89" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distill-and-compare:Auditing black-box models using transparent model distillation">

                                <b>[89]</b>Tan S,Caruana R,Hooker G,et al.Distill-and-compare:Auditing black-box models using transparent model distillation[C] //Proc of the 2018 AAAI/ACM Conf on AI,Ethics,and Society.New York:ACM,2018:303- 310
                            </a>
                        </p>
                        <p id="1417">
                            <a id="bibliography_90" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting bias in black-box models using transparent model distillation">

                                <b>[90]</b>Tan S,Caruana R,Hooker G,et al.Detecting bias in black-box models using transparent model distillation[J].arXiv preprint arXiv:1710.06169,2017
                            </a>
                        </p>
                        <p id="1419">
                            <a id="bibliography_91" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Examining CNN representations with respect to dataset bias">

                                <b>[91]</b>Zhang Quanshi,Wang Wenguan,Zhu Songchun.Examining CNN representations with respect to dataset bias[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Palo Alto,CA:AAAI,2018:4464- 4473
                            </a>
                        </p>
                        <p id="1421">
                            <a id="bibliography_92" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretable convolutional neural networks">

                                <b>[92]</b>Zhang Quanshi,Wu Yingnian,Zhu Songchun.Interpretable convolutional neural networks[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2018:8827- 8836
                            </a>
                        </p>
                        <p id="1423">
                            <a id="bibliography_93" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012549&amp;v=MTYwMzRqbVVMM0lKRnNSYmhRPU5pZkpaYks5SHRqTXFvOUZaT29OQ1hnd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[93]</b>Berkes P,Wiskott L.On the analysis and interpretation of inhomogeneous quadratic forms as receptive fields[J].Neural Computation,2006,18(8):1868- 1895
                            </a>
                        </p>
                        <p id="1425">
                            <a id="bibliography_94" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES999A72AA8421C0DBBDAD577B3F8639B4&amp;v=Mjg3NjZwbWFCdUhZZk9HUWxmQ3BiUTM1TnBod0wyNnhLND1OaWZPZmJxeEY2RExyZjQwYk84TkRRODV1MlJobms0SlRYamwzaEZEY2JTWFRNaWJDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[94]</b>Montavon G,Samek W,Müller K-R.Methods for interpreting and understanding deep neural networks[J].Digital Signal Processing,2018,73:1- 15
                            </a>
                        </p>
                        <p id="1427">
                            <a id="bibliography_95" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding deep image representations by inverting them">

                                <b>[95]</b>Mahendran A,Vedaldi A.Understanding deep image representations by inverting them[C] //Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2015:5188- 5196
                            </a>
                        </p>
                        <p id="1429">
                            <a id="bibliography_96" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Plug.&amp;amp;.play generative networks:conditional iterative generation of images in latent space">

                                <b>[96]</b>Nguyen A,Clune J,Bengio Y,et al.Plug &amp; play generative networks:Conditional iterative generation of images in latent space[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2017:4467- 4477
                            </a>
                        </p>
                        <p id="1431">
                            <a id="bibliography_97" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multifaceted feature visualization:Uncovering the different types of features learned by each neuron in deep neural networks">

                                <b>[97]</b>Nguyen A,Yosinski J,Clune J.Multifaceted feature visualization:Uncovering the different types of features learned by each neuron in deep neural networks[J].arXiv preprint arXiv:1602.03616,2016
                            </a>
                        </p>
                        <p id="1433">
                            <a id="bibliography_98" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sensitivity Analysis in Practice">

                                <b>[98]</b>Saltelli A,Tarantola S,Campolongo F,et al.Sensitivity Analysis in Practice:A Guide to Assessing Scientific Models[M].New York:John Wiley &amp; Sons,2004
                            </a>
                        </p>
                        <p id="1435">
                            <a id="bibliography_99" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sensitivity Analysis in Linear Regression">

                                <b>[99]</b>Chatterjee S,Hadi A S.Sensitivity Analysis in Linear Regression[M].New York:John Wiley &amp; Sons,2009
                            </a>
                        </p>
                        <p id="1437">
                            <a id="bibliography_100" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501718640&amp;v=MDIwMDFUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTDNJSkZzUmJoUT1OaWZPZmJLN0h0RE5xbzlFWStvSENuZzVvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[100]</b>Homma T,Saltelli A.Importance measures in global sensitivity analysis of nonlinear models[J].Reliability Engineering &amp; System Safety,1996,52(1):1- 17
                            </a>
                        </p>
                        <p id="1439">
                            <a id="bibliography_101" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quantitative model-independent method for global sensitivity analysis of model output">

                                <b>[101]</b>Saltelli A,Tarantola S,Chan K-S.A quantitative model-independent method for global sensitivity analysis of model output[J].Technometrics,1999,41(1):39- 56
                            </a>
                        </p>
                        <p id="1441">
                            <a id="bibliography_102" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012101044279&amp;v=MjczODRaZVp1SHlqbVVMM0lKRnNSYmhRPU5pZk9mYks3SHRET3JvOUVaTzhMRG5zd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[102]</b>Gevrey M,Dimopoulos I,Lek S.Review and comparison of methods to study the contribution of variables in artificial neural network models[J].Ecological Modelling,2003,160(3):249- 264
                            </a>
                        </p>
                        <p id="1443">
                            <a id="bibliography_103" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Determining the significance of input parameters using sensitivity analysis">

                                <b>[103]</b>Engelbrecht A P,Cloete I,Zurada J M.Determining the significance of input parameters using sensitivity analysis[C] //Proc of the 3rd Int Workshop on Artificial Neural Networks.Berlin:Springer,1995:382- 388
                            </a>
                        </p>
                        <p id="1445">
                            <a id="bibliography_104" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sensitivity analysis applied to artificial neural networks:What has my neural network actually learned?">

                                <b>[104]</b>Harrington P D B,Wan C.Sensitivity analysis applied to artificial neural networks:What has my neural network actually learned?[J].Analytical Chemistry,1998,70:2983- 2990
                            </a>
                        </p>
                        <p id="1447">
                            <a id="bibliography_105" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501644719&amp;v=MzAzODVMM0lKRnNSYmhRPU5pZk9mYks3SHRETnFvOUVZdThMQzMwd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[105]</b>Sung A.Ranking importance of input parameters of neural networks[J].Expert Systems with Applications,1998,15(3/4):405- 411
                            </a>
                        </p>
                        <p id="1449">
                            <a id="bibliography_106" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining classifications for individual instances">

                                <b>[106]</b>Robnik-Šikonja M,Kononenko I.Explaining classifications for individual instances[J].IEEE Transactions on Knowledge and Data Engineering,2008,20(5):589- 600
                            </a>
                        </p>
                        <p id="1451">
                            <a id="bibliography_107" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding neural networks through representation erasure">

                                <b>[107]</b>Li Jiwei,Monroe W,Jurafsky D.Understanding neural networks through representation erasure[J].arXiv preprint arXiv:1612.08220,2016
                            </a>
                        </p>
                        <p id="1453">
                            <a id="bibliography_108" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nothing else matters:Model-agnostic explanations by identifying prediction invariance">

                                <b>[108]</b>Ribeiro M T,Singh S,Guestrin C.Nothing else matters:Model-agnostic explanations by identifying prediction invariance[J].arXiv preprint arXiv:1611.05817,2016
                            </a>
                        </p>
                        <p id="1455">
                            <a id="bibliography_109" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpreting individual classifications of hierarchical networks">

                                <b>[109]</b>Landecker W,Thomure M D,Bettencourt L M,et al.Interpreting individual classifications of hierarchical networks[C] //Proc of the IEEE Symp on Computational Intelligence and Data Mining.Piscataway,NJ:IEEE,2013:32- 38
                            </a>
                        </p>
                        <p id="1457">
                            <a id="bibliography_110" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding neural machine translation">

                                <b>[110]</b>Ding Yanzhuo,Liu Yang,Luan Huanbo,et al.Visualizing and understanding neural machine translation[C] //Proc of the 55th Annual Meeting of the Association for Computational Linguistics.Stroudsburg,PA:Association for Computational Linguistics,2017:1150- 1159.
                            </a>
                        </p>
                        <p id="1459">
                            <a id="bibliography_111" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What is relevant in a text document?an interpretable machine learning approach">

                                <b>[111]</b>Arras L,Horn F,Montavon G,et al.What is relevant in a text document?:An interpretable machine learning approach[J].PloS One,2017,12(8):e0181142
                            </a>
                        </p>
                        <p id="1461">
                            <a id="bibliography_112" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring neural networks with activation atlases[OL]">

                                <b>[112]</b>Carter S.Exploring neural networks with activation atlases[OL].[2019-06-11].https://ai.googleblog.com/2019/03/exploring-neural-networks.html
                            </a>
                        </p>
                        <p id="1463">
                            <a id="bibliography_113" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverting Visual Representations with Convolutional Networks">

                                <b>[113]</b>Dosovitskiy A,Brox T.Inverting visual representations with convolutional networks[C] //Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:4829- 4837
                            </a>
                        </p>
                        <p id="1465">
                            <a id="bibliography_114" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object detectors emerge in deep scene CNNs">

                                <b>[114]</b>Zhou Bolei,Khosla A,Lapedriza A,et al.Object detectors emerge in deep scene CNNs[J].arXiv preprint arXiv:1412.6856,2014
                            </a>
                        </p>
                        <p id="1467">
                            <a id="bibliography_115" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning important features through propagating activation differences">

                                <b>[115]</b>Shrikumar A,Greenside P,Kundaje A.Learning important features through propagating activation differences[C] //Proc of the 34th Int Conf on Machine Learning.Tahoe,CA:International Machine Learning Society,2017:3145- 3153
                            </a>
                        </p>
                        <p id="1469">
                            <a id="bibliography_116" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analyzing classifiers:fisher vectors and deep neural networks">

                                <b>[116]</b>Lapuschkin S,Binder A,Montavon G,et al.Analyzing classifiers:Fisher vectors and deep neural networks[C] //Proc of the IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2912- 2920
                            </a>
                        </p>
                        <p id="1471">
                            <a id="bibliography_117" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Debugging machine learning models">

                                <b>[117]</b>Cadamuro G,Gilad-Bachrach R,Zhu X.Debugging machine learning models[C] //Proc of the 33rd ICML Workshop on Reliable Machine Learning in the Wild.Tahoe,CA:International Machine Learning Society,2016
                            </a>
                        </p>
                        <p id="1473">
                            <a id="bibliography_118" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Principles of explanatory debugging to personalize interactive machine learning">

                                <b>[118]</b>Kulesza T,Burnett M,Wong W-K,et al.Principles of explanatory debugging to personalize interactive machine learning[C] //Proc of the 20th ACM Int Conf on Intelligent User Interfaces.New York:ACM,2015:126- 137
                            </a>
                        </p>
                        <p id="1475">
                            <a id="bibliography_119" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explanatory debugging:Supporting end-user debugging of machine-learned programs">

                                <b>[119]</b>Kulesza T,Stumpf S,Burnett M,et al.Explanatory debugging:Supporting end-user debugging of machine-learned programs[C] //Proc of the IEEE Symp on Visual Languages and Human-Centric Computing.Piscataway,NJ:IEEE,2010:41- 48
                            </a>
                        </p>
                        <p id="1477">
                            <a id="bibliography_120" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretability via model extraction">

                                <b>[120]</b>Bastani O,Kim C,Bastani H.Interpretability via model extraction[J].arXiv preprint arXiv:1706.09773,2017
                            </a>
                        </p>
                        <p id="1479">
                            <a id="bibliography_121" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Activis:Visual exploration of industry-scale deep neural network models">

                                <b>[121]</b>Kahng M,Andrews P Y,Kalro A,et al.Activis:Visual exploration of industry-scale deep neural network models[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):88- 97
                            </a>
                        </p>
                        <p id="1481">
                            <a id="bibliography_122" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lstmvis:A tool for visual analysis of hidden state dynamics in recurrent neural networks">

                                <b>[122]</b>Strobelt H,Gehrmann S,Pfister H,et al.Lstmvis:A tool for visual analysis of hidden state dynamics in recurrent neural networks[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):667- 676
                            </a>
                        </p>
                        <p id="1483">
                            <a id="bibliography_123" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing dataflow graphs of deep learning models in TensorFlow">

                                <b>[123]</b>Wongsuphasawat K,Smilkov D,Wexler J,et al.Visualizing dataflow graphs of deep learning models in tensorflow[J].IEEE Transactions on Visualization and Computer Graphics,2018,24(1):1- 12
                            </a>
                        </p>
                        <p id="1485">
                            <a id="bibliography_124" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manifold:A model-agnostic framework for interpretation and diagnosis of machine learning models">

                                <b>[124]</b>Zhang Jiawei,Wang Yang,Molino P,et al.Manifold:A model-agnostic framework for interpretation and diagnosis of machine learning models[J].IEEE Transactions on Visualization and Computer Graphics,2019,25(1):364- 373
                            </a>
                        </p>
                        <p id="1487">
                            <a id="bibliography_125" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interacting with predictions:Visual inspection of black-box machine learning models">

                                <b>[125]</b>Krause J,Perer A,Ng K.Interacting with predictions:Visual inspection of black-box machine learning models[C] //Proc of the ACM CHI Conf on Human Factors in Computing Systems.New York:ACM,2016:5686- 5697
                            </a>
                        </p>
                        <p id="1489">
                            <a id="bibliography_126" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A workflow for visual diagnostics of binary classifiers using instance-level explanations">

                                <b>[126]</b>Krause J,Dasgupta A,Swartz J,et al.A workflow for visual diagnostics of binary classifiers using instance-level explanations[C] //Proc of the IEEE Conf on Visual Analytics Science and Technology.Piscataway,NJ:IEEE,2017:162- 172
                            </a>
                        </p>
                        <p id="1491">
                            <a id="bibliography_127" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An approach to supporting incremental visual data classification">

                                <b>[127]</b>Paiva J G S,Schwartz W R,Pedrini H,et al.An approach to supporting incremental visual data classification[J].IEEE Transactions on Visualization and Computer Graphics,2015,21(1):4- 17
                            </a>
                        </p>
                        <p id="1493">
                            <a id="bibliography_128" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature insight:Visual support for error-driven feature ideation in text classification">

                                <b>[128]</b>Brooks M,Amershi S,Lee B,et al.FeatureInsight:Visual support for error-driven feature ideation in text classification[C] //Proc of the IEEE Conf on Visual Analytics Science and Technology.Piscataway,NJ:IEEE,2015:105- 112
                            </a>
                        </p>
                        <p id="1495">
                            <a id="bibliography_129" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated Gleason grading of prostate cancer tissue microarrays via deep learning">

                                <b>[129]</b>Arvaniti E,Fricker K S,Moret M,et al.Automated Gleason grading of prostate cancer tissue microarrays via deep learning[J].Scientific Reports,2018,8(1):12054- 12054
                            </a>
                        </p>
                        <p id="1497">
                            <a id="bibliography_130" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The matter simulation (r) evolution">

                                <b>[130]</b>Aspuru-Guzik A,Lindh R,Reiher M.The matter simulation (r) evolution[J].ACS Central Science,2018,4(2):144- 152
                            </a>
                        </p>
                        <p id="1499">
                            <a id="bibliography_131" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Independent vector analysis for data fusion prior to molecular property prediction with machine learning">

                                <b>[131]</b>Boukouvalas Z,Elton D C,Chung P W,et al.Independent vector analysis for data fusion prior to molecular property prediction with machine learning[J].arXiv preprint arXiv:1811.00628,2018
                            </a>
                        </p>
                        <p id="1501">
                            <a id="bibliography_132" target="_blank" href="http://scholar.cnki.net/result.aspx?q=How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry">

                                <b>[132]</b>Häse F,Galván I F,Aspuru-Guzik A,et al.How machine learning can assist the interpretation of ab initio molecular dynamics simulations and conceptual understanding of chemistry[J].Chemical Science,2019,10(8):2298- 2307
                            </a>
                        </p>
                        <p id="1503">
                            <a id="bibliography_133" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quantum-chemical insights from deep tensor neural networks">

                                <b>[133]</b>Schütt K T,Arbabzadah F,Chmiela S,et al.Quantum-chemical insights from deep tensor neural networks[J].arXiv preprint arXiv:1609.08529,2016
                            </a>
                        </p>
                        <p id="1505">
                            <a id="bibliography_134" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for genomics:A concise overview">

                                <b>[134]</b>Yue Tianwei,Wang Haohan.Deep learning for genomics:A concise overview[J].arXiv preprint arXiv:1802.00810,2018
                            </a>
                        </p>
                        <p id="1507">
                            <a id="bibliography_135" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD2923078DA560F32A2CC739EA38FCD5E5&amp;v=MjQzMDFJY3hGZTRKREFvNnpXY1JtVXg2UzNhWDNSRTlEOEhnUU0rYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5waHdMMjZ4SzQ9Tmo3QmFyR3hITkxNcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[135]</b>Angermueller C,Lee H J,Reik W,et al.DeepCpG:Accurate prediction of single-cell DNA methylation states using deep learning[J].Genome Biology,2017,18(1):67
                            </a>
                        </p>
                        <p id="1509">
                            <a id="bibliography_136" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dan Q a hybrid convolutional and recurren deep neural network for quantifying the function of DNAsequences">

                                <b>[136]</b>Quang D,Xie Xiaohui.DanQ:A hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences[J].Nucleic Acids Research,2016,44(11):107
                            </a>
                        </p>
                        <p id="1511">
                            <a id="bibliography_137" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep motif dashboard:Visualizing and understanding genomic sequences using deep neural networks">

                                <b>[137]</b>Lanchantin J,Singh R,Wang B,et al.Deep motif dashboard:Visualizing and understanding genomic sequences using deep neural networks[C] //Proc of the 22nd Pacific Symp on Biocomputing.Singapore:World Scientific Publishing Co,2017:254- 265
                            </a>
                        </p>
                        <p id="1513">
                            <a id="bibliography_138" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning">

                                <b>[138]</b>Alipanahi B,Delong A,Weirauch M T,et al.Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning[J].Nature Biotechnology,2015,33(8):831- 838
                            </a>
                        </p>
                        <p id="1515">
                            <a id="bibliography_139" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A study on different classification models for knowledge discovery">

                                <b>[139]</b>Gulia N,Singh S,Sapra L.A study on different classification models for knowledge discovery[J].International Journal of Computer Science Mob Computer,2015,4(6):241- 248
                            </a>
                        </p>
                        <p id="1517">
                            <a id="bibliography_140" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD79E10CDA92B669B1067B43E6FB474986&amp;v=MTcyMDZuQmFyU3hhOURNM1BzMGJlbDlDbm93dlJjVDdEZ1BUSHlYcW1SSGZiV1FUTEtaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnBod0wyNnhLND1Oag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[140]</b>Helma C.Data mining and knowledge discovery in predictive toxicology[J].SAR and QSAR in Environmental Research,2004,15(5-6):367- 383
                            </a>
                        </p>
                        <p id="1519">
                            <a id="bibliography_141" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000085573&amp;v=MTY4NDhIeWptVUwzSUpGc1JiaFE9TmlmSVk3SzdIdGpOcjQ5RlpPTUtDWHM2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[141]</b>Bertini E,Lalanne D.Investigating and reflecting on the integration of automatic data analysis and visualization in knowledge discovery[J].ACM SIGKDD Explorations Newsletter,2010,11(2):9- 18
                            </a>
                        </p>
                        <p id="1521">
                            <a id="bibliography_142" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From data mining to knowledge discovery in databases">

                                <b>[142]</b>Fayyad U,Piatetsky-Shapiro G,Smyth P.From data mining to knowledge discovery in databases[J].AI Magazine,1996,17(3):37- 37
                            </a>
                        </p>
                        <p id="1523">
                            <a id="bibliography_143" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining and harnessing adversarial examples">

                                <b>[143]</b>Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[J].arXiv preprint arXiv:1412.6572,2014
                            </a>
                        </p>
                        <p id="1525">
                            <a id="bibliography_144" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards evaluating the robustness of neural networks">

                                <b>[144]</b>Carlini N,Wagner D.Towards evaluating the robustness of neural networks[C] //Proc of the 38th IEEE Symp on Security and Privacy.Piscataway,NJ:IEEE,2017:39- 57
                            </a>
                        </p>
                        <p id="1527">
                            <a id="bibliography_145" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attacks meet interpretability:Attribute-steered detection of adversarial samples">

                                <b>[145]</b>Tao Guanhong,Ma Shiqing,Liu Yingqi,et al.Attacks meet interpretability:Attribute-steered detection of adversarial samples[C] //Proc of the 32nd Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2018:7717- 7728
                            </a>
                        </p>
                        <p id="1529">
                            <a id="bibliography_146" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial detection with model interpretation">

                                <b>[146]</b>Liu Ninghao,Yang Hongxia,Hu Xia.Adversarial detection with model interpretation[C] //Proc of the 24th ACM SIGKDD Int Conf on Knowledge Discovery &amp; Data Mining.New York:ACM,2018:1803- 1811
                            </a>
                        </p>
                        <p id="1531">
                            <a id="bibliography_147" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial CAPTCHAs">

                                <b>[147]</b>Shi Chenghui,Xu Xiaogang,Ji Shouling,et al.Adversarial CAPTCHAs[J].arXiv preprint arXiv:1901.01107,2019
                            </a>
                        </p>
                        <p id="1533">
                            <a id="bibliography_148" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">

                                <b>[148]</b>Papernot N,Mcdaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the 1st IEEE European Symp on Security and Privacy.Piscataway,NJ:IEEE,2016:372- 387
                            </a>
                        </p>
                        <p id="1535">
                            <a id="bibliography_149" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial examples versus cloud-based detectors:A black-box empirical study">

                                <b>[149]</b>Li Xurong,Ji Shouling,Han Meng,et al.Adversarial examples versus cloud-based detectors:A black-box empirical study[J].arXiv preprint arXiv:1901.01223,2019
                            </a>
                        </p>
                        <p id="1537">
                            <a id="bibliography_150" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Practical black-box attacks against machine learning">

                                <b>[150]</b>Papernot N,Mcdaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 12th ACM Asia Conf on Computer and Communications Security.New York:ACM,2017:506- 519
                            </a>
                        </p>
                        <p id="1539">
                            <a id="bibliography_151" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretation of neural networks is fragile">

                                <b>[151]</b>Ghorbani A,Abid A,Zou J.Interpretation of neural networks is fragile[J].arXiv preprint arXiv:1710.10547,2017
                            </a>
                        </p>
                        <p id="1541">
                            <a id="bibliography_152" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interpretable deep learning under fire">

                                <b>[152]</b>Zhang Xinyang,Wang Ningfei,Ji Shouling,et al.Interpretable deep learning under fire[C] //Proc of the 29th USENIX Security Symp.Berkeley,CA:USENIX Association,2020
                            </a>
                        </p>
                        <p id="1543">
                            <a id="bibliography_153" >
                                    <b>[153]</b>
                                Dabkowski P,Gal Y.Real time image saliency for black box classifiers[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.Red Hook,NY:Curran Associates Inc,2017:6967- 6976■Ji Shouling,born in 1986.Received a PhD in electrical and computer engineering from Georgia Institute of Technology and a PhD in computer science from Georgia State University.Currently ZJU 100-Young Professor in the College of Computer Science and Technology at Zhejiang University and research faculty in the School of Electrical and Computer Engineering at Georgia Institute of Technology.Member of IEEE and ACM.His main research interests include big data security and privacy,big data driven decurity and privacy,adversarial learning,graph theory and algorithms,and wireless networks.■Li Jinfeng,born in 1994.MSc candidate in the College of Computer Science and Technology at Zhejiang University.His main research interests include big data driven security,AI security and deep learning interpretability.■Du Tianyu,born in 1996.PhD candidate in the College of Computer Science and Technology at Zhejiang University.Her main research interests include big data driven security,adversarial learning and AI security.■Li Bo,born in 1989.Assistant professor in the Department of Computer Science at University of Illinois at Urbana-Champaign,and recipient of the Symantec Research Labs Fellowship.Member of IEEE and ACM.Her main research interests include theoretical and practical aspects of security,machine learning,privacy,game theory,and blockchain.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201910004" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201910004&amp;v=MzI3MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJzRnl6Z1Zydk9MeXZTZExHNEg5ak5yNDlGWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJpQmlKUitWRTA4aUdDMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

