

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127910529806250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201907022%26RESULT%3d1%26SIGN%3dJ8ObMeJHKoERdJHpS51wRez%252bkJg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907022&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907022&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907022&amp;v=MDM4ODZPZVplUnJGeXJuVXJ2SUx5dlNkTEc0SDlqTXFJOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#38" data-title="&lt;b&gt;1.1 强化学习基础&lt;/b&gt;"><b>1.1 强化学习基础</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;1.2 强化学习算法&lt;/b&gt;"><b>1.2 强化学习算法</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;1.3 参数调优&lt;/b&gt;"><b>1.3 参数调优</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="&lt;b&gt;2 参数调节系统的设计与实现&lt;/b&gt; "><b>2 参数调节系统的设计与实现</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;2.1 参数调节系统的状态信息&lt;/b&gt;"><b>2.1 参数调节系统的状态信息</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;2.2 参数调节系统的动作信息&lt;/b&gt;"><b>2.2 参数调节系统的动作信息</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;2.3 参数调节系统的奖励信息&lt;/b&gt;"><b>2.3 参数调节系统的奖励信息</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;2.4 参数调节系统的接口模块&lt;/b&gt;"><b>2.4 参数调节系统的接口模块</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;2.5 参数调节系统的神经网络模型&lt;/b&gt;"><b>2.5 参数调节系统的神经网络模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#135" data-title="&lt;b&gt;3 验证系统调节效果的实验&lt;/b&gt; "><b>3 验证系统调节效果的实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="&lt;b&gt;3.1 实验环境&lt;/b&gt;"><b>3.1 实验环境</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;3.2 测试工具&lt;/b&gt;"><b>3.2 测试工具</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;3.3 测试项&lt;/b&gt;"><b>3.3 测试项</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;3.4 实验流程&lt;/b&gt;"><b>3.4 实验流程</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;3.5 实验结果&lt;/b&gt;"><b>3.5 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#163" data-title="&lt;b&gt;4 总结与展望&lt;/b&gt; "><b>4 总结与展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="图1 强化学习交互过程">图1 强化学习交互过程</a></li>
                                                <li><a href="#83" data-title="图2 A3C与A2C参数更新方式的对比">图2 A3C与A2C参数更新方式的对比</a></li>
                                                <li><a href="#102" data-title="图3 参数调节系统框架">图3 参数调节系统框架</a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;表1 状态信息&lt;/b&gt;"><b>表1 状态信息</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表2 选取的参数及其相关属性&lt;/b&gt;"><b>表2 选取的参数及其相关属性</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表3 各算法性能测试数据&lt;/b&gt;"><b>表3 各算法性能测试数据</b></a></li>
                                                <li><a href="#160" data-title="图4 各算法性能测试结果">图4 各算法性能测试结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="183">


                                    <a id="bibliography_1" title="Cheng Yaodong, Zhang Xiao, Wang Peijian, et al.Data management challenges and event index technologies in high energy physics[J].Journal of Computer Research and Development, 2017, 54 (2) :258- 266 (in Chinese) (程耀东, 张潇, 王培建, 等.高能物理大数据挑战与海量事例特征索引技术研究[J].计算机研究与发展, 2017, 54 (2) :258- 266) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201702003&amp;v=MjU2NjVMT2VaZVJyRnlyblVydklMeXZTZExHNEg5Yk1yWTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Cheng Yaodong, Zhang Xiao, Wang Peijian, et al.Data management challenges and event index technologies in high energy physics[J].Journal of Computer Research and Development, 2017, 54 (2) :258- 266 (in Chinese) (程耀东, 张潇, 王培建, 等.高能物理大数据挑战与海量事例特征索引技术研究[J].计算机研究与发展, 2017, 54 (2) :258- 266) 
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_2" title="Han J, Kim D, Eom H.Improving the performance of lustre file system in HPC environments[C] //Proc of the 1st Int Workshops on Foundations and Applications of Self* Systems.Piscataway, NJ:IEEE, 2016:84- 89" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving the performance of lustre file system in HPC environments">
                                        <b>[2]</b>
                                        Han J, Kim D, Eom H.Improving the performance of lustre file system in HPC environments[C] //Proc of the 1st Int Workshops on Foundations and Applications of Self* Systems.Piscataway, NJ:IEEE, 2016:84- 89
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_3" title="Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge:MIT Press, 2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning:An Introduction">
                                        <b>[3]</b>
                                        Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge:MIT Press, 2018
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                    Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529- 533</a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_5" title="Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:PMLR, 2016:1928- 1937" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">
                                        <b>[5]</b>
                                        Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:PMLR, 2016:1928- 1937
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_6" title="Schulman J, Levine S, Abbeel P, et al.Trust region policy optimization[C] //Proc of the 32nd Int Conf on Machine Learning.New York:PMLR, 2015:1889- 1897" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trust region policy optimization">
                                        <b>[6]</b>
                                        Schulman J, Levine S, Abbeel P, et al.Trust region policy optimization[C] //Proc of the 32nd Int Conf on Machine Learning.New York:PMLR, 2015:1889- 1897
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_7" title="Schulman J, Wolski F, Dhariwal P, et al.Proximal policy optimization algorithms[J].arXiv preprint arXiv:1707.06347, 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Proximal policy optimization algorithms">
                                        <b>[7]</b>
                                        Schulman J, Wolski F, Dhariwal P, et al.Proximal policy optimization algorithms[J].arXiv preprint arXiv:1707.06347, 2017
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_8" title="Diao Yixin, Hellerstein J L, Parekh S, et al.Managing Web server performance with AutoTune agents[J].IBM Systems Journal, 2003, 42 (1) :136- 149" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Managing Web server performance with AutoTune agents">
                                        <b>[8]</b>
                                        Diao Yixin, Hellerstein J L, Parekh S, et al.Managing Web server performance with AutoTune agents[J].IBM Systems Journal, 2003, 42 (1) :136- 149
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_9" title="Jamshidi P, Casale G.An uncertainty-aware approach to optimal configuration of stream processing systems[C] //Proc of the 24th IEEE Int Symp on Modeling, Analysis and Simulation of Computer and Telecommunication Systems.Piscataway, NJ:IEEE, 2016:39- 48" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An uncertainty-aware approach to optimal configuration of stream processing systems">
                                        <b>[9]</b>
                                        Jamshidi P, Casale G.An uncertainty-aware approach to optimal configuration of stream processing systems[C] //Proc of the 24th IEEE Int Symp on Modeling, Analysis and Simulation of Computer and Telecommunication Systems.Piscataway, NJ:IEEE, 2016:39- 48
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_10" title="Zhang Fan, Cao Junwei, Liu Lianchen, et al.Performance improvement of distributed systems by autotuning of the configuration parameters[J].Tsinghua Science and Technology, 2011, 16 (4) :440- 448" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501347107&amp;v=MDkwMjd1SHlqbVVMcklJbHdWYmhJPU5pZk9mYks3SHRETnFvOUVaKzhJRFh3K29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Zhang Fan, Cao Junwei, Liu Lianchen, et al.Performance improvement of distributed systems by autotuning of the configuration parameters[J].Tsinghua Science and Technology, 2011, 16 (4) :440- 448
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_11" title="Chen Haifeng, Jiang Guofei, Zhang Hui, et al.Boosting the performance of computing systems through adaptive configuration tuning[C] //Proc of the 24th ACM Symp on Applied Computing.New York:ACM, 2009:1045- 1049" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boosting the performance of computing systems through adaptive configuration tuning">
                                        <b>[11]</b>
                                        Chen Haifeng, Jiang Guofei, Zhang Hui, et al.Boosting the performance of computing systems through adaptive configuration tuning[C] //Proc of the 24th ACM Symp on Applied Computing.New York:ACM, 2009:1045- 1049
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_12" title="Li Yan, Chang K, Bel O, et al.CAPES:Unsupervised storage performance tuning using neural network-based deep reinforcement learning[C] //Proc of the 30th Int Conf for High Performance Computing, Networking, Storage and Analysis.New York:ACM, 2017:42:1- 42:14" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CAPES:unsupervised storage performance tuning using neural network-based deep reinforcement learning">
                                        <b>[12]</b>
                                        Li Yan, Chang K, Bel O, et al.CAPES:Unsupervised storage performance tuning using neural network-based deep reinforcement learning[C] //Proc of the 30th Int Conf for High Performance Computing, Networking, Storage and Analysis.New York:ACM, 2017:42:1- 42:14
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_13" title="Goodfellow I, Bengio Y, Courville A, et al.Deep Learning[M].Cambridge:MIT Press, 2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning">
                                        <b>[13]</b>
                                        Goodfellow I, Bengio Y, Courville A, et al.Deep Learning[M].Cambridge:MIT Press, 2016
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(07),1578-1586 DOI:10.7544/issn1000-1239.2019.20180797            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于强化学习的Lustre文件系统的性能调优</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%87%E9%9F%AC&amp;code=42264666&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张文韬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%AA%E7%92%90&amp;code=14202850&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汪璐</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A8%8B%E8%80%80%E4%B8%9C&amp;code=01454780&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">程耀东</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E9%AB%98%E8%83%BD%E7%89%A9%E7%90%86%E7%A0%94%E7%A9%B6%E6%89%80%E8%AE%A1%E7%AE%97%E4%B8%AD%E5%BF%83&amp;code=0253057&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院高能物理研究所计算中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>高能物理计算是典型的数据密集型计算.分布式存储系统的吞吐率和响应时间是最关键的性能指标, 往往也是重点关注的性能优化目标.存储系统中存在大量可供调节的参数, 这些参数的设置对系统的性能有着很大的影响.目前, 这些参数被直接设置为静态值, 或者由经验丰富的管理员定义一些启发式规则来自动调整.考虑到数据访问模式和硬件配置的多样性, 以及依靠人类经验来找到数百个交互参数的启发式规则的难度, 这2种方法的效果都不太乐观.实际上, 如果把调节引擎看作是智能体, 把存储系统看作是环境, 存储系统的参数调节问题是典型的顺序决策问题.因此, 基于高能物理计算的数据访问特点, 提出了用强化学习的方法来进行自动化的参数调优.实验表明:在相同的测试环境下, 以Lustre文件系统默认参数为基准, 该方法可使其吞吐率提升30%左右.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分布式存储;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">性能调优;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">参数调节;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Zhang Wentao, born in 1993.Master at the Institute of High Energy Physics, Chinese Academy of Sciences.His main research interests include reinforcement learning, machine learning and mass storage. zhangwt@ihep.ac.cn;
                                </span>
                                <span>
                                    Wang Lu, born in 1983.PhD and associate professor at the Institute of High Energy Physics, Chinese Academy of Sciences. Her main research interests include mass storage, machine learning and deep leaning.;
                                </span>
                                <span>
                                    Cheng Yaodong, born in 1977.PhD and professor at the Institute of High Energy Physics, Chinese Academy of Sciences. His main research interests include cloud computing, mass storage and grid computing.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2017YFB0203203);</span>
                                <span>国家自然科学基金项目 (11575223);</span>
                    </p>
            </div>
                    <h1><b>Performance Optimization of Lustre File System Based on Reinforcement Learning</b></h1>
                    <h2>
                    <span>Zhang Wentao</span>
                    <span>Wang Lu</span>
                    <span>Cheng Yaodong</span>
            </h2>
                    <h2>
                    <span>Computing Center, Institute of High Energy Physics, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Computing of high energy physics is a typical data-intensive application. The throughput and response time of distributed storage system are key performance indicators, and they are often the targets of performance optimization. There are a large number of parameters that can be adjusted in a distributed storage system. The setting of these parameters has great influence on the performance of the system. At present, these parameters are either set with static values or automatically tuned by some heuristic rules defined by experienced administrators. Neither of the method is optimistic taking into account the diversity of data access patterns and hardware capabilities, and the difficulty of finding heuristic rules for hundreds of interacted parameters based on human experience. In fact, if the tuning engine is regarded as an agent and the storage system is regarded as the environment, the parameter adjustment problem of the storage system can be treated as a typical sequential decision problem. Therefore, based on data access characteristics of high energy physics calculation, we propose an automated parameter tuning method using the reinforcement learning. Experiments show that in the same test environment, using the default parameters of the Lustre file system as a baseline, this method can increase the throughput by about 30%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=distributed%20storage&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">distributed storage;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=performance%20tuning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">performance tuning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=parameter%20adjustment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">parameter adjustment;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2017YFB0203203);</span>
                                <span>the National Natural Science Foundation of China (11575223);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="32">高能物理计算是一种数据密集型计算, 存储系统是其性能决定因素之一.未来10年内, 各大物理实验如江门中微子实验、高海拔宇宙线实验、北京谱仪实验等将累积近100 PB的物理数据.大规模的数据处理给存储系统提出了“百GB/s的聚合带宽、数万个客户端并发访问、数据可靠性和可用性、跨域站点数据共享以及数据长期保存”等需求和挑战<citation id="209" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="33">高能物理数据处理主要包括模拟计算、重建计算以及物理分析3种类型, 每种计算类型各有其特点.高能物理计算中数据是1次写入、多次读取.通过监控计算节点上的用户作业, 可以得到文件系统的访问模式:大部分文件的连续读请求大小分布在256 KB～4 MB之间, 每2个连续读请求之间都有offset, 65%的offset绝对值分布在1～4 MB之间, 这说明文件的读访问方式为大记录块的跳读.</p>
                </div>
                <div class="p1">
                    <p id="34">存储系统管理员往往会根据系统的历史情况以及系统的实时状态, 调节相应的参数值以提高系统的访问性能<citation id="210" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.参数调节和系统的反馈之间是有延时的, 如果采取了连续多个调节动作, 很难确定究竟是哪个动作起了作用, 或者每个动作对结果的影响是多少.因此, 人工调节不免存在偏差, 况且庞大的参数搜索空间、负载的连续性、负载和设备的多样性等因素也决定了传统方法是非常低效的.传统的参数配置算法包括基于模型的控制反馈算法和无模型的参数搜索2大类.前者需要系统管理员具备丰富的先验知识且不支持动态负载变化;后者在参数搜索空间很大时优化效率很低.</p>
                </div>
                <div class="p1">
                    <p id="35">强化学习由于其优秀的决策能力在人工智能领域得到了广泛应用.然而, 早期的强化学习主要依赖于人工提取特征, 难以处理复杂高维状态空间下的问题.随着深度学习的发展, 算法可以直接从原始的高维数据中提取出特征.深度学习具有较强的感知能力, 但是缺乏一定的决策能力;而强化学习具有较强的决策能力, 但对感知问题束手无策.因此, 将两者结合起来, 优势互补, 能够为复杂状态下的感知决策问题提供解决思路.</p>
                </div>
                <div class="p1">
                    <p id="36">把调节引擎看作是智能体, 把存储系统看作是环境, 存储系统的参数调节问题是典型的顺序决策问题.因此, 我们很自然地将强化学习引入到存储系统的参数调节中.本文基于高能物理计算的数据访问特点, 设计并实现了基于强化学习的参数调节系统, 实验表明, 该系统可使Lustre文件系统的吞吐率提升30%左右.</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="38" name="38"><b>1.1 强化学习基础</b></h4>
                <div class="p1">
                    <p id="39">Markov决策过程 (Markov decision process, MDP) 是强化学习的最基本的理论模型<citation id="211" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.在MDP中, Agent和环境之间的交互过程可如图1所示:</p>
                </div>
                <div class="area_img" id="40">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907022_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 强化学习交互过程" src="Detail/GetImg?filename=images/JFYZ201907022_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 强化学习交互过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907022_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Interactive process of reinforcement learning</p>

                </div>
                <div class="p1">
                    <p id="41">Agent的目标是找到1个最优策略<i>π</i><sup>*</sup>, 使得它在任意状态<i>s</i>和任意时间步骤<i>t</i>下, 都能够获得最大的长期累积奖赏, 即:</p>
                </div>
                <div class="p1">
                    <p id="42"><mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>π</mi></munder><mspace width="0.25em" /><mi>E</mi><msub><mrow></mrow><mi>π</mi></msub><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>k</mi></msup><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>. (1) </p>
                </div>
                <div class="p1">
                    <p id="44">强化学习的目标是寻找最优状态值函数 (optimal state value function) :</p>
                </div>
                <div class="p1">
                    <p id="45"><mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>π</mi></munder><mspace width="0.25em" /><mi>E</mi><msub><mrow></mrow><mi>π</mi></msub><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>k</mi></msup><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>. (2) </p>
                </div>
                <div class="p1">
                    <p id="47">基于此推导出贝尔曼方程:</p>
                </div>
                <div class="p1">
                    <p id="48"><mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mi>π</mi></msub><mrow><mo>[</mo><mrow><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>V</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mrow></math></mathml>. (3) </p>
                </div>
                <div class="p1">
                    <p id="50">强化学习的方法分为基于模型的方法和无模型的方法, 而现实世界中大部分问题模型是未知的, 所以解决无模型的方法是强化学习的精髓.无模型即意味着状态转移概率是未知的, 这样计算值函数时期望是无法计算的, 如何计算期望呢?强化学习在此处引入了蒙特卡罗方法.</p>
                </div>
                <div class="p1">
                    <p id="51">蒙特卡罗法, 即经验平均法.所谓经验, 是指利用该策略做很多次实验, 产生很多幕数据, 这里1幕是1次实验的意思, 平均就是求均值.利用蒙特卡罗方法求状态<i>s</i>处的值函数时, 又可以分为第1次访问蒙特卡罗方法和每次访问蒙特卡罗方法.计算为</p>
                </div>
                <div class="p1">
                    <p id="52"><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>G</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>+</mo><mi>G</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>+</mo><mo>⋯</mo></mrow><mrow><mi>Ν</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>. (4) </p>
                </div>
                <div class="p1">
                    <p id="54">蒙特卡罗的方法需要等到每次实验结束, 所以学习效率不高, 于是人们又提出了时间差分法 (temporal difference, TD) , 其是蒙特卡罗法与动态规划法的结合, 不用等到实验结束, 而是在每步都更新.TD方法更新值函数的公式为</p>
                </div>
                <div class="p1">
                    <p id="55"><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>←</mo><mi>V</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><mrow><mo>[</mo><mrow><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>V</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mi>V</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mrow></math></mathml>. (5) </p>
                </div>
                <div class="p1">
                    <p id="57">根据行动策略和评估策略是否是1个策略, 时间差分法又分同策略法与异策略法.时间差分法的同策略法即Sarsa方法, 异策略法即Q-learning方法.</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58"><b>1.2 强化学习算法</b></h4>
                <div class="p1">
                    <p id="59">本节介绍调节系统用到的3种强化学习算法:DQN (deep q network) 算法、A2C (synchronous advantage actor critic) 算法、PPO (proximal policy optimization) 算法.</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.2.1 DQN算法</h4>
                <div class="p1">
                    <p id="61">DQN是基于Q-learning的算法, 其对Q-learning的修改主要体现在3个方面<citation id="212" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="62">1) DQN利用深度卷积神经网络逼近值函数.利用神经网络逼近值函数的做法在强化学习领域早就存在了, 可以追溯到20世纪90年代.当时人们发现用深度神经网络去逼近值函数常常出现不稳定不收敛的情况, 所以这个方向一直没有突破, 那DQN做了什么其他的事情呢?</p>
                </div>
                <div class="p1">
                    <p id="63">2) DQN利用了经验回放对强化学习的学习过程进行训练.人在睡觉的时候, 海马体会把1天的记忆重放给大脑皮层.利用这个启发机制, DQN构造了一种新的神经网络训练方法:经验回放.训练的前提是训练数据是独立同分布的, 而通过强化学习采集到的数据之间存在着关联性, 利用这些数据进行顺序训练, 神经网络当然不稳定.经验回放可以打破数据间的关联, 从而使网络得以收敛.</p>
                </div>
                <div class="p1">
                    <p id="64">3) DQN独立设置了目标网络来单独处理时间差分算法中的TD偏差.我们称计算TD目标时所用的网络为TD网络.以往的神经网络逼近值函数时, 计算TD目标的动作值函数所用的网络参数为<i>θ</i>, 与梯度计算中要逼近的值函数所用的网络参数相同, 这样就容易使得数据间存在关联性, 训练不稳定.为了解决这个问题, DQN中计算TD目标的网络表示为<i>θ</i><sup>-</sup>, 计算值函数逼近的网络表示为<i>θ</i>, 用于动作值函数逼近的网络每一步都更新, 而用于计算TD目标的网络每个固定的步数更新1次.因此值函数的更新变为</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><mi>θ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>α</mi><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup></mrow></math></mathml>;<i>θ</i><sup>-</sup>) -<i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i>) ]∇<i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i>) . (6) </p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">1.2.2 从AC (actor critic) 算法到A2C算法的演化</h4>
                <div class="p1">
                    <p id="69">当要解决的问题动作空间很大或者动作为连续集时, 值函数方法无法有效求解.策略梯度法是将策略进行参数化, 利用线性或非线性函数对策略进行表示, 此时强化学习的目标回报函数可表示为</p>
                </div>
                <div class="p1">
                    <p id="70"><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>τ</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>τ</mi></mrow></math></mathml>;<i>θ</i>) <i>R</i> (<i>τ</i>) , (7) </p>
                </div>
                <div class="p1">
                    <p id="72"><i>τ</i>表示智能体的行动轨迹.我们的训练方法则可以表示为</p>
                </div>
                <div class="p1">
                    <p id="73"><i>θ</i><sub><i>t</i>+1</sub>←<i>θ</i><sub><i>t</i></sub>+<i>α</i>∇<sub><i>θ</i></sub><i>U</i> (<i>θ</i>) , (8) </p>
                </div>
                <div class="p1">
                    <p id="74">此时问题的关键是如何计算策略梯度:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mi>θ</mi></msub><mi>U</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>τ</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>τ</mi></mrow></math></mathml>;<i>θ</i>) ∇<sub><i>θ</i></sub> ln <i>P</i> (<i>τ</i>;<i>θ</i>) <i>R</i> (<i>τ</i>) . (9) </p>
                </div>
                <div class="p1">
                    <p id="77">利用经验平均来估算梯度:</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mi>θ</mi></msub><mi>U</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>≈</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>Η</mi></munderover><mo>∇</mo></mstyle><msub><mrow></mrow><mi>θ</mi></msub><mspace width="0.25em" /><mi>ln</mi><mspace width="0.25em" /><mi>π</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></math></mathml>. (10) </p>
                </div>
                <div class="p1">
                    <p id="80">式 (10) 的意义在于, 回报越高的动作越努力提高它出现的概率.但是某些情形下, 每个动作的总回报<i>r</i><sub><i>t</i></sub>都不为负, 那么所有的梯度值都≥0, 此时每个动作出现的概率都会提高, 这在很大程度下减缓了学习的速度, 而且也会使梯度的方差很大.因此需要对<i>r</i><sub><i>t</i></sub>使用某种标准化操作来降低梯度的方差.具体地, 可以让<i>r</i><sub><i>t</i></sub>减去1个基线<i>b</i> (baseline) , <i>b</i>通常设为<i>r</i><sub><i>t</i></sub>的1个期望估计, 通过求梯度更新<i>θ</i>, 总回报超过基线的动作的概率会提高, 反之则降低, 同时还可以降低梯度方差 (证明略) .这种方式被叫作行动者-评论家体系结构.</p>
                </div>
                <div class="p1">
                    <p id="81">A3C (asynchronous advantage actor critic) 算法为了提升训练速度采用异步训练的思想, 利用多个线程<citation id="213" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.每个线程相当于1个智能体在随机探索, 多个智能体共同探索, 并行计算策略梯度, 对参数进行更新.相比DQN算法, A3C算法不需要使用经验池来存储历史样本并随机抽取训练来打乱数据相关性, 节约了存储空间, 并且采用异步训练, 大大加倍了数据的采样速度, 也因此提升了训练速度.与此同时, 采用多个不同训练环境采集样本, 样本的分布更加均匀, 更有利于神经网络的训练.</p>
                </div>
                <div class="p1">
                    <p id="82">在A3C的基础上, OpenAI又提出了A2C.如图2所示, 2个算法的不同点在于, 在A3C中, 每个智能体并行独立地更新全局网络, 因此, 在特定时间, 智能体使用的网络权重与其他智能体是不同的, 这样导致每个智能体使用不同的策略在探索更多的环境.而在A2C中, 所有并行智能体的更新先被统一收集起来, 然后去更新全局网络, 全局网络更新完后再将权重分发到各个智能体.为了鼓励探索, 每个智能体最后执行的动作会被加入随机噪声.</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907022_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 A3C与A2C参数更新方式的对比" src="Detail/GetImg?filename=images/JFYZ201907022_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 A3C与A2C参数更新方式的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907022_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Comparison of parameter updating methods between A3C and A2C</p>

                </div>
                <h4 class="anchor-tag" id="84" name="84">1.2.3 从TRPO算法到PPO算法的演化</h4>
                <div class="p1">
                    <p id="85">对于普通的策略梯度方法, 如果更新步长太大, 则容易发散;如果更新步长太小, 即使收敛, 收敛速度也很慢, 因而实际情况下训练常处于振荡不稳定的状态.文献<citation id="214" type="reference">[<a class="sup">6</a>]</citation>为了解决普通的策略梯度算法无法保证性能单调非递减而提出了TRPO (trust region policy optimization) 算法, TRPO的主要改进是, 它可以设置较大的步长, 加快学习速度, 同时对目标函数进行优化时有一定的约束条件, 满足该约束条件后, 优化是安全的, 并能从数学上证明优化单调递增.约束表示新旧策略差异的KL散度期望小于一定值情形下最大化下面目标表达式, 通过KL散度来表示新旧策略差异, 它小于一定值表示1个置信域, 在这个置信域内进行优化.其目标函数:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>θ</mi></munder><mspace width="0.25em" /><mi>E</mi><mrow><mo>[</mo><mrow><mfrac><mrow><mi>π</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mrow><mrow><mi>π</mi><msub><mrow></mrow><msup><mi>θ</mi><mo>′</mo></msup></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mrow></mfrac><mi>A</mi><msub><mrow></mrow><msup><mi>θ</mi><mo>′</mo></msup></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext> </mtext><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mrow><mtext>Κ</mtext><mtext>L</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>π</mi><msub><mrow></mrow><msup><mi>θ</mi><mo>′</mo></msup></msub><mo>, </mo><mi>π</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false">) </mo><mo>≤</mo><mi>δ</mi><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">TRPO的标准解法是将目标函数进行一阶近似, 约束条件利用泰勒进行二阶展开, 然后利用共轭梯度的方法求解最优的更新参数.然而当策略选用深层神经网络表示时, TRPO的标准解法计算量会非常大.因为共轭梯度法需要将约束条件进行二阶展开, 二阶矩阵的计算量非常大.PPO是TRPO的一阶近似<citation id="215" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 把TRPO中的约束放到目标函数中, 减少了计算量, 可以应用到大规模的策略更新中, 其目标函数为</p>
                </div>
                <div class="p1">
                    <p id="88"><i>L</i> (<i>θ</i>) =<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mfrac><mrow><mi>π</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>;</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>π</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle></mrow></math></mathml><i>A</i><sub><i>θ</i>′</sub> (<i>a</i><sub><i>t</i></sub>|<i>s</i><sub><i>t</i></sub>) -<i>β D</i><sub>KL</sub> (<i>π</i><sub><i>θ</i>′</sub>, <i>π</i><sub><i>θ</i></sub>) . (12) </p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>1.3 参数调优</b></h4>
                <div class="p1">
                    <p id="92">参数调优是一个充满挑战的研究领域.</p>
                </div>
                <div class="p1">
                    <p id="93">传统的方法有控制反馈算法与参数搜索算法.控制反馈算法是基于模型的方法, 当已知负载运行情况且该负载情况非常简单时效果良好, 但是此方法往往需要管理员设置关键参数的取值<citation id="216" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>.参数搜索算法往往是针对特定系统的特定负载进行一次搜索的过程<citation id="217" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 当负载变化时其不适用.</p>
                </div>
                <div class="p1">
                    <p id="94">文献<citation id="218" type="reference">[<a class="sup">10</a>]</citation>提出了运用神经网络来加速传统的搜索方法.文献<citation id="219" type="reference">[<a class="sup">11</a>]</citation>最先尝试了运用深度强化学习算法来进行参数调优, 但是只是针对1台单独的服务器.文献<citation id="220" type="reference">[<a class="sup">12</a>]</citation>开发了一种系统来对更大规模的集群进行调参, 但是其存在5个缺点:</p>
                </div>
                <div class="p1">
                    <p id="95">1) 调节参数的范围较小, 其只调节了2个参数.</p>
                </div>
                <div class="p1">
                    <p id="96">2) Reward的设置过于简单, 不能适应复杂且动态变化的负载.</p>
                </div>
                <div class="p1">
                    <p id="97">3) 状态State的设置有误, 其将每个客户端的状态信息作为智能体的输入, 但是智能体针对每个客户端的决策应是相互独立的, 即智能体的输入应是每个单独客户端的状态, 返回的动作信息应由接口负责将其正确分发.</p>
                </div>
                <div class="p1">
                    <p id="98">4) 训练过程未做批标准化, 这样会导致训练不稳定, 模型难以收敛.</p>
                </div>
                <div class="p1">
                    <p id="99">5) 强化学习算法发展迅速, 最初的DQN算法无论是在训练速度以及模型效果上, 已经与前沿算法有了显著差距.</p>
                </div>
                <h3 id="100" name="100" class="anchor-tag"><b>2 参数调节系统的设计与实现</b></h3>
                <div class="p1">
                    <p id="101">如图3所示, 系统由策略节点和目标集群组成.策略节点包含强化学习智能体以及信息接口, 目标集群上的每个节点都包括1个用来收集节点信息的Monitor和1个用来执行参数调节动作的Actor.系统运行时, 每个节点的Monitor每隔固定的时间会收集系统状态信息, 并将信息发送给接口, 接口把状态信息发送给智能体, 智能体根据状态信息返回动作信息, 并将其发回给接口, 由接口将该动作发送给对应的节点, 该节点的Actor负责执行调节动作.然后不断迭代执行上述过程, 直至满足终止条件.在这整个的交互过程中, 强化学习智能体是在不断训练提升的.</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907022_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 参数调节系统框架" src="Detail/GetImg?filename=images/JFYZ201907022_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 参数调节系统框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907022_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Framework of parameter tuning system</p>

                </div>
                <div class="p1">
                    <p id="103">在实际的部署过程中, 策略节点应部署在目标集群外, 以尽量降低系统运行过程中对目标集群的影响.并且, 策略节点应尽可能地部署在含有GPU的节点上, 以加快训练速度.</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.1 参数调节系统的状态信息</b></h4>
                <div class="p1">
                    <p id="105">类似于人类需要对环境信息了若指掌后才能做出好的决策一样, 强化学习中的状态信息是十分重要的, 算法会分析这些信息并做出决策.传统的机器学习方法需要大量的特征工程, 近年来深度学习的飞速发展, 已经证明其拥有强大的感知能力, 所以深度强化学习算法只需要把状态信息输入深度神经网络, 神经网络会自动抽取重要特征进行训练.</p>
                </div>
                <div class="p1">
                    <p id="106">在做状态信息选取时, 应尽可能地把跟系统性能相关的因素都包含进来.本文选取的部分状态信息如表1所示:</p>
                </div>
                <div class="area_img" id="107">
                    <p class="img_tit"><b>表1 状态信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Status Information</b></p>
                    <p class="img_note"></p>
                    <table id="107" border="1"><tr><td><br />Status</td><td>Description</td></tr><tr><td><br />Read Throughput</td><td>Current read throughput of cluster.</td></tr><tr><td><br />Write Throughput</td><td>Current write throughput of cluster.</td></tr><tr><td><br />Dirty Pages Hits</td><td>The number of write operations that have been satisfied by the dirty page cache.</td></tr><tr><td><br />Dirty Pages Misses</td><td>The number of write operations that were not satisfied by the dirty page cache.</td></tr><tr><td><br />Read IOPS</td><td>Current read IOPS of cluster.</td></tr><tr><td><br />Write IOPS</td><td>Current read IOPS of cluster.</td></tr><tr><td><br />Used Space</td><td>Cache space used.</td></tr><tr><td><br />Unused Space</td><td>Free cache space.</td></tr><tr><td><br />Reclaim Count</td><td>Cache recycle count.</td></tr><tr><td><br />Current Dirty Space</td><td>A read-only value that returns the current space written and cached on this OSC.</td></tr><tr><td><br />Current Grant Space</td><td>The amount of space that this client has reserved for write back cache.</td></tr><tr><td><br />Inflight</td><td>The number of RPC (remote procedure call) to be processed.</td></tr><tr><td><br />Timeouts</td><td>RPC timeout value.</td></tr><tr><td><br />Average Wait Time</td><td>Request average wait time.</td></tr><tr><td><br />Cached Space</td><td>Current total cache space.</td></tr><tr><td><br />Requests Wait Time</td><td>The amount of time the request waits in the queue before the server processes it.</td></tr><tr><td><br />Active Requests</td><td>The number of requests currently being processed.</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>2.2 参数调节系统的动作信息</b></h4>
                <div class="p1">
                    <p id="109">动作决定了参数应如何调节, 按离散动作空间来处理, 为每个参数指定1个步长, 则每个参数对应2个动作:调高和调低, 调高即当前参数加步长, 调低即当前参数减步长.此外, 系统如果根据状态信息判断当前没有需要调节的参数, 那么也应可以选择什么都不做.这样, 动作空间即为:2×参数个数+1.</p>
                </div>
                <div class="p1">
                    <p id="110">从安全的角度考虑, 还应为每个参数设置范围, 当调节后的值大于或小于该范围时, 相应地设置值为最大值或最小值, 以保证系统的正常运行.</p>
                </div>
                <div class="p1">
                    <p id="111">当性能优化目标确定时, 系统应针对该优化目标进行参数选取.这需要存储领域一定的先验知识, 即需要知道所关注的性能变化是与哪些参数相关的.如果将存储系统的全部参数都纳入进来, 动作空间会很大, 模型不好收敛.本文的目标性能是吞吐率, 选择的参数有7个:</p>
                </div>
                <div class="p1">
                    <p id="112">1) 最大脏数据量 (max dirty space, MD) , 对象存储客户端 (object storage client, OSC) 中可以写入并排队等候的脏数据量.</p>
                </div>
                <div class="p1">
                    <p id="113">2) 单个RPC进行I/O的最大页数 (max pages per RPC, MPPR) , 在单个RPC中对对象存储目标 (object storage target, OST) 进行I/O的最大页数.</p>
                </div>
                <div class="p1">
                    <p id="114">3) RPC最大并发处理数 (max RPCs in flight, MRF) , 从OSC到其OST的最大可处理的并发RPC数.</p>
                </div>
                <div class="p1">
                    <p id="115">4) 文件预读的最大数据量 (max read ahead space, MRA) .</p>
                </div>
                <div class="p1">
                    <p id="116">5) 客户端缓存的最大非活动数据量 (max cached space, MC) .</p>
                </div>
                <div class="p1">
                    <p id="117">6) 完整读取文件的最大大小 (max read ahead whole space, MRAW) .</p>
                </div>
                <div class="p1">
                    <p id="118">7) 线程预取的最大文件属性数 (statahead max, STM) .</p>
                </div>
                <div class="p1">
                    <p id="119">参数相关信息如表2所示:</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表2 选取的参数及其相关属性</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 The Parameters Selected in This Paper and Their Related Attributes</b></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />Parameter</td><td>Default</td><td>Min</td><td>Max</td><td>Step Size</td></tr><tr><td><br />MD/MB</td><td>32</td><td>32</td><td>4 096</td><td>32</td></tr><tr><td><br />MPPR</td><td>256</td><td>64</td><td>1 024</td><td>64</td></tr><tr><td><br />MRF</td><td>8</td><td>4</td><td>256</td><td>4</td></tr><tr><td><br />MRA</td><td>40</td><td>0</td><td>160</td><td>40</td></tr><tr><td><br />MC/MB</td><td>128</td><td>128</td><td>32 234</td><td>128</td></tr><tr><td><br />MRAW/MB</td><td>2</td><td>0</td><td>8</td><td>2</td></tr><tr><td><br />STM</td><td>32</td><td>0</td><td>8192</td><td>32</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>2.3 参数调节系统的奖励信息</b></h4>
                <div class="p1">
                    <p id="122">奖励信息的设计是强化学习最关键之处, 因为模型的训练是依赖奖励信息进行的, 奖励信息设计的好坏往往决定了1个强化学习算法最后能不能成功应用.</p>
                </div>
                <div class="p1">
                    <p id="123">分布式存储系统的负载是不断变化的, 如果只是简单定义奖励信息为当前吞吐率与上一时刻吞吐率之差, 当负载剧烈变化时, 奖励信息会相应有很大变化, 那么此时的奖励信息系统无法分辨是因为负载变化导致, 还是因为参数调节导致模型无法收敛.</p>
                </div>
                <div class="p1">
                    <p id="124">如果不只是考虑当前时间点的吞吐率跟上时间点的吞吐率差, 而是考虑动作执行后某一时间段内的吞吐率变化情况作为奖励信息, 比如最近一定步数<i>N</i>的吞吐率差, 此处用<i>e</i>表示, 即:</p>
                </div>
                <div class="p1">
                    <p id="125"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>e</mi></mstyle><mi>γ</mi><msup><mrow></mrow><mi>n</mi></msup></mrow></math></mathml>. (13) </p>
                </div>
                <div class="p1">
                    <p id="127">这样当步数选取合适的值时, 可以克服某个时间点 (或某个时间段) 负载剧烈变化导致奖励信息受影响的问题.<i>γ</i>值的选取代表了是更关心当下的奖励还是长期奖励.</p>
                </div>
                <div class="p1">
                    <p id="128">可能会出现这个时间窗口内, 负载都一直在不断加大, 从而导致奖励信息因为负载的原因一直在变大, 这样的情况可以接受的, 并且奖励信息也理应给高, 因为这代表系统利用率高 (除了吞吐率外也考虑系统的利用率) .</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129"><b>2.4 参数调节系统的接口模块</b></h4>
                <div class="p1">
                    <p id="130">接口模块介于强化学习算法模块与目标集群之间, 负责2个模块之间的消息通信, 使得2个模块可以独立开发而互相不影响, 践行了强内聚、松耦合的设计模式.</p>
                </div>
                <div class="p1">
                    <p id="131">本文的消息通信模块选用了ZeroMQ, 它是一种基于消息队列的多线程网络库, 其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象, 提供跨越多种传输协议的套接字, 可并行运行, 分散在分布式系统间.ZeroMQ将消息通信分成4种模型, 分别是1对1结对模型、请求回应模型、发布订阅模型、推拉模型.基于系统的架构, 本文采用了请求回应模型.</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132"><b>2.5 参数调节系统的神经网络模型</b></h4>
                <div class="p1">
                    <p id="133">1个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数 (例如logistic sigmoid激活函数) 的隐藏层, 只要给予网络足够数量的隐藏单元, 它可以以任意的精度来近似任何从1个有限维空间到另一个有限维空间的Borel可测函数, 此即万能近似定理<citation id="221" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.万能近似定理意味着无论我们试图学习什么函数, 我们知道1个大的多层感知器一定能够表示这个函数.具有单层的前馈网络足以表示任何函数, 但是网络层可能大得不可实现, 并且可能无法正确地学习和泛化.在很多情况下, 使用更深的模型能够减少表示期望函数所需单元的数量, 并且可以减少泛化误差.</p>
                </div>
                <div class="p1">
                    <p id="134">深度学习与强化学习的结合, 即用深度神经网络去逼近强化学习的值函数或策略函数.本文我们采用Pytorch 0.4实现了含3个隐藏层的全连接神经网络, 每个隐藏层的单元数量是状态空间的2倍, 激活函数为ReLU函数, 优化算法为Adam算法.</p>
                </div>
                <h3 id="135" name="135" class="anchor-tag"><b>3 验证系统调节效果的实验</b></h3>
                <h4 class="anchor-tag" id="136" name="136"><b>3.1 实验环境</b></h4>
                <div class="p1">
                    <p id="137">整个存储系统有2个服务器节点和2个客户端节点.2个服务器节点中1个为元数据服务器 (meta data server, MDS) 节点, 1个为对象存储服务器 (object storage server, OSS) 节点, 对象存储服务器管理着22个对象存储目标 (object storage target, OST) .这些服务器节点在测试期间均为闲置节点, 以避免其他负载对测试结果的影响.2个客户端节点采用相同的配置:8个Intel<sup>®</sup> Xeon<sup>®</sup> CPU E5620@2.40 GHz, 32 GB RAM, 网络带宽为10 GB/s.</p>
                </div>
                <div class="p1">
                    <p id="138">策略节点配置有24个Intel<sup>®</sup> Xeon<sup>®</sup> CPU E5-2650 v4 @ 2.20 GHz, 64 GB RAM, 以及2个NVIDIA Tesla K80 GPU.</p>
                </div>
                <div class="p1">
                    <p id="139">本文的分布式文件系统选用了Lustre (2.5.3) .强化学习算法使用了Pytorch (0.4) 实现.</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140"><b>3.2 测试工具</b></h4>
                <div class="p1">
                    <p id="141">本文选用了Iozone (3.479) 来进行测试.Iozone是1个文件系统的 Benchmark 工具, 可以测试不同的操作系统中文件系统的读写性能.</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142"><b>3.3 测试项</b></h4>
                <div class="p1">
                    <p id="143">本文选取7个测试项及其定义为:</p>
                </div>
                <div class="p1">
                    <p id="144">1) Write.测试向1个新文件写入的性能.</p>
                </div>
                <div class="p1">
                    <p id="145">2) Re -Write.测试向1个已存在的文件写入的性能.</p>
                </div>
                <div class="p1">
                    <p id="146">3) Read.测试读1个已存在的文件的性能.</p>
                </div>
                <div class="p1">
                    <p id="147">4) Re-Read.测试读1个最近读过的文件的性能.</p>
                </div>
                <div class="p1">
                    <p id="148">5) Strided Read.测试跳跃读1个文件的性能.</p>
                </div>
                <div class="p1">
                    <p id="149">6) Random Read.测试读1个文件中的随机偏移量的性能.</p>
                </div>
                <div class="p1">
                    <p id="150">7) Random Write.测试写1个文件中的随机偏移量的性能.</p>
                </div>
                <div class="p1">
                    <p id="151">测试以吞吐量模式运行, 指定每个客户端节点启动16个进程来并行读写.测试文件大小8 GB, 文件块大小1 MB, 跳读跨度为2 MB.测试时会在测试目录里生成各个节点的数据包, 测试完成后在日志文件里会看到各个节点的读写速度、最大速度、最小速度、平均速度、总的吞吐量.</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>3.4 实验流程</b></h4>
                <div class="p1">
                    <p id="153">1) 采用Lustre默认参数配置测试3次, 取其测试结果均值作为 baseline.</p>
                </div>
                <div class="p1">
                    <p id="154">2) 启动Iozone测试, 独立地对每个算法训练一定的时间, 将训练好的模型储存以备调用.</p>
                </div>
                <div class="p1">
                    <p id="155">3) 将系统参数重置为默认值.</p>
                </div>
                <div class="p1">
                    <p id="156">4) 启动Iozone测试, 同时启动调节系统, 每个算法测试3次后, 取其测试结果均值得到最终结果.</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>3.5 实验结果</b></h4>
                <div class="p1">
                    <p id="158">搭建好测试环境后, 我们对各强化学习算法进行了测试.测试前, 对每个算法模型预先训练24 h, 后对每个算法测3次 (每次测试花费时长约10 h) , 取其均值作为测试结果, 如表3所示, 测试数据为32个并发读写进程的平均吞吐率.为了更加直观地体现测试结果, 我们采用柱状图对测试数据进行可视化, 但由于各测试项数据差异较大, 致使数据值较小的测试项展示效果不太明显, 故又对测试数据进行了标准化 (范围100～200) , 如图4所示.</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表3 各算法性能测试数据</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Performance Test Data</b></p>
                    <p class="img_note">KB/s</p>
                    <table id="159" border="1"><tr><td><br />Test Items</td><td>Baseline</td><td>DQN</td><td>A2C</td><td>PPO</td></tr><tr><td><br />Initial Write</td><td>1 233</td><td>1 186</td><td>1 172</td><td>1 215</td></tr><tr><td><br />Re-Write</td><td>1 249</td><td>1 202</td><td>1 171</td><td>1 214</td></tr><tr><td><br />Read</td><td>38 613</td><td>30 505</td><td>36 980</td><td>40 924</td></tr><tr><td><br />Re-Read</td><td>41 777</td><td>30 825</td><td>41 241</td><td>43 380</td></tr><tr><td><br />Stride Read</td><td>3 493</td><td>3 570</td><td>4 051</td><td>3 941</td></tr><tr><td><br />Random Read</td><td>1 893</td><td>1 566</td><td>2 505</td><td>2 407</td></tr><tr><td><br />Random Write</td><td>560</td><td>565</td><td>581</td><td>563</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907022_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 各算法性能测试结果" src="Detail/GetImg?filename=images/JFYZ201907022_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 各算法性能测试结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907022_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Performance test results</p>

                </div>
                <div class="p1">
                    <p id="161">可以看到, 在跳读和随机读测试项上, A2C和PPO强化学习算法都有明显的提升效果.而DQN算法在这种负载多变的测试环境上表现较差, 这在文献<citation id="222" type="reference">[<a class="sup">12</a>]</citation>中也有所体现, 其测试结果指出DQN算法在测试读写比例为1∶1时性能几乎没有提升, 本文的测试读写比例即为1∶1.因此, 针对存储系统参数调优任务来说, 策略梯度方法是明显优于值函数方法的.具体来说, 虽然PPO算法对5个测试项都有性能提升, 而就高能物理计算环境所关注的跳读和随机读来说, A2C 算法表现更好, 是最贴近实际应用的算法.</p>
                </div>
                <div class="p1">
                    <p id="162">经过对测试结果的分析, 我们发现使用强化学习的方法来对存储系统的参数进行调节, 确实对存储系统性能有了较为显著的提升, 而当负载发生变化时, 其可进行动态适应性的调整, 并且对生产系统的影响很小.可以预见:将其部署在生产系统后, 在提升系统性能的同时, 可大大减少人力成本及时间成本.</p>
                </div>
                <h3 id="163" name="163" class="anchor-tag"><b>4 总结与展望</b></h3>
                <div class="p1">
                    <p id="164">本文介绍了一种基于强化学习的参数调节方法, 实验表明该方法使我们所关注的性能得到了显著提升.事实上该方法不只是针对分布式存储领域, 通过自定义环境和奖励, 它可以泛化到更多的领域中.</p>
                </div>
                <div class="p1">
                    <p id="165">总的来说, 我们将强化学习应用到分布式存储领域中只是1次初步的尝试, 有很多方面需要进一步探索:</p>
                </div>
                <div class="p1">
                    <p id="166">1) 本文只是对Lustre客户端的参数进行了调节, 未来我们会增加Lustre服务器端以及其他分布式文件系统 (如eos, ceph) 的参数调节.</p>
                </div>
                <div class="p1">
                    <p id="167">2) 状态的表示.本文我们将当前时间点跟系统性能相关的因素作为状态来训练, 事实这可能存在一定的局限, 只了解到当前的状态, 而对系统的历史信息以及变化趋势没有涉及.因此, 未来的工作会对系统的状态表示加以改进.</p>
                </div>
                <div class="p1">
                    <p id="168">3) 算法.强化学习发展迅速, 不断有新的方法被提出, 如逆向强化学习、分层强化学习、世界模型等, 新的算法的性能往往会有较大提升, 我们会在未来的工作中加深对算法的研究以及应用, 并在前人的基础上尝试提出自己的强化学习算法.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="232" type="formula" href="images/JFYZ201907022_23200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张文韬</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="233" type="formula" href="images/JFYZ201907022_23300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">汪璐</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="234" type="formula" href="images/JFYZ201907022_23400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">程耀东</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="183">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201702003&amp;v=MzE4MDF5dlNkTEc0SDliTXJZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXJuVXJ2SUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Cheng Yaodong, Zhang Xiao, Wang Peijian, et al.Data management challenges and event index technologies in high energy physics[J].Journal of Computer Research and Development, 2017, 54 (2) :258- 266 (in Chinese) (程耀东, 张潇, 王培建, 等.高能物理大数据挑战与海量事例特征索引技术研究[J].计算机研究与发展, 2017, 54 (2) :258- 266) 
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving the performance of lustre file system in HPC environments">

                                <b>[2]</b>Han J, Kim D, Eom H.Improving the performance of lustre file system in HPC environments[C] //Proc of the 1st Int Workshops on Foundations and Applications of Self* Systems.Piscataway, NJ:IEEE, 2016:84- 89
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning:An Introduction">

                                <b>[3]</b>Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge:MIT Press, 2018
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529- 533
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">

                                <b>[5]</b>Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:PMLR, 2016:1928- 1937
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trust region policy optimization">

                                <b>[6]</b>Schulman J, Levine S, Abbeel P, et al.Trust region policy optimization[C] //Proc of the 32nd Int Conf on Machine Learning.New York:PMLR, 2015:1889- 1897
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Proximal policy optimization algorithms">

                                <b>[7]</b>Schulman J, Wolski F, Dhariwal P, et al.Proximal policy optimization algorithms[J].arXiv preprint arXiv:1707.06347, 2017
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Managing Web server performance with AutoTune agents">

                                <b>[8]</b>Diao Yixin, Hellerstein J L, Parekh S, et al.Managing Web server performance with AutoTune agents[J].IBM Systems Journal, 2003, 42 (1) :136- 149
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An uncertainty-aware approach to optimal configuration of stream processing systems">

                                <b>[9]</b>Jamshidi P, Casale G.An uncertainty-aware approach to optimal configuration of stream processing systems[C] //Proc of the 24th IEEE Int Symp on Modeling, Analysis and Simulation of Computer and Telecommunication Systems.Piscataway, NJ:IEEE, 2016:39- 48
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501347107&amp;v=MzIwODZySUlsd1ZiaEk9TmlmT2ZiSzdIdEROcW85RVorOElEWHcrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Zhang Fan, Cao Junwei, Liu Lianchen, et al.Performance improvement of distributed systems by autotuning of the configuration parameters[J].Tsinghua Science and Technology, 2011, 16 (4) :440- 448
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boosting the performance of computing systems through adaptive configuration tuning">

                                <b>[11]</b>Chen Haifeng, Jiang Guofei, Zhang Hui, et al.Boosting the performance of computing systems through adaptive configuration tuning[C] //Proc of the 24th ACM Symp on Applied Computing.New York:ACM, 2009:1045- 1049
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CAPES:unsupervised storage performance tuning using neural network-based deep reinforcement learning">

                                <b>[12]</b>Li Yan, Chang K, Bel O, et al.CAPES:Unsupervised storage performance tuning using neural network-based deep reinforcement learning[C] //Proc of the 30th Int Conf for High Performance Computing, Networking, Storage and Analysis.New York:ACM, 2017:42:1- 42:14
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning">

                                <b>[13]</b>Goodfellow I, Bengio Y, Courville A, et al.Deep Learning[M].Cambridge:MIT Press, 2016
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201907022" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907022&amp;v=MDM4ODZPZVplUnJGeXJuVXJ2SUx5dlNkTEc0SDlqTXFJOUhab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXF2cTF3MFRjZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

