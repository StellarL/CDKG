

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127128479676250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201910008%26RESULT%3d1%26SIGN%3dz3ARTu2MhJJQQTPJN7ATnbgrW9c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201910008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201910008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201910008&amp;v=MjYwNzJPM3pxcUJ0R0ZyQ1VSTE9lWmVSc0Z5emdWYnpCTHl2U2RMRzRIOWpOcjQ5RmJJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#251" data-title="&lt;b&gt;1 人工智能系统安全风险模型&lt;/b&gt; "><b>1 人工智能系统安全风险模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#253" data-title="&lt;b&gt;1.1 人工智能系统攻击面&lt;/b&gt;"><b>1.1 人工智能系统攻击面</b></a></li>
                                                <li><a href="#261" data-title="&lt;b&gt;1.2 攻击能力&lt;/b&gt;"><b>1.2 攻击能力</b></a></li>
                                                <li><a href="#269" data-title="&lt;b&gt;1.3 攻击目标&lt;/b&gt;"><b>1.3 攻击目标</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#278" data-title="&lt;b&gt;2 输入环节安全风险及对策&lt;/b&gt; "><b>2 输入环节安全风险及对策</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#490" data-title="1) 传感器欺骗.">1) 传感器欺骗.</a></li>
                                                <li><a href="#491" data-title="2) 应对措施.">2) 应对措施.</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#282" data-title="&lt;b&gt;3 数据预处理环节安全风险及对策&lt;/b&gt; "><b>3 数据预处理环节安全风险及对策</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#492" data-title="1) 重采样攻击.">1) 重采样攻击.</a></li>
                                                <li><a href="#493" data-title="2) 应对措施.">2) 应对措施.</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#286" data-title="&lt;b&gt;4 机器学习模型中的安全风险及对策&lt;/b&gt; "><b>4 机器学习模型中的安全风险及对策</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#494" data-title="1) 诱导攻击(causative attack).">1) 诱导攻击(causative attack).</a></li>
                                                <li><a href="#495" data-title="2) 逃逸攻击(evasion attack).">2) 逃逸攻击(evasion attack).</a></li>
                                                <li><a href="#496" data-title="3) 探索攻击(exploratory attack).">3) 探索攻击(exploratory attack).</a></li>
                                                <li><a href="#292" data-title="&lt;b&gt;4.1 数据投毒&lt;/b&gt;"><b>4.1 数据投毒</b></a></li>
                                                <li><a href="#295" data-title="&lt;b&gt;4.2 模型后门&lt;/b&gt;"><b>4.2 模型后门</b></a></li>
                                                <li><a href="#298" data-title="&lt;b&gt;4.3 对抗样本&lt;/b&gt;"><b>4.3 对抗样本</b></a></li>
                                                <li><a href="#307" data-title="&lt;b&gt;4.4 模型逆向&lt;/b&gt;"><b>4.4 模型逆向</b></a></li>
                                                <li><a href="#311" data-title="&lt;b&gt;4.5 模型萃取&lt;/b&gt;"><b>4.5 模型萃取</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#314" data-title="&lt;b&gt;5 输出环节安全风险&lt;/b&gt; "><b>5 输出环节安全风险</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#317" data-title="&lt;b&gt;6 系统实际搭建及运行中的安全风险及对策&lt;/b&gt; "><b>6 系统实际搭建及运行中的安全风险及对策</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#318" data-title="&lt;b&gt;6.1 代码漏洞&lt;/b&gt;"><b>6.1 代码漏洞</b></a></li>
                                                <li><a href="#321" data-title="&lt;b&gt;6.2 学习不完全/学习偏差&lt;/b&gt;"><b>6.2 学习不完全/学习偏差</b></a></li>
                                                <li><a href="#324" data-title="&lt;b&gt;6.3 系统设计缺陷利用&lt;/b&gt;"><b>6.3 系统设计缺陷利用</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#328" data-title="&lt;b&gt;7 人工智能安全分析与防护技术的研究展望&lt;/b&gt; "><b>7 人工智能安全分析与防护技术的研究展望</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#497" data-title="1) 物理对抗样本.">1) 物理对抗样本.</a></li>
                                                <li><a href="#498" data-title="2) 模型鲁棒性的形式化验证.">2) 模型鲁棒性的形式化验证.</a></li>
                                                <li><a href="#499" data-title="3) 人工智能系统自动化测试方法.">3) 人工智能系统自动化测试方法.</a></li>
                                                <li><a href="#500" data-title="4) 隐私保护.">4) 隐私保护.</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#334" data-title="&lt;b&gt;8 结  论&lt;/b&gt; "><b>8 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#260" data-title="图1 人工智能系统基本框架">图1 人工智能系统基本框架</a></li>
                                                <li><a href="#327" data-title="&lt;b&gt;表1 人工智能系统安全与隐私风险分析小结&lt;/b&gt;"><b>表1 人工智能系统安全与隐私风险分析小结</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title="He Kaiming,Zhang Xiangyu,Ren Shaoqing,et al.Deep residual learning for image recognition[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770- 778" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[1]</b>
                                        He Kaiming,Zhang Xiangyu,Ren Shaoqing,et al.Deep residual learning for image recognition[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770- 778
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title="Xiong W,Droppo J,Huang Xuedong,et al.Achieving human parity in conversational speech recognition[J].arXiv preprint arXiv:1610.05256,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Achieving human parity in conversational speech recognition">
                                        <b>[2]</b>
                                        Xiong W,Droppo J,Huang Xuedong,et al.Achieving human parity in conversational speech recognition[J].arXiv preprint arXiv:1610.05256,2016
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title="Fan Zhengguang,Qu Dan,Yan Honggang,et al.Fast incremental outlier mining algorithm based on grid and capacity[J].Journal of Computer Research and Development,2017,54(5):1036- 1044 (in Chinese)(范正光,屈丹,闫红刚,等.基于深层神经网络的多特征关联声学建模方法[J].计算机研究与发展,2017,54(5):1036- 1044)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201705013&amp;v=MjQ1MTVxcUJ0R0ZyQ1VSTE9lWmVSc0Z5emdWYnpCTHl2U2RMRzRIOWJNcW85RVo0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Fan Zhengguang,Qu Dan,Yan Honggang,et al.Fast incremental outlier mining algorithm based on grid and capacity[J].Journal of Computer Research and Development,2017,54(5):1036- 1044 (in Chinese)(范正光,屈丹,闫红刚,等.基于深层神经网络的多特征关联声学建模方法[J].计算机研究与发展,2017,54(5):1036- 1044)
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title="Yu A W,Dohan D,Luong M T,et al.QANet:Combining local convolution with global self-attention for reading comprehension[J].arXiv preprint arXiv:1804.09541,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=QANet:Combining local convolution with global self-attention for reading comprehension">
                                        <b>[4]</b>
                                        Yu A W,Dohan D,Luong M T,et al.QANet:Combining local convolution with global self-attention for reading comprehension[J].arXiv preprint arXiv:1804.09541,2018
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title="Silver D,Schrittwieser J,Simonyan K,et al.Mastering the game of Go without human knowledge[J].Nature,2017,550:354- 359" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mastering the game of Go without human knowledge">
                                        <b>[5]</b>
                                        Silver D,Schrittwieser J,Simonyan K,et al.Mastering the game of Go without human knowledge[J].Nature,2017,550:354- 359
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title="Stoica I,Song D,Popa R A,et al.A Berkeley view of systems challenges for AI[J].arXiv preprint arXiv:1712.05855,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Berkeley view of systems challenges for AI">
                                        <b>[6]</b>
                                        Stoica I,Song D,Popa R A,et al.A Berkeley view of systems challenges for AI[J].arXiv preprint arXiv:1712.05855,2017
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title="Jia Yangqing,Shelhamer E,Donahue J,et al.Caffe:Convolutional architecture for fast feature embedding[C] //Proc of the 22nd ACM Int Conf on Multimedia.New York:ACM,2014:675- 678" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">
                                        <b>[7]</b>
                                        Jia Yangqing,Shelhamer E,Donahue J,et al.Caffe:Convolutional architecture for fast feature embedding[C] //Proc of the 22nd ACM Int Conf on Multimedia.New York:ACM,2014:675- 678
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title="Abadi M,Barham P,Chen Jianmin,et al.Tensorflow:A system for large-scale machine learning[C] //Proc of the 12th USENIX Symp on Operating Systems Design and Implementation (OSDI&#39;16).Berkeley,CA:USENIX Association,2016:265- 283" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A System for Large-Scale Machine Learning">
                                        <b>[8]</b>
                                        Abadi M,Barham P,Chen Jianmin,et al.Tensorflow:A system for large-scale machine learning[C] //Proc of the 12th USENIX Symp on Operating Systems Design and Implementation (OSDI&#39;16).Berkeley,CA:USENIX Association,2016:265- 283
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title="Ronan,Cl&#233;ment,Koray,et al.Torch7[EB/OL].[2019-05-25].http://torch.ch/" target="_blank"
                                       href="">
                                        <b>[9]</b>
                                        Ronan,Cl&#233;ment,Koray,et al.Torch7[EB/OL].[2019-05-25].http://torch.ch/
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title="Chen Tianqi,Li Mu,Li Yutian,et al.MXNet:A flexible and efficient machine learning library for heterogeneous distributed systems[J].arXiv preprint arXiv:1512.01274,2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MXNet:A flexible and efficient machine learning library for heterogeneous distributed systems">
                                        <b>[10]</b>
                                        Chen Tianqi,Li Mu,Li Yutian,et al.MXNet:A flexible and efficient machine learning library for heterogeneous distributed systems[J].arXiv preprint arXiv:1512.01274,2015
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title="PaddlePaddle developers.PaddlePaddle[EB/OL].[2019-05-25].https://github.com/paddlepaddle/paddle" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PaddlePaddle">
                                        <b>[11]</b>
                                        PaddlePaddle developers.PaddlePaddle[EB/OL].[2019-05-25].https://github.com/paddlepaddle/paddle
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title="Tencent.Tencent AI open platform[EB/OL].[2019-05-25].https://ai.qq.com/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tencent AI open platform">
                                        <b>[12]</b>
                                        Tencent.Tencent AI open platform[EB/OL].[2019-05-25].https://ai.qq.com/
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title="Alibaba.ET brain[EB/OL].[2019-05-25].https://et.aliyun.com/index" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ET brain">
                                        <b>[13]</b>
                                        Alibaba.ET brain[EB/OL].[2019-05-25].https://et.aliyun.com/index
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title="Baidu.Baidu AI open platform[EB/OL].[2019-05-25].http://ai.baidu.com/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Baidu AI open platform">
                                        <b>[14]</b>
                                        Baidu.Baidu AI open platform[EB/OL].[2019-05-25].http://ai.baidu.com/
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title="Google.AI and machine learning products [EB/OL].[2019-05-25].https://cloud.google.com/products/ai/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AI and machine learning products">
                                        <b>[15]</b>
                                        Google.AI and machine learning products [EB/OL].[2019-05-25].https://cloud.google.com/products/ai/
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title="Azure.Azure AI [EB/OL].[2019-05-25].https://azure.microsoft.com/en-us/overview/ai-platform/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Azure AI">
                                        <b>[16]</b>
                                        Azure.Azure AI [EB/OL].[2019-05-25].https://azure.microsoft.com/en-us/overview/ai-platform/
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title="Amazon.Amazon machine learning[EB/OL].[2019-05-25].https://aws.amazon.com/cn/machine-learning/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Amazon machine learning">
                                        <b>[17]</b>
                                        Amazon.Amazon machine learning[EB/OL].[2019-05-25].https://aws.amazon.com/cn/machine-learning/
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title="IBM.IBM watson[EB/OL].[2019-05-25].https://www.ibm.com/watson" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=IBM watson">
                                        <b>[18]</b>
                                        IBM.IBM watson[EB/OL].[2019-05-25].https://www.ibm.com/watson
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title="Wakabayashi D.Self-driving uber car kills pedestrian in Arizona,where robots roam[EB/OL].[2019-04-28].https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-Driving Uber Car Kills Pedestrian in Arizona,Where Robots Roam">
                                        <b>[19]</b>
                                        Wakabayashi D.Self-driving uber car kills pedestrian in Arizona,where robots roam[EB/OL].[2019-04-28].https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title="Wikipedia.Tay (bot)[EB/OL].[2019-04-29].https://en.wikipedia.org/wiki/Tay_(bot)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tay (bot)">
                                        <b>[20]</b>
                                        Wikipedia.Tay (bot)[EB/OL].[2019-04-29].https://en.wikipedia.org/wiki/Tay_(bot)
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title="Bojarski M,Testa D D,Dworakowski D,et al.End to end learning for self-driving cars[J].arXiv preprint arXiv:1604.07316,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End to end learning for self-driving cars">
                                        <b>[21]</b>
                                        Bojarski M,Testa D D,Dworakowski D,et al.End to end learning for self-driving cars[J].arXiv preprint arXiv:1604.07316,2016
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title="Wang Juanjuan,Qiao Ying,Wang Hongan.Graph-based auto-driving reasoning task scheduling[J].Journal of Computer Research and Development,2017,54(8):1693- 1702 (in Chinese)(王娟娟,乔颖,王宏安.基于图模型的自动驾驶推理任务调度[J].计算机研究与发展,2017,54(8):1693- 1702)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708006&amp;v=MDE0ODdlWmVSc0Z5emdWYnpCTHl2U2RMRzRIOWJNcDQ5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        Wang Juanjuan,Qiao Ying,Wang Hongan.Graph-based auto-driving reasoning task scheduling[J].Journal of Computer Research and Development,2017,54(8):1693- 1702 (in Chinese)(王娟娟,乔颖,王宏安.基于图模型的自动驾驶推理任务调度[J].计算机研究与发展,2017,54(8):1693- 1702)
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title="Yuan Zhenlong,Lu Yongqiang,Wang Zhaoguo,et al.Droid-Sec:Deep learning in android malware detection[C] //Proc of the 2014 ACM Conf on SIGCOMM.New York:ACM,2014:371- 372" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Droid-Sec: Deep Learning in Android Malware Detection">
                                        <b>[23]</b>
                                        Yuan Zhenlong,Lu Yongqiang,Wang Zhaoguo,et al.Droid-Sec:Deep learning in android malware detection[C] //Proc of the 2014 ACM Conf on SIGCOMM.New York:ACM,2014:371- 372
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title="Metz R.Using deep learning to make video surveillance smarter[EB/OL].2015 [2019-05-05].https://www.technologyreview.com/s/540396/using-deep-learning-to-make-video-surveillance-smarter/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using deep learning to make video surveillance smarter">
                                        <b>[24]</b>
                                        Metz R.Using deep learning to make video surveillance smarter[EB/OL].2015 [2019-05-05].https://www.technologyreview.com/s/540396/using-deep-learning-to-make-video-surveillance-smarter/
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" >
                                        <b>[25]</b>
                                    The State Council.New-generation artificial intelligence development plan[EB/OL].2017 [2019-05-01].http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm (in Chinese)(国务院.新一代人工智能发展规划[EB/OL].2017 [2019-05-01].http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm)</a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title="Papernot N,McDaniel P,Sinha A,et al.Towards the science of security and privacy in machine learning[J].arXiv preprint arXiv:1611.03814,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards the science of security and privacy in machine learning">
                                        <b>[26]</b>
                                        Papernot N,McDaniel P,Sinha A,et al.Towards the science of security and privacy in machine learning[J].arXiv preprint arXiv:1611.03814,2016
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title="Shin H,Son Y,Park Y,et al.Sampling race:Bypassing timing-based analog active sensor spoofing detection on analog-digital systems[C] //Proc of the 10th USENIX Workshop on Offensive Technologies (WOOT&#39;16).Berkeley,CA:USENIX Association,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sampling race:Bypassing timing-based analog active sensor spoofing detection on analog-digital systems">
                                        <b>[27]</b>
                                        Shin H,Son Y,Park Y,et al.Sampling race:Bypassing timing-based analog active sensor spoofing detection on analog-digital systems[C] //Proc of the 10th USENIX Workshop on Offensive Technologies (WOOT&#39;16).Berkeley,CA:USENIX Association,2016
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title="Zhang Guoming,Chen Yan,Ji Xiaoyu,et al.DolphinAttack:Inaudible voice commands[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;17).New York:ACM,2017:103- 117" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DolphinAttack:Inaudible voice commands">
                                        <b>[28]</b>
                                        Zhang Guoming,Chen Yan,Ji Xiaoyu,et al.DolphinAttack:Inaudible voice commands[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;17).New York:ACM,2017:103- 117
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" title="Roy N,Hassanieh H,Roy Choudhury R.BackDoor:Making microphones hear inaudible sounds[C] //Proc of the 15th Annual Int Conf on Mobile Systems,Applications,and Services.New York:ACM,2017:2- 14" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making microphones hear inaudible sounds">
                                        <b>[29]</b>
                                        Roy N,Hassanieh H,Roy Choudhury R.BackDoor:Making microphones hear inaudible sounds[C] //Proc of the 15th Annual Int Conf on Mobile Systems,Applications,and Services.New York:ACM,2017:2- 14
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_30" title="Dean R N,Castro S T,Flowers G T,et al.A characterization of the performance of a MEMS gyroscope in acoustically harsh environments[J].IEEE Transactions on Industrial Electronics,2010,58(7):2591- 2596" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Characterization of the Performance of a MEMS Gyroscope in Acoustically Harsh Environments">
                                        <b>[30]</b>
                                        Dean R N,Castro S T,Flowers G T,et al.A characterization of the performance of a MEMS gyroscope in acoustically harsh environments[J].IEEE Transactions on Industrial Electronics,2010,58(7):2591- 2596
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_31" title="Dean R N,Flowers G T,Hodel A S,et al.On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise[C] //Proc of the 2007 IEEE Int Symp on Industrial Electronics.Piscataway,NJ:IEEE,2007:1435- 1440" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise">
                                        <b>[31]</b>
                                        Dean R N,Flowers G T,Hodel A S,et al.On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise[C] //Proc of the 2007 IEEE Int Symp on Industrial Electronics.Piscataway,NJ:IEEE,2007:1435- 1440
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_32" title="Xiao Qixue,Chen Yufei,Shen Chao,et al.Seeing is not believing:Camouflage attacks on image scaling algorithms[C] //Proc of the 28th USENIX Security Symp (USENIX Security&#39;19).Berkeley,CA:USENIX Association,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Seeing is not believing:Camouflage attacks on image scaling algorithms">
                                        <b>[32]</b>
                                        Xiao Qixue,Chen Yufei,Shen Chao,et al.Seeing is not believing:Camouflage attacks on image scaling algorithms[C] //Proc of the 28th USENIX Security Symp (USENIX Security&#39;19).Berkeley,CA:USENIX Association,2019
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_33" title="Dalvi N,Domingos P,Sanghai S,et al.Adversarial classification[C] //Proc of the 10th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2004:99- 108" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial classification">
                                        <b>[33]</b>
                                        Dalvi N,Domingos P,Sanghai S,et al.Adversarial classification[C] //Proc of the 10th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2004:99- 108
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_34" title="Lowd D,Meek C.Adversarial learning[C] //Proc of the 11th ACM SIGKDD Int Conf on Knowledge Discovery in Data Mining.New York:ACM,2005:641- 647" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial learning">
                                        <b>[34]</b>
                                        Lowd D,Meek C.Adversarial learning[C] //Proc of the 11th ACM SIGKDD Int Conf on Knowledge Discovery in Data Mining.New York:ACM,2005:641- 647
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_35" title="Huang L,Joseph A D,Nelson B,et al.Adversarial machine learning[C] //Proc of the 4th ACM Workshop on Security and Artificial Intelligence.New York:ACM,2011:43- 58" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial machine learning">
                                        <b>[35]</b>
                                        Huang L,Joseph A D,Nelson B,et al.Adversarial machine learning[C] //Proc of the 4th ACM Workshop on Security and Artificial Intelligence.New York:ACM,2011:43- 58
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_36" title="Fredrikson M,Jha S,Ristenpart T.Model inversion attacks that exploit confidence information and basic countermeasures[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;15).New York:ACM,2015:1322- 1333" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model inversion attacks that exploit confidence information and basic countermeasures">
                                        <b>[36]</b>
                                        Fredrikson M,Jha S,Ristenpart T.Model inversion attacks that exploit confidence information and basic countermeasures[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;15).New York:ACM,2015:1322- 1333
                                    </a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_37" title="Shokri R,Stronati M,Song Congzheng,et al.Membership inference attacks against machine learning models[C] //Proc of the 2017 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;17).Piscataway,NJ:IEEE,2017:3- 18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Membership inference attacks against machine learning models">
                                        <b>[37]</b>
                                        Shokri R,Stronati M,Song Congzheng,et al.Membership inference attacks against machine learning models[C] //Proc of the 2017 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;17).Piscataway,NJ:IEEE,2017:3- 18
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_38" title="Tram&#232;r F,Zhang Fan,Juels A,et al.Stealing machine learning models via prediction APIs[C] //Proc of the 25th USENIX Security Symp (USENIX Security&#39;16).Berkeley,CA:USENIX Association,2016:601- 618" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stealing machine learning models via prediction APIs">
                                        <b>[38]</b>
                                        Tram&#232;r F,Zhang Fan,Juels A,et al.Stealing machine learning models via prediction APIs[C] //Proc of the 25th USENIX Security Symp (USENIX Security&#39;16).Berkeley,CA:USENIX Association,2016:601- 618
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_39" title="Yang Guolei,Gong N Zhenqiang,Cai Ying.Fake co-visitation injection attacks to recommender systems[C] //Proc of the 24th Annual Network and Distributed System Security Symp (NDSS 2017).Reston,VA,USA:The Internet Society,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fake co-visitation injection attacks to recommender systems">
                                        <b>[39]</b>
                                        Yang Guolei,Gong N Zhenqiang,Cai Ying.Fake co-visitation injection attacks to recommender systems[C] //Proc of the 24th Annual Network and Distributed System Security Symp (NDSS 2017).Reston,VA,USA:The Internet Society,2017
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_40" title="Munňoz-Gonz&#225;lez L,Biggio B,Demontis A,et al.Towards poisoning of deep learning algorithms with back-gradient optimization[C] //Proc of the 10th ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:27- 38" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards poisoning of deep learning algorithms with back-gradient optimization">
                                        <b>[40]</b>
                                        Munňoz-Gonz&#225;lez L,Biggio B,Demontis A,et al.Towards poisoning of deep learning algorithms with back-gradient optimization[C] //Proc of the 10th ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:27- 38
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_41" title="Jagielski M,Oprea A,Biggio B,et al.Manipulating machine learning:Poisoning attacks and countermeasures for regression learning[C] //Proc of the 2018 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;18).Piscataway,NJ:IEEE,2018:19- 35" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manipulating machine learning:Poisoning attacks and countermeasures for regression learning">
                                        <b>[41]</b>
                                        Jagielski M,Oprea A,Biggio B,et al.Manipulating machine learning:Poisoning attacks and countermeasures for regression learning[C] //Proc of the 2018 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;18).Piscataway,NJ:IEEE,2018:19- 35
                                    </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_42" title="Cretu G F,Stavrou A,Locasto M E,et al.Casting out demons:Sanitizing training data for anomaly sensors[C] //Proc of the 2008 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;08).Piscataway,NJ:IEEE,2008:81- 95" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Casting out demons:Sanitizing training data for anomaly sensors">
                                        <b>[42]</b>
                                        Cretu G F,Stavrou A,Locasto M E,et al.Casting out demons:Sanitizing training data for anomaly sensors[C] //Proc of the 2008 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;08).Piscataway,NJ:IEEE,2008:81- 95
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_43" title="Gu Tianyu,Dolan-Gavitt B,Garg S.BadNets:Identifying vulnerabilities in the machine learning model supply chain[J].arXiv preprint arXiv:1708.06733,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identifying vulnerabilities in the machine learning model supply chain">
                                        <b>[43]</b>
                                        Gu Tianyu,Dolan-Gavitt B,Garg S.BadNets:Identifying vulnerabilities in the machine learning model supply chain[J].arXiv preprint arXiv:1708.06733,2017
                                    </a>
                                </li>
                                <li id="89">


                                    <a id="bibliography_44" title="Liu Yingqi,Ma Shiqing,Aafer Y,et al.Trojaning attack on neural networks[C] //Proc of the 25th Annual Network and Distributed System Security Symp (NDSS 2018).Reston,VA,USA:The Internet Society,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trojaning attack on neural networks">
                                        <b>[44]</b>
                                        Liu Yingqi,Ma Shiqing,Aafer Y,et al.Trojaning attack on neural networks[C] //Proc of the 25th Annual Network and Distributed System Security Symp (NDSS 2018).Reston,VA,USA:The Internet Society,2018
                                    </a>
                                </li>
                                <li id="91">


                                    <a id="bibliography_45" title="Song Congzheng,Ristenpart T,Shmatikov V.Machine learning models that remember too much[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;17).New York:ACM,2017:587- 601" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning models that remember too much">
                                        <b>[45]</b>
                                        Song Congzheng,Ristenpart T,Shmatikov V.Machine learning models that remember too much[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;17).New York:ACM,2017:587- 601
                                    </a>
                                </li>
                                <li id="93">


                                    <a id="bibliography_46" title="Wang Bolun,Yao Yuanshun,Shan S,et al.Neural cleanse:Identifying and mitigating backdoor attacks in neural networks[C] //Proc of 2019 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;19).Piscataway,NJ:IEEE,2019:530- 546" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural cleanse:identifying and mitigating backdoor attacks in neural networks">
                                        <b>[46]</b>
                                        Wang Bolun,Yao Yuanshun,Shan S,et al.Neural cleanse:Identifying and mitigating backdoor attacks in neural networks[C] //Proc of 2019 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;19).Piscataway,NJ:IEEE,2019:530- 546
                                    </a>
                                </li>
                                <li id="95">


                                    <a id="bibliography_47" title="Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[J].arXiv preprint arXiv:1312.6199,2013" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks">
                                        <b>[47]</b>
                                        Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[J].arXiv preprint arXiv:1312.6199,2013
                                    </a>
                                </li>
                                <li id="97">


                                    <a id="bibliography_48" title="Papernot N,McDaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the 2016 IEEE European Symp on Security and Privacy (EuroS&amp;amp;P).Piscataway,NJ:IEEE,2016:372- 387" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">
                                        <b>[48]</b>
                                        Papernot N,McDaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the 2016 IEEE European Symp on Security and Privacy (EuroS&amp;amp;P).Piscataway,NJ:IEEE,2016:372- 387
                                    </a>
                                </li>
                                <li id="99">


                                    <a id="bibliography_49" title="Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[J].arXiv preprint arXiv:1412.6572,2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining and harnessing adversarial examples">
                                        <b>[49]</b>
                                        Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[J].arXiv preprint arXiv:1412.6572,2014
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_50" title="Moosavi-Dezfooli S M,Fawzi A,Frossard P.Deepfool:A simple and accurate method to fool deep neural networks[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition (CVPR).Piscataway,NJ:IEEE,2016:2574- 2582" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepfool:A simple and accurate method to fool deep neural networks">
                                        <b>[50]</b>
                                        Moosavi-Dezfooli S M,Fawzi A,Frossard P.Deepfool:A simple and accurate method to fool deep neural networks[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition (CVPR).Piscataway,NJ:IEEE,2016:2574- 2582
                                    </a>
                                </li>
                                <li id="103">


                                    <a id="bibliography_51" title="Papernot N,McDaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 2017 ACM on Asia Conf on Computer and Communications Security (ASIACCS&#39;17).New York:ACM,2017:506- 519" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Practical black-box attacks against machine learning">
                                        <b>[51]</b>
                                        Papernot N,McDaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 2017 ACM on Asia Conf on Computer and Communications Security (ASIACCS&#39;17).New York:ACM,2017:506- 519
                                    </a>
                                </li>
                                <li id="105">


                                    <a id="bibliography_52" title="Tram&#232;r F,Papernot N,Goodfellow I,et al.The space of transferable adversarial examples[J].arXiv preprint arXiv:1704.03453,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The space of transferable adversarial examples">
                                        <b>[52]</b>
                                        Tram&#232;r F,Papernot N,Goodfellow I,et al.The space of transferable adversarial examples[J].arXiv preprint arXiv:1704.03453,2017
                                    </a>
                                </li>
                                <li id="107">


                                    <a id="bibliography_53" title="Papernot N,Faghri F,Carlini N,et al.Technical report on the cleverhans v2.1.0 adversarial examples library[J].arXiv preprint arXiv:1610.00768,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Technical report on the cleverhans v2.1.0 adversarial examples library">
                                        <b>[53]</b>
                                        Papernot N,Faghri F,Carlini N,et al.Technical report on the cleverhans v2.1.0 adversarial examples library[J].arXiv preprint arXiv:1610.00768,2016
                                    </a>
                                </li>
                                <li id="109">


                                    <a id="bibliography_54" title="Kurakin A,Goodfellow I,Bengio S.Adversarial examples in the physical world[J].arXiv preprint arXiv:1607.02533,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial examples in the physical world">
                                        <b>[54]</b>
                                        Kurakin A,Goodfellow I,Bengio S.Adversarial examples in the physical world[J].arXiv preprint arXiv:1607.02533,2016
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_55" title="Athalye A,Engstrom L,Ilyas A,et al.Synthesizing robust adversarial examples[J].arXiv preprint arXiv:1707.07397,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synthesizing robust adversarial examples">
                                        <b>[55]</b>
                                        Athalye A,Engstrom L,Ilyas A,et al.Synthesizing robust adversarial examples[J].arXiv preprint arXiv:1707.07397,2017
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_56" title="Sharif M,Bhagavatula S,Bauer L,et al.Accessorize to a crime:Real and stealthy attacks on state-of-the-art face recognition[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;16).New York:ACM,2016:1528- 1540" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accessorize to a crime:real and stealthy attacks on state-of-the-art face recognition">
                                        <b>[56]</b>
                                        Sharif M,Bhagavatula S,Bauer L,et al.Accessorize to a crime:Real and stealthy attacks on state-of-the-art face recognition[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;16).New York:ACM,2016:1528- 1540
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_57" title="Eykholt K,Evtimov I,Fernandes E,et al.Robust physical-world attacks on deep learning visual classification[C] //Proc of the 2018 IEEE Conf on Computer Vision and Pattern Recognition (CVPR).Piscataway,NJ:IEEE,2018:1625- 1634" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust physical-world attacks on deep learning visual classification">
                                        <b>[57]</b>
                                        Eykholt K,Evtimov I,Fernandes E,et al.Robust physical-world attacks on deep learning visual classification[C] //Proc of the 2018 IEEE Conf on Computer Vision and Pattern Recognition (CVPR).Piscataway,NJ:IEEE,2018:1625- 1634
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_58" title="Kumar D,Paccagnella R,Murley P,et al.Skill squatting attacks on amazon alexa[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:33- 47" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Skill squatting attacks on amazon alexa">
                                        <b>[58]</b>
                                        Kumar D,Paccagnella R,Murley P,et al.Skill squatting attacks on amazon alexa[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:33- 47
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_59" title="Zhang Nan,Mi Xianghang,Feng Xuan,et al.Dangerous skills:Understanding and mitigating security risks of voice-controlled third-party functions on virtual personal assistant systems[C] //Proc of the 2019 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;19).Piscataway,NJ:IEEE,2019:263- 278" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dangerous skills Understanding and mitigating security risks of voice-controlled third-party functions on virtual personal assistant systems">
                                        <b>[59]</b>
                                        Zhang Nan,Mi Xianghang,Feng Xuan,et al.Dangerous skills:Understanding and mitigating security risks of voice-controlled third-party functions on virtual personal assistant systems[C] //Proc of the 2019 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;19).Piscataway,NJ:IEEE,2019:263- 278
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_60" title="Zhang Yangyong,Xu Lei,Mendoza A,et al.Life after speech recognition:Fuzzing semantic misinterpretation for voice assistant applications[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA:The Internet Society,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Life after speech recognition:Fuzzing semantic misinterpretation for voice assistant applications">
                                        <b>[60]</b>
                                        Zhang Yangyong,Xu Lei,Mendoza A,et al.Life after speech recognition:Fuzzing semantic misinterpretation for voice assistant applications[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA:The Internet Society,2019
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_61" title="Carlini N,Wagner D.Audio adversarial examples:Targeted attacks on speech-to-text[C] //Proc of the 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:1- 7" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Audio adversarial examples:Targeted attacks on speech-to-text">
                                        <b>[61]</b>
                                        Carlini N,Wagner D.Audio adversarial examples:Targeted attacks on speech-to-text[C] //Proc of the 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:1- 7
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_62" title="Yuan Xuejing,Chen Yuxuan,Zhao Yue,et al.CommanderSong:A systematic approach for practical adversarial voice recognition[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:49- 64" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CommanderSong:A systematic approach for practical adversarial voice recognition">
                                        <b>[62]</b>
                                        Yuan Xuejing,Chen Yuxuan,Zhao Yue,et al.CommanderSong:A systematic approach for practical adversarial voice recognition[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:49- 64
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_63" title="Vaidya T,Zhang Yuankai,Sherr M,et al.Cocaine noodles:Exploiting the gap between human and machine speech recognition[C] //Proc of the 9th USENIX Workshop on Offensive Technologies (WOOT&#39;15).Berkeley,CA:USENIX Association,2015" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cocaine noodles:Exploiting the gap between human and machine speech recognition">
                                        <b>[63]</b>
                                        Vaidya T,Zhang Yuankai,Sherr M,et al.Cocaine noodles:Exploiting the gap between human and machine speech recognition[C] //Proc of the 9th USENIX Workshop on Offensive Technologies (WOOT&#39;15).Berkeley,CA:USENIX Association,2015
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_64" title="Carlini N,Mishra P,Vaidya T,et al.Hidden voice commands[C] //Proc of the 25th USENIX Security Symp (USENIX Security&#39;16).Berkeley,CA:USENIX Association,2016:513- 530" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hidden voice commands">
                                        <b>[64]</b>
                                        Carlini N,Mishra P,Vaidya T,et al.Hidden voice commands[C] //Proc of the 25th USENIX Security Symp (USENIX Security&#39;16).Berkeley,CA:USENIX Association,2016:513- 530
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_65" title="Abdullah H,Garcia W,Peeters C,et al.Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA,USA:The Internet Society,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems">
                                        <b>[65]</b>
                                        Abdullah H,Garcia W,Peeters C,et al.Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA,USA:The Internet Society,2019
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_66" title="Papernot N,McDaniel P,Swami A,et al.Crafting adversarial input sequences for recurrent neural networks[C] //Proc of the 2016 IEEE Military Communications Conf (MILCOM 2016).Piscataway,NJ:IEEE,2016:49- 54" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crafting adversarial input sequences for recurrent neural networks">
                                        <b>[66]</b>
                                        Papernot N,McDaniel P,Swami A,et al.Crafting adversarial input sequences for recurrent neural networks[C] //Proc of the 2016 IEEE Military Communications Conf (MILCOM 2016).Piscataway,NJ:IEEE,2016:49- 54
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_67" title="Samanta S,Mehta S.Towards crafting text adversarial samples[J].arXiv preprint arXiv:1707.02812,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards crafting text adversarial samples">
                                        <b>[67]</b>
                                        Samanta S,Mehta S.Towards crafting text adversarial samples[J].arXiv preprint arXiv:1707.02812,2017
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_68" title="Ebrahimi J,Rao A,Lowd D,et al.Hotflip:White-box adversarial examples for text classification[J].arXiv preprint arXiv:1712.06751,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hotflip:White-box adversarial examples for text classification">
                                        <b>[68]</b>
                                        Ebrahimi J,Rao A,Lowd D,et al.Hotflip:White-box adversarial examples for text classification[J].arXiv preprint arXiv:1712.06751,2017
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_69" title="Alzantot M,Sharma Y,Elgohary A,et al.Generating natural language adversarial examples[J].arXiv preprint arXiv:1804.07998,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating natural language adversarial examples">
                                        <b>[69]</b>
                                        Alzantot M,Sharma Y,Elgohary A,et al.Generating natural language adversarial examples[J].arXiv preprint arXiv:1804.07998,2018
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_70" title="Belinkov Y,Bisk Y.Synthetic and natural noise both break neural machine translation[J].arXiv preprint arXiv:1711.02173,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synthetic and natural noise both break neural machine translation">
                                        <b>[70]</b>
                                        Belinkov Y,Bisk Y.Synthetic and natural noise both break neural machine translation[J].arXiv preprint arXiv:1711.02173,2017
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_71" title="Gao Ji,Lanchantin J,Soffa M L,et al.Black-box generation of adversarial text sequences to evade deep learning classifiers[C] //Proc of 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:50- 56" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Black-box generation of adversarial text sequences to evade deep learning classifiers">
                                        <b>[71]</b>
                                        Gao Ji,Lanchantin J,Soffa M L,et al.Black-box generation of adversarial text sequences to evade deep learning classifiers[C] //Proc of 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:50- 56
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_72" title="Hosseini H,Kannan S,Zhang Baosen,et al.Deceiving Google&#39;s perspective API built for detecting toxic comments[J].arXiv preprint arXiv:1702.08138,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deceiving Google&amp;#39;&amp;#39;s perspective API built for detecting toxic comments">
                                        <b>[72]</b>
                                        Hosseini H,Kannan S,Zhang Baosen,et al.Deceiving Google&#39;s perspective API built for detecting toxic comments[J].arXiv preprint arXiv:1702.08138,2017
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_73" title="Zhao Zhengli,Dua D,Singh S.Generating natural adversarial examples[J].arXiv preprint arXiv:1710.11342,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating natural adversarial examples">
                                        <b>[73]</b>
                                        Zhao Zhengli,Dua D,Singh S.Generating natural adversarial examples[J].arXiv preprint arXiv:1710.11342,2017
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_74" title="Xu Weilin,Qi Yanjun,Evans D.Automatically Evading Classifiers[C] //Proc of the 23rd Annual Network and Distributed System Security Symp (NDSS 2016).Reston,VA,USA:The Internet Society,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatically Evading Classifiers">
                                        <b>[74]</b>
                                        Xu Weilin,Qi Yanjun,Evans D.Automatically Evading Classifiers[C] //Proc of the 23rd Annual Network and Distributed System Security Symp (NDSS 2016).Reston,VA,USA:The Internet Society,2016
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_75" title="Grosse K,Papernot N,Manoharan P,et al.Adversarial examples for malware detection[C] //Proc of the 2017 European Symp on Research in Computer Security.Berlin:Springer,2017:62- 79" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial Examples for Malware Detection">
                                        <b>[75]</b>
                                        Grosse K,Papernot N,Manoharan P,et al.Adversarial examples for malware detection[C] //Proc of the 2017 European Symp on Research in Computer Security.Berlin:Springer,2017:62- 79
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_76" title="Papernot N,McDaniel P,Sinha A,et al.SoK:Security and privacy in machine learning[C] //Proc of the 2018 IEEE European Symp on Security and Privacy (EuroS&amp;amp;P).Piscataway,NJ:IEEE,2018:399- 414" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SoK:Security and privacy in machine learning">
                                        <b>[76]</b>
                                        Papernot N,McDaniel P,Sinha A,et al.SoK:Security and privacy in machine learning[C] //Proc of the 2018 IEEE European Symp on Security and Privacy (EuroS&amp;amp;P).Piscataway,NJ:IEEE,2018:399- 414
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_77" title="Xu Weilin,Evans D,Qi Yanjun.Feature squeezing:Detecting adversarial examples in deep neural networks[J].arXiv preprint arXiv:1704.01155,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature squeezing:Detecting adversarial examples in deep neural networks">
                                        <b>[77]</b>
                                        Xu Weilin,Evans D,Qi Yanjun.Feature squeezing:Detecting adversarial examples in deep neural networks[J].arXiv preprint arXiv:1704.01155,2017
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_78" title="Dziugaite G K,Ghahramani Z,Roy D M.A study of the effect of jpg compression on adversarial images[J].arXiv preprint arXiv:1608.00853,2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A study of the effect of jpg compression on adversarial images">
                                        <b>[78]</b>
                                        Dziugaite G K,Ghahramani Z,Roy D M.A study of the effect of jpg compression on adversarial images[J].arXiv preprint arXiv:1608.00853,2016
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_79" title="Goodfellow I,McDaniel P,Papernot N.Making machine learning robust against adversarial inputs[J].Communications of the ACM,2018,61(7):56- 66" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM4954B27195E7F927ED86A0315058A3C0&amp;v=MDY4MzVtbmpkN09YL2hyUmMxZkxybFJzbWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOcGh3TDI1dzZFPU5pZklZN2V4RzlXK3JZaEViZTU2Q3dvd3pSRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[79]</b>
                                        Goodfellow I,McDaniel P,Papernot N.Making machine learning robust against adversarial inputs[J].Communications of the ACM,2018,61(7):56- 66
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_80" title="Carlini N,Athalye A,Papernot N,et al.On evaluating adversarial robustness[J].arXiv preprint arXiv:1902.06705,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On evaluating adversarial robustness">
                                        <b>[80]</b>
                                        Carlini N,Athalye A,Papernot N,et al.On evaluating adversarial robustness[J].arXiv preprint arXiv:1902.06705,2019
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_81" title="Herley C,Van Oorschot P C.Sok:Science,security and the elusive goal of security as a scientific pursuit[C] //Proc of the 2017 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;17).Piscataway,NJ:IEEE,2017:99- 120" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Science,security and the elusive goal of security as a scientific pursuit">
                                        <b>[81]</b>
                                        Herley C,Van Oorschot P C.Sok:Science,security and the elusive goal of security as a scientific pursuit[C] //Proc of the 2017 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;17).Piscataway,NJ:IEEE,2017:99- 120
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_82" title="Carlini N,Wagner D.Adversarial examples are not easily detected:Bypassing ten detection methods[C] //Proc of the 10th ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:3- 14" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial examples are not easily detected:Bypassing ten detection methods">
                                        <b>[82]</b>
                                        Carlini N,Wagner D.Adversarial examples are not easily detected:Bypassing ten detection methods[C] //Proc of the 10th ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:3- 14
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_83" title="Athalye A,Carlini N,Wagner D.Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples[J].arXiv preprint arXiv:1802.00420,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples">
                                        <b>[83]</b>
                                        Athalye A,Carlini N,Wagner D.Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples[J].arXiv preprint arXiv:1802.00420,2018
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_84" title="Lecuyer M,Atlidakis V,Geambasu R,et al.Certified robustness to adversarial examples with differential privacy[J].arXiv preprint arXiv:1802.03471,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Certified robustness to adversarial examples with differential privacy">
                                        <b>[84]</b>
                                        Lecuyer M,Atlidakis V,Geambasu R,et al.Certified robustness to adversarial examples with differential privacy[J].arXiv preprint arXiv:1802.03471,2018
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_85" title="Raghunathan A,Steinhardt J,Liang P.Certified defenses against adversarial examples[J].arXiv preprint arXiv:1801.09344,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Certified defenses against adversarial examples">
                                        <b>[85]</b>
                                        Raghunathan A,Steinhardt J,Liang P.Certified defenses against adversarial examples[J].arXiv preprint arXiv:1801.09344,2018
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_86" title="Wong E,Kolter J Z.Provable defenses against adversarial examples via the convex outer adversarial polytope[J].arXiv preprint arXiv:1711.00851,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Provable defenses against adversarial examples via the convex outer adversarial polytope">
                                        <b>[86]</b>
                                        Wong E,Kolter J Z.Provable defenses against adversarial examples via the convex outer adversarial polytope[J].arXiv preprint arXiv:1711.00851,2017
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_87" title="Tjeng V,Xiao Kai Y,Tedrake R.Evaluating robustness of neural networks with mixed integer programming[J].arXiv preprint arXiv:1711.07356,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating robustness of neural networks with mixed integer programming">
                                        <b>[87]</b>
                                        Tjeng V,Xiao Kai Y,Tedrake R.Evaluating robustness of neural networks with mixed integer programming[J].arXiv preprint arXiv:1711.07356,2017
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_88" title="Xiao Kai Y,Tjeng V,Shafiullah N M,et al.Training for faster adversarial robustness verification via inducing relu stability[J].arXiv preprint arXiv:1809.03008,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training for faster adversarial robustness verification via inducing relu stability">
                                        <b>[88]</b>
                                        Xiao Kai Y,Tjeng V,Shafiullah N M,et al.Training for faster adversarial robustness verification via inducing relu stability[J].arXiv preprint arXiv:1809.03008,2018
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_89" title="Fredrikson M,Lantz E,Jha S,et al.Privacy in pharmacogenetics:An end-to-end case study of personalized warfarin dosing[C] //Proc of the 23rd USENIX Security Symp (USENIX Security&#39;14).Berkeley,CA:USENIX Association,2014:17- 32" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy in pharmacogenetics:an end-to-end case study of personalized warfarin dosing">
                                        <b>[89]</b>
                                        Fredrikson M,Lantz E,Jha S,et al.Privacy in pharmacogenetics:An end-to-end case study of personalized warfarin dosing[C] //Proc of the 23rd USENIX Security Symp (USENIX Security&#39;14).Berkeley,CA:USENIX Association,2014:17- 32
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_90" title="Homer N,Szelinger S,Redman M,et al.Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays[J].PLoS Genetics,2008,4(8):e1000167" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJLA&amp;filename=SJLAFE7FADDE2C75FB5A58897FF9E2B947ED&amp;v=MTYyOTBYTkdhZTkyL3N3WnBnSUNRcEx5bWNXNGpkMFR3bVVwV2MzQzd1UVFzL3JDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOcGh3TDI1dzZFPU5pZkhiOA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[90]</b>
                                        Homer N,Szelinger S,Redman M,et al.Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays[J].PLoS Genetics,2008,4(8):e1000167
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_91" title="Salem A,Zhang Yang,Humbert M,et al.ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA,USA:The Internet Society,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models">
                                        <b>[91]</b>
                                        Salem A,Zhang Yang,Humbert M,et al.ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA,USA:The Internet Society,2019
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_92" title="Pyrgelis A,Troncoso C,De Cristofaro E.Knock knock,who&#39;s there?Membership inference on aggregate location data[C] //Proc of the 25th Annual Network and Distributed System Security Symp (NDSS 2018).Reston,VA,USA:The Internet Society,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Knock knock who&amp;#39;&amp;#39;s there?Membership inference on aggregate location data">
                                        <b>[92]</b>
                                        Pyrgelis A,Troncoso C,De Cristofaro E.Knock knock,who&#39;s there?Membership inference on aggregate location data[C] //Proc of the 25th Annual Network and Distributed System Security Symp (NDSS 2018).Reston,VA,USA:The Internet Society,2018
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_93" title="Hayes J,Melis L,Danezis G,et al.LOGAN:Membership inference attacks against generative models[J].arXiv preprint arXiv:1705.07663,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LOGAN:Membership inference attacks against generative models">
                                        <b>[93]</b>
                                        Hayes J,Melis L,Danezis G,et al.LOGAN:Membership inference attacks against generative models[J].arXiv preprint arXiv:1705.07663,2017
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_94" title="Salem A,Bhattacharya A,Backes M,et al.Updates-Leak:Data set inference and reconstruction attacks in online learning[J].arXiv preprint arXiv:1904.01067,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Updates-Leak:Data set inference and reconstruction attacks in online learning">
                                        <b>[94]</b>
                                        Salem A,Bhattacharya A,Backes M,et al.Updates-Leak:Data set inference and reconstruction attacks in online learning[J].arXiv preprint arXiv:1904.01067,2019
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_95" title="Carlini N,Liu Chang,Erlingsson &#218;,et al.The secret sharer:Evaluating and testing unintended memorization in neural networks[C] //Proc of the 28th USENIX Security Symp (USENIX Security&#39;19).Berkeley,CA:USENIX Association,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The secret sharer:Evaluating and testing unintended memorization in neural networks">
                                        <b>[95]</b>
                                        Carlini N,Liu Chang,Erlingsson &#218;,et al.The secret sharer:Evaluating and testing unintended memorization in neural networks[C] //Proc of the 28th USENIX Security Symp (USENIX Security&#39;19).Berkeley,CA:USENIX Association,2019
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_96" title="Dwork C,Roth A.The algorithmic foundations of differential privacy[J].Foundations and Trends in Theoretical Computer Science,2014,9(3/4):211- 407" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Algorithmic Foundations of Differential Privacy">
                                        <b>[96]</b>
                                        Dwork C,Roth A.The algorithmic foundations of differential privacy[J].Foundations and Trends in Theoretical Computer Science,2014,9(3/4):211- 407
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_97" title="Chaudhuri K,Monteleoni C,Sarwate A D.Differentially private empirical risk minimization[J].Journal of Machine Learning Research,2011,12(Mar):1069- 1109" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Differentially private empirical risk minimization">
                                        <b>[97]</b>
                                        Chaudhuri K,Monteleoni C,Sarwate A D.Differentially private empirical risk minimization[J].Journal of Machine Learning Research,2011,12(Mar):1069- 1109
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_98" title="Abadi M,Chu A,Goodfellow I,et al.Deep learning with differential privacy[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;16).New York:ACM,2016:308- 318" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning with differential privacy">
                                        <b>[98]</b>
                                        Abadi M,Chu A,Goodfellow I,et al.Deep learning with differential privacy[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;16).New York:ACM,2016:308- 318
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_99" title="Shokri R,Shmatikov V.Privacy-preserving deep learning[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;15).New York:ACM,2015:1310- 1321" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy-Preserving Deep Learning">
                                        <b>[99]</b>
                                        Shokri R,Shmatikov V.Privacy-preserving deep learning[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security (CCS&#39;15).New York:ACM,2015:1310- 1321
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_100" title="Gilad-Bachrach R,Dowlin N,Laine K,et al.CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy[C] //Proc of the 33rd Int Conf on Machine Learning (ICML&#39;16).New York:ACM,2016:201- 210" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cryptonets:Applying neural networks to encrypted data with high throughput and accuracy">
                                        <b>[100]</b>
                                        Gilad-Bachrach R,Dowlin N,Laine K,et al.CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy[C] //Proc of the 33rd Int Conf on Machine Learning (ICML&#39;16).New York:ACM,2016:201- 210
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_101" title="Geyer R C,Klein T,Nabi M.Differentially private federated learning:A client level perspective[J].arXiv preprint arXiv:1712.07557,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Differentially private federated learning:A client level perspective">
                                        <b>[101]</b>
                                        Geyer R C,Klein T,Nabi M.Differentially private federated learning:A client level perspective[J].arXiv preprint arXiv:1712.07557,2017
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_102" title="Wang Binghui,Gong N Zhenqiang.Stealing hyperparameters in machine learning[C] //Proc of the 2018 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;18).Piscataway,NJ:IEEE,2018:36- 52" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stealing hyperparameters in machine learning">
                                        <b>[102]</b>
                                        Wang Binghui,Gong N Zhenqiang.Stealing hyperparameters in machine learning[C] //Proc of the 2018 IEEE Symp on Security and Privacy (S&amp;amp;P&#39;18).Piscataway,NJ:IEEE,2018:36- 52
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_103" title="Venugopal A,Uszkoreit J,Talbot D,et al.Watermarking the outputs of structured prediction with an application in statistical machine translation[C] //Proc of the 2011 Conf on Empirical Methods in Natural Language Processing (EMNLP).Stroudsburg,PA:ACL,2011:1363- 1372" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Watermarking the outputs of structured prediction with an application in statistical machine translation">
                                        <b>[103]</b>
                                        Venugopal A,Uszkoreit J,Talbot D,et al.Watermarking the outputs of structured prediction with an application in statistical machine translation[C] //Proc of the 2011 Conf on Empirical Methods in Natural Language Processing (EMNLP).Stroudsburg,PA:ACL,2011:1363- 1372
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_104" title="Uchida Y,Nagai Y,Sakazawa S,et al.Embedding watermarks into deep neural networks[C] //Proc of the 2017 ACM on Int Conf on Multimedia Retrieval.New York:ACM,2017:269- 277" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Embedding watermarks into deep neural networks">
                                        <b>[104]</b>
                                        Uchida Y,Nagai Y,Sakazawa S,et al.Embedding watermarks into deep neural networks[C] //Proc of the 2017 ACM on Int Conf on Multimedia Retrieval.New York:ACM,2017:269- 277
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_105" title="Chen Huili,Rohani B D,Koushanfar F.DeepMarks:A digital fingerprinting framework for deep neural networks[J].arXiv preprint arXiv:1804.03648,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A digital fingerprinting framework for deep neural networks">
                                        <b>[105]</b>
                                        Chen Huili,Rohani B D,Koushanfar F.DeepMarks:A digital fingerprinting framework for deep neural networks[J].arXiv preprint arXiv:1804.03648,2018
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_106" title="Merrer E L,Perez P,Tr&#233;dan G.Adversarial frontier stitching for remote neural network watermarking[J].arXiv preprint arXiv:1711.01894,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial frontier stitching for remote neural network watermarking">
                                        <b>[106]</b>
                                        Merrer E L,Perez P,Tr&#233;dan G.Adversarial frontier stitching for remote neural network watermarking[J].arXiv preprint arXiv:1711.01894,2017
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_107" title="Hosseini H,Chen Yize,Kannan S,et al.Blocking transferability of adversarial examples in black-box learning systems[J].arXiv preprint arXiv:1703.04318,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blocking transferability of adversarial examples in black-box learning systems">
                                        <b>[107]</b>
                                        Hosseini H,Chen Yize,Kannan S,et al.Blocking transferability of adversarial examples in black-box learning systems[J].arXiv preprint arXiv:1703.04318,2017
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_108" title="Adi Y,Baum C,Cisse M,et al.Turning your weakness into a strength:Watermarking deep neural networks by backdooring[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:1615- 1631" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Turning your weakness into a strength:Watermarking deep neural networks by backdooring">
                                        <b>[108]</b>
                                        Adi Y,Baum C,Cisse M,et al.Turning your weakness into a strength:Watermarking deep neural networks by backdooring[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:1615- 1631
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_109" title="Elsayed G F,Goodfellow I,Sohl-Dickstein J.Adversarial reprogramming of neural networks[J].arXiv preprint arXiv:1806.11146,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial reprogramming of neural networks">
                                        <b>[109]</b>
                                        Elsayed G F,Goodfellow I,Sohl-Dickstein J.Adversarial reprogramming of neural networks[J].arXiv preprint arXiv:1806.11146,2018
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_110" title="Xiao Qixue,Li Kang,Zhang Deyue,et al.Security risks in deep learning implementations[C] //Proc of the 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:123- 128" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Security risks in deep learning implementations">
                                        <b>[110]</b>
                                        Xiao Qixue,Li Kang,Zhang Deyue,et al.Security risks in deep learning implementations[C] //Proc of the 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:123- 128
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_111" title="Pei Kexin,Cao Yinzhi,Yang Junfeng,et al.DeepXplore:Automated whitebox testing of deep learning systems[C] //Proc of the 26th Symp on Operating Systems Principles (SOSP&#39;17).New York:ACM,2017:1- 18" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepxplore:Automated whitebox testing of deep learning systems">
                                        <b>[111]</b>
                                        Pei Kexin,Cao Yinzhi,Yang Junfeng,et al.DeepXplore:Automated whitebox testing of deep learning systems[C] //Proc of the 26th Symp on Operating Systems Principles (SOSP&#39;17).New York:ACM,2017:1- 18
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_112" title="Ma Lei,Juefei-Xu F,Zhang Fuyuan,et al.DeepGauge:Multi-granularity testing criteria for deep learning systems[C] //Proc of the 33rd ACM/IEEE Int Conf on Automated Software Engineering.New York:ACM,2018:120- 131" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepGauge:Multi-granularity testing criteria for deep learning systems">
                                        <b>[112]</b>
                                        Ma Lei,Juefei-Xu F,Zhang Fuyuan,et al.DeepGauge:Multi-granularity testing criteria for deep learning systems[C] //Proc of the 33rd ACM/IEEE Int Conf on Automated Software Engineering.New York:ACM,2018:120- 131
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_113" title="Sun Youcheng,Huang Xiaowei,Kroening D.Testing deep neural networks[J].arXiv preprint arXiv:1803.04792,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Testing deep neural networks">
                                        <b>[113]</b>
                                        Sun Youcheng,Huang Xiaowei,Kroening D.Testing deep neural networks[J].arXiv preprint arXiv:1803.04792,2018
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_114" title="Ma Lei,Zhang Fuyuan,Xue Minhui,et al.Combinatorial testing for deep learning systems[J].arXiv preprint arXiv:1806.07723,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combinatorial testing for deep learning systems">
                                        <b>[114]</b>
                                        Ma Lei,Zhang Fuyuan,Xue Minhui,et al.Combinatorial testing for deep learning systems[J].arXiv preprint arXiv:1806.07723,2018
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_115" title="Ma Lei,Zhang Fuyuan,Sun Jiyuan,et al.DeepMutation:Mutation testing of deep learning systems[C] //Proc of the 2018 IEEE 29th Int Symp on Software Reliability Engineering (ISSRE 2018).Piscataway,NJ:IEEE,2018:100- 111" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepMutation:Mutation testing of deep learning systems">
                                        <b>[115]</b>
                                        Ma Lei,Zhang Fuyuan,Sun Jiyuan,et al.DeepMutation:Mutation testing of deep learning systems[C] //Proc of the 2018 IEEE 29th Int Symp on Software Reliability Engineering (ISSRE 2018).Piscataway,NJ:IEEE,2018:100- 111
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_116" title="Odena A,Goodfellow I.Tensorfuzz:Debugging neural networks with coverage-guided fuzzing[J].arXiv preprint arXiv:1807.10875,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensorfuzz:Debugging neural networks with coverage-guided fuzzing">
                                        <b>[116]</b>
                                        Odena A,Goodfellow I.Tensorfuzz:Debugging neural networks with coverage-guided fuzzing[J].arXiv preprint arXiv:1807.10875,2018
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_117" title="Wang Shiqi,Pei Kexin,Whitehouse J,et al.Formal security analysis of neural networks using symbolic intervals[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:1599- 1614" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Formal security analysis of neural networks using symbolic intervals">
                                        <b>[117]</b>
                                        Wang Shiqi,Pei Kexin,Whitehouse J,et al.Formal security analysis of neural networks using symbolic intervals[C] //Proc of the 27th USENIX Security Symp (USENIX Security&#39;18).Berkeley,CA:USENIX Association,2018:1599- 1614
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_118" >
                                        <b>[118]</b>
                                    Diao Wenrui,Liu Xiangyu,Zhou Zhe,et al.Your voice assistant is mine:How to abuse speakers to steal information and control your phone[C] //Proc of the 4th ACM Workshop on Security and Privacy in Smartphones &amp;amp; Mobile Devices.New York:ACM,2014:63- 74&lt;image href=&quot;images/JFYZ201910008_239.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Chen Yufei,born in 1994.PhD candidate.His main research interests include security of intelligent systems and behavioral analysis.&lt;image href=&quot;images/JFYZ201910008_240.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Shen Chao,born in 1985.PhD,professor,PhD supervisor.Member of CCF.His main research interests include cyber-physical system optimization and security,network and system security,and artificial intelligence security.&lt;image href=&quot;images/JFYZ201910008_241.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Wang Qian,born in 1980.PhD,professor,PhD supervisor.Member of CCF.His main research interests include AI security,data storage,search and computation outsourcing security and privacy,wireless systems security,big data security and privacy,and applied cryptography etc.&lt;image href=&quot;images/JFYZ201910008_242.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Li Qi,born in 1979.PhD,associate professor,PhD supervisor.Senior member of CCF.His main research interests include network and system security,particularly in Internet security,mobile security,and big data security.&lt;image href=&quot;images/JFYZ201910008_243.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Wang Cong,born in 1982.PhD,associate professor,PhD supervisor.His main research interests include data and computation outsourcing security in the context of cloud computing,blockchain and decentralized application,network security in emerging Internet architecture,multimedia security,and privacy-enhancing technologies in the context of big data and IoT.&lt;image href=&quot;images/JFYZ201910008_244.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Ji Shouling,born in 1986.PhD,professor,PhD supervisor.Member of CCF.His main research interests include AI security,data-driven security,software and system security,and data analytics.&lt;image href=&quot;images/JFYZ201910008_245.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Li Kang,born in 1973.PhD,professor,PhD supervisor.His main research interests include computer network and operating systems,especially system issues related to data security and privacy.&lt;image href=&quot;images/JFYZ201910008_246.jpg&quot; type=&quot;&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;Guan Xiaohong,born in 1955.PhD,professor,PhD supervisor.His main research interests include allocation and scheduling of complex networked resources,network security,and sensor networks.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-10-28 09:51</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(10),2135-2150 DOI:10.7544/issn1000-1239.2019.20190415            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>人工智能系统安全与隐私风险</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%AE%87%E9%A3%9E&amp;code=33516259&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈宇飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B2%88%E8%B6%85&amp;code=14179374&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">沈超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%AA%9E&amp;code=22664371&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王骞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%90%A6&amp;code=08170391&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李琦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%81%AA&amp;code=36518895&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王聪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BA%AA%E5%AE%88%E9%A2%86&amp;code=39798184&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">纪守领</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BA%B7&amp;code=43050187&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李康</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AE%A1%E6%99%93%E5%AE%8F&amp;code=10177219&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">管晓宏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%99%BA%E8%83%BD%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6)&amp;code=0189085&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">智能网络与网络安全教育部重点实验室(西安交通大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%83%A8&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安交通大学电子与信息学部</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E5%AD%A6%E9%99%A2&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学网络安全学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E7%BD%91%E7%BB%9C%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=0088683&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学网络科学与网络空间研究院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%A6%99%E6%B8%AF%E5%9F%8E%E5%B8%82%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E7%B3%BB&amp;code=0157820&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">香港城市大学计算机科学系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E5%AE%89%E5%85%A8%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0172914&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江大学网络空间安全研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B9%94%E6%B2%BB%E4%BA%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E7%B3%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">乔治亚大学计算机科学系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>人类正在经历着由深度学习技术推动的人工智能浪潮,它为人类生产和生活带来了巨大的技术革新.在某些特定领域中,人工智能已经表现出达到甚至超越人类的工作能力.然而,以往的机器学习理论大多没有考虑开放甚至对抗的系统运行环境,人工智能系统的安全和隐私问题正逐渐暴露出来.通过回顾人工智能系统安全方面的相关研究工作,揭示人工智能系统中潜藏的安全与隐私风险.首先介绍了包含攻击面、攻击能力和攻击目标的安全威胁模型.从人工智能系统的4个关键环节——数据输入(传感器)、数据预处理、机器学习模型和输出,分析了相应的安全隐私风险及对策.讨论了未来在人工智能系统安全研究方面的发展趋势.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">智能系统安全;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">系统安全;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人工智能;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Chen Yufei,born in 1994.PhD candidate.His main research interests include securityof intelligent systems and behavioral analysis.yfchen@sei.xjtu.edu.cn&lt;image id="470" type="formula" href="images/JFYZ201910008_47000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *Shen Chao,born in 1985.PhD,professor,PhD supervisor.Member of CCF.Hismain research interests include cyber-physical system optimization and security,network and system security,and artificialintelligence security.chaoshen@mail.xjtu.edu.cn&lt;image id="472" type="formula" href="images/JFYZ201910008_47200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Wang Qian,born in 1980.PhD,professor,PhD supervisor.Member of CCF.His mainresearch interests include AI security,datastorage,search and computation outsourcingsecurity and privacy, wireless systemssecurity,big data security and privacy,and applied cryptography etc.&lt;image id="474" type="formula" href="images/JFYZ201910008_47400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Li Qi, born in 1979.PhD, associateprofessor,PhD supervisor.Senior memberof CCF. His main research interestsinclude network and system security,particularly in Internet security, mobilesecurity,and big data security.&lt;image id="476" type="formula" href="images/JFYZ201910008_47600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Wang Cong,born in 1982.PhD,associateprofessor, PhD supervisor. His mainresearch interests include data andcomputation outsourcing security in thecontext of cloud computing,blockchainand decentralized application, networksecurity in emerging Internet architecture,multimedia security, and privacy-enhancing technologies in the context ofbig data and IoT.&lt;image id="478" type="formula" href="images/JFYZ201910008_47800.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Ji Shouling,born in 1986.PhD,professor,PhD supervisor.Member of CCF.Hismain research interests include AIsecurity,data-driven security,softwareand system security,and data analytics.&lt;image id="480" type="formula" href="images/JFYZ201910008_48000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Li Kang,born in 1973.PhD,professor,PhD supervisor. His main researchinterests include computer network andoperating systems, especially systemissues related to data security and privacy.&lt;image id="482" type="formula" href="images/JFYZ201910008_48200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    Guan Xiaohong, born in 1955.PhD,professor, PhD supervisor. His mainresearch interests include allocation andscheduling of complex networked resources,network security,and sensor networks.&lt;image id="484" type="formula" href="images/JFYZ201910008_48400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61822309,61773310,U1736205);</span>
                    </p>
            </div>
                    <h1><b>Security and Privacy Risks in Artificial Intelligence Systems</b></h1>
                    <h2>
                    <span>Chen Yufei</span>
                    <span>Shen Chao</span>
                    <span>Wang Qian</span>
                    <span>Li Qi</span>
                    <span>Wang Cong</span>
                    <span>Ji Shouling</span>
                    <span>Li Kang</span>
                    <span>Guan Xiaohong</span>
            </h2>
                    <h2>
                    <span>Key Laboratory for Intelligent Networks and Network Security(Xi&apos;an Jiaotong University), Ministry of Education</span>
                    <span>Faculty of Electronic and Information Engineering, Xi&apos;an Jiaotong University</span>
                    <span>School of Cyber Science and Engineering, Wuhan University</span>
                    <span>Institute for Network Sciences and Cyberspace, Tsinghua University</span>
                    <span>Department of Computer Science, City University of Hong Kong</span>
                    <span>Institute of Cyberspace Research, Zhejiang University</span>
                    <span>College of Computer Science and Technology, Zhejiang University</span>
                    <span>Department of Computer Science, University of Georgia</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Human society is witnessing a wave of artificial intelligence(AI) driven by deep learning techniques, bringing a technological revolution for human production and life. In some specific fields, AI has achieved or even surpassed human-level performance. However, most previous machine learning theories have not considered the open and even adversarial environments, and the security and privacy issues are gradually rising. Besides of insecure code implementations, biased models, adversarial examples, sensor spoofing can also lead to security risks which are hard to be discovered by traditional security analysis tools. This paper reviews previous works on AI system security and privacy, revealing potential security and privacy risks. Firstly, we introduce a threat model of AI systems, including attack surfaces, attack capabilities and attack goals. Secondly, we analyze security risks and counter measures in terms of four critical components in AI systems: data input(sensor), data preprocessing, machine learning model and output. Finally, we discuss future research trends on the security of AI systems. The aim of this paper is to arise the attention of the computer security society and the AI society on security and privacy of AI systems, and so that they can work together to unlock AI's potential to build a bright future.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=intelligent%20system%20security&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">intelligent system security;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=system%20security&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">system security;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=artificial%20intelligence%20(AI)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">artificial intelligence (AI);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-06-12</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China(61822309,61773310,U1736205);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="247">近年来人工智能技术,尤其是深度学习理论方法,取得了重大突破.在计算机视觉<citation id="336" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、语音识别<citation id="352" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>、自然语言处理<citation id="337" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、棋牌博弈<citation id="338" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等多类任务上,人工智能技术的判断准确水平和决策能力已经追平甚至超越人类.人工智能技术已经“走出实验室,跨入工业界”<citation id="339" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,迅速触及到人类生产和生活的方方面面.与此同时,人工智能技术开发日趋大众化.Caffe<citation id="340" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,Tensorflow<citation id="341" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,Torch<citation id="342" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,MXNet<citation id="343" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,PaddlePaddle<citation id="344" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等开源深度学习框架提供了丰富的高级模块化函数支持,大大降低了应用的开发难度;腾讯<citation id="345" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、阿里云<citation id="346" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、百度<citation id="347" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、谷歌<citation id="348" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、微软<citation id="349" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、亚马逊<citation id="350" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、IBM<citation id="351" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等厂商也都提供了人工智能服务,涵盖图像识别、语音识别、自动机器学习等多个方面.通过调用API接口,开发者可以实现高性能的人工智能应用.得益于理论与工具的发展,人工智能系统正大范围地部署.</p>
                </div>
                <div class="p1">
                    <p id="248">然而,随着各类人工智能应用的出现和发展,其中的安全隐患也逐渐暴露出来.2018年3月发生在美国亚利桑那州的优步无人车事故中,事发时处于自动驾驶模式的无人车并没有检测到前方行人,驾驶员也未及时进行干预,最终致使行人被撞身亡<citation id="353" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.微软于2016年上线的社交机器人Tay,在一天之内受到用户的不良诱导逐渐学习成为一位种族主义者,迫使微软将该机器人紧急下线<citation id="354" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.在包括自动驾驶<citation id="357" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>、恶意软件检测<citation id="355" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、视频安防<citation id="356" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>等在内的安全敏感领域,需对人工智能系统安全性和稳定性提出更高的要求.除了安全问题之外,隐私问题同样也受到人工智能服务提供商和用户的关注.由于机器模型的训练需要依赖大量的训练数据和计算资源,模型隐私与知识产权保护成为服务提供商最为关心的问题之一.而对用户而言,他们则更关注其个人信息作为训练数据是否会被泄露,如何才能确保个人敏感信息不被第三方窃取.</p>
                </div>
                <div class="p1">
                    <p id="249">目前,人工智能系统与人类生产生活关系日益紧密,其安全问题越来越受到社会重视.<citation id="359" type="reference"><link href="51" rel="bibliography" />国务院于2017</citation>年发布的《新一代人工智能发展规划》中明确指出:“在大力发展人工智能的同时,必须高度重视可能带来的安全风险挑战,加强前瞻预防与约束引导,最大限度降低风险,确保人工智能安全、可靠、可控发展”<citation id="358" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.遗憾的是,以往的人工智能理论大多基于一种“好人假设”,较少考虑到在开放甚至是对抗环境下的机器学习安全与隐私问题.</p>
                </div>
                <div class="p1">
                    <p id="250">从上述问题出发,本文结合当前人工智能系统安全领域的相关研究工作,系统地分析和归纳了人工智能系统中可能存在的安全与隐私风险及现有的应对方法,并对未来的发展趋势进行了展望,以期引起相关研究者的关注并提供指导.</p>
                </div>
                <h3 id="251" name="251" class="anchor-tag"><b>1 人工智能系统安全风险模型</b></h3>
                <div class="p1">
                    <p id="252">对于系统进行安全风险分析,首先需要建立安全风险模型.对此,本节首先对人工智能系统中潜在的攻击面进行简要分析,并从攻击能力和攻击目标2个角度建立攻击者模型.</p>
                </div>
                <h4 class="anchor-tag" id="253" name="253"><b>1.1 人工智能系统攻击面</b></h4>
                <div class="p1">
                    <p id="254">人工智能系统的应用场合和作用功能多样,例如无人驾驶、声音识别、机器翻译等,核心部分主要包括数据和模型.如图1所示,根据数据流向,人工智能系统主要包含了4个关键环节<citation id="360" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="255">1) 输入环节.人工智能系统通过传感器(摄像头、麦克风、激光雷达、GPS等)获取外部环境数据,或者通过直接读取文件获取数据.</p>
                </div>
                <div class="p1">
                    <p id="256">2) 数据预处理环节.输入的原始数据需要经过格式转换、尺度变换、数据压缩等预处理工作,以满足机器学习模型输入格式要求,同时降低数据量以保证系统工作的实时性.</p>
                </div>
                <div class="p1">
                    <p id="257">3) 机器学习模型.机器学习模型是人工智能系统的核心,即“大脑”,主要包括训练和测试2个阶段.在训练阶段,机器学习模型利用预处理过的训练数据对模型参数进行调节,以提升对于特定任务的工作性能(通常用准确率、召回率等指标衡量).对于强化学习(reinforcement learning),还存在模型与环境的动态交互过程.当训练完成时,机器学习模型就进入了测试阶段.训练好的模型将根据输入提供相应的输出结果.</p>
                </div>
                <div class="p1">
                    <p id="258">4) 输出环节.人工智能系统会以标签、置信度等多种形式给予输出,为后续的分类、决策等任务提供支持.</p>
                </div>
                <div class="p1">
                    <p id="259">由于人工智能系统所处环境的开放性,输入、输出2个环节会直接暴露在攻击威胁环境中.在后续的介绍中将会看到,即使在预处理环节或机器学习模型被隐藏的情况下,攻击者仍然可以通过发送轮询样本的方式对系统内部结构进行推测并发动攻击.</p>
                </div>
                <div class="area_img" id="260">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201910008_260.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 人工智能系统基本框架[26]" src="Detail/GetImg?filename=images/JFYZ201910008_260.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 人工智能系统基本框架<citation id="361" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201910008_260.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The basic framework of artificial intelligence systems</p>

                </div>
                <h4 class="anchor-tag" id="261" name="261"><b>1.2 攻击能力</b></h4>
                <div class="p1">
                    <p id="262">在攻击者的攻击能力模型中,一般需要考虑2个要素:攻击者掌握的情报以及攻击者能够采取的攻击手段.</p>
                </div>
                <h4 class="anchor-tag" id="263" name="263">1) 依据攻击者掌握的情报,攻击可以分为:</h4>
                <div class="p1">
                    <p id="264">① 白盒攻击(white-box attack).攻击者了解目标系统的详细信息,如数据预处理方法、模型结构、模型参数,某些情况下攻击者还能够掌握部分或全部的训练数据信息.在白盒攻击模型中,攻击者能够更容易地发现可攻击环节并设计相应的攻击策略.</p>
                </div>
                <div class="p1">
                    <p id="265">② 黑盒攻击(black-box attack).系统对于攻击者而言并不透明,关键细节都被隐藏,攻击者仅能够接触输入和输出环节.在黑盒攻击模型中,攻击者可以通过构造并发送输入样本,并根据相应的输出信息来对系统的某些特性进行推理.</p>
                </div>
                <h4 class="anchor-tag" id="266" name="266">2) 依据攻击者能够采取的干扰手段,攻击被分为:</h4>
                <div class="p1">
                    <p id="267">① 训练阶段攻击(attack in the training stage).攻击者可以干扰系统的训练阶段,主要方式包括对训练数据进行修改以及对环境施加影响(强化学习).</p>
                </div>
                <div class="p1">
                    <p id="268">② 推断阶段攻击(attack in the inference stage).攻击者仅能接触到训练完成之后的系统.该假设在真实场景中更为多见.</p>
                </div>
                <h4 class="anchor-tag" id="269" name="269"><b>1.3 攻击目标</b></h4>
                <div class="p1">
                    <p id="270">攻击目标是指攻击者希望借助攻击所能达到的攻击效果.根据信息安全的CIA三要素,针对人工智能系统的攻击目标主要可以分为3类:</p>
                </div>
                <h4 class="anchor-tag" id="485" name="485">1) 保密性(confidentiality)攻击.</h4>
                <div class="p1">
                    <p id="271">攻击者期望从人工智能系统中盗取训练数据、模型参数等保密信息,破坏数据和模型隐私.</p>
                </div>
                <h4 class="anchor-tag" id="486" name="486">2) 完整性(integrity)攻击.</h4>
                <div class="p1">
                    <p id="272">攻击者期望能够影响系统输出,使其偏离预期.例如通过欺骗、篡改等攻击手段使得系统错误地接受假类样本,即错误接受(false acceptance).</p>
                </div>
                <h4 class="anchor-tag" id="487" name="487">3) 可用性(availability)攻击.</h4>
                <div class="p1">
                    <p id="273">攻击者期望降低系统的工作性能(如准确率)或者服务质量(如响应速度),甚至导致系统拒绝服务.</p>
                </div>
                <div class="p1">
                    <p id="274">而根据攻击者的攻击效果,攻击目标又被划分为</p>
                </div>
                <h4 class="anchor-tag" id="488" name="488">1) 目标攻击(targeted attack).</h4>
                <div class="p1">
                    <p id="275">攻击者限定攻击范围和攻击效果,如诱导机器学习模型误分类到特定结果;</p>
                </div>
                <h4 class="anchor-tag" id="489" name="489">2) 无目标/无差别攻击(untargeted/indiscrimi-nate attack).</h4>
                <div class="p1">
                    <p id="276">攻击者的攻击目标更为宽泛,攻击目的可能是造成更大的攻击影响,或者仅使得模型犯错而不限定欺骗结果.</p>
                </div>
                <div class="p1">
                    <p id="277">本文将基于所建立的安全风险模型来审视人工智能系统在输入环节、数据预处理环节、机器学习模型以及输出环节4个核心模块,以及系统实现与运行中所面临的安全风险,并结合相关研究工作进行阐述和讨论.</p>
                </div>
                <h3 id="278" name="278" class="anchor-tag"><b>2 输入环节安全风险及对策</b></h3>
                <div class="p1">
                    <p id="279">人工智能系统依靠传感器(如摄像头、麦克风等)或数据文件输入(文件上传)获取信息,并通过数据预处理环节,依据模型输入要求将采集到的原始数据进行格式、大小等属性的调整.一旦攻击者借助某种方式对输入环节进行了干扰,就能够从源头上对系统发动攻击.传感器欺骗攻击即为一种典型的针对输入环节的缺陷利用.</p>
                </div>
                <h4 class="anchor-tag" id="490" name="490">1) 传感器欺骗.</h4>
                <div class="p1">
                    <p id="280">传感器欺骗是指攻击者针对传感器的工作特性,恶意构造相应的攻击样本并输送至传感器,造成人类和传感器对数据的感知差异,从而达成欺骗效果.该问题被认为是对配备有传感器的设备的最关键威胁之一,受到研究者的广泛关注.Shin等人调查并将传感器欺骗攻击<citation id="362" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>分为3类:常规信道攻击(重放攻击)、传输信道攻击和侧信道攻击.传感器欺骗一个典型的例子是“无声”语音命令攻击.该类攻击借助人类听觉系统难以察觉的声音信号对语音识别系统开展攻击<citation id="364" type="reference"><link href="57" rel="bibliography" /><link href="59" rel="bibliography" /><sup>[<a class="sup">28</a>,<a class="sup">29</a>]</sup></citation>.对现代电子设备中普遍使用的非线性麦克风硬件而言,其可录制范围上限为24 kHz,超越了人类对20 kHz可识别声音频率的上限.攻击者可以在麦克风超出人类听觉的接收频率范围内发送声音信号,从而使得设备能够感知而不被听众察觉.由于其不可闻性,该类攻击方法攻击效果更强.Zhang等人提出的“海豚音攻击” 通过生成超声频段的语音控制信号实现了对语音系统的“无声”控制<citation id="363" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>.Dean等人证明<citation id="365" type="reference"><link href="61" rel="bibliography" /><link href="63" rel="bibliography" /><sup>[<a class="sup">30</a>,<a class="sup">31</a>]</sup></citation>,当声频成分接近陀螺仪传感质量的共振频率时,MEMS陀螺仪容易受到高功率高频声噪声的影响.攻击者可以借此干扰无人机等智能设备的环境感知能力,致使设备瘫痪.但是上述攻击假设攻击源可以在物理上靠近目标设备,难以实现远程攻击.</p>
                </div>
                <h4 class="anchor-tag" id="491" name="491">2) 应对措施.</h4>
                <div class="p1">
                    <p id="281">对于传感器欺骗攻击,可以采取传感器增强(忽略相应的攻击频段)、输入滤波等措施<citation id="366" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>来检测破坏恶意构造的攻击信息,实现对系统输入环节的安全增强.</p>
                </div>
                <h3 id="282" name="282" class="anchor-tag"><b>3 数据预处理环节安全风险及对策</b></h3>
                <div class="p1">
                    <p id="283">信息预处理环节是信息处理系统中的必备环节,其作用是将输入数据转换为后续模型输入所要求的特定形式.最近的研究表明,在数据的转换过程中也存在安全风险.</p>
                </div>
                <h4 class="anchor-tag" id="492" name="492">1) 重采样攻击.</h4>
                <div class="p1">
                    <p id="284">信息预处理环节的作用通常是为了将输入数据转换为模型输入要求的特定形式.数据重采样就是一种常见的数据预处理操作,其目的为:一是改变数据信息格式以满足输入要求,如当前主流视觉深度学习模型输入大小固定,需要对输入图片进行缩放操作;二是信息压缩,提升信息系统处理效率.这一过程会造成数据信息发生变化,成为一个潜在的攻击面.Xiao等人提出了针对图像预处理环节的欺骗攻击<citation id="367" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>,该方法是一种针对插值算法的逆向攻击方法,当攻击图片被图像识别系统缩放后,被隐藏图片得以显现.与经典的对抗样本攻击方法不同,该方法针对的是图像预处理环节,理论上与图像识别模型无关,并且该方法可以实现源-目标攻击(source-to-target attack).此外,该工作还显示,即使识别系统部署在云端,攻击者仍然可以通过轮询的方式对重采样过程进行推测和还原,进而发动重采样攻击.</p>
                </div>
                <h4 class="anchor-tag" id="493" name="493">2) 应对措施.</h4>
                <div class="p1">
                    <p id="285">针对重采样攻击,可以采取对输入预处理引入随机化或者重采样质量监测方法来增大攻击难度<citation id="368" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>.</p>
                </div>
                <h3 id="286" name="286" class="anchor-tag"><b>4 机器学习模型中的安全风险及对策</b></h3>
                <div class="p1">
                    <p id="287">机器学习模型是人工智能系统进行感知和决策的核心部分,其应用过程主要包含训练和预测2个重要阶段.关于机器学习模型的安全问题,<citation id="372" type="reference"><link href="67" rel="bibliography" />Dalvi等人于2004</citation>年最早提出了对抗分类(adversarial classification)的概念<citation id="369" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>,<citation id="373" type="reference"><link href="69" rel="bibliography" />Lowd等人于2005</citation>年进一步提出了对抗学习(adversarial learning)的概念<citation id="370" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>.Huang等人则对抗机器学习提出了更为具体和系统的分类方式<citation id="371" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>.目前,机器学习模型安全问题可以主要分为3类:</p>
                </div>
                <h4 class="anchor-tag" id="494" name="494">1) 诱导攻击(causative attack).</h4>
                <div class="p1">
                    <p id="288">攻击者借助向训练数据加入毒化数据等手段,影响模型训练过程,进而干扰模型的工作效果.</p>
                </div>
                <h4 class="anchor-tag" id="495" name="495">2) 逃逸攻击(evasion attack).</h4>
                <div class="p1">
                    <p id="289">攻击者在正常样本基础上人为地构造异常输入样本,致使模型在分类或决策时出现错误,达到规避检测的攻击效果.</p>
                </div>
                <h4 class="anchor-tag" id="496" name="496">3) 探索攻击(exploratory attack).</h4>
                <div class="p1">
                    <p id="290">攻击者试图推断机器学习模型是如何工作的,包括对模型边界的预测、训练数据的推测等.</p>
                </div>
                <div class="p1">
                    <p id="291">从保密性角度考虑,一般人工智能系统需要考虑2个要素——数据与模型.人工智能服务提供商需要投入资金和时间收集数据,设计、训练和改进模型,同时需要对用户负责,保证数据不被泄露.然而,已有研究证明存在模型逆向攻击(model inversion attack)——可以根据系统输出推测输入特征,还原敏感信息<citation id="375" type="reference"><link href="73" rel="bibliography" /><link href="75" rel="bibliography" /><sup>[<a class="sup">36</a>,<a class="sup">37</a>]</sup></citation>,以及模型萃取攻击(model extraction attack)——通过发送轮询数据推测模型参数并尝试还原出功能相近的替身模型(substitution model)<citation id="374" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>,二者会分别侵犯数据隐私和模型隐私.</p>
                </div>
                <h4 class="anchor-tag" id="292" name="292"><b>4.1 数据投毒</b></h4>
                <div class="p1">
                    <p id="293">数据投毒是指攻击者通过修改训练数据内容和分布,来影响模型的训练结果.例如Yang等人展示了攻击者通过对推荐系统注入构造的虚假关联数据,污染训练数据集,实现对推荐系统反馈结果的人为干预<citation id="376" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>.实验表明:通过对共同访问(co-visitation)推荐系统进行数据投毒,可以对YouTube,eBay,Amazon,Yelp,LinkedIn等Web推荐系统功能产生干扰.Munňoz-González等人提出了基于反向梯度优化的攻击方法,针对包含深度学习模型等在内的一系列基于梯度方法训练的模型,都可以实现数据投毒效果<citation id="377" type="reference"><link href="81" rel="bibliography" /><sup>[<a class="sup">40</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="294">应对措施.针对投毒攻击的防御,一般考虑污染数据和正常数据分布差异,方法主要包括鲁棒性机器学习<citation id="378" type="reference"><link href="83" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>以及数据清洗<citation id="379" type="reference"><link href="85" rel="bibliography" /><sup>[<a class="sup">42</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="295" name="295"><b>4.2 模型后门</b></h4>
                <div class="p1">
                    <p id="296">模型后门(backdoor)是指通过训练得到的、深度神经网络中的隐藏模式.当且仅当输入为触发样本(trigger)时,模型才会产生特定的隐藏行为;否则,模型工作表现保持正常.Gu等人提出了BadNets,通过数据投毒方式来注入后门数据集<citation id="380" type="reference"><link href="87" rel="bibliography" /><sup>[<a class="sup">43</a>]</sup></citation>.针对MNIST手写数据集识别模型网络,使用BadNets可以达到99%以上的攻击成功率,但不会影响模型在正常手写样本上的识别性能.Liu等人提出了一种针对神经网络的特洛伊木马攻击<citation id="381" type="reference"><link href="89" rel="bibliography" /><sup>[<a class="sup">44</a>]</sup></citation>.相较于Gu等人的工作<citation id="382" type="reference"><link href="87" rel="bibliography" /><sup>[<a class="sup">43</a>]</sup></citation>,该方法的一个优点是攻击者无需直接接触训练集.该方法另一个优点在于在触发样本和神经元之间构建了更强的连接,在训练样本较少的情况下也能够注入有效后门.然而,该后门构造方法基于引起特定最大响应值的内部神经元来设计触发样本,无法构造任意触发样本.除了触发模型的异常行为外,Song等人展示了一种利用模型后门的训练数据隐私窃取攻击方式<citation id="383" type="reference"><link href="91" rel="bibliography" /><sup>[<a class="sup">45</a>]</sup></citation>:依靠类似机器学习中正则化或数据增强方法对机器学习算法进行微调,第三方机器学习服务提供商可以借助用户数据训练出高准确度和高泛化性能的模型,并使得该模型能够暴露训练数据信息.</p>
                </div>
                <div class="p1">
                    <p id="297">针对模型后门问题,Wang等人提出了相应的检测和后门还原方案<citation id="384" type="reference"><link href="93" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>,该方法的思想相对直观:对于模型后门相对应的标签,很小的输入扰动会引起该标签对应置信度明显的变化.此外,作者还提出了包括输入过滤、神经元裁剪以及去学习等后门去除策略.</p>
                </div>
                <h4 class="anchor-tag" id="298" name="298"><b>4.3 对抗样本</b></h4>
                <div class="p1">
                    <p id="299">传统机器学习模型大多基于一个稳定性假设:训练数据与测试数据近似服从相同分布.当罕见样本甚至是恶意构造的非正常样本输入到机器学习模型时,就有可能导致机器学习模型输出异常结果.一个典型例子即<citation id="386" type="reference"><link href="95" rel="bibliography" />Szegedy等人在2013</citation>年所描述的视觉“对抗样本”(adversarial examples)现象:对输入图片构造肉眼难以发现的轻微扰动,可导致基于深度神经网络的图像识别器输出错误的结果<citation id="385" type="reference"><link href="95" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>.通过构造对抗样本,攻击者可以通过干扰人工智能服务推理过程来达成逃避检测等攻击效果.</p>
                </div>
                <div class="p1">
                    <p id="300">在机器视觉领域,针对对抗样本的生成方法和对抗样本特性已得到较多的研究.根据攻击效果分类,对抗样本攻击可以被分类为目标攻击<citation id="387" type="reference"><link href="97" rel="bibliography" /><sup>[<a class="sup">48</a>]</sup></citation>和非目标攻击<citation id="388" type="reference"><link href="99" rel="bibliography" /><sup>[<a class="sup">49</a>]</sup></citation>,而根据攻击者对机器学习模型的攻击能力则可以将攻击分类为白盒攻击<citation id="389" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">50</a>]</sup></citation>(white-box attack)和黑盒攻击<citation id="390" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>(black-box attack).为了达到欺骗效果,对抗样本的一个显著特点是隐蔽性,即对抗扰动难以被人类所察觉,最大限度保持原样本的语义信息.除了隐蔽性之外,Tramèr等人的工作<citation id="391" type="reference"><link href="105" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>还揭示了对抗样本的另一个突出特性——可传递性(transferability).借助可传递性,同一个对抗样本可以同时作用于多个模型,这部分解释了对抗样本问题为什么得以广泛存在.可传递性使得对抗样本防御工作变得更具挑战性.针对目前常见的对抗样本生成算法,一些研究人员合作发布了开源对抗样本算法库CleverHans<citation id="392" type="reference"><link href="107" rel="bibliography" /><sup>[<a class="sup">53</a>]</sup></citation>,以推动对抗样本攻/防研究工作的发展.与此同时,一些研究者探究了物理世界中的对抗样本现象:Kurakin等人尝试了将对抗样本进行打印<citation id="393" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">54</a>]</sup></citation>.针对Inception-V3模型生成对抗样本并利用600像素分辨率的打印机进行打印,打印出的对抗样本对于识别系统仍然具有欺骗性.然而该方法需要在所打印出的图片四周配备二维码以帮助图像识别系统对图像进行定位和裁剪,其结果并不具有普适性;Athalye等人提出了鲁棒性更高的物理对抗样本生成技术,利用3D打印技术制作了物理世界对抗样本模型,可以在多个角度下实现对识别模型的欺骗<citation id="394" type="reference"><link href="111" rel="bibliography" /><sup>[<a class="sup">55</a>]</sup></citation>;Sharif等人引入不可打印分数(non-printability score, NPS)到目标函数中,通过优化方法计算扰动并打印到眼镜框上,攻击者佩戴眼镜框后可以成功误导人脸识别系统<citation id="395" type="reference"><link href="113" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>;Eykholt等人则综合考虑了相机角度、干扰形状、打印效果等物理因素,设计了针对无人驾驶系统的对抗样本攻击方法,通过对路标覆盖扰动标记,诱导无人驾驶系统将“停车”标志被误识别为“限速”标志<citation id="396" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">57</a>]</sup></citation>.这些例子进一步说明了对抗样本威胁不仅局限于信息域,对抗样本攻击能够在物理域产生实际影响,在一些关键应用上可能会引发灾难性后果.</p>
                </div>
                <div class="p1">
                    <p id="301">在语音系统方面,Kumar等人开展了针对语音错误解释攻击的实证研究<citation id="397" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">58</a>]</sup></citation>.此外,Zhang等人展示了一种类似的方法<citation id="398" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>:针对语音助手的“技能”(skill,即某种功能)调用方式,攻击者通过引入具有相似/部分覆盖的发音名称或释义名称的恶意“技能”,来劫持目标“技能”的语音命令.Zhang等人则系统地探究了自然语言处理和意图分类器(intent classifier)的工作归因,并创建了第1个语言模型引导的模糊测试工具,以发现现有明显更易受攻击的语音应用<citation id="399" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>.除了利用自然语言理解缺陷的语音系统攻击,一些研究人员提出了一系列的语音对抗样本生成方法,来欺骗语音识别系统.Carlini等人开发了一种针对Mozilla DeepSpeech的对抗音频生成技术,利用优化方法直接对原始输入进行修改从而对模型进行欺骗<citation id="400" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">61</a>]</sup></citation>.Yuan等人提出针对服务接口的对抗语音样本生成方法,攻击者将一组命令嵌入到一首歌中,可以有效地控制目标系统而不被察觉<citation id="401" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">62</a>]</sup></citation>.Vaidya等人提出的方法利用合成和自然声音之间的差异,制造可以被计算机语音识别系统识别但人类不易理解的对抗样本<citation id="402" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>.Carlini等人的工作展示了利用一种迭代方法来构造针对黑盒语音系统的攻击语音<citation id="403" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">64</a>]</sup></citation>.为了获得更好的结果,Carlini等人同时提出了一种针对白盒模型的改进攻击方法.在攻击者完全了解语音识别系统中所使用算法的条件下,这种改进后的攻击可以保证合成的语音命令不被人类所理解.在威胁模型假设方面,该类攻击要求将攻击者的发言者放置在受害者设备附近的物理位置(距离超过3.5 m时会失效).上述4个攻击方法局限于特定的模型和硬件平台.不同于此,Abdullah等人提出了一种针对声音处理环节的攻击方法<citation id="404" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">65</a>]</sup></citation>.该文作者提出了4种扰动类型,并在包括Google语音API,Bing语音API等7种语音服务在内的12个语音识别模型上进行了测试,均成功实现了有效攻击,展示了该攻击影响的广泛性.</p>
                </div>
                <div class="p1">
                    <p id="302">除了视觉系统与语音系统外,文本处理系统也是人工智能技术的一类典型应用,被广泛应用于垃圾邮件检测、不良信息过滤、机器翻译等任务上.当前研究表明文本处理系统也正受到对抗样本的威胁.Papernot等人提出了一种基于梯度的白盒对抗样本生成方式<citation id="405" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">66</a>]</sup></citation>,该方法迭代地修改输入文本,直到生成的序列被循环神经网络错误分类,但该攻击引发的词级变化会明显影响文本语义,攻击容易被察觉;Samanta等人利用嵌入梯度来确定重要单词<citation id="406" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">67</a>]</sup></citation>,并设计了启发式规则、人工构造的同义词及笔误来对文本进行删除、增加或替换;Ebrahimi等人提出了一种基于梯度的字符级分类器对抗样本构造方法,对one-hot编码形式的输入向量进行修改<citation id="407" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">68</a>]</sup></citation>;Alzantot等人提出了同义词替换攻击方法<citation id="408" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">69</a>]</sup></citation>,利用遗传算法生成使用同义词或近义词替换的方法,通过对抗性文本来欺骗语义识别系统; Belinkov等人的研究表明字符级的机器翻译系统对数据噪声十分敏感,可以借助非词汇符号进行攻击<citation id="409" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">70</a>]</sup></citation>;同样地,Gao等人提出一种黑盒文字对抗样本攻击方法,应用字符扰动来生成针对深度学习分类器的对抗性文本<citation id="410" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">71</a>]</sup></citation>;Hosseini等人的工作展示,通过在字符之间添加空格或点号,就可以彻底改变Google有害信息检测服务的评分<citation id="411" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">72</a>]</sup></citation>;Zhao等人还提出了利用生成对抗网络生成针对机器翻译应用程序的对抗序列<citation id="412" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">73</a>]</sup></citation>,然而该方法仅限于短文本.</p>
                </div>
                <div class="p1">
                    <p id="303">除了上述研究工作外,还存在针对其他应用类型的对抗样本攻击.Xu等人利用遗传编程(genetic programming)方法随机修改文件,成功攻击了2个号称准确率极高的恶意PDF文件分类器:PDFrate和Hidost<citation id="413" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">74</a>]</sup></citation>.这些逃避检测的恶意文件都由算法自动修改生成,并不需要PDF安全专家介入.在恶意代码检测方面,Grosse等人提出了在离散和二进制输入域修改输入样本,可以绕过恶意有效代码检测<citation id="414" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">75</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="304">为了应对对抗样本的问题,近年来研究人员提出了一些包括直接对抗训练<citation id="415" type="reference"><link href="95" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>(adversarial training)——将对抗样本及正确标签重新输入到模型中进行重训练,该方法较为简单但防御未知对抗样本能力较差;梯度掩模<citation id="416" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">76</a>]</sup></citation>(gradient masking)——针对基于梯度的对抗样本攻击方式,通过隐藏梯度,令此类攻击失效;对抗样本检测<citation id="417" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">77</a>]</sup></citation>——直接检测是否存在对抗样本的防御方法.此外,Dziugaite等人使用 JPG 图像压缩的方法,减少对抗扰动对准确率的影响<citation id="418" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">78</a>]</sup></citation>.实验证明该方法对部分对抗攻击算法有效,但通常仅采用压缩方法是远远不够的,并且压缩图像时也会降低正常分类的准确率.</p>
                </div>
                <div class="p1">
                    <p id="305">虽然对抗样本防御方法已经得到较多研究,但是当前仍然缺少一个通用有效的防御方案.事实上,当前大多数的防御评估方法都是在衡量对抗攻击的能力下界<citation id="419" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">79</a>]</sup></citation>:这类评估所验证的是一个样本集合的邻域内的攻击样本攻击效果,仅能发现当前区域而非所有防御失效点.而且这些防御评估方法都是基于一种非适应性攻击模型,即假设攻击者并不知晓防御方法.Carlini等人认为考虑非适应性攻击模型是有必要的,但是有很大的局限性<citation id="420" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">80</a>]</sup></citation>.相对应地,一种有效的模型鲁棒性评估应该基于适应性攻击模型,即假设攻击者知晓防御者已采取的防御策略并可以采取反制措施<citation id="423" type="reference"><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><sup>[<a class="sup">81</a>,<a class="sup">82</a>]</sup></citation>.例如针对梯度掩模防御策略,Papernot等人提出了一种通过黑盒轮询输入标签的策略来对梯度进行回推<citation id="421" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">51</a>]</sup></citation>;Athalye等人则提出了通过改变代价函数来进行对抗样本攻击<citation id="422" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">83</a>]</sup></citation>.从安全评估结果的可靠性考虑,需要对所有已知或未知攻击(最坏情况)的防御效果得出被测试模型的鲁棒性下界,即模型鲁棒性的最低保证.</p>
                </div>
                <div class="p1">
                    <p id="306">当前一个发展方向是对模型鲁棒性进行形式化验证.虽然Lecuyer等人的研究成果可以应用于对ImageNet分类器的鲁棒性分析<citation id="424" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">84</a>]</sup></citation>,但当前的模型鲁棒性验证方法,如文献<citation id="426" type="reference">[<a class="sup">85</a>,<a class="sup">86</a>]</citation>,还大多只能局限于特定的网络模型.文献<citation id="427" type="reference">[<a class="sup">87</a>,<a class="sup">88</a>]</citation>等工作已经开始探索对任意神经网络模型鲁棒性进行形式化验证的可能性,但是由于计算复杂度过高,无法应用于中大规模的网络模型.此外,鲁棒性证明方法的一个显著缺点是,该类方法给出了对于特定集合的邻域对抗样本存在性证明,但是尚无法对该集合外的样本提供理论上的证明和保证<citation id="425" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">80</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="307" name="307"><b>4.4 模型逆向</b></h4>
                <div class="p1">
                    <p id="308">由于机器学习模型在训练时会或多或少地在训练数据上发生过拟合,攻击者可以根据训练数据与非训练数据的拟合差异来窥探训练数据隐私.Fredrikson等人以医疗机器学习中的隐私问题为例阐述了模型逆向攻击(model inversion attack)<citation id="428" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">89</a>]</sup></citation>:对某一个被训练好的机器学习模型,攻击者利用模型、未知属性以及模型输出的相关相关性,实现对隐私属性的推测.具体到实例中,Fredrikson等人根据华法林剂量信息来尝试对患者的基因型进行推测.此外,Fredrikson等人在<citation id="429" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>展示了针对另外2个模型进行逆向攻击的例子:借助模型置信度输出,攻击者可以估计生活调查中的受访者是否承认对其他重要人物存在欺骗行为;针对人脸识别系统,攻击者可以根据用户姓名恢复出对应的可识别的人脸照片.一些研究证明了另外一类的模型逆向攻击——成员推理攻击(membership inference attack),即攻击者可以推断某个特定实例是否在训练数据集中.早在2008年Homer等人就展示了对基因组数据的成员推理攻击(membership attack)<citation id="430" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">90</a>]</sup></citation>.在此基础上,Shokri等人展示了可以通过训练多个“影子模型”(shadow models)来模拟被攻击模型,并利用机器学习模型输出中暗含的训练数据之间的区分性,来发动成员推理攻击<citation id="431" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>.Salem等人通过实验证明了通过单个影子模型开展相同攻击的可能性<citation id="432" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>,即使在攻击者无法获取被攻击模型的训练数据情况下,也根据模型输出的统计特征进行推测攻击.针对地理位置聚集信息,Pyrgelis等人建立了博弈模型,并将其转化为是否属于特定集合成员的分类问题,进一步实现了对于地理位置信息的成员推理攻击<citation id="433" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">92</a>]</sup></citation>.除了判别模型,Hayes等人还提出了白盒和黑盒情况下针对于生成模型的成员推理攻击,并在多个数据集上开展了实证研究<citation id="434" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">93</a>]</sup></citation>;Salem等人提出了针对在线学习算法的数据重构攻击,对在线学习模型的更新训练数据进行了推测和复原<citation id="435" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">94</a>]</sup></citation>.以往大多数相关研究工作会采用一些攻击者拥有同分布数据、影子模型或者目标模型结构等假设.对此Salem等人研究了这些假设逐步弱化时的成员推理攻击情况<citation id="436" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>.结果表明,即使在已知信息很有限的情况下,攻击者仍然具有进行成员推理攻击的能力.</p>
                </div>
                <div class="p1">
                    <p id="309">除此之外,Carlini等人揭露了深度学习模型,尤其是生成模型中存在的“意外记忆问题”<citation id="437" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">95</a>]</sup></citation>——模型在对低频的敏感训练数据(如用户密码等)进行学习的同时,会倾向于完整地记忆与目标任务无关的训练数据细节,这就为该类数据带来了泄露风险.实验结果表明,传统的过拟合抑制方法很难解决意外记忆问题.对此,Carlini等人提出了对应的“暴露度”(exposure)指标来评估意外记忆程度,用于辅助开发者进行模型结构和参数的选择、调整.</p>
                </div>
                <div class="p1">
                    <p id="310">针对用户数据保护问题,研究者提出了多种解决方案.常见的一种方法是利用差分隐私(differential privacy)模型<citation id="438" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">96</a>]</sup></citation>来分析算法所能提供的隐私性保证.Chaudhuri等人证明在训练时通过向代价函数,即模型预测值与标签的误差加入指数分布的噪声,可以实现ε-差分隐私<citation id="439" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">97</a>]</sup></citation>.Abadi等人提出在梯度被用于参数更新前对梯度添加扰动,可以达到单一训练方场景下的一种强差分隐私边界<citation id="440" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">98</a>]</sup></citation>.Shokri等人证明对于类似深度神经网络的大容量模型,借助引入噪声参数的多方计算,可以保证差分隐私性<citation id="441" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">99</a>]</sup></citation>.Gilad-Bachrach等人提出了一种神经网络模型的加密方法——CryptoNets,该方法使得神经网络可以被应用于加密数据<citation id="442" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">100</a>]</sup></citation>.CryptoNets允许用户向云端服务上传加密数据,而无需提供秘钥,从而保证了用户数据的机密性.为了保证用户数据隐私,拥有训练数据的双方或者多方可能不被允许直接进行训练数据的交换和合并,这就造成了“数据孤岛”问题.对此,有研究提出利用联邦学习(federated learning)方法来进行多方联合学习<citation id="443" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">101</a>]</sup></citation>.在该模型下,训练数据并不会离开本地.各方建立一个虚拟共有模型,通过加噪机制交换参数,对共有模型进行共同训练.</p>
                </div>
                <h4 class="anchor-tag" id="311" name="311"><b>4.5 模型萃取</b></h4>
                <div class="p1">
                    <p id="312">模型萃取攻击(model extraction attack)是指攻击者可以通过发送轮询数据并查看对应的响应结果,推测机器学习模型的参数或功能,复制一个功能相似甚至完全相同的机器学习模型.例如理论上讲,针对<i>n</i>维线性回归模型,通过<i>n</i>组线性不相关轮询数据及模型输出可准确求解出权重参数<citation id="444" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>.该攻击可破坏算法机密性,造成对知识产权的侵犯,并使攻击者随后能够依据被复制模型进行对抗样本攻击或模型逆向攻击.Lowd和Meek提出了有效的算法来窃取线性分类器的模型参数<citation id="445" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>.Tramèr等人证明,当API为返回置信度分数时,可以更准确和有效地推测模型参数<citation id="446" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>.此外,超参数在机器学习中至关重要,因为超参数的差异通常会导致模型具有显著不同的性能.根据机器学习模型最终学习到的参数往往会最小化代价函数这一原则,Wang等人提出了机器学习模型的超参数推测方法<citation id="447" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">102</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="313">对于模型萃取攻击,最直接最简单的防御策略是对模型参数<citation id="448" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">102</a>]</sup></citation>或者输出结果进行近似处理<citation id="449" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>.除此之外,为了避免模型被盗用、保护知识产权,一些研究者还提出了模型水印(watermarking)的概念.Venugopal等人较早地提出关于学习模型水印技术的方法<citation id="450" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">103</a>]</sup></citation>,但是它侧重于标记模型的输出而非标记模型本身.文献<citation id="454" type="reference">[<a class="sup">104</a>,<a class="sup">105</a>]</citation>提出通过向损失函数添加新的正则化项来对神经网络添加水印的方法.虽然他们的方法保持了模型的高识别精度,同时使水印具有一定的抗毁能力,但其并没有明确解决所有权的虚假声明问题,也没有明确考虑水印生成算法遭泄露后的抗攻击情况.此外,在文献<citation id="455" type="reference">[<a class="sup">104</a>,<a class="sup">105</a>]</citation>中,为了避免因密钥泄露而发生的水印移除情况,验证密钥只能使用一次,这带来了一定的局限性.Merrer等人建议结合对抗样本与对抗训练方法为神经网络注入水印<citation id="451" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">106</a>]</sup></citation>.他们提出生成2种类型(被模型正确和错误地分类)的对抗样本,然后微调模型以使其正确地对所有类型进行分类.这种方法在很大程度上依赖于对抗样本以及它们在不同模型中的可迁移性,但目前尚不明确对抗样本在什么条件下能够进行跨模型迁移,或者这种迁移性是否会被削弱<citation id="452" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">107</a>]</sup></citation>.Adi等人提出了一种黑盒方式的深度神经网络水印技术<citation id="453" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">108</a>]</sup></citation>,从理论上分析了该方法与模型后门的联系,并通过实验证明了该方法不影响原模型性能,同时对水印的鲁棒性进行了评估.</p>
                </div>
                <h3 id="314" name="314" class="anchor-tag"><b>5 输出环节安全风险</b></h3>
                <div class="p1">
                    <p id="315">模型输出将会直接决定人工智能系统的分类和决策.通过对决策输出部分的劫持和结果篡改可以直接实现对系统的干扰或控制.另一个需要注意的问题是,多数人工智能服务接口会反馈丰富的信息,但是丰富/准确的决策输出值可能会带来安全隐患——攻击者据此可以开展模型逆向攻击和模型萃取攻击,或者利用置信度来迭代式构造对抗样本.此外,Elsayed等人介绍了一种对抗性重编程方法(adversarial reprogramming)<citation id="456" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">109</a>]</sup></citation>.即使模型的训练目的并非是完成攻击者所指定的任务,攻击者通过制造一个对抗扰动并添加至机器学习模型的所有测试输入,可以使模型在处理这些输入时执行攻击者选择的任务.利用该方法,攻击者只需要付出很小的代价,就可以借助他人训练好的模型资源实现所需的系统功能.</p>
                </div>
                <div class="p1">
                    <p id="316">如4.4节和4.5节所述,针对利用模型输出置信度进行数据逆向、模型萃取或模型重用等探索攻击行为,可以采用输出值近似处理或引入随机波动来降低探索攻击反馈结果的准确性,提高攻击难度.</p>
                </div>
                <h3 id="317" name="317" class="anchor-tag"><b>6 系统实际搭建及运行中的安全风险及对策</b></h3>
                <h4 class="anchor-tag" id="318" name="318"><b>6.1 代码漏洞</b></h4>
                <div class="p1">
                    <p id="319">当前流行的深度学习框架,如Caffe,Tensor-flow,Torch等,提供了高效、便捷的人工智能系统开发支持环境,为人工智能技术的推广作出了巨大贡献.仅需几百行甚至几十行的核心代码就可以完成模型的搭建、训练和运行.但与框架的使用简洁性恰恰相反,为了完成对多种软硬件平台的支持以及复杂计算功能的集成,深度学习框架往往需要依赖于种类纷繁的基础库和第三方组件支持,如Caffe包含有超过130种的依赖库<citation id="457" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">110</a>]</sup></citation>.这种组件的依赖复杂度会严重降低深度学习框架的安全性.某个组件开发者的疏忽,或者不同组件开发者之间开发规范的不统一,都可能会向深度学习框架引入漏洞.更为严重的是,一个底层依赖库的漏洞(如图像处理库OpenCV)有可能会蔓延到多个高层深度学习框架,进而影响到所支持的一系列应用中.此时攻击者可以基于控制流改写人工智能系统关键数据,或者通过数据流劫持控制代码执行,实现对人工智能系统的干扰、控制甚至破坏.Xiao等人分析了深度学习应用的层级结构,并披露了Tensorflow,Caffe与Torch三种深度学习框架及其依赖库中的数十种代码漏洞,同时展示了如何利用该漏洞引发基于3种框架的深度学习应用发生崩溃、识别结果篡改、非法提权等问题<citation id="458" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">110</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="320">通常来说,可以利用传统的漏洞测试方法,例如模糊测试来发现软件中的代码漏洞.但是,传统的漏洞测试方法在应用于深度学习框架时具有其局限性.Xiao等人指出,基于覆盖率的模糊测试方法对于深度学习应用的测试效果并不理想<citation id="459" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">110</a>]</sup></citation>.其原因在于基本上所有的输入数据都经过相同的网络层进行计算,导致大量输入样本覆盖的是同一条执行路径.另一个问题则在于难以区分代码逻辑漏洞和模型本身的对抗样本/训练不完全问题.</p>
                </div>
                <h4 class="anchor-tag" id="321" name="321"><b>6.2 学习不完全/学习偏差</b></h4>
                <div class="p1">
                    <p id="322">虽然依靠海量数据和深度学习技术的人工智能系统在多种任务上表现出突出的工作性能,但由于诸如训练数据偏差、过拟合和模型缺陷等原因,即便在非对抗环境下,系统罕见或边缘样本输入可能会引起人工智能系统出现意外或错误的行为.例如自动驾驶训练数据无法覆盖所有光照、天气、道路及周围物体分布下的行驶情况,致使未知路况下无人驾驶汽车行为的准确性和可预测性难以得到保证.在安全要求较高的场合,罕见/边缘样本的训练缺失有可能导致灾难性后果.由于目前深度学习模型可解释性差,难以对系统异常行为进行预测或归因,发现由训练不完全或偏差导致的模型缺陷成为一个极具挑战性的问题.</p>
                </div>
                <div class="p1">
                    <p id="323">为了发现人工智能系统中潜藏的漏洞,相关研究工作将软件自动化测试中的概念迁移到了人工智能领域.Pei等人设计了深度学习系统的白盒测试框架DeepXplore<citation id="460" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">111</a>]</sup></citation>,并提出了“神经元覆盖率(neuron coverage)”的概念,该框架会按照一定策略自动生成测试样本来触发潜在的异常行为,以帮助发现网络缺陷;Ma等人在文献<citation id="461" type="reference">[<a class="sup">111</a>]</citation>基础上提出了多方位细粒度的自动化测试方法DeepGauge<citation id="462" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">112</a>]</sup></citation>,并提出了更详细的神经网络自动化测试指标;受MC/DC测试覆盖率指标的启发,Sun等人提出了基于DNN结构特征和语义的4种测试指标<citation id="463" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">113</a>]</sup></citation>,并在MNIST,CIFAR-10和ImageNet分类任务上进行了验证测试;Ma等人将传统软件测试中的组合测试(combinatorial testing)概念延伸到深度学习模型上并提出了DeepCT<citation id="464" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">114</a>]</sup></citation>;Ma等人还将变异测试(mutation testing)概念沿用到深度学习模型上并提出了DeepMutation测试框架<citation id="465" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">115</a>]</sup></citation>,设计了针对训练数据和训练过程的原始级变异方法,以及针对无训练环节的模型级变异方法;针对神经网络在数值传递过程中可能存在的漏洞,Odena等人提出了针对神经网络的基于覆盖指导的模糊(coverage-guided fuzzing)方法TensorFuzz,以帮助代码调试<citation id="466" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">116</a>]</sup></citation>.除了自动化测试之外,还有学者尝试了形式化分析方法:Wang等人基于区间型符号的神经网络形式化安全分析方法<citation id="467" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">117</a>]</sup></citation>,根据输入估计网络的输出范围,判断是否会触犯某些安全限定.</p>
                </div>
                <h4 class="anchor-tag" id="324" name="324"><b>6.3 系统设计缺陷利用</b></h4>
                <div class="p1">
                    <p id="325">为了提高系统的智能化,诸如语音助手等的人工智能服务需要被赋予很高的系统操作权限,一旦设计不当,很容易被攻击者利用进行系统非法操作.例如Diao等人展示了攻击者可以控制设备扬声器,在后台播放准备好的音频文件,同时借助安卓系统内嵌的谷歌语音助手,进行无权限情况下的发送信息、读取隐私数据、甚至是远程控制等操作<citation id="468" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">118</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="326">综上对人工智能系统中的安全风险进行总结,如表1所示:</p>
                </div>
                <div class="area_img" id="327">
                    <p class="img_tit"><b>表1 人工智能系统安全与隐私风险分析小结</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 A Brief Summary of Security and Privacy Risks Against Artificial Intelligence Systems</b></p>
                    <p class="img_note"></p>
                    <table id="327" border="1"><tr><td>Pipeline</td><td>Type</td><td>Threat Model</td><td>Description</td><td>Instances</td><td>Impacts</td></tr><tr><td>Data Input</td><td>Sensor<br />Spoofing</td><td>Inference/White-box <br />or Black-box/Impact <br />integrity or <br />availability</td><td>Leverage the sensibility difference between human and hardware to inject deceiving/malicious information into inputs which are hard to be perceived by human.</td><td>Ref [28-29]</td><td>Spoofing</td></tr><tr><td><br />Data<br />Preprocessing</td><td>Scaling<br />Attack</td><td>Inference/White-box <br />or Black-box/Impact <br />integrity</td><td>Conceal a tiny amount of deceiving/malicious information under a large amount of normal information, and the injected part will get recovered after resampling.</td><td>Ref [32]</td><td>Spoofing,<br />detection<br />evasion</td></tr><tr><td rowspan="6"><br />Machine<br />Learning<br />Model</td><td>Data<br />Poisoning</td><td>Training/White-box/<br />Impact integrity</td><td>Tampering the content or distribution of the training data.</td><td>Ref [39-40]</td><td>Destroyor <br />control model <br />functionality</td></tr><tr><td><br />Backdoor</td><td>Training/White-box/<br />Impact integrity</td><td>A hidden pattern trained into a model,which can be activated and produce unexpected behavior if and only if when specific “triggers” get input.</td><td>Ref [43,45]</td><td>Hide <br />unexpected <br />behavior</td></tr><tr><td><br />Adversarial<br />Examples</td><td>Inference/White-box <br />or Black-box/Impact <br />integrity</td><td>Applying small but intentionally designed perturbations to test samples,which can cause the models to give incorrect outputs.</td><td>Ref [47,70]</td><td>Spoofing,<br />detection <br />evasion</td></tr><tr><td><br />Model<br />Inversion</td><td>Inference/Black-box/<br />Impact(data) <br />confidentiality</td><td>For an already-trained model,one adversary can infer private or sensitive attributes by leveraging the correlation among model,hidden attributes and model outputs.</td><td>Ref [36-37,<br />89,92]</td><td>Training <br />data leakage</td></tr><tr><td><br />Unintended<br />Memory</td><td>Inference/White-<br />box/Impact (data) <br />confidentiality</td><td>When trained on rarely-occurring data,the model tends to remember too much details even unrelated to the learning task.</td><td>Ref [95]</td><td>Sensitive <br />data leakage</td></tr><tr><td><br />Model<br />Extraction</td><td>Inference/Black-box/<br />Impact (model) <br />confidentiality</td><td>Steal parameters of a machine learning model by sending queries,to construct a new model with similar functionality and performance.</td><td>Ref [34,38,<br />102]</td><td>Partial or complete <br />model functionality <br />duplication,which <br />enables black-box <br />attacks</td></tr><tr><td>Output</td><td>Model<br />Reuse</td><td>Inference/Black-box/<br />impact (model) <br />confidentiality</td><td>Add perturbation to all testing-time inputs to reprogram neural networks to perform a specific task chosen by the adversary.</td><td>Ref [109]</td><td>Steal and <br />transfer model<br />functionality</td></tr><tr><td rowspan="3"><br />Implementation<br />Or Execution</td><td>Code<br />Vulnerability</td><td>Inference/White-<br />box/Impact integrity <br />or availability</td><td>Vulnerabilities buried in codes of machine learning libraries or systems,which can be propagated and spread following the “third-party dependencies → deep learning frame-works → AI systems” path.</td><td>Ref [110]</td><td>Spoofing,hijacking,<br />denial of service<br />and etc.</td></tr><tr><td><br />Incomplete/<br />Biased<br />Learning</td><td>Inherent weaknesses,<br />which may affect <br />integrity or <br />availability</td><td>Unexpected or incorrect output due to underfitting,overfitting or biased training data.</td><td></td><td>Unexpected or <br />incorrect behavior</td></tr><tr><td><br />System<br />Design<br />Flaw</td><td>Inference/Black-box/<br />Impact integrity or <br />availability</td><td>Inappropriate system design in logic or permission control.</td><td>Ref [118]</td><td>Remote sensitive<br />data access or control<br />without any permission</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="328" name="328" class="anchor-tag"><b>7 人工智能安全分析与防护技术的研究展望</b></h3>
                <div class="p1">
                    <p id="329">针对表1中所总结的人工智能系统安全与隐私问题,在本节,我们将讨论在人工智能安全分析与防护研究工作中的4个发展方向:</p>
                </div>
                <h4 class="anchor-tag" id="497" name="497">1) 物理对抗样本.</h4>
                <div class="p1">
                    <p id="330">针对无人驾驶、人脸识别、语音识别等关键应用,需要评估其在真实场景下的安全性能,尤其是潜在的物理对抗样本威胁.不同于信息域内对图像、音频等文件直接进行修改的对抗样本攻击方式,物理对抗样本攻击效能评估还需要同时考虑物理环境以及输入输出设备特性等因素的影响.例如针对视觉系统而言,还需考虑光照、角度、摄像头光学特性、打印设备分辨率及色差等因素对构造对抗样本的影响;对音频处理系统而言,进行音频对抗样本的重放攻击需同时考虑攻击扬声器的声音播放质量、目标麦克风的收音性能以及背景噪声等因素的影响.</p>
                </div>
                <h4 class="anchor-tag" id="498" name="498">2) 模型鲁棒性的形式化验证.</h4>
                <div class="p1">
                    <p id="331">形式化验证可以给出对于攻击的上界/模型鲁棒性下界的估计,对于安全系数要求较高的场合而言是十分必要的.可以预见,形式化验证将是今后模型安全评估的一个重要研究方向,会有越来越多的研究集中在如何降低验证复杂度以及提高方法的模型普适性上.</p>
                </div>
                <h4 class="anchor-tag" id="499" name="499">3) 人工智能系统自动化测试方法.</h4>
                <div class="p1">
                    <p id="332">当前形式化验证方法计算复杂度高、难以应用到实际深度模型上.此外,复杂的代码依赖层级给人工智能系统的人工分析带来极大的难度.对此,可以借助自动化测试方法来持续提高对攻击强度的平均估计,发现模型可能出现的异常行为或者安全漏洞.除了代码自动化测试方法以外,模型的自动化测试也可以作为模型形式化验证的一种辅助措施.在设计和应用自动化测试方法时需要关注3个问题:①如何定义模型异常行为;②如何区分模型在无意义分类边界下和关键分类边界下的异常行为;③如何定义自动化评测的引导指标.</p>
                </div>
                <h4 class="anchor-tag" id="500" name="500">4) 隐私保护.</h4>
                <div class="p1">
                    <p id="333">在某些应用场景中,相较于人工智能服务的精度,用户更重视个人数据的隐私保护.尤其在大规模分布式数据存储和模型训练的情况下,如何同时保证用户数据隐私和模型的训练效率及工作精度是在人工智能服务提供商需要解决的关键问题.</p>
                </div>
                <h3 id="334" name="334" class="anchor-tag"><b>8 结  论</b></h3>
                <div class="p1">
                    <p id="335">随着深度学习技术及计算硬件架构的发展和变革,人工智能技术在机器视觉、语音识别、机器视觉等关键任务上取得了重大突破,接近甚至超过人类水平,这些成果推动了人工智能技术的技术落地,衍生出诸如人脸识别、语音助手、无人驾驶等应用领域.在促进人工智能系统为人类生产生活带来便利的同时,如何发现、修复人工系统中的安全缺陷,规避人工智能应用风险也成为了人类和社会日渐关心的问题.本文在对国内外人工智能安全研究调研和分析的基础上,总结归纳了数据输入、数据预处理、学习模型与模型输出4个系统关键点中可能存在的安全风险及应对措施,并进一步指出了人工智能安全分析与防护技术未来的研究趋势.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[1]</b>He Kaiming,Zhang Xiangyu,Ren Shaoqing,et al.Deep residual learning for image recognition[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770- 778
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Achieving human parity in conversational speech recognition">

                                <b>[2]</b>Xiong W,Droppo J,Huang Xuedong,et al.Achieving human parity in conversational speech recognition[J].arXiv preprint arXiv:1610.05256,2016
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201705013&amp;v=MzE3MjZPZVplUnNGeXpnVmJ6Qkx5dlNkTEc0SDliTXFvOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Fan Zhengguang,Qu Dan,Yan Honggang,et al.Fast incremental outlier mining algorithm based on grid and capacity[J].Journal of Computer Research and Development,2017,54(5):1036- 1044 (in Chinese)(范正光,屈丹,闫红刚,等.基于深层神经网络的多特征关联声学建模方法[J].计算机研究与发展,2017,54(5):1036- 1044)
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=QANet:Combining local convolution with global self-attention for reading comprehension">

                                <b>[4]</b>Yu A W,Dohan D,Luong M T,et al.QANet:Combining local convolution with global self-attention for reading comprehension[J].arXiv preprint arXiv:1804.09541,2018
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mastering the game of Go without human knowledge">

                                <b>[5]</b>Silver D,Schrittwieser J,Simonyan K,et al.Mastering the game of Go without human knowledge[J].Nature,2017,550:354- 359
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Berkeley view of systems challenges for AI">

                                <b>[6]</b>Stoica I,Song D,Popa R A,et al.A Berkeley view of systems challenges for AI[J].arXiv preprint arXiv:1712.05855,2017
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">

                                <b>[7]</b>Jia Yangqing,Shelhamer E,Donahue J,et al.Caffe:Convolutional architecture for fast feature embedding[C] //Proc of the 22nd ACM Int Conf on Multimedia.New York:ACM,2014:675- 678
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A System for Large-Scale Machine Learning">

                                <b>[8]</b>Abadi M,Barham P,Chen Jianmin,et al.Tensorflow:A system for large-scale machine learning[C] //Proc of the 12th USENIX Symp on Operating Systems Design and Implementation (OSDI'16).Berkeley,CA:USENIX Association,2016:265- 283
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="">

                                <b>[9]</b>Ronan,Clément,Koray,et al.Torch7[EB/OL].[2019-05-25].http://torch.ch/
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MXNet:A flexible and efficient machine learning library for heterogeneous distributed systems">

                                <b>[10]</b>Chen Tianqi,Li Mu,Li Yutian,et al.MXNet:A flexible and efficient machine learning library for heterogeneous distributed systems[J].arXiv preprint arXiv:1512.01274,2015
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PaddlePaddle">

                                <b>[11]</b>PaddlePaddle developers.PaddlePaddle[EB/OL].[2019-05-25].https://github.com/paddlepaddle/paddle
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tencent AI open platform">

                                <b>[12]</b>Tencent.Tencent AI open platform[EB/OL].[2019-05-25].https://ai.qq.com/
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ET brain">

                                <b>[13]</b>Alibaba.ET brain[EB/OL].[2019-05-25].https://et.aliyun.com/index
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Baidu AI open platform">

                                <b>[14]</b>Baidu.Baidu AI open platform[EB/OL].[2019-05-25].http://ai.baidu.com/
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AI and machine learning products">

                                <b>[15]</b>Google.AI and machine learning products [EB/OL].[2019-05-25].https://cloud.google.com/products/ai/
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Azure AI">

                                <b>[16]</b>Azure.Azure AI [EB/OL].[2019-05-25].https://azure.microsoft.com/en-us/overview/ai-platform/
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Amazon machine learning">

                                <b>[17]</b>Amazon.Amazon machine learning[EB/OL].[2019-05-25].https://aws.amazon.com/cn/machine-learning/
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=IBM watson">

                                <b>[18]</b>IBM.IBM watson[EB/OL].[2019-05-25].https://www.ibm.com/watson
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-Driving Uber Car Kills Pedestrian in Arizona,Where Robots Roam">

                                <b>[19]</b>Wakabayashi D.Self-driving uber car kills pedestrian in Arizona,where robots roam[EB/OL].[2019-04-28].https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tay (bot)">

                                <b>[20]</b>Wikipedia.Tay (bot)[EB/OL].[2019-04-29].https://en.wikipedia.org/wiki/Tay_(bot)
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End to end learning for self-driving cars">

                                <b>[21]</b>Bojarski M,Testa D D,Dworakowski D,et al.End to end learning for self-driving cars[J].arXiv preprint arXiv:1604.07316,2016
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708006&amp;v=MDEwMDdlWmVSc0Z5emdWYnpCTHl2U2RMRzRIOWJNcDQ5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>Wang Juanjuan,Qiao Ying,Wang Hongan.Graph-based auto-driving reasoning task scheduling[J].Journal of Computer Research and Development,2017,54(8):1693- 1702 (in Chinese)(王娟娟,乔颖,王宏安.基于图模型的自动驾驶推理任务调度[J].计算机研究与发展,2017,54(8):1693- 1702)
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Droid-Sec: Deep Learning in Android Malware Detection">

                                <b>[23]</b>Yuan Zhenlong,Lu Yongqiang,Wang Zhaoguo,et al.Droid-Sec:Deep learning in android malware detection[C] //Proc of the 2014 ACM Conf on SIGCOMM.New York:ACM,2014:371- 372
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using deep learning to make video surveillance smarter">

                                <b>[24]</b>Metz R.Using deep learning to make video surveillance smarter[EB/OL].2015 [2019-05-05].https://www.technologyreview.com/s/540396/using-deep-learning-to-make-video-surveillance-smarter/
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" >
                                    <b>[25]</b>
                                The State Council.New-generation artificial intelligence development plan[EB/OL].2017 [2019-05-01].http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm (in Chinese)(国务院.新一代人工智能发展规划[EB/OL].2017 [2019-05-01].http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm)
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards the science of security and privacy in machine learning">

                                <b>[26]</b>Papernot N,McDaniel P,Sinha A,et al.Towards the science of security and privacy in machine learning[J].arXiv preprint arXiv:1611.03814,2016
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sampling race:Bypassing timing-based analog active sensor spoofing detection on analog-digital systems">

                                <b>[27]</b>Shin H,Son Y,Park Y,et al.Sampling race:Bypassing timing-based analog active sensor spoofing detection on analog-digital systems[C] //Proc of the 10th USENIX Workshop on Offensive Technologies (WOOT'16).Berkeley,CA:USENIX Association,2016
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DolphinAttack:Inaudible voice commands">

                                <b>[28]</b>Zhang Guoming,Chen Yan,Ji Xiaoyu,et al.DolphinAttack:Inaudible voice commands[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security (CCS'17).New York:ACM,2017:103- 117
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making microphones hear inaudible sounds">

                                <b>[29]</b>Roy N,Hassanieh H,Roy Choudhury R.BackDoor:Making microphones hear inaudible sounds[C] //Proc of the 15th Annual Int Conf on Mobile Systems,Applications,and Services.New York:ACM,2017:2- 14
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Characterization of the Performance of a MEMS Gyroscope in Acoustically Harsh Environments">

                                <b>[30]</b>Dean R N,Castro S T,Flowers G T,et al.A characterization of the performance of a MEMS gyroscope in acoustically harsh environments[J].IEEE Transactions on Industrial Electronics,2010,58(7):2591- 2596
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise">

                                <b>[31]</b>Dean R N,Flowers G T,Hodel A S,et al.On the degradation of MEMS gyroscope performance in the presence of high power acoustic noise[C] //Proc of the 2007 IEEE Int Symp on Industrial Electronics.Piscataway,NJ:IEEE,2007:1435- 1440
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Seeing is not believing:Camouflage attacks on image scaling algorithms">

                                <b>[32]</b>Xiao Qixue,Chen Yufei,Shen Chao,et al.Seeing is not believing:Camouflage attacks on image scaling algorithms[C] //Proc of the 28th USENIX Security Symp (USENIX Security'19).Berkeley,CA:USENIX Association,2019
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial classification">

                                <b>[33]</b>Dalvi N,Domingos P,Sanghai S,et al.Adversarial classification[C] //Proc of the 10th ACM SIGKDD Int Conf on Knowledge Discovery and Data Mining.New York:ACM,2004:99- 108
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial learning">

                                <b>[34]</b>Lowd D,Meek C.Adversarial learning[C] //Proc of the 11th ACM SIGKDD Int Conf on Knowledge Discovery in Data Mining.New York:ACM,2005:641- 647
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial machine learning">

                                <b>[35]</b>Huang L,Joseph A D,Nelson B,et al.Adversarial machine learning[C] //Proc of the 4th ACM Workshop on Security and Artificial Intelligence.New York:ACM,2011:43- 58
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model inversion attacks that exploit confidence information and basic countermeasures">

                                <b>[36]</b>Fredrikson M,Jha S,Ristenpart T.Model inversion attacks that exploit confidence information and basic countermeasures[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security (CCS'15).New York:ACM,2015:1322- 1333
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Membership inference attacks against machine learning models">

                                <b>[37]</b>Shokri R,Stronati M,Song Congzheng,et al.Membership inference attacks against machine learning models[C] //Proc of the 2017 IEEE Symp on Security and Privacy (S&amp;P'17).Piscataway,NJ:IEEE,2017:3- 18
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stealing machine learning models via prediction APIs">

                                <b>[38]</b>Tramèr F,Zhang Fan,Juels A,et al.Stealing machine learning models via prediction APIs[C] //Proc of the 25th USENIX Security Symp (USENIX Security'16).Berkeley,CA:USENIX Association,2016:601- 618
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fake co-visitation injection attacks to recommender systems">

                                <b>[39]</b>Yang Guolei,Gong N Zhenqiang,Cai Ying.Fake co-visitation injection attacks to recommender systems[C] //Proc of the 24th Annual Network and Distributed System Security Symp (NDSS 2017).Reston,VA,USA:The Internet Society,2017
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards poisoning of deep learning algorithms with back-gradient optimization">

                                <b>[40]</b>Munňoz-González L,Biggio B,Demontis A,et al.Towards poisoning of deep learning algorithms with back-gradient optimization[C] //Proc of the 10th ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:27- 38
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manipulating machine learning:Poisoning attacks and countermeasures for regression learning">

                                <b>[41]</b>Jagielski M,Oprea A,Biggio B,et al.Manipulating machine learning:Poisoning attacks and countermeasures for regression learning[C] //Proc of the 2018 IEEE Symp on Security and Privacy (S&amp;P'18).Piscataway,NJ:IEEE,2018:19- 35
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Casting out demons:Sanitizing training data for anomaly sensors">

                                <b>[42]</b>Cretu G F,Stavrou A,Locasto M E,et al.Casting out demons:Sanitizing training data for anomaly sensors[C] //Proc of the 2008 IEEE Symp on Security and Privacy (S&amp;P'08).Piscataway,NJ:IEEE,2008:81- 95
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identifying vulnerabilities in the machine learning model supply chain">

                                <b>[43]</b>Gu Tianyu,Dolan-Gavitt B,Garg S.BadNets:Identifying vulnerabilities in the machine learning model supply chain[J].arXiv preprint arXiv:1708.06733,2017
                            </a>
                        </p>
                        <p id="89">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trojaning attack on neural networks">

                                <b>[44]</b>Liu Yingqi,Ma Shiqing,Aafer Y,et al.Trojaning attack on neural networks[C] //Proc of the 25th Annual Network and Distributed System Security Symp (NDSS 2018).Reston,VA,USA:The Internet Society,2018
                            </a>
                        </p>
                        <p id="91">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning models that remember too much">

                                <b>[45]</b>Song Congzheng,Ristenpart T,Shmatikov V.Machine learning models that remember too much[C] //Proc of the 2017 ACM SIGSAC Conf on Computer and Communications Security (CCS'17).New York:ACM,2017:587- 601
                            </a>
                        </p>
                        <p id="93">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural cleanse:identifying and mitigating backdoor attacks in neural networks">

                                <b>[46]</b>Wang Bolun,Yao Yuanshun,Shan S,et al.Neural cleanse:Identifying and mitigating backdoor attacks in neural networks[C] //Proc of 2019 IEEE Symp on Security and Privacy (S&amp;P'19).Piscataway,NJ:IEEE,2019:530- 546
                            </a>
                        </p>
                        <p id="95">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks">

                                <b>[47]</b>Szegedy C,Zaremba W,Sutskever I,et al.Intriguing properties of neural networks[J].arXiv preprint arXiv:1312.6199,2013
                            </a>
                        </p>
                        <p id="97">
                            <a id="bibliography_48" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">

                                <b>[48]</b>Papernot N,McDaniel P,Jha S,et al.The limitations of deep learning in adversarial settings[C] //Proc of the 2016 IEEE European Symp on Security and Privacy (EuroS&amp;P).Piscataway,NJ:IEEE,2016:372- 387
                            </a>
                        </p>
                        <p id="99">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining and harnessing adversarial examples">

                                <b>[49]</b>Goodfellow I J,Shlens J,Szegedy C.Explaining and harnessing adversarial examples[J].arXiv preprint arXiv:1412.6572,2014
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_50" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepfool:A simple and accurate method to fool deep neural networks">

                                <b>[50]</b>Moosavi-Dezfooli S M,Fawzi A,Frossard P.Deepfool:A simple and accurate method to fool deep neural networks[C] //Proc of the 2016 IEEE Conf on Computer Vision and Pattern Recognition (CVPR).Piscataway,NJ:IEEE,2016:2574- 2582
                            </a>
                        </p>
                        <p id="103">
                            <a id="bibliography_51" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Practical black-box attacks against machine learning">

                                <b>[51]</b>Papernot N,McDaniel P,Goodfellow I,et al.Practical black-box attacks against machine learning[C] //Proc of the 2017 ACM on Asia Conf on Computer and Communications Security (ASIACCS'17).New York:ACM,2017:506- 519
                            </a>
                        </p>
                        <p id="105">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The space of transferable adversarial examples">

                                <b>[52]</b>Tramèr F,Papernot N,Goodfellow I,et al.The space of transferable adversarial examples[J].arXiv preprint arXiv:1704.03453,2017
                            </a>
                        </p>
                        <p id="107">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Technical report on the cleverhans v2.1.0 adversarial examples library">

                                <b>[53]</b>Papernot N,Faghri F,Carlini N,et al.Technical report on the cleverhans v2.1.0 adversarial examples library[J].arXiv preprint arXiv:1610.00768,2016
                            </a>
                        </p>
                        <p id="109">
                            <a id="bibliography_54" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial examples in the physical world">

                                <b>[54]</b>Kurakin A,Goodfellow I,Bengio S.Adversarial examples in the physical world[J].arXiv preprint arXiv:1607.02533,2016
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synthesizing robust adversarial examples">

                                <b>[55]</b>Athalye A,Engstrom L,Ilyas A,et al.Synthesizing robust adversarial examples[J].arXiv preprint arXiv:1707.07397,2017
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_56" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accessorize to a crime:real and stealthy attacks on state-of-the-art face recognition">

                                <b>[56]</b>Sharif M,Bhagavatula S,Bauer L,et al.Accessorize to a crime:Real and stealthy attacks on state-of-the-art face recognition[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security (CCS'16).New York:ACM,2016:1528- 1540
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_57" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust physical-world attacks on deep learning visual classification">

                                <b>[57]</b>Eykholt K,Evtimov I,Fernandes E,et al.Robust physical-world attacks on deep learning visual classification[C] //Proc of the 2018 IEEE Conf on Computer Vision and Pattern Recognition (CVPR).Piscataway,NJ:IEEE,2018:1625- 1634
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Skill squatting attacks on amazon alexa">

                                <b>[58]</b>Kumar D,Paccagnella R,Murley P,et al.Skill squatting attacks on amazon alexa[C] //Proc of the 27th USENIX Security Symp (USENIX Security'18).Berkeley,CA:USENIX Association,2018:33- 47
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_59" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dangerous skills Understanding and mitigating security risks of voice-controlled third-party functions on virtual personal assistant systems">

                                <b>[59]</b>Zhang Nan,Mi Xianghang,Feng Xuan,et al.Dangerous skills:Understanding and mitigating security risks of voice-controlled third-party functions on virtual personal assistant systems[C] //Proc of the 2019 IEEE Symp on Security and Privacy (S&amp;P'19).Piscataway,NJ:IEEE,2019:263- 278
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Life after speech recognition:Fuzzing semantic misinterpretation for voice assistant applications">

                                <b>[60]</b>Zhang Yangyong,Xu Lei,Mendoza A,et al.Life after speech recognition:Fuzzing semantic misinterpretation for voice assistant applications[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA:The Internet Society,2019
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Audio adversarial examples:Targeted attacks on speech-to-text">

                                <b>[61]</b>Carlini N,Wagner D.Audio adversarial examples:Targeted attacks on speech-to-text[C] //Proc of the 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:1- 7
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_62" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CommanderSong:A systematic approach for practical adversarial voice recognition">

                                <b>[62]</b>Yuan Xuejing,Chen Yuxuan,Zhao Yue,et al.CommanderSong:A systematic approach for practical adversarial voice recognition[C] //Proc of the 27th USENIX Security Symp (USENIX Security'18).Berkeley,CA:USENIX Association,2018:49- 64
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_63" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cocaine noodles:Exploiting the gap between human and machine speech recognition">

                                <b>[63]</b>Vaidya T,Zhang Yuankai,Sherr M,et al.Cocaine noodles:Exploiting the gap between human and machine speech recognition[C] //Proc of the 9th USENIX Workshop on Offensive Technologies (WOOT'15).Berkeley,CA:USENIX Association,2015
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_64" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hidden voice commands">

                                <b>[64]</b>Carlini N,Mishra P,Vaidya T,et al.Hidden voice commands[C] //Proc of the 25th USENIX Security Symp (USENIX Security'16).Berkeley,CA:USENIX Association,2016:513- 530
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_65" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems">

                                <b>[65]</b>Abdullah H,Garcia W,Peeters C,et al.Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA,USA:The Internet Society,2019
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_66" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crafting adversarial input sequences for recurrent neural networks">

                                <b>[66]</b>Papernot N,McDaniel P,Swami A,et al.Crafting adversarial input sequences for recurrent neural networks[C] //Proc of the 2016 IEEE Military Communications Conf (MILCOM 2016).Piscataway,NJ:IEEE,2016:49- 54
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_67" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards crafting text adversarial samples">

                                <b>[67]</b>Samanta S,Mehta S.Towards crafting text adversarial samples[J].arXiv preprint arXiv:1707.02812,2017
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hotflip:White-box adversarial examples for text classification">

                                <b>[68]</b>Ebrahimi J,Rao A,Lowd D,et al.Hotflip:White-box adversarial examples for text classification[J].arXiv preprint arXiv:1712.06751,2017
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating natural language adversarial examples">

                                <b>[69]</b>Alzantot M,Sharma Y,Elgohary A,et al.Generating natural language adversarial examples[J].arXiv preprint arXiv:1804.07998,2018
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synthetic and natural noise both break neural machine translation">

                                <b>[70]</b>Belinkov Y,Bisk Y.Synthetic and natural noise both break neural machine translation[J].arXiv preprint arXiv:1711.02173,2017
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Black-box generation of adversarial text sequences to evade deep learning classifiers">

                                <b>[71]</b>Gao Ji,Lanchantin J,Soffa M L,et al.Black-box generation of adversarial text sequences to evade deep learning classifiers[C] //Proc of 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:50- 56
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_72" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deceiving Google&amp;#39;&amp;#39;s perspective API built for detecting toxic comments">

                                <b>[72]</b>Hosseini H,Kannan S,Zhang Baosen,et al.Deceiving Google's perspective API built for detecting toxic comments[J].arXiv preprint arXiv:1702.08138,2017
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_73" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating natural adversarial examples">

                                <b>[73]</b>Zhao Zhengli,Dua D,Singh S.Generating natural adversarial examples[J].arXiv preprint arXiv:1710.11342,2017
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatically Evading Classifiers">

                                <b>[74]</b>Xu Weilin,Qi Yanjun,Evans D.Automatically Evading Classifiers[C] //Proc of the 23rd Annual Network and Distributed System Security Symp (NDSS 2016).Reston,VA,USA:The Internet Society,2016
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_75" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial Examples for Malware Detection">

                                <b>[75]</b>Grosse K,Papernot N,Manoharan P,et al.Adversarial examples for malware detection[C] //Proc of the 2017 European Symp on Research in Computer Security.Berlin:Springer,2017:62- 79
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_76" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SoK:Security and privacy in machine learning">

                                <b>[76]</b>Papernot N,McDaniel P,Sinha A,et al.SoK:Security and privacy in machine learning[C] //Proc of the 2018 IEEE European Symp on Security and Privacy (EuroS&amp;P).Piscataway,NJ:IEEE,2018:399- 414
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_77" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature squeezing:Detecting adversarial examples in deep neural networks">

                                <b>[77]</b>Xu Weilin,Evans D,Qi Yanjun.Feature squeezing:Detecting adversarial examples in deep neural networks[J].arXiv preprint arXiv:1704.01155,2017
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_78" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A study of the effect of jpg compression on adversarial images">

                                <b>[78]</b>Dziugaite G K,Ghahramani Z,Roy D M.A study of the effect of jpg compression on adversarial images[J].arXiv preprint arXiv:1608.00853,2016
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_79" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM4954B27195E7F927ED86A0315058A3C0&amp;v=MTUwMjhPWC9oclJjMWZMcmxSc21mQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnBod0wyNXc2RT1OaWZJWTdleEc5VytyWWhFYmU1NkN3b3d6UkZtbmpkNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[79]</b>Goodfellow I,McDaniel P,Papernot N.Making machine learning robust against adversarial inputs[J].Communications of the ACM,2018,61(7):56- 66
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_80" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On evaluating adversarial robustness">

                                <b>[80]</b>Carlini N,Athalye A,Papernot N,et al.On evaluating adversarial robustness[J].arXiv preprint arXiv:1902.06705,2019
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_81" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Science,security and the elusive goal of security as a scientific pursuit">

                                <b>[81]</b>Herley C,Van Oorschot P C.Sok:Science,security and the elusive goal of security as a scientific pursuit[C] //Proc of the 2017 IEEE Symp on Security and Privacy (S&amp;P'17).Piscataway,NJ:IEEE,2017:99- 120
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_82" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial examples are not easily detected:Bypassing ten detection methods">

                                <b>[82]</b>Carlini N,Wagner D.Adversarial examples are not easily detected:Bypassing ten detection methods[C] //Proc of the 10th ACM Workshop on Artificial Intelligence and Security.New York:ACM,2017:3- 14
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_83" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples">

                                <b>[83]</b>Athalye A,Carlini N,Wagner D.Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples[J].arXiv preprint arXiv:1802.00420,2018
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_84" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Certified robustness to adversarial examples with differential privacy">

                                <b>[84]</b>Lecuyer M,Atlidakis V,Geambasu R,et al.Certified robustness to adversarial examples with differential privacy[J].arXiv preprint arXiv:1802.03471,2018
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_85" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Certified defenses against adversarial examples">

                                <b>[85]</b>Raghunathan A,Steinhardt J,Liang P.Certified defenses against adversarial examples[J].arXiv preprint arXiv:1801.09344,2018
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_86" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Provable defenses against adversarial examples via the convex outer adversarial polytope">

                                <b>[86]</b>Wong E,Kolter J Z.Provable defenses against adversarial examples via the convex outer adversarial polytope[J].arXiv preprint arXiv:1711.00851,2017
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_87" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating robustness of neural networks with mixed integer programming">

                                <b>[87]</b>Tjeng V,Xiao Kai Y,Tedrake R.Evaluating robustness of neural networks with mixed integer programming[J].arXiv preprint arXiv:1711.07356,2017
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_88" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training for faster adversarial robustness verification via inducing relu stability">

                                <b>[88]</b>Xiao Kai Y,Tjeng V,Shafiullah N M,et al.Training for faster adversarial robustness verification via inducing relu stability[J].arXiv preprint arXiv:1809.03008,2018
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_89" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy in pharmacogenetics:an end-to-end case study of personalized warfarin dosing">

                                <b>[89]</b>Fredrikson M,Lantz E,Jha S,et al.Privacy in pharmacogenetics:An end-to-end case study of personalized warfarin dosing[C] //Proc of the 23rd USENIX Security Symp (USENIX Security'14).Berkeley,CA:USENIX Association,2014:17- 32
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_90" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJLA&amp;filename=SJLAFE7FADDE2C75FB5A58897FF9E2B947ED&amp;v=MjI5NzFtYUJ1SFlmT0dRbGZDcGJRMzVOcGh3TDI1dzZFPU5pZkhiOFhOR2FlOTIvc3dacGdJQ1FwTHltY1c0amQwVHdtVXBXYzNDN3VRUXMvckNPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[90]</b>Homer N,Szelinger S,Redman M,et al.Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays[J].PLoS Genetics,2008,4(8):e1000167
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_91" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models">

                                <b>[91]</b>Salem A,Zhang Yang,Humbert M,et al.ML-Leaks:Model and data independent membership inference attacks and defenses on machine learning models[C] //Proc of the 26th Annual Network and Distributed System Security Symp (NDSS 2019).Reston,VA,USA:The Internet Society,2019
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_92" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Knock knock who&amp;#39;&amp;#39;s there?Membership inference on aggregate location data">

                                <b>[92]</b>Pyrgelis A,Troncoso C,De Cristofaro E.Knock knock,who's there?Membership inference on aggregate location data[C] //Proc of the 25th Annual Network and Distributed System Security Symp (NDSS 2018).Reston,VA,USA:The Internet Society,2018
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_93" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LOGAN:Membership inference attacks against generative models">

                                <b>[93]</b>Hayes J,Melis L,Danezis G,et al.LOGAN:Membership inference attacks against generative models[J].arXiv preprint arXiv:1705.07663,2017
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_94" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Updates-Leak:Data set inference and reconstruction attacks in online learning">

                                <b>[94]</b>Salem A,Bhattacharya A,Backes M,et al.Updates-Leak:Data set inference and reconstruction attacks in online learning[J].arXiv preprint arXiv:1904.01067,2019
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_95" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The secret sharer:Evaluating and testing unintended memorization in neural networks">

                                <b>[95]</b>Carlini N,Liu Chang,Erlingsson Ú,et al.The secret sharer:Evaluating and testing unintended memorization in neural networks[C] //Proc of the 28th USENIX Security Symp (USENIX Security'19).Berkeley,CA:USENIX Association,2019
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_96" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Algorithmic Foundations of Differential Privacy">

                                <b>[96]</b>Dwork C,Roth A.The algorithmic foundations of differential privacy[J].Foundations and Trends in Theoretical Computer Science,2014,9(3/4):211- 407
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_97" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Differentially private empirical risk minimization">

                                <b>[97]</b>Chaudhuri K,Monteleoni C,Sarwate A D.Differentially private empirical risk minimization[J].Journal of Machine Learning Research,2011,12(Mar):1069- 1109
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_98" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning with differential privacy">

                                <b>[98]</b>Abadi M,Chu A,Goodfellow I,et al.Deep learning with differential privacy[C] //Proc of the 2016 ACM SIGSAC Conf on Computer and Communications Security (CCS'16).New York:ACM,2016:308- 318
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_99" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy-Preserving Deep Learning">

                                <b>[99]</b>Shokri R,Shmatikov V.Privacy-preserving deep learning[C] //Proc of the 2015 ACM SIGSAC Conf on Computer and Communications Security (CCS'15).New York:ACM,2015:1310- 1321
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_100" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cryptonets:Applying neural networks to encrypted data with high throughput and accuracy">

                                <b>[100]</b>Gilad-Bachrach R,Dowlin N,Laine K,et al.CryptoNets:Applying neural networks to encrypted data with high throughput and accuracy[C] //Proc of the 33rd Int Conf on Machine Learning (ICML'16).New York:ACM,2016:201- 210
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_101" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Differentially private federated learning:A client level perspective">

                                <b>[101]</b>Geyer R C,Klein T,Nabi M.Differentially private federated learning:A client level perspective[J].arXiv preprint arXiv:1712.07557,2017
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_102" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stealing hyperparameters in machine learning">

                                <b>[102]</b>Wang Binghui,Gong N Zhenqiang.Stealing hyperparameters in machine learning[C] //Proc of the 2018 IEEE Symp on Security and Privacy (S&amp;P'18).Piscataway,NJ:IEEE,2018:36- 52
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_103" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Watermarking the outputs of structured prediction with an application in statistical machine translation">

                                <b>[103]</b>Venugopal A,Uszkoreit J,Talbot D,et al.Watermarking the outputs of structured prediction with an application in statistical machine translation[C] //Proc of the 2011 Conf on Empirical Methods in Natural Language Processing (EMNLP).Stroudsburg,PA:ACL,2011:1363- 1372
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_104" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Embedding watermarks into deep neural networks">

                                <b>[104]</b>Uchida Y,Nagai Y,Sakazawa S,et al.Embedding watermarks into deep neural networks[C] //Proc of the 2017 ACM on Int Conf on Multimedia Retrieval.New York:ACM,2017:269- 277
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_105" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A digital fingerprinting framework for deep neural networks">

                                <b>[105]</b>Chen Huili,Rohani B D,Koushanfar F.DeepMarks:A digital fingerprinting framework for deep neural networks[J].arXiv preprint arXiv:1804.03648,2018
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_106" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial frontier stitching for remote neural network watermarking">

                                <b>[106]</b>Merrer E L,Perez P,Trédan G.Adversarial frontier stitching for remote neural network watermarking[J].arXiv preprint arXiv:1711.01894,2017
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_107" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blocking transferability of adversarial examples in black-box learning systems">

                                <b>[107]</b>Hosseini H,Chen Yize,Kannan S,et al.Blocking transferability of adversarial examples in black-box learning systems[J].arXiv preprint arXiv:1703.04318,2017
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_108" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Turning your weakness into a strength:Watermarking deep neural networks by backdooring">

                                <b>[108]</b>Adi Y,Baum C,Cisse M,et al.Turning your weakness into a strength:Watermarking deep neural networks by backdooring[C] //Proc of the 27th USENIX Security Symp (USENIX Security'18).Berkeley,CA:USENIX Association,2018:1615- 1631
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_109" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial reprogramming of neural networks">

                                <b>[109]</b>Elsayed G F,Goodfellow I,Sohl-Dickstein J.Adversarial reprogramming of neural networks[J].arXiv preprint arXiv:1806.11146,2018
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_110" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Security risks in deep learning implementations">

                                <b>[110]</b>Xiao Qixue,Li Kang,Zhang Deyue,et al.Security risks in deep learning implementations[C] //Proc of the 2018 IEEE Security and Privacy Workshops (SPW).Piscataway,NJ:IEEE,2018:123- 128
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_111" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepxplore:Automated whitebox testing of deep learning systems">

                                <b>[111]</b>Pei Kexin,Cao Yinzhi,Yang Junfeng,et al.DeepXplore:Automated whitebox testing of deep learning systems[C] //Proc of the 26th Symp on Operating Systems Principles (SOSP'17).New York:ACM,2017:1- 18
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_112" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepGauge:Multi-granularity testing criteria for deep learning systems">

                                <b>[112]</b>Ma Lei,Juefei-Xu F,Zhang Fuyuan,et al.DeepGauge:Multi-granularity testing criteria for deep learning systems[C] //Proc of the 33rd ACM/IEEE Int Conf on Automated Software Engineering.New York:ACM,2018:120- 131
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_113" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Testing deep neural networks">

                                <b>[113]</b>Sun Youcheng,Huang Xiaowei,Kroening D.Testing deep neural networks[J].arXiv preprint arXiv:1803.04792,2018
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_114" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combinatorial testing for deep learning systems">

                                <b>[114]</b>Ma Lei,Zhang Fuyuan,Xue Minhui,et al.Combinatorial testing for deep learning systems[J].arXiv preprint arXiv:1806.07723,2018
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_115" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepMutation:Mutation testing of deep learning systems">

                                <b>[115]</b>Ma Lei,Zhang Fuyuan,Sun Jiyuan,et al.DeepMutation:Mutation testing of deep learning systems[C] //Proc of the 2018 IEEE 29th Int Symp on Software Reliability Engineering (ISSRE 2018).Piscataway,NJ:IEEE,2018:100- 111
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_116" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensorfuzz:Debugging neural networks with coverage-guided fuzzing">

                                <b>[116]</b>Odena A,Goodfellow I.Tensorfuzz:Debugging neural networks with coverage-guided fuzzing[J].arXiv preprint arXiv:1807.10875,2018
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_117" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Formal security analysis of neural networks using symbolic intervals">

                                <b>[117]</b>Wang Shiqi,Pei Kexin,Whitehouse J,et al.Formal security analysis of neural networks using symbolic intervals[C] //Proc of the 27th USENIX Security Symp (USENIX Security'18).Berkeley,CA:USENIX Association,2018:1599- 1614
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_118" >
                                    <b>[118]</b>
                                Diao Wenrui,Liu Xiangyu,Zhou Zhe,et al.Your voice assistant is mine:How to abuse speakers to steal information and control your phone[C] //Proc of the 4th ACM Workshop on Security and Privacy in Smartphones &amp; Mobile Devices.New York:ACM,2014:63- 74<image href="images/JFYZ201910008_239.jpg" type="" display="inline" placement="inline"><alt></alt></image>Chen Yufei,born in 1994.PhD candidate.His main research interests include security of intelligent systems and behavioral analysis.<image href="images/JFYZ201910008_240.jpg" type="" display="inline" placement="inline"><alt></alt></image>Shen Chao,born in 1985.PhD,professor,PhD supervisor.Member of CCF.His main research interests include cyber-physical system optimization and security,network and system security,and artificial intelligence security.<image href="images/JFYZ201910008_241.jpg" type="" display="inline" placement="inline"><alt></alt></image>Wang Qian,born in 1980.PhD,professor,PhD supervisor.Member of CCF.His main research interests include AI security,data storage,search and computation outsourcing security and privacy,wireless systems security,big data security and privacy,and applied cryptography etc.<image href="images/JFYZ201910008_242.jpg" type="" display="inline" placement="inline"><alt></alt></image>Li Qi,born in 1979.PhD,associate professor,PhD supervisor.Senior member of CCF.His main research interests include network and system security,particularly in Internet security,mobile security,and big data security.<image href="images/JFYZ201910008_243.jpg" type="" display="inline" placement="inline"><alt></alt></image>Wang Cong,born in 1982.PhD,associate professor,PhD supervisor.His main research interests include data and computation outsourcing security in the context of cloud computing,blockchain and decentralized application,network security in emerging Internet architecture,multimedia security,and privacy-enhancing technologies in the context of big data and IoT.<image href="images/JFYZ201910008_244.jpg" type="" display="inline" placement="inline"><alt></alt></image>Ji Shouling,born in 1986.PhD,professor,PhD supervisor.Member of CCF.His main research interests include AI security,data-driven security,software and system security,and data analytics.<image href="images/JFYZ201910008_245.jpg" type="" display="inline" placement="inline"><alt></alt></image>Li Kang,born in 1973.PhD,professor,PhD supervisor.His main research interests include computer network and operating systems,especially system issues related to data security and privacy.<image href="images/JFYZ201910008_246.jpg" type="" display="inline" placement="inline"><alt></alt></image>Guan Xiaohong,born in 1955.PhD,professor,PhD supervisor.His main research interests include allocation and scheduling of complex networked resources,network security,and sensor networks.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201910008" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201910008&amp;v=MjYwNzJPM3pxcUJ0R0ZyQ1VSTE9lWmVSc0Z5emdWYnpCTHl2U2RMRzRIOWpOcjQ5RmJJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EWTJZN0VwcUxGMDdzVmh3QT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

