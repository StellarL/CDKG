

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128626141368750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201906007%26RESULT%3d1%26SIGN%3doyrJMrYKp99WQgNDRP%252f67hbM2%252fU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906007&amp;v=MjcyMzlOTHl2U2RMRzRIOWpNcVk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZpRG1VYjM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;1 背景与相关工作&lt;/b&gt; "><b>1 背景与相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="&lt;b&gt;1.1 ReRAM&lt;/b&gt;"><b>1.1 ReRAM</b></a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;1.2 LSTM神经网络&lt;/b&gt;"><b>1.2 LSTM神经网络</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;1.3 相关工作&lt;/b&gt;"><b>1.3 相关工作</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="&lt;b&gt;2 针对ReRAM特性与加速器结构的LSTM神经网络训练&lt;/b&gt; "><b>2 针对ReRAM特性与加速器结构的LSTM神经网络训练</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="&lt;b&gt;2.1 神经网络权重量化&lt;/b&gt;"><b>2.1 神经网络权重量化</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;2.2 层间I/O精度限制&lt;/b&gt;"><b>2.2 层间I/O精度限制</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;2.3 DAC与ADC及移位相加&lt;/b&gt;"><b>2.3 DAC与ADC及移位相加</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;2.4 ReRAM噪声分布&lt;/b&gt;"><b>2.4 ReRAM噪声分布</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.5 针对ReRAM特性的神经网络训练算法&lt;/b&gt;"><b>2.5 针对ReRAM特性的神经网络训练算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="&lt;b&gt;3 模拟器架构&lt;/b&gt; "><b>3 模拟器架构</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#116" data-title="&lt;b&gt;3.1 整体架构&lt;/b&gt;"><b>3.1 整体架构</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;3.2 处理单元&lt;/b&gt;"><b>3.2 处理单元</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;3.3 GPU加速计算&lt;/b&gt;"><b>3.3 GPU加速计算</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="&lt;b&gt;4 案例研究与结果评估&lt;/b&gt; "><b>4 案例研究与结果评估</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#145" data-title="&lt;b&gt;4.1 模拟器的实现&lt;/b&gt;"><b>4.1 模拟器的实现</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;4.2 仿真结果的评估&lt;/b&gt;"><b>4.2 仿真结果的评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#164" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="图1 ReRAM 交叉开关阵列">图1 ReRAM 交叉开关阵列</a></li>
                                                <li><a href="#64" data-title="图2 LSTM计算单元">图2 LSTM计算单元</a></li>
                                                <li><a href="#107" data-title="图3 DAC工作流程图">图3 DAC工作流程图</a></li>
                                                <li><a href="#118" data-title="图4 整体架构">图4 整体架构</a></li>
                                                <li><a href="#132" data-title="图5 LSTM模块的计算模式">图5 LSTM模块的计算模式</a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表1 使用GPU计算的加速效果&lt;/b&gt;"><b>表1 使用GPU计算的加速效果</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表2 模拟器所需电路参数&lt;/b&gt;"><b>表2 模拟器所需电路参数</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表3 模拟器所需神经网络参数&lt;/b&gt;"><b>表3 模拟器所需神经网络参数</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表4 模拟器所需硬件参数&lt;/b&gt;"><b>表4 模拟器所需硬件参数</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表5 训练算法评估结果&lt;/b&gt;"><b>表5 训练算法评估结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="190">


                                    <a id="bibliography_1" title="Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C] //Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1- 12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indatacenter performance analysis of a tensor processing unit">
                                        <b>[1]</b>
                                        Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C] //Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1- 12
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_2" title="Xcelerit.Benchmarks:Deep Learning Nvidia P100 vs V100 GPU[OL].[2017-11-27].https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Benchmarks:Deep Learning Nvidia P100 vs V100 GPU[OL]">
                                        <b>[2]</b>
                                        Xcelerit.Benchmarks:Deep Learning Nvidia P100 vs V100 GPU[OL].[2017-11-27].https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_3" title="Han Song, Kang Junlong, Mao Huizi, et al.ESE:Efficient speech recognition engine with sparse LSTM on FPGA[C]//Proc of the 2017 ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2017:75- 84" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ESE:Efficient Speech Recognition Engine with Sparse LSTM on FPGA">
                                        <b>[3]</b>
                                        Han Song, Kang Junlong, Mao Huizi, et al.ESE:Efficient speech recognition engine with sparse LSTM on FPGA[C]//Proc of the 2017 ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2017:75- 84
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_4" title="Wang Suo, Li Zhe, Ding Caiwen, et al.C-LSTM:Enabling efficient LSTM using structured compression techniques on FPGAs[C] //Proc of the 2018 ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2018:11- 20" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=C-LSTM:Enabling efficient LSTM using structured compression techniques on FPGAs">
                                        <b>[4]</b>
                                        Wang Suo, Li Zhe, Ding Caiwen, et al.C-LSTM:Enabling efficient LSTM using structured compression techniques on FPGAs[C] //Proc of the 2018 ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2018:11- 20
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_5" title="Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:14- 26" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ISAAC:a convolutional neural network accelerator with in-situ analog arithmetic in crossbars">
                                        <b>[5]</b>
                                        Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:14- 26
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_6" title="Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjEzODYvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SktGMFdhQmM9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_7" title="Evangelopoulos G N.Efficient hardware mapping of long short-term memory neural networks for automatic speech recognition[D].Belgium:KU Leuven, 2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient hardware mapping of long short-term memory neural networks for automatic speech recognition">
                                        <b>[7]</b>
                                        Evangelopoulos G N.Efficient hardware mapping of long short-term memory neural networks for automatic speech recognition[D].Belgium:KU Leuven, 2016
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_8" title="Liu Beiye, Li Hai, Chen Yiran, et al.Vortex:Variation-aware training for memristor x-bar[C]// Proc of the 52nd ACM/EDAC/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2015:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vortex:Variation-aware training for memristor x-bar">
                                        <b>[8]</b>
                                        Liu Beiye, Li Hai, Chen Yiran, et al.Vortex:Variation-aware training for memristor x-bar[C]// Proc of the 52nd ACM/EDAC/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2015:1- 6
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_9" title="Tang Tianqi, Xia Lixue, Li Boxun, et al.Binary convolutional neural network on RRAM[C] //Proc of the 22nd Asia and South Pacific Design Automation Conf.Piscataway, NJ:IEEE, 2017:782- 787" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Binary convolutional neural network on RRAM">
                                        <b>[9]</b>
                                        Tang Tianqi, Xia Lixue, Li Boxun, et al.Binary convolutional neural network on RRAM[C] //Proc of the 22nd Asia and South Pacific Design Automation Conf.Piscataway, NJ:IEEE, 2017:782- 787
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_10" title="Song Linghao, Qian Xuehai, Li Hai, et al.PipeLayer:A pipelined ReRAM-based accelerator for deep learning[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture.Piscataway, NJ:IEEE, 2017:541- 552" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PipeL ayer:Apipelined reR AM-based accelerator for deep learning">
                                        <b>[10]</b>
                                        Song Linghao, Qian Xuehai, Li Hai, et al.PipeLayer:A pipelined ReRAM-based accelerator for deep learning[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture.Piscataway, NJ:IEEE, 2017:541- 552
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_11" title="Dong Xiangyu, Xu Cong, Xie Yuan, et al.NVSim:A circuit-level performance, energy, and area model for emerging nonvolatile memory[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2012, 31 (7) :994- 1007" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory">
                                        <b>[11]</b>
                                        Dong Xiangyu, Xu Cong, Xie Yuan, et al.NVSim:A circuit-level performance, energy, and area model for emerging nonvolatile memory[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2012, 31 (7) :994- 1007
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_12" title="Xu Sheng, Chen Xiaoming, Wang Ying, et al.PIMSim:A flexible and detailed processing-in-memory simulator[J].IEEE Computer Architecture Letters, 2018, 18 (1) :6- 9" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PIMSim:A flexible and detailed processing-in-memory simulator">
                                        <b>[12]</b>
                                        Xu Sheng, Chen Xiaoming, Wang Ying, et al.PIMSim:A flexible and detailed processing-in-memory simulator[J].IEEE Computer Architecture Letters, 2018, 18 (1) :6- 9
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_13" title="Chen Paiyu, Peng Xiaochen, Yu Shimeng.NeuroSim:A circuit-level macro model for benchmarking neuro-inspired architectures in online learning[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (12) :3067- 3080" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NeuroSim:A circuit-level macro model for benchmarking neuro-inspired architectures in online learning">
                                        <b>[13]</b>
                                        Chen Paiyu, Peng Xiaochen, Yu Shimeng.NeuroSim:A circuit-level macro model for benchmarking neuro-inspired architectures in online learning[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (12) :3067- 3080
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_14" title="Xia Lixue, Li Boxun, Tang Tianqi, et al.MNSIM:Simulation platform for memristor-based neuromorphic computing system[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (5) :1009- 1022" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MNSIM:Simulation platform for memristor-based neuromorphic computing system">
                                        <b>[14]</b>
                                        Xia Lixue, Li Boxun, Tang Tianqi, et al.MNSIM:Simulation platform for memristor-based neuromorphic computing system[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (5) :1009- 1022
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_15" title="Yao Peng, Wu Huaqiang, Gao Bin, et al.Face classification using electronic synapses[J].Nature Communications, 2017, 8:15199" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face classification using electronic synapses.">
                                        <b>[15]</b>
                                        Yao Peng, Wu Huaqiang, Gao Bin, et al.Face classification using electronic synapses[J].Nature Communications, 2017, 8:15199
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_16" title="Muralimanohar N, Balasubramonian R, Jouppi N P.Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0[C] //Proc of the 40th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO-40 2007) .Piscataway, NJ:IEEE, 2007:3- 14" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Opti-mizing NUCA organizations and wiring alternatives for large caches with CACTI6.0">
                                        <b>[16]</b>
                                        Muralimanohar N, Balasubramonian R, Jouppi N P.Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0[C] //Proc of the 40th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO-40 2007) .Piscataway, NJ:IEEE, 2007:3- 14
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_17" title="Long Yun, Na T, Mukhopadhyay S.ReRAM-based processing-in-memory architecture for recurrent neural network acceleration[J].IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2018, 26 (12) :2781- 2794" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ReRAM-based processing-in-memory architecture for recurrent neural network acceleration">
                                        <b>[17]</b>
                                        Long Yun, Na T, Mukhopadhyay S.ReRAM-based processing-in-memory architecture for recurrent neural network acceleration[J].IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2018, 26 (12) :2781- 2794
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_18" title="Gu Peng, Li Boxun, Tang Tianqi, et al.Technological exploration of RRAM crossbar array for matrix-vector multiplication[J].Journal of Computer Science and Technology, 2016, 31 (1) :3- 19" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Technological exploration of RRAM crossbar array for matrix-vector multiplication">
                                        <b>[18]</b>
                                        Gu Peng, Li Boxun, Tang Tianqi, et al.Technological exploration of RRAM crossbar array for matrix-vector multiplication[J].Journal of Computer Science and Technology, 2016, 31 (1) :3- 19
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_19" title="Lee S R, Kim Y B, Chang M, et al.Multi-level switching of triple-layered TaOx RRAM with excellent reliability for storage class memory[C] //Proc of 2012 Symp on VLSI Technology.Piscataway, NJ:IEEE, 2012:71- 72" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-level switching of triple-layered TaOx RRAMwith excellent reliability for storage class memory">
                                        <b>[19]</b>
                                        Lee S R, Kim Y B, Chang M, et al.Multi-level switching of triple-layered TaOx RRAM with excellent reliability for storage class memory[C] //Proc of 2012 Symp on VLSI Technology.Piscataway, NJ:IEEE, 2012:71- 72
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(06),1182-1191 DOI:10.7544/issn1000-1239.2019.20190113            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向阻变存储器的长短期记忆网络加速器的训练和软件仿真</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E9%B9%A4&amp;code=39925620&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘鹤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%A3%E5%AE%87&amp;code=34769079&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">季宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E5%BB%BA%E8%BE%89&amp;code=42095398&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩建辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%82%A0%E6%85%A7&amp;code=08186539&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张悠慧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E7%BA%AC%E6%B0%91&amp;code=05965723&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑纬民</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学计算机科学与技术系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E5%BE%AE%E7%94%B5%E5%AD%90%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学微电子学研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>长短期记忆 (long short-term memory, LSTM) 网络是一种循环神经网络, 其擅长处理和预测时间序列中间隔和延迟较长的事件, 多用于语音识别、机器翻译等领域.然而受限于内存带宽的限制, 现今的多数神经网络加速器件的计算模式并不能高效处理长短期记忆网络计算;而阻变存储器交叉开关结构能够以存内计算形式完成高效、高密度的向量矩阵乘运算, 从而成为一种高效处理长短期记忆网络的极具潜力的加速器设计模式.研究了面向阻变存储器的长短期记忆神经网络加速器模拟工具以及相应的神经网络训练算法.该模拟工具能够以时钟驱动的形式模拟设计者提出的以阻变存储器交叉开关结构为核心加速部件的长短期记忆加速器微体系结构, 从而进行设计空间探索;同时改进了神经网络训练算法以适应阻变存储器特性.这一模拟工具基于System-C实现, 且对于核心计算部分实现了图形处理器加速, 可以提高阻变存储器器件的仿真速度, 为探索设计空间提供便利.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%98%BB%E5%8F%98%E5%AD%98%E5%82%A8%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">阻变存储器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短期记忆网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">训练算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%BF%E7%9C%9F%E6%A1%86%E6%9E%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">仿真框架;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘鹤, liuhe94@hotmail.com, born in 1994.Master candidate.His main research interests include simulation framework for neural network accelerator.;
                                </span>
                                <span>
                                    *张悠慧, zyh02@tsinghua.edu.cn, born in 1976.Professor and PhD supervisor.Member of CCF, ACM and IEEE.His main research interests include computer architecture and neuromorphic computing.;
                                </span>
                                <span>
                                    Ji Yu, born in 1993.PhD candidate.His main research interests include neural network accelerator and compiler and machine learning for system optimization.;
                                </span>
                                <span>
                                    Han Jianhui, born in 1994.PhD candidate.His main research interests include emerging technology-based machine learning accelerator design.;
                                </span>
                                <span>
                                    Zheng Weimin, born in 1946.Professor and PhD supervisor.His main research interests include high performance computing, network storage and parallel compiler.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国防科技创新特区项目;</span>
                    </p>
            </div>
                    <h1><b>Training and Software Simulation for ReRAM-Based LSTM Neural Network Acceleration</b></h1>
                    <h2>
                    <span>Liu He</span>
                    <span>Ji Yu</span>
                    <span>Han Jianhui</span>
                    <span>Zhang Youhui</span>
                    <span>Zheng Weimin</span>
            </h2>
                    <h2>
                    <span>Deparment of Computer Science and Technology, Tsinghua University</span>
                    <span>Institute of Microelectronics, Tsinghua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Long short-term memory (LSTM) is mostly used in fields of speech recognition, machine translation, etc., owing to its expertise in processing and predicting events with long intervals and long delays in time series. However, most of existing neural network acceleration chips cannot perform LSTM computation efficiently, as limited by the low memory bandwidth. ReRAM-based crossbars, on the other hand, can process matrix-vector multiplication efficiently due to its characteristic of processing in memory (PIM) . However, a software tool of broad architectural exploration and end-to-end evaluation for ReRAM-based LSTM acceleration is still missing. This paper proposes a simulator for ReRAM-based LSTM neural network acceleration and a corresponding training algorithm. Main features (including imperfections) of ReRAM devices and circuits are reflected by the highly configurable tools, and the core computation of simulation can be accelerated by general-purpose graphics processing unit (GPGPU) . Moreover, the core component of simulator has been verified by the corresponding circuit simulation of a real chip design. Within this framework, architectural exploration and comprehensive end-to-end evaluation can be achieved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ReRAM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ReRAM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=long%20short-term%20memory%20(LSTM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">long short-term memory (LSTM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=training%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">training algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=simulation%20framework&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">simulation framework;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Science and Technology Innovation Special Zone Project;</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="48">长短期记忆 (long short-term memory, LSTM) 神经网络擅长处理和预测时间序列中间隔和延迟很长的事件, 因此多用于语音识别、机器翻译、控制机器人、合成音乐等领域.LSTM的推断计算大概占用了谷歌数据中心的30%的工作负载<citation id="228" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.然而, LSTM网络在传统的神经网络加速器上的计算效率却很低.举例来说, 对于TPU<citation id="229" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>而言, 相较于卷积神经网络 (convolutional neural network, CNN) 的86 Tops/s的速度, LSTM神经网络的速度只能达到2.8Tops/s.其主要原因在于LSTM的访存更为密集, 因此其计算性能受到内存带宽的严格限制.对于GPU<citation id="230" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和FPGA<citation id="231" type="reference"><link href="194" rel="bibliography" /><link href="196" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>等加速器件, 也有类似的结论.</p>
                </div>
                <div class="p1">
                    <p id="49">为了解决内存瓶颈对计算性能的影响, 用ReRAM器件可作为神经网络加速器件的计算核心.非易失性阻变存储器 (resistive random-access memory, ReRAM) 是新一代的存储器件, 其阻值可动态设置.利用此器件的物理特性, 可以将ReRAM制作成交叉开关阵列 (Crossbar) 进行神经网络所需的矩阵向量乘法计算.其计算原理是基尔霍夫定律, 计算速度快、功耗低, 并且避免了一般神经网络加速器计算期间的权重数据移动, 很大程度上削弱了内存带宽对计算效率的限制.</p>
                </div>
                <div class="p1">
                    <p id="50">由于ReRAM Crossbar本身是模拟电路, 相对于一般的数字电路设计有更多的约束条件, 为此本文提出了定制化的训练算法, 使得LSTM网络在训练时就充分考虑了相应的硬件约束, 能够更好地反映推断计算的真实运行环境.同时本文还完成了一个针对ReRAM器件的System-C模拟器, 采用行为级电路模拟技术避免了现有的基于SPICE的ReRAM电路仿真的复杂计算, 可以大幅度提高模拟运行速度.相对于SPICE仿真, 该模拟器引入的误差不超过2.68%<citation id="253" type="note"><link href="251" rel="footnote" /><sup>①</sup></citation>;且核心计算部分实现了GPU加速, 大幅提高了模拟速度, 为探索设计空间提供了便利.</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag"><b>1 背景与相关工作</b></h3>
                <h4 class="anchor-tag" id="52" name="52"><b>1.1 ReRAM</b></h4>
                <div class="p1">
                    <p id="53">ReRAM是新一代非易失性存储器件, 其具有非易失性、高密度、低功耗、存算合一、易于3D堆叠等特点.在神经网络计算中可以利用其阻值可调的特点作为突触器件, 并利用其存算合一的优点以Crossbar形式完成高速向量矩阵乘运算.</p>
                </div>
                <div class="p1">
                    <p id="54">具体的计算方式为: ReRAM是一种多级 (multi-level) 内存设备, 理论上可以将ReRAM的电导值设为其取值范围中的任意值.我们可以将ReRAM在Crossbar交叉点的阻值设置为神经网络权重矩阵的对应值, 这样就可以用一个ReRAM Crossbar来表示一个神经网络的权重矩阵, 并且根据其物理特性就地 (in-situ) 完成矩阵向量乘操作.对于理想的Crossbar器件, 将输入电压<i>V</i><sub><i>i</i></sub>加到每一行字线 (word-line) , 电压与每一交叉点的电导值<i>G</i><sub><i>ij</i></sub>相乘, 根据基尔霍夫定律<i>I</i>=<i>GV</i>, 每一列的输出电流<i>I</i>可累加得到.该计算过程能达到很高的并行度和速度, 过程如图1所示:</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906007_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 ReRAM 交叉开关阵列" src="Detail/GetImg?filename=images/JFYZ201906007_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 ReRAM 交叉开关阵列  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906007_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 ReRAM Crossbar</p>

                </div>
                <div class="p1">
                    <p id="56">但实际上基于ReRAM Crossbar的计算并不像上文提到的理想情况那样, 主要包含2种非理想因素, 分别是设备Variation和周期Variation.设备Variation主要反映在多个ReRAM单元之间的参数变动, 即当我们设置多个ReRAM单元的电导值至某一个给定值时, 其实际设置的结果值并不准确地等于预期值, 而是服从预期值附近的一个数值分布.周期级的Variation则表示单个ReRAM单元的不同次操作之间的电导值并不完全一致.</p>
                </div>
                <div class="p1">
                    <p id="57">具体地, 在设置ReRAM电导值的过程中, 一个单元的电导值的变化受极性、磁性和输入电压等的影响.同时一些非理想情况, 如电导值突变和波动会在反复循环设置电导的过程中出现, 因此需要写验证电路来验证所设置的电导值是否符合要求.通常的设置过程如下:将电压脉冲加到ReRAM单元上来增大 (或减小) 单元的电导值, 直到该电导值大于 (或小于) 期望的电导值.如果实际电导值与期望的电导值相差过大, 则需要进行相反的操作来减小 (或增大) 电导值.同时由于器件不完美特性和设置过程开销 (用于验证电路的参考电压数量有限) , 实际可用的电导取值通常是离散值.这是当前很多基于ReRAM进行神经网络计算加速的研究工作使用离散电导值的一个原因.另一个原因在于, 使用离散的权重值有利于提升神经网络的抗噪能力.</p>
                </div>
                <div class="p1">
                    <p id="58">由于整个加速芯片系统的主要部分仍是数字电路, 因此数模转换 (模数转换) , 即DAC/ADC, 是很大的硬件开销.在配置DAC/ADC的时候, 需要综合考虑神经网络的准确率和硬件开销的均衡.</p>
                </div>
                <div class="p1">
                    <p id="59">数模转换器 (digital analog converter, DAC) 的作用是将Crossbar每一行的数字输入向量转换成模拟信号.其重要参数是转换分辨率<i>d</i>, 即每次可转换的信号位数.在电路模块中, DAC要占用不小的硬件面积, 因此很多设计 (如ISAAC<citation id="232" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等) 将<i>S</i>-bit宽度的数字输入信号先转换成一系列低精度的数字信号 (通常精度是宽度为1, 即<i>d</i>=1) , 再经过DAC;这也意味着Crossbar要进行 (<i>S</i>/<i>d</i>) 次的顺序操作, 这是一种用时间换空间的优化策略.DAC的个数设为2<sup><i>r</i></sup>×<i>C</i>.</p>
                </div>
                <div class="p1">
                    <p id="60">模数转换器 (analog digital converter, ADC) 的作用是将Crossbar的每一列电流输出转换成数字信号, 其传感分辨率为<i>a</i>.为了减小硬件开销, 通常使用时分复用的方式, 即每个Crossbar列的输出依次进入到ADC中, ADC将其逐个转换成输出信号.每个Crossbar的ADC的数量是<i>A</i>.</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>1.2 LSTM神经网络</b></h4>
                <div class="p1">
                    <p id="62">LSTM<citation id="233" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>网络是一种带门结构的循环神经网络 (recurrent neural network, RNN) , 适合于处理和预测时间序列中间隔和延迟较长的重要事件.</p>
                </div>
                <div class="p1">
                    <p id="63">LSTM网络引入了判断旧有信息是否有用的单元 (cell) .Cell的构成如图2所示:</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 LSTM计算单元[7]" src="Detail/GetImg?filename=images/JFYZ201906007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 LSTM计算单元<citation id="234" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 LSTM Cell<citation id="235" type="reference"><link href="202" rel="bibliography" /><sup>[7]</sup></citation><sup></sup></p>

                </div>
                <div class="p1">
                    <p id="65">遗忘门 (forget gate) 控制当前输入<b><i>x</i></b><sub><i>t</i></sub>对来自之前Cell状态删除信息的影响程度:</p>
                </div>
                <div class="p1">
                    <p id="66"><b><i>f</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub>for</sub>·<b><i>x</i></b><sub><i>t</i></sub>+<b><i>R</i></b><sub>for</sub>·<b><i>h</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sub>for</sub>) .      (1) </p>
                </div>
                <div class="p1">
                    <p id="67">输入门 (input gate) 控制当前输入<b><i>x</i></b><sub><i>t</i></sub>对当前Cell状态中新信息的增量的影响程度:</p>
                </div>
                <div class="p1">
                    <p id="68"><b><i>i</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub>in</sub>·<b><i>x</i></b><sub><i>t</i></sub>+<b><i>R</i></b><sub>in</sub>·<b><i>h</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sub>in</sub>) .      (2) </p>
                </div>
                <div class="p1">
                    <p id="69">输出门 (output gate) 控制当前输入<b><i>x</i></b><sub><i>t</i></sub>对当前网络输出的直接影响:</p>
                </div>
                <div class="p1">
                    <p id="70"><b><i>o</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub>out</sub>·<b><i>x</i></b><sub><i>t</i></sub>+<b><i>R</i></b><sub>out</sub>·<b><i>h</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sub>out</sub>) .      (3) </p>
                </div>
                <div class="p1">
                    <p id="71">候选阶段门 (candidate stage gate) 代表当前输入<b><i>x</i></b><sub><i>t</i></sub>所创建的新的信息:</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>.      (4) </p>
                </div>
                <div class="p1">
                    <p id="74">当前细胞状态 (current cell state) 是需要被遗忘和需要被吸收的内容的组合, 其计算模式⨂表示逐点乘:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊗</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊗</mo><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>.      (5) </p>
                </div>
                <div class="p1">
                    <p id="77">网络输出 (network output) 是输入<b><i>x</i></b><sub><i>t</i></sub>与当前细胞状态的组合, 其计算模式为逐点乘:</p>
                </div>
                <div class="p1">
                    <p id="78"><b><i>h</i></b><sub><i>t</i></sub>=<b><i>o</i></b><sub><i>t</i></sub>⨂tanh (<b><i>C</i></b><sub><i>t</i></sub>) .      (6) </p>
                </div>
                <div class="p1">
                    <p id="79">式 (1) ～ (6) 中, <b><i>x</i></b><sub><i>t</i></sub>为当前输入向量, <b><i>h</i></b><sub><i>t</i></sub>为当前LSTM的输出, <b><i>h</i></b><sub><i>t</i>-1</sub>为前一时刻LSTM的输出.<b><i>f</i></b><sub><i>t</i></sub>, <b><i>i</i></b><sub><i>t</i></sub>, <b><i>o</i></b><sub><i>t</i></sub>分别为遗忘门、输入门和输出门的输出, <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>为候选状态门的输出, <b><i>C</i></b><sub><i>t</i></sub>为当前细胞状态, <b><i>C</i></b><sub><i>t</i>-1</sub>为前一时刻细胞状态.<b><i>W</i></b><sub>for</sub>, <b><i>W</i></b><sub>in</sub>, <b><i>W</i></b><sub>out</sub>, <b><i>W</i></b><sub>can</sub>分别是对应门的输入权重.<b><i>R</i></b><sub>for</sub>, <b><i>R</i></b><sub>in</sub>, <b><i>R</i></b><sub>out</sub>, <b><i>R</i></b><sub>can</sub>分别是对应门的递归权重.<b><i>b</i></b><sub>for</sub>, <b><i>b</i></b><sub>in</sub>, <b><i>b</i></b><sub>out</sub>, <b><i>b</i></b><sub>can</sub>分别是对应门的偏置权重.</p>
                </div>
                <div class="p1">
                    <p id="81">在实际的神经网络计算过程中, 由于LSTM各门计算均为矩阵向量乘操作, 因此常常将<b><i>f</i></b><sub><i>t</i></sub>, <b><i>i</i></b><sub><i>t</i></sub>, <b><i>o</i></b><sub><i>t</i></sub>, <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>这4个门的计算置于一次矩阵运算之中, 以减少计算的调用次数, 提高计算的效能.在矩阵运算之后, 再分别进行对应的激活函数和逐点乘法的操作.</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>1.3 相关工作</b></h4>
                <div class="p1">
                    <p id="84">在训练层面, 文献<citation id="236" type="reference">[<a class="sup">8</a>]</citation>提出了一种通过补偿设备变动的方式来增强鲁棒性的训练方案.文献<citation id="237" type="reference">[<a class="sup">9</a>]</citation>使用二值网络训练来降低ReRAM表示能力的需求.在基于ReRAM的神经网络加速器设计方面也有一些相关工作.ISAAC<citation id="238" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>是一个使用Crossbar的CNN加速器, 提出了流水线化的微体系结构和新的数据编码技术.PipeLayer<citation id="239" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是一个面向CNN的基于ReRAM的内存计算 (processing in memory, PIM) 加速器, 可以支持训练和推断计算.</p>
                </div>
                <div class="p1">
                    <p id="85">在模拟器层面, 尚未有研究提出一个开放的模拟框架.NVSim<citation id="240" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对器件面积、时序、动态能量等非易失性存储技术进行建模, 但不是针对神经网络加速计算.PIMSim<citation id="241" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是一个内存计算的模拟器, 用于传统的内存技术而不支持内存和计算的本地化.NeuroSim<citation id="242" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>是一个面向神经网络的基于非易失性存储阵列结构的集成模拟框架, 但是其目标用户是希望利用自身的模拟突触设备快速评估系统级性能的设备工程师.该模拟架构结构简单 (仅支持针对MNIST手写体识别数据集的2层MLP) 并且缺乏相应的训练和映射软件.MNSIM<citation id="243" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>是一个针对基于ReRAM的神经形态系统的仿真平台, 包括一个层次化的神经形态计算加速器结构和可以灵活定制的接口, 以及一个行为级的计算准确度模型.但是MNSIM只实现了仿真功能而不支持实际神经网络应用的评估.因此, 本文是第1个开源的针对LSTM的基于ReRAM的神经网络加速的训练和仿真平台.</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag"><b>2 针对ReRAM特性与加速器结构的LSTM神经网络训练</b></h3>
                <h4 class="anchor-tag" id="87" name="87"><b>2.1 神经网络权重量化</b></h4>
                <div class="p1">
                    <p id="88">神经网络权重量化 (quantization) 是神经网络的常用压缩算法.一般认为, 神经网络参数过多、计算过于密集, 因此需要通过压缩算法以降低神经网络的有效权重数量, 这样既可以减少参数, 又能简化计算;同时也能够降低数值表示精度, 即用低精度计算来代替高精度浮点数计算, 达到降低开销的目的.本文采用神经网络权重量化的出发点, 主要是从ReRAM器件特性的角度来考虑.</p>
                </div>
                <div class="p1">
                    <p id="89">正如1.1节介绍的, ReRAM器件的优势在于速度快、功耗低、并行性非常好, 但是由于当前工艺不够成熟以及器件本身的不完美特性, 其数值表示精度低 (每个单元所能表示的权重的离散值范围与个数都有限) 且受器件噪声的影响较大, 因此需要在训练阶段对神经网络权重进行量化, 使得神经网络能够符合实际器件的约束.其主要思想是将原有的权重进行放缩至器件精度范围如2<sup>8</sup>, 然后四舍五入为整数, 并用该整数来表示权重值.</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.2 层间I/O精度限制</b></h4>
                <div class="p1">
                    <p id="91">在层间数据的传输时, 数据的精度也常常受到硬件开销的限制.若该层间传输的精度限制为<i>S</i>-bit, 权重精度为<i>b</i>-bit, 输入的行数有2<sup><i>c</i></sup>行, 则在Crossbar中计算时, 其结果数据的数值范围为2<sup><i>S</i></sup>×2<sup><i>b</i></sup>×2<sup><i>c</i></sup>=2<sup><i>S</i>+<i>b</i>+<i>c</i></sup>, 即输出的结果精度为 (<i>S</i>+<i>b</i>+<i>c</i>) -bit, 该结果需要被截取到层间数据传输的限制<i>S</i>-bit.截取策略一般有2种, 最直接的方法是对输出结果等比例放缩到<i>S</i>-bit.这种方法的弊端是, 当结果分布不均匀的时候会有较大的精度损失.另一种策略是截取结果中连续的<i>S</i>-bit作为最终结果, 截取的起始位置则由数据分布来确定.</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>2.3 DAC与ADC及移位相加</b></h4>
                <div class="p1">
                    <p id="93">在训练阶段还需要考虑ReRAM加速器所需的DAC和ADC以及移位相加 (shift-add) 对神经网络的影响.因此在训练的每次矩阵向量乘计算之前, 需要将原有的输入值按位进行拆分, 即将原有的1个8-bit输入向量拆分为8个1-bit输入向量, 并将其合并成新的输入矩阵 (训练阶段以此来加速训练过程) , 以此操作模拟DAC的转换过程.</p>
                </div>
                <div class="p1">
                    <p id="94">在训练阶段的矩阵向量乘计算之后, 首先需要统计该层所有输出结果的最大值, 并根据此最大值确定ADC的参考电流 (该参考电流值也将用于模拟器的ADC模块) .在确定了该层ADC的参考电流之后, 再将输出结果按照DAC的顺序, 每8个行向量移位相加成1个行向量 (即将扩展了8倍大小的输出矩阵还原成DAC拆分之前的同一层输出大小) .</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>2.4 ReRAM噪声分布</b></h4>
                <div class="p1">
                    <p id="96">ReRAM器件的噪声是指每个单元的噪声, 其大小与单元的电导值 (即神经网络权重) 有关.每个单元的实际电导值为均值为单元目标电导值的正态分布, 则每个单元的噪音为均值为0、标准差受电导值影响的正态分布:<i>G</i><sub>noise</sub>=<i>N</i> (0, <i>δ</i><sup>2</sup>) .本文采用的ReRAM器件标准差<i>δ</i>符合 (由实际测量值拟合得出<citation id="244" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation><sup>) </sup>:</p>
                </div>
                <div class="p1">
                    <p id="97"><i>δ</i>=-0.000 603 4<i>x</i><sup>2</sup>+0.061 84<i>x</i>+0.724 0,      (7) </p>
                </div>
                <div class="p1">
                    <p id="98">其中, <i>x</i>是ReRAM单元的电导值.在Crossbar计算的过程中, 由于每次读取权重都会有偏差 (ReRAM特性) , 因此在以上正态分布的约束下, 每次计算都需要重新生成<i>G</i><sub>noise</sub>值, 并将其加在ReRAM单元的原有电导值上, 作为新的读出值.</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.5 针对ReRAM特性的神经网络训练算法</b></h4>
                <div class="p1">
                    <p id="100">在神经网络训练时, 为了快速收敛, 首先按照经典神经网络训练方法进行训练, 得到高精度模型, 在此基础上, 使用本文提出的定制化的训练算法, 训练得到符合ReRAM约束的神经网络模型.综合上文提到的ReRAM特性, 本文提出的神经网络训练算法如下:</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">1) 前向过程 (forward pass) </h4>
                <div class="p1">
                    <p id="102">步骤1. 神经网络权重8-bit量化.针对各神经网络层, 分别选取各层的最大值max (<b><i>W</i></b>) ;保存现有权重为<b><i>W</i></b><sub>old</sub>;计算<b><i>W</i></b><sub>new</sub>:</p>
                </div>
                <div class="p1">
                    <p id="103"><b><i>W</i></b><sub>new</sub>=<i>round</i> (<b><i>W</i></b><sub>old</sub>/max (<b><i>W</i></b>) × (2<sup>7</sup>-1) ) / (2<sup>7</sup>-1) ×max (<b><i>W</i></b>) .      (8) </p>
                </div>
                <div class="p1">
                    <p id="105">步骤2. 层间I/O精度8-bit限制.对每一层输入<b><i>input</i></b>, 分别选取最大值max (<b><i>input</i></b>) , 继而计算出最大值位数<i>n</i><sub>max</sub>=<i>ceil</i> (lb (max (<b><i>input</i></b>) ) ) ;对<b><i>input</i></b>进行缩放:<b><i>input</i></b><sub>new</sub>=<i>round</i> (<b><i>input</i></b>×2<sup>8-<i>n</i><sub>max</sub></sup>) .</p>
                </div>
                <div class="p1">
                    <p id="106">步骤3. DAC转换.将步骤2中获得的<b><i>input</i></b><sub>new</sub>进行按位拆分, 生成新的1-bit向量.工作流程如图3所示.</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906007_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 DAC工作流程图" src="Detail/GetImg?filename=images/JFYZ201906007_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 DAC工作流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906007_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 DAC Workflow</p>

                </div>
                <div class="p1">
                    <p id="108">步骤4. 计算并添加噪音.首先生成标准正态分布噪音<b><i>normal</i></b>, 进一步根据式 (7) 得到标准差为<i>δ</i>的<b><i>V</i></b><sub>noise</sub>;将<b><i>V</i></b><sub>noise</sub>加到现有<b><i>W</i></b><sub>new</sub>上, 得到带噪音的权重值<b><i>W</i></b><sub>noise</sub>.将DAC生成的1-bit向量与<b><i>W</i></b><sub>noise</sub>相乘, 得到输出<b><i>output</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="109">步骤5. ADC与shift-add.首先确定各层参考电压<i>AD</i>_<i>V</i>, 其值为每层<b><i>output</i></b>的最大值, 即<i>AD</i>_<i>V</i>=max (<b><i>output</i></b>) .将每个<b><i>output</i></b>值转换成8-bit向量: <b><i>output</i></b><sub><i>t</i></sub>=<i>round</i> (<b><i>output</i></b>/<i>AD</i>_<i>V</i>× (2<sup>7</sup>-1) ) .按照DAC拆分的顺序, 每8个<b><i>output</i></b><sub><i>t</i><sub><i>i</i></sub></sub>进行移位相加:<b><i>res</i></b>=<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>7</mn></munderover><mi mathvariant="bold-italic">o</mi></mstyle><mi mathvariant="bold-italic">u</mi><mi mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">u</mi><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>×</mo><mn>2</mn><msup><mrow></mrow><mi>i</mi></msup></mrow></math></mathml>.<b><i>res</i></b>为最终输出结果向量.</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">2) 反向过程 (backward pass) </h4>
                <div class="p1">
                    <p id="112">步骤1. 将现有网络权重值<b><i>W</i></b><sub>noise</sub>恢复为<b><i>W</i></b><sub>old</sub>.</p>
                </div>
                <div class="p1">
                    <p id="113">步骤2. 使用反向传播算法 (backpropagation) 更新权重值.</p>
                </div>
                <div class="p1">
                    <p id="114">以上为本文提出的针对ReRAM特性的神经网络训练算法.利用此算法可以训练得到符合ReRAM约束的神经网络模型.</p>
                </div>
                <h3 id="115" name="115" class="anchor-tag"><b>3 模拟器架构</b></h3>
                <h4 class="anchor-tag" id="116" name="116"><b>3.1 整体架构</b></h4>
                <div class="p1">
                    <p id="117">模拟器的主要计算模块为LSTM模块和Linear模块, 分别负责LSTM网络和全连接网络的计算.除了计算模块, 还有数据输入和输出模块, 以及相应的数据缓冲区模块 (LSTM模块的缓冲区与全连接模块的缓冲区结构相似, 仅有规模参数不同, 因此视为同类模块) .如图4所示, Linear模块中包括DAC、若干ReRAM计算阵列、ADC和shift-add以及激活函数, 其输出值将发送给下一层数据缓冲区 (或者作为模拟器的输出结果) .LSTM模块的特殊性在于, 其激活函数为非线性函数Sigmoid和tanh (而非Linear和CNN中常用的ReLU) , 并且需要引入新的控制和计算功能——向量拼接和逐点乘.</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906007_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 整体架构" src="Detail/GetImg?filename=images/JFYZ201906007_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 整体架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906007_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Overall architecture</p>

                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>3.2 处理单元</b></h4>
                <h4 class="anchor-tag" id="120" name="120">1) DAC模块</h4>
                <div class="p1">
                    <p id="121">DAC模块的主要作用是将输入数据 (8-bit) 逐位转换成1-bit数据, 即将一个8-bit向量转换成8个1-bit向量, 依次输送给Crossbar模块进行计算.如图4所示, 对于4个8-bit (层间I/O限制) 输入数据, 对其bit位进行拆分, 每个8-bit输入数据按照从高位到低位拆分成8个1-bit数据;再按照高低位次序, 分8次输入到ReRAM Crossbar中进行计算, 因此Crossbar中的计算需要8个时间步完成.</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">2) Crossbar模块</h4>
                <div class="p1">
                    <p id="123">如图1所示, 将一列输入电压与ReRAM单元的电导值 (权重值) 相乘加得到输出电流.在此过程中, 同时需考虑2.4节提到的ReRAM单元的噪音, 即对每一个ReRAM单元, 其计算电流值为<i>I</i><sub><i>ij</i></sub>=<i>V</i><sub><i>i</i></sub>× (<i>G</i><sub><i>ij</i></sub>+<i>noise</i><sub><i>ij</i></sub>) .Crossbar中的权重值经预先训练和转换得到, 在模拟器运行启动时读入.Crossbar模块的大小预先设置为1 152×128, 其参数也是可调的.</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">3) ADC与shift-add模块</h4>
                <div class="p1">
                    <p id="125">本模块的主要作用是将Crossbar模块的计算输出转换成数字电路数值, 并且根据DAC转换的高低位依次进行移位相加.在ADC的运行之前, 需要预先确定参考电流<i>I</i>.其作用是当ADC的输入电流<i>i</i>&gt;<i>I</i>或<i>i</i>&lt;-<i>I</i>时, 要将其截取为<i>I</i>或-<i>I</i>;否则, 保持<i>i</i>不变.然后将<i>i</i>放缩到8-bit的区间 (-127, 127) , 该值为ADC的输出值<i>a</i>.在经过ADC之后, 根据DAC转换的高低位, 将<i>a</i>依次左移相加, 其计算公式为<i>result</i>=<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>7</mn></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mn>2</mn><msup><mrow></mrow><mi>i</mi></msup></mrow></math></mathml>, 其中<i>a</i><sub><i>i</i></sub>为第<i>i</i>个ADC输出值.累加结果<i>result</i>即为本模块的计算结果.</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">4) LSTM模块</h4>
                <div class="p1">
                    <p id="128">在LSTM模块中, 集成了从上层获取数据、DAC转换、Crossbar的计算、ADC转换这4部分.除此之外, 由于LSTM网络计算的特殊性, 还需要引入逐点乘操作和非线性 (激活) 函数.另外还需要在模拟器中开辟专门的存储空间来存放本模块循环输入给下一个时间步的结果<b><i>C</i></b><sub><i>t</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="129">与传统的CNN计算方式不同, LSTM模块的计算需要对数据进行连接, 即将输入数据<b><i>x</i></b><sub><i>t</i></sub>与隐层状态<b><i>h</i></b><sub><i>t</i>-1</sub> (同时也是上一个时间步<i>t</i>-1时刻的LSTM模块的输出) 拼接成一个新的向量.受此影响, ReRAM Crossbar中的权重在映射时也需要考虑将各门权重值进行拼接.如图5所示, 在Crossbar计算和ADC转换之后, 得到的中间结果为<b><i>f</i></b><sub><i>t</i></sub>, <b><i>i</i></b><sub><i>t</i></sub>, <b><i>o</i></b><sub><i>t</i></sub>, <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>各门的输出.该输出结果需要进一步进行逐点乘操作和激活函数计算.逐点乘的操作模式为2个向量对应位相乘, 得到新的同维度的输出向量.LSTM模块中的激活函数为Sigmoid和tanh, 都是非线性函数, 直接使用数值计算的方式使其计算开销很大.考虑到Crossbar计算过程中数据的精度一般限制为8-bit, 因此在激活函数的实现上可以使用查表的方式 (表项不超过2<sup>8</sup>个) .</p>
                </div>
                <div class="p1">
                    <p id="131">经过激活函数的计算后得出的结果<b><i>h</i></b><sub><i>t</i></sub>作为下一时刻本层的输入, <b><i>C</i></b><sub><i>t</i></sub>作为当前细胞状态值用于计算下一时刻的细胞状态, 因此需要在本层中存入相应的数据缓冲区, 待下一时刻本层计算<b><i>C</i></b><sub><i>t</i>+1</sub>时使用.</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906007_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 LSTM模块的计算模式" src="Detail/GetImg?filename=images/JFYZ201906007_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 LSTM模块的计算模式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906007_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Computation Mode in LSTM module</p>

                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>3.3 GPU加速计算</b></h4>
                <div class="p1">
                    <p id="134">模拟器的主要计算部分为矩阵向量乘, 同时还要在每次进行Crossbar计算时生成满足高斯分布的噪音 (需要生成随机数) , 所以使用CPU进行计算时其时间开销很大.而GPU作为SIMD硬件, 其计算并行性很高, 适合做矩阵相乘的操作, 目前也多用于神经网络计算, 具有良好的加速效果.因此我们考虑使用GPU来加速模拟器, 即将Crossbar的主要计算部分由GPU来完成, 包括ReRAM单元噪音的生成.在GPU计算时, 通过编写cuda代码, 调用用于生成随机数的curand库生成符合条件的高斯分布噪声, 并将其加到对应的ReRAM单元的电导值上.加速算法的步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="135">步骤1. 初始化权重<b><i>W</i></b>.将所有层Crossbar的权重加载到GPU内存中, 并根据权重值计算该层权重对应的噪音标准差<i>δ</i>.</p>
                </div>
                <div class="p1">
                    <p id="136">步骤2. 在System-C模拟器中, 将每层输入拼接成1个大输入向量<b><i>input</i></b>, 将<b><i>input</i></b>发送给GPU.</p>
                </div>
                <div class="p1">
                    <p id="137">步骤3. 调用curand库, 生成标准差为<i>δ</i>的噪音<b><i>V</i></b><sub>noise</sub>, 并将该<b><i>V</i></b><sub>noise</sub>与<b><i>W</i></b>相加, 得到<b><i>W</i></b><sub>noise</sub>.GPU调用cublas库进行<b><i>input</i></b>和<b><i>W</i></b><sub>noise</sub>的矩阵乘计算, 得到输出向量<b><i>output</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="138">步骤4. 将输出向量<b><i>output</i></b>进行转置 (GPU中显存的存储方式为列优先, 而CPU内存中的存储方式为行有限) .将结果拷贝回CPU内存.</p>
                </div>
                <div class="p1">
                    <p id="139">在具体实现上, 使用CMake进行编译, 其中System-C相关代码需要调用System-C 2.3.2库, 而Crossbar计算部分则单独拆分出来, 使用NVCC进行编译.在程序链接时, 将2部分代码生成一个可执行文件.</p>
                </div>
                <div class="p1">
                    <p id="140">本项目所用的GPU为NVIDIA Tesla P100, 其显存大小为12 GB.加速效果如表1所示:</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表1 使用GPU计算的加速效果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 GPU Acceleration Result</b></p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td rowspan="2"><br />Data Format</td><td colspan="2"><br />No-noise</td><td colspan="2">Noise</td></tr><tr><td><br />CPU/s</td><td>GPU/s</td><td>CPU/s</td><td>GPU/s</td></tr><tr><td><br />512×1 024×1 024</td><td>2.3</td><td>0.13</td><td>65.1</td><td>0.156</td></tr><tr><td><br />100×3 000×3 000</td><td>3.9</td><td>0.021</td><td>109</td><td>0.039</td></tr><tr><td><br />3 000×1 152×128</td><td>2.0</td><td>0.118</td><td>54.5</td><td>0.135</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="142">由表1所示, “数据规格”为所设置的Crossbar大小, 其中第1维表示Crossbar个数, 后两维表示单个Crossbar的长和宽.在表1中, 第1, 2行分别列出了中等矩阵规模 (1 024×1 024) 和较大矩阵规模 (3 000×3 000) 的加速数据.由于当前ReRAM工艺所限, 其Crossbar阵列规模亦有限, 因此在最后一行给出了Crossbar尺寸为1 152×128的计算加速数据.表1中的第2列和第3列首先使用无噪音的随机生成数据作为权重验证计算准确性, 可以看出, 在无噪音情况下, 当阵列规模较小的时候 (第1行和第3行) 使用GPU进行计算可以达到16～17×左右的加速比, 在阵列规模较大的时候 (第2行) 更是能够发挥GPU的并行优势, 达到185×的加速比.之后再用有噪音的随机生成数据评估性能, 在使用GPU进行加速计算的情况下, 当阵列规模较小时 (第1行和第3行) 加速比可达到400×左右, 而当阵列规模较大时 (第2行) 其加速比更可达2 794×.由以上数据可以看出加速效果十分显著.</p>
                </div>
                <h3 id="143" name="143" class="anchor-tag"><b>4 案例研究与结果评估</b></h3>
                <div class="p1">
                    <p id="144">作为案例研究, 我们实现了如第3节架构的一个模拟器, 并据此评估我们的训练算法.</p>
                </div>
                <h4 class="anchor-tag" id="145" name="145"><b>4.1 模拟器的实现</b></h4>
                <div class="p1">
                    <p id="146">我们实现了一个ReRAM Crossbar仿真框架, 并给出了基于Pytorch的定制化训练算法, 包含第3节中提到的诸多模拟电路特性;模拟器仿真部分基于System-C实现, ReRAM设备模型来自文献<citation id="245" type="reference">[<a class="sup">15</a>]</citation>, 默认的I/O和权重精度均为8-bit (可以配置) .</p>
                </div>
                <div class="p1">
                    <p id="147">在具体的仿真实现上有诸多参数可以设置, 包括电路参数和神经网络参数, 如表2和表3所示:</p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表2 模拟器所需电路参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Circuit Parameters for Simulator</b></p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td><br />Parameters</td><td>Value</td></tr><tr><td><br />DA Reference Voltage/V</td><td>1.0</td></tr><tr><td><br />AD Reference Voltage</td><td>Get after training</td></tr><tr><td><br />Crossbar Length</td><td>1 152</td></tr><tr><td><br />Crossbar Width</td><td>128</td></tr><tr><td><br />Crossbar Number in Each PE</td><td>4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表3 模拟器所需神经网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Neural Network Parameters for Simulator</b></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br />Parameters</td><td>Value</td></tr><tr><td><br />Input Length</td><td>118 848</td></tr><tr><td><br />Input Size</td><td>39</td></tr><tr><td><br />Hidden Size</td><td>128</td></tr><tr><td><br />Output Size</td><td>61</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">基于表2和表3所列参数, 针对所需的仿真模型, 通过提前设置LSTM模块数、Linear模块数、数据缓冲区模块数, 并通过System-C中的信号将相关模块串联, 即可生成特定的模拟器, 且该流程可通过代码脚本自动生成.</p>
                </div>
                <div class="p1">
                    <p id="151">对ReRAM神经网络加速器而言, 面积和功耗是重要指标之一, 因此本项目模拟器对此进行了32 nm尺寸下的参考设计, 具体参数如表4所示.</p>
                </div>
                <div class="p1">
                    <p id="152">其中, 缓冲区 (buffer) 相关的模型参数来自CACTI<citation id="246" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, Crossbar面积参数来源于文献<citation id="247" type="reference">[<a class="sup">17</a>]</citation>, 其他参数采用了ISAAC<citation id="248" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的参数.</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表4 模拟器所需硬件参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Hardware Parameters for Simulator</b></p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td><br />Module</td><td>Power/mW</td><td>Area/mm<sup>2</sup></td></tr><tr><td><br />ADC (8-bit) </td><td>2</td><td>0.0012</td></tr><tr><td><br />DAC (1-bit) </td><td>0.00391</td><td>1.66016E-07</td></tr><tr><td><br />S&amp;H</td><td>9.76563E-06</td><td>3.90625E-08</td></tr><tr><td><br />X-bar</td><td>0.3</td><td>0.000148</td></tr><tr><td><br />Shift-add</td><td>0.05</td><td>0.00006</td></tr><tr><td><br />Input-buf (2 KB) </td><td>1.24</td><td>0.0021</td></tr><tr><td><br />Output-buf (256 KB) </td><td>0.23</td><td>0.00077</td></tr><tr><td><br />ReLU</td><td>0.003 2</td><td>8.9E-06</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="154" name="154"><b>4.2 仿真结果的评估</b></h4>
                <h4 class="anchor-tag" id="155" name="155">4.2.1 仿真算法评估</h4>
                <div class="p1">
                    <p id="156">我们实现的LSTM网络为应用在一个在TIMIT数据集上进行语音识别的网络.该数据集的训练集、验证集和测试集大小分别为3 696, 400和192.其中每条测试集语音帧数为619, 因此总计有118 848帧测试数据.其神经网络输入维度为39 (经过MFCC预处理后的语音数据, 即每一帧的向量长度) , 输出维度为61 (语素分类数) , 包含一个LSTM层 (隐层大小128) 和一个全连接层.表5为仿真算法评估结果.</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表5 训练算法评估结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Evaluation Result for Training Algorithm</b></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td><br />Training</td><td>Inference</td><td>TIMIT <br />Accuracy/%</td></tr><tr><td><br />No ReRAM crossbar <br />constraint</td><td>No ReRAM crossbar <br />constraint</td><td>84.22</td></tr><tr><td><br />8-bit weight quantize <br />and I/O constraint</td><td>8-bit weight quantize <br />and I/O constraint</td><td>84.17</td></tr><tr><td><br />No ReRAM crossbar <br />constraint</td><td>With ReRAM <br />crossbar constraint</td><td>82.53</td></tr><tr><td><br />Customized training <br />algorithm (with ReRAM <br />crossbar constraint) </td><td>With ReRAM <br />crossbar constraint</td><td>84.05</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158">我们使用通用的神经网络训练算法作为基准, 经过30个周期的训练, 其分类准确度可以达到84.22%, 此网络模型称为基准模型.在此基础上, 我们在训练中引入了神经网络8-bit的权重量化和I/O限制, 结果显示其准确度为84.17%, 即有轻微的下降.接下来将基准模型直接部署模拟器上 (后者在模拟过程中引入了所有的限制因素, 包括权重量化、I/O限制、ReRAM的非理想因素等) , 结果显示由于权重量化、器件噪音以及DA/AD等的影响, 识别准确度下降为82.53%.最后我们将基准模型在ReRAM约束下进行微调 (fine-tune) , 引入了所有限制因素, 在网络收敛后, 可以得到其识别准确度84.05%, 这个数据与将该网络部署到模拟器上获得的精度一样.</p>
                </div>
                <div class="p1">
                    <p id="159">表5表明:传统的训练算法在应用到ReRAM的推断过程中, 权重的量化和I/O精度限制对神经网络的表达结果影响很小, 但是由于器件噪音以及DA/AD等的约束, 会导致性能下降.而我们的定制化训练算法将以上因素考虑到训练过程中, 再将训练好的模型应用到ReRAM Crossbar的计算之上, 可以有效减小器件因素带来的性能下降.</p>
                </div>
                <h4 class="anchor-tag" id="160" name="160">4.2.2 模拟器的仿真准确度评估</h4>
                <div class="p1">
                    <p id="161">在模拟器本身的准确性方面, 我们做了基于SPICE模拟的评估.这是因为模拟器本身需要能够满足实际器件的物理特性和约束.现有的基于SPICE的ReRAM模拟工作, 其电路方程非常复杂 (对于一个2<sup><i>r</i></sup>×2<sup><i>c</i></sup>的Crossbar, 需要求解2<sup><i>r</i></sup>×2<sup><i>c</i></sup>+2<sup><i>r</i></sup>× (2<sup><i>c</i></sup>-1) 个电压参数和3×2<sup><i>r</i></sup>×2<sup><i>c</i></sup>个电流参数, 且方程是非线性的) , 因此求解难度很高, 模拟仿真速度慢.</p>
                </div>
                <div class="p1">
                    <p id="162">本项目模拟器在计算速度和仿真准确性上做了权衡:1) Crossbar的输入电压为1-bit的值 (只有高低电压之分) , 以此来消除非线性的影响;2) 考虑到Crossbar阵列中连线上的电容和电感对于计算影响很小<citation id="249" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 因此将其忽略;3) 每当读取电导值时, 都要引入ReRAM variation<citation id="250" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="163">在此基础上, 我们将结果与实际芯片的电路级仿真进行了比较.在阵列大小为1 152×128规模下, ReRAM单元高阻和低阻分别为800 KΩ和50 KΩ, 导线电阻 (bit-line, source-line, work-line) 分别为87 mΩ, 100 mΩ, 1.16 Ω (130 nm工艺下的CMOS电路) .在此条件下, 我们得到的行为级模型计算的结果电流值与电路仿真的结果电流值其误差不超过2.68%.</p>
                </div>
                <h3 id="164" name="164" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="165">本文提出了基于ReRAM的长短期记忆网络加速器训练和仿真框架, 包括针对ReRAM器件特性的定制训练算法、时钟驱动的行为级模拟器及其GPU加速.验证结果显示训练算法能够有效降低模拟器件带来的噪音和数值精度损失等不利因素的影响, 而该模拟器与SPICE仿真的计算结果误差在2.68%以内, 并且避免了SPICE对电路仿真耗时过长的缺点.与现有的工作相比, 本文提出的模拟器项目完成了端到端的训练和仿真, 并将一个实际应用于TIMIT数据集语音分类的LSTM网络部署并运行.未来我们会继续完善该模拟器, 增强各模块的可配置性, 为相关的加速器硬件设计提供探索方案.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="259" type="formula" href="images/JFYZ201906007_25900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘鹤</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="260" type="formula" href="images/JFYZ201906007_26000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">季宇</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="261" type="formula" href="images/JFYZ201906007_26100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">韩建辉</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="262" type="formula" href="images/JFYZ201906007_26200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张悠慧</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="263" type="formula" href="images/JFYZ201906007_26300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">郑纬民</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="190">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indatacenter performance analysis of a tensor processing unit">

                                <b>[1]</b>Jouppi N P, Young C, Patil N, et al.In-datacenter performance analysis of a tensor processing unit[C] //Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1- 12
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Benchmarks:Deep Learning Nvidia P100 vs V100 GPU[OL]">

                                <b>[2]</b>Xcelerit.Benchmarks:Deep Learning Nvidia P100 vs V100 GPU[OL].[2017-11-27].https://www.xcelerit.com/computing-benchmarks/insights/benchmarks-deep-learning-nvidia-p100-vs-v100-gpu/
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ESE:Efficient Speech Recognition Engine with Sparse LSTM on FPGA">

                                <b>[3]</b>Han Song, Kang Junlong, Mao Huizi, et al.ESE:Efficient speech recognition engine with sparse LSTM on FPGA[C]//Proc of the 2017 ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2017:75- 84
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=C-LSTM:Enabling efficient LSTM using structured compression techniques on FPGAs">

                                <b>[4]</b>Wang Suo, Li Zhe, Ding Caiwen, et al.C-LSTM:Enabling efficient LSTM using structured compression techniques on FPGAs[C] //Proc of the 2018 ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2018:11- 20
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ISAAC:a convolutional neural network accelerator with in-situ analog arithmetic in crossbars">

                                <b>[5]</b>Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[C] //Proc of the 43rd Annual Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:14- 26
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTgyMDd1SHlqbVVMdkpLRjBXYUJjPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient hardware mapping of long short-term memory neural networks for automatic speech recognition">

                                <b>[7]</b>Evangelopoulos G N.Efficient hardware mapping of long short-term memory neural networks for automatic speech recognition[D].Belgium:KU Leuven, 2016
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vortex:Variation-aware training for memristor x-bar">

                                <b>[8]</b>Liu Beiye, Li Hai, Chen Yiran, et al.Vortex:Variation-aware training for memristor x-bar[C]// Proc of the 52nd ACM/EDAC/IEEE Design Automation Conf (DAC) .Piscataway, NJ:IEEE, 2015:1- 6
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Binary convolutional neural network on RRAM">

                                <b>[9]</b>Tang Tianqi, Xia Lixue, Li Boxun, et al.Binary convolutional neural network on RRAM[C] //Proc of the 22nd Asia and South Pacific Design Automation Conf.Piscataway, NJ:IEEE, 2017:782- 787
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PipeL ayer:Apipelined reR AM-based accelerator for deep learning">

                                <b>[10]</b>Song Linghao, Qian Xuehai, Li Hai, et al.PipeLayer:A pipelined ReRAM-based accelerator for deep learning[C] //Proc of 2017 IEEE Int Symp on High Performance Computer Architecture.Piscataway, NJ:IEEE, 2017:541- 552
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory">

                                <b>[11]</b>Dong Xiangyu, Xu Cong, Xie Yuan, et al.NVSim:A circuit-level performance, energy, and area model for emerging nonvolatile memory[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2012, 31 (7) :994- 1007
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PIMSim:A flexible and detailed processing-in-memory simulator">

                                <b>[12]</b>Xu Sheng, Chen Xiaoming, Wang Ying, et al.PIMSim:A flexible and detailed processing-in-memory simulator[J].IEEE Computer Architecture Letters, 2018, 18 (1) :6- 9
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NeuroSim:A circuit-level macro model for benchmarking neuro-inspired architectures in online learning">

                                <b>[13]</b>Chen Paiyu, Peng Xiaochen, Yu Shimeng.NeuroSim:A circuit-level macro model for benchmarking neuro-inspired architectures in online learning[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (12) :3067- 3080
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MNSIM:Simulation platform for memristor-based neuromorphic computing system">

                                <b>[14]</b>Xia Lixue, Li Boxun, Tang Tianqi, et al.MNSIM:Simulation platform for memristor-based neuromorphic computing system[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018, 37 (5) :1009- 1022
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face classification using electronic synapses.">

                                <b>[15]</b>Yao Peng, Wu Huaqiang, Gao Bin, et al.Face classification using electronic synapses[J].Nature Communications, 2017, 8:15199
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Opti-mizing NUCA organizations and wiring alternatives for large caches with CACTI6.0">

                                <b>[16]</b>Muralimanohar N, Balasubramonian R, Jouppi N P.Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0[C] //Proc of the 40th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO-40 2007) .Piscataway, NJ:IEEE, 2007:3- 14
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ReRAM-based processing-in-memory architecture for recurrent neural network acceleration">

                                <b>[17]</b>Long Yun, Na T, Mukhopadhyay S.ReRAM-based processing-in-memory architecture for recurrent neural network acceleration[J].IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2018, 26 (12) :2781- 2794
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Technological exploration of RRAM crossbar array for matrix-vector multiplication">

                                <b>[18]</b>Gu Peng, Li Boxun, Tang Tianqi, et al.Technological exploration of RRAM crossbar array for matrix-vector multiplication[J].Journal of Computer Science and Technology, 2016, 31 (1) :3- 19
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-level switching of triple-layered TaOx RRAMwith excellent reliability for storage class memory">

                                <b>[19]</b>Lee S R, Kim Y B, Chang M, et al.Multi-level switching of triple-layered TaOx RRAM with excellent reliability for storage class memory[C] //Proc of 2012 Symp on VLSI Technology.Piscataway, NJ:IEEE, 2012:71- 72
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="251" href="javascript:void(0)">
                            <b>1</b> 与真实芯片的电路设计作对比得出该结论.
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201906007" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906007&amp;v=MjcyMzlOTHl2U2RMRzRIOWpNcVk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZpRG1VYjM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGZvdUhkT1pzcmhUWHhKMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

