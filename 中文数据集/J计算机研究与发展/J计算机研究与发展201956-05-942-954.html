

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128640031993750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201905006%26RESULT%3d1%26SIGN%3dPh2JE8EVc3HBH4vg%252bExPtZUz5%252bw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201905006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905006&amp;v=MDI0NzBqTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGaURnVkxyS0x5dlNkTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 研究现状&lt;/b&gt; "><b>1 研究现状</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;2 基于随机博弈的攻防对抗建模&lt;/b&gt; "><b>2 基于随机博弈的攻防对抗建模</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="&lt;b&gt;2.1 网络攻防对抗问题描述与分析&lt;/b&gt;"><b>2.1 网络攻防对抗问题描述与分析</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;2.2 攻防随机博弈模型&lt;/b&gt;"><b>2.2 攻防随机博弈模型</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;2.3 基于攻防图的网络状态与攻防动作提取方法&lt;/b&gt;"><b>2.3 基于攻防图的网络状态与攻防动作提取方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;3 基于WoLF-PHC的博弈分析与策略选取&lt;/b&gt; "><b>3 基于WoLF-PHC的博弈分析与策略选取</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#123" data-title="&lt;b&gt;3.1 WoLF-PHC算法原理&lt;/b&gt;"><b>3.1 WoLF-PHC算法原理</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;3.2 基于资格迹的改进WoLF-PHC及防御决策算法&lt;/b&gt;"><b>3.2 基于资格迹的改进WoLF-PHC及防御决策算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#193" data-title="&lt;b&gt;4 实验分析&lt;/b&gt; "><b>4 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#194" data-title="&lt;b&gt;4.1 实验场景描述&lt;/b&gt;"><b>4.1 实验场景描述</b></a></li>
                                                <li><a href="#202" data-title="&lt;b&gt;4.2 构建实验场景的AD-SGM&lt;/b&gt;"><b>4.2 构建实验场景的AD-SGM</b></a></li>
                                                <li><a href="#219" data-title="&lt;b&gt;4.3 测试与分析&lt;/b&gt;"><b>4.3 测试与分析</b></a></li>
                                                <li><a href="#247" data-title="&lt;b&gt;4.4 方法综合比较&lt;/b&gt;"><b>4.4 方法综合比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#251" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="图1 攻防图生成">图1 攻防图生成</a></li>
                                                <li><a href="#129" data-title="图2 Q-learning学习机制">图2 Q-learning学习机制</a></li>
                                                <li><a href="#196" data-title="图3 实验网络结构">图3 实验网络结构</a></li>
                                                <li><a href="#198" data-title="&lt;b&gt;表1 网络脆弱性信息&lt;/b&gt;"><b>表1 网络脆弱性信息</b></a></li>
                                                <li><a href="#199" data-title="图4 攻击图">图4 攻击图</a></li>
                                                <li><a href="#200" data-title="图5 防御图">图5 防御图</a></li>
                                                <li><a href="#201" data-title="&lt;b&gt;表2 防御动作描述&lt;/b&gt;"><b>表2 防御动作描述</b></a></li>
                                                <li><a href="#221" data-title="图6 不同参数设置下的防御决策">图6 不同参数设置下的防御决策</a></li>
                                                <li><a href="#225" data-title="&lt;b&gt;表3 不同参数设置&lt;/b&gt;"><b>表3 不同参数设置</b></a></li>
                                                <li><a href="#227" data-title="图7 不同参数设置下的防御收益变化">图7 不同参数设置下的防御收益变化</a></li>
                                                <li><a href="#228" data-title="图8 不同参数设置下的平均收益">图8 不同参数设置下的平均收益</a></li>
                                                <li><a href="#229" data-title="图9 不同参数设置下的防御收益标准差">图9 不同参数设置下的防御收益标准差</a></li>
                                                <li><a href="#234" data-title="图10 防御收益变化对比">图10 防御收益变化对比</a></li>
                                                <li><a href="#236" data-title="图11 本文方法的防御决策">图11 本文方法的防御决策</a></li>
                                                <li><a href="#237" data-title="图12 防御收益变化对比">图12 防御收益变化对比</a></li>
                                                <li><a href="#241" data-title="图13 防御决策对比">图13 防御决策对比</a></li>
                                                <li><a href="#243" data-title="图14 防御收益变化对比">图14 防御收益变化对比</a></li>
                                                <li><a href="#244" data-title="图15 平均防御收益对比">图15 平均防御收益对比</a></li>
                                                <li><a href="#249" data-title="&lt;b&gt;表4 典型方法综合比较&lt;/b&gt;"><b>表4 典型方法综合比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="354">


                                    <a id="bibliography_1" title="Hu Qing, L&#252;Shichao, Shi Zhiqiang, et al.Advanced persistent threats detection game with expert system for cloud[J].Journal of Computer Research and Development, 2017, 54 (10) :2344-2355 (in Chinese) (胡晴, 吕世超, 石志强, 等.基于专家系统的高级持续性威胁云端检测博弈[J].计算机研究与发展, 2017, 54 (10) :2344-2355) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201710021&amp;v=MjcwMTQ5SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZpRGdWTHJLTHl2U2RMRzRIOWJOcjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Hu Qing, L&#252;Shichao, Shi Zhiqiang, et al.Advanced persistent threats detection game with expert system for cloud[J].Journal of Computer Research and Development, 2017, 54 (10) :2344-2355 (in Chinese) (胡晴, 吕世超, 石志强, 等.基于专家系统的高级持续性威胁云端检测博弈[J].计算机研究与发展, 2017, 54 (10) :2344-2355) 
                                    </a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_2" title="Wang Yuanzhuo, Yu Jianye, Qiu Wen, et al.Evolutionary Game Model and Analysis Methods for Network Group Behavior[J].Chinese Journal of Computers, 2015, 38 (2) :282-300" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201502006&amp;v=MTMyNzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGaURnVkxyS0x6N0Jkckc0SDlUTXJZOUZZb1E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Wang Yuanzhuo, Yu Jianye, Qiu Wen, et al.Evolutionary Game Model and Analysis Methods for Network Group Behavior[J].Chinese Journal of Computers, 2015, 38 (2) :282-300
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_3" title="Lei Cheng, Zhang Hongqi, Wan Liming, et al.Incomplete information Markov game theoretic approach to strategy generation for moving target defense[J].Computer Communications, 2018, 116 (12) :184-199" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES22E136CA16B0147B12D794CEDF9B0ECB&amp;v=MTE3MzZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54Z3pMMjR4YW89TmlmT2ZiRzZhOURQcWZ3MFplMTlESDA5eUdRUzZFdDZRWHVSMldaRGNNQ1VNTW50Q09Odg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Lei Cheng, Zhang Hongqi, Wan Liming, et al.Incomplete information Markov game theoretic approach to strategy generation for moving target defense[J].Computer Communications, 2018, 116 (12) :184-199
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_4" title="Li Jiawei, Kendall G, John R.Computing nash equilibria and evolutionarily stable states of evolutionary games[J].IEEETransactions on Evolutionary Computation, 2016, 20 (3) :460-469" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computing Nash equilibria and evolutionarily stable states of evolutionary games">
                                        <b>[4]</b>
                                        Li Jiawei, Kendall G, John R.Computing nash equilibria and evolutionarily stable states of evolutionary games[J].IEEETransactions on Evolutionary Computation, 2016, 20 (3) :460-469
                                    </a>
                                </li>
                                <li id="362">


                                    <a id="bibliography_5" title="Bowling M, Veloso M.Multiagent learning using a variable learning rate[J].Artificial Intelligence, 2002, 136 (2) :215-250" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702006942&amp;v=MTIzMDNUYnhBPU5pZk9mYks3SHRETnFJOUhaT3NKQlhnN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SktGcw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Bowling M, Veloso M.Multiagent learning using a variable learning rate[J].Artificial Intelligence, 2002, 136 (2) :215-250
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_6" title="Seaar A D, Donald W.The boundedness conditions for model-free HDP (λ) [J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 125 (99) :1-15" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The boundedness conditions for model-free HDP (λ)">
                                        <b>[6]</b>
                                        Seaar A D, Donald W.The boundedness conditions for model-free HDP (λ) [J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 125 (99) :1-15
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_7" title="Do C T, Tran N H, Hong C, et al.Game theory for cyber security and privacy[J].ACM Computing Surveys, 2017, 50 (2) :30-31" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMC13BD3AEB1862B58D5054F0472230113&amp;v=MTAxMDJmQ3BiUTM1TnhnekwyNHhhbz1OaWZJWThDNUhhTzRyUDR3RnVvSENuNUx5aDVuN3o5NFRBbmlxQlUzZTdHVVJMdWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        Do C T, Tran N H, Hong C, et al.Game theory for cyber security and privacy[J].ACM Computing Surveys, 2017, 50 (2) :30-31
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_8" title="Liu Yuling, Feng Dengguo, Wu Lihui, et al.Performance evaluation of worm attack and defense strategies based on static Bayesian game[J].Journal of Software, 2012, 23 (3) :712-723 (in Chinese) (刘玉岭, 冯登国, 吴丽辉, 等.基于静态贝叶斯博弈的蠕虫攻防策略绩效评估[J].软件学报, 2012, 23 (3) :712-723) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201203022&amp;v=MDcxMTMzenFxQnRHRnJDVVJMT2VaZVJxRmlEZ1ZMcktOeWZUYkxHNEg5UE1ySTlIWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Liu Yuling, Feng Dengguo, Wu Lihui, et al.Performance evaluation of worm attack and defense strategies based on static Bayesian game[J].Journal of Software, 2012, 23 (3) :712-723 (in Chinese) (刘玉岭, 冯登国, 吴丽辉, 等.基于静态贝叶斯博弈的蠕虫攻防策略绩效评估[J].软件学报, 2012, 23 (3) :712-723) 
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_9" title="Li Yuzhe, Quevedo D E, Dey S, et al.A game-theoretic approach to fake-acknowledgment attack on cyber-physical systems[J].IEEE Transactions on Signal and Information Processing Over Networks, 2017, 3 (1) :1-11" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A game-theoretic approach to fake-acknowledgment attack on cyber-physical systems">
                                        <b>[9]</b>
                                        Li Yuzhe, Quevedo D E, Dey S, et al.A game-theoretic approach to fake-acknowledgment attack on cyber-physical systems[J].IEEE Transactions on Signal and Information Processing Over Networks, 2017, 3 (1) :1-11
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_10" title="Zhang Hengwei, Li Tao.Optimal active defense based on multi-stage attack-defense signaling game[J].Acta Electronica Sinica, 2017, 45 (2) :431-439 (in Chinese) (张恒巍, 李涛.基于多阶段攻防信号博弈的最优主动防御[J].电子学报, 2017, 45 (2) :431-439) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201702023&amp;v=Mjg5NzFxRmlEZ1ZMcktJVGZUZTdHNEg5Yk1yWTlIWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Zhang Hengwei, Li Tao.Optimal active defense based on multi-stage attack-defense signaling game[J].Acta Electronica Sinica, 2017, 45 (2) :431-439 (in Chinese) (张恒巍, 李涛.基于多阶段攻防信号博弈的最优主动防御[J].电子学报, 2017, 45 (2) :431-439) 
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_11" title="Afrand A, Das S K.Preventing DoS attacks in wireless sensor networks:A repeated game theory approach[J].Internet Journal of Network Security, 2007, 5 (2) :145-153" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Preventing DoS Attacks in Wireless Sensor Networks: A Repeated Game Theory Approach">
                                        <b>[11]</b>
                                        Afrand A, Das S K.Preventing DoS attacks in wireless sensor networks:A repeated game theory approach[J].Internet Journal of Network Security, 2007, 5 (2) :145-153
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_12" title="Jiang Wei, Fang Binxing, Tian Zhihong, et al.Research on defense strategies selection based on attack-defense stochastic game model[J].Journal of Computer Research and Development, 2010, 47 (10) :1714-1723 (in Chinese) (姜伟, 方滨兴, 田志宏, 等.基于攻防随机博弈模型的防御策略选取研究[J].计算机研究与发展, 2010, 47 (10) :1714-1723) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201010008&amp;v=MjE1NDZHNEg5SE5yNDlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEZ1ZMcktMeXZTZEw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Jiang Wei, Fang Binxing, Tian Zhihong, et al.Research on defense strategies selection based on attack-defense stochastic game model[J].Journal of Computer Research and Development, 2010, 47 (10) :1714-1723 (in Chinese) (姜伟, 方滨兴, 田志宏, 等.基于攻防随机博弈模型的防御策略选取研究[J].计算机研究与发展, 2010, 47 (10) :1714-1723) 
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_13" title="Wang Changchun, Cheng Xiaohang, Zhu Yongwen, et al.AMarkov game model of computer network operation[J].Systems Engineering-Theory and Practice, 2014, 34 (9) :2402-2410 (in Chinese) (王长春, 程晓航, 朱永文, 等.计算机网络对抗行动策略的Markov博弈模型[J].系统工程理论与实践, 2014, 34 (9) :2402-2410) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTLL201409025&amp;v=MjEyOTBScUZpRGdWTHJLUFRuSFlyRzRIOVhNcG85SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Wang Changchun, Cheng Xiaohang, Zhu Yongwen, et al.AMarkov game model of computer network operation[J].Systems Engineering-Theory and Practice, 2014, 34 (9) :2402-2410 (in Chinese) (王长春, 程晓航, 朱永文, 等.计算机网络对抗行动策略的Markov博弈模型[J].系统工程理论与实践, 2014, 34 (9) :2402-2410) 
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_14" title="Hofbauer J, Sigmund K.Evolutionary game dynamics[J].Bulletin of the American Mathematical Society, 2011, 40 (4) :479-519" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524050452&amp;v=MTA3MTdEZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdmhVcmZOSjFzV05pZktZN0s2SHRUT3E0OUFaTzhLRGhNOHp4VVNt&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Hofbauer J, Sigmund K.Evolutionary game dynamics[J].Bulletin of the American Mathematical Society, 2011, 40 (4) :479-519
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_15" title="Hayel Y, Zhu Quanyuan.Protection over heterogeneous networks using evolutionary poisson games[J].IEEETransactions on Information Forensics and Security, 2017, 12 (8) :1786-1800" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Epidemic protection over heterogeneous networks using evolutionary Poisson games">
                                        <b>[15]</b>
                                        Hayel Y, Zhu Quanyuan.Protection over heterogeneous networks using evolutionary poisson games[J].IEEETransactions on Information Forensics and Security, 2017, 12 (8) :1786-1800
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_16" title="Huang Jianming, Zhang Hengwei.Improving replicator dynamic evolutionary game model for selecting optimal defense strategies[J].Journal on Communications, 2018 (1) :170-182 (in Chinese) (黄健明, 张恒巍.基于改进复制动态演化博弈模型的最优防御策略选取[J].通信学报, 2018 (1) :170-182) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201801016&amp;v=MDE5NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEZ1ZMcktNVFhUYkxHNEg5bk1ybzlFWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Huang Jianming, Zhang Hengwei.Improving replicator dynamic evolutionary game model for selecting optimal defense strategies[J].Journal on Communications, 2018 (1) :170-182 (in Chinese) (黄健明, 张恒巍.基于改进复制动态演化博弈模型的最优防御策略选取[J].通信学报, 2018 (1) :170-182) 
                                    </a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_17" title="Huang Jianming, Zhang Hengwei, Wang Jindong.Markov evolutionary games for network defense strategy selection[J].IEEE Access, 2017, 122 (99) :1-10" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Markov evolutionary games for network defense strategy selection">
                                        <b>[17]</b>
                                        Huang Jianming, Zhang Hengwei, Wang Jindong.Markov evolutionary games for network defense strategy selection[J].IEEE Access, 2017, 122 (99) :1-10
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_18" title="Lye K W, Wing J M.Game strategies in network security[J].International Journal of Information Security, 2005, 4 (1?2) :71-86" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001215348&amp;v=MjkxODY3QmFyTzRIdEhOclk1QVorOEhZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDemtXN3JQSlZ3PU5q&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Lye K W, Wing J M.Game strategies in network security[J].International Journal of Information Security, 2005, 4 (1?2) :71-86
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_19" title="Hewett R, Kijsanayothin P.Host-centric model checking for network vulnerability analysis[C]Proc of Computer Security Applications Conf.Piscataway, NJ:IEEE, 2009:225-234" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Host-centric model checking for network vulnerability analysis">
                                        <b>[19]</b>
                                        Hewett R, Kijsanayothin P.Host-centric model checking for network vulnerability analysis[C]Proc of Computer Security Applications Conf.Piscataway, NJ:IEEE, 2009:225-234
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_20" title="Zimmer M, Doncieux S.Bootstrapping Q-learning for robotics from neuro-evolution results[J].IEEE Transactions on Cognitive&amp;amp;Developmental Systems, 2017, 125 (99) :1-16" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bootstrapping Q-learning for robotics from neuro-evolution results">
                                        <b>[20]</b>
                                        Zimmer M, Doncieux S.Bootstrapping Q-learning for robotics from neuro-evolution results[J].IEEE Transactions on Cognitive&amp;amp;Developmental Systems, 2017, 125 (99) :1-16
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_21" title="Richard S, Andrew B.Reinforcement learning:An introduction, bradford book[J].Machine Learning, 2005, 16 (1) :285-286" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning:An introduction,bradford book">
                                        <b>[21]</b>
                                        Richard S, Andrew B.Reinforcement learning:An introduction, bradford book[J].Machine Learning, 2005, 16 (1) :285-286
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_22" title="Zhang Hengwei, Wang Jindong, Yu Dongsheng, et al.Active defense strategy selection based on static Bayesian game[C]Proc of International Conf on Cyberspace Technology.New York:ACM, 2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active defense strategy selection based on static Bayesian game">
                                        <b>[22]</b>
                                        Zhang Hengwei, Wang Jindong, Yu Dongsheng, et al.Active defense strategy selection based on static Bayesian game[C]Proc of International Conf on Cyberspace Technology.New York:ACM, 2016
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(05),942-954 DOI:10.7544/issn1000-1239.2019.20180877            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于随机博弈与改进WoLF-PHC的网络防御决策方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%B3%BB%E6%A5%A0&amp;code=40725260&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨峻楠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%BA%A2%E6%97%97&amp;code=39999698&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张红旗</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BC%A0%E5%AF%8C&amp;code=40725261&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张传富</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E6%88%98%E7%95%A5%E6%94%AF%E6%8F%B4%E9%83%A8%E9%98%9F%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6&amp;code=1702647&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军战略支援部队信息工程大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>当前运用随机博弈的网络攻防分析方法采用完全理性假设, 但在实际的网络攻防对抗中攻防双方很难达到完全理性的高要求, 降低了现有方法的准确性和指导价值.从网络攻防对抗实际出发, 分析有限理性对攻防随机博弈的影响, 在有限理性约束下构建攻防随机博弈模型.针对网络状态爆炸的问题, 提出一种基于攻防图的网络状态与攻防动作提取方法, 有效压缩了博弈状态空间.在上述基础上引入强化学习中的WoLF-PHC算法进行有限理性随机博弈分析并设计了具有在线学习能力的防御决策算法.该算法通过学习可以获得针对当前攻击者的最优防御策略, 所得策略在有限理性下优于现有攻防随机博弈模型的纳什均衡策略.通过引入资格迹改进WoLF-PHC算法, 进一步提高了防御者的学习速度.通过实验验证了所提方法的有效性与先进性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E5%8D%9A%E5%BC%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机博弈;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=WoLF-PHC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">WoLF-PHC;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络攻防;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%98%B2%E5%BE%A1%E5%86%B3%E7%AD%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">防御决策;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%89%E9%99%90%E7%90%86%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">有限理性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                                            </p>
                                    <p><b>收稿日期：</b>2018-12-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家“八六三”高技术研究发展计划基金项目 (2014AA7116082, 2015AA7116040);</span>
                    </p>
            </div>
                    <h1><b>Network Defense Decision-Making Method Based on Stochastic Game and Improved WoLF-PHC</b></h1>
                    <h2>
                    <span>Yang Junnan</span>
                    <span>Zhang Hongqi</span>
                    <span>Zhang Chuanfu</span>
            </h2>
                    <h2>
                    <span>Zhengzhou Information Science and Technology Institute</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>At present, the method of network attack and defense analysis based on stochastic game adopts the assumption of complete rationality, but in the actual network attack-defense confrontation, it is difficult for both sides of attack and defense to meet the high requirement of complete rationality, which reduces the accuracy and guiding value of the existing methods. Based on the reality of network attack-defense confrontation, the influence of bounded rationality on attack-defense stochastic game is analyzed. Under the constraints of bounded rationality, a stochastic game model is constructed. Aiming at the problem of network state explosion, a method of extracting network state and attack-defense action based on attack-defense graph is proposed, which the game state space is effectively reduced. On this basis, WoLF-PHC algorithm in reinforcement learning is introduced to carry out bounded rational stochastic game analysis and design a defensive decision-making algorithm with online learning ability. By learning, the algorithm can obtain the optimal defense strategy for the current attacker. The obtained strategy is superior to the Nash equilibrium strategy of the existing attack-defense stochastic game model under bounded rationality. By introducing eligibility trace to improve WoLF-PHC, the learning speed of defenders is further improved. The experimental results verify the effectiveness and advancement of the proposed method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stochastic%20game&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stochastic game;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=WoLF-PHC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">WoLF-PHC;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=network%20attack-defense&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">network attack-defense;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=defense%20strategies&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">defense strategies;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=bounded%20rationality&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">bounded rationality;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Yang Junnan, born in 1993. Master candidate.His main research interests include network security, game and reinforcement learning.<image id="305" type="formula" href="images/JFYZ201905006_30500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhang Hongqi, born in 1962. PhD, professor. His main research interests include network information security and classification protection.<image id="307" type="formula" href="images/JFYZ201905006_30700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National High Technology Research and Development Program of China (863 Program) (2014AA7116082, 2015AA7116040);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="50">随着社会信息化程度的不断加强, 网络攻击也日趋频繁, 给防御者造成巨大损失<citation id="398" type="reference"><link href="354" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.由于网络本身的复杂性及防御者能力的限制, 导致网络无法达到绝对的安全, 亟需一种能够对攻防行为进行分析和对网络风险与安全投入进行有效折中的技术, 使得防御者能利用有限的资源做出合理的决策.博弈论与网络攻防所具有的目标对立性、关系非合作性和策略依存性高度契合<citation id="399" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.博弈论在网络安全中的研究和应用日渐兴起<citation id="400" type="reference"><link href="358" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 其中基于随机博弈的攻防对抗分析更是成为一个热点.随机博弈是博弈论与Markov决策的结合, 不仅将传统博弈的单状态扩展到多状态, 还能对网络攻防的随机性进行刻画.目前基于随机博弈的网络安全分析已取得一定成果, 但仍存在不足和挑战:现有攻防随机博弈以完全理性假设为前提, 通过纳什均衡来进行攻击预测和防御指导.完全理性包括理性意识 (追求最大收益) 、分析推理能力、识别判断能力、记忆能力和准确行为能力等多方面的完美性要求, 其中任何一方面不完美就属于有限理性<citation id="401" type="reference"><link href="360" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.完全理性的高要求对于攻防双方过于苛刻, 导致完全理性假设下得到的纳什均衡在实际中很难出现, 降低了现有研究成果的准确性和指导价值.</p>
                </div>
                <div class="p1">
                    <p id="51">针对上述问题, 本文在有限理性约束下, 研究基于随机博弈的防御决策方法.介绍了基于随机博弈的防御决策研究现状;分析有限理性下研究网络攻防随机博弈的难点及本文解决思想, 在有限理性约束下构建攻防随机博弈模型并提出一种以主机为中心的攻防图用于提取博弈模型中的网络状态与攻防动作;基于资格迹改进WoLF-PHC<citation id="402" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 利用改进的WoLF-PHC对所构建的随机博弈模型进行博弈分析并设计防御决策算法;通过实验验证所提方法的有效性;最后总结全文并对未来研究进行展望.</p>
                </div>
                <div class="p1">
                    <p id="52">本文的贡献主要有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="53">1) 网络状态及攻防动作提取是随机博弈模型构建的关键之一.现有随机博弈模型的网络状态包含网络中所有节点的安全要素, 存在“状态爆炸”问题.针对此问题, 提出了一种以主机为中心的攻防图模型并设计了攻防图生成算法, 有效压缩了博弈状态空间.</p>
                </div>
                <div class="p1">
                    <p id="54">2) 有限理性意味着攻防双方需要通过试错和学习来寻找最优策略, 确定参与人的学习机制是一个关键点.本文将强化学习引入到随机博弈中, 使随机博弈由完全理性拓展到有限理性领域, 防御者利用WoLF-PHC在攻防博弈中进行博弈学习, 从而针对当前攻击者做出最优选择.现有有限理性博弈大多采用生物进化机制进行学习, 以群体为研究对象, 与其相比, 本文所提方法降低了博弈参与人之间的信息交换, 更适用于指导个体防御决策.</p>
                </div>
                <div class="p1">
                    <p id="55">3) 基于资格迹<citation id="403" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>对WoLF-PHC算法进行了改进, 加快了防御者的学习速度, 减少了算法对数据的依赖并通过实验证明了方法的有效性.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>1 研究现状</b></h3>
                <div class="p1">
                    <p id="57">国内外基于博弈论的网络安全研究已取得一定进展, 但是目前研究大多以完全理性假设为前提<citation id="404" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.完全理性下依据攻防双方在博弈过程中的决策次数, 可以分为单阶段博弈和多阶段博弈.单阶段网络攻防博弈的研究起步较早:中国科学院刘玉岭等人<citation id="405" type="reference"><link href="368" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>采用静态博弈理论对蠕虫病毒攻击和防御策略的效能进行了分析.加拿大艾伯塔大学的Li等人<citation id="406" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>建立了攻击者与传感器信任节点间的非合作博弈模型, 依据纳什均衡给出了最优攻击策略.在网络攻防中, 虽然部分简单的攻防对抗属于单阶段博弈, 但大多数场景中攻防过程往往持续多个阶段, 所以多阶段网络攻防博弈成为一种趋势:信息工程大学张恒巍等人<citation id="407" type="reference"><link href="372" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>将防御方看作信号发送源, 攻击者为信号接收方, 构建多阶段攻防信号博弈模型并给出均衡求解方法.密苏里科技大学的Afrand等人<citation id="408" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>建立了入侵检测系统和无线传感器节点间的重复博弈模型, 分析了节点包的转发策略.上述成果虽然可以对多阶段攻防对抗进行分析, 但是阶段之间的状态转移不仅受到攻防动作的影响, 还受到系统运行环境以及其他外来因素的干扰, 存在随机性, 而上述成果忽略了这种随机性, 削弱了其指导价值.</p>
                </div>
                <div class="p1">
                    <p id="58">随机博弈是博弈论与Markov理论的结合, 是一种多阶段博弈模型, 采用Markov过程来描述状态的转移, 能够准确分析随机性对攻防过程的影响.哈尔滨工程大学姜伟等人<citation id="409" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将网络攻防抽象为随机博弈问题, 给出了适用于攻防随机博弈模型的较为科学、准确的攻防收益量化方法.空军装备研究院王长春等人<citation id="410" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>使用随机博弈对网络对抗问题进行研究, 运用凸分析理论证明均衡的存在性并将均衡求解转换为一个非线性规划问题.Lei等人<citation id="411" type="reference"><link href="358" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>基于不完全信息随机博弈提出了一种网络移动目标防御决策方法.上述文献全部以完全理性假设为前提, 这对攻防双方过于严苛, 大多数情况攻防双方都只是有限理性水平, 导致上述研究成果在攻防博弈分析时难免产生偏差.因此, 探究有限理性的网络攻防博弈规律, 具有重要的研究价值和现实意义.</p>
                </div>
                <div class="p1">
                    <p id="59">有限理性意味着攻防双方不会在一开始就找到最优策略, 会在攻防博弈中学习攻防博弈, 合适的学习机制是在博弈中取胜的关键.目前有限理性攻防博弈的研究主要围绕演化博弈<citation id="412" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>展开.Hayel等人<citation id="413" type="reference"><link href="382" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>建立了恶意软件和反病毒程序间的演化泊松博弈模型, 借鉴复制动态方程分析反病毒程序策略.黄健明等人<citation id="414" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>通过引入激励系数, 改进传统复制动态方程, 完善复制动态速率计算方法, 基于此构建演化博弈模型用于防御.演化博弈以群体为研究对象, 采用生物进化机制, 通过模仿其他成员的优势策略来完成学习.演化博弈中参与人之间信息交换过多且主要是对攻防群体策略的调整过程、趋势和稳定性进行研究, 不利于指导个体成员的实时策略选择.</p>
                </div>
                <div class="p1">
                    <p id="60">强化学习是一种经典的在线学习方法, 其参与人通过环境的反馈进行独立学习, 相比生物进化方法, 强化学习更适于指导个体的决策.本文把强化学习机制引入到随机博弈中, 将随机博弈由完全理性扩展到有限理性, 并采用有限理性随机博弈对网络攻防进行分析.与现有攻防随机博弈相比, 本文方法采用有限理性假设, 更加符合实际;与演化博弈相比, 本文方法采用强化学习机制, 更适用于指导个体防御决策.</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>2 基于随机博弈的攻防对抗建模</b></h3>
                <h4 class="anchor-tag" id="62" name="62"><b>2.1 网络攻防对抗问题描述与分析</b></h4>
                <div class="p1">
                    <p id="63">网络攻防对抗是一个复杂问题, 但从策略选取的层面, 可以将其描述为一个随机博弈问题.将连续的时间轴划分为一个个时间片, 每个时间片内包含且只包含一个网络状态.不同时间片内的网络状态可能相同.每个时间片都是一个攻防博弈问题, 攻防双方检测当前网络状态, 然后依据策略选取攻防动作并获得立即回报.攻防策略与网络状态相关联.网络系统在攻防双方的联合行为下由一个状态转移到另一个状态.网络状态之间的转移不仅受到攻防动作的影响, 还受到系统运行环境以及外部环境等因素的影响, 存在随机性.本文的目标是使得防御者在攻防随机博弈中获得较高的长期收益.</p>
                </div>
                <div class="p1">
                    <p id="64">完全理性下攻防双方都能预测到纳什均衡的存在, 所以纳什均衡就是双方的最优策略.由引言中对完全理性的描述可知, 完全理性对于攻防双方的要求过于苛刻, 实际中攻防双方会受到有限理性的约束.有限理性意味着攻防双方至少有一方不会在一开始就采取纳什均衡策略, 意味着有限理性下攻防双方很难在博弈初期找到最优策略, 需要针对对手的策略不断调整和改进, 意味着博弈均衡不是一次选择的结果, 是攻防双方在攻防对抗中不断学习达到的, 而且由于不同学习机制的影响, 即使达到了均衡也有可能再次偏离.</p>
                </div>
                <div class="p1">
                    <p id="65">由上述分析可知, 学习机制是有限理性随机博弈取胜的关键.针对防御决策问题, 有限理性下的攻防随机博弈学习机制需满足2点需求:1) 学习算法的收敛性.有限理性下攻击者策略具有动态变化特性, 又因为攻防策略的相互依存性, 使得防御者在面对不同攻击策略时都要学习到对应的最优策略才能保证自己立于不败之地.2) 学习过程不需要过多攻击者信息.网络攻防双方具有目标对立性和非合作性, 攻防双方会刻意隐藏自己的关键信息, 学习过程如果需要过多对手信息会降低学习算法的实用性.</p>
                </div>
                <div class="p1">
                    <p id="66">WoLF-PHC算法是一种典型的策略梯度强化学习方法, 使防御者通过网络反馈进行学习, 不需要与攻击者之间过多的信息交换.WoLF机制的引入保证了WoLF-PHC算法的收敛性<citation id="415" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>:在攻击者通过学习采用纳什均衡策略后, WoLF机制使得防御者能够收敛到对应的纳什均衡策略;在攻击者尚未学习到纳什均衡策略时, WoLF机制使得防御者能够收敛到对应的最优防御策略.综上可知WoLF-PHC算法能够满足有限理性下的攻防随机博弈需求.</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>2.2 攻防随机博弈模型</b></h4>
                <div class="p1">
                    <p id="68">随机博弈由每个状态中的攻防博弈及状态之间的转移模型构成.</p>
                </div>
                <div class="p1">
                    <p id="69">对每个状态下博弈所需的“信息”和“行动顺序”2个关键要素进行假定.1) “信息”.受有限理性的约束, 将攻击者历史动作和攻击者的收益函数设定为攻击者的私有信息.网络状态为双方的共同知识.2) “行动顺序”.由于攻防双方的非合作性, 双方只能通过检测网络来观察对方的行动, 这会比动作的执行时间至少延迟一个时间片, 所以在每个时间片攻防双方是同时行动, 这里的“同时”是一个信息概念而非时间概念, 即尽管从时间概念上攻防双方的选择可能不在同一时刻, 但由于攻防双方在选择行动时不知道对方的选择则认为是同时行动.</p>
                </div>
                <div class="p1">
                    <p id="70">构建网络状态转移模型:采用概率来表示网络状态转移的随机性.由于当前网络状态主要与前一个网络状态有关, 所以采用一阶Markov来表示状态转移关系, 故转移概率为<i>P</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>, <i>d</i><sub><i>t</i></sub>, <i>s</i><sub><i>t</i>+1</sub>) , 其中<i>s</i>为网络状态, (<i>a</i>, <i>d</i>) 为攻防动作.由于攻防双方受有限理性的约束, 为了增加模型的通用性将转移概率设定为攻防双方的未知信息.</p>
                </div>
                <div class="p1">
                    <p id="71">在上述基础上, 为解决防御决策问题, 构建博弈模型.</p>
                </div>
                <div class="p1">
                    <p id="72"><b>定义1</b>. 攻防随机博弈模型 (attack defense stochastic game model, AD-SGM) 是一个六元组<i>AD</i>-<i>SGM</i>= (<i>N</i>, <i>S</i>, <i>D</i>, <i>R</i>, <i>Q</i>, <i>π</i>) , 其中:</p>
                </div>
                <div class="p1">
                    <p id="73">① <i>N</i>= (<i>attacker</i>, <i>defender</i>) 为参与博弈的2个局中人, 分别代表网络攻击者和防御者;</p>
                </div>
                <div class="p1">
                    <p id="74">② <i>S</i>= (<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>n</i></sub>) 为随机博弈状态集合, 由网络状态组成, 具体含义及生成方法见2.3节;</p>
                </div>
                <div class="p1">
                    <p id="75">③ <i>D</i>= (<i>D</i><sub>1</sub>, <i>D</i><sub>2</sub>, …, <i>D</i><sub><i>n</i></sub>) 为防御者动作集合, 其中<i>D</i><sub><i>k</i></sub>={<i>d</i><sub>1</sub>, <i>d</i><sub>2</sub>, …, <i>d</i><sub><i>m</i></sub>}为防御者在博弈状态<i>S</i><sub><i>k</i></sub>的动作集合;</p>
                </div>
                <div class="p1">
                    <p id="76">④ <i>R</i><sub><i>d</i></sub> (<i>s</i><sub><i>i</i></sub>, <i>d</i>, <i>s</i><sub><i>j</i></sub>) 为防御者在状态<i>s</i><sub><i>i</i></sub>执行防御动作<i>d</i>网络转移到状态<i>s</i><sub><i>j</i></sub>后的立即回报;</p>
                </div>
                <div class="p1">
                    <p id="77">⑤ <i>Q</i><sub><i>d</i></sub> (<i>s</i><sub><i>i</i></sub>, <i>d</i>) 为防御者的状态-动作收益函数, 表示在状态<i>s</i><sub><i>i</i></sub>下防御者采取动作<i>d</i>后的期望收益;</p>
                </div>
                <div class="p1">
                    <p id="78">⑥ <i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>) 为防御者在状态<i>s</i><sub><i>k</i></sub>的防御策略.</p>
                </div>
                <div class="p1">
                    <p id="79">防御策略与防御动作是2个不同的概念, 防御策略是防御动作的规则, 而不是动作本身.防御策略以概率的形式规定了防御者在每个网络状态选择什么动作, 如<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>) = (<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub>1</sub>) , <i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub>2</sub>) , …, <i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub><i>m</i></sub>) ) 为防御者在网络状态<i>s</i><sub><i>k</i></sub>的策略, <i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub><i>m</i></sub>) ) 为其选择动作<i>d</i><sub><i>m</i></sub>的概率, 其中<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>d</mi><mo>∈</mo><mi>D</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></munder><mi>π</mi></mstyle><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>.</mo></mrow></math></mathml></p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>2.3 基于攻防图的网络状态与攻防动作提取方法</b></h4>
                <div class="p1">
                    <p id="82">网络状态与攻防动作是随机博弈模型的重要组成部分, 对网络状态和攻防动作的提取是构建攻防随机博弈模型的一个关键点<citation id="416" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.现有攻防随机博弈在进行网络状态描述时, 每个网络状态包含当前网络所有节点的安全要素, 网络状态的数量是安全要素的幂集, 会产生“状态爆炸”<citation id="417" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.为此, 提出以主机为中心的攻防图模型, 每个状态节点仅描述主机状态, 可以有效压缩状态节点规模<citation id="418" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.利用此攻防图提取的网络状态及攻防动作更有利于进行网络攻防对抗分析.</p>
                </div>
                <div class="p1">
                    <p id="83"><b>定义2</b>. 攻防图是一个二元组<i>G</i>= (<i>S</i>, <i>E</i>) .其中, <i>S</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>n</i></sub>}是节点安全状态集合, <i>s</i><sub><i>i</i></sub>=&lt;<i>host</i>, <i>privilege</i>&gt;, 其中<i>host</i>是节点的唯一标识, <i>privilege</i>={<i>none</i>, <i>user</i>, <i>root</i>}分别表示不具有任何权限、具有普通用户权限、具有管理员权限.<i>E</i>= (<i>E</i><sub><i>a</i></sub>, <i>E</i><sub><i>d</i></sub>) 为有向边, 表示攻击动作或防御动作的发生引起节点状态的转移, <i>e</i><sub><i>k</i></sub>= (<i>s</i><sub><i>r</i></sub>, <i>v</i>/<i>d</i>, <i>s</i><sub><i>d</i></sub>) , <i>k</i>=<i>a</i>, <i>d</i>, 其中<i>s</i><sub><i>r</i></sub>为源结点, <i>s</i><sub><i>d</i></sub>为目的结点.</p>
                </div>
                <div class="p1">
                    <p id="84">攻防图的生成过程如图1所示.首先对目标网络扫描获取网络安全要素, 然后与攻击模板结合进行攻击实例化, 再与防御模板结合进行防御实例化, 最后生成攻防图.攻防随机博弈模型的状态集合由攻防图节点提取, 防御动作集合由攻防图的边提取.</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 攻防图生成" src="Detail/GetImg?filename=images/JFYZ201905006_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 攻防图生成  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Attack-defense graph generation</p>

                </div>
                <h4 class="anchor-tag" id="86" name="86">1) 网络安全要素</h4>
                <div class="p1">
                    <p id="87">网络安全要素<i>NSE</i>由网络连接关系矩阵<i>C</i>、节点脆弱性信息<i>V</i>、节点服务信息<i>F</i>、节点访问权限<i>P</i>组成.其中, <i>C</i>=<i>host</i>×<i>host</i>×<i>port</i>描述节点之间的连接关系, 矩阵的行表示源节点<i>shost</i>, 矩阵的列表示目的节点<i>dhost</i>, 矩阵元素表示<i>shost</i>到<i>dhost</i>的端口<i>port</i>访问关系, 当<i>port</i>=∅时, 表示<i>shost</i>与<i>dhost</i>之间不存在连接关系;<i>V</i>=&lt;<i>host</i>, <i>service</i>, <i>cveid</i>&gt;表示节点<i>host</i>上的服务<i>service</i>存在脆弱性<i>cveid</i>, 包括系统软件、应用软件存在的安全漏洞和配置不当或配置错误引起的安全漏洞;<i>F</i>=&lt;<i>host</i>, <i>service</i>&gt;表示节点<i>host</i>上开启服务<i>service</i>;<i>P</i>=&lt;<i>host</i>, <i>privilege</i>&gt;表示攻击者在节点<i>host</i>上拥有<i>privilege</i>访问权限.</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">2) 攻击模板</h4>
                <div class="p1">
                    <p id="89">攻击模板<i>AM</i>是对脆弱性利用的描述:<i>AM</i>=&lt;<i>tid</i>, <i>prec</i>, <i>postc</i>&gt;.其中, <i>tid</i>是攻击模式标识;<i>prec</i>=&lt;<i>P</i>, <i>V</i>, <i>C</i>, <i>F</i>&gt;描述攻击者利用一个脆弱性所需具备的前提条件集合, 包括攻击者在源节点<i>shost</i>上具有的初始访问权限<i>privilege</i>、目标节点的脆弱性信息<i>cveid</i>、网络连接关系<i>C</i>、节点运行服务<i>F</i>, 只有满足该条件集合, 攻击者才能成功利用该脆弱性;<i>postc</i>=&lt;<i>P</i>, <i>C</i>, <i>sd</i>&gt;描述攻击者成功利用一个脆弱性而产生的后果, 包括攻击者在目标节点上获得权限的提升、网络连接关系的变化以及服务破坏等.</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">3) 防御模板</h4>
                <div class="p1">
                    <p id="91">防御模板<i>DM</i>是防御者在预测或者识别攻击后采取的响应措施:<i>DM</i>=&lt;<i>tid</i>, <i>dset</i>&gt;, <i>tid</i>是攻击标识, <i>dset</i>={&lt;<i>d</i><sub>1</sub>, <i>postd</i><sub>1</sub>&gt;, &lt;<i>d</i><sub>2</sub>, <i>postd</i><sub>2</sub>&gt;, …, &lt;<i>d</i><sub><i>m</i></sub>, <i>postd</i><sub><i>m</i></sub>&gt;}是应对特定攻击的防御策略集.其中, <i>d</i><sub><i>i</i></sub>是防御策略标识;<i>postd</i><sub><i>i</i></sub>=&lt;<i>F</i>, <i>V</i>, <i>P</i>, <i>C</i>&gt;描述防御策略对网络安全要素的影响, 包括对节点服务信息、节点漏洞信息、攻击者权限信息、节点连接关系等的影响.</p>
                </div>
                <div class="p1">
                    <p id="92">攻防图生成过程中如果2个节点间具有连通关系, 同时满足攻击发生所需的所有前提条件, 则增加从源节点到目的节点的边, 如果攻击改变了连通关系等安全要素, 那么要及时对网络安全要素进行更新;如果防御策略实施改变了节点的连通关系或者攻击者的既有权限, 则增加从目的节点到源节点的边, 具体如算法1所示, 其中行①是利用网络安全要素生成所有可能状态节点并初始化边;行②～ (11) 是攻击实例化, 生成所有攻击边;行 (12) ～ (18) 是防御实例化, 生成所有防御边;行 (19) ～ (23) 是去除所有孤立节点;行 (24) 是输出攻防图.</p>
                </div>
                <div class="p1">
                    <p id="93"><b>算法1</b>. 攻防图生成算法.</p>
                </div>
                <div class="p1">
                    <p id="94">输入:网络安全要素<i>NSE</i>、攻击模板<i>AM</i>、防御模板<i>DM</i>;</p>
                </div>
                <div class="p1">
                    <p id="95">输出:攻防图<i>G</i>= (<i>S</i>, <i>E</i>) .</p>
                </div>
                <div class="area_img" id="308">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201905006_30800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="308">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201905006_30801.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="120">假设目标网络的节点数为<i>n</i>, 每个节点的脆弱性个数为<i>m</i>, 则攻防图中的节点数目最多为3<i>n</i>.在攻击实例化阶段分析每两个节点之间连接关系的计算复杂度为<i>O</i> (<i>n</i><sup>2</sup>-<i>n</i>) , 节点的脆弱性与连接关系进行匹配的计算复杂度为<i>O</i> (<i>m</i> (<i>n</i><sup>2</sup>-<i>n</i>) ) ;在防御实例化与去除孤立节点阶段, 遍历其所有节点的出、入节点的边的计算复杂度为<i>O</i> (9<i>n</i><sup>2</sup>-3<i>n</i>) .综上可知, 该算法的计算复杂度为<i>O</i> (<i>n</i><sup>2</sup>) 数量级.攻防图<i>G</i>的节点可以提取网络状态, 攻防图<i>G</i>的边可以提取攻防动作.</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>3 基于WoLF-PHC的博弈分析与策略选取</b></h3>
                <div class="p1">
                    <p id="122">本节将强化学习机制引入到有限理性随机博弈中, 采用WoLF-PHC算法在AD-SGM基础上进行防御策略选取.</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>3.1 WoLF-PHC算法原理</b></h4>
                <h4 class="anchor-tag" id="124" name="124">3.1.1 Q-learning算法</h4>
                <div class="p1">
                    <p id="125">Q-learning<citation id="419" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>是WoLF-PHC算法的基础, 是一种典型的免模型强化学习算法, 其学习机制如图2所示.Q-learning中Agent通过与环境的交互获得回报和环境状态转移的知识, 知识用收益<i>Q</i><sub><i>d</i></sub>来表示, 通过更新<i>Q</i><sub><i>d</i></sub>来进行学习.其收益函数<i>Q</i><sub><i>d</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><mo stretchy="false">[</mo><mi>R</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>d</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>d</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>-</mo><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">其中, <i>α</i>为收益学习率;<i>γ</i>为折扣因子.Q-learning的策略为<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>d</mi></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_129.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Q-learning学习机制" src="Detail/GetImg?filename=images/JFYZ201905006_129.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Q-learning学习机制  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_129.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Q-learning learning mechanism</p>

                </div>
                <h4 class="anchor-tag" id="130" name="130">3.1.2 PHC算法</h4>
                <div class="p1">
                    <p id="131">爬山策略 (policy hill-climbing) 算法<citation id="420" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>是一种适用于混合策略的简单实用的梯度下降学习算法, 是对Q-learning的改进.PHC的状态-动作收益函数<i>Q</i><sub><i>d</i></sub>与Q-learning相同, 但不再沿用Q-learning的策略更新方式, 而是通过执行爬山算法对混合策略<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>) 进行更新, <i>δ</i>为策略学习率.</p>
                </div>
                <div class="p1">
                    <p id="132"><i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub><i>i</i></sub>) ←<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub><i>i</i></sub>) +<i>Δ</i><sub><i>s</i><sub><i>k</i></sub><i>d</i><sub><i>i</i></sub></sub>, (2) </p>
                </div>
                <div class="p1">
                    <p id="133">其中:</p>
                </div>
                <div class="area_img" id="135">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201905006_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="137"><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mrow><mi>min</mi></mrow><mrow><mo> (</mo><mrow><mi>π</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mfrac><mi>δ</mi><mrow><mo stretchy="false">|</mo><mi>D</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mn>1</mn><mo stretchy="false">|</mo></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>. (4) </p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">3.1.3 WoLF-PHC算法</h4>
                <div class="p1">
                    <p id="140">WoLF-PHC (狼爬山策略) 算法是对PHC算法的改进.通过引入WoLF机制, 使防御者具有2种不同的策略学习率, 当获胜时采用低策略学习率<i>δ</i><sub><i>w</i></sub>, 当失败时采用高策略学习率<i>δ</i><sub><i>l</i></sub>, 如式 (5) 所示.2个学习率使得防御者在比预期表现差时能快速适应攻击者的策略, 比预期表现好时能谨慎学习.最重要的是WoLF机制的引入, 保证了算法的收敛性<citation id="421" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.WoLF-PHC算法采用平均策略作为胜利和失败的判断标准:</p>
                </div>
                <div class="area_img" id="141">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201905006_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="143"><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>π</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>π</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>+</mo><mfrac><mn>1</mn><mrow><mi>C</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow></mfrac><mo stretchy="false"> (</mo><mi>π</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>π</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>, (6) </p>
                </div>
                <div class="p1">
                    <p id="145"><i>C</i> (<i>s</i>) =<i>C</i> (<i>s</i>) +1. (7) </p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>3.2 基于资格迹的改进WoLF-PHC及防御决策算法</b></h4>
                <div class="p1">
                    <p id="147">为了提高WoLF-PHC算法的学习速度, 减少算法对数据量的依赖程度, 引入资格迹对WoLF-PHC进行改进.资格迹能跟踪最近访问的特定状态-动作轨迹, 然后将当前回报分配给最近访问的状态-动作.WoLF-PHC算法是对Q-learning算法的扩展, 目前有很多资格迹与Q-learning结合的算法, 本文借鉴其中典型的Peng’s <i>Q</i> (<i>λ</i>) 算法<citation id="422" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>对WoLF-PHC进行改进.定义每个状态-动作的资格迹为<i>e</i> (<i>s</i>, <i>a</i>) , 设当前网络状态为<i>s</i><sup>*</sup>, 资格迹更新:</p>
                </div>
                <div class="area_img" id="148">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201905006_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="150">其中, <i>λ</i>为轨迹衰减因子.</p>
                </div>
                <div class="p1">
                    <p id="151">WoLF-PHC算法由Q-learning算法扩展而来, 属于off-policy算法, 即在每个网络状态评估防御动作时使用greedy policy, 而选择执行防御动作时为了学习偶尔会引入non-greedy policy.为了保持WoLF-PHC算法的off-policy特点, 采用式 (9) ～ (12) 对状态-动作值进行更新, 其中<i>d</i><sup>*</sup>为在<i>s</i><sup>*</sup>被选择执行的防御动作.由于只有最近被访问的状态-动作对的资格迹才会明显大于0, 而其余绝大数状态-动作对的资格迹几乎为0.为了减少资格迹带来的内存和运算时的消耗, 在实际应用时可以只保存和更新最近状态-动作对的资格迹.</p>
                </div>
                <div class="p1">
                    <p id="152"><i>Q</i><sub><i>d</i></sub> (<i>s</i><sup>*</sup>, <i>d</i><sup>*</sup>) =<i>Q</i><sub><i>d</i></sub> (<i>s</i><sup>*</sup>, <i>d</i><sup>*</sup>) +<i>α ρ</i><sup>*</sup>, (9) </p>
                </div>
                <div class="p1">
                    <p id="153"><i>Q</i><sub><i>d</i></sub> (<i>s</i>, <i>d</i>) =<i>Q</i><sub><i>d</i></sub> (<i>s</i>, <i>d</i>) +<i>α ρ</i><sub><i>g</i></sub><i>e</i> (<i>s</i>, <i>d</i>) , (10) </p>
                </div>
                <div class="p1">
                    <p id="154" class="code-formula">
                        <mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ρ</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mi>R</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>d</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>d</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>d</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mi>g</mi></msub><mo>=</mo><mi>R</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>d</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>+</mo><mi>γ</mi><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>d</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>d</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>d</mi></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">基于WoLF-PHC防御决策方法要想取得较好效果, 还需要对<i>α</i>, <i>δ</i>, <i>λ</i>和<i>γ</i>这4个参数进行合理设置.1) 收益学习率<i>α</i>取值范围为0&lt;<i>α</i>&lt;1, <i>α</i>越大代表越靠后的累积奖赏越重要, 学习速度也更快;<i>α</i>越小算法的稳定性越好.2) 策略学习率<i>δ</i>取值范围为0&lt;<i>δ</i>&lt;1, 根据实验得出<citation id="423" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 采取<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mi>δ</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></mfrac><mo>=</mo><mn>4</mn></mrow></math></mathml>时能够取得较好效果.3) 资格迹衰减因子<i>λ</i>取值范围为0&lt;<i>λ</i>&lt;1, 负责对状态-动作分配信誉, 可以看作时间的标度, <i>λ</i>越大则分配给历史状态-动作的信誉越大.4) 折扣因子<i>γ</i>取值范围为0&lt;<i>γ</i>&lt;1, 代表防御者对立即回报与未来回报的偏好.当<i>γ</i>接近于0时, 表示未来回报无关紧要, 更看重立即回报;当<i>γ</i>接近于1时, 代表立即回报无关紧要, 更看重未来回报.</p>
                </div>
                <div class="p1">
                    <p id="158">WoLF-PHC中的Agent对应攻防随机博弈模型AD-SGM中的防御者, Agent的状态对应AD-SGM中的博弈状态, Agent的行为对应AD-SGM中的防御动作, Agent的立即回报对应AD-SGM中的立即回报, Agent的策略对应AD-SGM中的防御策略.</p>
                </div>
                <div class="p1">
                    <p id="159">在上述基础上给出具体的防御决策算法.算法2行①是对攻防随机博弈模型AD-SGM和相关参数的初始化, 其中网络状态和攻防动作由算法1提取, 行②是防御者检测当前网络状态, 行③～ (22) 是进行防御决策和在线学习, 其中行④～⑤是依据当前策略选取防御动作, 行⑥～ (14) 是利用资格迹对收益<i>Q</i><sub><i>d</i></sub>进行更新, 行 (15) ～ (21) 是依据新的收益<i>Q</i><sub><i>d</i></sub>利用爬山算法更新防御策略<i>π</i><sub><i>d</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="160"><b>算法2</b>. 防御决策算法.</p>
                </div>
                <div class="p1">
                    <p id="161">输入:AD-SGM;参数<i>α</i>, <i>δ</i>, <i>λ</i>和<i>γ</i>;</p>
                </div>
                <div class="p1">
                    <p id="162">输出:防御动作<i>d</i>.</p>
                </div>
                <div class="area_img" id="309">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201905006_30900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="309">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201905006_30901.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="191">算法2的空间复杂度主要集中在对<mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>, </mo><mrow><mi>e</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>, </mo></mrow><mi>π</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>π</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo></mrow></math></mathml>和<i>Q</i><sub><i>d</i></sub> (<i>s</i>, <i>d</i>) 的存储, 设|<i>S</i>|为状态数, |<i>D</i>|为每个状态防御者的措施数, 则空间复杂度为<i>O</i> (4|<i>S</i>||<i>D</i>|+|<i>S</i>|<sup>2</sup>|<i>D</i>|) .算法不需要对博弈均衡进行求解, 与现有随机博弈模型相比大大减少了计算复杂度, 增加了算法的实用性.</p>
                </div>
                <h3 id="193" name="193" class="anchor-tag"><b>4 实验分析</b></h3>
                <h4 class="anchor-tag" id="194" name="194"><b>4.1 实验场景描述</b></h4>
                <div class="p1">
                    <p id="195">为验证本文方法的有效性, 搭建如图3所示的典型企业网络进行实验.攻防事件发生在内网, 攻击者来自外网.网络管理员作为防御者, 负责内网的安全.由于防火墙1和防火墙2的设置, 导致外网正常用户只能访问Web服务器, 而Web服务器可以访问数据库服务器、FTP服务器和电子邮件服务器.利用Nessus工具对实验网络进行扫描, 实验网络脆弱性信息如表1所示.</p>
                </div>
                <div class="area_img" id="196">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_196.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 实验网络结构" src="Detail/GetImg?filename=images/JFYZ201905006_196.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 实验网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_196.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Experimental network topology</p>

                </div>
                <div class="p1">
                    <p id="197">参考MIT林肯实验室攻防行为数据库构建攻击、防御模板, 采用<i>A</i>标识攻击者主机、<i>W</i>标识Web服务器、<i>D</i>标识数据库服务器、<i>F</i>标识FTP服务器、<i>E</i>标识电子邮件服务器, 利用算法1生成网络攻防图, 为便于展示和描述, 将攻防图分为攻击图和防御图, 分别如图4和图5所示.防御图中防御动作含义如表2所示.</p>
                </div>
                <div class="area_img" id="198">
                    <p class="img_tit"><b>表1 网络脆弱性信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Network Vulnerability Information</b></p>
                    <p class="img_note"></p>
                    <table id="198" border="1"><tr><td><br />Attack<br />Identifier</td><td>Host</td><td>CVE</td><td>Target<br />Privilege</td></tr><tr><td><br /><i>tid</i><sub>1</sub></td><td>Web Server</td><td>CVE-2015-1635</td><td>user</td></tr><tr><td><br /><i>tid</i><sub>2</sub></td><td>Web Server</td><td>CVE-2017-7269</td><td>root</td></tr><tr><td><br /><i>tid</i><sub>3</sub></td><td>Web Server</td><td>CVE-2014-8517</td><td>root</td></tr><tr><td><br /><i>tid</i><sub>4</sub></td><td>FTP Server</td><td>CVE-2014-3556</td><td>root</td></tr><tr><td><br /><i>tid</i><sub>5</sub></td><td>E-mail Server</td><td>CVE-2014-4877</td><td>root</td></tr><tr><td><br /><i>tid</i><sub>6</sub></td><td>Database Server</td><td>CVE-2013-4730</td><td>user</td></tr><tr><td><br /><i>tid</i><sub>7</sub></td><td>Database Server</td><td>CVE-2016-6662</td><td>root</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="199">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_199.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 攻击图" src="Detail/GetImg?filename=images/JFYZ201905006_199.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 攻击图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_199.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Attack graph</p>

                </div>
                <div class="area_img" id="200">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 防御图" src="Detail/GetImg?filename=images/JFYZ201905006_200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 防御图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Defense graph</p>

                </div>
                <div class="area_img" id="201">
                    <p class="img_tit"><b>表2 防御动作描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Defense Action Description</b></p>
                    <p class="img_note"></p>
                    <table id="201" border="1"><tr><td><br />Atomic Defense Action</td><td><i>d</i><sub>1</sub></td><td><i>d</i><sub>2</sub></td><td><i>d</i><sub>3</sub></td><td><i>d</i><sub>4</sub></td><td><i>d</i><sub>5</sub></td><td><i>d</i><sub>6</sub></td><td><i>d</i><sub>7</sub></td></tr><tr><td><br />Renew Root Data</td><td>√</td><td></td><td>√</td><td></td><td>√</td><td>√</td><td></td></tr><tr><td><br />Limit SYN/ICMP Packets</td><td></td><td>√</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><br />Install Oracle Patches</td><td>√</td><td></td><td></td><td></td><td></td><td></td><td>√</td></tr><tr><td><br />Reinstall Listener Program</td><td>√</td><td></td><td></td><td></td><td>√</td><td></td><td></td></tr><tr><td><br />Uninstall Delete Trojan</td><td></td><td>√</td><td></td><td></td><td></td><td>√</td><td></td></tr><tr><td><br />Limit Access to MDSYS</td><td></td><td>√</td><td></td><td>√</td><td></td><td></td><td></td></tr><tr><td><br />Restart Database Server</td><td></td><td></td><td>√</td><td>√</td><td>√</td><td></td><td></td></tr><tr><td><br />Delete Suspicious Account</td><td></td><td>√</td><td></td><td></td><td></td><td></td><td>√</td></tr><tr><td><br />Add Physical Resource</td><td>√</td><td></td><td></td><td>√</td><td>√</td><td>√</td><td></td></tr><tr><td><br />Repair Database</td><td></td><td></td><td>√</td><td>√</td><td></td><td></td><td>√</td></tr><tr><td><br />Limit Packets from Ports</td><td>√</td><td>√</td><td>√</td><td></td><td></td><td>√</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: “√” means <i>d</i><sub><i>i</i></sub> contain the atomic defense action.</p>
                </div>
                <h4 class="anchor-tag" id="202" name="202"><b>4.2 构建实验场景的AD-SGM</b></h4>
                <div class="p1">
                    <p id="203">① <i>N</i>= (<i>attacker</i>, <i>defender</i>) 为参与博弈的局中人, 分别代表网络攻击者和防御者.</p>
                </div>
                <div class="p1">
                    <p id="204">② 随机博弈状态集合<i>S</i>= (<i>s</i><sub>0</sub>, <i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, <i>s</i><sub>3</sub>, <i>s</i><sub>4</sub>, <i>s</i><sub>5</sub>, <i>s</i><sub>6</sub>) , 随机博弈状态由网络状态组成, 由图4和图5中的节点提取.</p>
                </div>
                <div class="p1">
                    <p id="205">③ 防御者动作集合为</p>
                </div>
                <div class="p1">
                    <p id="206"><i>D</i>= (<i>D</i><sub>0</sub>, <i>D</i><sub>1</sub>, <i>D</i><sub>2</sub>, <i>D</i><sub>3</sub>, <i>D</i><sub>4</sub>, <i>D</i><sub>5</sub>, <i>D</i><sub>6</sub>) , </p>
                </div>
                <div class="p1">
                    <p id="207">其中, <i>D</i><sub>0</sub>={NULL}, <i>D</i><sub>1</sub>={<i>d</i><sub>1</sub>, <i>d</i><sub>2</sub>}, <i>D</i><sub>2</sub>={<i>d</i><sub>3</sub>, <i>d</i><sub>4</sub>}, <i>D</i><sub>3</sub>={<i>d</i><sub>1</sub>, <i>d</i><sub>5</sub>, <i>d</i><sub>6</sub>}, <i>D</i><sub>4</sub>={<i>d</i><sub>1</sub>, <i>d</i><sub>5</sub>, <i>d</i><sub>6</sub>}, <i>D</i><sub>5</sub>={<i>d</i><sub>1</sub>, <i>d</i><sub>2</sub>, <i>d</i><sub>7</sub>}, <i>D</i><sub>6</sub>={<i>d</i><sub>3</sub>, <i>d</i><sub>4</sub>}, 由图5的边提取.</p>
                </div>
                <div class="p1">
                    <p id="208">④ 防御者立即回报<i>R</i><sub><i>d</i></sub> (<i>s</i><sub><i>i</i></sub>, <i>d</i>, <i>s</i><sub><i>j</i></sub>) 的量化<citation id="424" type="reference"><link href="376" rel="bibliography" /><link href="396" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">22</a>]</sup></citation>结果为</p>
                </div>
                <div class="p1">
                    <p id="209"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>0</sub>, NULL, <i>s</i><sub>0</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>0</sub>, NULL, <i>s</i><sub>1</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>0</sub>, NULL, <i>s</i><sub>2</sub>) ) = (0, -40, -59) ;</p>
                </div>
                <div class="p1">
                    <p id="210"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>1</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>0</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>1</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>1</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>1</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>1</sub>, <i>d</i><sub>2</sub>, <i>s</i><sub>0</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>1</sub>, <i>d</i><sub>2</sub>, <i>s</i><sub>1</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>1</sub>, <i>d</i><sub>2</sub>, <i>s</i><sub>2</sub>) ) = (40, 0, -29, 5, -15, -32) ;</p>
                </div>
                <div class="p1">
                    <p id="211"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>0</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>1</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>5</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>0</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>1</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>5</sub>) ) = (24, 9, -15, -55, -49, -65, 19, 5, -21, -61, -72, -68) ;</p>
                </div>
                <div class="p1">
                    <p id="212"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>5</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>5</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>5</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>6</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>6</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>3</sub>, <i>d</i><sub>6</sub>, <i>s</i><sub>6</sub>) ) = (21, -16, -72, 15, -23, -81, -21, -36, -81) ;</p>
                </div>
                <div class="p1">
                    <p id="213"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>5</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>5</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>5</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>6</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>6</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>4</sub>, <i>d</i><sub>6</sub>, <i>s</i><sub>6</sub>) ) = (26, 0, -62, 11, -23, -75, 9, -25, -87) ;</p>
                </div>
                <div class="p1">
                    <p id="214"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>5</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>1</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>2</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>2</sub>, <i>s</i><sub>5</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>2</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>7</sub>, <i>s</i><sub>2</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>7</sub>, <i>s</i><sub>5</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>5</sub>, <i>d</i><sub>7</sub>, <i>s</i><sub>6</sub>) ) = (29, 0, -63, 11, -21, -76, 2, -27, -88) ;</p>
                </div>
                <div class="p1">
                    <p id="215"> (<i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>5</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>3</sub>, <i>s</i><sub>6</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>3</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>4</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>5</sub>) , <i>R</i><sub><i>d</i></sub> (<i>s</i><sub>6</sub>, <i>d</i><sub>4</sub>, <i>s</i><sub>6</sub>) ) = (-23, -21, -19, -42, -28, -31, -24, -49) .</p>
                </div>
                <div class="p1">
                    <p id="216">⑤ 为了更充分地检测算法的学习性能, 防御者的状态-动作收益<i>Q</i><sub><i>d</i></sub> (<i>s</i><sub><i>i</i></sub>, <i>d</i>) 初始化时统一置0, 不引入额外的先验知识.</p>
                </div>
                <div class="p1">
                    <p id="217">⑥防御者的防御策略<i>π</i><sub><i>d</i></sub>采取平均策略进行初始化, 即<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub>1</sub>) =<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub>2</sub>) =…<i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i><sub><i>m</i></sub>) ) 且<mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>d</mi><mo>∈</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml><i>π</i><sub><i>d</i></sub> (<i>s</i><sub><i>k</i></sub>, <i>d</i>) =1, ∀<i>s</i><sub><i>k</i></sub>∈<i>S</i>, 不引入额外的先验知识.</p>
                </div>
                <h4 class="anchor-tag" id="219" name="219"><b>4.3 测试与分析</b></h4>
                <div class="p1">
                    <p id="220">本节的实验有3个目的:1) 为了测试不同参数设置对算法的影响, 从而找出适用于本场景的实验参数;2) 将本文方法与现有典型方法进行对比, 验证本文方法的先进性;3) 测试基于资格迹对WoLF-PHC算法改进的有效性.</p>
                </div>
                <div class="area_img" id="221">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_221.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同参数设置下的防御决策" src="Detail/GetImg?filename=images/JFYZ201905006_221.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同参数设置下的防御决策  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_221.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Defense decision making under different parameter settings</p>

                </div>
                <div class="p1">
                    <p id="222">由图4和图5可知, 状态<i>s</i><sub>2</sub>的攻防策略选取情况最复杂、最具有代表性, 故实验选取状态<i>s</i><sub>2</sub>对算法的性能进行分析, 其余网络状态分析方式相同不再赘述.</p>
                </div>
                <h4 class="anchor-tag" id="223" name="223">4.3.1 参数测试与分析</h4>
                <div class="p1">
                    <p id="224">不同的参数取值会影响学习的速度和效果, 目前并没有相关理论能够确定具体的参数设置.第4节中对相关参数做了初步分析, 在此基础上, 这里对不同的参数设置做进一步测试, 寻找适用于本攻防场景的参数设置.实验对6种不同的参数设置进行了测试.具体的参数设置如表3所示.实验中攻击者初始策略为随机策略, 学习机制与本文方法相同.</p>
                </div>
                <div class="area_img" id="225">
                    <p class="img_tit"><b>表3 不同参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Different Parameter Settings</b></p>
                    <p class="img_note"></p>
                    <table id="225" border="1"><tr><td><br />Set Number</td><td><i>α</i></td><td><i>δ</i><sub><i>l</i></sub></td><td><i>δ</i><sub><i>w</i></sub></td><td><i>λ</i></td><td><i>γ</i></td></tr><tr><td><br /><i>Set</i>1</td><td>0.01</td><td>0.004</td><td>0.001</td><td>0.01</td><td>0.01</td></tr><tr><td><br /><i>Set</i>2</td><td>0.1</td><td>0.004</td><td>0.001</td><td>0.01</td><td>0.01</td></tr><tr><td><br /><i>Set</i>3</td><td>0.01</td><td>0.004</td><td>0.001</td><td>0.01</td><td>0.1</td></tr><tr><td><br /><i>Set</i>4</td><td>0.01</td><td>0.004</td><td>0.001</td><td>0.1</td><td>0.01</td></tr><tr><td><br /><i>Set</i>5</td><td>0.01</td><td>0.04</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td><br /><i>Set</i>6</td><td>0.01</td><td>0.008</td><td>0.001</td><td>0.01</td><td>0.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="226">防御者在状态<i>s</i><sub>2</sub>对防御动作<i>d</i><sub>3</sub>和<i>d</i><sub>4</sub>的选择概率结果如图6所示.从图6中可以观测不同参数设置下算法的学习速度和收敛性.图6中显示设置参数<i>Set</i>1, <i>Set</i>3, <i>Set</i>6的学习速度较快, 3种设置下算法经过1 500次以内的学习即可得到最佳策略, 但<i>Set</i>3和<i>Set</i>6的收敛性较差.虽然<i>Set</i>3和<i>Set</i>6能学习到最佳策略, 但之后会出现震荡, 没有设置<i>Set</i>1的稳定性好.防御收益可以代表算法对策略的优化程度, 为了确保收益值不是只反应一次防御结果, 取1 000次防御收益的平均值, 其每1 000次的平均收益变化如图7所示.从图7中可以看到设置3的收益明显低于其他设置, 但其他设置的优劣难以区分.为了更加直观地显示, 对图7中不同设置下的25 000次防御收益计算平均值如图8所示.从图8中可以看到设置1和设置6的平均值较高, 为了进一步对比, 在平均值基础上进行一步计算标准差以反应收益的离散程度, 如图9所示, 设置1和设置2的标准差较小且设置1还要小于设置2.</p>
                </div>
                <div class="area_img" id="227">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_227.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同参数设置下的防御收益变化" src="Detail/GetImg?filename=images/JFYZ201905006_227.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同参数设置下的防御收益变化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_227.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Defense return under different parameter settings</p>

                </div>
                <div class="area_img" id="228">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_228.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同参数设置下的平均收益" src="Detail/GetImg?filename=images/JFYZ201905006_228.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同参数设置下的平均收益  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_228.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Average return under different parameter settings</p>

                </div>
                <div class="area_img" id="229">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_229.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同参数设置下的防御收益标准差" src="Detail/GetImg?filename=images/JFYZ201905006_229.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同参数设置下的防御收益标准差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_229.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Standard deviation of defense revenue under  different parameter settings</p>

                </div>
                <div class="p1">
                    <p id="230">综上可知:6组参数中设置1最适合于本场景.由于设置1已经取得了一个较理想的效果, 能够满足实验需求, 所以不再对参数做进一步的优化.</p>
                </div>
                <h4 class="anchor-tag" id="231" name="231">4.3.2 与典型博弈方法对比测试</h4>
                <div class="p1">
                    <p id="232">本节选取随机博弈中文献<citation id="425" type="reference">[<a class="sup">12</a>]</citation>和演化博弈中文献<citation id="426" type="reference">[<a class="sup">16</a>]</citation>与本文方法进行对比实验.依据攻击者学习能力的差异, 本节设计2组对比实验:第1组中攻击者学习能力较弱, 不会针对攻防结果做出调整;第2组中攻击者学习能力较强, 采取与本文方法相同的学习机制.2组实验中攻击者初始策略均为随机策略.</p>
                </div>
                <div class="p1">
                    <p id="233">第1组实验中本文方法的防御策略如图6中的 (a) 所示.由文献<citation id="427" type="reference">[<a class="sup">12</a>]</citation>方法计算所得的防御策略为<i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>) =0.7, <i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>) =0.3, 文献<citation id="428" type="reference">[<a class="sup">16</a>]</citation>的演化稳定均衡的防御策略为<i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>) =0.8, <i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>) =0.2.其每1 000次的平均收益变化如图10所示.由3种方法的策略和收益结果可知, 本文方法能依据攻击者的策略进行学习从而调整到最优策略, 因此本文方法能够获得最高收益.文献<citation id="429" type="reference">[<a class="sup">12</a>]</citation>的方法面对任何攻击者时都采取固定策略, 当攻击者因受有限理性的约束而没有采取纳什均衡策略时, 该方法所获收益较低.文献<citation id="430" type="reference">[<a class="sup">16</a>]</citation>方法虽然考虑了攻防双方的学习因素, 但是模型所需参数很难准确量化, 导致最终结果与实际存在偏差, 因此该方法收益仍低于本文方法.</p>
                </div>
                <div class="area_img" id="234">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_234.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 防御收益变化对比" src="Detail/GetImg?filename=images/JFYZ201905006_234.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 防御收益变化对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_234.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Contrast of defense revenue change</p>

                </div>
                <div class="p1">
                    <p id="235">第2组实验中文献<citation id="431" type="reference">[<a class="sup">12</a>]</citation>和文献<citation id="432" type="reference">[<a class="sup">16</a>]</citation>计算结果仍为<i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>) =0.7, <i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>) =0.3和<i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>3</sub>) =0.8, <i>π</i><sub><i>d</i></sub> (<i>s</i><sub>2</sub>, <i>d</i><sub>4</sub>) =0.2, 本文方法的决策如图11所示, 本文方法在经过1 800次左右的学习后达到稳定, 收敛到了与文献<citation id="433" type="reference">[<a class="sup">12</a>]</citation>相同的防御策略.从图12中可以看到, 文献<citation id="434" type="reference">[<a class="sup">16</a>]</citation>的收益要低于其他2种方法, 本文方法在前2 000次防御的平均收益要高于文献<citation id="435" type="reference">[<a class="sup">12</a>]</citation>, 随后与文献<citation id="436" type="reference">[<a class="sup">12</a>]</citation>的收益大体相同.结合图11和图12可知, 具有学习能力的攻击者在初始阶段无法得到纳什均衡策略, 此时本文方法要优于文献<citation id="437" type="reference">[<a class="sup">12</a>]</citation>, 当攻击者经过学习得到纳什均衡策略后, 本文方法也能收敛到纳什均衡策略, 此时本文方法与文献<citation id="438" type="reference">[<a class="sup">12</a>]</citation>性能相同.</p>
                </div>
                <div class="area_img" id="236">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_236.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 本文方法的防御决策" src="Detail/GetImg?filename=images/JFYZ201905006_236.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 本文方法的防御决策  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_236.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Defense decision-making method in  this paper</p>

                </div>
                <div class="area_img" id="237">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_237.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 防御收益变化对比" src="Detail/GetImg?filename=images/JFYZ201905006_237.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 防御收益变化对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_237.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Contrast of defense revenue change</p>

                </div>
                <div class="p1">
                    <p id="238">综上, 可知当面对学习能力较弱的攻击者时, 本文方法优于文献<citation id="439" type="reference">[<a class="sup">12</a>]</citation>和文献<citation id="440" type="reference">[<a class="sup">16</a>]</citation>的方法.当面对学习能力较强的攻击者时, 如果攻击者尚未通过学习得到纳什均衡, 此时本文方法仍然优于文献<citation id="441" type="reference">[<a class="sup">12</a>]</citation>和文献<citation id="442" type="reference">[<a class="sup">16</a>]</citation>;如果攻击者通过学习得到纳什均衡, 此时本文也能通过学习得到与文献<citation id="443" type="reference">[<a class="sup">12</a>]</citation>相同的纳什均衡策略, 取得与其相同的效果, 并优于文献<citation id="444" type="reference">[<a class="sup">16</a>]</citation>.</p>
                </div>
                <h4 class="anchor-tag" id="239" name="239">4.3.3 有无资格迹的对比测试</h4>
                <div class="p1">
                    <p id="240">本节关于资格迹对算法影响的实际效果进行测试.有无资格迹对策略选取的影响结果如图13所示.从图13中可以看到有资格迹时算法的学习速度较快, 经过1 000次的学习后即可收敛到最优策略, 而没有资格迹时, 算法需要经过2 500次左右的学习才能够收敛.</p>
                </div>
                <div class="area_img" id="241">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_241.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 防御决策对比" src="Detail/GetImg?filename=images/JFYZ201905006_241.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 防御决策对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_241.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Comparison of defense decision making</p>

                </div>
                <div class="p1">
                    <p id="242">每1 000次的平均收益变化对比如图14所示.从图14中可以看到收敛以后有、无资格迹时算法的收益大体相同.从图14中可以看到收敛前的3 000次防御, 有资格迹收益要比无资格迹的收益高, 为了对其进一步验证, 分别统计有、无资格迹下前3 000次防御收益的平均值, 各统计10次, 结果如图15所示.图15进一步证明在收敛前的防御阶段, 有资格迹要比无资格迹的防御效果好.</p>
                </div>
                <div class="area_img" id="243">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_243.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 防御收益变化对比" src="Detail/GetImg?filename=images/JFYZ201905006_243.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 防御收益变化对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_243.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 14 Contrast of defense revenue change</p>

                </div>
                <div class="area_img" id="244">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201905006_244.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 平均防御收益对比" src="Detail/GetImg?filename=images/JFYZ201905006_244.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 平均防御收益对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201905006_244.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 15 Contrast of average defense revenue</p>

                </div>
                <div class="p1">
                    <p id="245">资格迹的引入加快了学习速度, 同时也会带来额外的内存和运算开销.实验中只保存最近被访问的10个状态-动作对, 有效减少了内存消耗的增加.为了测试资格迹带来的运算开销, 分别统计了20次有、无资格迹时算法进行10万次防御决策的时间, 其20次的平均值为:有资格迹9.51 s, 无资格迹3.74 s.虽然资格迹的引入会使得决策时间增加近2.5倍, 但是引入资格迹后10万次的决策所需时间仍然只有9.51 s, 仍可以满足实时性的需求.</p>
                </div>
                <div class="p1">
                    <p id="246">综上, 引入资格迹后, 以牺牲少量内存和运算开销, 可以有效增加算法的学习速度, 提高防御收益.</p>
                </div>
                <h4 class="anchor-tag" id="247" name="247"><b>4.4 方法综合比较</b></h4>
                <div class="p1">
                    <p id="248">本文方法与一些典型研究成果综合对比, 如表4所示:</p>
                </div>
                <div class="area_img" id="249">
                    <p class="img_tit"><b>表4 典型方法综合比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Comprehensive Comparison of Typical Methods</b></p>
                    <p class="img_note"></p>
                    <table id="249" border="1"><tr><td>Reference</td><td>Game Type</td><td>Model Assumption</td><td>Learning Mechanism</td><td>Game Process</td><td>Applicable Object</td><td>Practicability</td></tr><tr><td><br />Ref [3]</td><td>Stochastic Game</td><td>Rationality</td><td></td><td>Multi-stage</td><td>Personal</td><td>Bad</td></tr><tr><td><br />Ref [8]</td><td>Static Game</td><td>Rationality</td><td></td><td>Single-stage</td><td>Personal</td><td>Bad</td></tr><tr><td><br />Ref [10]</td><td>Dynamic Game</td><td>Rationality</td><td></td><td>Multi-stage</td><td>Personal</td><td>Bad</td></tr><tr><td><br />Ref [12]</td><td>Stochastic Game</td><td>Rationality</td><td></td><td>Multi-stage</td><td>Personal</td><td>Bad</td></tr><tr><td><br />Ref [16]</td><td>Evolutionary Game</td><td>Bounded Rationality</td><td>Biological Evolution</td><td></td><td>Group</td><td>Good</td></tr><tr><td><br />Our Method</td><td>Stochastic Game</td><td>Bounded Rationality</td><td>Reinforcement Learning</td><td>Multi-stage</td><td>Personal</td><td>Good</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="250">文献<citation id="447" type="reference">[<a class="sup">3</a>,<a class="sup">8</a>,<a class="sup">10</a>,<a class="sup">12</a>]</citation>以完全理性假设为前提, 其所得的均衡策略在实际中很难出现, 对实际的防御决策指导作用较低.文献<citation id="445" type="reference">[<a class="sup">16</a>]</citation>和本文以有限理性假设为前提, 更具有实用性, 但文献<citation id="446" type="reference">[<a class="sup">16</a>]</citation>基于生物进化理论, 主要针对群体演化进行研究, 其博弈分析的核心不是参与人的最优策略选择, 而是有限理性参与人组成的群体成员的策略调整过程、趋势和稳定性, 且此处的稳定性是指群体成员采用特定策略的比例不变, 而不是某个参与人的策略不变, 该方法不适用于指导个体实时决策.相比之下, 本文方法的防御者采用强化学习机制, 是在与攻击者对抗中依据系统的反馈进行学习, 更适用于个体策略的研究.</p>
                </div>
                <h3 id="251" name="251" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="252">本文针对防御决策问题, 在有限理性的约束下将网络攻防对抗抽象为一个随机博弈问题.在模型构建时提出了一种以主机为中心的攻防图模型用于网络状态及攻防动作提取并设计了攻防图生成算法, 有效压缩了博弈状态空间;在模型求解时提出了一种基于WoLF-PHC的防御决策方法, 使得有限理性下的防御者在面对不同攻击者时都能做出最优选择;基于资格迹对WoLF-PHC算法进行了改进, 加快了防御者的学习速度, 减少了算法对数据的依赖.本文方法既满足了有限理性的约束, 又不需要防御者获知过多攻击者信息, 是一种实用防御决策方法.</p>
                </div>
                <div class="p1">
                    <p id="253">下一步拟针对具体攻防场景对WoLF-PHC算法中的输赢标准做进一步优化, 以加快防御者学习速度, 增加防御收益.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="354">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201710021&amp;v=Mjc3Mjk5Yk5yNDlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEZ1ZMcktMeXZTZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Hu Qing, LüShichao, Shi Zhiqiang, et al.Advanced persistent threats detection game with expert system for cloud[J].Journal of Computer Research and Development, 2017, 54 (10) :2344-2355 (in Chinese) (胡晴, 吕世超, 石志强, 等.基于专家系统的高级持续性威胁云端检测博弈[J].计算机研究与发展, 2017, 54 (10) :2344-2355) 
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201502006&amp;v=MTczMDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGaURnVkxyS0x6N0Jkckc0SDlUTXJZOUZZb1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Wang Yuanzhuo, Yu Jianye, Qiu Wen, et al.Evolutionary Game Model and Analysis Methods for Network Group Behavior[J].Chinese Journal of Computers, 2015, 38 (2) :282-300
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES22E136CA16B0147B12D794CEDF9B0ECB&amp;v=MjEyNDh4YW89TmlmT2ZiRzZhOURQcWZ3MFplMTlESDA5eUdRUzZFdDZRWHVSMldaRGNNQ1VNTW50Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhnekwyNA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Lei Cheng, Zhang Hongqi, Wan Liming, et al.Incomplete information Markov game theoretic approach to strategy generation for moving target defense[J].Computer Communications, 2018, 116 (12) :184-199
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computing Nash equilibria and evolutionarily stable states of evolutionary games">

                                <b>[4]</b>Li Jiawei, Kendall G, John R.Computing nash equilibria and evolutionarily stable states of evolutionary games[J].IEEETransactions on Evolutionary Computation, 2016, 20 (3) :460-469
                            </a>
                        </p>
                        <p id="362">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702006942&amp;v=Mjc5NjFJOUhaT3NKQlhnN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SktGc1RieEE9TmlmT2ZiSzdIdEROcQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Bowling M, Veloso M.Multiagent learning using a variable learning rate[J].Artificial Intelligence, 2002, 136 (2) :215-250
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The boundedness conditions for model-free HDP (λ)">

                                <b>[6]</b>Seaar A D, Donald W.The boundedness conditions for model-free HDP (λ) [J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 125 (99) :1-15
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMC13BD3AEB1862B58D5054F0472230113&amp;v=Mjc4NTlDcGJRMzVOeGd6TDI0eGFvPU5pZklZOEM1SGFPNHJQNHdGdW9IQ241THloNW43ejk0VEFuaXFCVTNlN0dVUkx1Y0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>Do C T, Tran N H, Hong C, et al.Game theory for cyber security and privacy[J].ACM Computing Surveys, 2017, 50 (2) :30-31
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201203022&amp;v=MTk2ODJmVGJMRzRIOVBNckk5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZpRGdWTHJLTnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Liu Yuling, Feng Dengguo, Wu Lihui, et al.Performance evaluation of worm attack and defense strategies based on static Bayesian game[J].Journal of Software, 2012, 23 (3) :712-723 (in Chinese) (刘玉岭, 冯登国, 吴丽辉, 等.基于静态贝叶斯博弈的蠕虫攻防策略绩效评估[J].软件学报, 2012, 23 (3) :712-723) 
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A game-theoretic approach to fake-acknowledgment attack on cyber-physical systems">

                                <b>[9]</b>Li Yuzhe, Quevedo D E, Dey S, et al.A game-theoretic approach to fake-acknowledgment attack on cyber-physical systems[J].IEEE Transactions on Signal and Information Processing Over Networks, 2017, 3 (1) :1-11
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201702023&amp;v=Mjk0NTdMcktJVGZUZTdHNEg5Yk1yWTlIWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEZ1Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Zhang Hengwei, Li Tao.Optimal active defense based on multi-stage attack-defense signaling game[J].Acta Electronica Sinica, 2017, 45 (2) :431-439 (in Chinese) (张恒巍, 李涛.基于多阶段攻防信号博弈的最优主动防御[J].电子学报, 2017, 45 (2) :431-439) 
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Preventing DoS Attacks in Wireless Sensor Networks: A Repeated Game Theory Approach">

                                <b>[11]</b>Afrand A, Das S K.Preventing DoS attacks in wireless sensor networks:A repeated game theory approach[J].Internet Journal of Network Security, 2007, 5 (2) :145-153
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201010008&amp;v=MTc1NjdlWmVScUZpRGdWTHJLTHl2U2RMRzRIOUhOcjQ5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Jiang Wei, Fang Binxing, Tian Zhihong, et al.Research on defense strategies selection based on attack-defense stochastic game model[J].Journal of Computer Research and Development, 2010, 47 (10) :1714-1723 (in Chinese) (姜伟, 方滨兴, 田志宏, 等.基于攻防随机博弈模型的防御策略选取研究[J].计算机研究与发展, 2010, 47 (10) :1714-1723) 
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTLL201409025&amp;v=MzIyNzNpRGdWTHJLUFRuSFlyRzRIOVhNcG85SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Wang Changchun, Cheng Xiaohang, Zhu Yongwen, et al.AMarkov game model of computer network operation[J].Systems Engineering-Theory and Practice, 2014, 34 (9) :2402-2410 (in Chinese) (王长春, 程晓航, 朱永文, 等.计算机网络对抗行动策略的Markov博弈模型[J].系统工程理论与实践, 2014, 34 (9) :2402-2410) 
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524050452&amp;v=MDYxNjZNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZoVXJmTkoxc1dOaWZLWTdLNkh0VE9xNDlBWk84S0Ro&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Hofbauer J, Sigmund K.Evolutionary game dynamics[J].Bulletin of the American Mathematical Society, 2011, 40 (4) :479-519
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Epidemic protection over heterogeneous networks using evolutionary Poisson games">

                                <b>[15]</b>Hayel Y, Zhu Quanyuan.Protection over heterogeneous networks using evolutionary poisson games[J].IEEETransactions on Information Forensics and Security, 2017, 12 (8) :1786-1800
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201801016&amp;v=MTc2NDBNVFhUYkxHNEg5bk1ybzlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEZ1ZMcks=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Huang Jianming, Zhang Hengwei.Improving replicator dynamic evolutionary game model for selecting optimal defense strategies[J].Journal on Communications, 2018 (1) :170-182 (in Chinese) (黄健明, 张恒巍.基于改进复制动态演化博弈模型的最优防御策略选取[J].通信学报, 2018 (1) :170-182) 
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Markov evolutionary games for network defense strategy selection">

                                <b>[17]</b>Huang Jianming, Zhang Hengwei, Wang Jindong.Markov evolutionary games for network defense strategy selection[J].IEEE Access, 2017, 122 (99) :1-10
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001215348&amp;v=MDkyMTlCYXJPNEh0SE5yWTVBWis4SFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkN6a1c3clBKVnc9Tmo3&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Lye K W, Wing J M.Game strategies in network security[J].International Journal of Information Security, 2005, 4 (1?2) :71-86
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Host-centric model checking for network vulnerability analysis">

                                <b>[19]</b>Hewett R, Kijsanayothin P.Host-centric model checking for network vulnerability analysis[C]Proc of Computer Security Applications Conf.Piscataway, NJ:IEEE, 2009:225-234
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bootstrapping Q-learning for robotics from neuro-evolution results">

                                <b>[20]</b>Zimmer M, Doncieux S.Bootstrapping Q-learning for robotics from neuro-evolution results[J].IEEE Transactions on Cognitive&amp;Developmental Systems, 2017, 125 (99) :1-16
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning:An introduction,bradford book">

                                <b>[21]</b>Richard S, Andrew B.Reinforcement learning:An introduction, bradford book[J].Machine Learning, 2005, 16 (1) :285-286
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active defense strategy selection based on static Bayesian game">

                                <b>[22]</b>Zhang Hengwei, Wang Jindong, Yu Dongsheng, et al.Active defense strategy selection based on static Bayesian game[C]Proc of International Conf on Cyberspace Technology.New York:ACM, 2016
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201905006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201905006&amp;v=MDI0NzBqTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGaURnVkxyS0x5dlNkTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4aitnQ2dpS09RZXlETnJrZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

