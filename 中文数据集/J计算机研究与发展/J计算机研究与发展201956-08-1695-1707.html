

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127884924493750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908013%26RESULT%3d1%26SIGN%3dx0h05q6oKzfx8SPD04EHmIRYXmE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908013&amp;v=MDkwMTBqTXA0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnV3IvSUx5dlNkTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="&lt;b&gt;1 预备知识&lt;/b&gt; "><b>1 预备知识</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="&lt;b&gt;2 针对异常点的自适应回归特征选择方法&lt;/b&gt; "><b>2 针对异常点的自适应回归特征选择方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;2.1 AWLASSO模型&lt;/b&gt;"><b>2.1 AWLASSO模型</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;2.2 样本权重确定&lt;/b&gt;"><b>2.2 样本权重确定</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;2.3 模型求解&lt;/b&gt;"><b>2.3 模型求解</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;2.4 算法描述&lt;/b&gt;"><b>2.4 算法描述</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="&lt;b&gt;3 实验结果及分析&lt;/b&gt; "><b>3 实验结果及分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#146" data-title="&lt;b&gt;3.1 数据集及评价指标&lt;/b&gt;"><b>3.1 数据集及评价指标</b></a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;3.2 构造数据集上的实验结果&lt;/b&gt;"><b>3.2 构造数据集上的实验结果</b></a></li>
                                                <li><a href="#174" data-title="&lt;b&gt;3.3 标准数据集上的实验结果&lt;/b&gt;"><b>3.3 标准数据集上的实验结果</b></a></li>
                                                <li><a href="#189" data-title="&lt;b&gt;3.4 高维数据集上的实验结果&lt;/b&gt;"><b>3.4 高维数据集上的实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#197" data-title="&lt;b&gt;4 结  语&lt;/b&gt; "><b>4 结  语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#151" data-title="&lt;b&gt;表1 构造数据集&lt;/b&gt;"><b>表1 构造数据集</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表2 标准数据集&lt;/b&gt;"><b>表2 标准数据集</b></a></li>
                                                <li><a href="#153" data-title="图1 在D1数据集上的特征选择结果">图1 在D1数据集上的特征选择结果</a></li>
                                                <li><a href="#171" data-title="图2 3种方法在D2数据集上的&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;和&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;比较结果">图2 3种方法在D2数据集上的<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>比较结果</a></li>
                                                <li><a href="#176" data-title="&lt;b&gt;表3 3种方法在标准数据集上的实验结果&lt;/b&gt;"><b>表3 3种方法在标准数据集上的实验结果</b></a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表4 较优参数&lt;i&gt;λ&lt;/i&gt;下的标准数据集实验结果&lt;/b&gt;"><b>表4 较优参数<i>λ</i>下的标准数据集实验结果</b></a></li>
                                                <li><a href="#186" data-title="&lt;b&gt;表5 含异常点的标准数据集特征选择结果&lt;/b&gt;"><b>表5 含异常点的标准数据集特征选择结果</b></a></li>
                                                <li><a href="#187" data-title="图3 含异常点的标准数据集上&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;3&lt;/sub&gt;的比较结果">图3 含异常点的标准数据集上<i>MSE</i><sub>3</sub>的比较结果</a></li>
                                                <li><a href="#191" data-title="&lt;b&gt;表6 高维数据集&lt;/b&gt;"><b>表6 高维数据集</b></a></li>
                                                <li><a href="#194" data-title="图4 在高维数据集上的特征选择结果">图4 在高维数据集上的特征选择结果</a></li>
                                                <li><a href="#195" data-title="图5 3种方法在D3数据集上的&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;和&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;比较结果">图5 3种方法在D3数据集上的<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>比较结果</a></li>
                                                <li><a href="#196" data-title="图6 3种方法在D4数据集上的&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;和&lt;i&gt;MSE&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;比较结果">图6 3种方法在D4数据集上的<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>比较结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="227">


                                    <a id="bibliography_1" title="Guyon I, Elisseeff A.An introduction to variable and feature selection[J].Journal of Machine Learning Research, 2003, 3 (6) :1157- 1182" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An introduction to variable and feature selection">
                                        <b>[1]</b>
                                        Guyon I, Elisseeff A.An introduction to variable and feature selection[J].Journal of Machine Learning Research, 2003, 3 (6) :1157- 1182
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_2" title="Liu Huan, Motoda H, Setiono R, et al.Feature selection:An ever evolving frontier in data mining[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 19 (2) :153- 158" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection:An ever evolving frontier in data mining">
                                        <b>[2]</b>
                                        Liu Huan, Motoda H, Setiono R, et al.Feature selection:An ever evolving frontier in data mining[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 19 (2) :153- 158
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_3" title="Zhou Zhihua.Machine Learning[M].Beijing:Tsinghua University Press, 2016:249- 254 (in Chinese) (周志华.机器学习[M].北京:清华大学出版社, 2016:249- 254) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MzIzNTd4RTlmYnZuS3JpZlp1OXVGQ3ZnVTd6TktWNFVYRnF6R2JDNEhOWE9ySTFOWStzUERCTTh6eFVTbURkOVNIN24z&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Zhou Zhihua.Machine Learning[M].Beijing:Tsinghua University Press, 2016:249- 254 (in Chinese) (周志华.机器学习[M].北京:清华大学出版社, 2016:249- 254) 
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_4" title="Reunanen J.Overfitting in making comparisons between variable delection methods[J].Journal of Machine Learning Research, 2003, 3 (3) :1371- 1382" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overfitting in making comparisonsbetween variable selection methods">
                                        <b>[4]</b>
                                        Reunanen J.Overfitting in making comparisons between variable delection methods[J].Journal of Machine Learning Research, 2003, 3 (3) :1371- 1382
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_5" title="Nakariyakul S, Casasent D P.An improvement on floating search algorithms for feature subset selection[J].Pattern Recognition, 2009, 42 (9) :1932- 1940" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738831&amp;v=MjIwNTlUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZk9mYks3SHRETnFZOUZZK2dIQkg4NG9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Nakariyakul S, Casasent D P.An improvement on floating search algorithms for feature subset selection[J].Pattern Recognition, 2009, 42 (9) :1932- 1940
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_6" title="Peng Hanchuan, Long Fuhui, Ding C.Feature selection based on mutual information:Criteria of max-dependency, max-relevance, and min-redundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226- 1238" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy">
                                        <b>[6]</b>
                                        Peng Hanchuan, Long Fuhui, Ding C.Feature selection based on mutual information:Criteria of max-dependency, max-relevance, and min-redundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226- 1238
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_7" title="Hastie T.The Element of Statistical Learning:Data Mining, Inference, and Prediction[M].Berlin:Springer, 2009:55- 75" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The element of statistical learning">
                                        <b>[7]</b>
                                        Hastie T.The Element of Statistical Learning:Data Mining, Inference, and Prediction[M].Berlin:Springer, 2009:55- 75
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_8" title="Tibshirani R.Regression shrinkage and selection via the Lasso[J].Journal of the Royal Statistical Society, 1996, 58 (1) :267- 288" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MjE0ODVoST1OaWZZZXJLOEg5UE1xWTlHYmVrTEMzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Tibshirani R.Regression shrinkage and selection via the Lasso[J].Journal of the Royal Statistical Society, 1996, 58 (1) :267- 288
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_9" title="Wang Lei.The L1 penalized LAD estimator for high dimensional linear regression[J].Journal of Multivariate Analysis, 2013, 120 (9) :135- 151" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080200137531&amp;v=MTgzOTQvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2RhaEk9TmlmT2ZiSzdIdG5Nclk5RlplZ0lDWDg0b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Wang Lei.The L1 penalized LAD estimator for high dimensional linear regression[J].Journal of Multivariate Analysis, 2013, 120 (9) :135- 151
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_10" title="Zhang Hai, Wang Yao, Chang Xiangyu, et al.L1/2 regularization[J].Science China Information Sciences, 2010, 40 (3) :412- 422 (in Chinese) (张海, 王尧, 常象宇, 等.L1/2正则化[J].中国科学:信息科学, 2010, 40 (3) :412- 422) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201003005&amp;v=Mjg0NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1dyL0lOVGZBZHJHNEg5SE1ySTlGWVk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Zhang Hai, Wang Yao, Chang Xiangyu, et al.L1/2 regularization[J].Science China Information Sciences, 2010, 40 (3) :412- 422 (in Chinese) (张海, 王尧, 常象宇, 等.L1/2正则化[J].中国科学:信息科学, 2010, 40 (3) :412- 422) 
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_11" title="Hoerl A, Kennard R.Encyclopedia of Statistical Sciences[M].New York:Wiley-Interscience, 1998:129- 136" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Encyclopedia of Statistical Sciences">
                                        <b>[11]</b>
                                        Hoerl A, Kennard R.Encyclopedia of Statistical Sciences[M].New York:Wiley-Interscience, 1998:129- 136
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_12" title="Zou Hui, Hastie T.Regularization and variable selection via the elastic net[J].Journal of the Royal Statistical Society, 2005, 67 (2) :301- 320" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256895&amp;v=MjA0NTJyeG94Y01IN1I3cWVidWR0RkMzbFVMckJJRjQ9TmlmY2FyTzRIdEhOcllwRGJPSUtZM2s1ekJkaDRqOTlTWHFS&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Zou Hui, Hastie T.Regularization and variable selection via the elastic net[J].Journal of the Royal Statistical Society, 2005, 67 (2) :301- 320
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_13" title="Yuan Ming, Lin Yi.Model selection and estimation in regression with grouped variables[J].Journal of the Royal Statistical Society, 2006, 68 (1) :49- 67" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256921&amp;v=MTg5MjBjTUg3UjdxZWJ1ZHRGQzNsVUxyQklGND1OaWZjYXJPNEh0SE5yWXBEYmVrT1kzazV6QmRoNGo5OVNYcVJyeG94&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Yuan Ming, Lin Yi.Model selection and estimation in regression with grouped variables[J].Journal of the Royal Statistical Society, 2006, 68 (1) :49- 67
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_14" title="Fan Jianqi, Li Runze.Variable selection via nonconcave penalized likelihood and its oracle properties[J].Publications of the American Statistical Association, 2001, 96 (456) :1348- 1360" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007272&amp;v=MjYyOTdlWnVIeWptVUxySUkxc2RhaEk9TmpuQmFySzdIdGZPcDQ5RlpPc0lEbnM3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Fan Jianqi, Li Runze.Variable selection via nonconcave penalized likelihood and its oracle properties[J].Publications of the American Statistical Association, 2001, 96 (456) :1348- 1360
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_15" title="Zhang Cunhui.Nearly unbiased variable selection under minimax concave penalty[J].Annals of Statistics, 2010, 38 (2) :894- 942" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600630119&amp;v=MDQ0NDdUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZlllcks4SDlET3FZOUZZdWdQRFgwd29CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        Zhang Cunhui.Nearly unbiased variable selection under minimax concave penalty[J].Annals of Statistics, 2010, 38 (2) :894- 942
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_16" title="Li Youjuan, Zhu Ji.L1-norm quantile regression[J].Journal of Computational &amp;amp; Graphical Statistics, 2008, 17 (1) :163- 185" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004455&amp;v=MjY4OTQvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2RhaEk9TmpuQmFySzdIdGZPcDQ5RlpPc0xDSGs4b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Li Youjuan, Zhu Ji.L1-norm quantile regression[J].Journal of Computational &amp;amp; Graphical Statistics, 2008, 17 (1) :163- 185
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_17" title="Belloni A, Chernozhukov V.ℓ-penalized quantile regression in high-dimensional sparse models[J].Annals of Statistics, 2011, 39 (1) :82- 130" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST15012900205944&amp;v=MDI2MzZIeWptVUxySUkxc2RhaEk9TmlmWWVySzlIdERPcG85Rlp1c0tCWGc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Belloni A, Chernozhukov V.ℓ-penalized quantile regression in high-dimensional sparse models[J].Annals of Statistics, 2011, 39 (1) :82- 130
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_18" title="Fan Jianqing, Fan Yingying, Barut E.Adaptive robust variable selection[J].Annals of Statistics, 2014, 42 (1) :324- 351" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST1607190FFA453E51453D068B0E02D5CB&amp;v=MjQ2MTF0Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjFoeDcyMndLZz1OaWZZZXJLK0h0Yk5wbzh6RXBvTENYOU15aGNYN3p3SlNIbnEzaEpBZWJEZ1FNbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Fan Jianqing, Fan Yingying, Barut E.Adaptive robust variable selection[J].Annals of Statistics, 2014, 42 (1) :324- 351
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_19" title="Arslan O.Weighted LAD-LASSO method for robust parameter estimation and variable selection in regression[J].Computational Statistics and Data Analysis, 2012, 56 (6) :1952- 1965" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300389119&amp;v=MjMxOTFPZmJLN0h0RE9ySTlGWitNR0RYMHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        Arslan O.Weighted LAD-LASSO method for robust parameter estimation and variable selection in regression[J].Computational Statistics and Data Analysis, 2012, 56 (6) :1952- 1965
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_20" title="Alfons A, Gelper S.Sparse least trimmed squares regression for analyzing high-dimensional large data sets[J].Annals of Applied Statistics, 2013, 7 (1) :226- 248" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTE1A2F2ADE9F0B38D247BD7B6862F1155&amp;v=MTU2ODBlOFNWUkwrYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjJ3S2c9TmlmWWVzYTViOU82cmY0eEVlSjVEQTQ2eDJJUjdqZ1BQSGlRcWhveg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        Alfons A, Gelper S.Sparse least trimmed squares regression for analyzing high-dimensional large data sets[J].Annals of Applied Statistics, 2013, 7 (1) :226- 248
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_21" title="Omara T M.Weighted robust Lasso and adaptive elastic net method for regularization and variable selection in robust regression with optimal scaling transformations[J].American Journal of Mathematics and Statistics, 2017, 7 (2) :71- 77" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJSB&amp;filename=SJSB388667D645189FFA3B4C5EDEEDB9A6AC&amp;v=MDM0Mjh1V2NRbURzT1RRcVcyV2RCQzd2bFE4dnNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzIyd0tnPU5pZlliTEN3RnRmS3FQdERZTzRPQkhWUA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Omara T M.Weighted robust Lasso and adaptive elastic net method for regularization and variable selection in robust regression with optimal scaling transformations[J].American Journal of Mathematics and Statistics, 2017, 7 (2) :71- 77
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_22" title="Wang Yanxin, Zhu Li.Variable selection and parameter estimation via WLAD-SCAD with a diverging number of parameters[J].Journal of the Korean Statistical Society, 2017, 46 (3) :390- 403" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES36E4AA088A790F375E09DB4AEB43A28E&amp;v=MTA3NjROaWZPZmJDK2E5VzkzbzlOYkpvSUJYeFB6QkVXbno5MFBBM20zV2RIZmJIbFI3THFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzIyd0tnPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        Wang Yanxin, Zhu Li.Variable selection and parameter estimation via WLAD-SCAD with a diverging number of parameters[J].Journal of the Korean Statistical Society, 2017, 46 (3) :390- 403
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_23" title="Osbome M R, Presnell B, Turlach B A.A new approach to variable selection in least squares problems[J].IMA Journal of Numerical Analysis, 2000, 20 (3) :389- 403 (15) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new approach to variable selection in least squares problems">
                                        <b>[23]</b>
                                        Osbome M R, Presnell B, Turlach B A.A new approach to variable selection in least squares problems[J].IMA Journal of Numerical Analysis, 2000, 20 (3) :389- 403 (15) 
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_24" title="Efron B, Hastie T, Johnstone I, et al.Least angle regression[J].Annals of Statistics, 2004, 32 (2) :407- 451" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600629333&amp;v=MDMyMDdPcVk5Rll1a0dEMzg2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSTFzZGFoST1OaWZZZXJLOEg5RA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        Efron B, Hastie T, Johnstone I, et al.Least angle regression[J].Annals of Statistics, 2004, 32 (2) :407- 451
                                    </a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_25" title="Friedman J, Hastie T, H&#246;fling H, et al.Pathwise coordinate optimization[J].Annals of Applied Statistics, 2007, 1 (2) :302- 332" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTFC7183B4B890081CC97417653E0D4F36&amp;v=MjM3MjQ1TjFoeDcyMndLZz1OaWZZZXNYTEdkREVyUDFCRnVNR0RId3h6bVZnNHpoNVNYamtxUkZBZWNhUU03bVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        Friedman J, Hastie T, H&#246;fling H, et al.Pathwise coordinate optimization[J].Annals of Applied Statistics, 2007, 1 (2) :302- 332
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_26" title="Wang Yujun, Gao Qiankun, Zhang Xian, et al.A coordinate descent algorithm for solving capped-L1 regularization problems[J].Journal of Computer Research and Development, 2014, 51 (6) :1304- 1312 (in Chinese) (王玉军, 高乾坤, 章显, 等.一种求解截断L1正则化项问题的坐标下降算法[J].计算机研究与发展, 2014, 51 (6) :1304- 1312) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201406016&amp;v=MjUyNjF5dlNkTEc0SDlYTXFZOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnV3IvSUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        Wang Yujun, Gao Qiankun, Zhang Xian, et al.A coordinate descent algorithm for solving capped-L1 regularization problems[J].Journal of Computer Research and Development, 2014, 51 (6) :1304- 1312 (in Chinese) (王玉军, 高乾坤, 章显, 等.一种求解截断L1正则化项问题的坐标下降算法[J].计算机研究与发展, 2014, 51 (6) :1304- 1312) 
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_27" title="Benezra M, Peleg S, Werman M.Real-time motion analysis with linear-programming[J].Computer Vision &amp;amp; Image Understanding, 2000, 78 (1) :32- 52" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084869&amp;v=MDU1ODBmYks3SHRETnFvOUVaT01MQkhvd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2RhaEk9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                        Benezra M, Peleg S, Werman M.Real-time motion analysis with linear-programming[J].Computer Vision &amp;amp; Image Understanding, 2000, 78 (1) :32- 52
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_28" title="Zhao Qian, Meng Deyu, Jiang Lu, et al.Self-paced learning for matrix factorization[C] //Proc of the 29th National Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2015:3196- 3202" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-paced learning for matrix factorization">
                                        <b>[28]</b>
                                        Zhao Qian, Meng Deyu, Jiang Lu, et al.Self-paced learning for matrix factorization[C] //Proc of the 29th National Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2015:3196- 3202
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_29" title="Kumar M P, Packer B, Koller D.Self-paced learning for latent variable models[C] //Proc of Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2010:1189- 1197" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Self-paced learning for latent variable models">
                                        <b>[29]</b>
                                        Kumar M P, Packer B, Koller D.Self-paced learning for latent variable models[C] //Proc of Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2010:1189- 1197
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_30" title="Chang Chih-Chung, Lin Chih-Jen.LIBSVM:A library for support vector machines[EB/OL].2011 [2019-03-10].http://www.csie.ntu.edu.tw/～cjlin/libsvm" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIBSVM:A library for support vector machines">
                                        <b>[30]</b>
                                        Chang Chih-Chung, Lin Chih-Jen.LIBSVM:A library for support vector machines[EB/OL].2011 [2019-03-10].http://www.csie.ntu.edu.tw/～cjlin/libsvm
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1695-1707 DOI:10.7544/issn1000-1239.2019.20190313            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种针对异常点的自适应回归特征选择方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E4%BA%9A%E5%BA%86&amp;code=42588558&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭亚庆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%96%87%E5%89%91&amp;code=08402641&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王文剑</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8B%8F%E7%BE%8E%E7%BA%A2&amp;code=42588559&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏美红</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0176514&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西大学计算机与信息技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%99%BA%E8%83%BD%E4%B8%8E%E4%B8%AD%E6%96%87%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%B1%B1%E8%A5%BF%E5%A4%A7%E5%AD%A6)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算智能与中文信息处理教育部重点实验室(山西大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>数据集中含有不相关特征和冗余特征会使学习任务难度提高, 特征选择可以有效解决该问题, 从而提高学习效率和学习器性能.现有的特征选择方法大多针对分类问题, 面向回归问题的较少, 特别是当数据集含异常点时, 现有方法对异常点敏感.虽然某些方法可以通过给样本损失函数加权来提高其稳健性, 但是其权值一般都已预先设定好, 且在特征选择和学习器训练过程中固定不变, 因此方法的自适应性不强.针对上述问题, 提出了一种针对异常点的回归特征选择方法 (adaptive weight LASSO, AWLASSO) , 它首先根据回归系数更新样本误差, 并通过自适应正则项将误差大于当前阈值的样本的损失函数赋予较小权重, 误差小于阈值的样本的损失函数赋予较大权重, 再在更新权重后的加权损失函数下重新估计回归系数, 不断迭代上述过程.AWLASSO算法采用阈值来控制样本是否参与回归系数的估计, 在阈值作用下, 误差较小的样本才可参与估计, 所以迭代完成后会获得较优的回归系数估计.另外, AWLASSO算法的阈值不是固定不变的, 而是不断增大的 (为使初始回归系数估计值较准确, 其初始值较小) , 这样误判为异常点的样本可以重新进入训练集, 并保证训练集含有足够的样本.对于误差大于最大阈值的样本点, 由于其学习代价较大, 算法将其识别为异常点, 令其损失函数权重为0, 从而有效降低了异常点的影响.在构造数据和标准数据上的实验结果表明:对于含有异常点的数据集, 提出的方法比经典方法具有更好的稳健性和稀疏性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">回归;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E5%B8%B8%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异常点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%B3%E5%81%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稳健;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E6%AD%A3%E5%88%99%E9%A1%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应正则项;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王文剑, wjwang@sxu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-05-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61673249);</span>
                                <span>国家自然科学联合基金重点项目 (U1805263);</span>
                                <span>山西省回国留学人员科研基金项目 (2016-004);</span>
                    </p>
            </div>
                    <h1><b>An Adaptive Regression Feature Selection Method for Datasets with Outliers</b></h1>
                    <h2>
                    <span>Guo Yaqing</span>
                    <span>Wang Wenjian</span>
                    <span>Su Meihong</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information Technology, Shanxi University</span>
                    <span>Key Laboratory of Computational Intelligence and Chinese Information Processing (Shanxi University) , Ministry of Education</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Irrelevant and redundant features embedded in data will raise the difficulty for learning tasks, and feature selection can solve this problem effectively and improve learning efficiency and learner performance. Most of existing feature selection approaches are proposed for classification problems, while there are few studies on regression problems. Eespecially in presence of outliers, the present methods do not perform well. Although some methods can increase their robustness by weighting sample loss functions, the weights are set in advance and fixed throughout feature selection and learner training, which leads to bad adaptability. This paper proposes a regression feature selection method named adaptive weight LASSO (AWLASSO) for outliers. Firstly, it updates sample errors according to regression coefficients. Then the weights for loss functions of all samples are set according to the adaptive regularization term, i.e., the loss functions of samples whose errors are larger than current threshold are set smaller weights and loss functions of samples whose errors are less than threshold are set larger weights. The regression coefficient will be estimated iteratively under weighted loss function whose weights are updated. AWLASSO controls whether samples participate in regression coefficient estimation by the threshold. Only those samples with small errors participate in estimation, so a better regression coefficient estimation may be obtained in the end. In addition, the error threshold of AWLASSO algorithm is not fixed but increasing (To make initial regression coefficient estimation be accurate, initial threshold is often smaller) . So some samples which are misjudged as outliers will have chance to be added again in training set. The AWLASSO regards samples whose errors are larger than the maximum threshold as outliers for their learning cost is bigger, and the weights of their loss functions are set to 0. Hence, the influence of outliers will be reduced. Experiment results on artificial data and benchmark datasets demonstrate that the proposed AWLASSO has better robustness and sparsity specially for datasets with outliers in comparison with classical methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature selection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">regression;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=outliers&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">outliers;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=robustness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">robustness;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20regularization%20term&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive regularization term;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Guo Yaqing, born in 1990.PhD candidate. Her main research interest is machine learning. <image id="316" type="" href="images/JFYZ201908013_31600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wang Wenjian, born in 1968. PhD. Professor, PhD supervisor.Senior member of CCF.Her main research interests include machine learning, data mining, computing intelligence, image processing, etc.<image id="318" type="" href="images/JFYZ201908013_31800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Su Meihong, born in 1988.PhD candidate. Her main  research  interest  includes machine learning.<image id="320" type="" href="images/JFYZ201908013_32000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-05-30</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61673249);</span>
                                <span>the Key Union of National Natural Science Foundation of China (U1805263);</span>
                                <span>the Scientific Research Foundation for Returned Scholars of Shanxi Province (2016-004);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="66">一些实际学习任务的数据集中常含有大量不相关特征和冗余特征, 特征数目巨大, 如基因组分析、文本分类和图像检索等, 故会导致维数灾难和学习任务难度提高等问题, 以至于学习效果不好或学得模型可解释性差.此外, 观测某些特征代价昂贵, 若这些特征为无关特征, 则会造成大量不必要开销.解决上述问题的一种有效途径是特征选择.特征选择是将可以代表整体的含有关键性度量信息的部分特征挑选出来的过程, 它使得后续学习过程仅需在一部分特征上构建模型<citation id="287" type="reference"><link href="227" rel="bibliography" /><link href="229" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>.另外, 现有针对回归问题的特征选择方法, 当数据集含异常点时, 对其敏感或自适应能力不佳, 导致特征选择和学习效果较差.故如何自适应地进行稳健回归特征选择仍然是一个挑战性的课题.</p>
                </div>
                <div class="p1">
                    <p id="67">针对分类问题的特征选择方法已有很多, 常用的方法可分为2类:一类为过滤式 (如Relief (relevant features) 、mRMR (max-relevancy, min-redundancy) 和Relief-F等) ;另一类为包裹式 (如LVM (Las Vegas wrapper) 、SFFS (sequential floating forward selection) 、SFS (sequential feature selection) 和LRS (Plus-L-Minus-R search) 等) <citation id="288" type="reference"><link href="231" rel="bibliography" /><link href="233" rel="bibliography" /><link href="235" rel="bibliography" /><link href="237" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>.这些方法都是先对数据集进行特征选择, 再训练学习器, 其中过滤式方法特征选择过程与后续学习器无关, 导致最终学习器性能不好;包裹式方法虽然在选择特征时考虑了学习器性能, 但因为多次训练学习器造成了大量时间开销.</p>
                </div>
                <div class="p1">
                    <p id="68">上述面向分类的特征选择方法往往不能直接用于回归问题或应用后效果不好.目前针对回归问题的特征选择方法较少, 其代表性方法分为两大类:</p>
                </div>
                <div class="p1">
                    <p id="69">1) 先对数据集进行特征选择, 然后再训练学习器, 如向前选择法 (forward-stepwise selection) 、向后剔除法 (backward-stepwise selection) 和逐步筛选法 (forward-stagewise regression) 等, 这些方法不仅具有分类特征选择方法的某些缺点, 还不适用于特征数目巨大和有相关特征的数据集, 适用范围较小, 故并不常用<citation id="289" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="70">2) 将特征选择过程与学习器训练过程融为一体同时完成, 提高了最终学习器的性能, 降低了开销, 其典型方法有LASSO<citation id="290" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、LAD-LASSO (least absolute deviation) <citation id="291" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、L<sub>1/2</sub>正则化<citation id="292" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、岭回归 (ridge regression) <citation id="293" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、Elastic Net<citation id="294" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、Group Lasso<citation id="295" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、SCAD (smoothly clipped absolute deviation) <citation id="296" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和MCP (minimax concave penalty) <sup></sup><citation id="297" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等.其中岭回归因使用L<sub>2</sub>正则项而不易于获得稀疏解;L<sub>1/2</sub>正则化的实现算法效率较低;Elastic Net适用于特征之间相关性较高的数据集;Group Lasso适用于协变量之间存在组结构的回归数据集;SCAD和MCP虽然降低了LASSO的泛化误差, 但正则项复杂, 较难求解, 故LASSO和LAD-LASSO这2种方法更为常用.LASSO可以较为准确地完成特征选择, 并且计算快捷, 故被广泛使用.</p>
                </div>
                <div class="p1">
                    <p id="71">上述回归特征选择方法对异常点 (数据集中与数据的一般行为或模型不一致的数据对象) 极其敏感, 导致对于含有异常点的数据集, 其稳健性和稀疏性都有所下降.目前提出的稳健回归特征选择方法不多且大多针对含有噪声的数据集, 如分位数回归及其改进方法<citation id="305" type="reference"><link href="257" rel="bibliography" /><link href="259" rel="bibliography" /><link href="261" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>和LAD-LASSO等, 其中分位数回归及其改进方法模型复杂.针对异常点的稳健回归估计方法有WLAD (weight least absolute deviation) <citation id="298" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和LTS (least trimmed squares estimator) <citation id="299" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>等, 在其基础上WLAD-LASSO<sup></sup><citation id="300" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, LTS-LASSO<citation id="301" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, reweighted LTS-LASSO<citation id="302" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, WLAD-CATREG (categorical regres-sion model) adoptive elastic net<citation id="303" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和WLAD-SCAD<citation id="304" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>等被相继提出, 这些方法增加了易于获得稀疏解的正则项, 可以同时完成特征选择和学习器训练.其中LTS-LASSO通过将训练误差较小的数据集子集作为训练集来降低异常点影响, 但其时间开销较大;其余针对异常点的回归特征选择方法通过给损失函数加权来提高其稳健性, 其中reweighted LTS-LASSO将LTS-LASSO求得的回归系数作为参数初值, WLAD-LASSO, WLAD-CATREG和WLAD-SCAD根据数据集稳健位置估计量、数据集散点估计量和各样本的稳健距离得样本权重, 上述通过加权来提高稳健性的回归特征选择方法都是先计算好样本损失函数权重, 再进行特征选择和学习器训练, 样本权重在整个算法执行过程中固定不变, 故它们无法在特征选择和学习器训练过程中根据学习效果多次自主修改权重来进一步提高算法性能, 算法自适应能力不佳.此外, 针对现有回归特征选择方法当数据集含异常点时性能较差这一固有问题, 近年来并没有很好的研究成果.</p>
                </div>
                <div class="p1">
                    <p id="72">鉴于此, 本文提出一种能不断根据数据集和学习效果自主更新样本权重的用于线性回归的稳健特征选择方法AWLASSO (adaptive weight LASSO) , 其使用在[0, 1]中连续变化的自适应权重以更好地提高自适应性.该方法将特征选择与学习器训练过程融为一体同时完成, 以提高学习器性能和降低模型复杂度.AWLASSO算法通过阈值确定样本的损失函数权重;一方面可以使迭代过程总朝着较好的回归系数估计值方向进行;另一方面能保证训练集含有足够的样本, 同时可以排除异常点的影响.本文在构造数据和标准数据上验证了提出方法的有效性.</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag"><b>1 预备知识</b></h3>
                <div class="p1">
                    <p id="74">为便于理解本文提出方法及与LASSO和LAD-LASSO进行比较, 本节简要介绍LASSO和LAD-LASSO.</p>
                </div>
                <div class="p1">
                    <p id="75">给定数据集<i>D</i>={ (<b><i>x</i></b><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) }<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>, 其中<b><i>x</i></b><sub><i>i</i></sub>∈R<sup><i>p</i></sup>, <i>y</i><sub><i>i</i></sub>∈R.考虑线性回归模型<b><i>y</i></b>=<b><i>x</i></b><sup>T</sup><i>β</i>, 其中回归变量<i>y</i>∈R, 预测变量<b><i>x</i></b>∈R<sup><i>p</i></sup>, 回归系数<i>β</i>= (<i>β</i><sub>1</sub>, <i>β</i><sub>2</sub>, …, <i>β</i><sub><i>p</i></sub>) <sup>T</sup>∈R<sup><i>p</i></sup>.估计线性回归模型系数广泛使用最小二乘法, 它在求解优化目标<mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">β</mi></munder><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>时需要对<b><i>X</i></b><sup>T</sup><b><i>X</i></b>求逆, 其中<b><i>X</i></b>为特征矩阵, 然而事实上大多数<b><i>X</i></b><sup>T</sup><b><i>X</i></b>不一定可逆.此外, 当样本特征很多, 样本数较少时, 该优化目标易陷入过拟合.LASSO方法可以解决上述问题, 它以平方误差为损失函数, 增加了一个L<sub>1</sub>范数的正则化项, 其优化目标为</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">β</mi></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">β</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="80">其中, 正则化参数<i>λ</i>&gt;0.求解LASSO的方法有Homotopy<citation id="306" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、LARS (Least Angle RegresSion) <citation id="307" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、坐标下降法<citation id="308" type="reference"><link href="275" rel="bibliography" /><link href="277" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>等.</p>
                </div>
                <div class="p1">
                    <p id="81">与LASSO方法相比, LAD-LASSO方法以绝对值误差为损失函数, 其优化目标为</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">β</mi></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">β</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>, (2) </p>
                </div>
                <div class="p1">
                    <p id="84">将其转化成线性规划问题即可求解<citation id="309" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>.</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag"><b>2 针对异常点的自适应回归特征选择方法</b></h3>
                <h4 class="anchor-tag" id="86" name="86"><b>2.1 AWLASSO模型</b></h4>
                <div class="p1">
                    <p id="87">对于不含异常点的数据集, LASSO和LAD-LASSO方法都具有良好的性能, 然而对于含有异常点的数据集, 这2种方法没有区别对待异常点, 可能使得回归系数估计值与真实回归系数相差较大, 导致特征选择和学习器训练效果不好.此外, LASSO使用平方误差作为损失函数, 相比LAD-LASSO以绝对值误差为损失函数, 可能会使异常点的影响被放大, 故其稳健性和稀疏性被破坏更为严重.</p>
                </div>
                <div class="p1">
                    <p id="88">本文提出的AWLASSO首先根据更新后的回归系数更新样本误差, 并通过自适应正则项将误差大于当前阈值的样本的损失函数赋予较小权重, 误差小于阈值的样本的损失函数赋予较大权重, 再在更新了权重的加权损失函数下重新估计回归系数.通过不断迭代上述过程, 它每次在较优样本权重估计值下完成回归系数估计, 在较优回归系数估计值下完成样本权重估计.多次自主修正权重后其在合适的加权损失函数下完成特征选择和学习器训练.本文在第1次迭代时随机挑选部分样本作为训练集, 该训练集可能含有异常点, 故为防止异常点进入下一次迭代, 在下一轮迭代中得到较好的回归系数估计值, AWLASSO阈值初始值取较小值.在上述迭代过程中, 阈值不断增大, 被误判为异常点的样本有机会重新进入训练集, 以保证训练集含有足够的样本和保留多种样本信息.相比阈值由大到小进行迭代, 上述阈值选取方式, 大量异常点进入训练集的可能性较小, 不会出现即使减小阈值, 由于各样本误差累积, 仍无法对样本损失函数准确赋权重, 最终得到偏差较大的回归系数估计值的情况.AWLASSO当达到最大阈值时迭代停止, 此时它将误差大于最大阈值, 即学习代价较大, 会严重影响学习效果的样本视作异常点, 令其损失函数权重为0, 以降低异常点的影响.</p>
                </div>
                <div class="p1">
                    <p id="89">AWLASSO具体模型为</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">β</mi><mo>, </mo><mi mathvariant="bold-italic">v</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mi>n</mi></msup></mrow></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">v</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">该模型前2项与LASSO模型相似, 但第1项增加了加权向量<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold-italic">v</mi></mrow></math></mathml>, 其中<b><i>v</i></b>= (<i>v</i><sub>1</sub>, <i>v</i><sub>2</sub>, …, <i>v</i><sub><i>n</i></sub>) <sup>T</sup>是自适应权重向量, 其各分量为自适应权重.模型第3项<i>f</i> (<b><i>v</i></b>, <i>k</i>) 为自适应正则项, 其中<i>k</i>为自适应正则化参数.AWLASSO模型也是通过L<sub>1</sub>范数的正则项来较为准确地完成特征选择, 其特征选择过程与学习器训练过程融为一体同时完成.AWLASSO算法主要包括更新样本权重和更新回归系数这2个阶段.</p>
                </div>
                <div class="p1">
                    <p id="93">1) 更新样本权重.首先根据当前的回归系数估计值更新各样本误差, 然后更新自适应正则化参数, 最后利用更新后的各参数和自适应正则项更新样本权重, 此时, 误差大于当前阈值的样本的损失函数被赋予较小权重, 误差小于阈值的样本的损失函数被赋予较大权重, 并利用更新后的权重修正加权损失函数.</p>
                </div>
                <div class="p1">
                    <p id="94">2) 更新回归系数.求解更新后的目标函数, 即完成特征选择和学习器训练, 并反馈回归系数估计值.</p>
                </div>
                <div class="p1">
                    <p id="95">AWLASSO算法多次迭代上述2个阶段, 不断根据数据集和学习效果自主更新样本权重.在上述迭代过程中, 阈值不断增大, 当达到最大阈值时迭代停止, 此时AWLASSO将误差大于最大阈值的样本视作异常点, 令其损失函数权重为0, 以降低异常点的影响, 提高算法性能.其在处理异常点时, 不仅不需要较好地回归系数参数初值, 也不只依赖数据集, 算法具有较好的自适应能力.</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>2.2 样本权重确定</b></h4>
                <div class="p1">
                    <p id="97">权重是反映样本误差大小的一项指标, 一般取[0, 1]之间的数, 其自适应正则项为<i>f</i> (<b><i>v</i></b>, <i>k</i>) =<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mo>-</mo><mn>1</mn><mo>/</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 参数<i>k</i>在算法执行过程中不断减小, 以使阈值不断增大.在线性回归模型中, 当<i>β</i>固定时, 可得自适应向量各分量为</p>
                </div>
                <div class="area_img" id="99">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908013_09900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="101">其中, <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mfrac><mn>1</mn><mi>k</mi></mfrac></mrow></math></mathml>为阈值.当<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≤</mo><mfrac><mn>1</mn><mi>k</mi></mfrac></mrow></math></mathml>时, 即样本误差小于阈值时, 样本损失函数权重为1;当<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo></mrow><mfrac><mn>1</mn><mi>k</mi></mfrac></mrow></math></mathml>时, 即样本误差大于阈值时, 样本损失函数权重为0.该自适应正则项认为权重为0的样本不包含任何有用信息, 故采用它后, 在特征选择和学习器训练过程中只考虑权重为1的样本.但在算法迭代过程中, 由于尚未得到最优回归系数, 根据当前回归系数估计值对样本的判断不一定准确, 即某些含有有用信息的样本可能被误判为异常点, 在训练学习器时, 仍需考虑这些样本.故采用0-1权重不能充分利用样本信息, 特征选择和学习器训练效果不是很好.因此本文选用在区间[0, 1]中连续变化的权重, 认为样本误差大于当前阈值的某些样本仍然含有较少的信息, 赋予其损失函数大于0、小于1的权重.</p>
                </div>
                <div class="p1">
                    <p id="105">文献<citation id="310" type="reference">[<a class="sup">28</a>]</citation>对连续变化的自适应权重进行了研究, 并从理论和实验2方面验证了其有效性.本文选用该文提出的自适应正则项:<i>f</i> (<b><i>v</i></b>, <i>k</i>) =<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mrow><mi>γ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>γ</mi><mi>k</mi></mrow></mfrac></mrow></mstyle></mrow></math></mathml>, 其中参数<i>k</i>在AWLASSO算法执行过程中不断减小, 参数<i>γ</i>&gt;0控制样本权重取值力度.该正则项增加了大于0、小于1的权重, 能更精确地给各样本的损失函数赋权重.引入它的AWLASSO模型可被优化, 在其作用下, 误差大于当前阈值样本的损失函数被赋予较小权重, 误差小于阈值样本的损失函数被赋予较大权重.此外, 不断减小该正则项的自适应参数<i>k</i>, 阈值不断增大, 被误判为异常点的样本重新进入训练集以保证训练集含有足够的样本<citation id="311" type="reference"><link href="281" rel="bibliography" /><link href="283" rel="bibliography" /><sup>[<a class="sup">28</a>,<a class="sup">29</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="107">通过优化</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>{</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>γ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>γ</mi><mi>k</mi></mrow></mfrac></mrow><mo>}</mo></mrow></mrow></mstyle><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">可得自适应向量各分量为</p>
                </div>
                <div class="area_img" id="110">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908013_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>2.3 模型求解</b></h4>
                <div class="p1">
                    <p id="113">本文使用交替迭代方法求解AWLASSO模型, 每次迭代先固定<b><i>v</i></b>求<i>β</i>, 再固定<i>β</i>求<b><i>v</i></b>, 直到获得较为满意的结果为止.固定<b><i>v</i></b>求<i>β</i>时, AWLASSO的优化目标为</p>
                </div>
                <div class="p1">
                    <p id="114"><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">β</mi></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>, (6) </p>
                </div>
                <div class="p1">
                    <p id="116">与常规的LASSO相同, 本文也选用坐标下降法<citation id="312" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>求解该优化目标, 即:</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow></munder><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mrow><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>, </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">对<i>β</i><sub><i>j</i></sub>求导得:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>F</mi></mrow><mrow><mo>∂</mo><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow></munder><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>) </mo></mrow><mo stretchy="false"> (</mo><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>λ</mi><mspace width="0.25em" /><mtext>s</mtext><mtext>i</mtext><mtext>g</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">当<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>F</mi></mrow><mrow><mo>∂</mo><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>时, 有:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mi>β</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow></munder><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>) </mo></mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123"><i>λ</i> sign (<i>β</i><sub><i>j</i></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="124">令<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>z</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow></munder><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>) </mo></mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></mfrac><mo>, </mo><mi>g</mi><mo>=</mo><mfrac><mi>λ</mi><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></mfrac></mrow></math></mathml>, 则:</p>
                </div>
                <div class="area_img" id="126">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908013_12600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="128">其中, <i>β</i><sub><i>j</i></sub>∈[0, <i>z</i>) 或 (<i>z</i>, 0], 且当<i>z</i>≠0时<i>β</i><sub><i>j</i></sub>与<i>λ</i>有关, 当<i>λ</i>值较大时, <i>β</i><sub><i>j</i></sub>有可能成为0.</p>
                </div>
                <div class="p1">
                    <p id="129">在下次迭代过程中, 通过式 (5) 更新<b><i>v</i></b>.</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>2.4 算法描述</b></h4>
                <div class="p1">
                    <p id="131">求解AWLASSO的主要步骤如算法1所示.</p>
                </div>
                <div class="p1">
                    <p id="132"><b>算法1</b>. AWLASSO模型求解算法.</p>
                </div>
                <div class="p1">
                    <p id="133">输入:训练集<b><i>X</i></b>∈R<sup><i>n</i>×<i>p</i></sup>和<b><i>Y</i></b>∈R<sup><i>n</i></sup>、自适应参数初始值<i>k</i><sub>0</sub>、自适应参数终止值<i>k</i><sub>end</sub>、正则化参数<i>λ</i>, 且<i>k</i><sub>0</sub>&gt;<i>k</i><sub>end</sub>, <i>μ</i>&gt;1;</p>
                </div>
                <div class="p1">
                    <p id="134">输出:回归系数<i>β</i>.</p>
                </div>
                <div class="p1">
                    <p id="135">Step1. 初始化自适应向量<b><i>v</i></b>为一个固定值 (一般随机令<b><i>v</i></b>一半分量为0, 另一半分量为1) , 自适应参数<i>k</i>=<i>k</i><sub>0</sub>;</p>
                </div>
                <div class="p1">
                    <p id="136">Step2. 当自适应参数<i>k</i>&gt;<i>k</i><sub>end</sub>时, 循环执行以下步骤:</p>
                </div>
                <div class="p1">
                    <p id="137">Step2.1. 更新回归系数<i>β</i>;</p>
                </div>
                <div class="p1">
                    <p id="138">Step2.2. 更新样本误差{<i>L</i><sub><i>i</i></sub>}<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="140">Step2.3. 将各参数带入式 (5) , 更新<b><i>v</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>S</mtext><mtext>t</mtext><mtext>e</mtext><mtext>p</mtext><mn>2</mn><mo>.</mo><mn>4</mn><mo>.</mo><mspace width="0.25em" /><mi>k</mi><mo>=</mo><mfrac><mi>k</mi><mi>μ</mi></mfrac><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">AWLASSO算法的执行时间主要由Step2决定, 由于每更新1次{<i>β</i><sub><i>j</i></sub>}<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow></math></mathml>的值, 坐标下降法时间复杂度为<i>O</i> (<i>n</i>) , 假设每次执行坐标下降法时最多需要更新<i>a</i>次{<i>β</i><sub><i>j</i></sub>}<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow></math></mathml>, 并且执行了坐标下降法<i>b</i>次, 则AWLASSO算法的时间复杂度仍为<i>O</i> (<i>n</i>) .</p>
                </div>
                <h3 id="145" name="145" class="anchor-tag"><b>3 实验结果及分析</b></h3>
                <h4 class="anchor-tag" id="146" name="146"><b>3.1 数据集及评价指标</b></h4>
                <div class="p1">
                    <p id="147">为验证本文提出方法AWLASSO的有效性, 分别在2个构造数据集和4个标准数据集上进行实验, 并与LASSO和LAD-LASSO进行对比.</p>
                </div>
                <div class="p1">
                    <p id="148">对于给定的数据集, 一般无法知道其中是否包含异常点, 为验证算法, 采用文献<citation id="313" type="reference">[<a class="sup">19</a>]</citation>中的方法来构造含异常点的数据集, 该方法通过污染率<i>θ</i>随机选定一定数量的数据改变其分布, 改变后的数据被称为异常点, 被污染数据数量为<i>m</i>=<i>n</i>×<i>θ</i>.对于构造数据集, 由多维正态分布<i>N</i><sub>p</sub> (<b><i>0</i></b>, <i>Σ</i>) 生成{<b><i>x</i></b><sub><i>i</i></sub>}<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mi>m</mi></mrow></msubsup></mrow></math></mathml>, <b><i>y</i></b><sub><i>i</i></sub>=<b><i>x</i></b><sup>T</sup><sub><i>i</i></sub><i>β</i><sub>true</sub>+<i>ε</i><sub><i>i</i></sub>, 其中<i>β</i><sub>true</sub>= (1, 2.5, 1.5, 2, 0, 0, 0, 0) <sup>T</sup>, <i>β</i><sub>true</sub>是真实回归系数, <mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Σ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo stretchy="false">|</mo><mi>i</mi><mo>-</mo><mi>j</mi><mo stretchy="false">|</mo></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">ε</mi><mo>=</mo><mo stretchy="false"> (</mo><mi>ε</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>ε</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>ε</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mi>m</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>使预测变量上可能含有异常值.然后生成m个被污染数据:它们服从多维正态分布N<sub><i>p</i></sub> (<i>μ</i><sub>0</sub>, <b><i>I</i></b>) , <b><i>y</i></b><sub><i>i</i></sub>=<b><i>x</i></b><sup>T</sup><sub><i>i</i></sub><i>β</i><sub>false</sub>, 其中<i>μ</i><sub>0</sub>≠0, <i>β</i><sub>false</sub>≠<i>β</i><sub>true</sub>, <i>β</i><sub>false</sub>是干扰回归系数.本文按照<i>ε</i>服从不同分布得到不同的构造数据集, 实验中构造的2个数据集如表1所示, 标准数据集<citation id="314" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>如表2所示.</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表1 构造数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Artificial Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td><br />Artificial Datasets</td><td><i>ε</i> Distribution</td><td>#Samples</td></tr><tr><td><br />D1</td><td><i>t</i> (5) </td><td>100</td></tr><tr><td><br />D2</td><td><i>N</i> (0, 1) </td><td>200</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表2 标准数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Benchmark Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td><br />Benchmark Datasets</td><td>#Samples</td><td>#Features</td></tr><tr><td><br />Eunite2001</td><td>367</td><td>15</td></tr><tr><td><br />Triazines</td><td>186</td><td>59</td></tr><tr><td><br />Housing</td><td>506</td><td>12</td></tr><tr><td><br />Mpg</td><td>392</td><td>6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908013_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 在D1数据集上的特征选择结果" src="Detail/GetImg?filename=images/JFYZ201908013_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 在D1数据集上的特征选择结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908013_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Feature selection results on D1</p>

                </div>
                <div class="p1">
                    <p id="154">实验中AWLASSO方法的参数<i>γ</i>=0.4, <i>μ</i>=1.2, <i>k</i>初始值为2.5, 终止值为0.000 1.在构造数据集上, 实验重复进行100次, 取平均值作为最终结果.</p>
                </div>
                <div class="p1">
                    <p id="155">本文用平均平方误差 (<i>MSE</i>) 作为评价算法稳健性的性能指标, 用<i>MSE</i><sub>1</sub>表示回归系数估计值<i>β</i><sup>*</sup>与<i>β</i><sub>true</sub>的差别, 即:</p>
                </div>
                <div class="p1">
                    <p id="156"><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mn>1</mn><mi>w</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">β</mi><msubsup><mrow></mrow><mi>t</mi><mo>*</mo></msubsup><mo>-</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>u</mtext><mtext>e</mtext></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, (8) </p>
                </div>
                <div class="p1">
                    <p id="158">用<i>MSE</i><sub>2</sub>表示回归系数估计值<i>β</i><sup>*</sup>与<i>β</i><sub>false</sub>的差别, 即:</p>
                </div>
                <div class="p1">
                    <p id="159"><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mfrac><mn>1</mn><mi>w</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">β</mi><msubsup><mrow></mrow><mi>t</mi><mo>*</mo></msubsup><mo>-</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>a</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, (9) </p>
                </div>
                <div class="p1">
                    <p id="161">其中, <i>w</i>表示实验重复次数, <i>β</i><sup>*</sup><sub><i>t</i></sub>表示第<i>t</i>次实验得到的回归系数估计值.用<i>MSE</i><sub>3</sub>表示使用回归系数估值求得的<b><i>Y</i></b><sub><i>t</i></sub>与真实<b><i>Y</i></b>之间的差别, 即:</p>
                </div>
                <div class="p1">
                    <p id="162"><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mfrac><mn>1</mn><mi>w</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi></mrow><mo>|</mo></mrow></mrow></mstyle><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="164">其中, <i>w</i>表示实验重复次数, <b><i>Y</i></b><sub><i>t</i></sub>表示第<i>t</i>次实验得到的回归向量预测值.如果某种方法的<i>MSE</i><sub>1</sub>较小且<i>MSE</i><sub>2</sub>较大或<i>MSE</i><sub>3</sub>较小, 说明该方法估计出的回归系数与真实回归系数相差较小, 与干扰回归系数相差较大, 其稳健性较好, 反之稳健性较差.同时本文用无关特征选择正确个数的平均表现来评估这3种方法的稀疏性, 其值越接近真实回归系数含0总数, 对应方法稀疏性越好, 反之则越差.</p>
                </div>
                <div class="p1">
                    <p id="165">所有实验用MATLABR2014a实现.实验环境为4 GB内存, Intel<sup>®</sup> Core<sup>TM</sup>2 Quad处理器, 2.66 GHz, Windows10操作系统.</p>
                </div>
                <h4 class="anchor-tag" id="166" name="166"><b>3.2 构造数据集上的实验结果</b></h4>
                <h4 class="anchor-tag" id="167" name="167">3.2.1 特征选择结果</h4>
                <div class="p1">
                    <p id="168">首先比较LASSO, LAD-LASSO和AWLASSO这3种方法特征选择的结果.由于这3种方法在构造数据集D1和D2上特征选择结果基本一致, 故本文只给出构造数据集D1上的实验结果.图1为构造数据集D1上的特征选择结果, 图1 (a) 是选出无关特征的个数的平均结果, 图1 (b) 给出了无关特征选择正确个数的平均结果与选出无关特征的个数的平均结果的比例<i>r</i>.在D1数据集上, 真实回归系数有4个分量为0, 即有4个无关特征, 故在图1 (a) 中选出无关特征的个数的平均结果越接近4, 对应方法特征选择效果越好.由于LASSO在各污染率下无关特征选择正确个数的平均结果和选出无关特征的个数的平均结果皆为0, 且LAD-LASSO和AWLASSO在各污染率下当<i>λ</i>&gt;25时, 得到的回归系数估计值各分量皆为0或极小的数, 方法失效, 故未在图1中给出上述实验特征选择结果.从图1 (a) 中可以看出, 在不同污染率下, LASSO和LAD-LASSO在不同<i>λ</i>值下选出无关特征的个数的平均结果都接近于0, 严重偏离4;AWLASSO当<i>λ</i>取值较小时接近于4.由于LAD-LASSO并未完成特征选择, 图1 (b) 只给出AWLASSO方法的<i>r</i>, <i>r</i>值应介于0到1之间.由图1 (b) 可知AWLASSO方法当选出无关特征的个数的平均结果接近于4时其<i>r</i>都接近于1, 即它正确选出了无关特征, 特征选择结果较好, 但它对参数<i>λ</i>较为敏感, 当<i>λ</i>值增大到一定程度后, 其得到的回归系数估计值各分量都为0, <i>r</i>=1/2, 无法完成特征选择.</p>
                </div>
                <h4 class="anchor-tag" id="169" name="169">3.2.2 稳健性比较</h4>
                <div class="p1">
                    <p id="170">本文还比较了3种方法的稳健性.由于这3种方法在构造数据集D1和D2上实验结果基本一致, 故本文只给出构造数据集D2上的实验结果.图2是构造数据集D2在不同污染率下<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>的比较结果, 其中不含空心圆的曲线表示各方法的<i>MSE</i><sub>1</sub>, 含空心圆的曲线表示各方法的<i>MSE</i><sub>2</sub>.从图2中可以看出, 在不同污染率下, 无论是<i>MSE</i><sub>1</sub>还是<i>MSE</i><sub>2</sub>, LASSO方法都较大, 说明其对含有异常点的数据处理能力较差.对于<i>MSE</i><sub>1</sub>, AWLASSO方法在一定的<i>λ</i>值之下, 都小于LAD-LASSO, 当<i>λ</i>值继续增大时, LAD-LASSO的<i>MSE</i><sub>1</sub>才减小至与AWLASSO的相同.对于<i>MSE</i><sub>2</sub>, 在绝大多数情况下AWLASSO要高于LAD-LASSO, 当<i>λ</i>大于一定值之后, 2种方法的<i>MSE</i><sub>2</sub>才相同.实验结果表明, AWLASSO方法估计出的回归系数都与回归系数真实值相差较小 (<i>MSE</i><sub>1</sub>较小) , 与干扰回归系数相差较大 (<i>MSE</i><sub>2</sub>较大) , 它不会像LAD-LASSO方法那样受干扰回归系数的影响, 故AWLASSO方法的稳健性更好.</p>
                </div>
                <div class="area_img" id="171">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908013_171.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 3种方法在D2数据集上的MSE1和MSE2比较结果" src="Detail/GetImg?filename=images/JFYZ201908013_171.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 3种方法在D2数据集上的<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>比较结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908013_171.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Comparisons of <i>MSE</i><sub>1</sub> and <i>MSE</i><sub>2</sub> on D2</p>

                </div>
                <div class="p1">
                    <p id="172">为了更好地说明AWLASSO方法与LAD-LASSO方法的稳健性, 通过对比图2各分图可得它们在构造数据集D2上污染率取不同值时<i>MSE</i><sub>1</sub>的比较结果.从中可以看出, 当其他参数取值相同时, LAD-LASSO方法对应的<i>MSE</i><sub>1</sub>随着污染率的增大而显著增大, AWLASSO方法对应的<i>MSE</i><sub>1</sub>并没有随着污染率的增大而显著增大, 而是一直处于某一值附近, 其性能不会随着数据集中被污染数据的增加而显著变差, 即AWLASSO方法相比LAD-LASSO方法更稳健.</p>
                </div>
                <div class="p1">
                    <p id="173">在构造数据集上的所有实验结果表明:无论数据分布如何, 异常点分布如何, AWLASSO都比LASSO和LAD-LASSO更稳健更稀疏.</p>
                </div>
                <h4 class="anchor-tag" id="174" name="174"><b>3.3 标准数据集上的实验结果</b></h4>
                <div class="p1">
                    <p id="175">对于标准数据集, 选2/3的数据作为训练集, 剩余部分作为测试集.令<i>SE</i><sub>3</sub>表示在测试集上利用训练集的回归系数估计值求得的<b><i>Y</i></b>与真实<b><i>Y</i></b>之间的平方误差, 以此来判断方法的稳健性.实验结果如表3所示, 该表中“0”表示某方法在对应参数组合下选出0个无关特征.</p>
                </div>
                <div class="area_img" id="176">
                    <p class="img_tit"><b>表3 3种方法在标准数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Experiment Results of Three Methods on Benchmark Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="176" border="1"><tr><td rowspan="2"><br /><i>λ</i></td><td rowspan="2">Datasets</td><td colspan="2"><br />LASSO</td><td colspan="2">LAD-LASSO</td><td colspan="2">AWLASSO</td></tr><tr><td><br />Irrelevant Features</td><td><i>SE</i><sub>3</sub> ↓</td><td>Irrelevant Features</td><td><i>SE</i><sub>3</sub> ↓</td><td>Irrelevant Features</td><td><i>SE</i><sub>3</sub>↓</td></tr><tr><td rowspan="4"><br />10</td><td>Eunite2001</td><td>2 (7, 9) </td><td>1.3435e+152</td><td>2 (7, 9) </td><td>0.843 8</td><td>2 (7, 9) </td><td>2.8829e+152</td></tr><tr><td><br />Housing</td><td>0</td><td>3.1012e+126</td><td>0</td><td>20.436 5</td><td>12</td><td>149.988 7</td></tr><tr><td><br />Mpg</td><td>0</td><td>2.6395e+67</td><td>0</td><td>12.174 1</td><td>0</td><td>5.0662e+61</td></tr><tr><td><br />Triazines</td><td>2 (42, 43) </td><td>Inf</td><td>5 (37, 38, 42, 43, 53) </td><td>1.342 9</td><td>2 (42, 43) </td><td>Inf</td></tr><tr><td rowspan="4"><br />20</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>4.2613e+151</td><td>2 (7, 9) </td><td>0.941 1</td><td>15</td><td>60.429 2</td></tr><tr><td><br />Housing</td><td>0</td><td>2.0405e+126</td><td>0</td><td>21.935 6</td><td>0</td><td>3.0059e+108</td></tr><tr><td><br />Mpg</td><td>0</td><td>6.1209e+65</td><td>0</td><td>16.737 2</td><td>0</td><td>2.8652e+58</td></tr><tr><td><br />Triazines</td><td>2 (42, 43) </td><td>Inf</td><td>3 (20, 42, 43) </td><td>1.342 9</td><td>59</td><td>17.520 0</td></tr><tr><td rowspan="4"><br />30</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>8.3136e+150</td><td>2 (7, 9) </td><td>0.950 1</td><td>2 (7, 9) </td><td>8.7994e+151</td></tr><tr><td><br />Housing</td><td>0</td><td>1.2173e+126</td><td>0</td><td>23.724 3</td><td>0</td><td>9.2335e+107</td></tr><tr><td><br />Mpg</td><td>1 (4) </td><td>33.904 1</td><td>0</td><td>19.235 3</td><td>4 (2～5) </td><td>23.253 8</td></tr><tr><td><br />Triazines</td><td>2 (42, 43) </td><td>Inf</td><td>5 (24, 26, 37, 42, 43) </td><td>1.342 9</td><td>58 (1～7, 9～59) </td><td>11.533 9</td></tr><tr><td rowspan="4"><br />40</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>1.5251e+149</td><td>2 (7, 9) </td><td>0.939 0</td><td>15</td><td>60.429 2</td></tr><tr><td><br />Housing</td><td>0</td><td>6.1109e+125</td><td>0</td><td>23.634 4</td><td>0</td><td>7.0974e+106</td></tr><tr><td><br />Mpg</td><td>2 (4, 5) </td><td>20.4815</td><td>0</td><td>24.557 2</td><td>4 (2～5) </td><td>30.458 7</td></tr><tr><td><br />Triazines</td><td>2 (42, 43) </td><td>Inf</td><td>2 (42, 43) </td><td>1.342 9</td><td>58 (1～7, 9～59) </td><td>16.234 8</td></tr><tr><td rowspan="4"><br />50</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>7.5285e+148</td><td>2 (7, 9) </td><td>0.942 8</td><td>2 (7, 9) </td><td>1.4692e+151</td></tr><tr><td><br />Housing</td><td>0</td><td>2.2762e+125</td><td>0</td><td>23.519 8</td><td>0</td><td>4.5222e+99</td></tr><tr><td><br />Mpg</td><td>4 (2～5) </td><td>33.284 8</td><td>0</td><td>21.717 3</td><td>5 (2～6) </td><td>40.973 1</td></tr><tr><td><br />Triazines</td><td>2 (42, 43) </td><td>4.8160e+307</td><td>10 (2, 3, 22～24, 37, <br />38, 42, 43, 55) </td><td>3.673 4</td><td>59</td><td>17.520 0</td></tr><tr><td rowspan="4"><br />60</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>2.6272e+145</td><td>2 (7, 9) </td><td>1.260 4</td><td>2 (7, 9) </td><td>2.2272e+150</td></tr><tr><td><br />Housing</td><td>0</td><td>5.8648e+124</td><td>0</td><td>23.259 1</td><td>8 (2, 4～6, 8～10, 12) </td><td>31.287 7</td></tr><tr><td><br />Mpg</td><td>4 (2～5) </td><td>39.958 8</td><td>0</td><td>23.469 8</td><td>5 (2～6) </td><td>52.752 0</td></tr><tr><td><br />Triazines</td><td>59</td><td>17.520 0</td><td>4 (11, 37, 42, 43) </td><td>3.673 4</td><td>59</td><td>17.520 0</td></tr><tr><td rowspan="4"><br />70</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>1.1880e+149</td><td>0</td><td>0.950 4</td><td>9 (1～9) </td><td>41.774 3</td></tr><tr><td><br />Housing</td><td>0</td><td>8.0709e+123</td><td>1 (7) </td><td>23.784 6</td><td>12</td><td>149.988 7</td></tr><tr><td><br />Mpg</td><td>4 (2～5) </td><td>47.682 9</td><td>0</td><td>23.469 8</td><td>5 (2～6) </td><td>62.121 5</td></tr><tr><td><br />Triazines</td><td>59</td><td>17.520 0</td><td>5 (3, 17, 20, 42, 43) </td><td>8.871 7</td><td>59</td><td>17.520 0</td></tr><tr><td rowspan="4"><br />80</td><td><br />Eunite2001</td><td>2 (7, 9) </td><td>3.7001e+145</td><td>2 (7, 9) </td><td>1.113 1</td><td>2 (7, 9) </td><td>3.7001e+145</td></tr><tr><td><br />Housing</td><td>0</td><td>4.4582e+122</td><td>0</td><td>24.937 1</td><td>9 (2, 4～10, 12) </td><td>32.597 6</td></tr><tr><td><br />Mpg</td><td>5 (2～6) </td><td>55.389 7</td><td>0</td><td>24.552 9</td><td>6</td><td>65.120 0</td></tr><tr><td><br />Triazines</td><td>59</td><td>17.520 0</td><td>10 (4, 6, 10, 23, 30, <br />37, 38, 42, 43, 48) </td><td>8.871 7</td><td>59</td><td>17.520 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: “↓” represents the most robust method is the one having the lowest <i>SE</i><sub>3</sub>.</p>
                </div>
                <h4 class="anchor-tag" id="177" name="177">1) 原始数据集上的实验结果</h4>
                <div class="p1">
                    <p id="178">由表3知, LASSO在上述标准数据集上的32个回归系数估计值中有10个不含无关特征, LAD-LASSO的有16个不含无关特征, AWLASSO的有6个不含无关特征.AWLASSO在Eunite2001数据集上, 当<i>λ</i>=70时, 选出了9个无关特征;在Housing数据集上, 当<i>λ</i>=80时, 选出了9个无关特征;在Mpg数据集上, 当<i>λ</i>=50时, 选出了5个无关特征;在Tiazines据集上, 当<i>λ</i>=30时, 选出了58个无关特征, 即其在各数据集上选出无关特征最多, 且没有将所有特征视作无关特征.在各数据集上, AWLASSO方法对参数<i>λ</i>较为敏感, 它只在某些<i>λ</i>值下特征选择效果好, 学习器训练效果中等;LAD-LASSO方法在各<i>λ</i>值下学习器训练效果都好, 但特征选择效果都不好;LASSO方法在数据集Eunite2001, Housing和Triazines上, 特征选择和学习器训练效果都不好, 但在数据集MPG上, 当参数<i>λ</i>取某些值时, 其特征选择和学习器训练效果较好.</p>
                </div>
                <div class="p1">
                    <p id="179">由于LASSO方法整体表现不稳定, 所以后边实验只比较了LAD-LASSO和AWLASSO方法的性能.表4给出了这2种方法在各自较优参数范围内的实验结果比较, “0”表示在较优参数范围内求得的各回归系数估计值无重叠无关特征.由表4知, 当参数<i>λ</i>在较优参数范围内时, LAD-LASSO方法在4个数据集上都没有重叠无关特征, 它在各较优参数<i>λ</i>下只有少数回归系数估计值含有少量0分量, 其选出的无关特征较少.AWLASSO在所有的数据集上都有大量重叠无关特征, 其在较优参数范围内得到的各回归系数都含大量的0分量, 它选出了大量无关特征且不会将所有特征视作无关特征.AWLASSO方法的最小<i>SE</i><sub>3</sub>和最大<i>SE</i><sub>3</sub>要稍大于LAD-LASSO方法的.故在标准数据集上AWLASSO没有LAD-LASSO稳健, 但比LAD-LASSO稀疏.</p>
                </div>
                <h4 class="anchor-tag" id="180" name="180">2) 含异常点数据集上的实验结果</h4>
                <div class="p1">
                    <p id="181">由于上述标准数据集是否含异常点未知, 所以为了验证本文提出方法的有效性, 本文在训练集上增加了一些异常点.首先从训练集中随机选取<i>m</i>个数据, 其特征矩阵为<b><i>X</i></b><sub>2</sub>, 再通过<mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi>σ</mi><mo>×</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo><mo>×</mo><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>¯</mo></mover></mrow></math></mathml>生成异常点, 其中<mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><mo>≠</mo><mn>0</mn><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>¯</mo></mover></mrow></math></mathml>为干扰回归系数, 对于剩下的<i>n</i>-<i>m</i>个数据, 在其回归变量上增加偏移量<i>ε</i><sub><i>i</i></sub>, <i>ε</i>服从<i>t</i> (3) 分布.由于在标准数据集上, AWLASSO方法对参数敏感而LAD-LASSO在各<i>λ</i>值下学习器训练效果都好且优于AWLASSO, 故在AWLASSO较优参数<i>λ</i>下比较两者<i>MSE</i><sub>3</sub>和多次重复实验的重叠无关特征.本文在各污染率<i>θ</i>下实验重复进行50次, 实验结果如表5和图3所示.在表5中“0”表示50次重复实验求得的各回归系数估计值无重叠无关特征.</p>
                </div>
                <div class="area_img" id="184">
                    <p class="img_tit"><b>表4 较优参数<i>λ</i>下的标准数据集实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Experiment Results with Fitted Parameter <i>λ</i> on Benchmark Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="184" border="1"><tr><td><br />Datasets</td><td><i>λ</i></td><td>Methods</td><td>Irrelevant Features</td><td>Min <i>SE</i><sub>3</sub>/Max <i>SE</i><sub>3</sub></td></tr><tr><td rowspan="2"><br />Eunite2001</td><td rowspan="2">[71, 74.1]</td><td><br />LAD-LASSO</td><td>0</td><td>0.5395/3.5913</td></tr><tr><td><br />AWLASSO</td><td>9 (1～9) </td><td>22.0186/36.3562</td></tr><tr><td rowspan="2"><br />Housing</td><td rowspan="2">[63, 66.35]</td><td><br />LAD-LASSO</td><td>0</td><td>23.0775/24.6851</td></tr><tr><td><br />AWLASSO</td><td>8 (2, 4～6, 8～10, 12) </td><td>23.8427/26.7850</td></tr><tr><td rowspan="2"><br />MPG</td><td rowspan="2">[21.53, 27]</td><td><br />LAD-LASSO</td><td>0</td><td>15.5883/18.6142</td></tr><tr><td><br />AWLASSO</td><td>4 (2～5) </td><td>20.1550/22.0695</td></tr><tr><td rowspan="2"><br />Triazines</td><td rowspan="2">[23.7, 25.57]</td><td><br />LAD-LASSO</td><td>0</td><td>1.2319/3.2475</td></tr><tr><td><br />AWLASSO</td><td>57 (1～5, 7, 9～59) </td><td>8.6841/9.7916</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="185">由表5可知, 在上述标准数据集上, LAD-LASSO在各<i>θ</i>下的50个回归系数估计值都没有重叠无关特征.它在各数据集上所有参数组合下的200个回归系数估计值, 在Eunite2001上有174个不含无关特征, 有26个有无关特征但无重叠无关特征;在Housing数据集上有193个不含无关特征, 有7个有无关特征但无重叠无关特征;在Triazines数据集上有75个不含无关特征, 有125个有无关特征但无重叠无关特征;在MPG数据集上有197个不含无关特征, 有3个有无关特征但无重叠无关特征.而AWLASSO只在MPG数据集上当污染率<i>θ</i>=0.5时没有重叠无关特征, 剩余情况下, 其皆有大量重叠无关特征, 而且它重叠无关特征数小于数据集特征总数, 即AWLASSO没有将所有特征视作无关特征.</p>
                </div>
                <div class="area_img" id="186">
                    <p class="img_tit"><b>表5 含异常点的标准数据集特征选择结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Feature Selection Results on Benchmark Datasets with Outliers</b></p>
                    <p class="img_note"></p>
                    <table id="186" border="1"><tr><td><br />Datasets (<i>λ</i>) </td><td>Methods</td><td><i>θ</i>=0.2</td><td><i>θ</i>=0.3</td><td><i>θ</i>=0.4</td><td><i>θ</i>=0.5</td></tr><tr><td rowspan="2"><br />Eunite2001 (74) </td><td><br />LAD-LASSO</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />AWLASSO</td><td>15 (1～15) </td><td>9 (1～9) </td><td>9 (1～9) </td><td>9 (1～9) </td></tr><tr><td rowspan="2"><br />Housing (66) </td><td><br />LAD-LASSO</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />AWLASSO</td><td>10 (2, 4～12) </td><td>10 (2, 4～12) </td><td>10 (2, 4～12) </td><td>10 (2, 4～12) </td></tr><tr><td rowspan="2"><br />MPG (22) </td><td><br />LAD-LASSO</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />AWLASSO</td><td>2 (4, 6) </td><td>1 (6) </td><td>1 (6) </td><td>0</td></tr><tr><td rowspan="2"><br />Triazines (24) </td><td><br />LAD-LASSO</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />AWLASSO</td><td>27 (1, 2, 4, 7, 10, 20～25, 30～35, 37～40, 42, 43, 48～51, 57, 58) </td><td>30 (1～3, 7, 10, 11, 20, 21, 27, 29～35, 37～40, 42, 43, 47, 48, 50, 51, 56～59) </td><td>58 (1～7, 9～59) </td><td>58 (1～7, 9～59) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="187">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908013_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 含异常点的标准数据集上MSE3的比较结果" src="Detail/GetImg?filename=images/JFYZ201908013_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 含异常点的标准数据集上<i>MSE</i><sub>3</sub>的比较结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908013_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparisons of <i>MSE</i><sub>3</sub> on benchmark datasets with outliers</p>

                </div>
                <div class="p1">
                    <p id="188">由图3可知当异常点含量为20%时, AWLASSO方法只在MPG数据集上<i>MSE</i><sub>3</sub>比LAD-LASSO的小, 但在Triazines数据集上两者<i>MSE</i><sub>3</sub>相差不大.当异常点含量为30%～50%时, AWLASSO方法的<i>MSE</i><sub>3</sub>要比LAD-LASSO的小很多, 且它不会像LAD-LASSO那样其<i>MSE</i><sub>3</sub>随着污染率的增大而显著增大.在标准数据集上的实验结果表明当数据集含异常点时, AWLASSO方法的特征选择能力更强、稳健性更好.</p>
                </div>
                <h4 class="anchor-tag" id="189" name="189"><b>3.4 高维数据集上的实验结果</b></h4>
                <div class="p1">
                    <p id="190">为验证AWLASSO方法在特征数量较多的数据集上的性能, 本文构造高维数据集D3和D4, 其构造方法与构造数据集的构造方法相同.高维数据集的真实回归系数<i>β</i><sub>true</sub>= (1, 2.5, 1.5, 2, 0, …, 0) <sup>T</sup>, 数据集如表6所示:</p>
                </div>
                <div class="area_img" id="191">
                    <p class="img_tit"><b>表6 高维数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 High Dimensional Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="191" border="1"><tr><td><br />Datasets</td><td><i>ε</i> Distribution</td><td>#Samples</td><td>#Features</td></tr><tr><td><br />D3</td><td><i>t</i> (5) </td><td>200</td><td>100</td></tr><tr><td><br />D4</td><td><i>N</i> (0, 1) </td><td>200</td><td>500</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="192">高维数据集上LASSO, LAD-LASSO和AWL-ASSO这3种方法特征选择的结果如图4所示.由于LASSO未完成特征选择, 故在图中未给出其结果.由图4 (a) (b) 可知, 当<i>λ</i>取合适值时, LAD-LASSO几乎没有选出无关特征, AWLASSO在D3和D4数据集上选出无关特征数目的均值接近于数据集所含无关特征总数, 且它正确选出了无关特征.</p>
                </div>
                <div class="p1">
                    <p id="193">图5和图6分别是3种模型在不同污染率下<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>的比较结果.从图5和图6中可以看出, 在不同污染率下, 相比LASSO和LAD-LASSO, 绝大多数情况下AWLASSO方法<i>MSE</i><sub>1</sub>都较小, <i>MSE</i><sub>2</sub>都较大, 且其对应的<i>MSE</i><sub>1</sub>并没有随着污染率的增大而显著增大.高维数据集上的实验结果表明, 当数据集含大量特征时, AWLASSO方法仍有较好的稳健性和特征选择能力.</p>
                </div>
                <div class="area_img" id="194">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908013_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在高维数据集上的特征选择结果" src="Detail/GetImg?filename=images/JFYZ201908013_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 在高维数据集上的特征选择结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908013_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Feature selection results on high dimensional data sets</p>

                </div>
                <div class="area_img" id="195">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908013_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 3种方法在D3数据集上的MSE1和MSE2比较结果" src="Detail/GetImg?filename=images/JFYZ201908013_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 3种方法在D3数据集上的<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>比较结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908013_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Comparisons of <i>MSE</i><sub>1</sub> and <i>MSE</i><sub>2</sub> on D3</p>

                </div>
                <div class="area_img" id="196">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908013_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 3种方法在D4数据集上的MSE1和MSE2比较结果" src="Detail/GetImg?filename=images/JFYZ201908013_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 3种方法在D4数据集上的<i>MSE</i><sub>1</sub>和<i>MSE</i><sub>2</sub>比较结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908013_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Comparisons of <i>MSE</i><sub>1</sub> and <i>MSE</i><sub>2</sub> on D4</p>

                </div>
                <h3 id="197" name="197" class="anchor-tag"><b>4 结  语</b></h3>
                <div class="p1">
                    <p id="198">目前针对回归问题的特征选择方法研究较少, 特别地, 当数据集含有异常点时, 现有的特征选择方法几乎都不能很好地选出有效特征.本文提出的面向异常点的稳健回归特征选择方法AWLASSO, 通过自适应正则项自主更新损失函数权重, 进而迭代估计回归系数.AWLASSO的迭代过程总是朝着较好的回归系数估计值方向进行, 在迭代后期其训练集含有足够的样本, 因而其获得了较好的实验结果.此外算法可以排除异常点的影响, 故其能较好地同时完成特征选择和学习器训练.与经典的LASSO和LAD-LASSO相比, 本文提出方法更稳健、更稀疏, 即使异常点含量较多该方法依然有效.然而该方法中的正则参数<i>λ</i>对方法性能有一定影响, 如何进一步提高方法的稳健性是我们未来的研究工作.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="227">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An introduction to variable and feature selection">

                                <b>[1]</b>Guyon I, Elisseeff A.An introduction to variable and feature selection[J].Journal of Machine Learning Research, 2003, 3 (6) :1157- 1182
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection:An ever evolving frontier in data mining">

                                <b>[2]</b>Liu Huan, Motoda H, Setiono R, et al.Feature selection:An ever evolving frontier in data mining[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 19 (2) :153- 158
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=Mjk2MjdOS1Y0VVhGcXpHYkM0SE5YT3JJMU5ZK3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZnVTd6&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Zhou Zhihua.Machine Learning[M].Beijing:Tsinghua University Press, 2016:249- 254 (in Chinese) (周志华.机器学习[M].北京:清华大学出版社, 2016:249- 254) 
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overfitting in making comparisonsbetween variable selection methods">

                                <b>[4]</b>Reunanen J.Overfitting in making comparisons between variable delection methods[J].Journal of Machine Learning Research, 2003, 3 (3) :1371- 1382
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738831&amp;v=MjQ1NzVUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZk9mYks3SHRETnFZOUZZK2dIQkg4NG9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Nakariyakul S, Casasent D P.An improvement on floating search algorithms for feature subset selection[J].Pattern Recognition, 2009, 42 (9) :1932- 1940
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature selection based on mutual information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy">

                                <b>[6]</b>Peng Hanchuan, Long Fuhui, Ding C.Feature selection based on mutual information:Criteria of max-dependency, max-relevance, and min-redundancy[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (8) :1226- 1238
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The element of statistical learning">

                                <b>[7]</b>Hastie T.The Element of Statistical Learning:Data Mining, Inference, and Prediction[M].Berlin:Springer, 2009:55- 75
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MDEzMjVIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZlllcks4SDlQTXFZOUdiZWtMQzNRN29CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Tibshirani R.Regression shrinkage and selection via the Lasso[J].Journal of the Royal Statistical Society, 1996, 58 (1) :267- 288
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080200137531&amp;v=MDg5OTFGWmVnSUNYODRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZk9mYks3SHRuTXJZOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Wang Lei.The L1 penalized LAD estimator for high dimensional linear regression[J].Journal of Multivariate Analysis, 2013, 120 (9) :135- 151
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=PZKX201003005&amp;v=MzIzOTFyRnl2Z1dyL0lOVGZBZHJHNEg5SE1ySTlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Zhang Hai, Wang Yao, Chang Xiangyu, et al.L1/2 regularization[J].Science China Information Sciences, 2010, 40 (3) :412- 422 (in Chinese) (张海, 王尧, 常象宇, 等.L1/2正则化[J].中国科学:信息科学, 2010, 40 (3) :412- 422) 
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Encyclopedia of Statistical Sciences">

                                <b>[11]</b>Hoerl A, Kennard R.Encyclopedia of Statistical Sciences[M].New York:Wiley-Interscience, 1998:129- 136
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256895&amp;v=Mjk1Nzd0SE5yWXBEYk9JS1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFVMckJJRjQ9TmlmY2FyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Zou Hui, Hastie T.Regularization and variable selection via the elastic net[J].Journal of the Royal Statistical Society, 2005, 67 (2) :301- 320
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001256921&amp;v=MTg5NTBPNEh0SE5yWXBEYmVrT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFVMckJJRjQ9TmlmY2Fy&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Yuan Ming, Lin Yi.Model selection and estimation in regression with grouped variables[J].Journal of the Royal Statistical Society, 2006, 68 (1) :49- 67
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800007272&amp;v=MDExMzZaZVp1SHlqbVVMcklJMXNkYWhJPU5qbkJhcks3SHRmT3A0OUZaT3NJRG5zN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Fan Jianqi, Li Runze.Variable selection via nonconcave penalized likelihood and its oracle properties[J].Publications of the American Statistical Association, 2001, 96 (456) :1348- 1360
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600630119&amp;v=MTY0NDdJSTFzZGFoST1OaWZZZXJLOEg5RE9xWTlGWXVnUERYMHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>Zhang Cunhui.Nearly unbiased variable selection under minimax concave penalty[J].Annals of Statistics, 2010, 38 (2) :894- 942
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800004455&amp;v=MDc1ODIvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2RhaEk9TmpuQmFySzdIdGZPcDQ5RlpPc0xDSGs4b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Li Youjuan, Zhu Ji.L1-norm quantile regression[J].Journal of Computational &amp; Graphical Statistics, 2008, 17 (1) :163- 185
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST15012900205944&amp;v=MDc0OTlLQlhnOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2RhaEk9TmlmWWVySzlIdERPcG85Rlp1cw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Belloni A, Chernozhukov V.ℓ-penalized quantile regression in high-dimensional sparse models[J].Annals of Statistics, 2011, 39 (1) :82- 130
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST1607190FFA453E51453D068B0E02D5CB&amp;v=MTQ0MjNHUWxmQ3BiUTM1TjFoeDcyMndLZz1OaWZZZXJLK0h0Yk5wbzh6RXBvTENYOU15aGNYN3p3SlNIbnEzaEpBZWJEZ1FNbnRDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Fan Jianqing, Fan Yingying, Barut E.Adaptive robust variable selection[J].Annals of Statistics, 2014, 42 (1) :324- 351
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300389119&amp;v=MTM0ODJNR0RYMHdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNkYWhJPU5pZk9mYks3SHRET3JJOUZaKw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>Arslan O.Weighted LAD-LASSO method for robust parameter estimation and variable selection in regression[J].Computational Statistics and Data Analysis, 2012, 56 (6) :1952- 1965
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTE1A2F2ADE9F0B38D247BD7B6862F1155&amp;v=MTc0Njg1TjFoeDcyMndLZz1OaWZZZXNhNWI5TzZyZjR4RWVKNURBNDZ4MklSN2pnUFBIaVFxaG96ZThTVlJMK2FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>Alfons A, Gelper S.Sparse least trimmed squares regression for analyzing high-dimensional large data sets[J].Annals of Applied Statistics, 2013, 7 (1) :226- 248
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJSB&amp;filename=SJSB388667D645189FFA3B4C5EDEEDB9A6AC&amp;v=MDAyNzRnPU5pZlliTEN3RnRmS3FQdERZTzRPQkhWUHVXY1FtRHNPVFFxVzJXZEJDN3ZsUTh2c0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjJ3Sw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Omara T M.Weighted robust Lasso and adaptive elastic net method for regularization and variable selection in robust regression with optimal scaling transformations[J].American Journal of Mathematics and Statistics, 2017, 7 (2) :71- 77
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES36E4AA088A790F375E09DB4AEB43A28E&amp;v=MjM2NDhmYkMrYTlXOTNvOU5iSm9JQlh4UHpCRVduejkwUEEzbTNXZEhmYkhsUjdMcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjJ3S2c9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>Wang Yanxin, Zhu Li.Variable selection and parameter estimation via WLAD-SCAD with a diverging number of parameters[J].Journal of the Korean Statistical Society, 2017, 46 (3) :390- 403
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new approach to variable selection in least squares problems">

                                <b>[23]</b>Osbome M R, Presnell B, Turlach B A.A new approach to variable selection in least squares problems[J].IMA Journal of Numerical Analysis, 2000, 20 (3) :389- 403 (15) 
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600629333&amp;v=MjI0NTNZOUZZdWtHRDM4Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc2RhaEk9TmlmWWVySzhIOURPcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>Efron B, Hastie T, Johnstone I, et al.Least angle regression[J].Annals of Statistics, 2004, 32 (2) :407- 451
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTFC7183B4B890081CC97417653E0D4F36&amp;v=MTM4MDVzWExHZERFclAxQkZ1TUdESHd4em1WZzR6aDVTWGprcVJGQWVjYVFNN21aQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjFoeDcyMndLZz1OaWZZZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>Friedman J, Hastie T, Höfling H, et al.Pathwise coordinate optimization[J].Annals of Applied Statistics, 2007, 1 (2) :302- 332
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201406016&amp;v=MDA3MTVnV3IvSUx5dlNkTEc0SDlYTXFZOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>Wang Yujun, Gao Qiankun, Zhang Xian, et al.A coordinate descent algorithm for solving capped-L1 regularization problems[J].Journal of Computer Research and Development, 2014, 51 (6) :1304- 1312 (in Chinese) (王玉军, 高乾坤, 章显, 等.一种求解截断L1正则化项问题的坐标下降算法[J].计算机研究与发展, 2014, 51 (6) :1304- 1312) 
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084869&amp;v=MDc0MzhNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSTFzZGFoST1OaWZPZmJLN0h0RE5xbzlFWk9NTEJIb3dvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b>Benezra M, Peleg S, Werman M.Real-time motion analysis with linear-programming[J].Computer Vision &amp; Image Understanding, 2000, 78 (1) :32- 52
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-paced learning for matrix factorization">

                                <b>[28]</b>Zhao Qian, Meng Deyu, Jiang Lu, et al.Self-paced learning for matrix factorization[C] //Proc of the 29th National Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2015:3196- 3202
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Self-paced learning for latent variable models">

                                <b>[29]</b>Kumar M P, Packer B, Koller D.Self-paced learning for latent variable models[C] //Proc of Int Conf on Neural Information Processing Systems.New York:Curran Associates, 2010:1189- 1197
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIBSVM:A library for support vector machines">

                                <b>[30]</b>Chang Chih-Chung, Lin Chih-Jen.LIBSVM:A library for support vector machines[EB/OL].2011 [2019-03-10].http://www.csie.ntu.edu.tw/～cjlin/libsvm
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908013&amp;v=MDkwMTBqTXA0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnV3IvSUx5dlNkTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RlUyVGEwOXg2bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

