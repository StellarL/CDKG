

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127884416681250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908011%26RESULT%3d1%26SIGN%3dCQQbxffjburTJNfg3fQ1vLoJhXE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908011&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908011&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908011&amp;v=MTY3ODdlWmVSckZ5dmdWYnpKTHl2U2RMRzRIOWpNcDQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 相关算法介绍&lt;/b&gt; "><b>1 相关算法介绍</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;1.1 MVKKM&lt;/b&gt;"><b>1.1 MVKKM</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;1.2 WMCFS&lt;/b&gt;"><b>1.2 WMCFS</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;1.3 TW-Co-&lt;i&gt;K&lt;/i&gt;-means&lt;/b&gt;"><b>1.3 TW-Co-<i>K</i>-means</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;1.4 Adapt-SA&lt;/b&gt;"><b>1.4 Adapt-SA</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="&lt;b&gt;2 基于样本加权的多视图聚类算法&lt;/b&gt; "><b>2 基于样本加权的多视图聚类算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="&lt;b&gt;2.1 基本定义&lt;/b&gt;"><b>2.1 基本定义</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;2.2 SWMVC算法&lt;/b&gt;"><b>2.2 SWMVC算法</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.3 SWMVC模型求解&lt;/b&gt;"><b>2.3 SWMVC模型求解</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="&lt;b&gt;3 实验设计与结果分析&lt;/b&gt; "><b>3 实验设计与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#142" data-title="&lt;b&gt;3.1 数据集描述&lt;/b&gt;"><b>3.1 数据集描述</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;3.2 实验方案&lt;/b&gt;"><b>3.2 实验方案</b></a></li>
                                                <li><a href="#161" data-title="&lt;b&gt;3.3 实验结果分析&lt;/b&gt;"><b>3.3 实验结果分析</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;3.4 参数敏感性分析&lt;/b&gt;"><b>3.4 参数敏感性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#183" data-title="&lt;b&gt;4 总  结&lt;/b&gt; "><b>4 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="&lt;b&gt;表1 符号含义&lt;/b&gt;"><b>表1 符号含义</b></a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;表2 不同聚类算法在多视图数据集上的&lt;i&gt;ACC&lt;/i&gt;值&lt;/b&gt;"><b>表2 不同聚类算法在多视图数据集上的<i>ACC</i>值</b></a></li>
                                                <li><a href="#169" data-title="&lt;b&gt;表3 不同聚类算法在多视图数据集上的&lt;i&gt;NMI&lt;/i&gt;值&lt;/b&gt;"><b>表3 不同聚类算法在多视图数据集上的<i>NMI</i>值</b></a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;表4 不同聚类算法在多视图数据集上的&lt;i&gt;F&lt;/i&gt;值&lt;/b&gt;"><b>表4 不同聚类算法在多视图数据集上的<i>F</i>值</b></a></li>
                                                <li><a href="#172" data-title="图1 各个样本在给定视图上的权重">图1 各个样本在给定视图上的权重</a></li>
                                                <li><a href="#182" data-title="图2 参数&lt;i&gt;λ&lt;/i&gt;对样本权重分布的影响">图2 参数<i>λ</i>对样本权重分布的影响</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="216">


                                    <a id="bibliography_1" title="Yarowsky D.Unsupervised word sense disambiguation rivaling supervised methods[C] //Proc of the 33rd Annual Meeting on Association for Computational Linguistics.Association for Computational Linguistics.Stroudsburg:ACL, 1995:189- 196" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised word sense disambiguation rivaling supervised methods">
                                        <b>[1]</b>
                                        Yarowsky D.Unsupervised word sense disambiguation rivaling supervised methods[C] //Proc of the 33rd Annual Meeting on Association for Computational Linguistics.Association for Computational Linguistics.Stroudsburg:ACL, 1995:189- 196
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_2" title="Blum A, Mitchell T.Combining labeled and unlabeled data with co-training[C] //Proc of the 11th Annual Conf on Computational Learning Theory.New York:ACM, 1998:92- 100" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining Labeled and Unlabeled Data with Co-Training">
                                        <b>[2]</b>
                                        Blum A, Mitchell T.Combining labeled and unlabeled data with co-training[C] //Proc of the 11th Annual Conf on Computational Learning Theory.New York:ACM, 1998:92- 100
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_3" title="Bickel S, Scheffer T.Multi-view clustering[C] //Proc of the 4th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2004:19- 26" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-View clustering">
                                        <b>[3]</b>
                                        Bickel S, Scheffer T.Multi-view clustering[C] //Proc of the 4th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2004:19- 26
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_4" title="Cleuziou G, Exbrayat M, Martin L, et al.CoFKM:A centralized method for multiple-view clustering[C] //Proc of the 9th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2009:752- 757" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Co FKM:a centralized method for multiple-view clustering">
                                        <b>[4]</b>
                                        Cleuziou G, Exbrayat M, Martin L, et al.CoFKM:A centralized method for multiple-view clustering[C] //Proc of the 9th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2009:752- 757
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_5" title="De Sa V R.Spectral clustering with two views[C] //Proc of the Workshop on Learning with Multiple Views.New York:ACM, 2005:20- 27" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral Clustering with Two Views">
                                        <b>[5]</b>
                                        De Sa V R.Spectral clustering with two views[C] //Proc of the Workshop on Learning with Multiple Views.New York:ACM, 2005:20- 27
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_6" title="Kumar A, Daum&#233; H.A co-training approach for multi-view spectral clustering[C] //Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:393- 400" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A co-training approach for multi-view spectral clustering">
                                        <b>[6]</b>
                                        Kumar A, Daum&#233; H.A co-training approach for multi-view spectral clustering[C] //Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:393- 400
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_7" title="Kumar A, Rai P, Daume H.Co-regularized multi-view spectral clustering[C] //Proc of the 24th Int Conf on Neural Information Processing System.Vancouver:Curran Associates, 2011:1413- 1421" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Co-regularized multiview spectral clustering">
                                        <b>[7]</b>
                                        Kumar A, Rai P, Daume H.Co-regularized multi-view spectral clustering[C] //Proc of the 24th Int Conf on Neural Information Processing System.Vancouver:Curran Associates, 2011:1413- 1421
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_8" title="Tzortzis G F, Likas A C.Multiple view clustering using a weighted combination of exemplar-based mixture models[J].IEEE Transactions on Neural Networks, 2010, 21 (12) :1925- 1938" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple view clustering using a weighted combination of exemplar-based mixture models">
                                        <b>[8]</b>
                                        Tzortzis G F, Likas A C.Multiple view clustering using a weighted combination of exemplar-based mixture models[J].IEEE Transactions on Neural Networks, 2010, 21 (12) :1925- 1938
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_9" title="Tzortzis G, Likas A.Kernel-Based Weighted Multi-view Clustering[C] //Proc of the 12th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2012:675- 684" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Kernel-based weighted multiview clustering">
                                        <b>[9]</b>
                                        Tzortzis G, Likas A.Kernel-Based Weighted Multi-view Clustering[C] //Proc of the 12th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2012:675- 684
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_10" title="Cai Xiao, Nie Feiping, Huang Heng.Multi-view K-means clustering on big data[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.San Francisco:Morgan Kaufmann, 2013:2598- 2604" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view k-means clustering on big data">
                                        <b>[10]</b>
                                        Cai Xiao, Nie Feiping, Huang Heng.Multi-view K-means clustering on big data[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.San Francisco:Morgan Kaufmann, 2013:2598- 2604
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_11" title="Nie Feiping, Cai Guohao, Li Jing, et al.Auto-weighted multi-view learning for image clustering and semi-supervised classification[J].IEEE Transactions on Image Processing, 2018, 27 (3) :1501- 1511" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auto-Weighted Multi-View Learning for Image Clustering and Semi-Supervised Classification">
                                        <b>[11]</b>
                                        Nie Feiping, Cai Guohao, Li Jing, et al.Auto-weighted multi-view learning for image clustering and semi-supervised classification[J].IEEE Transactions on Image Processing, 2018, 27 (3) :1501- 1511
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_12" title="Nie Feiping, Cai Guohao, Li Xuelong, et al.Multi-View Clustering and Semi-Supervised Classification with Adaptive Neighbours[C] //Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park:AAAI, 2017:2408- 2414" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view clustering and semi-supervised classification with adaptive neighbours">
                                        <b>[12]</b>
                                        Nie Feiping, Cai Guohao, Li Xuelong, et al.Multi-View Clustering and Semi-Supervised Classification with Adaptive Neighbours[C] //Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park:AAAI, 2017:2408- 2414
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_13" title="Chen Xiaojun, Xu Xiaofei, Huang Joshuazhexue, et al.TW-K-means:Automated two-level variable weighting clustering algorithm for multiview data[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (4) :932- 944" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TW-k-means: Automated Two-level VariableWeighting Clustering Algorithm for Multiview Data">
                                        <b>[13]</b>
                                        Chen Xiaojun, Xu Xiaofei, Huang Joshuazhexue, et al.TW-K-means:Automated two-level variable weighting clustering algorithm for multiview data[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (4) :932- 944
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_14" title="Xu Yumeng, Wang Changdong, Lai Jianhuang.Weighted multi-view clustering with feature selection[J].Pattern Recognition, 2016, 53:25- 35" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCC0E24345C8E7AD09D2EF1E98AF69286&amp;v=MzE0NDZmQ3BiUTM1TjFoeDcyNXc2az1OaWZPZmNETEhxVE9xNHhCWVpnSGVYdEl1eFlhbmowSVBuNlhwUnBFRDdTZFI3S1pDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Xu Yumeng, Wang Changdong, Lai Jianhuang.Weighted multi-view clustering with feature selection[J].Pattern Recognition, 2016, 53:25- 35
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_15" title="Zhang Guangyu, Wang Changdong, Huang Dong, et al.TW-Co-K-means:two-level weighted collaborative K-means for multi-view clustering[J].Knowledge-Based Systems, 2018, 150:127- 138" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES07C7273BA8B9DE2CCBF5CDB351FEB2B6&amp;v=MTIxNDliTy9iZGJPcUl3M0ZlTjlCUWhNeldWZ21FbDRPd3VRcnhjMEQ4Zm1SOGlaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjFoeDcyNXc2az1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        Zhang Guangyu, Wang Changdong, Huang Dong, et al.TW-Co-K-means:two-level weighted collaborative K-means for multi-view clustering[J].Knowledge-Based Systems, 2018, 150:127- 138
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_16" title="Li Yafang, Jia Caiyan, Kong Xiangnan, et al.Locally weighted Fusion of structural and attribute information in graph clustering[J].IEEE Transactions on Cybernetics, 2017, 49 (1) :247- 260" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locally weighted Fusion of structural and attribute information in graph clustering">
                                        <b>[16]</b>
                                        Li Yafang, Jia Caiyan, Kong Xiangnan, et al.Locally weighted Fusion of structural and attribute information in graph clustering[J].IEEE Transactions on Cybernetics, 2017, 49 (1) :247- 260
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_17" title="Hu Yanqing, Li Menghui, Zhang Peng, et al.Community detection by signaling on complex networks[J].Physical Review E, 2008, 78 (1) :139- 143" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Community detection by signaling on complex networks">
                                        <b>[17]</b>
                                        Hu Yanqing, Li Menghui, Zhang Peng, et al.Community detection by signaling on complex networks[J].Physical Review E, 2008, 78 (1) :139- 143
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1677-1685 DOI:10.7544/issn1000-1239.2019.20190150            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>样本加权的多视图聚类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B4%AA%E6%95%8F&amp;code=29717270&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">洪敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BE%E5%BD%A9%E7%87%95&amp;code=24731575&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贾彩燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E4%BA%9A%E8%8A%B3&amp;code=42588557&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李亚芳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E5%89%91&amp;code=06320533&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于剑</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BA%A4%E9%80%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%8C%96%E6%8E%98%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%8C%97%E4%BA%AC%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6)&amp;code=0111847&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">交通数据分析与挖掘北京市重点实验室(北京交通大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0034856&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京交通大学计算机与信息技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%83%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京工业大学信息学部</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>大数据时代, 人类收集、存储、传输、管理数据的能力日益提高, 各行各业已经积累了大量的数据资源, 这些数据常呈现出多源性和异构性.如何对这些多源数据进行有效的聚类 (也称为多视图聚类) 已成为当今机器学习研究关注的焦点之一.现有的多视图聚类算法主要从“全局”角度关注不同视图和特征对簇结构的贡献, 没有考虑不同样本间存在的“局部”信息间的差异.因此, 提出一种新的多视图样本加权聚类算法 (sample-weighted multi-view clustering, SWMVC) , 该算法对每个样本的不同视图进行加权, 采用交替方向乘子法自适应学习样本权值, 不仅可以学习不同样本点间不同视图权重的“局部”差异, 还可以从学习到的“局部”差异反映出不同视图对簇结构贡献的“全局”差异, 具有较好的灵活性.多个数据集上的实验表明:SWMVC方法在异质视图数据上具有较好的聚类效果.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据挖掘;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%A7%86%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多视图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3EK%3C%2Fi%3E-means&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>K</i>-means;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%B7%E6%9C%AC%E6%9D%83%E9%87%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">样本权重;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *贾彩燕, cyjia@bjtu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61876016, 61632004);</span>
                                <span>中央高校基本科研业务费专项资金项目 (2018JBZ006);</span>
                    </p>
            </div>
                    <h1><b>Sample-Weighted Multi-View Clustering</b></h1>
                    <h2>
                    <span>Hong Min</span>
                    <span>Jia Caiyan</span>
                    <span>Li Yafang</span>
                    <span>Yu Jian</span>
            </h2>
                    <h2>
                    <span>Beijing Key Laboratory of Traffic Data Analysis and Mining (Beijing Jiaotong University)</span>
                    <span>School of Computer and Information Technology, Beijing Jiaotong University</span>
                    <span>Faculty of Information Technology, Beijing University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the era of big data, the ability of humans to collect, store, transmit and manage data has been increasingly improved. Various industries have accumulated a large amount of data resources, which are often multi-source and heterogeneous. How to effectively cluster these multi-source data (also known as multi-view clustering) has become one of the focuses of today's machine learning research. The existing multi-view clustering algorithms mainly pay attention to the contribution of different views and features to the cluster structure from the “global” perspective, without considering the “local” information complementary differences between different samples. Therefore, this paper proposes a new sample-weighted multi-view clustering (SWMVC) . The method weights each sample with different views and adopts alternating direction method of multipliers (ADMM) to learn sample weight, which can not only learn the “local” difference of weights among multiple views in different sample points, but also reflect the “global” difference of the contribution of different views to the cluster structure, and has better flexibility. Experiments on multiple datasets show that the SWMVC method has a better clustering effect on heterogeneous view data.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20mining&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data mining;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-view&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-view;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cluster&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cluster;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3EK%3C%2Fi%3E-means&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>K</i>-means;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sample%20weights&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sample weights;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Hong Min, born in 1993.Master in Beijing Jiaotong University.Her main research interest is multi-view clustering. <image id="282" type="" href="images/JFYZ201908011_28200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Jia Caiyan, born in 1976.Professor and PhD supervisor in Beijing Jiaotong University.Her main research interests include data mining, complex network analysis, text mining, bioinformatics, text mining and etc. <image id="284" type="" href="images/JFYZ201908011_28400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Li Yafang, born in 1988.Postdoctoral fellow in the Faculty of Information Technology, Beijing University of Technology. Her main research interests include data mining and complex network analysis, etc. <image id="286" type="" href="images/JFYZ201908011_28600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Yu Jian, born in 1969.Professor and PhD supervisor in Beijing Jiaotong University. Senior member of CCF.His main research interests include machine learning, data mining and image segmentation and computational intelligence.<image id="288" type="" href="images/JFYZ201908011_28800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61876016, 61632004);</span>
                                <span>the Fundamental Research Funds for the Central Universities (2018JBZ006);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="51">飞速发展的信息技术使现实世界中的数据不仅呈现出规模庞大的特性, 而且多源信息采集技术和多样化的特征表示使得多视图数据在众多实际应用中越来越普遍.例如Web网页可以从3个不同的角度描述:词向量视图直观刻画了网页文本中单词的出现情况;网页中的图像提供了丰富的视觉特征视图;网页与网页之间的链接关系展示了网页内容彼此之间的相关性.多视图数据的互补性和一致性有利于从不同方位协同完成特定的机器学习任务.因此, 多视图学习逐渐受到人们的关注.</p>
                </div>
                <div class="p1">
                    <p id="52">多视图研究最早起源于Yarowsky<citation id="250" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Blum等人<citation id="251" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的消除单词歧义算法和协同训练算法.Yarowsky<citation id="252" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>将文档中单词信息和单词所在文档信息定义为2个视图, 并将不同视图加入各自的分类器进行交互学习.Blum等人<citation id="253" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在对多视图定义分类器的同时, 基于视图独立性的原则引入协同训练.Bickel等人<citation id="254" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>以协同EM (expectation maximiza-tion) 算法为基础, 提出了多视图EM和两视图球状<i>K</i>-means算法.Cleuziou等人<citation id="255" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了CoFKM (centralized method for multiple-view clustering) 算法, 通过集中式策略使多个视图的信息尽可能的一致.Sa<citation id="256" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>构造了一个二视图谱聚类算法, 创建一个描述2个视图共现信息的二部图, 以最小化视图之间的不一致性.Kumar等人<citation id="257" type="reference"><link href="226" rel="bibliography" /><link href="228" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>相继提出了在多视图谱聚类加入协同训练和协同正则的方法, 保持了不同视图上聚类结果的一致性.</p>
                </div>
                <div class="p1">
                    <p id="53">但是, 上述方法平等看待各个视图, 难以体现不同视图信息对簇结构重要性的差异, 因此出现了一些视图权重学习的经典算法.较早的工作是Tzortzis等人<citation id="258" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>给出的多视图带权凸混合模型 (convex mixture models, CMM) .该模型对不同视图训练相应的CMM模型, 同时为各个视图分配自适应权重, 并将自学习视图权重思想推广至核<i>K</i>-means, 设计了多视图核函数<i>K</i>-means算法 (multi-view kernel <i>K</i>-means, MVKKM) <citation id="259" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>.该算法通过预先定义的核函数将原始数据映射到新的特征空间, 并根据视图对簇划分结果的贡献度为每一个视图分配权重.为了适用大规模多视图数据, Cai等人<citation id="260" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>给出了RMKMC模型, 不仅在目标函数中新增视图权值参数, 同时ℓ<sub>2, 1</sub>范式还增强了算法的鲁棒性.最近, Nie等人<citation id="263" type="reference"><link href="236" rel="bibliography" /><link href="238" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>从图的角度提出了2种适用聚类和半监督分类的多视图学习模型:AMGL和MLAN.AMGL<citation id="261" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>凭借视图近邻图学习, 构建出了多视图谱聚类权重学习框架;MLAN<citation id="262" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过学习多图局部流形结构, 不断修正簇结构矩阵以优化聚类性能.</p>
                </div>
                <div class="p1">
                    <p id="54">多视图学习不仅是不同视图之间的有效性融合, 而且视图特征的选择也是多视图学习的重要组成部分.Chen等人<citation id="264" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了自适应双权多视图算法 (TW-<i>K</i>-means) .该算法能同时学习视图权重和特征权重对簇结构的贡献.Xu等人<citation id="265" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了具有特征选择的加权多视图聚类算法 (weighted multi-view clustering with feature selection, WMCFS) .Zhang等人<citation id="266" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>在多视图权重学习模型中增加了视图间的一致性约束, 设计了性能良好的TW-Co-<i>K</i>-means模型.</p>
                </div>
                <div class="p1">
                    <p id="55">除了以上的学习不同视图和特征对簇结构贡献差异的“全局”模型外, Li等人以节点属性和节点的链接关系为2种数据源, 学习了不同样本点上不同类型信息对社区结构 (即簇结构) 贡献的“局部”差异<citation id="267" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 给出了Adapt-SA模型, 取得了不错的效果.但该模型需要对2种信息下的数据进行同维变换, 不但缺乏灵活性且同维变换易造成原有数据上信息的损失.因此, 我们基于两视图同维变换样本局部权值学习模型Adapt-SA, 提出了一种新的多视图样本加权聚类算法 (sample-weighted multi-view clustering, SWMVC) .该算法不仅可以学习不同样本点对多个视图间权重的差异, 具有“局部”学习能力, 而且还可以从“全局”角度反映不同视图对簇结构的贡献差异.另外, 该方法不仅克服了Adapt-SA方法同维变换的约束, 而且将Adapt-SA的思想从2个视图扩展到多个视图上.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>1 相关算法介绍</b></h3>
                <div class="p1">
                    <p id="57">本节, 我们介绍4个经典的加权多视图聚类算法:MVKKM<citation id="268" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, WMCFS<citation id="269" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, TW-Co-<i>K</i>-means<citation id="270" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和Adapt-SA<citation id="271" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.给定多视图数据集<i>X</i>={<b><i>X</i></b><sup>1</sup>, <b><i>X</i></b><sup>2</sup>, …, <b><i>X</i></b><sup><i>V</i></sup>}, 其中, <b><i>X</i></b><sup><i>v</i></sup>∈R<sup><i>N</i>×<i>D</i><sub><i>v</i></sub></sup>, 即每个数据视图包括<i>N</i>个数据.<b><i>x</i></b><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>是<b><i>X</i></b><sup><i>v</i></sup>的第<i>i</i>个样本.</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>1.1 MVKKM</b></h4>
                <div class="p1">
                    <p id="60">MVKKM (multi-view clustering kernel <i>K</i>-means) <citation id="272" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>是一种基于核函数的多视图权值模型.相比传统的视图权值方法, 该算法使用事先定义的核函数对数据进行映射, 然后在多视图<i>K</i>-means中学习各个视图的重要性.其目标函数:</p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Κ</mi><mo>˜</mo></mover><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>ω</mi><msubsup><mrow></mrow><mi>v</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>δ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow></msub><mrow><mo>|</mo><mrow><mtext>ϕ</mtext><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="63">其中, <i>ω</i><sub><i>v</i></sub>, ϕ<sup> (<i>v</i>) </sup>和<i>m</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mrow><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分别表示各个视图的权重、核函数和中心, <i>p</i>是控制视图权重分布的超参数, <i>δ</i><sub><i>i k</i></sub>是各个视图的共有划分结果.对于核函数, 可以选用线性核、高斯径向基核等.</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>1.2 WMCFS</b></h4>
                <div class="p1">
                    <p id="66">基于MVKKM思想, WMCFS (weighted multi-view clustering with feature selection) <citation id="273" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>模型在多视图<i>K</i>-means聚类过程中同时考虑不同视图和各个特征的差异, 设计了目标函数:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ε</mi><msub><mrow></mrow><mi>Η</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mi>p</mi></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>δ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow></msub><mrow><mo>|</mo><mrow><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false"> (</mo><mi>τ</mi><msup><mrow></mrow><mi>v</mi></msup><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>τ</mi><msup><mrow></mrow><mi>v</mi></msup></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中, <i>ω</i><sub><i>v</i></sub>和<i>τ</i><sup><i>v</i></sup>分别表示视图权重和视图中各个特征的权重, <i>p</i>和<i>β</i>是分别控制视图和特征稀疏性的参数.</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>1.3 TW-Co-<i>K</i>-means</b></h4>
                <div class="p1">
                    <p id="70">TW-Co-<i>K</i>-means (two-level weighted collaborative <i>K</i>-means) <citation id="274" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>采用协同策略约束不同视图之间的一致性, 进而学习视图和特征对簇结构的重要性.其目标函数:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>v</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>G</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup></mrow></munder><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>u</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>k</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mi>η</mi><mrow><mo stretchy="false"> (</mo><mi>Τ</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac><mi>Δ</mi><mo>+</mo><mi>α</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>G</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup></mrow></munder><mi>w</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mtext> </mtext><mi>w</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>v</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mtext> </mtext><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Δ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>t</mi><mo>′</mo></msup><mo>≠</mo><mi>t</mi></mrow></munder><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow><mrow><mo stretchy="false"> (</mo><msup><mi>t</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi>u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mi>ξ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><msup><mi>t</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></msup><mi>ξ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><msup><mi>t</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mstyle></mrow></math></mathml>, (4) </p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ξ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>G</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup></mrow></munder><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>k</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></math></mathml>, (5) </p>
                </div>
                <div class="p1">
                    <p id="76">其中, <i>v</i><sup> (<i>t</i>) </sup>, <i>w</i><sup> (<i>t</i>) </sup>, <i>u</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>和<i>m</i><sup> (<i>t</i>) </sup>分别表示各个视图权重、特征权重、各个视图的指派和中心, <i>Δ</i>衡量不同视图之间的不一致性.<i>α</i>, <i>β</i>和<i>η</i>分别控制<i>v</i><sup> (<i>t</i>) </sup>, <i>w</i><sup> (<i>t</i>) </sup>的分布, 以及<i>Δ</i>的影响.根据各个视图的指派, 融合视图权值, 便可获得最终的共有指派.</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>1.4 Adapt-SA</b></h4>
                <div class="p1">
                    <p id="79">Adapt-SA (adaptive fusion of structural and attribute information) <citation id="275" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>是面向图聚类问题提出的一种学习样本上不同种类信息 (链接信息和节点属性) 对节点簇结构重要性差异的<i>K</i>-means型算法.该算法首先将2类信息映射到同维空间上, 再进行加权融合.因此, 融合后的表示具有统一的簇中心.其目标函数:</p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi mathvariant="bold-italic">U</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mrow><mo>|</mo><mrow><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>z</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, (6) </p>
                </div>
                <div class="p1">
                    <p id="82">其中, <b><i>U</i></b>和<i>z</i>分别表示划分矩阵和融合中心.<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">A</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>是经过Signal相似度<citation id="276" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>变换的链接向量, <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">C</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>是使用余弦相似度变换的属性向量.</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag"><b>2 基于样本加权的多视图聚类算法</b></h3>
                <div class="p1">
                    <p id="86">Adapt-SA虽然可以学习不同样本间2类信息重要性的差异, 但该方法在进行信息融合时, 需要对原空间数据进行同维变换, 可能会造成一定的信息损失.并且, 同维变化也会带来算法复杂性的增加.在实际应用中, 可能某个视图的维度适当, 最宜在原空间进行簇结构学习, 使得Adapt-SA缺乏灵活性.因此, 本部分我们给出了一种普适性更强且能够学习不同样本点权值的多视图<i>K</i>-means算法, 以弥补Adapt-SA算法的不足.</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.1 基本定义</b></h4>
                <div class="p1">
                    <p id="88">为方便介绍所提出的算法模型, 本节所用符号的具体含义如表1所示:</p>
                </div>
                <div class="area_img" id="89">
                    <p class="img_tit"><b>表1 符号含义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 The Symbol Meaning</b></p>
                    <p class="img_note"></p>
                    <table id="89" border="1"><tr><td><br />Symbol</td><td> Meaning</td></tr><tr><td><br /><i>V</i></td><td>Number of views</td></tr><tr><td><br /><i>N</i></td><td>Number of samples in each view</td></tr><tr><td><br /><i>K</i></td><td>Number of clusters</td></tr><tr><td><br /><i>δ</i><sub><i>i k</i></sub></td><td>Cluster assignment showing whether the <i>i</i>-th sample belongs to the <i>k</i>-th cluster</td></tr><tr><td><br /><i>x</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></td><td><i>The</i> i-<i>th sample of the</i> v-<i>th view</i></td></tr><tr><td><br />m<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow></math></td><td><i>The cluster center of the</i> k-<i>th cluster in the</i> v-<i>th view</i></td></tr><tr><td><br />α<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></td><td><i>Weight for the</i> i-<i>th sample of the</i> v-<i>th view</i></td></tr><tr><td><br />λ</td><td><i>Parameter controlling the sparisity of sample weight</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.2 SWMVC算法</b></h4>
                <div class="p1">
                    <p id="91">受Adapt-SA思想的启发, 并结合多视图<i>K</i>-means算法, 为了描述多视图聚类过程中每个样本点的不同视图对簇结构的贡献, SWMVC算法优化模型:</p>
                </div>
                <div class="area_img" id="93">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908011_09300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="94">SWMVC的目标函数由2部分组成:首先是在多视图<i>K</i>-means的基础上增加了样本权参数<i>α</i><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>, 用于分析样本<i>x</i><sub><i>i</i></sub>对第<i>v</i>个视图的重要性.同时, 为了使模型能够在所有可能解的分布中得到<i>α</i><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>的最优分布, 增加了基于信息熵的加权项, 并通过<i>λ</i>控制每个数据点权值的大小, 有利于调节样本权<i>α</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>的稀疏性.</p>
                </div>
                <div class="p1">
                    <p id="98">因此, SWMVC模型具备3方面的特点:1) 能使每一个视图拥有独立的簇中心;2) 每个样本点对多个视图拥有自己对簇结构贡献的权重影响, 具有局部学习能力, 同时通过样本点在各视图上的不同重要程度, 易于统计各视图对簇结构的全局重要性;3) 不但可以在多个视图的原空间进行操作, 也可以在变换后的视图空间 (如使用Adapt-SA模型中的同维变换策略) 进行学习, 灵活性高.</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.3 SWMVC模型求解</b></h4>
                <div class="p1">
                    <p id="100">求解SWMVC目标函数式 (7) 时使用交替方向乘子法.对于3个未知项<i>δ</i><sub><i>i k</i></sub>, <i>α</i><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>和<i>m</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow></math></mathml>, 在固定2个未知项的同时使用梯度下降法进行求解.具体求解流程为:</p>
                </div>
                <div class="p1">
                    <p id="103">1) 固定所有视图节点权重<i>α</i><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>和视图类中心<i>m</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow></math></mathml>, 更新视图的公共隶属度元素<i>δ</i><sub><i>i k</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="106">在求解时, 针对每一个类寻求所有视图到每个类中心的带权距离最小值.即按照式 (8) 进行计算, 之后根据得到的最小值结果对每个数据点进行指派.</p>
                </div>
                <div class="p1">
                    <p id="107"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow></msub></mrow></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>α</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>. (8) </p>
                </div>
                <div class="p1">
                    <p id="109">2) 固定视图共有节点隶属度<i>δ</i><sub><i>i k</i></sub>和视图类中心<i>m</i><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow></math></mathml>, 更新每个视图的节点权重<i>α</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="112">对每个视图而言, 可看成是求解<i>K</i>个彼此独立的子问题, 视图<i>v</i>的节点<i>x</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>隶属于第<i>k</i>个类时, 隶属度<i>δ</i><sub><i>i k</i></sub>=1.为了最小化目标函数, 利用对视图节点权重在多个视图上施加的约束, 采用拉格朗日乘子法求解, 加入拉格朗日乘子<i>β</i>后, 目标函数:</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>, </mo><mi>β</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>δ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow></msub><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mtext> </mtext><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>+</mo><mi>β</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn><mo>, </mo><mi>i</mi><mo>∈</mo><mi>Ν</mi></mrow><mi>V</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">通过对式 (9) 中<i>α</i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>求导, 得到:</p>
                </div>
                <div class="p1">
                    <p id="117"><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mi>λ</mi><mo>-</mo><mi>β</mi></mrow><mi>λ</mi></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="119">利用对<i>α</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>的现有约束条件, 可以推出最终的更新公式</p>
                </div>
                <div class="p1">
                    <p id="121"><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>λ</mi></mfrac></mrow><mo>) </mo></mrow></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mo>-</mo><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>λ</mi></mfrac></mrow><mo>) </mo></mrow></mrow></mfrac></mrow></math></mathml>, (11) </p>
                </div>
                <div class="p1">
                    <p id="123">3) 固定视图节点权重<i>α</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>和视图共有节点隶属度<i>δ</i><sub><i>i k</i></sub>, 更新每个视图的类中心<i>m</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="126"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>δ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow></msub><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup><mi>δ</mi><msub><mrow></mrow><mrow><mi>i</mi><mtext> </mtext><mi>k</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>. (12) </p>
                </div>
                <div class="p1">
                    <p id="128">根据上述模型求解过程知, 本节构建的基于样本加权的多视图聚类算法 (SWMVC) 的详细步骤如算法1所示:</p>
                </div>
                <div class="p1">
                    <p id="129"><b>算法1</b>. SWMVC.</p>
                </div>
                <div class="p1">
                    <p id="130">输入:多视图数据集<i>X</i>={<b><i>X</i></b><sup>1</sup>, <b><i>X</i></b><sup>2</sup>, …, <b><i>X</i></b><sup><i>V</i></sup>}、类个数<i>K</i>、参数<i>λ</i>、最大迭代次数<i>t</i><sub>max</sub>;</p>
                </div>
                <div class="p1">
                    <p id="131">输出:<i>X</i>的共有隶属度<i>δ</i>、样本权集<i>α</i><sup><i>v</i></sup>.</p>
                </div>
                <div class="p1">
                    <p id="132">① 初始化:每个视图随机选取<i>K</i>个中心, <i>α</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>=1/<i>V</i>, <i>t</i>=0;</p>
                </div>
                <div class="p1">
                    <p id="134">② 根据式 (8) 更新<i>δ</i>;</p>
                </div>
                <div class="p1">
                    <p id="135">③ 根据式 (11) 更新<i>α</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="137">④ 根据式 (12) 更新<i>m</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>v</mi></msubsup></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="139">⑤ <i>t</i>=<i>t</i>+1;</p>
                </div>
                <div class="p1">
                    <p id="140">⑥ 如果<i>t</i>&gt;<i>t</i><sub>max</sub>, 停止算法, 返回<i>δ</i>和<i>α</i><sup><i>v</i></sup>.</p>
                </div>
                <h3 id="141" name="141" class="anchor-tag"><b>3 实验设计与结果分析</b></h3>
                <h4 class="anchor-tag" id="142" name="142"><b>3.1 数据集描述</b></h4>
                <div class="p1">
                    <p id="143">为了评估本文提出的基于样本加权的多视图聚类模型 (SWMVC) 算法的效果, 对不同的多视图权值聚类模型进行实验和对比分析.我们选取了视图之间是异质特性的6个数据集, 分别是WebKB中的Cornell, Texas, Washington, Wisconsin网络/文本数据集和图像/文本数据集Wiki和VOC以及视图具有同质特性的2个数据集:Handwritten numerals和Caltech101-7.</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144">1) WebKB数据集</h4>
                <div class="p1">
                    <p id="145">Cornell, Texas, Washington和Wisconsin是WebKB网页数据集的子集, 分别包含195, 187, 230, 265个样本.每个样本通过引用关系视图和文本内容属性视图进行描述, 对应的成对视图维数分别是195维和1 703维、187维和1 703维、230维和1 703维、265维和1 626维.该数据集涉及到5个类别课程、学院、学生、工程和员工.</p>
                </div>
                <h4 class="anchor-tag" id="146" name="146">2) Wiki数据集</h4>
                <div class="p1">
                    <p id="147">Wiki是从维基百科专题文章中提取的数据集, 常用于跨模态检索中.该数据集由2 173个训练样本和693个测试样本组成的图片-文本对数据, 共有10个类.其中, 每一张图片使用128维SIFT特征向量视图表示, 并含有相关图片的10维文本LDA主题描述向量视图.</p>
                </div>
                <h4 class="anchor-tag" id="148" name="148">3) Pascal Visual Object Classes 2007数据集</h4>
                <div class="p1">
                    <p id="149">Pascal Visual Object Classes 2007 (VOC) 是一个自然图像数据集.选用其中的2 808张图片, 每一张图像有512维的GIST特征和399维的TF-IDF (term frequency-inverse document frequency) 文本特征.涉及到20个类aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, tv/monitor.</p>
                </div>
                <h4 class="anchor-tag" id="150" name="150">4) Handwritten numerals数据集</h4>
                <div class="p1">
                    <p id="151">Handwritten numerals (HW) 是一个含有10个类的2 000个手写体数字数据集.本实验选取其中的5个视图数据, 分别是维数76维FOU特征、216维FAC特征、64维KAR特征、240维PIX特征和47维ZER特征.</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152">5) Caltech101-7数据集</h4>
                <div class="p1">
                    <p id="153">Caltech101-7是一个对象识别数据集.包括1 474张图片, 被划分为7个类, 分别是Face, Motorbike, DollaBill, Garfield, Snoopy, Stop-Sign, Windsor-Chair, 含有图像的Gabor, Wavelet, CENTRIST, HOG, GIST和LBP 6个特征, 并将其作为Caltech101-7的6个视图, 对应视图的特征维数分别为48维、40维、254维、1984维、512维和928维.</p>
                </div>
                <h4 class="anchor-tag" id="154" name="154"><b>3.2 实验方案</b></h4>
                <div class="p1">
                    <p id="155">为了全面评估本文提出的基于样本加权的多视图聚类模型的性能, 在实验部分中对比研究了多视图聚类领域中的多个代表性算法:</p>
                </div>
                <div class="p1">
                    <p id="156">1) WMCFS<citation id="277" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>是一种具有特征选择功能的多视图聚类算法.该方法的2个参数<i>p</i>和<i>β</i>利用网格贪心搜索方法在3.1节介绍的8个数据集上依次设置为{5, 0.5}, {30, 0.0005}, {20, 0.5}, {30, 0.0002}, {10, 0.5}, {20, 0.0005}, {10, 0.1}和{5, 0.0005}.</p>
                </div>
                <div class="p1">
                    <p id="157">2) MLAN<citation id="278" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是一种面向图聚类的多视图聚类算法.与传统图聚类不同的是, 它考虑局部流形结构在每一轮迭代不断修改相似性矩阵, 直至获得最优矩阵.该方法可以自动学习各个视图的权重系数.其隐含参数近邻数<i>k</i>=9.</p>
                </div>
                <div class="p1">
                    <p id="158">3) AMGL<citation id="279" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>该方法假设所有图共享潜在簇结构, 是一种无参数的多图学习框架, 即在学习时能够为各个图分配合适的权值, 可以应用于多视图聚类和半监督分类任务.其隐含参数近邻数<i>k</i>=5.</p>
                </div>
                <div class="p1">
                    <p id="159">4) TW-Co-<i>K</i>-means<citation id="280" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是一种融合视图和特征的多视图算法.该方法全面考虑不同视图的互补性和一致性, 以协同方式挖掘不同视图间的共享信息, 同时考量了每个视图的多样性特点.参数<i>η</i>, <i>α</i>和<i>β</i>在各个数据集分别采用网格贪心搜索方法设置为{0.8, 10, 60}, {0.8, 30, 60}, {0.1, 20, 80}, {0.9, 50, 80}, {0.3, 30, 8}, {0.01, 80, 50}, {0.6, 10, 50}和{0.2, 30, 8}.</p>
                </div>
                <div class="p1">
                    <p id="160">5) SWMVC是本文提出的基于样本加权的多视图聚类算法.模型中含有样本权稀疏系数, 参数<i>λ</i>经过网格贪心搜索方法在各数据集的值分别设为100, 75, 35, 60, 0.5, 1, 10, 45.</p>
                </div>
                <h4 class="anchor-tag" id="161" name="161"><b>3.3 实验结果分析</b></h4>
                <div class="p1">
                    <p id="162">本节首先展示不同方法在各种多视图数据集上的聚类效果, 然后展示WebKB, Wiki和VOC所有样本在给定视图的权重分布, 最后研究了参数<i>λ</i>的敏感性.</p>
                </div>
                <h4 class="anchor-tag" id="163" name="163">1) 多视图聚类效果对比</h4>
                <div class="p1">
                    <p id="164">表2～4分别展示了各多视图聚类方法在8个数据集上的聚类评价指标<i>ACC</i>, <i>NMI</i>和<i>F</i>-measure.其中, 各个算法精度选用20次运行结束的最大值, 各多视图聚类算法在同一数据集上的最好结果用黑体表示.</p>
                </div>
                <div class="p1">
                    <p id="165">由表2～4可以看出:</p>
                </div>
                <div class="p1">
                    <p id="166">① 相比传统多视图聚类方法, 本文提出的基于样本加权的多视图聚类算法 (SWMVC) 在Cornell, Texas, Washington, Wisconsin, Wiki和VOC数据集上有较好的性能提升.具体来说, SWMVC的<i>ACC</i>和<i>F</i>-meansure指标值比传统的多视图聚类算法TW-Co-<i>K</i>-means在前5个数据上分别提高了约0.03和0.02, 在VOC上取得了次优结果.在<i>NMI</i>指标上, SWMVC在异质数据Texas, Washington, Wisconsin和VOC获得了最高结果, 在Cornell和Wiki以0.208 2和0.538 8取得了第2高结果.究其原因, 我们发现:WebKB的4个数据集由链接结构视图和文本内容视图2种异构视图描述, 能够形成较强的互补关系;Wiki和VOC由图像特征视图和文本特征视图2种异构视图组成, 同样具有较好的互补性, 且文本特征视图解释力强于图像特征描述子.</p>
                </div>
                <div class="p1">
                    <p id="167">② SWMVC在手写数据集HW和图像数据集Caltech101-7表现欠佳, 原因在于尽管这2个数据集的多个视图从各个方面进行了描述, 但是视图之间表现出同质特性, 视图间的信息互补性相对较弱, 且对簇结构的刻画能力在局部样本上的差异性不明显.</p>
                </div>
                <div class="area_img" id="168">
                    <p class="img_tit"><b>表2 不同聚类算法在多视图数据集上的<i>ACC</i>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 <i>ACC</i> of Different Clustering Algorithms on Multi-View Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="168" border="1"><tr><td><br />Dataset</td><td>WMCFS</td><td>MLAN</td><td>AMGL</td><td>TW-Co-<i>K</i>-means</td><td>SWMVC</td></tr><tr><td><br />Cornell</td><td>0.487 2</td><td>0.430 8</td><td>0.466 7</td><td>0.533 3</td><td><b>0.574</b><b>4</b></td></tr><tr><td><br />Texas</td><td>0.615 0</td><td>0.604 3</td><td>0.534 6</td><td>0.636 4</td><td><b>0.673</b><b>8</b></td></tr><tr><td><br />Washington</td><td>0.643 5</td><td>0.656 5</td><td>0.578 3</td><td>0.669 6</td><td><b>0.713</b><b>0</b></td></tr><tr><td><br />Wisconsin</td><td>0.539 6</td><td>0.498 1</td><td>0.524 5</td><td>0.679 2</td><td><b>0.709</b><b>4</b></td></tr><tr><td><br />Wiki</td><td>0.580 6</td><td>0.180 0</td><td>0.546 4</td><td>0.542 9</td><td><b>0.599</b><b>8</b></td></tr><tr><td><br />VOC</td><td>0.674 2</td><td>0.371 2</td><td>0.152 4</td><td><b>0.713</b><b>0</b></td><td>0.680 2</td></tr><tr><td><br />HW</td><td>0.802 5</td><td><b>0.973</b><b>0</b></td><td>0.853 0</td><td>0.896 0</td><td>0.813 5</td></tr><tr><td><br />Caltech101-7</td><td>0.464 0</td><td><b>0.780</b><b>2</b></td><td>0.677 1</td><td>0.536 0</td><td>0.555 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best results are highlighted in bold.</p>
                </div>
                <div class="area_img" id="169">
                    <p class="img_tit"><b>表3 不同聚类算法在多视图数据集上的<i>NMI</i>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 <i>NMI</i> of Different Clustering Algorithms on Multi-View Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="169" border="1"><tr><td><br />Dataset</td><td>WMCFS</td><td>MLAN</td><td>AMGL</td><td>TW-Co-<i>K</i>-means</td><td>SWMVC</td></tr><tr><td><br />Cornell</td><td>0.144 6</td><td><b>0.220</b><b>4</b></td><td>0.062 1</td><td>0.161 3</td><td>0.208 2</td></tr><tr><td><br />Texas</td><td>0.232 0</td><td>0.252 0</td><td>0.044 9</td><td>0.243 2</td><td><b>0.271</b><b>6</b></td></tr><tr><td><br />Washington</td><td>0.345 4</td><td>0.357 1</td><td>0.153 7</td><td>0.2310</td><td><b>0.358</b><b>5</b></td></tr><tr><td><br />Wisconsin</td><td>0.271 8</td><td>0.277 3</td><td>0.093 5</td><td>0.3351</td><td><b>0.387</b><b>6</b></td></tr><tr><td><br />Wiki</td><td><b>0.545</b><b>6</b></td><td>0.039 3</td><td>0.519 3</td><td>0.537 0</td><td>0.538 8</td></tr><tr><td><br />VOC</td><td>0.641 2</td><td>0.257 1</td><td>0.017 2</td><td>0.655 8</td><td><b>0.661</b><b>8</b></td></tr><tr><td><br />HW</td><td>0.795 3</td><td><b>0.939</b><b>0</b></td><td>0.884 3</td><td>0.821 7</td><td>0.795 6</td></tr><tr><td><br />Caltech101-7</td><td>0.453 9</td><td><b>0.630</b><b>4</b></td><td>0.607 5</td><td>0.465 9</td><td>0.442 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best results are highlighted in bold.</p>
                </div>
                <div class="area_img" id="170">
                    <p class="img_tit"><b>表4 不同聚类算法在多视图数据集上的<i>F</i>值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 <i>F</i>-measure of Different Clustering Algorithms on Multi-View Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="170" border="1"><tr><td><br />Dataset</td><td>WMCFS</td><td>MLAN</td><td>AMGL</td><td>TW-Co-<i>K</i>-means</td><td>SWMVC</td></tr><tr><td><br />Cornell</td><td>0.378 9</td><td>0.391 6</td><td>0.440 3</td><td>0.476 1</td><td><b>0.514</b><b>7</b></td></tr><tr><td><br />Texas</td><td>0.535 3</td><td>0.554 0</td><td>0.504 2</td><td>0.595 6</td><td><b>0.614</b><b>6</b></td></tr><tr><td><br />Washington</td><td>0.567 7</td><td>0.594 7</td><td>0.511 0</td><td>0.589 0</td><td><b>0.637</b><b>2</b></td></tr><tr><td><br />Wisconsin</td><td>0.481 8</td><td>0.470 9</td><td>0.479 7</td><td>0.606 8</td><td><b>0.612</b><b>0</b></td></tr><tr><td><br />Wiki</td><td>0.505 6</td><td>0.180 0</td><td>0.420 8</td><td>0.460 4</td><td><b>0.510</b><b>8</b></td></tr><tr><td><br />VOC</td><td>0.600 5</td><td>0.257 1</td><td>0.152 0</td><td><b>0.617</b><b>1</b></td><td>0.606 0</td></tr><tr><td><br />HW</td><td>0.837 0</td><td><b>0.946</b><b>6</b></td><td>0.844 8</td><td>0.822 9</td><td>0.768 0</td></tr><tr><td><br />Caltech101-7</td><td>0.504 3</td><td><b>0.736</b><b>5</b></td><td>0.710 2</td><td>0.551 8</td><td>0.545 2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best results are highlighted in bold.</p>
                </div>
                <div class="area_img" id="172">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908011_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 各个样本在给定视图上的权重" src="Detail/GetImg?filename=images/JFYZ201908011_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 各个样本在给定视图上的权重  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908011_172.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Weights of samples in a given view</p>

                </div>
                <div class="p1">
                    <p id="173">③ 根据实验结果可以发现:MLAN模型在HW和Caltech101-7效果显著.究其原因, MLAN学习了数据的局部流形结构, 多个同质视图上局部流形结构的整合较好地反映出了全局的簇结构信息, 是目前较为突出的模型.</p>
                </div>
                <div class="p1">
                    <p id="174">④ 综合WMCFS和TW-Co-<i>K</i>-means多视图特征算法在上述8个数据集的实验结果可见:良好的特征选择机制可以提高多视图聚类算法的性能.</p>
                </div>
                <h4 class="anchor-tag" id="175" name="175">2) 节点权重分析</h4>
                <div class="p1">
                    <p id="176">为了进一步验证本文提出的SWMVC算法, 我们在图1中展示了该方法在6个异构视图数据集Cornell, Texas, Washington, Wisconsin, Wiki和VOC上学习到的样本权重.其中, 横坐标表示样本编号, 纵坐标表示各样本在WebKB四个数据链接视图上的权重及Wiki, VOC数据集的各样本点在文本视图上的权重.</p>
                </div>
                <div class="p1">
                    <p id="177">由图1可以看出:整体上WebKB四个数据集的样本在链接视图所占权重较大, 即拓扑结构更能刻画数据特性;Wiki和VOC各样本点在文本视图权重大于图像特征视图.同时视图包含重要度样本越多, 说明该视图该视图越重要, 实现了对数据“全局”信息重要性的学习.</p>
                </div>
                <div class="p1">
                    <p id="178">对于同质的多视图数据集HW和Caltech101-7, SWMVC算法学习到的各样本点的权重差异性小.由于这2个数据集视图多, 考虑到空间限制, 只简单描述如下:HW数据集所有样本在FAC, FOU, KAR和ZER视图权重基本分布在0.21左右, 在PIX特征视图权值约为0.16;Caltech101-7所有样本点在前3个视图权重约为0.20, 第4个视图权值约是0.06, 最后2个权值为0.17左右.从而进一步可见, 我们的方法更适于差异性较大的异构互补视图的簇结构学习问题.</p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>3.4 参数敏感性分析</b></h4>
                <div class="p1">
                    <p id="180">SWMVC样本重要度正则项参数<i>λ</i>对数据集有不同程度的影响.具体而言:参数<i>λ</i>对异质数据集的样本在视图权重分布上有明显影响.以Wiki数据集为例, 图2显示了<i>λ</i>为0.5和50时所有样本在文本视图上的权重分布情况.可以发现:<i>λ</i>取值较小时, 文本特征权重相对较大, 聚类结果也较好, 印证了<i>λ</i>可以调节样本权<i>α</i><mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>v</mi></msubsup></mrow></math></mathml>的稀疏性.</p>
                </div>
                <div class="area_img" id="182">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908011_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 参数λ对样本权重分布的影响" src="Detail/GetImg?filename=images/JFYZ201908011_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 参数<i>λ</i>对样本权重分布的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908011_182.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The influence of <i>λ</i> on sample weights</p>

                </div>
                <h3 id="183" name="183" class="anchor-tag"><b>4 总  结</b></h3>
                <div class="p1">
                    <p id="184">本文主要提出了一种基于样本加权的多视图聚类算法 (SWMVC) .为了更好地学习多视图蕴含的信息, 本文受Adapt-SA算法启发, 引进多视图学习思想, 从不同视图角度学习样本对簇结构的贡献度.这种多视图样本加权学习机制不仅体现了样本差异性, 而且还刻画了视图的重要度, 实现了对数据“局部”和“全局”性质的学习.与此同时, SWMVC能够保留数据原始信息, 有助于降低模型复杂度.在6个异质视图数据集和2个同质视图数据集上的大量实验对比表明:本文提出的模型在异质视图数据上具有不错的聚类效果.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="216">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised word sense disambiguation rivaling supervised methods">

                                <b>[1]</b>Yarowsky D.Unsupervised word sense disambiguation rivaling supervised methods[C] //Proc of the 33rd Annual Meeting on Association for Computational Linguistics.Association for Computational Linguistics.Stroudsburg:ACL, 1995:189- 196
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining Labeled and Unlabeled Data with Co-Training">

                                <b>[2]</b>Blum A, Mitchell T.Combining labeled and unlabeled data with co-training[C] //Proc of the 11th Annual Conf on Computational Learning Theory.New York:ACM, 1998:92- 100
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-View clustering">

                                <b>[3]</b>Bickel S, Scheffer T.Multi-view clustering[C] //Proc of the 4th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2004:19- 26
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Co FKM:a centralized method for multiple-view clustering">

                                <b>[4]</b>Cleuziou G, Exbrayat M, Martin L, et al.CoFKM:A centralized method for multiple-view clustering[C] //Proc of the 9th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2009:752- 757
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral Clustering with Two Views">

                                <b>[5]</b>De Sa V R.Spectral clustering with two views[C] //Proc of the Workshop on Learning with Multiple Views.New York:ACM, 2005:20- 27
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A co-training approach for multi-view spectral clustering">

                                <b>[6]</b>Kumar A, Daumé H.A co-training approach for multi-view spectral clustering[C] //Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:393- 400
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Co-regularized multiview spectral clustering">

                                <b>[7]</b>Kumar A, Rai P, Daume H.Co-regularized multi-view spectral clustering[C] //Proc of the 24th Int Conf on Neural Information Processing System.Vancouver:Curran Associates, 2011:1413- 1421
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple view clustering using a weighted combination of exemplar-based mixture models">

                                <b>[8]</b>Tzortzis G F, Likas A C.Multiple view clustering using a weighted combination of exemplar-based mixture models[J].IEEE Transactions on Neural Networks, 2010, 21 (12) :1925- 1938
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Kernel-based weighted multiview clustering">

                                <b>[9]</b>Tzortzis G, Likas A.Kernel-Based Weighted Multi-view Clustering[C] //Proc of the 12th IEEE Int Conf on Data Mining.Los Alamitos:IEEE Communicatoins Society, 2012:675- 684
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view k-means clustering on big data">

                                <b>[10]</b>Cai Xiao, Nie Feiping, Huang Heng.Multi-view K-means clustering on big data[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.San Francisco:Morgan Kaufmann, 2013:2598- 2604
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auto-Weighted Multi-View Learning for Image Clustering and Semi-Supervised Classification">

                                <b>[11]</b>Nie Feiping, Cai Guohao, Li Jing, et al.Auto-weighted multi-view learning for image clustering and semi-supervised classification[J].IEEE Transactions on Image Processing, 2018, 27 (3) :1501- 1511
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view clustering and semi-supervised classification with adaptive neighbours">

                                <b>[12]</b>Nie Feiping, Cai Guohao, Li Xuelong, et al.Multi-View Clustering and Semi-Supervised Classification with Adaptive Neighbours[C] //Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park:AAAI, 2017:2408- 2414
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TW-k-means: Automated Two-level VariableWeighting Clustering Algorithm for Multiview Data">

                                <b>[13]</b>Chen Xiaojun, Xu Xiaofei, Huang Joshuazhexue, et al.TW-K-means:Automated two-level variable weighting clustering algorithm for multiview data[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (4) :932- 944
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCC0E24345C8E7AD09D2EF1E98AF69286&amp;v=MjM1NzBnSGVYdEl1eFlhbmowSVBuNlhwUnBFRDdTZFI3S1pDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzI1dzZrPU5pZk9mY0RMSHFUT3E0eEJZWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Xu Yumeng, Wang Changdong, Lai Jianhuang.Weighted multi-view clustering with feature selection[J].Pattern Recognition, 2016, 53:25- 35
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES07C7273BA8B9DE2CCBF5CDB351FEB2B6&amp;v=Mjk2MjdPZmJPL2JkYk9xSXczRmVOOUJRaE16V1ZnbUVsNE93dVFyeGMwRDhmbVI4aVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzI1dzZrPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>Zhang Guangyu, Wang Changdong, Huang Dong, et al.TW-Co-K-means:two-level weighted collaborative K-means for multi-view clustering[J].Knowledge-Based Systems, 2018, 150:127- 138
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locally weighted Fusion of structural and attribute information in graph clustering">

                                <b>[16]</b>Li Yafang, Jia Caiyan, Kong Xiangnan, et al.Locally weighted Fusion of structural and attribute information in graph clustering[J].IEEE Transactions on Cybernetics, 2017, 49 (1) :247- 260
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Community detection by signaling on complex networks">

                                <b>[17]</b>Hu Yanqing, Li Menghui, Zhang Peng, et al.Community detection by signaling on complex networks[J].Physical Review E, 2008, 78 (1) :139- 143
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908011" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908011&amp;v=MTY3ODdlWmVSckZ5dmdWYnpKTHl2U2RMRzRIOWpNcDQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR29GSXg5RmJtc2NCcEljST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

