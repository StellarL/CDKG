

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127909314181250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201907017%26RESULT%3d1%26SIGN%3dyMIeX%252bI5ssKVe%252b4sEaIe7e7dHo4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907017&amp;v=MjM2NDNTZExHNEg5ak1xSTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnlybVdydkpMeXY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;2 基于MSER和SVM的中文文本提取方法&lt;/b&gt; "><b>2 基于MSER和SVM的中文文本提取方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="&lt;b&gt;2.1 边缘增强的MSER的图像分割&lt;/b&gt;"><b>2.1 边缘增强的MSER的图像分割</b></a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;2.2 先验知识初步过滤&lt;/b&gt;"><b>2.2 先验知识初步过滤</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;2.3 中文聚合方法&lt;/b&gt;"><b>2.3 中文聚合方法</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;2.4 基于支持向量机的文本分类&lt;/b&gt;"><b>2.4 基于支持向量机的文本分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="&lt;b&gt;3 实验与结果分析&lt;/b&gt; "><b>3 实验与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#131" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;3.2 评价标准&lt;/b&gt;"><b>3.2 评价标准</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;3.3 实验结果对比&lt;/b&gt;"><b>3.3 实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#160" data-title="&lt;b&gt;4 总结与展望&lt;/b&gt; "><b>4 总结与展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="图1 算法流程图">图1 算法流程图</a></li>
                                                <li><a href="#54" data-title="图2 MSER与边缘增强的MSER实验结果对比">图2 MSER与边缘增强的MSER实验结果对比</a></li>
                                                <li><a href="#152" data-title="图3 先验知识参数取值比较">图3 先验知识参数取值比较</a></li>
                                                <li><a href="#153" data-title="图4 中文聚合参数&lt;i&gt;λ&lt;/i&gt;取值比较">图4 中文聚合参数<i>λ</i>取值比较</a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;表1 2种中文文本定位分类方法的性能比较&lt;/b&gt;"><b>表1 2种中文文本定位分类方法的性能比较</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表2 5种中文文本定位提取方法的性能比较&lt;/b&gt;"><b>表2 5种中文文本定位提取方法的性能比较</b></a></li>
                                                <li><a href="#159" data-title="图5 算法实验结果展示">图5 算法实验结果展示</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="176">


                                    <a id="bibliography_1" title="G&#243;mez-Torres E, Luj&#225;n-Mora S.An approach of context-aware mobile applications for Internet of things [C] //Proc of the 2nd Int Conf on Information Systems and Computer Science.Piscataway, NJ:IEEE, 2017:41- 48" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An approach of context-aware mobile applications for Internet of things">
                                        <b>[1]</b>
                                        G&#243;mez-Torres E, Luj&#225;n-Mora S.An approach of context-aware mobile applications for Internet of things [C] //Proc of the 2nd Int Conf on Information Systems and Computer Science.Piscataway, NJ:IEEE, 2017:41- 48
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_2" title="Sun Yimin.Research and simulation of large data differentiation classification technology under the Internet of things [C] //Proc of the 3rd Int Conf on Intelligent Transportation, Big Data &amp;amp; Smart City.Piscataway, NJ:IEEE, 2018:191- 194" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research and simulation of large data differentiation classification technology under the Internet of things">
                                        <b>[2]</b>
                                        Sun Yimin.Research and simulation of large data differentiation classification technology under the Internet of things [C] //Proc of the 3rd Int Conf on Intelligent Transportation, Big Data &amp;amp; Smart City.Piscataway, NJ:IEEE, 2018:191- 194
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_3" title="Wiwatcharakoses C, Patanukhom K.MSER based text localization for multi-language using double-threshold scheme [C] //Proc of the 1st Int Conf on Industrial Networks and Intelligent Systems.Piscataway, NJ:IEEE, 2015:62- 71" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MSER based text localization for multi-language using double-threshold scheme">
                                        <b>[3]</b>
                                        Wiwatcharakoses C, Patanukhom K.MSER based text localization for multi-language using double-threshold scheme [C] //Proc of the 1st Int Conf on Industrial Networks and Intelligent Systems.Piscataway, NJ:IEEE, 2015:62- 71
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_4" title="Soni R, Kumar B, Chand S.Text detection and localization in natural scene images using MSER and fast guided filter [C] //Proc of the 4th Int Conf on Image Information Processing.Piscataway, NJ:IEEE, 2017:351- 356" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text detection and localization in natural scene images using MSER and fast guided filter">
                                        <b>[4]</b>
                                        Soni R, Kumar B, Chand S.Text detection and localization in natural scene images using MSER and fast guided filter [C] //Proc of the 4th Int Conf on Image Information Processing.Piscataway, NJ:IEEE, 2017:351- 356
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_5" title="Huang Weilin, Qiao Yu, Tang Xiaoou.Robust scene text detection with convolution neural network induced MSER trees [C] //Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:497- 511" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees">
                                        <b>[5]</b>
                                        Huang Weilin, Qiao Yu, Tang Xiaoou.Robust scene text detection with convolution neural network induced MSER trees [C] //Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:497- 511
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_6" title="G&#243;mez L, Karatzas D.Object proposals for text extraction in the wild [C] //Proc of the 13th Int Conf on Document Analysis and Recognition.Piscataway, NJ:IEEE, 2015:1786- 1812" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object proposals for text extraction in the wild">
                                        <b>[6]</b>
                                        G&#243;mez L, Karatzas D.Object proposals for text extraction in the wild [C] //Proc of the 13th Int Conf on Document Analysis and Recognition.Piscataway, NJ:IEEE, 2015:1786- 1812
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_7" title="Zhou Xinyu, Yao Cong, Wen He, et al.EAST:An efficient and accurate scene text detector [C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2642- 2651" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EAST:An efficient and accurate scene text detector">
                                        <b>[7]</b>
                                        Zhou Xinyu, Yao Cong, Wen He, et al.EAST:An efficient and accurate scene text detector [C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2642- 2651
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_8" title="Minetto R, Thome N, Cord M, et al.Text detection and recognition in urban scenes [C] //Proc of the 2nd IEEE Int Conf on Computer Vision Workshops.Piscataway, NJ:IEEE, 2016:227- 234" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text detection and recognition in urban scenes">
                                        <b>[8]</b>
                                        Minetto R, Thome N, Cord M, et al.Text detection and recognition in urban scenes [C] //Proc of the 2nd IEEE Int Conf on Computer Vision Workshops.Piscataway, NJ:IEEE, 2016:227- 234
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_9" title="Rajan V, Raj S.Text detection and character extraction in natural scene images using fractional poisson model [C] //Proc of the 1st Int Conf on Computing Methodologies and Communication.Piscataway, NJ:IEEE, 2017:1136- 1141" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text detection and character extraction in natural scene images using fractional poisson model">
                                        <b>[9]</b>
                                        Rajan V, Raj S.Text detection and character extraction in natural scene images using fractional poisson model [C] //Proc of the 1st Int Conf on Computing Methodologies and Communication.Piscataway, NJ:IEEE, 2017:1136- 1141
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_10" title="Yao Cong, Bai Xiang, Liu Wenyu, et al.Detecting texts of arbitrary orientations in natural images [C] //Proc of the 25th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:1083- 1090" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting Texts of Arbitrary Orientations in Natural Images">
                                        <b>[10]</b>
                                        Yao Cong, Bai Xiang, Liu Wenyu, et al.Detecting texts of arbitrary orientations in natural images [C] //Proc of the 25th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:1083- 1090
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_11" title="Zhang Weiwei, Tang Guangming, Sun Yifeng, et al.Chinese scene text localization algorithm based on character features [J].Journal of Information Engineering University, 2014, 15 (6) :729- 736 (in Chinese) (张伟伟, 汤光明, 孙怡峰, 等.一种针对汉字特点的场景图像中文文本定位算法[J].信息工程大学学报, 2014, 15 (6) :729- 736) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXGC201406015&amp;v=MjQzMzZxQnRHRnJDVVJMT2VaZVJyRnlybVdydkpQVFhNYmJHNEg5WE1xWTlFWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Zhang Weiwei, Tang Guangming, Sun Yifeng, et al.Chinese scene text localization algorithm based on character features [J].Journal of Information Engineering University, 2014, 15 (6) :729- 736 (in Chinese) (张伟伟, 汤光明, 孙怡峰, 等.一种针对汉字特点的场景图像中文文本定位算法[J].信息工程大学学报, 2014, 15 (6) :729- 736) 
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_12" title="Yu Boran, Wan Hongjie.Chinese text localization in natural scene based on heuristic rules and SVM [J].Electronic Design Engineering, 2016, 24 (24) :161- 164 (in Chinese) (喻勃然, 万洪杰.基于启发式规则和SVM的自然场景中文文本定位[J].电子设计工程, 2016, 24 (24) :161- 164) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GWDZ201624046&amp;v=MzE1NjFqclBkTEc0SDlmT3E0OUJZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXJtV3J2Skk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Yu Boran, Wan Hongjie.Chinese text localization in natural scene based on heuristic rules and SVM [J].Electronic Design Engineering, 2016, 24 (24) :161- 164 (in Chinese) (喻勃然, 万洪杰.基于启发式规则和SVM的自然场景中文文本定位[J].电子设计工程, 2016, 24 (24) :161- 164) 
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_13" title="Matas J, Chum O, Urban M, et al.Robust wide-baseline stereo from maximally stable extremal regions [J].Image &amp;amp; Vision Computing, 2004, 22 (10) :761- 767" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MDMyMDV0RE9yWTlFWis4R0MzUTdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJbDBkYmhNPU5pZk9mYks3SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Matas J, Chum O, Urban M, et al.Robust wide-baseline stereo from maximally stable extremal regions [J].Image &amp;amp; Vision Computing, 2004, 22 (10) :761- 767
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_14" title="Gao Shilin, Ji Lixin, Li Shaomei, et al.Fast scene-text localization algorithm based on MESR&#39;s fitting ellipse [J].Computer Engineering and Design, 2015, 3 (36) :693- 698 (in Chinese) (高士林, 吉立新, 李绍梅, 等.基于MSER拟合椭圆的快速场景文本定位算法[J].计算机工程与设计, 2015, 3 (36) :693- 698) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201503026&amp;v=MjgyMzMzenFxQnRHRnJDVVJMT2VaZVJyRnlybVdydkpOaWZZWkxHNEg5VE1ySTlIWW9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Gao Shilin, Ji Lixin, Li Shaomei, et al.Fast scene-text localization algorithm based on MESR&#39;s fitting ellipse [J].Computer Engineering and Design, 2015, 3 (36) :693- 698 (in Chinese) (高士林, 吉立新, 李绍梅, 等.基于MSER拟合椭圆的快速场景文本定位算法[J].计算机工程与设计, 2015, 3 (36) :693- 698) 
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_15" title="Li Chuang, Ding Xiaoqing, Wu Youshou.An algorithm for text location in images based on histogram features and AdaBoost [J].Journal of Image and Graphics, 2006, 11 (3) :325- 331 (in Chinese) (李闯, 丁晓青, 吴佑寿.一种基于直方图特征和AdaBoost的图像中的文字定位算法[J].中国图像图形学报, 2006, 11 (3) :325- 331) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200603003&amp;v=Mjk0NDVMT2VaZVJyRnlybVdydkpQeXJmYkxHNEh0Zk1ySTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        Li Chuang, Ding Xiaoqing, Wu Youshou.An algorithm for text location in images based on histogram features and AdaBoost [J].Journal of Image and Graphics, 2006, 11 (3) :325- 331 (in Chinese) (李闯, 丁晓青, 吴佑寿.一种基于直方图特征和AdaBoost的图像中的文字定位算法[J].中国图像图形学报, 2006, 11 (3) :325- 331) 
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_16" title="Liu Xiaopei, Lu Zhaoyang, Li Jing.Complex scene text location method based on WTLBP and SVM [J].Journal of Xidian University:Natural Science, 2012, 39 (4) :103- 108 (in Chinese) (刘晓佩, 卢朝阳, 李静.结合WTLBP 特征和SVM的复杂场景文本定位方法[J].西安电子科技大学学报:自然科学版, 2012, 39 (4) :103- 108) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDKD201204020&amp;v=MjY0NzJxNDlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnlybVdydkpQU25BYXJHNEg5UE0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Liu Xiaopei, Lu Zhaoyang, Li Jing.Complex scene text location method based on WTLBP and SVM [J].Journal of Xidian University:Natural Science, 2012, 39 (4) :103- 108 (in Chinese) (刘晓佩, 卢朝阳, 李静.结合WTLBP 特征和SVM的复杂场景文本定位方法[J].西安电子科技大学学报:自然科学版, 2012, 39 (4) :103- 108) 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(07),1525-1533 DOI:10.7544/issn1000-1239.2019.20180543            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于城市监控的自然场景图像的中文文本提取方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%82%96%E7%8F%82&amp;code=21933319&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肖珂</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%88%B4%E8%88%9C&amp;code=36105841&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">戴舜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E4%BA%91%E5%8D%8E&amp;code=36091981&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何云华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E5%88%A9%E6%B0%91&amp;code=32902268&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙利民</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0226398&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方工业大学信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1698670&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院信息工程研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>智慧城市的首要任务是城市场景监控及其信息分析, 场景图像中文本信息的识别是一种直观且高效的场景信息分析手段, 但目前场景图像的中文文本提取由于图像光照和模糊、中文字符结构复杂等因素, 未能达到很好的效果.为解决这一问题, 提出一种边缘增强的最大稳定极值区域 (maximally stable extremal regions, MSER) 检测方法, 可在光照和模糊影响的条件下提取MSER, 通过几何特征约束条件高效地过滤明显的非MSER, 得到高质量的候选MSER.之后使用提出的中心聚合方法对分割成多个MSER的候选中文文本域进行中文的聚合, 使得候选区域成为单个候选的中文文本分量, 再对这些分量进行分析, 并运用机器学习选出正确的中文文本.实验结果表明:该算法能够更有效地提取出自然场景图像中的中文文本.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7%E7%A8%B3%E5%AE%9A%E6%9E%81%E5%80%BC%E5%8C%BA%E5%9F%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大稳定极值区域;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E6%96%87%E8%81%9A%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中文聚合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">支持向量机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%A9%E8%81%94%E7%BD%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">物联网;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Xiao Ke, born in 1980.PhD, professor. Member of IEEE.His main research interests include wireless comm-unications, physical layer security, the Internet of things, and embedded systems. zehan_xiao@163.com;
                                </span>
                                <span>
                                    Dai Shun, born in 1994.Master.His main research interests include graphic processing and the Internet of things.;
                                </span>
                                <span>
                                    *He Yunhua, born in 1987.PhD.Student member of IEEE. His main research interests include security and privacy in cyber-physical systems, bitcoin based incentive mechanism, security and privacy in vehicle ad hoc networks. heyunhua610@163.com;
                                </span>
                                <span>
                                    Sun Limin, born in 1966.PhD, professor, PhD supervisor.Member of IEEE.His main research interest include wireless sensor networks and vehicular ad hoc network.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-02</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2017YFB0802300);</span>
                                <span>国家自然科学基金项目 (61802005);</span>
                                <span>北京市自然科学基金项目 (4184085);</span>
                    </p>
            </div>
                    <h1><b>Chinese Text Extraction Method of Natural Scene Images Based on City Monitoring</b></h1>
                    <h2>
                    <span>Xiao Ke</span>
                    <span>Dai Shun</span>
                    <span>He Yunhua</span>
                    <span>Sun Limin</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Technology, North China University of Technology</span>
                    <span>Institute of Information Engineering, Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Efficient environment monitoring and information analysis in urban scenes has become one of primary tasks of smart cities. In smart cities, the recognition of text information in scene images, especially the extraction of Chinese text in scene images, is an intuitive and efficient method for analyzing scene information. However, the Chinese text extraction of the current scene images fails to achieve good results because of the uneven illumination and blurred images. In addition, the complexity of Chinese character structure is also an important factor affecting the Chinese text extraction. In order to solve this problem, this paper proposes an edge enhanced maximally stable extremal regions (MSER) detection method, which can extract the MSER under the conditions of illumination and blurring influence, and the non-MSER can be efficiently filtered by geometric feature constraints to obtain high quality candidate MSER. Then the proposed central aggregation is used to aggregate the candidate Chinese text field that has been divided into multiple MSER, so that the candidate region becomes a single candidate Chinese text component, and then these components are analyzed, and finally the correct Chinese text is selected by machine learning. Experiments show that the algorithm can extract Chinese text in natural scene images more effectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=maximally%20stable%20extremal%20regions%20(MSER)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">maximally stable extremal regions (MSER) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Chinese%20aggregation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Chinese aggregation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=support%20vector%20machine%20(SVM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">support vector machine (SVM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Internet%20of%20things%20(IoT)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Internet of things (IoT) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-02</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2017YFB0802300);</span>
                                <span>the National Natural Science Foundation of China (61802005);</span>
                                <span>the Beijing Natural Science Foundation (4184085);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="39">目前传感器已被应用于各种环境的实时感知, 感知数据的分析与利用逐渐改变着人们的生活方式, 由此激起了各类物联网<citation id="208" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation> (Internet of things, IoT) 场景应用, 如智慧城市、智能医疗和国防军事等<citation id="209" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.随着城市化的进展, 智慧城市在大数据基础上, 通过物联网将现实城市与数据进行有效融合, 自动和实时地感知现实世界中人与物体的各种状态和变化, 为城市管理和公众提供各种智能化的服务.在智慧城市的推动过程中, 视频图像的检测和识别成为一项关键的任务.视频图像检测和识别是基于内容的视觉媒体, 对图像的颜色、纹理和布局等进行分析和检索, 从中挖掘出规律性的内容, 这样能方便城市电子警察对城市监控和管理.</p>
                </div>
                <div class="p1">
                    <p id="40">针对城市应用环境, 视频图像的检测与识别方案也存在一些问题, 如捕获的照片模糊失真, 无法用于城市管理.电子产品往往暴露在外, 受外界环境影响较大, 采集的图像会受到外界噪声、散射等因素影响导致处理效果不理想.本文针对智慧城市系统架构中图像处理模块, 研究高效的自然场景文本提取算法, 通过高效快速文本提取算法为智慧城市中场景检测和识别功能提供保障.</p>
                </div>
                <div class="p1">
                    <p id="41">现有文本提取方法可以分为两大类:基于滑动窗口的方法和基于连通域的方法<citation id="210" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.1) 基于滑动窗口的方法<citation id="211" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>通常利用固定大小的滑动窗口来搜索图像中的单个候选字符或候选字词, 然后使用机器学习技术来分类和识别文本.尽管这样的方法对于噪声和模糊是鲁棒的, 但是由于搜索空间大使得它们的速度偏慢.2) 基于连通域的方法首先通过使用图像的局部属性 (例如强度、颜色、笔画宽度) 从图像中作为候选文本提取连通域, 然后使用字符或文本行的属性作为特征, 利用统计学或机器学习等来去除非文本连通域.该方法能够实现高鲁棒性和低计算量, 且针对英文文本的检测在文档分析与识别国际会议 (International Conference on Document Analysis and Recognition, ICDAR) 的竞赛中已有了很好表现.但其应用到中文的文本提取, 并不能达到处理英文时的良好效果.这是由于中文的单个字符并不具有英文那样单个连通域的形式, 难以保证候选文本连通域的提取质量.再加上文本提取中的一些公开性问题, 如光照不均和非文本的形状非常类似于文本字符等, 针对中文的文本提取很难达到满意的效果.而已有的针对中文的提取算法在效率和提取能力上仍需提高.</p>
                </div>
                <div class="p1">
                    <p id="42">针对上述问题, 本文提出了一种基于边缘增强的最大稳定极值区域 (maximally stable extremal regions, MSER) 和支持向量机 (support vector machine, SVM) 结合的自然场景中文文本提取算法.首先, 在考虑图像光照和模糊等因素的情况下, 使用基于边缘增强的MSER检测方法, 过滤和聚合候补MSER得到有效的中文文本域;再根据中文文本域的特征使用高效的机器学习算法将类似文本的结构剔除从而保障中文文本提取的准确性.</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="44">近年来, 对于自然场景的文本检测和提取的工作已备受关注, 学者们也提出一些优秀方法值得参考.在基于滑动窗口的一些方法中, Huang等人<citation id="212" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了基于滑动窗口和MSER结合的文本提取方法, MSER可以显著减少扫描的窗口数量, 并增强对低质量文本的检测, 最后使用卷积神经网络 (convolu-tional neural network, CNN) 分类出正确的文本;Gómez等人<citation id="213" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>探讨了对象提议技术在场景文本理解中的适用性, 提出了一种简单的文本特定的选择性搜索策略, 搜索图像中的特定窗口, 并通过凝聚聚类在层次结构中分组, 对每个节点定义可能的语义假设, 根据语义来检测场景图像中文本单词;Zhou等人<citation id="214" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了能够直接预测全图像中任意方位和矩形形状的文字或文字线管道的方法, 通过设计高效的损失函数和神经网络结构, 用单个神经网络消除不必要的中间步骤 (例如候选聚合和单词分割) .这些方法有效地利用滑动窗口的特性, 得到不错的提取效果.</p>
                </div>
                <div class="p1">
                    <p id="45">以连通域为基础的方法中, Minetto等人<citation id="215" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一个结合自下而上和自上而下机制来检测文本框的综合策略, 自下而上的部分是基于连通域分割和分组进行的, 而自上而下的部分是通过基于框描述符的统计学习方法实现的, 该部分主要贡献在于引入一个适用于文本框分析的新描述符——模糊方向梯度直方图, 以此实现场景图像的文本提取;Rajan等人<citation id="216" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了一种基于分数泊松的增强模型来提高拉普拉斯算子图像的质量, 通过图像增强操作以获得目标和背景之间更好的对比度, 增强图像有效避免拉普拉斯算子图像的噪声, 实现了更高精度的文本检测和识别;Yao等人<citation id="217" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一种利用2级分类方案的文本提取方法, 采用笔画宽度变化 (stroke width transform, SWT) , 并根据文本的一些固有属性设计了对文本非常有效的2级分类方案, 再以适度的训练来消除敏感的手动参数调整, 在场景图像的文本提取方面取得了很好的效果.</p>
                </div>
                <div class="p1">
                    <p id="46">以上这些算法虽然具有很好的效果, 但它们的目标都是针对英文.而如果将该类方法用于中文的文本提取, 难以达到他们处理英文时的优越性能, 针对中文的提取方法, 国内学者也做出一些不错的工作.例如张伟伟等人<citation id="218" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>通过剪枝策略对图像存在嵌套关系的连通域进行取舍, 得到候选笔画区域, 利用结构元参数对图像进行动态闭操作, 以消除同一汉字笔画之间的间隙, 得候选汉字区域, 之后利用结构和角点规则过滤掉非汉字区域, 并用颜色规则聚类得到候选文本区域;喻勃然等人<citation id="219" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过最大稳定极值算法提取区域, 对于汉字笔画分离的问题, 用形态学运算进行笔画融合, 再根据汉字的特点, 设计启发式规则过滤非文本区域, 其中通过候选字符区域的椭圆拟合, 引入椭圆的偏心率作为文本判别规则.但由于效率和图像噪声敏感等原因, 这些算法无法满足物联网的环境, 为了将文本提取算法实现在物联网这样的实时性平台上, 本文提出了一种基于MESR和SVM结合的高效中文提取方法.</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>2 基于MSER和SVM的中文文本提取方法</b></h3>
                <div class="p1">
                    <p id="48">在智慧城市概念中, 有效监测和分析城市中各场景信息可加强对城市的管理, 而场景中包含的一些文本信息可以极大地提高场景信息分析的效率.因此, 本文研究针对自然场景下的高效中文文本提取算法, 并解决现有中文文本算法因效率不足而无法应用于城市场景监测的问题.其中算法的流程如图1所示.首先, 提出使用基于边缘增强的MSER检测算法, 提取出图像的MSER;以MSER为单位进行分析, 并使用几何特征的约束, 对所得到的MSER进行过滤;对于过滤后的MSER进行中文聚合, 图像中的中文文本往往会被分割成多个MSER, 使分散的结构形成候选中文文本域;最后根据中文文本的特征, 对文本进行SVM分类, 得到正确的正确文本.</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907017_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法流程图" src="Detail/GetImg?filename=images/JFYZ201907017_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907017_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Algorithm flowchart</p>

                </div>
                <h4 class="anchor-tag" id="50" name="50"><b>2.1 边缘增强的MSER的图像分割</b></h4>
                <div class="p1">
                    <p id="51">本文复现了Matas等人<citation id="220" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出的最大稳定极值区域检测算法, 检测结果如图2所示.由图2可知, 自然场景图像中存在大量的MSER, 这些区域中包含了大量的非文本区域, 需要进一步的过滤.而在某些场景下部分文本区域并没有被判断为MSER, 这将直接影响后续的提取结果.</p>
                </div>
                <div class="p1">
                    <p id="52">由于文本与其背景的灰度对比通常极为重要, 并且可以假定每个文本具有均匀灰度或颜色, 因此MSER是文本区域检测和提取的自然选择.虽然MSER被视为最好的区域检测器之一<citation id="221" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 但由于其对视点、比例和光照变化的鲁棒性, 加上它对模糊图像的敏感, 将MSER直接应用于有限分辨率的图像时, 不能有效地检测或区分某些特殊的场景图像的文本区域.</p>
                </div>
                <div class="p1">
                    <p id="53">针对多个文本被检测为单个MSER区域这类由图像模糊造成的现象, 本文结合Canny边缘检测和MSER的提取特性, 通过Canny边缘来增强极值区域的轮廓, 然后沿着原始灰度图像计算出的梯度方向修剪MSER, 从而移除了由Canny边缘形成的边界外MSER像素.由于文本类型 (亮或暗) 在MSER检测阶段是已知的, 因此可以调整梯度方向以保证它们的指向背景.边缘增强的MSER, 提供了显著改进的文本表示, 其中分开单独的连通区.不仅可以提高几何过滤器的性能, 而且还可以增加在不同图像特殊条件下基于MSER的特征匹配的可重复性, 这种边缘增强的MSER检测算法, 结合边缘和MSER区域的优点, 相比于传统的MSER算法, 不仅能够提高检测算法对复杂场景的应用性, 同时还可以减少背景的干扰, 有利于后续对文本区域鉴别.图2显示了边缘增强的MSER图像分割的良好效果.</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907017_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 MSER与边缘增强的MSER实验结果对比" src="Detail/GetImg?filename=images/JFYZ201907017_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 MSER与边缘增强的MSER实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907017_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Comparison of results between MSER and edge enhanced MSER</p>

                </div>
                <h4 class="anchor-tag" id="55" name="55"><b>2.2 先验知识初步过滤</b></h4>
                <div class="p1">
                    <p id="56">本文基于MSER的提取效果和中文字符的特点制定了一些高效的先验知识, 作为约束条件进行初步的过滤:</p>
                </div>
                <div class="p1">
                    <p id="57">1) 基于长短轴长度比的过滤.由于中文存在偏旁部首, 而偏旁部首不像整个字符那样特征鲜明, 所以适当放宽长轴与短轴的比例约束, 将长短比大于4∶1的MSER过滤掉.值得注意的是, 中文字符有些特殊的偏旁部首, 如“亻”、“一”和“刂”等结构不能满足先前的约束, 为了防止这样的MSER被过滤, 对这些MSER的过滤采用新的约束.经过研究发现, “亻”、“一”和“刂”等结构的共同特征是拟合椭圆方向都接近竖直或者水平方向, 所以当MSER的椭圆拟合方向为水平和竖直时, 长短比大于8∶1的MSER才会过滤.</p>
                </div>
                <div class="p1">
                    <p id="58">2) 基于孔洞数过滤.中文字符中包含孔洞数的范围并不能轻易地约束, 但是此时的MSER只是一个代表中文字符部分的连通区域, 这样的区域通常没有过多的空洞数.在众多中文字符中, 其所包含的单个偏旁部首结构, 孔洞数量一般不超过5个, 即<i>MSER</i>_<i>hole</i><sub><i>i</i></sub>≤5, 所以该约束条件能够把孔洞数大于5的MSER都过滤.</p>
                </div>
                <div class="p1">
                    <p id="59">3) 基于占空比过滤.中文字符的部分结构通常具有一定占空比, 即像素面积与椭圆面积的比例, 正确的结构其占空比通常不会太小也不会过大.因为字符偏旁部首的像素除了某些特殊“亻”和“一”等, 其他都分布得相对松散.而由MSER通过整体形状拟合出椭圆, 其面积必定不会比MSER像素组成面积小.所以将满足占空比小于0.2且大于0.85的MSER过滤掉.</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>2.3 中文聚合方法</b></h4>
                <div class="p1">
                    <p id="61">中文字符不同于英文字符的一笔而就, 它通常是由多个MSER组成, 为了得到正确的中文文本, 在验证之前需要将分散的多个MSER聚合成候补的文本区域.对此, 本文提出了如算法1所示的基于文本中心聚合的方法, 有4个步骤:</p>
                </div>
                <div class="p1">
                    <p id="62">1) 统计MSER属性.得到每个MSER的矩形包围盒信息、质心坐标、平均颜色分量以及平均笔画宽度.由于中文字符被称为“方块字”, 单个中文字符的最佳凸包通常是一个正方形.因此, 矩形包围盒的使用能够更加有效地迎合中文字符的特点.</p>
                </div>
                <div class="p1">
                    <p id="63">2) 约束合并范围.除了“一”等特殊的中文字符, 单个完整的中文字符通常拥有相近的高度和宽度, 并且在场景图像中, 为了方便人们辨识文字, 字体的各个结构会具有相似的笔画宽度和颜色.所以该聚合算法在MSER相互合并之前, 先在二维空间中找出每个MSER能够实现合并的一些对象, 即对每个待处理的MSER只考虑质心在距离约束范围内的MSER作为备选的合并结构, 该距离约束范围是以待处理的MSER的质心为圆心、12倍的平均笔画宽度为半径的圆圈.同时为了避免背景的类似结构误入, 以2个MSER之间平均颜色比值 (颜色分量的比值) 和平均笔画宽度比值小于1.2作为约束.</p>
                </div>
                <div class="p1">
                    <p id="64">3) 初步相交合并.由于中文的特性, 无论是书写还是印刷体, 为了不让汉字的偏旁部首, 被误判成相邻汉字的一部分, 相邻字体之间会有一定距离, 而这个距离会比字体的部首结构之间距离大很多.这种距离的差距对字体的结构合并成一个完整的字体很重要, 因为它有利于将正确的笔画结构归并到字体中, 从而得到完整的中文文本区域.在合并判断时, 本文将所有情况分为2种:相交和相邻.遍历合并范围内的MSER, 首先判断2个MSER的包围盒之间是否相交:</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi><mi>e</mi><mi>c</mi><mi>t</mi><mo>=</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mi>R</mi><mo>_</mo><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>R</mi><mo>_</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mtext>Δ</mtext><mi>h</mi></mrow></mfrac><mo>≥</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>∨</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mi>R</mi><mo>_</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>R</mi><mo>_</mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mtext>Δ</mtext><mi>w</mi></mrow></mfrac><mo>≥</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="67">Δ<i>h</i>=max (|<i>R</i>_<i>b</i><sub><i>i</i></sub>-<i>R</i>_<i>t</i><sub><i>j</i></sub>|, |<i>R</i>_<i>b</i><sub><i>j</i></sub>-<i>R</i>_<i>t</i><sub><i>i</i></sub>|) , (2) </p>
                </div>
                <div class="p1">
                    <p id="68">Δ<i>w</i>=max (|<i>R</i>_<i>r</i><sub><i>i</i></sub>-<i>R</i>_<i>l</i><sub><i>j</i></sub>|, |<i>R</i>_<i>r</i><sub><i>j</i></sub>-<i>R</i>_<i>l</i><sub><i>i</i></sub>|) , (3) </p>
                </div>
                <div class="p1">
                    <p id="69">其中, <i>R</i>_<i>w</i>和<i>R</i>_<i>h</i>表示MSER的宽和高;<i>R</i>_<i>t</i>, <i>R</i>_<i>l</i>, <i>R</i>_<i>b</i>, <i>R</i>_<i>r</i>分别表示连通域包围盒的左上角和右下角的横纵坐标, <i>intersect</i>代表2个MSER是否相交.如果<i>intersect</i>值为真就进行合并操作, 将已经合并的MSER标记.在第1次相交合并时, 被处理的MSER有可能被扩大, 造成与原本未相交的MSER开始出现相交.因此遍历完合并范围内的MSER后, 对未被标记的MSER再次进行相交判断并标记.</p>
                </div>
                <div class="p1">
                    <p id="70">4) 相邻合并.此时如果合并范围内的MSER仍未被完全标记, 则进行相邻合并, 当2个MSER满足:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi>R</mi><mo>_</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>R</mi><mo>_</mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mtext>Δ</mtext><mi>w</mi><mo stretchy="false">|</mo><mo>, </mo></mtd></mtr><mtr><mtd><mo stretchy="false">|</mo><mi>R</mi><mo>_</mo><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>R</mi><mo>_</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mtext>Δ</mtext><mi>h</mi><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mo>&lt;</mo><mfrac><mi>λ</mi><mi>κ</mi></mfrac><mi>Τ</mi><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">max (Δ<i>w</i>, Δ<i>h</i>) &lt;<i>λT</i>, (5) </p>
                </div>
                <div class="p1">
                    <p id="73"><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>S</mi><mi>W</mi><mo stretchy="false"> (</mo><mi>Μ</mi><mi>S</mi><mi>E</mi><mi>R</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>, (6) </p>
                </div>
                <div class="p1">
                    <p id="75">其中, <i>N</i>表示约束范围内连通域的总个数, 经实验证明<i>κ</i>和<i>λ</i>设置为4和10时效果最佳.通过限制合并集合的宽度、高度以及宽高比例, 避免邻近的包含完整字符的MSER被合并.</p>
                </div>
                <div class="p1">
                    <p id="76"><b>算法1</b>. 中文文本中心聚合.</p>
                </div>
                <div class="area_img" id="230">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201907017_23000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="230">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201907017_23001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>2.4 基于支持向量机的文本分类</b></h4>
                <div class="p1">
                    <p id="102">经过中文聚合后, 形成了大量候选中文字符区域, 在中文聚合前, 初步过滤伪MSER仍然会存在许多类似的文本结构, 所以需要经过再次分类.本文选取了一些针对中文字符的特征, 作为SVM的特征向量进行训练与分类.</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">1) 面积比例特征</h4>
                <div class="p1">
                    <p id="104"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>_</mo><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo>=</mo><mfrac><mrow><mi>A</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo stretchy="false">) </mo></mrow><mrow><mi>A</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mi>i</mi><mi>c</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (7) </p>
                </div>
                <div class="p1">
                    <p id="106">其中, <i>Area</i> (<i>CC</i>) 代表候选文本区域面积, <i>Area</i> (<i>Pic</i>) 表示图像面积.</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">2) 长度比例特征</h4>
                <div class="p1">
                    <p id="108"><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>_</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo>=</mo><mfrac><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>w</mi><mo>, </mo><mi>h</mi><mo stretchy="false">) </mo></mrow><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mi>i</mi><mi>c</mi><mi>W</mi><mo>, </mo><mi>Ρ</mi><mi>i</mi><mi>c</mi><mi>Η</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (8) </p>
                </div>
                <div class="p1">
                    <p id="110">其中, <i>w</i>, <i>h</i>分别表示候选文本区域的宽和高, 而<i>PicW</i>和<i>PicH</i>分别表示图像的宽和高.</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">3) 长宽比特征</h4>
                <div class="p1">
                    <p id="112"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>_</mo><mi>a</mi><mi>s</mi><mi>p</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo>=</mo><mrow><mi>max</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mi>w</mi><mi>h</mi></mfrac><mo>, </mo><mfrac><mi>h</mi><mi>w</mi></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>. (9) </p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">4) 边缘对比度特征</h4>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><mo>_</mo><mi>E</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>s</mi><mi>t</mi><mo>=</mo></mtd></mtr><mtr><mtd><mspace width="0.25em" /><mfrac><mrow><mi>B</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mo stretchy="false"> (</mo></mstyle><mi>C</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>y</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>S</mi></mstyle><mi>o</mi><mi>b</mi><mi>e</mi><mi>l</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mi>i</mi><mi>c</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>B</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">其中, <i>Canny</i> (<i>Picture</i>) 和<i>Sobel</i> (<i>Picture</i>) 分别表示图像的归一化Canny和Sobel边缘检测;<i>Border</i> (<i>CC</i>) 表示候选文本区域的边界框包含的像素.</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">5) 形状规则特征</h4>
                <div class="area_img" id="118">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907017_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="120">其中, <i>imfill</i> (<i>CC</i>) 表示填充候选文本区域;<i>open</i> (·) 表示进行开运算;<i>imholes</i> (<i>CC</i>) 表示统计候选文本区域中的孔洞数.</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121">6) 笔画宽度特征</h4>
                <div class="p1">
                    <p id="122"><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>_</mo><mi>s</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>k</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mi>v</mi><mi>a</mi><mi>r</mi><mi>S</mi><mi>W</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo stretchy="false">) </mo></mrow><mrow><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>S</mi><mi>W</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (12) </p>
                </div>
                <div class="p1">
                    <p id="124">其中, <i>varSW</i> (<i>CC</i>) 表示候选文本区域的笔画宽度方差, <i>meanSW</i> (<i>CC</i>) 表示候选文本区域的笔画宽度均值.</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125">7) 空间相干性面积比特征</h4>
                <div class="p1">
                    <p id="126"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>_</mo><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo>_</mo><mi>S</mi><mo>=</mo><mfrac><mrow><mi>A</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>i</mi><mi>m</mi><mi>d</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo stretchy="false"> (</mo><mi>C</mi><mi>C</mi><mo>, </mo><mn>5</mn><mo>×</mo><mn>5</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>A</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mi>i</mi><mi>c</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (13) </p>
                </div>
                <div class="p1">
                    <p id="128">其中, <i>imdilate</i> (·, <i>strel</i>) 代表结构元素<i>strel</i>的形态膨胀操作.</p>
                </div>
                <h3 id="129" name="129" class="anchor-tag"><b>3 实验与结果分析</b></h3>
                <div class="p1">
                    <p id="130">本文算法的实验平台为戴尔台式计算机, 其CPU为Intel core i7的处理器, 运行内存为8 GB, 操作系统为64位的Windows 7系统.</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="132">公开的实验数据集对文本提取的研究责任重大, 当研究人员使用公开数据集进行算法评估时, 算法的性能体现才更具说服力.对于中文文本的提取, 目前没有公开且权威的自然场景图像数据集.虽然有西安电子科技大学建立的中文图像数据集, 却只在校园内研究使用.为了更好地评定本文的研究, 根据ICDAR数据集的图像组成规则, 建立了针对中文文本提取的图像库, 具体建立方法如下:</p>
                </div>
                <div class="p1">
                    <p id="133">1) 数量组成.220幅训练样本的图像和180幅测试图像.</p>
                </div>
                <div class="p1">
                    <p id="134">2) 图像分辨率范围.ICDAR竞赛图像库的图像分辨率范围是860×640至1 600×1 200, 本文采集的图像其分辨率从860×640至2 048×1 536.</p>
                </div>
                <div class="p1">
                    <p id="135">3) 难度比例.根据图像中文本提取的难度, 将图像分为难、中和易3个等级.ICDAR竞赛图像库中图像难度比例约为3∶1∶1, 因此自建的中文图像库也遵循着这一难度比例.</p>
                </div>
                <div class="p1">
                    <p id="136">4) 图像文本内容.ICDAR图像库中文本内容包括路边标志牌文本、服饰标签文本、图书封面文本、车辆车牌号、宣传字画文本、包装袋封皮文本和建筑物名称等, 自建库也同样包含这些内容.</p>
                </div>
                <div class="p1">
                    <p id="137">5) 字符组成.ICDAR图像库的图像中只包含英文文本, 而自建库是针对中文的, 因此主要由大量的中文文本、少量的阿拉伯数字和英文文本组成.</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138"><b>3.2 评价标准</b></h4>
                <div class="p1">
                    <p id="139">在ICDAR比赛出现之后, 学术界对文本检测、提取和识别的评价标准都迎合了ICDAR比赛中使用的评价方法.根据ICDAR评估协议, 算法的性能是通过<i>f</i>值评定的, 它是通过精确率和召回率调和平均值测定的.2个矩形之间的匹配度<i>m</i>被定义为交点面积与包含2个矩形的最小边界矩形的比值.由每种算法估计的矩形集称为估计值, 而在ICDAR数据集中提供的基准矩形集称为目标.对于每个矩形, 找到具有最大值的匹配.因此, 1组矩形<i>R</i>中矩形<i>r</i>的最佳匹配是</p>
                </div>
                <div class="p1">
                    <p id="140"><i>m</i> (<i>r</i>, <i>R</i>) =max{<i>m</i> (<i>r</i>, <i>r</i><sub><i>g</i></sub>) |<i>r</i><sub><i>g</i></sub>∈<i>R</i>}. (14) </p>
                </div>
                <div class="p1">
                    <p id="141">然后, 精确率和召回率的含义是</p>
                </div>
                <div class="p1">
                    <p id="142"><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>e</mi></msub><mo>∈</mo><mi>E</mi></mrow></munder><mi>m</mi></mstyle><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mi>e</mi></msub><mo>, </mo><mi>Τ</mi><mo stretchy="false">) </mo></mrow><mrow><mrow><mo>|</mo><mi>E</mi><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>, (15) </p>
                </div>
                <div class="p1">
                    <p id="144"><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mi>Τ</mi></mrow></munder><mi>m</mi></mstyle><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>E</mi><mo stretchy="false">) </mo></mrow><mrow><mrow><mo>|</mo><mi>Τ</mi><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>, (16) </p>
                </div>
                <div class="p1">
                    <p id="146">其中, <i>E</i>和<i>T</i>分别是目标矩形和估计矩形的集合.<i>f</i>是算法性能的单一度量, 是精确率和召回率的组合指数.提取结果的精确率和召回率的相对权重由1个参数<i>α</i>控制, 其被设置为0.5, 得到相等权重的精确率和召回率:</p>
                </div>
                <div class="p1">
                    <p id="147"><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mfrac><mi>α</mi><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>α</mi></mrow><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mfrac></mrow></math></mathml>. (17) </p>
                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>3.3 实验结果对比</b></h4>
                <div class="p1">
                    <p id="150">本文对图像集中每张图像的平均处理时长为0.86 s, 满足物联网的实时响应要求.在本文算法利用先验知识初步过滤处理过程中, 选取合适的约束条件值进行初步的过滤, 可以为后续的处理过程提供有力支撑.从图3可以看出, 当过滤条件选取不恰当时, 会对召回率的影响巨大, 从而间接地影响了<i>f</i>值.</p>
                </div>
                <div class="p1">
                    <p id="151">中文聚合步骤对于最终中文文本的提取至关重要, 因此我们对其中涉及的关键参数<i>λ</i>的取值进行了实验.图4显示了不同的<i>λ</i>取值对算法性能<i>f</i>的影响, 能够看出:当<i>λ</i>=10时精确率和召回率都达到了峰值, 算法具有最佳的性能;而当<i>λ</i>的取值逐渐增大, 会造成聚合过度, 即将2个独立中文文本被错误地合并成1个字符, 这个错误聚合的文本无法通过SVM的验证, 影响了精确率和召回率;相对地, 当<i>λ</i>的取值逐渐变小, 会逐渐地使得分散的笔画无法被有效地聚合为1个完整中文文本, 同样会影响精确率和召回率, 导致<i>f</i>值偏低.</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907017_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 先验知识参数取值比较" src="Detail/GetImg?filename=images/JFYZ201907017_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 先验知识参数取值比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907017_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Comparison of prior knowledge parameters</p>

                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907017_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 中文聚合参数λ取值比较" src="Detail/GetImg?filename=images/JFYZ201907017_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 中文聚合参数<i>λ</i>取值比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907017_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Comparison of Chinese aggregate parameter <i>λ</i></p>

                </div>
                <div class="p1">
                    <p id="154">SVM分类时本文使用多项式核函数, 由于特征维数低, 样本数远超过特征维数.分类的情况如表1所示, 训练样本为220幅训练图像中提取出候选中文字符区域, 而测试样本为180幅测试图像中提取的所有候选中文字符区域, 可以看出SVM的分类效果明显优于KNN算法.</p>
                </div>
                <div class="area_img" id="155">
                    <p class="img_tit"><b>表1 2种中文文本定位分类方法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Performance Comparison of Several Chinese Text Positioning Classification Methods</b></p>
                    <p class="img_note"></p>
                    <table id="155" border="1"><tr><td>Classification</td><td>Number of<br />Training<br />Samples</td><td>Number of<br />Testing<br />Samples</td><td><i>precision</i></td><td><i>recall</i></td><td><i>f</i></td></tr><tr><td><br />SVM</td><td>5 292</td><td>4 436</td><td>0.85</td><td>0.81</td><td>0.83</td></tr><tr><td><br />KNN</td><td>5 292</td><td>4 436</td><td>0.81</td><td>0.78</td><td>0.79</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="156">将本文的算法与近几年提到的中文文本提取算法进行对比, 结果如表2所示.由表2可知, 本文的算法在自建库上具有较好的提取效果, 相较前人的中文文本提取算法, 由于本文的算法对光照不均图像和模糊图像具有更好的处理能力, 并对自然场景中复杂背景图像具有更稳定的提取效果, 所以精确率和召回率有一定程度的提高.</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表2 5种中文文本定位提取方法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Performance Comparison of Several Chinese Text Positioning Extraction Methods</b></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td><br />Chinese Text Extraction Algorithm</td><td><i>precision</i></td><td><i>recall</i></td><td><i>f</i></td></tr><tr><td><br />Edge-enhanced MSER Extraction Algorithm</td><td>0.85</td><td>0.81</td><td>0.83</td></tr><tr><td><br />Ref [11]</td><td>0.74</td><td>0.71</td><td>0.72</td></tr><tr><td><br />Ref [12]</td><td>0.75</td><td>0.78</td><td>0.76</td></tr><tr><td><br />Ref [15]</td><td>0.72</td><td>0.80</td><td>0.75</td></tr><tr><td><br />Ref [16]</td><td>0.78</td><td>0.81</td><td>0.80</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158">图5所示的为本文算法中文文本提取的效果, 图5 (a) 为原始自然场景图像, 而图5 (b) 为提取后的二值图像.从图5中可以看出, 自然场景图像里的中文文本都被很好地提取出.</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907017_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 算法实验结果展示" src="Detail/GetImg?filename=images/JFYZ201907017_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 算法实验结果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907017_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Algorithm experimental result display</p>

                </div>
                <h3 id="160" name="160" class="anchor-tag"><b>4 总结与展望</b></h3>
                <div class="p1">
                    <p id="161">本文在迎合物联网与图像处理结合的思想上, 针对智慧城市中应用提取文本信息来加速对城市场景的监测, 研究了针对自然场景下的高效中文文本提取算法, 并解决了现有中文文本算法因效率不足而无法应用于城市场景监测的问题.提出了在自然场景图像下基于边缘增强的最大稳定极值区域中文文本提取方法, 首先得到候选MSER, 并使用字符的长短轴和面积、空洞数目等约束条件高效地过滤明显的非MSER, 对候选文本进行初步验证.经过初步过滤后, 运用中心聚合的方法, 使得MSER聚合成各个候选文本区域, 最后通过SVM验证得到文本.通过对算法性能的测试和评估, 结果表明, 本文提出的算法具有较高的精确率和召回率, 解决了现有的在自然场景图像下针对中文文本提取效率不足的问题, 且较少的处理时间也满足了智慧城市架构下对城市场景分析和识别的实效性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="235" type="formula" href="images/JFYZ201907017_23500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">肖珂</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="236" type="formula" href="images/JFYZ201907017_23600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">戴舜</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="237" type="formula" href="images/JFYZ201907017_23700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">何云华</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="238" type="formula" href="images/JFYZ201907017_23800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">孙利民</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="176">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An approach of context-aware mobile applications for Internet of things">

                                <b>[1]</b>Gómez-Torres E, Luján-Mora S.An approach of context-aware mobile applications for Internet of things [C] //Proc of the 2nd Int Conf on Information Systems and Computer Science.Piscataway, NJ:IEEE, 2017:41- 48
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research and simulation of large data differentiation classification technology under the Internet of things">

                                <b>[2]</b>Sun Yimin.Research and simulation of large data differentiation classification technology under the Internet of things [C] //Proc of the 3rd Int Conf on Intelligent Transportation, Big Data &amp; Smart City.Piscataway, NJ:IEEE, 2018:191- 194
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MSER based text localization for multi-language using double-threshold scheme">

                                <b>[3]</b>Wiwatcharakoses C, Patanukhom K.MSER based text localization for multi-language using double-threshold scheme [C] //Proc of the 1st Int Conf on Industrial Networks and Intelligent Systems.Piscataway, NJ:IEEE, 2015:62- 71
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text detection and localization in natural scene images using MSER and fast guided filter">

                                <b>[4]</b>Soni R, Kumar B, Chand S.Text detection and localization in natural scene images using MSER and fast guided filter [C] //Proc of the 4th Int Conf on Image Information Processing.Piscataway, NJ:IEEE, 2017:351- 356
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees">

                                <b>[5]</b>Huang Weilin, Qiao Yu, Tang Xiaoou.Robust scene text detection with convolution neural network induced MSER trees [C] //Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:497- 511
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object proposals for text extraction in the wild">

                                <b>[6]</b>Gómez L, Karatzas D.Object proposals for text extraction in the wild [C] //Proc of the 13th Int Conf on Document Analysis and Recognition.Piscataway, NJ:IEEE, 2015:1786- 1812
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EAST:An efficient and accurate scene text detector">

                                <b>[7]</b>Zhou Xinyu, Yao Cong, Wen He, et al.EAST:An efficient and accurate scene text detector [C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:2642- 2651
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text detection and recognition in urban scenes">

                                <b>[8]</b>Minetto R, Thome N, Cord M, et al.Text detection and recognition in urban scenes [C] //Proc of the 2nd IEEE Int Conf on Computer Vision Workshops.Piscataway, NJ:IEEE, 2016:227- 234
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text detection and character extraction in natural scene images using fractional poisson model">

                                <b>[9]</b>Rajan V, Raj S.Text detection and character extraction in natural scene images using fractional poisson model [C] //Proc of the 1st Int Conf on Computing Methodologies and Communication.Piscataway, NJ:IEEE, 2017:1136- 1141
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting Texts of Arbitrary Orientations in Natural Images">

                                <b>[10]</b>Yao Cong, Bai Xiang, Liu Wenyu, et al.Detecting texts of arbitrary orientations in natural images [C] //Proc of the 25th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:1083- 1090
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXGC201406015&amp;v=MTY0OTBSckZ5cm1XcnZKUFRYTWJiRzRIOVhNcVk5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Zhang Weiwei, Tang Guangming, Sun Yifeng, et al.Chinese scene text localization algorithm based on character features [J].Journal of Information Engineering University, 2014, 15 (6) :729- 736 (in Chinese) (张伟伟, 汤光明, 孙怡峰, 等.一种针对汉字特点的场景图像中文文本定位算法[J].信息工程大学学报, 2014, 15 (6) :729- 736) 
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GWDZ201624046&amp;v=MDY0MTZXcnZKSWpyUGRMRzRIOWZPcTQ5QllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5cm0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Yu Boran, Wan Hongjie.Chinese text localization in natural scene based on heuristic rules and SVM [J].Electronic Design Engineering, 2016, 24 (24) :161- 164 (in Chinese) (喻勃然, 万洪杰.基于启发式规则和SVM的自然场景中文文本定位[J].电子设计工程, 2016, 24 (24) :161- 164) 
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MDQ0OTBmT2ZiSzdIdERPclk5RVorOEdDM1E3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSWwwZGJoTT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Matas J, Chum O, Urban M, et al.Robust wide-baseline stereo from maximally stable extremal regions [J].Image &amp; Vision Computing, 2004, 22 (10) :761- 767
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201503026&amp;v=MjI2NjllUnJGeXJtV3J2Sk5pZllaTEc0SDlUTXJJOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Gao Shilin, Ji Lixin, Li Shaomei, et al.Fast scene-text localization algorithm based on MESR's fitting ellipse [J].Computer Engineering and Design, 2015, 3 (36) :693- 698 (in Chinese) (高士林, 吉立新, 李绍梅, 等.基于MSER拟合椭圆的快速场景文本定位算法[J].计算机工程与设计, 2015, 3 (36) :693- 698) 
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200603003&amp;v=MDI4NDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnlybVdydkpQeXJmYkxHNEh0Zk1ySTlGWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>Li Chuang, Ding Xiaoqing, Wu Youshou.An algorithm for text location in images based on histogram features and AdaBoost [J].Journal of Image and Graphics, 2006, 11 (3) :325- 331 (in Chinese) (李闯, 丁晓青, 吴佑寿.一种基于直方图特征和AdaBoost的图像中的文字定位算法[J].中国图像图形学报, 2006, 11 (3) :325- 331) 
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDKD201204020&amp;v=MjA5MzVtV3J2SlBTbkFhckc0SDlQTXE0OUhaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Liu Xiaopei, Lu Zhaoyang, Li Jing.Complex scene text location method based on WTLBP and SVM [J].Journal of Xidian University:Natural Science, 2012, 39 (4) :103- 108 (in Chinese) (刘晓佩, 卢朝阳, 李静.结合WTLBP 特征和SVM的复杂场景文本定位方法[J].西安电子科技大学学报:自然科学版, 2012, 39 (4) :103- 108) 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201907017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907017&amp;v=MjM2NDNTZExHNEg5ak1xSTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnlybVdydkpMeXY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVQzSmZwMVR5TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

