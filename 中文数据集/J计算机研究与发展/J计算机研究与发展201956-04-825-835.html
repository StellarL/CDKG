

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129044298868750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201904015%26RESULT%3d1%26SIGN%3d%252fk4NOls%252b50h8HwCXuRoBH1a1TRk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201904015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201904015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201904015&amp;v=MTc0OTR6cXFCdEdGckNVUkxPZVplUnFGQ2prVXJ6QUx5dlNkTEc0SDlqTXE0OUVZWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;1 背景知识&lt;/b&gt; "><b>1 背景知识</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="&lt;b&gt;1.1 卷积神经网络&lt;/b&gt;"><b>1.1 卷积神经网络</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;1.2 Winograd算法&lt;/b&gt;"><b>1.2 Winograd算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="&lt;b&gt;2 算法优化&lt;/b&gt; "><b>2 算法优化</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#128" data-title="&lt;b&gt;2.1 虚拟化填充&lt;/b&gt;"><b>2.1 虚拟化填充</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;2.2 分区填充&lt;/b&gt;"><b>2.2 分区填充</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;2.3 填充规模策略&lt;/b&gt;"><b>2.3 填充规模策略</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;2.4 合并策略&lt;/b&gt;"><b>2.4 合并策略</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;2.5 不同尺度Winograd算法混合&lt;/b&gt;"><b>2.5 不同尺度Winograd算法混合</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;2.6 分离转换替换统一转换&lt;/b&gt;"><b>2.6 分离转换替换统一转换</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;2.7 混合统一批处理和分块批处理&lt;/b&gt;"><b>2.7 混合统一批处理和分块批处理</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#163" data-title="&lt;b&gt;3 结果与分析&lt;/b&gt; "><b>3 结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#164" data-title="&lt;b&gt;3.1 Intel KNL&lt;/b&gt;"><b>3.1 Intel KNL</b></a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;3.2 性能测试&lt;/b&gt;"><b>3.2 性能测试</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#205" data-title="&lt;b&gt;4 总结展望&lt;/b&gt; "><b>4 总结展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#120" data-title="图1 Winograd快速卷积设计原理">图1 Winograd快速卷积设计原理</a></li>
                                                <li><a href="#132" data-title="图2 虚拟化填充">图2 虚拟化填充</a></li>
                                                <li><a href="#135" data-title="图3 分区填充">图3 分区填充</a></li>
                                                <li><a href="#141" data-title="图4 填充规模策略">图4 填充规模策略</a></li>
                                                <li><a href="#146" data-title="图5 合并策略">图5 合并策略</a></li>
                                                <li><a href="#167" data-title="图6 Intel KNL处理器架构">图6 Intel KNL处理器架构</a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;表1 Intel KNL 7250配置参数&lt;/b&gt;"><b>表1 Intel KNL 7250配置参数</b></a></li>
                                                <li><a href="#178" data-title="图7 VGG19卷积层前向运行总时间对比">图7 VGG19卷积层前向运行总时间对比</a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表2 不同模式下Winograd性能&lt;/b&gt; ms"><b>表2 不同模式下Winograd性能</b> ms</a></li>
                                                <li><a href="#200" data-title="图8 无填充下的不同卷积性能对比">图8 无填充下的不同卷积性能对比</a></li>
                                                <li><a href="#201" data-title="图9 填充下的不同卷积性能对比">图9 填充下的不同卷积性能对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="227">


                                    <a id="bibliography_1" title="Gu Jiuxiang, Wang Zhenhua, Kuen J, et al.Rencent advance in convolutional neural networks[J].Journal of Pattern Recognition, 2018, 77 (2) :354-377" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCBE0CC12C687BF3E4C199F2E09DF286E&amp;v=MjA2NTVxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExtK3dxaz1OaWZPZmNES2E5Ry8zSTVIRiswSEN3NVB6R01YbVQ1MFFRbmcyUkk4RGNTV1Rieg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Gu Jiuxiang, Wang Zhenhua, Kuen J, et al.Rencent advance in convolutional neural networks[J].Journal of Pattern Recognition, 2018, 77 (2) :354-377
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_2" title="Zhou Feiyan, Jin Linpeng, Dong Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251 (in Chinese) (周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MjY0MjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqa1VyM0pMejdCZHJHNEg5Yk1xWTlGWllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Zhou Feiyan, Jin Linpeng, Dong Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251 (in Chinese) (周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251) 
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_3" title="Wang Jichen, Lin Jun, Wang Zhongfeng.Efficient convolution architectures for convolutional neural network[C]//Proc of the 8th IEEE Wireless Communications&amp;amp;Signal Processing.Piscataway, NJ:IEEE, 2016:1-5" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient convolution architectures for convolutional neural network">
                                        <b>[3]</b>
                                        Wang Jichen, Lin Jun, Wang Zhongfeng.Efficient convolution architectures for convolutional neural network[C]//Proc of the 8th IEEE Wireless Communications&amp;amp;Signal Processing.Piscataway, NJ:IEEE, 2016:1-5
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_4" title="Zhang Yong, Er M J, Pratama M, et al.Extractive document summarization based on convolutional neural networks[C]//Proc of the 42nd Annual Conf of the IEEEIndustrial Electronics Society.Piscataway, NJ:IEEE, 2016:918-922" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extractive document summarization based on convolutional neural networks">
                                        <b>[4]</b>
                                        Zhang Yong, Er M J, Pratama M, et al.Extractive document summarization based on convolutional neural networks[C]//Proc of the 42nd Annual Conf of the IEEEIndustrial Electronics Society.Piscataway, NJ:IEEE, 2016:918-922
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_5" title="Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 26th Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1106-1114" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">
                                        <b>[5]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 26th Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1106-1114
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_6" title="Jaderberg M, Vedaldi A, Zisserman A.Speeding up convolutional neural networks with low rank expansions[C]//Proc of British Machine Vision Conf.Cambridge, UK:BMVA, 2014:073" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speeding up convolutional neural networks with low rank expansions">
                                        <b>[6]</b>
                                        Jaderberg M, Vedaldi A, Zisserman A.Speeding up convolutional neural networks with low rank expansions[C]//Proc of British Machine Vision Conf.Cambridge, UK:BMVA, 2014:073
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_7" title="CoRR.cuDNN:Efficient primitives for deep learning[OL].[2018-01-03].https://arxiv.org/pdf/1410.0759.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=cuDNN:Efficient primitives for deep learning[OL]">
                                        <b>[7]</b>
                                        CoRR.cuDNN:Efficient primitives for deep learning[OL].[2018-01-03].https://arxiv.org/pdf/1410.0759.pdf
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_8" title="Nguyen-Thanh N, Le-Duc H, Ta D T, et al.Energy efficient techniques using FFT for deep convolutional neural networks[C]//Proc of IEEE Advanced Technologies for Communications.Piscataway, NJ:IEEE, 2016:231-236" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy efficient techniques using FFT for deep convolutional neural networks">
                                        <b>[8]</b>
                                        Nguyen-Thanh N, Le-Duc H, Ta D T, et al.Energy efficient techniques using FFT for deep convolutional neural networks[C]//Proc of IEEE Advanced Technologies for Communications.Piscataway, NJ:IEEE, 2016:231-236
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_9" title="CoRR.Fast training of convolutional networks through FFTs[OL].[2017-07-12].https://arxiv.org/pdf/1312.5851.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast training of convolutional networks through FFTs[OL]">
                                        <b>[9]</b>
                                        CoRR.Fast training of convolutional networks through FFTs[OL].[2017-07-12].https://arxiv.org/pdf/1312.5851.pdf
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_10" title="Lavin A, Gray S.Fast algorithms for convolution neural networks[C]//Proc of the 22nd IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:4013-4021" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast algorithms for convolution neural networks">
                                        <b>[10]</b>
                                        Lavin A, Gray S.Fast algorithms for convolution neural networks[C]//Proc of the 22nd IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:4013-4021
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_11" title="St-Charles P L, Biodeau G A, Bergevin R.Fast image gradients using binary feature convolutions[C]//Proc of the22nd IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1074-1081" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast image gradients using binary feature convolutions">
                                        <b>[11]</b>
                                        St-Charles P L, Biodeau G A, Bergevin R.Fast image gradients using binary feature convolutions[C]//Proc of the22nd IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1074-1081
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_12" title="Zhang Xiangyu, Zou Jianhua, Ming Xiang, et al.Efficient and accurate approximations of nonlinear convolutional networks[C]//Proc of the 21st IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1984-1992" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient and Accurate Approximations of Nonlinear Convolutional Networks">
                                        <b>[12]</b>
                                        Zhang Xiangyu, Zou Jianhua, Ming Xiang, et al.Efficient and accurate approximations of nonlinear convolutional networks[C]//Proc of the 21st IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1984-1992
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_13" title="Wang Min, Liu Baoyuan, Foroosh B.Factorized convolutional neural network[C]//Proc of IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:545-553" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Factorized convolutional neural network">
                                        <b>[13]</b>
                                        Wang Min, Liu Baoyuan, Foroosh B.Factorized convolutional neural network[C]//Proc of IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:545-553
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_14" title="Jiang Wenbin, Chen Yiming, Zheng Ran, et al.A novel GPU-based efficient approach for convolutional neural networks[J].Journal of Signal Processing Systems, 2017, 86 (3) , 313-325" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel GPU-based efficient approach for convolutional neural networks with small filters">
                                        <b>[14]</b>
                                        Jiang Wenbin, Chen Yiming, Zheng Ran, et al.A novel GPU-based efficient approach for convolutional neural networks[J].Journal of Signal Processing Systems, 2017, 86 (3) , 313-325
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_15" title="Cong Jason, Xiao Bingjun.Minimizing computation in convolutional neural networks[C]//Proc of the 24th Artificial Neural Networks.Berlin:Springer, 2014:281-290" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Minimizing computation in convolutional neural networks">
                                        <b>[15]</b>
                                        Cong Jason, Xiao Bingjun.Minimizing computation in convolutional neural networks[C]//Proc of the 24th Artificial Neural Networks.Berlin:Springer, 2014:281-290
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_16" title="Selesnick I W, Burrus C S.Extending Winograd’s small convolution algorithm to longer lengths[C]//Proc of IEEEInt Symp on Circuits and Systems.Piscataway, NJ:IEEE, 1994:449-452" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extending Winograd&amp;#39;&amp;#39;s small convolution algorithm to longer lengths">
                                        <b>[16]</b>
                                        Selesnick I W, Burrus C S.Extending Winograd’s small convolution algorithm to longer lengths[C]//Proc of IEEEInt Symp on Circuits and Systems.Piscataway, NJ:IEEE, 1994:449-452
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_17" title="Jeffers J, Reinders J, Sodani A.Intel?Xeon Phi TMProcessor High Performance Programming[M].Burlington:Morgan Kaufmann, 2016:46-50" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intel?Xeon Phi TMProcessor High Performance Programming">
                                        <b>[17]</b>
                                        Jeffers J, Reinders J, Sodani A.Intel?Xeon Phi TMProcessor High Performance Programming[M].Burlington:Morgan Kaufmann, 2016:46-50
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_18" title="Sodani A.Knights landing (KNL) :2nd generation Intel?Xeon Phi TM processor, 16124655[R].Piscataway.NJ:IEEE, 2015, " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Knights landing (KNL):2nd generation Intel?Xeon Phi TM processor,16124655">
                                        <b>[18]</b>
                                        Sodani A.Knights landing (KNL) :2nd generation Intel?Xeon Phi TM processor, 16124655[R].Piscataway.NJ:IEEE, 2015, 
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(04),825-835 DOI:10.7544/issn1000-1239.2019.20170932            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Intel平台的Winograd快速卷积算法研究与优化</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AD%A6%E9%93%AE&amp;code=41491410&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武铮</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%89%E8%99%B9&amp;code=09572060&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安虹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%87%91%E6%97%AD&amp;code=35111441&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">金旭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%BF%9F%E5%AD%9F%E8%B4%A4&amp;code=35111442&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迟孟贤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E5%9B%BD%E9%94%8B&amp;code=41491411&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕国锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%96%87%E5%8F%AF&amp;code=41491412&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文可</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E9%91%AB&amp;code=26283571&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周鑫</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6&amp;code=0002522&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学技术大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着深度学习的快速发展, 其在语音处理、图像识别和自然语言理解等领域被广泛应用, 为科研产业以及日常生活带去了巨大的变革.Intel紧跟深度学习的浪潮, 推出了第2代Xeon Phi处理器KNL (knights landing) , 其后又发布了第3代Xeon Phi处理器KNM (knights mill) , 为深度学习的蓬勃发展带去了新的活力.通过在Intel平台上进行快速卷积算法Winograd的研究与优化, 对比Intel MKL (math kernel library) DNN (deep neural network) 中的卷积性能, 推动Intel MKL DNN中深度神经网络接口的完善以及Intel平台上深度学习的发展.研究中结合Intel最新深度学习平台的AVX-512指令集、高速内存MCDRAM、多Memory/SNC模式、二维网格状内核结构等特性, 并通过对内存分配、数据调度等情况的分析, 设计优化Winograd算法, 一方面选取典型的卷积神经网络 (convolutional neural network, CNN) 网络模型VGG19, 测试对比Intel MKL DNN的卷积实现, 最终取得了2倍多的性能加速比;另一方面, 通过测试常用卷积类型, 对比Intel MKL DNN和NVIDIA cuDNN, 验证了实现的Winograd对于常用卷积类型具有很好的适用性且具有实际使用价值.该研究工作期望为Intel平台在深度学习领域的发展提供重要的指导意义.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Winograd&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Winograd;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%90%91%E9%87%8F%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">向量化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    武铮, zhengwu@mail.ustc.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-04</p>

                    <p>

                            <b>基金：</b>
                                                        <span>科技部国家重点研发计划基金项目 (2016YFB1000403);</span>
                    </p>
            </div>
                    <h1><b>Research and Optimization of Fast Convolution Algorithm Winograd on Intel Platform</b></h1>
                    <h2>
                    <span>Wu Zheng</span>
                    <span>An Hong</span>
                    <span>Jin Xu</span>
                    <span>Chi Mengxian</span>
                    <span>Lü Guofeng</span>
                    <span>Wen Ke</span>
                    <span>Zhou Xin</span>
            </h2>
                    <h2>
                    <span>University of Science and Technology of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the rapid development of deep learning, it's applied extensively for many fields, such as speech processing, image recognition, natural language understanding and so on, bringing great changes for scientific research and daily life. Intel which follows the trend of deep learning launched the second generation of Xeon Phi processor Intel KNL (knights landing) , and released the third generation Intel KNM (knights mill) , which brings new impetus and vitality for the prosperous development of deep learning. This paper mainly contributes to promoting perfect Intel MKL (math kernel library) DNN (deep neural network) , and develops deep learning on Intel platform, according to research and optimization for the fast convolution algorithm Winograd. Combined with characteristics of Intel latest deep learning platform, such as AVX512, high-speed memory MCDRAM, various memory/SNC modes, two-dimensional grid-type cores structure and so on, this work aims to design and optimize the implementation of Winograd algorithm by analyzing memory allocation, data dependency, etc. Finally, on one hand, the typical CNN (convolutional neural network) model VGG19 is used to test and compare performance with Intel MKL convolution, achieving more than doubled acceleration of performance. On the other hand, the common different types of convolutions are used to test and compare performance with Intel MKL DNN and NVIDIA cuDNN, verifying applicability and objective use value about Winograd. The purpose of the paper is to provide important guiding significance for development of Intel platform in the field of deep learning.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Winograd&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Winograd;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20neural%20network%20(DNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep neural network (DNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=vectorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">vectorization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Wu Zheng, born in 1992.PhD candidate. His main research interests include parallel compute system architecture, and machine learning.<image id="283" type="" href="images/JFYZ201904015_28300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    An Hong, born in 1963.PhD, professor, PhD supervisor.Her main research interests include chip multiprocessor architecture, parallel compute system architecture, parallel programming environment and tools, large data parallel storage and processing, and so on. (han@ustc.edu.cn) <image id="285" type="" href="images/JFYZ201904015_28500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Jin Xu, born in 1992.PhD candidate.His main research interests include machine learning and distributed system. (jinxu@ mail.ustc.edu.cn) <image id="287" type="" href="images/JFYZ201904015_28700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Chi Mengxian, born in 1991.PhD candidate. His main research interests include machine learning, parallel computing, and micro-architecture. (mxchi10@ mail.ustc.edu. cn) <image id="289" type="" href="images/JFYZ201904015_28900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    LüGuofeng, born in 1995.Master candidate. His main research interests include machine learning, parallel computing, and micro-architecture. (lvguofeng@mail.ustc.edu. cn) <image id="291" type="" href="images/JFYZ201904015_29100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wen Ke, born in 1991.Master candidate. His  main  research  interests  include machine learning and parallel computing. (wenke51@mail.ustc.edu.cn) <image id="293" type="" href="images/JFYZ201904015_29300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhou Xin, born in 1993.Master candidate. Her main research interests include parallel programming and optimizing, deep neural network. (zxustc@mail.ustc.edu.cn) <image id="295" type="" href="images/JFYZ201904015_29500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-12-04</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Plan Foundation from Ministry of Science and Technology of China (2016YFB1000403);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="46">卷积神经网络 (convolutional neural network, CNN) <citation id="263" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>由于感受野、权值共享和池化等特点, 大大降低了神经网络训练需要的参数数目, 使得更深层神经网络的使用成为可能, 提高了训练的效率以及收敛的效果, 成为了深度学习中最为典型且性能优越的一种网络模型.近年来, CNN更是在多个方向持续发力<citation id="264" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 语音处理、文本提取、图像识别、通用物体识别、运动分析、自然语言理解甚至电脑波分析等方面均具有巨大突破<citation id="270" type="reference"><link href="231" rel="bibliography" /><link href="233" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>.同时CNN也推动着人工智能的快速发展, ImageNet大赛<citation id="265" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、Facebook图像自动标签、Google无人驾驶汽车以及第1个战胜世界围棋冠军的人工智能程序AlphaGo, 这些无一不彰显着智能化时代到来的可能.对于CNN, 其灵魂就是卷积<citation id="266" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 而卷积不仅仅局限于CNN, 在很多其他网络模型中也都有涉及.由此可见, 卷积性能的好坏对于整个深度学习领域有着非凡的影响.对于卷积, 当前主流的算法有GEMM<citation id="267" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, FFT<citation id="271" type="reference"><link href="241" rel="bibliography" /><link href="243" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>, Winograd<citation id="268" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.这3种算法中除了GEMM, 其他2种都是以增加一定的操作复杂度为代价获取更低的计算复杂度, 从而提升卷积的性能, 其中又以Winograd做卷积时计算复杂度最低<citation id="269" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="47">尽管如今对于深度学习的探索和创新层出不穷, 但是这些研究多数是基于NVIDIA GPU平台<citation id="272" type="reference"><link href="247" rel="bibliography" /><link href="249" rel="bibliography" /><link href="251" rel="bibliography" /><link href="253" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>.本文另辟蹊径, 选择Intel平台作为研究环境, 其主要原因是Intel是高性能计算的支柱之一, 但进军深度学习领域时间较短, 因而具有更大的潜力和挖掘空间.以卷积举例, 目前Intel平台仅对于GEMM算法有较高的支持力度.</p>
                </div>
                <div class="p1">
                    <p id="48">本次研究选择Intel第2代Xeon Phi处理器KNL (knigths landing) 7250作为研究平台, 结合该平台特性对Winograd快速卷积进行实现与优化.一方面通过测试VGG19, 对比Intel MKL (math kernel library) DNN (deep neural network) 中卷积性能, 最终取得了2倍多的加速比;另一方面通过测试常用卷积类型, 对比Intel MKL DNN和NVIDIA cuDNN<citation id="273" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 证明了本文的Winograd算法对于常用卷积类型具有很好的适应性且具有实际使用价值.本次研究, 我们并不是要提供一个功能完善的卷积接口, 而是借此让更多的研究者看到Intel平台在深度学习领域的潜力和价值, 为Intel平台在深度学习领域的发展打开一道里程碑式的大门, 也期望能够为深度学习领域的蓬勃发展带去更多的动力.</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>1 背景知识</b></h3>
                <h4 class="anchor-tag" id="50" name="50"><b>1.1 卷积神经网络</b></h4>
                <div class="p1">
                    <p id="51">20世纪60年代, Hubel和Wiesel在研究猫的大脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂度, 继而提出了卷积神经网络.</p>
                </div>
                <div class="p1">
                    <p id="52">CNN相比于其他的神经网络最大的优势在于局部权值共享的特征, 使其在数据量较大的图像识别、语音处理等方面只需要非常少的网络参数, 在保证收敛结果的同时大大降低了网络的运行时间, 提高了网络的运行效率.</p>
                </div>
                <div class="p1">
                    <p id="53">卷积是CNN的核心, 而卷积的过程就是依据局部感受野的原理对输入通过卷积核进行特征提取, 最终得到特征总结后的输出.</p>
                </div>
                <div class="p1">
                    <p id="54">我们将输入记为<i>IN</i><sub><i>n</i>, <i>c</i>, <i>i nh</i>, <i>i nw</i></sub> (1≤<i>n</i>≤<i>N</i>, 1≤<i>c</i>≤<i>C</i>, 1≤<i>inh</i>≤<i>H</i><sub>in</sub>, 1≤<i>inw</i>≤<i>W</i><sub>in</sub>) , 表示<i>N</i>个图片、<i>C</i>个通道以及图片大小为<i>H</i><sub>in</sub>×<i>W</i><sub>in</sub>;卷积核记为<i>FLT</i><sub><i>k</i>, <i>c</i>, <i>r</i>, <i>s</i></sub> (1≤<i>k</i>≤<i>K</i>, 1≤<i>r</i>≤<i>R</i>, 1≤<i>s</i>≤<i>S</i>) , 表示<i>K</i>个特征映射、<i>C</i>个通道以及卷积核大小<i>R</i>×<i>S</i>;输出记为<i>OUT</i><sub><i>n</i>, <i>k</i>, <i>o uth</i>, <i>o utw</i></sub> (1≤<i>outh</i>≤<i>H</i><sub>out</sub>, 1≤<i>outw</i>≤<i>W</i><sub>out</sub>) , 表示<i>N</i>个图片、<i>K</i>个特征映射以及图片大小<i>H</i><sub>out</sub>×<i>W</i><sub>out</sub>.由此, 可将卷积公式记录为<citation id="274" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="55"><i>OUT</i><sub><i>n</i>, <i>k</i>, <i>o uth</i>, <i>o utw</i></sub>=</p>
                </div>
                <div class="p1">
                    <p id="56"><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>IN</i><sub><i>n</i>, <i>c</i>, <i>o uth</i>+<i>r</i>, <i>o utw</i>+<i>s</i></sub><i>FLT</i><sub><i>k</i>, <i>c</i>, <i>r</i>, <i>s</i></sub>. (1) </p>
                </div>
                <div class="p1">
                    <p id="58">我们可将式 (1) 简写为</p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ο</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi mathvariant="bold-italic">Ι</mi></mstyle><mi mathvariant="bold-italic">Ν</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>c</mi></mrow></msub><mo>*</mo><mi mathvariant="bold-italic">F</mi><mi mathvariant="bold-italic">L</mi><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>k</mi></mrow></msub></mrow></math></mathml>, (2) </p>
                </div>
                <div class="p1">
                    <p id="61">其中, *表示卷积操作.</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62"><b>1.2 Winograd算法</b></h4>
                <div class="p1">
                    <p id="63">Winograd算法是基于1980年Winograd的最小滤波算法 (minimal filtering algorithm) 提出的一种快速卷积算法<citation id="275" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 适用于小尺寸的卷积核做卷积, 主要是通过降低卷积的计算复杂度从而提升卷积效率.</p>
                </div>
                <div class="p1">
                    <p id="64">对于mininal filtering algorithm, 以下简称为mini-filter算法.一维数据的情况下, 当输出大小为<i>m</i>, 过滤器大小为<i>r</i>, 可将该算法记为<i>F</i> (<i>m</i>, <i>r</i>) .此时, 算法运算需要<i>μF</i> ( (<i>m</i>, <i>r</i>) ) =<i>m</i>+<i>r</i>-1次乘法操作<citation id="276" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="65">一维的mini-filter算法可以表示成矩阵:</p>
                </div>
                <div class="p1">
                    <p id="66"><b><i>y</i></b>=<b><i>A</i></b><sup>T</sup> ( (<b><i>Gw</i></b>) ⊙ (<b><i>B</i></b><sup>T</sup><b><i>x</i></b>) ) , (3) </p>
                </div>
                <div class="p1">
                    <p id="67">其中, ⊙表示hadamard乘积;<b><i>x</i></b>表示输入, <b><i>w</i></b>表示过滤器, <b><i>y</i></b>表示输出;<b><i>A</i></b><sup>T</sup>, <b><i>G</i></b>, <b><i>B</i></b><sup>T</sup>表示该算法的系数矩阵, 其值由<i>m</i>和<i>r</i>决定, 例如<i>F</i> (2, 3) 有:</p>
                </div>
                <div class="area_img" id="68">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201904015_06800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">G</mi><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="71">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201904015_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="73">从上述计算过程可以看出, 其就是跨度为1的卷积操作.由此, 我们可以把一维的<b>mini-filter</b>看成是一维的卷积操作.通过嵌套一维的<b>mini-filter</b>, 可以得到特殊情况下的二维<b>mini-filter</b>矩阵:</p>
                </div>
                <div class="p1">
                    <p id="74"><b><i>y</i></b>=<b><i>A</i></b><sup>T</sup> ( (<b><i>G</i><i>w</i><i>G</i></b><sup>T</sup>) ⊙ (<b><i>B</i></b><sup>T</sup><b><i>xB</i></b>) ) <b><i>A</i></b>, (4) </p>
                </div>
                <div class="p1">
                    <p id="75">可将此时的mini-filter算法表示成<i>F</i> (<i>m</i>×<i>m</i>, <i>r</i>×<i>r</i>) .</p>
                </div>
                <div class="p1">
                    <p id="76">将上述计算过程应用到CNN中的卷积, 也就是算法1<citation id="277" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.</p>
                </div>
                <div class="area_img" id="296">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201904015_29600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="117" name="117" class="anchor-tag"><b>2 算法优化</b></h3>
                <div class="p1">
                    <p id="118">对于二维数据, Winograd的卷积如式 (4) 所示, 其中<b><i>x</i></b>表示输入块, <b><i>w</i></b>表示卷积核, <b><i>y</i></b>表示输出块, 矩阵<b><i>A</i></b><sup>T</sup>, <b><i>G</i></b>, <b><i>B</i></b><sup>T</sup>则表示转换时的系数矩阵.由此, 本次研究中Winograd设计原理如图1所示.</p>
                </div>
                <div class="p1">
                    <p id="119">假设输入维度为[<i>N</i>, <i>C</i>, <i>H</i><sub>in</sub>, <i>W</i><sub>in</sub>], 卷积核的维度为[<i>K</i>, <i>C</i>, <i>r</i>, <i>r</i>], 填充为0, 跨度为1, 使用Winograd算法<i>F</i> (<i>m</i>×<i>m</i>, <i>r</i>×<i>r</i>) .为了方便描述, 设<i>t</i>=<i>m</i>+<i>r</i>-1.</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Winograd快速卷积设计原理" src="Detail/GetImg?filename=images/JFYZ201904015_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Winograd快速卷积设计原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The priciple of design for Winograd fast convolution</p>

                </div>
                <div class="p1">
                    <p id="121">步骤1. 确定对应系数矩阵<b><i>A</i></b><sup>T</sup>, <b><i>G</i></b>, <b><i>B</i></b><sup>T</sup>, 为了方便后续说明, 我们假设<b><i>w</i></b><sub>Tile</sub>=<b><i>G</i><i>w</i><i>G</i></b><sup>T</sup>, <b><i>x</i></b><sub>Tile</sub>=<b><i>B</i></b><sup>T</sup><b><i>xB</i></b>, 所以Winograd算法变形为<b><i>y</i></b>=<b><i>A</i></b><sup>T</sup> (<b><i>w</i></b><sub>Tile</sub>⊙<b><i>x</i></b><sub>Tile</sub>) <b><i>A</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="122">步骤2. 以<i>t</i>×<i>t</i>为模具对输入进行块映射, 其中2个相邻的块间覆盖范围为 (<i>r</i>-1) × (<i>r</i>-1) .因此, 每个通道上能够分出的块数量为-<i>H</i><sub>out</sub>/<i>m</i>--<i>W</i><sub>out</sub>/<i>m</i>-.对每个块做<b><i>B</i></b><sup>T</sup><b><i>xB</i></b>转换, 并重排转换后的数据成一组矩阵.这组矩阵由<i>t</i><sup>2</sup>个大矩阵组成, 每个大矩阵又由<i>N</i>个<i>C</i>-<i>H</i><sub>out</sub>/<i>m</i>--<i>W</i><sub>out</sub>/<i>m</i>-的列优先的小矩阵组成, 每个小矩阵索引为 (<i>t</i><sub>in</sub>, <i>n</i><sub>in</sub>) (其中1≤<i>t</i><sub>in</sub>≤<i>t</i><sup>2</sup>, 1≤<i>n</i><sub>in</sub>≤<i>N</i>) .</p>
                </div>
                <div class="p1">
                    <p id="124">步骤3. 对每个卷积核做<b><i>G</i><i>w</i><i>G</i></b><sup>T</sup>转换, 并重排转换后的数据成1组矩阵.这组矩阵由<i>t</i><sup>2</sup>个<i>K</i>×<i>C</i>的列优先的小矩阵组成, 每个小矩阵索引为 (<i>t</i><sub>flt</sub>) (其中1≤<i>t</i><sub>flt</sub>≤<i>t</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="125">步骤4. 对步骤2和步骤3中由输入数据和卷积核转换重排得到的索引为 (<i>t</i>, <i>n</i>) 的输入数据小矩阵和索引为 (<i>t</i>) 的卷积核小矩阵做矩阵乘, 最终得到1组输出矩阵.该组矩阵由<i>t</i><sup>2</sup>个大矩阵组成, 每个大矩阵又由<i>N</i>个<i>K</i>-<i>H</i><sub>out</sub>/<i>m</i>--<i>W</i><sub>out</sub>/<i>m</i>-的列优先的小矩阵组成, 每个小矩阵索引为 (<i>t</i><sub>out</sub>, <i>n</i><sub>out</sub>) (其中1≤<i>t</i><sub>out</sub>≤<i>t</i><sup>2</sup>, 1≤<i>n</i><sub>out</sub>≤<i>N</i>) .最后对结果做对应数据<b><i>A</i></b><sup>T</sup><b><i>y</i></b><sub>Tile</sub><b><i>A</i></b>逆转换并重排得到维度为[<i>N</i>, <i>K</i>, <i>H</i><sub>out</sub>, <i>W</i><sub>out</sub>]的输出数据.</p>
                </div>
                <div class="p1">
                    <p id="126">出于计算效率的考虑, 通过对输入和卷积核转换后的数据进行布局重排, 将核心计算⊙转换为矩阵乘, 调用MKL中的矩阵乘接口.</p>
                </div>
                <div class="p1">
                    <p id="127">出于内存重用的考虑, 每层卷积层使用共有的内存空间存储输入、卷积核和输出转换后数据, 这样不仅能够节省大量的内存空间, 同时也有利于提高数据的空间局部性.</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>2.1 虚拟化填充</b></h4>
                <div class="p1">
                    <p id="129">在深度学习中, 卷积层进行卷积时, 大小为1的零填充是十分常见的现象, 因为这样不仅能够保留输入的边缘特征, 同时也能够防止输出大小缩减太快以至于神经网络模型层数不深.</p>
                </div>
                <div class="p1">
                    <p id="130">对于传统零填充, 往往需要开辟出一块新的内存, 再将原始数据和填充数据按对应位置赋值给这段内存.这种方式不仅访存代价高, 而且会消耗大量内存.</p>
                </div>
                <div class="p1">
                    <p id="131">为解决这一问题, 本文提出了新的数据填充方式——虚拟化填充.我们定义2种数据类型:实数据和虚数据, 其中实数据来自原始输入, 虚数据为填充时边缘填充为0的数据.如图2所示, 在模取输入数据时, 假设输入数据已经填充完成, 通过对数据位置的判断, 决定该位置是虚数据置0, 还是实数据对应原始输入数据值.</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 虚拟化填充" src="Detail/GetImg?filename=images/JFYZ201904015_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 虚拟化填充  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Virtualized padding</p>

                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>2.2 分区填充</b></h4>
                <div class="p1">
                    <p id="134">虚拟化填充解决了额外内存开销和填充后数据访存开销的问题, 但是对于Winograd卷积中输入的转换部分, 单纯的虚拟化填充需要在最内层循环中使用大量的分支判断语句来判定该位置是属于实数据还是虚数据, 大大破坏了程序的连贯性.同时, 分支预测失败会引起流水线空泡, 降低程序的IPC (instructions per cycle) , 影响程序的性能.</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 分区填充" src="Detail/GetImg?filename=images/JFYZ201904015_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 分区填充  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Padding partition</p>

                </div>
                <div class="p1">
                    <p id="136">基于上述情况, 提出了分区填充, 旨在消除虚拟化填充中分支判断语句的存在, 具体操作如图3所示.</p>
                </div>
                <div class="p1">
                    <p id="137">将输入数据划分为3个区域——块中所有数据都是实数据组成的区域、块中整数行或列是虚数据的区域、块中整数行且列都是虚数据的区域.通过对输入数据进行分区, 可以消除分支判断语句, 从而避免分支预测失败对程序整体连贯性的影响, 提高指令级并行度, 保证程序的执行效率.</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138"><b>2.3 填充规模策略</b></h4>
                <div class="p1">
                    <p id="139">虚拟化填充结合分区填充的方式针对大规模的数据填充起到了非常好的效果.但当数据规模较小时, 由于内存空间的需求不是很高, 访存时间比较少, 可以尝试使用传统填充方法.虽然会造成一定的内存浪费和访存开销, 但是会大大降低操作的复杂度, 反而更为有利.</p>
                </div>
                <div class="p1">
                    <p id="140">为了更进一步提升传统填充的效率, 可以从减少频繁的内存空间分配和释放入手.如图4所示, 在输入数据转换前分配一块内存空间, 大小为<i>nt</i> (<i>H</i>+2<i>ph</i>) (<i>W</i>+2<i>pw</i>) , 并将其通过线程索引进行分块绑定.对于某个线程, 用于存储转换数据的内存空间大小为<i>t</i> (<i>H</i>+2<i>ph</i>) (<i>W</i>+2<i>pw</i>) , 起始地址为<i>initAddr</i>+<i>tIndex</i> (<i>H</i>+2<i>ph</i>) (<i>W</i>+2<i>pw</i>) (其中, <i>nt</i>表示线程数, <i>ph</i>和<i>pw</i>表示横纵向的填充大小, <i>initAddr</i>表示起始地址, <i>tIndex</i>表示线程索引) .</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 填充规模策略" src="Detail/GetImg?filename=images/JFYZ201904015_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 填充规模策略  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 The strategy for the scale of padding</p>

                </div>
                <div class="p1">
                    <p id="142">通过引入填充规模策略, 针对不同规模的输入数据采用不同的填充方式:大规模数据采用虚拟化填充加分区填充, 小规模数据采用优化后的传统填充, 从而尽可能实现不同数据规模下的填充最优化.</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143"><b>2.4 合并策略</b></h4>
                <div class="p1">
                    <p id="144">根据图1中Winograd快速卷积原理, 在进行核心计算时, 其本质是做<i>t</i><sup>2</sup>×<i>N</i>个小矩阵乘, 其中每个卷积核的小矩阵会同<i>N</i>个输入数的小矩阵相乘, 因此可以考虑将这<i>N</i>个输入数据小矩阵合并到一起变成一个大矩阵再同卷积核的小矩阵做矩阵乘, 从而增加矩阵乘的规模, 提升矩阵运算的效率.</p>
                </div>
                <div class="p1">
                    <p id="145">因为在做核心计算时的矩阵乘是并行执行的, 一个线程负责数个矩阵乘.因此如图5所示, 在做合并时, 可以选择不同尺度的合并.以<i>Merge</i>值为合并尺度, 从而将<i>N</i>个输入小矩阵合并为<i>N</i>/<i>Merge</i>个<i>C</i>-<i>H</i><sub>out</sub>/<i>m</i>--<i>W</i><sub>out</sub>/<i>m</i>-<i>Merge</i>的大矩阵, 同对应的卷积核小矩阵做矩阵乘.一方面兼顾了并行时的多线程粒度, 另一方面也兼顾了MKL矩阵乘接口的性能.</p>
                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 合并策略" src="Detail/GetImg?filename=images/JFYZ201904015_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 合并策略  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Merge strategy</p>

                </div>
                <h4 class="anchor-tag" id="147" name="147"><b>2.5 不同尺度Winograd算法混合</b></h4>
                <div class="p1">
                    <p id="148">Winograd快速卷积算法在某种意义上可以看成是传统卷积的扩展, 不同点在于传统卷积在计算Output数据时是一次1个元素 (对于二维数据, 可以看成1×1的块) , 而Winograd则是一次<i>n</i>个元素 (其中<i>n</i>&gt;1, 同样对于二维数据, 可以看成<i>n</i>×<i>n</i>的块) .</p>
                </div>
                <div class="p1">
                    <p id="149">通过<i>F</i> (2×2, 3×3) 同MKL卷积的比较中我们知道, 两者对于不同规模的卷积各有优势.由此, 不难猜想通过扩展Winograd算法的尺度, 可以更好地适应不同规模的卷积 (对于Winograd算法<i>F</i> (<i>m</i>×<i>m</i>, <i>r</i>×<i>r</i>) , <i>m</i>值的大小决定着算法的尺度) .</p>
                </div>
                <div class="p1">
                    <p id="150">本次研究中使用3种不同尺度的Winograd算法, 分别为<i>F</i> (2×2, 3×3) , <i>F</i> (3×3, 3×3) , <i>F</i> (4×4, 3×3) .在混合使用时, 可以分为结构级混合和数据级混合, 前者是指当前卷积层选用3种Winograd算法中的一种做卷积, 后者是指当前卷积层的卷积分成多个部分, 每个部分分别使用3种Winograd算法中的一种.</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>2.6 分离转换替换统一转换</b></h4>
                <div class="p1">
                    <p id="152">Winograd算法中, 对于输入、卷积核和输出的转换可以表述为</p>
                </div>
                <div class="p1">
                    <p id="153" class="code-formula">
                        <mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>i</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">B</mi><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>i</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">G</mi><mtext> </mtext><mi mathvariant="bold-italic">w</mi><mtext> </mtext><mi mathvariant="bold-italic">G</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mrow><mtext>Τ</mtext><mtext>i</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mi mathvariant="bold-italic">A</mi><mo>, </mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="154">其中, <b><i>A</i></b><sup>T</sup>, <b><i>G</i></b>, <b><i>B</i></b><sup>T</sup>为Winograd算法的转换系数, 是由<i>F</i> (<i>m</i>×<i>m</i>, <i>r</i>×<i>r</i>) 确定的已知数组.因此, 在进行数据转换时, 可以分为2种转换方式:分离转换和统一转换.</p>
                </div>
                <div class="p1">
                    <p id="155">以输入数据为例说明, 统一转换是将Winograd算法<b><i>B</i></b><sup>T</sup><b><i>xB</i></b>转换过程中的2次矩阵乘合并到一起, 以输入模取的数据作为未知量<b><i>x</i></b>, 由此获取转换后的数据<b><i>x</i></b><sub>Tile</sub>关于<b><i>x</i></b>的线性关系.</p>
                </div>
                <div class="p1">
                    <p id="156">以输入数据为例说明, 分离转换是将Winograd算法转换过程中的2次矩阵乘分开进行, 以输入模取的数据为未知量<b><i>x</i></b>, 由此获取第1次矩阵运算后得到的桥数据<b><i>b</i></b>关于<b><i>x</i></b>的线性关系;再以<b><i>b</i></b>为未知量, 得到转换后的数据<b><i>x</i></b><sub>Tile</sub>关于<b><i>b</i></b>的线性关系.</p>
                </div>
                <div class="p1">
                    <p id="157">对于统一转换, 计算量少, 但是所有的线性关系呈横向锯齿形, 杂乱无规律, 难以优化.对于分离转换, 计算量较多, 但所有的线性关系呈平面方形, 平整有序, 非常有利于向量化操作.</p>
                </div>
                <div class="p1">
                    <p id="158">本次研究中, 将前期的统一转换用分离转换替换, 可以大大提升程序的性能.</p>
                </div>
                <h4 class="anchor-tag" id="159" name="159"><b>2.7 混合统一批处理和分块批处理</b></h4>
                <div class="p1">
                    <p id="160">随着batch的变大, 处理的数据变得越来越多, 使用到的内存也变得越来越大.当该层实际使用到的内存超出MCDRAM大小16 GB时, 程序性能将会急速下降.</p>
                </div>
                <div class="p1">
                    <p id="161">因此, 对于大batch的卷积层, 可以将batch分块处理, 从而保证程序的性能.依据此, 可以将batch的处理分为2种方式:统一批处理和分块批处理, 前者是一次性处理所有batch, 后者是将batch分块再处理.</p>
                </div>
                <div class="p1">
                    <p id="162">分块的性能并不总是最优的, 对于小batch, 在进行Winograd卷积时, 实际使用到的内存也会很小, 此时如果仍然使用分块批处理, 反而会因为无法发挥MCDRAM的优势以及多线程的效率, 导致程序性能下降.因此, Winograd算法卷积实际使用到的内存在不超过MCDRAM大小时, 选用统一批处理;实际使用到的内存超过MCDRAM大小时, 选用分块批处理.</p>
                </div>
                <h3 id="163" name="163" class="anchor-tag"><b>3 结果与分析</b></h3>
                <h4 class="anchor-tag" id="164" name="164"><b>3.1 Intel KNL</b></h4>
                <div class="p1">
                    <p id="165">Intel KNL是Intel第2代MIC架构Xeon Phi至强融合处理器<citation id="278" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 是Intel首款专门针对高度并行工作负载而设计的可独立自启动的处理器, 可继续做协处理器, 也可独立做中央处理器.</p>
                </div>
                <div class="p1">
                    <p id="166">如图6所示, Intel KNL处理器架构衍生于Intel Atom处理器, 采用Silvermont架构的改进定制版和14 nm工艺, 由38个Tile组成 (最多有36个Tile处于活动状态) , 核心数量可达72个, 最大支持线程数为288, 双精度浮点性能超3 TFLOPS, 单精度浮点性能超6 TFLOPS.每个Tile包含2个Core, 每个Core拥有2个向量化处理单元VPU, 相同Tile里面的2个Core共享1块大小为1 MB的L2缓存.</p>
                </div>
                <div class="area_img" id="167">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Intel KNL处理器架构[18]" src="Detail/GetImg?filename=images/JFYZ201904015_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Intel KNL处理器架构<citation id="279" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation><sup></sup>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The structure of Intel KNL processor<citation id="280" type="reference"><link href="261" rel="bibliography" /><sup>[18]</sup></citation><sup></sup></p>

                </div>
                <div class="p1">
                    <p id="168">Intel KNL相比于第1代Xeon Phi至强融合处理器, 为了能够获取更高的性能, 在架构方面有着非常显著的改变.Intel KNL的核布局不再是传统的环形布局, 而变成了二维网格状布局, 可以说是从线的布局方式提升到了面的布局方式, 在进行核与核之间交互时具备了更高的效率;内存分成2部分:1) 2个3通道的DDR4, 每个通道可承担64 GB DIMMs, 总容量可达384 GB;2) 8个2 GB的MCDRAM设备, 共16 GB.其中DDR4的访存带宽是90 GBps, MCDRAM带宽远超DDR4, 可达450 GBps;MCDRAM既可做内存也可做缓存, 据此, 在启动设置时可以有3种模式:1) 当MCDRAM做内存使用时, 为Flat模式;2) 当MCDRAM做缓存使用时, 为Cache模式;3) 当MCDRAM一部分做内存使用且一部分做缓存使用时, 为Hybrid模式;单节点KNL可模拟成多节点, 称之为SNC模式, 分别是No SNC (All2All, Heimisphere, Quadrant) , SNC-2, SNC-4;支持AVX-512指令集, 向量化操作更加完善.</p>
                </div>
                <div class="p1">
                    <p id="169">本次研究中使用的是Intel KNL 7250, 配置如表1所示:</p>
                </div>
                <div class="area_img" id="170">
                    <p class="img_tit"><b>表1 Intel KNL 7250配置参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Configure Parameters for Intel KNL 7250</b></p>
                    <p class="img_note"></p>
                    <table id="170" border="1"><tr><td><br />Parameter</td><td>Value</td></tr><tr><td><br />CPU</td><td>Intel<sup>®</sup> Xeon Phi<sup>TM</sup> Processor 7250</td></tr><tr><td><br />CPU Frequency/GHz</td><td>1.40</td></tr><tr><td><br />Cores</td><td>68</td></tr><tr><td><br />Threads/Core</td><td>4</td></tr><tr><td><br />L1d Cache/KB</td><td>32</td></tr><tr><td><br />L1i Cache/KB</td><td>32</td></tr><tr><td><br />L2 Cache/KB</td><td>1 024</td></tr><tr><td><br />MCDRAM/GB</td><td>16</td></tr><tr><td><br />DDR4/GB</td><td>98</td></tr><tr><td><br />SP/TFLOPS</td><td>3+</td></tr><tr><td><br />DP/TFLOPS</td><td>6+</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="171" name="171"><b>3.2 性能测试</b></h4>
                <div class="p1">
                    <p id="172">为了更好地验证本文Winograd的实现效果, 故将实验过程分为3个阶段:</p>
                </div>
                <div class="p1">
                    <p id="173">1) 选择VGG19作为测试网络, 验证Winograd各级优化方案累加效果.</p>
                </div>
                <div class="p1">
                    <p id="174">2) 同样选用VGG19作为测试网络, 测试Intel KNL不同SNC模式和Memory模式对本文Winograd卷积性能的影响.</p>
                </div>
                <div class="p1">
                    <p id="175">3) 抽取典型卷积网络模型AlexNet, GoogleNet, ResNet, VGG11, VGG16, VGG19中卷积层的卷积参数, 测试本文的Winograd, Intel MKL DNN, NVIDIA cuDNN三者的卷积性能, 对比验证本文Winograd对于常用卷积类型的适用性以及是否具有实际使用价值.</p>
                </div>
                <h4 class="anchor-tag" id="176" name="176">3.2.1 各级优化累加</h4>
                <div class="p1">
                    <p id="177">通过测试VGG19中卷积在Winograd算法各级优化累加下的运行时间, 对比Intel MKL DNN, 如图7所示.其中, 柱状图显示的是运行的总时间大小, 折线图显示的是Winograd的加速比.</p>
                </div>
                <div class="area_img" id="178">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 VGG19卷积层前向运行总时间对比" src="Detail/GetImg?filename=images/JFYZ201904015_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 VGG19卷积层前向运行总时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Comparison of the whole running time about  VGG19 forward</p>

                </div>
                <div class="p1">
                    <p id="179">由图7可以看出:虚拟化填充在避免传统填充方式额外内存开销和访存弊端的同时引入大量分支判断, 严重破坏了程序的连贯性, 使得程序的整体性能很差, 加速比仅有0.45;分区填充避免了填充虚拟化造成的大量分支判断, 大大提升了程序的性能, 使得加速比达到0.91;填充规模策略考虑小规模卷积使用优化后的传统填充方式, 此时额外的内存开销和访存不会对程序性能造成太大的影响, 通过该策略加速比达到0.92;合并策略通过配置最佳的合并尺度, 充分发挥了矩阵乘的性能, 加速比达到1.15;不同尺度Winograd算法对于不同规模的卷积具有更好的适应性, 通过混合<i>F</i> (3×3, 3×3) , 加速比达到1.34.通过混合<i>F</i> (4×4, 3×3) , 加速比达到1.41;分离转换将数据转换分成2部分, 使得转换过程具有工整对齐的计算公式, 向量化得以充分发挥, 加速比达到2.01;理论上, 如果将Winograd快速卷积同Intel MKL DNN混合使用, 最佳加速比可以达到2.11.</p>
                </div>
                <h4 class="anchor-tag" id="180" name="180">3.2.2 混合模式</h4>
                <div class="p1">
                    <p id="181">Intel KNL新的架构特征, 在很大程度上提升了访存的性能.同时, 为了进一步提高核与内存、核与核之间的交互效率, 加入了Memory模式和SNC模式<citation id="281" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="182">Memory模式又分为Flat, Cache, Hybrid, 取决于MCDRAM在处理器中的角色 (memory还是cache) .SNC模式又分为No SNC (All2All, Heimi-sphere, Quadrant) , SNC-2, SNC-4, 取决于KNL处理器NUMA后的节点数.</p>
                </div>
                <div class="p1">
                    <p id="183">通过混合2种模式, 我们期望找出Winograd算法的最适情况.如表2所示:</p>
                </div>
                <div class="area_img" id="184">
                    <p class="img_tit"><b>表2 不同模式下Winograd性能</b> ms <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 The Performance of Winograd with Different Modes</b></p>
                    <p class="img_note"></p>
                    <table id="184" border="1"><tr><td rowspan="2"><br />Memory<br />Mode</td><td colspan="5"><br />SNC Mode</td></tr><tr><td>Quadrant</td><td>All2All</td><td>Heimisphere</td><td>SNC-2</td><td>SNC-4</td></tr><tr><td>Flat</td><td>526.82</td><td>533.89</td><td>529.87</td><td>644.38</td><td>1 835.36</td></tr><tr><td><br />Cache</td><td>526.91</td><td>533.01</td><td>529.57</td><td>557.24</td><td>616.69</td></tr><tr><td><br />Hybrid</td><td>531.34</td><td>539.14</td><td>534.39</td><td>642.27</td><td>1 814.27</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="185">SNC-2以及SNC-4模式下, 程序性能均有很明显的下降, 其他模式下性能略有差距.对于本文的Winograd算法, Intel KNL的最适SNC模式是Quadrant, 对于Memory模式, Flat和Cache差距极小, 考虑到误差的可能, 可以认为两者皆为最佳模式.</p>
                </div>
                <h4 class="anchor-tag" id="186" name="186">3.2.3 常用卷积类型</h4>
                <div class="p1">
                    <p id="187">为了进一步验证本文的Winograd算法对于常用卷积是否具有实际意义, 我们抽取典型卷积网络模型AlexNet, GoogleNet, ResNet, VGG11, VGG16, VGG19中卷积层的卷积参数, 测试对比本文Winograd, Intel MKL DNN, NVIDIA cuDNN的卷积性能.</p>
                </div>
                <div class="p1">
                    <p id="188">在GPU的选择上, 我们选取的是Intel KNL的同代GPU Tesla M40.因此在对比时要综合考虑硬件平台本身性能的差距, 其中KNL7250的单精浮点性能是6.1 TFLOPS, Tesla M40的单精浮点性能是7 TFLOPS, 所以时间计算为</p>
                </div>
                <div class="p1">
                    <p id="189" class="code-formula">
                        <mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>l</mtext><mspace width="0.25em" /><mtext>Μ</mtext><mtext>Κ</mtext><mtext>L</mtext><mspace width="0.25em" /><mtext>D</mtext><mtext>Ν</mtext><mtext>Ν</mtext></mrow></msub><mo>=</mo><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>Ι</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>l</mtext><mspace width="0.25em" /><mtext>Μ</mtext><mtext>Κ</mtext><mtext>L</mtext><mspace width="0.25em" /><mtext>D</mtext><mtext>Ν</mtext><mtext>Ν</mtext></mrow></msub><mo>×</mo><mfrac><mrow><mn>6</mn><mo>.</mo><mn>1</mn></mrow><mn>7</mn></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>Ν</mtext><mtext>V</mtext><mtext>Ι</mtext><mtext>D</mtext><mtext>Ι</mtext><mtext>A</mtext><mspace width="0.25em" /><mtext>c</mtext><mtext>u</mtext><mtext>D</mtext><mtext>Ν</mtext><mtext>Ν</mtext></mrow></msub><mo>=</mo><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>Ν</mtext><mtext>V</mtext><mtext>Ι</mtext><mtext>D</mtext><mtext>Ι</mtext><mtext>A</mtext><mspace width="0.25em" /><mtext>c</mtext><mtext>u</mtext><mtext>D</mtext><mtext>Ν</mtext><mtext>Ν</mtext></mrow></msub><mo>, </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="190"><mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>W</mtext><mtext>i</mtext><mtext>n</mtext><mtext>o</mtext><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>W</mtext><mtext>i</mtext><mtext>n</mtext><mtext>o</mtext><mtext>g</mtext><mtext>r</mtext><mtext>a</mtext><mtext>d</mtext></mrow></msub><mo>×</mo><mfrac><mrow><mn>6</mn><mo>.</mo><mn>1</mn></mrow><mn>7</mn></mfrac></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="192">考虑不同卷积运行时间差距太大, 可以将Intel MKL DNN的卷积时间作为标准, 从而直接对比三者间的性能比</p>
                </div>
                <div class="p1">
                    <p id="193"><i>Performance</i><sub>Intel MKL DNN</sub>=</p>
                </div>
                <div class="p1">
                    <p id="194"><i>Time</i><sub>Intel MKL DNN</sub>/<i>Time</i><sub>Intel MKL DNN</sub>, </p>
                </div>
                <div class="p1">
                    <p id="195"><i>Performance</i><sub>NVIDIA cuDNN</sub>=</p>
                </div>
                <div class="p1">
                    <p id="196"><i>Time</i><sub>Intel MKL DNN</sub>/<i>Time</i><sub>NVIDIA cuDNN</sub>, </p>
                </div>
                <div class="p1">
                    <p id="197"><i>Performance</i><sub>Winograd</sub>=</p>
                </div>
                <div class="p1">
                    <p id="198"><i>Time</i><sub>Intel MKL DNN</sub>/<i>Time</i><sub>Winograd</sub>.</p>
                </div>
                <div class="p1">
                    <p id="199">我们总共抽取了46种不同规模的卷积, 并按照计算量从小到大排列, 分别就无填充和填充2种情况做测试, 结果如图8和图9所示:</p>
                </div>
                <div class="area_img" id="200">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 无填充下的不同卷积性能对比" src="Detail/GetImg?filename=images/JFYZ201904015_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 无填充下的不同卷积性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Different convolution performance comparison with no pad</p>

                </div>
                <div class="area_img" id="201">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904015_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 填充下的不同卷积性能对比" src="Detail/GetImg?filename=images/JFYZ201904015_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 填充下的不同卷积性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904015_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Different convolution performance comparison with pad</p>

                </div>
                <div class="p1">
                    <p id="202">从总体卷积来看, 无填充和填充情况下, 本文的Winograd算法相比Intel MKL DNN有61%和60%的性能提升, 相比NVIDIA cuDNN也有13%和8%的性能提升.</p>
                </div>
                <div class="p1">
                    <p id="203">从单个卷积来看, 无填充和填充情况下, 有性能提升的卷积占总数的67.4%和58.7%, 性能相当的卷积占总数的17.4%和13%;卷积的规模越大, Winograd算法的性能往往越好.</p>
                </div>
                <div class="p1">
                    <p id="204">综上可得, 本文的Winograd快速卷积对于大多数常用卷积类型具有性能提升, 且大规模卷积性能提升最为明显.在对比Intel MKL DNN时具有明显优势, 即使对比NVIDIA cuDNN也是略胜一筹, 可见本文的Winograd快速卷积具有实际使用价值.</p>
                </div>
                <h3 id="205" name="205" class="anchor-tag"><b>4 总结展望</b></h3>
                <div class="p1">
                    <p id="206">通过在Intel KNL平台上对Winograd快速卷积的研究与优化, 一方面以VGG19作为测试网络, 对比Intel MKL DNN, 最终提升了1倍多 (加速比2.01) 的卷积性能;另一方面, 测试常用卷积类型性能, 对比Intel MKL DNN和NVIDIA cuDNN, 证明了本文的Winograd算法对于常用卷积类型具有很好的适用性且具有实际使用价值.但这并不是结束, 相反这仅仅是一个开始, 未来仍有许多的路要走, 比如:提升算法对于小规模卷积的适用性;合并策略在进行尺度判断时, 仍需要手动配置, 无法自动决定最佳的合并尺度; 统一批处理和分块批处理的判断仍然需要进行大量测试以决定最终的边界, 简单地以MCDRAM的大小作为边界并不是最合适的方案;Intel MKL DNN和Winograd最适情况的标准仍需研究界定等.同时, 基于算法的角度考虑, 不同尺度的Winograd算法对于不同规模的卷积适应性更强, 可以从这一方面出发, 通过进一步扩大Winograd算法的尺度提升性能;基于Intel平台的考虑, 可以从数据预取出发, 进一步提升算法性能.未来我们将从这些方面入手, 对Winograd算法做进一步的完善.</p>
                </div>
                <div class="p1">
                    <p id="207">本文研究工作并不是去优化一个完善的快速卷积算法, 更多的是彰显Intel平台在深度学习领域的潜力和价值, 为其后续发展提供重要的指导意义, 借此为人工智能领域的蓬勃发展贡献一份力量.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="227">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESCBE0CC12C687BF3E4C199F2E09DF286E&amp;v=MTk5MjVTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExtK3dxaz1OaWZPZmNES2E5Ry8zSTVIRiswSEN3NVB6R01YbVQ1MFFRbmcyUkk4RGNTV1RienFDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Gu Jiuxiang, Wang Zhenhua, Kuen J, et al.Rencent advance in convolutional neural networks[J].Journal of Pattern Recognition, 2018, 77 (2) :354-377
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MTc1NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqa1VyM0pMejdCZHJHNEg5Yk1xWTlGWllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Zhou Feiyan, Jin Linpeng, Dong Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251 (in Chinese) (周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251) 
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient convolution architectures for convolutional neural network">

                                <b>[3]</b>Wang Jichen, Lin Jun, Wang Zhongfeng.Efficient convolution architectures for convolutional neural network[C]//Proc of the 8th IEEE Wireless Communications&amp;Signal Processing.Piscataway, NJ:IEEE, 2016:1-5
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extractive document summarization based on convolutional neural networks">

                                <b>[4]</b>Zhang Yong, Er M J, Pratama M, et al.Extractive document summarization based on convolutional neural networks[C]//Proc of the 42nd Annual Conf of the IEEEIndustrial Electronics Society.Piscataway, NJ:IEEE, 2016:918-922
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">

                                <b>[5]</b>Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 26th Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1106-1114
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speeding up convolutional neural networks with low rank expansions">

                                <b>[6]</b>Jaderberg M, Vedaldi A, Zisserman A.Speeding up convolutional neural networks with low rank expansions[C]//Proc of British Machine Vision Conf.Cambridge, UK:BMVA, 2014:073
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=cuDNN:Efficient primitives for deep learning[OL]">

                                <b>[7]</b>CoRR.cuDNN:Efficient primitives for deep learning[OL].[2018-01-03].https://arxiv.org/pdf/1410.0759.pdf
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy efficient techniques using FFT for deep convolutional neural networks">

                                <b>[8]</b>Nguyen-Thanh N, Le-Duc H, Ta D T, et al.Energy efficient techniques using FFT for deep convolutional neural networks[C]//Proc of IEEE Advanced Technologies for Communications.Piscataway, NJ:IEEE, 2016:231-236
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast training of convolutional networks through FFTs[OL]">

                                <b>[9]</b>CoRR.Fast training of convolutional networks through FFTs[OL].[2017-07-12].https://arxiv.org/pdf/1312.5851.pdf
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast algorithms for convolution neural networks">

                                <b>[10]</b>Lavin A, Gray S.Fast algorithms for convolution neural networks[C]//Proc of the 22nd IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:4013-4021
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast image gradients using binary feature convolutions">

                                <b>[11]</b>St-Charles P L, Biodeau G A, Bergevin R.Fast image gradients using binary feature convolutions[C]//Proc of the22nd IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1074-1081
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient and Accurate Approximations of Nonlinear Convolutional Networks">

                                <b>[12]</b>Zhang Xiangyu, Zou Jianhua, Ming Xiang, et al.Efficient and accurate approximations of nonlinear convolutional networks[C]//Proc of the 21st IEEE Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:1984-1992
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Factorized convolutional neural network">

                                <b>[13]</b>Wang Min, Liu Baoyuan, Foroosh B.Factorized convolutional neural network[C]//Proc of IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2017:545-553
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel GPU-based efficient approach for convolutional neural networks with small filters">

                                <b>[14]</b>Jiang Wenbin, Chen Yiming, Zheng Ran, et al.A novel GPU-based efficient approach for convolutional neural networks[J].Journal of Signal Processing Systems, 2017, 86 (3) , 313-325
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Minimizing computation in convolutional neural networks">

                                <b>[15]</b>Cong Jason, Xiao Bingjun.Minimizing computation in convolutional neural networks[C]//Proc of the 24th Artificial Neural Networks.Berlin:Springer, 2014:281-290
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extending Winograd&amp;#39;&amp;#39;s small convolution algorithm to longer lengths">

                                <b>[16]</b>Selesnick I W, Burrus C S.Extending Winograd’s small convolution algorithm to longer lengths[C]//Proc of IEEEInt Symp on Circuits and Systems.Piscataway, NJ:IEEE, 1994:449-452
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intel?Xeon Phi TMProcessor High Performance Programming">

                                <b>[17]</b>Jeffers J, Reinders J, Sodani A.Intel?Xeon Phi TMProcessor High Performance Programming[M].Burlington:Morgan Kaufmann, 2016:46-50
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Knights landing (KNL):2nd generation Intel?Xeon Phi TM processor,16124655">

                                <b>[18]</b>Sodani A.Knights landing (KNL) :2nd generation Intel?Xeon Phi TM processor, 16124655[R].Piscataway.NJ:IEEE, 2015, 
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201904015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201904015&amp;v=MTc0OTR6cXFCdEdGckNVUkxPZVplUnFGQ2prVXJ6QUx5dlNkTEc0SDlqTXE0OUVZWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

