

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127909023087500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201907016%26RESULT%3d1%26SIGN%3dfc7hT%252bO1ldjMPY7yUPifKS8vEW4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907016&amp;v=MDk2MTZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXJtV3J6SUx5dlNkTEc0SDlqTXFJOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;2 基于IndRNN-Attention的用户意图分类方法&lt;/b&gt; "><b>2 基于IndRNN-Attention的用户意图分类方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;2.1 输入层&lt;/b&gt;"><b>2.1 输入层</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;2.2 IndRNN层&lt;/b&gt;"><b>2.2 IndRNN层</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;2.3 注意力机制层&lt;/b&gt;"><b>2.3 注意力机制层</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.4 分类层&lt;/b&gt;"><b>2.4 分类层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="&lt;b&gt;3 实验与分析&lt;/b&gt; "><b>3 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#106" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;3.2 评价指标&lt;/b&gt;"><b>3.2 评价指标</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;3.3 实验设置&lt;/b&gt;"><b>3.3 实验设置</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;3.4 结果与分析&lt;/b&gt;"><b>3.4 结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#150" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;表1 用户意图举例&lt;/b&gt;"><b>表1 用户意图举例</b></a></li>
                                                <li><a href="#66" data-title="图2 基本RNN单元结构">图2 基本RNN单元结构</a></li>
                                                <li><a href="#69" data-title="图1 IndRNN-Attention结构">图1 IndRNN-Attention结构</a></li>
                                                <li><a href="#84" data-title="图3 IndRNN单元结构">图3 IndRNN单元结构</a></li>
                                                <li><a href="#108" data-title="图4 SMP2017-ECDT数据集规模">图4 SMP2017-ECDT数据集规模</a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表2 各种方法对比实验结果&lt;/b&gt;"><b>表2 各种方法对比实验结果</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表3 不同方法在每个类别的&lt;i&gt;F&lt;/i&gt;值比较&lt;/b&gt;"><b>表3 不同方法在每个类别的<i>F</i>值比较</b></a></li>
                                                <li><a href="#142" data-title="图5 SMP2017-ECDT数据集中一条用户输入的注意力权重示例">图5 SMP2017-ECDT数据集中一条用户输入的注意力权重示例</a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表4 分类错误示例&lt;/b&gt;"><b>表4 分类错误示例</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="178">


                                    <a id="bibliography_1" title="Wang Ziheng, Qi Yonggang, Liu Jun.User intention understanding from scratch[C] //Proc of the 1st Int Workshop on Sensing, Processing and Learning for Intelligent Machines.Piscataway, NJ:IEEE, 2016:1- 4" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=User intention understanding from scratch">
                                        <b>[1]</b>
                                        Wang Ziheng, Qi Yonggang, Liu Jun.User intention understanding from scratch[C] //Proc of the 1st Int Workshop on Sensing, Processing and Learning for Intelligent Machines.Piscataway, NJ:IEEE, 2016:1- 4
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_2" title="Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C] //Proc of the 31st Conf on Neural Information Processing Systems.New York:Curran Associates, 2017:5998- 6008" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention is All You Need">
                                        <b>[2]</b>
                                        Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C] //Proc of the 31st Conf on Neural Information Processing Systems.New York:Curran Associates, 2017:5998- 6008
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_3" title="Tur G, Celikyilmaz A, Hakkani-T&#252;r D.Latent semantic modeling for slot filling in conversational understanding[C] //Proc of the 38th Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:8307- 8311" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent semantic modeling for slot filling in conversational understanding">
                                        <b>[3]</b>
                                        Tur G, Celikyilmaz A, Hakkani-T&#252;r D.Latent semantic modeling for slot filling in conversational understanding[C] //Proc of the 38th Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:8307- 8311
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_4" title="Li Shuai, Li Wangqing, Cook C, et al.Independently recurrent neural network (IndRNN) :Building A longer and deeper RNN[C] //Proc of the 2018 Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:5457- 5466" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Independently Recurrent Neural Network(IndRNN):Building A Longer and Deeper RNN">
                                        <b>[4]</b>
                                        Li Shuai, Li Wangqing, Cook C, et al.Independently recurrent neural network (IndRNN) :Building A longer and deeper RNN[C] //Proc of the 2018 Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:5457- 5466
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_5" title="Schatzmann J, Weilhammer K, Stuttle M, et al.A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies[J].Knowledge Engineering Review, 2006, 21 (2) :97- 126" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of statistical user simulation techniques for reinforcement- learning of dialogue management strategies">
                                        <b>[5]</b>
                                        Schatzmann J, Weilhammer K, Stuttle M, et al.A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies[J].Knowledge Engineering Review, 2006, 21 (2) :97- 126
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_6" title="Zhang Weinan, Chen Zhigang, Che Wanxiang, et al.The first evaluation of Chinese human-computer dialogue technology[J].arXiv preprint, arXiv:1709.10217, 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The first evaluation of Chinese human-computer dialogue technology">
                                        <b>[6]</b>
                                        Zhang Weinan, Chen Zhigang, Che Wanxiang, et al.The first evaluation of Chinese human-computer dialogue technology[J].arXiv preprint, arXiv:1709.10217, 2017
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_7" title="Yang Huahai, Li Yunyao.Identifying user needs from social media[R].San Jose, CA:IBM Research Division, 2013" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identifying user needs from social media">
                                        <b>[7]</b>
                                        Yang Huahai, Li Yunyao.Identifying user needs from social media[R].San Jose, CA:IBM Research Division, 2013
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_8" title="Fu Bo, Liu Ting.Weakly-supervised consumption intent detection in microblogs[J].Journal of Computational Information Systems, 2013, 6 (9) :2423- 2431" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weakly-supervised consumption intent detection in microblogs">
                                        <b>[8]</b>
                                        Fu Bo, Liu Ting.Weakly-supervised consumption intent detection in microblogs[J].Journal of Computational Information Systems, 2013, 6 (9) :2423- 2431
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_9" title="Subramani S, Vu H Q, Wang Hua.Intent classification using feature sets for domestic violence discourse on social media[C] //Proc of the 4th Asia-Pacific World Congress on Computer Science and Engineering.Piscataway, NJ:IEEE, 2017:129- 136" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intent classification using feature sets for domestic violence discourse on social media">
                                        <b>[9]</b>
                                        Subramani S, Vu H Q, Wang Hua.Intent classification using feature sets for domestic violence discourse on social media[C] //Proc of the 4th Asia-Pacific World Congress on Computer Science and Engineering.Piscataway, NJ:IEEE, 2017:129- 136
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_10" title="Chenlo J M, Losada D E.An empirical study of sentence features for subjectivity and polarity classification[J].Information Sciences, 2014, 280:275- 288" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600034477&amp;v=MjY2MzhadUh5am1VTHJJSWwwZGFSST1OaWZPZmJLOEh0Zk5xWTlGWk9nTENIcytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Chenlo J M, Losada D E.An empirical study of sentence features for subjectivity and polarity classification[J].Information Sciences, 2014, 280:275- 288
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                    Pang Bo, Lee L, Vaithyanathan S.Thumbs up?Sentiment classification using machine learning techniques[C] //Proc of EMNLP&#39;02.Stroudsburg, PA:ACL, 2002:79- 86</a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_12" title="Wang Sida, Manning C D.Baselines and bigrams:Simple, good sentiment and topic classification[C] //Proc of Association for Computational Linguistics.Stroudsburg, PA:ACL, 2012:90- 94" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Baselines and bigrams:Simple,good sentiment and topic classification">
                                        <b>[12]</b>
                                        Wang Sida, Manning C D.Baselines and bigrams:Simple, good sentiment and topic classification[C] //Proc of Association for Computational Linguistics.Stroudsburg, PA:ACL, 2012:90- 94
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_13" title="Huang Dazhen, Huang Zhihua.Consumption pattern recognition system based on SVM[C] //Proc of the 4th Int Conf on Intelligent Computation Technology and Automation (ICICTA) .Piscataway, NJ:IEEE, 2011:79- 82" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Consumption Pattern Recognition System Based on SVM">
                                        <b>[13]</b>
                                        Huang Dazhen, Huang Zhihua.Consumption pattern recognition system based on SVM[C] //Proc of the 4th Int Conf on Intelligent Computation Technology and Automation (ICICTA) .Piscataway, NJ:IEEE, 2011:79- 82
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_14" title="Kim Y.Convolutional neural networks for sentence classification[J].arXiv preprint, arXiv:1408.5882, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">
                                        <b>[14]</b>
                                        Kim Y.Convolutional neural networks for sentence classification[J].arXiv preprint, arXiv:1408.5882, 2014
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_15" title="Gao Yunlong, Zuo Wanli, Wang Ying, et al.Sentence classification model based on sparse and self-taught convolu-tional neural networks[J].Journal of Computer Research and Development, 2018, 55 (1) :179- 187 (in Chinese) (高云龙, 左万利, 王英, 等.基于稀疏自学习卷积神经网络的句子分类模型[J].计算机研究与发展, 2018, 55 (1) :179- 187) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201801016&amp;v=MjU2MDJDVVJMT2VaZVJyRnlybVdyeklMeXZTZExHNEg5bk1ybzlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        Gao Yunlong, Zuo Wanli, Wang Ying, et al.Sentence classification model based on sparse and self-taught convolu-tional neural networks[J].Journal of Computer Research and Development, 2018, 55 (1) :179- 187 (in Chinese) (高云龙, 左万利, 王英, 等.基于稀疏自学习卷积神经网络的句子分类模型[J].计算机研究与发展, 2018, 55 (1) :179- 187) 
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_16" title="Xu Pengfei, Wang Lei, Guan Ziyu, et al.Evaluating brush movements for Chinese calligraphy:A computer vision based approach[C] //Proc of the 2018 Int Joint Conf on Artificial Intelligence (IJCAI) .San Francisco, CA:Morgan Kaufmann, 2018:1050- 1056" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating brush movements for Chinese calligraphy:a computer vision based approach">
                                        <b>[16]</b>
                                        Xu Pengfei, Wang Lei, Guan Ziyu, et al.Evaluating brush movements for Chinese calligraphy:A computer vision based approach[C] //Proc of the 2018 Int Joint Conf on Artificial Intelligence (IJCAI) .San Francisco, CA:Morgan Kaufmann, 2018:1050- 1056
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_17" title="Bhardwaj H B, Asiaee A, Kraft R.Query intent detection using convolutional neural networks[C/OL] //Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016[2018-08-22].http://people.cs.pitt.edu/～hashemi/papers/QRUMS2016_HBHashemi.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Query intent detection using convolutional neural networks[C/OL]">
                                        <b>[17]</b>
                                        Bhardwaj H B, Asiaee A, Kraft R.Query intent detection using convolutional neural networks[C/OL] //Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016[2018-08-22].http://people.cs.pitt.edu/～hashemi/papers/QRUMS2016_HBHashemi.pdf
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_18" title="Bhardwaj A, Rudnicky A.User intent classification using memory networks:A comparative analysis for a limited data scenario[J].arXiv preprint, arXiv:1706.06160, 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=User intent classification using memory networks:A comparative analysis for a limited data scenario">
                                        <b>[18]</b>
                                        Bhardwaj A, Rudnicky A.User intent classification using memory networks:A comparative analysis for a limited data scenario[J].arXiv preprint, arXiv:1706.06160, 2017
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_19" title="Kim Y B, Lee S, Stratos K.ONENET:Joint domain, intent, slot prediction for spoken language understanding[C] //Proc of Automatic Speech Recognition and Understanding Workshop.Piscataway, NJ:IEEE, 2017:547- 553" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ONENET:Joint domain intent slot prediction for spoken language understanding">
                                        <b>[19]</b>
                                        Kim Y B, Lee S, Stratos K.ONENET:Joint domain, intent, slot prediction for spoken language understanding[C] //Proc of Automatic Speech Recognition and Understanding Workshop.Piscataway, NJ:IEEE, 2017:547- 553
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_20" title="Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjkzMzJvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJbDBkYVJJPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_21" title="Chung J, Gulcehre C, Cho K H, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].arXiv preprint, arXiv:1412.3555, 2014" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of gated recurrent neural networks on sequence modeling">
                                        <b>[21]</b>
                                        Chung J, Gulcehre C, Cho K H, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].arXiv preprint, arXiv:1412.3555, 2014
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(07),1517-1524 DOI:10.7544/issn1000-1239.2019.20180648            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于IndRNN-Attention的用户意图分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">张志昌</a>
                                <a href="javascript:;">张珍文</a>
                                <a href="javascript:;">张治满</a>
                </h2>
                    <h2>

                    <span>西北师范大学计算机科学与工程学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对人机对话中的用户意图分类问题, 提出了一种基于独立循环神经网络 (independently recurrent neural network, IndRNN) 和词级别注意力 (word-level attention) 融合的用户意图分类方法.通过构造一个多层独立循环神经网络模型实现对用户输入文本编码, 有效解决了循环神经网络中容易出现的梯度消失和梯度爆炸问题;结合词级别注意力提高了领域相关词汇对用户输入文本编码的贡献度, 有效提高了分类精度.实验结果表明:提出的方法在用户意图分类任务上的效果取得了显著的提升.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E6%9C%BA%E5%AF%B9%E8%AF%9D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人机对话;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%A8%E6%88%B7%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">用户意图分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%8B%AC%E7%AB%8B%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">独立循环神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Zhang Zhichang, born in 1976.PhD, professor.Member of CCF.His main research  interests  include  question answering and clinical natural language processing. zzc@nwnu.edu.cn;
                                </span>
                                <span>
                                    Zhang Zhenwen, born in 1993.Master candidate.Student member of CCF.His main research interests include natural language processing and question answering.;
                                </span>
                                <span>
                                    Zhang Zhiman, born in 1993. Master candidate.Student member of CCF.His main research interests include natural language processing and question answering.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61762081, 61662067, 61662068);</span>
                                <span>甘肃省重点研发计划项目 (2017GS10781);</span>
                    </p>
            </div>
                    <h1><b>User Intent Classification Based on IndRNN-Attention</b></h1>
                    <h2>
                    <span>Zhang Zhichang</span>
                    <span>Zhang Zhenwen</span>
                    <span>Zhang Zhiman</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering, Northwest Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Recently, with the development of big data and deep learning techniques, human-computer dialogue technology has been emerged as a hot topic, which has attracted the attention from academia and industry. Massive application products based on human-computer dialogue technology appear in our lives and bring us great convenience, such as Apple Siri, Microsoft Cortana, and Huawei smart speaker. However, how to make dialogue system identify and understand user intent more accurately, is still a great challenge. This paper therefore proposes a novel method named IndRNN-Attention based on independently recurrent neural network (IndRNN) and word-level attention mechanism for user intent classification problem. Firstly, we encode the user input message text through Multi-layer IndRNN. Secondly, we use word-level attention mechanism to improve the contribution of domain-related words to encode user input message text, and generate final representation vector of the user input message text. Finally, we classify this representation vector through softmax layer and output classification result. We not only introduce the IndRNN in our approach to solve the problems of gradient vanishing and gradient explosion, but also integrate word-level attention mechanism to improve the quality of text representation. Experimental results show that the proposed IndRNN-Attention approach achieves 0.93 <i>F</i><sub>macro</sub> value on the user intent classification task and outperforms the state-of-the-art approaches significantly.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human-computer%20dialogue&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human-computer dialogue;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=user%20intent%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">user intent classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=independently%20recurrent%20neural%20network%20(IndRNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">independently recurrent neural network (IndRNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-13</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61762081, 61662067, 61662068);</span>
                                <span>the Key Research and Development Project of Gansu Province (2017GS10781);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="57">人机对话作为智能场景中的关键技术, 近年来引起了学术界和产业界的广泛关注, 相关方法已经在不少产品 (如苹果Siri<citation id="174" type="note"><link href="4" rel="footnote" /><sup>①</sup></citation>、Google Now<citation id="175" type="note"><link href="6" rel="footnote" /><sup>②</sup></citation>、微软Cortana<citation id="176" type="note"><link href="8" rel="footnote" /><sup>③</sup></citation>和阿里小蜜<citation id="177" type="note"><link href="10" rel="footnote" /><sup>④</sup></citation>等) 中得到了广泛应用.人机对话系统运行时, 首要的任务就是在用户输入消息 (文本或者语音) 后, 准确理解用户对话意图<citation id="220" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 例如判断用户是想和系统闲聊, 还是希望系统完成特定的任务 (如预定机票、订外卖、查询天气等) ;然后再根据用户意图做出正确的回应, 让对话顺利进行.精准的用户意图理解能有效提高人机交互的自然度<citation id="221" type="reference"><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>, 提升用户体验.因此, 用户意图分类在人机对话系统研究中具有重要的研究意义.表1为常见用户意图举例:</p>
                </div>
                <div class="area_img" id="58">
                    <p class="img_tit"><b>表1 用户意图举例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Examples of User Intent</b></p>
                    <p class="img_note"></p>
                    <table id="58" border="1"><tr><td><br />Input Message</td><td>Intent Category</td><td>Label</td></tr><tr><td><br />打开会说话的汤姆猫</td><td>Open App</td><td>App</td></tr><tr><td><br />我看看现在的最新新闻是什么?</td><td>Read News</td><td>News</td></tr><tr><td><br />我想订一张去北京的机票</td><td>Book a Flight</td><td>Flight</td></tr><tr><td><br />你好啊, 很高兴见到你!</td><td>Chat</td><td>Chat</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="59">针对人机对话中的用户意图分类问题目前已有不少研究, 但依然存在很多需要深入解决的问题, 如万能回复、回复相关性差及对聊天文本语义理解不够深入, 无法从较短的用户聊天文本中理解用户意图等问题.如当用户输入“我的ThinkPad已经用了5年了”时, 可能是称赞这个笔记本电脑性能很好, 也可能是表达想换新笔记本电脑的一种意愿.在这种情况下, 对话系统很难准确判断用户的真实意图到底是哪一种.现有的用户意图分类方法存在对聊天文本语义理解不够深入、难以有效表达用户聊天文本的真实语义信息等问题.</p>
                </div>
                <div class="p1">
                    <p id="60">针对上述问题, 本文提出了基于独立循环神经网络 (independently recurrent neural network, IndRNN) 和词级别注意力机制 (word-level attention mechanism) 的用户意图分类方法.通过多层IndRNN<citation id="222" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>网络作为编码器对用户聊天文本编码, 有效改善了传统循环神经网络 (recurrent neural network, RNN) 结构在序列任务处理中出现的“梯度消失”和“梯度爆炸”等问题, 而词级别注意力机制<citation id="223" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>则显著地增强了领域词汇对意图分类的突出贡献.通过在第六届全国社会媒体处理大会中文人机对话技术评测<citation id="224" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation> (SMP2017-ECDT) 用户意图领域分类语料上实验发现, 本文提出的方法相比其他已有的几种分类方法, 取得了更好的分类效果.</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="62">对用户意图分类方面已有的研究方法进行总结, 可以划分为3类:</p>
                </div>
                <div class="p1">
                    <p id="63">1) 基于规则的方法.这种方法主要是根据知识工程师或领域专家的经验和知识归纳总结出相关分类规则, 然后构建相应的规则模板作为分类器分类.Yang等人<citation id="225" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation> 提出了人们日常生活中经常出现的12 种普遍需求, 通过模板匹配方法获得了购买产品的用户, 然后基于Twitter 中的Unigram特征、WordNet 中词的语义特征和表达需求的动词特征来训练分类器完成对用户消费意图的识别.Fu等人<citation id="226" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>采用基于模板的匹配方法来检测用户微博消费意图.基于规则的方法优点是分类准确率较高.缺点是:①规则之间的关系不透明, 缺乏分层的知识表达;②不具备从经验中学习的能力, 维护困难;③覆盖率低, 规则制定需要专业人员参与, 耗时耗力, 可扩展性较差, 很难在多领域推广使用.</p>
                </div>
                <div class="p1">
                    <p id="64">2) 基于传统机器学习的方法.传统机器学习方法基于特征工程<citation id="227" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 常用的方法有朴素贝叶斯 (Naive Bayes, NB) 、支持向量机 (support vector machine, SVM) 、最大熵等.用户意图分类任务需要获取用户输入文本的语言特征, 如词法、句法等.大量相关研究证明模型所学习的语言学特征对于自然语言处理能够提供十分重要的价值<citation id="228" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.Pang等人<citation id="229" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出使用N-gram特征可以有效地识别影评的极性, 其中, Unigram特征的效果最佳;Wang等人<citation id="230" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>利用词语Bigram特征训练NB和SVM模型, 在句子或文本主题分类问题中取得一致较好的效果;Huang等人<citation id="231" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> 在用户消费模式识别中提出了基于SVM的识别方法.基于传统机器学习的分类方法, 其优点是不需要手动编写规则模板, 能有效解决基于规则的方法中所存在的问题.缺点是:①提取特征耗时, 其消耗往往随着数据集规模的变大而增长, 容易出现维度灾难;②不容易构造有效的分类特征, 且大部分的特征基于字或词, 难以表达句子较深层的语义信息.</p>
                </div>
                <div class="p1">
                    <p id="65">3) 基于深度学习的方法.深度学习的出现极大降低了获取文本特征的难度.Kim<citation id="232" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>首次将卷积神经网络 (CNN) 应用到句子分类任务中, 并提出了几种变形;Gao等人<citation id="233" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>在句子分类任务中提出基于稀疏自学习卷积神经网络对句子编码, 实现句子分类;Xu等人<citation id="234" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出利用神经网络的方法评估书法练习者书法临摹的质量;Bhardwaj等人<citation id="235" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在搜索引擎查询意图识别任务中提出利用卷积神经网络获取查询文本的向量表示作为查询分类的特征;Bhardwaj等人<citation id="236" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出利用记忆网络对用户交互信息编码以实现应用推荐任务中的意图分类;Kim等人<citation id="237" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>在口语对话理解中提出利用Bi-LSTM方法对用户交互文本编码实现领域和意图分类.端到端的深度学习方法使用词向量表示, 可不依赖于特定语言的句法关系, 减少了人工设计和构造分类特征的负担, 还可以抓取到视觉上无法获取的特征, 从而提高了分类的精度.</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907016_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基本RNN单元结构" src="Detail/GetImg?filename=images/JFYZ201907016_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基本RNN单元结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907016_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Basic RNN unit structure</p>

                </div>
                <h3 id="67" name="67" class="anchor-tag"><b>2 基于IndRNN-Attention的用户意图分类方法</b></h3>
                <div class="p1">
                    <p id="68">本文提出的基于IndRNN-Attention的用户意图分类方法模型结构如图1所示, 由输入层、编码层、注意力层和分类层组成.</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907016_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 IndRNN-Attention结构" src="Detail/GetImg?filename=images/JFYZ201907016_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 IndRNN-Attention结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907016_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Architecture of IndRNN-Attention</p>

                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>2.1 输入层</b></h4>
                <div class="p1">
                    <p id="71">模型的输入是用户输入的一个句子对应的词向量矩阵, 词向量由预先训练好的word2vec模型得到.对于数据集中的任意一个句子, 将其分词后参照数据集中句子的最大长度进行填充, 并将句子中的每个词转换成对应的词向量表示, 便可得到该句子的词向量矩阵.</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>2.2 IndRNN层</b></h4>
                <div class="p1">
                    <p id="73">RNN被广泛地应用在动作识别、场景标注和自然语言处理等领域.基本的RNN单元结构如图2所示.</p>
                </div>
                <div class="p1">
                    <p id="74">在RNN中每个神经元接收当前时刻的输入和上一时刻的输出作为输入, 时刻<i>t</i>的状态更新为</p>
                </div>
                <div class="p1">
                    <p id="75"><b><i>h</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>Wx</i></b><sub><i>t</i></sub>+<b><i>Uh</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b>) , (1) </p>
                </div>
                <div class="p1">
                    <p id="76">其中<b><i>x</i></b><sub><i>t</i></sub>∈R<sup><i>M</i></sup>和<b><i>h</i></b><sub><i>t</i>-1</sub>∈R<sup><i>N</i></sup>分别是时刻<i>t</i>的输入和时刻<i>t</i>-1的隐藏状态, <b><i>W</i></b>∈R<sup><i>N</i>×<i>M</i></sup>和<b><i>U</i></b>∈R<sup><i>N</i>×<i>N</i></sup>分别是神经元当前时刻的输入和循环输入的权重矩阵, <b><i>b</i></b>∈R<sup><i>N</i></sup>是偏置项, <i>σ</i>是神经元激活函数, <i>N</i> 是每个RNN层神经元数量, <i>M</i>是输入层的大小.</p>
                </div>
                <div class="p1">
                    <p id="77">RNN在训练过程中容易出现“梯度消失”和 “梯度爆炸”问题, 这使得RNN学习长期依赖关系的能力比较差.为解决这些问题, 研究人员提出了长短时记忆网络<citation id="238" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation> (long short-term memory, LSTM) 和门控循环单元<citation id="239" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation> (gated recurrent unit, GRU) .虽然这2种网络结构在一定程度上改善了这些梯度问题, 但是由于LSTM和GRU使用tanh函数和sigmoid函数作为激活函数, 会导致层与层之间的梯度衰减, 因此要构建和训练一个深层的循环神经网络实际上较困难.</p>
                </div>
                <div class="p1">
                    <p id="78">通过使用IndRNN可以有效地解决上述问题.IndRNN单元结构如图3所示.</p>
                </div>
                <div class="p1">
                    <p id="79">IndRNN状态更新如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="80"><b><i>h</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>Wx</i></b><sub><i>t</i></sub>+<b><i>U</i></b>⊙<b><i>h</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b>) , (2) </p>
                </div>
                <div class="p1">
                    <p id="81">其中, <b><i>x</i></b><sub><i>t</i></sub>∈R<sup><i>M</i></sup>和<b><i>h</i></b><sub><i>t</i>-1</sub>∈R<sup><i>N</i></sup>分别是时刻<i>t</i>的输入和时刻<i>t</i>-1的隐藏状态, 其中<b><i>W</i></b>∈R<sup><i>N</i>×<i>M</i></sup>表示输入层到隐层的权重, <b><i>U</i></b>∈R<sup><i>N</i></sup>表示上一时刻隐层到当前隐层的权重, ⊙表示哈达马积 (hadamard product) .IndRNN中每一层的每个神经元之间都是独立的, 神经元之间的连接可以通过堆叠2层或者多层IndRNN单元实现.对于第<i>n</i>个神经元, 时刻<i>t</i>的隐藏状态可通过计算得到:</p>
                </div>
                <div class="p1">
                    <p id="82"><b><i>h</i></b><sub><i>n</i>, <i>t</i></sub>=<i>σ</i> (<b><i>w</i></b><sub><i>n</i></sub><b><i>x</i></b><sub><i>t</i></sub>+<b><i>u</i></b><sub><i>n</i></sub><b><i>h</i></b><sub><i>n</i>, <i>t</i>-1</sub>+<b><i>b</i></b><sub><i>n</i></sub>) , (3) </p>
                </div>
                <div class="p1">
                    <p id="83">其中, <b><i>w</i></b><sub><i>n</i></sub>和<b><i>u</i></b><sub><i>n</i></sub>分别表示第<i>n</i>行神经元的输入权重和隐层权重.</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907016_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 IndRNN单元结构" src="Detail/GetImg?filename=images/JFYZ201907016_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 IndRNN单元结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907016_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 IndRNN unit structure</p>

                </div>
                <div class="p1">
                    <p id="85">在IndRNN中, 每个神经元只接收来自当前时刻的输入和它本身在上一时刻的隐藏状态信息, 每个神经元独立处理一种类型的时空模式.传统的RNN通常被视为通过时间共享参数的多层感知机, 而 IndRNN则展现了一种随着时间步的延伸 (通过<b><i>u</i></b>) 独立地聚集空间模式 (通过<b><i>w</i></b>) 的新视角.通过堆叠2层或多层神经元, 下一层中的每个神经元独立地处理前一层中所有神经元的输出, 降低了构建深度网络结构的难度, 增强了对更长序列的建模能力.同时, 借助ReLU等非饱和激活函数, 训练之后的 IndRNN 鲁棒性更高.</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>2.3 注意力机制层</b></h4>
                <div class="p1">
                    <p id="87">获取文本语义表示常用的方法是取编码器最后时刻的隐藏状态输出向量作为最终的编码向量, 优点是一定程度上可以有效地涵盖文本语义信息, 缺点是这种办法很难将输入文本的所有信息编码在一个固定长度的向量中.如果直接将每个时刻的输出向量相加或者平均, 那么可以认为每个输入的字或词对计算输入文本的语义表示结果的贡献是相等的, 这种方法降低了那些对表达文本含义有重要作用的字或词的贡献度.</p>
                </div>
                <div class="p1">
                    <p id="88">利用智能移动终端帮用户完成订机票、导航、订酒店等是常会发生的情景.例如用户输入“上海回合肥怎么坐汽车?”, 用户意图是查询路线, 属于“bus”类, 在分类过程中“汽车”这个词对正确分类贡献最大, 所占的权重也应该最高.因此, 我们引入词级别注意力机制来提取对句子含义重要的词的信息.</p>
                </div>
                <div class="p1">
                    <p id="89">给定一个序列<i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>T</i></sub>) , <i>T</i>表示序列长度.序列<i>S</i>中的第<i>i</i>个词在时刻<i>t</i>的隐藏状态<b><i>h</i></b><sub><i>it</i></sub>可由式 (3) 计算得到, 词级别注意力机制可以通过3个步骤实现:</p>
                </div>
                <div class="p1">
                    <p id="90">1) 使用多层感知机获得<b><i>h</i></b><sub><i>it</i></sub>的隐藏表示<b><i>u</i></b><sub><i>it</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="91"><b><i>u</i></b><sub><i>it</i></sub>=tanh (<b><i>W</i></b><sub>w</sub><b><i>h</i></b><sub><i>it</i></sub>+<b><i>b</i></b><sub>w</sub>) ; (4) </p>
                </div>
                <div class="p1">
                    <p id="92">2) 计算<b><i>u</i></b><sub><i>it</i></sub>和词级别上下文<b><i>u</i></b><sub>w</sub>的相似性将其作为单词的重要性度量, 通过softmax函数计算归一化权重<i>α</i><sub><i>it</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="93"><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>t</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>t</mi></mrow><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mtext>w</mtext></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>t</mi></munder><mspace width="0.25em" /></mstyle><mi>exp</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>t</mi></mrow><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">u</mi><msub><mrow></mrow><mtext>w</mtext></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>; (5) </p>
                </div>
                <div class="p1">
                    <p id="95">单词上下文向量<b><i>u</i></b><sub>w</sub>是在训练过程中随机初始化和共同学习的.</p>
                </div>
                <div class="p1">
                    <p id="96">3) 计算句子向量<b><i>C</i></b>:</p>
                </div>
                <div class="p1">
                    <p id="97"><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>t</mi></munder><mi mathvariant="bold-italic">α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>t</mi></mrow></msub><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>t</mi></mrow></msub></mrow></math></mathml>. (6) </p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.4 分类层</b></h4>
                <div class="p1">
                    <p id="100">将句子编码结果<b><i>C</i></b>输入到一个全连接层并使用全连接层的输出作为句子最终的特征向量, 将其输入softmax层即可输出各类别的概率.计算方法为</p>
                </div>
                <div class="p1">
                    <p id="101"><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo>=</mo><mtext>s</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></math></mathml> (<b><i>WC</i></b>+<b><i>b</i></b>) , (7) </p>
                </div>
                <div class="p1">
                    <p id="103">其中, <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>是预测概率向量, <b><i>W</i></b>是学习的权重参数, <b><i>b</i></b>是偏置项.</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag"><b>3 实验与分析</b></h3>
                <h4 class="anchor-tag" id="106" name="106"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="107">本文实验数据集来源于SMP2017-ECDT, 该数据集覆盖闲聊和垂直类2大类, 其中垂直类细分为30个垂直领域, 数据集统计结果如图4所示:</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907016_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 SMP2017-ECDT数据集规模" src="Detail/GetImg?filename=images/JFYZ201907016_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 SMP2017-ECDT数据集规模  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907016_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Size of SMP2017-ECDT dataset</p>

                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>3.2 评价指标</b></h4>
                <div class="p1">
                    <p id="110">本文用户意图分类是一个多分类问题, 我们使用精确率 (precision, <i>P</i>) 、召回率 (recall, <i>R</i>) 和<i>F</i>值作为每个类别的评价指标, 使用宏平均值 (macro-average) 作为每种分类方法最终的评价指标, 计算方法为</p>
                </div>
                <div class="p1">
                    <p id="111"><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mrow></math></mathml>, (8) </p>
                </div>
                <div class="p1">
                    <p id="113"><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow></mfrac></mrow></math></mathml>, (9) </p>
                </div>
                <div class="p1">
                    <p id="115"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><mo>×</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="117"><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Ρ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, (11) </p>
                </div>
                <div class="p1">
                    <p id="119"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>R</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, (12) </p>
                </div>
                <div class="p1">
                    <p id="121"><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub><mo>×</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub><mo>+</mo><mi>R</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>c</mtext><mtext>r</mtext><mtext>o</mtext></mrow></msub></mrow></mfrac></mrow></math></mathml>, (13) </p>
                </div>
                <div class="p1">
                    <p id="123">其中, <i>TP</i>表示将正类预测为正类数, <i>TN</i>表示将负类预测为负类数, <i>FP</i>表示将负类预测为正类数, <i>FN</i>表示将正类预测为负类数, <i>n</i>是类别总数.</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124"><b>3.3 实验设置</b></h4>
                <div class="p1">
                    <p id="125">在数据预处理过程中, 我们使用结巴分词工具对用户聊天文本分词, 并去除了原始文本中包含的标点符号和特殊字符等.</p>
                </div>
                <div class="p1">
                    <p id="126">在基于CNN的分类实验中, 设置卷积核大小为3, 4, 5的过滤器各100个;在基于LSTM的分类实验中, 采用了2层LSTM网络结构;在基于IndRNN和IndRNN-Attention的分类实验中, <i>attention</i>_<i>size</i>=128.上述4种实验中隐藏层大小均设置为128, 词向量的维度设置为300, <i>batch</i>_<i>size</i>=64, Padding的最大长度参考训练语料中最大文本长度设置为26, 在倒数第2层与最后1层的softmax层之间设置<i>dropout</i>_<i>rate</i>=0.5, 在softmax层中使用了值为1的<i>l</i><sub>2</sub>正则化项.</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>3.4 结果与分析</b></h4>
                <h4 class="anchor-tag" id="128" name="128">3.4.1 实验结果</h4>
                <div class="p1">
                    <p id="129">5种分类方法在测试语料上的整体分类性能见表2, 各个子类别上的<i>F</i>值如表3所示:</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表2 各种方法对比实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Comparison of Overall Classification Result of Different Models</b></p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td>Method</td><td>SVM</td><td>CNN</td><td>LSTM</td><td>IndRNN</td><td>IndRNN-Attention</td></tr><tr><td><br /><i>P</i><sub>macro</sub></td><td>0.82</td><td>0.83</td><td>0.88</td><td>0.91</td><td><b>0.94</b></td></tr><tr><td><br /><i>R</i><sub>macro</sub></td><td>0.82</td><td>0.85</td><td>0.82</td><td>0.89</td><td><b>0.92</b></td></tr><tr><td><br /><i>F</i><sub>macro</sub></td><td>0.82</td><td>0.84</td><td>0.85</td><td>0.90</td><td><b>0.93</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Notes: The bold values are the best results obtained by our method. </p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表3 不同方法在每个类别的<i>F</i>值比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3 F Score Comparison of Different Models on Individual Category</p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td>Method</td><td>SVM</td><td>CNN</td><td>LSTM</td><td>IndRNN</td><td>IndRNN-Attention</td></tr><tr><td><br />app</td><td>0.64</td><td>0.82</td><td>0.85</td><td>0.80</td><td>0.84</td></tr><tr><td><br />bus</td><td>0.93</td><td>0.95</td><td>0.93</td><td>0.97</td><td>1.00</td></tr><tr><td><br />calc</td><td>0.94</td><td>0.90</td><td>0.80</td><td>0.82</td><td>1.00</td></tr><tr><td><br />chat</td><td>0.81</td><td>0.87</td><td>0.85</td><td>0.89</td><td>0.97</td></tr><tr><td><br />cinemas</td><td>0.62</td><td>0.87</td><td>0.85</td><td>0.78</td><td>0.57</td></tr><tr><td><br />contacts</td><td>0.89</td><td>0.88</td><td>0.95</td><td>0.95</td><td>0.88</td></tr><tr><td><br />cookbook</td><td>0.89</td><td>0.94</td><td>0.92</td><td>0.95</td><td>0.95</td></tr><tr><td><br />datetime</td><td>0.73</td><td>0.73</td><td>0.65</td><td>0.76</td><td>0.78</td></tr><tr><td><br />email</td><td>0.93</td><td>0.91</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td><br />epg</td><td>0.79</td><td>0.79</td><td>0.82</td><td>0.89</td><td>0.91</td></tr><tr><td><br />flight</td><td>0.92</td><td>0.95</td><td>0.98</td><td>0.98</td><td>0.95</td></tr><tr><td><br />health</td><td>0.78</td><td>0.71</td><td>0.64</td><td>0.84</td><td>0.85</td></tr><tr><td><br />lottery</td><td>0.93</td><td>0.93</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td><br />map</td><td>0.86</td><td>0.86</td><td>0.83</td><td>0.84</td><td>0.92</td></tr><tr><td><br />match</td><td>0.77</td><td>0.87</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td><br />message</td><td>0.92</td><td>1.00</td><td>0.92</td><td>0.95</td><td>1.00</td></tr><tr><td><br />music</td><td>0.55</td><td>0.64</td><td>0.62</td><td>0.75</td><td>0.81</td></tr><tr><td><br />news</td><td>0.90</td><td>0.90</td><td>0.87</td><td>0.90</td><td>1.00</td></tr><tr><td><br />novel</td><td>0.55</td><td>0.57</td><td>0.86</td><td>0.93</td><td>0.93</td></tr><tr><td><br />poetry</td><td>0.87</td><td>0.87</td><td>0.90</td><td>0.87</td><td>1.00</td></tr><tr><td><br />radio</td><td>0.77</td><td>0.74</td><td>0.77</td><td>0.88</td><td>0.87</td></tr><tr><td><br />riddle</td><td>0.90</td><td>0.89</td><td>0.95</td><td>1.00</td><td>1.00</td></tr><tr><td><br />schedule</td><td>0.75</td><td>0.78</td><td>0.80</td><td>0.90</td><td>0.97</td></tr><tr><td><br />stock</td><td>0.88</td><td>0.86</td><td>0.88</td><td>0.91</td><td>1.00</td></tr><tr><td><br />telephone</td><td>0.93</td><td>0.93</td><td>0.91</td><td>1.00</td><td>0.89</td></tr><tr><td><br />train</td><td>0.96</td><td>0.97</td><td>1.00</td><td>1.00</td><td>1.00</td></tr><tr><td><br />translation</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.97</td><td>0.98</td></tr><tr><td><br />tvchannel</td><td>0.60</td><td>0.69</td><td>0.70</td><td>0.83</td><td>0.92</td></tr><tr><td><br />video</td><td>0.66</td><td>0.62</td><td>0.67</td><td>0.77</td><td>0.79</td></tr><tr><td><br />weather</td><td>0.98</td><td>1.00</td><td>0.93</td><td>0.96</td><td>1.00</td></tr><tr><td><br />website</td><td>0.78</td><td>0.84</td><td>0.65</td><td>0.82</td><td>0.93</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">3.4.2 实验分析</h4>
                <div class="p1">
                    <p id="134">从表2中可以看出:相比其他4种方法 (SVM, CNN, LSTM和IndRNN) , 本文基于IndRNN-Attention的方法取得了最好的分类效果, 在<i>P</i><sub>macro</sub>, <i>R</i><sub>macro</sub>, <i>F</i><sub>macro</sub>这3个指标上有显著的提高.通过对不同方法与本文方法在31个类别上的<i>F</i>值进行统计<i>t</i>检验发现, 本文方法在所有类别上显著地 (<i>p</i>&lt;0.01) 超过了SVM, CNN和LSTM这3种方法, 而与IndRNN方法在统计上没有显著差异 (<i>p</i>=0.067) , 但在19个类别上, 本文方法相对于IndRNN方法有一定提高.</p>
                </div>
                <div class="p1">
                    <p id="135">从表2中可以看出, 基于SVM的分类方法分类结果最差.可能的原因有2点:</p>
                </div>
                <div class="p1">
                    <p id="136">1) 传统机器学习方法基于特征工程, 以词或者句法结构作为分类特征, 难以考虑句子中词之间的时序关系, 也很难将句子表达的语义信息通过固定的分类特征来进行体现.如“上海到北京的火车”和“刚下火车, 到上海了”, 2句表达在用词和结构上基本相似, 但是表达的意图不同, 使用传统的机器学习方法很难将其进行准确分类.</p>
                </div>
                <div class="p1">
                    <p id="137">2) 验证集和测试集中的部分句子 (验证集23条、测试集24条) 分词后的结果不在通过训练集构造的词典中, 导致这些句子的特征表示结果是一个全零向量, 影响了模型的训练和测试.</p>
                </div>
                <div class="p1">
                    <p id="138">在基于CNN的分类实验中, 本文采用Kim<citation id="240" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出CNN分类结构.CNN可以抽取句子中丰富的局部特征, 提高文本表示质量, 但是CNN对句子整体的语义结构和上下文时序信息表达不够, 对句子的语义表达不全面, 因此分类结果较基于SVM的方法在性能上有所提高, 但比其他3种方法 (LSTM, IndRNN和IndRNN-Attention) 要低.</p>
                </div>
                <div class="p1">
                    <p id="139">利用LSTM网络, 不仅考虑到了用户聊天文本的上下文时序关系对语义的影响, 还可以在一定长度范围内有效处理长期依赖问题.本文采用双层LSTM网络结构对用户聊天文本进行编码, 取隐层最后的输出作为句子的编码表示并通过softmax层得到分类结果.</p>
                </div>
                <div class="p1">
                    <p id="140">实验结果表明:基于LSTM的分类方法比基于SVM和CNN的分类方法性能有所提高, 但存在一定的问题.LSTM网络需要依赖原始的词向量输入, 无法从聊天文本中挖掘句子表达的深层隐含信息, 同时存在“梯度消失”和“梯度爆炸”等问题.对比表3发现, 基于LSTM的分类方法在部分类别上的性能提高不明显, 反而有所下降, 如“calc”, “website”等类别.通过分析实验结果, 我们发现基于LSTM的分类方法将“你知不知道三十四加六十五等于多少”和“帮我算一算45的平方根”等表达错分到“chat”类别中, 而它们的真实标签是“calc”.</p>
                </div>
                <div class="p1">
                    <p id="141">针对这些已有分类方法中出现的问题, 本文提出了多层独立循环神经网络和词级别注意力机制融合的用户意图分类方法 (即IndRNN-Attention) , 通过堆叠多层IndRNN网络, 构建了一个更深更长的网络结构对用户聊天文本编码, 有效地解决了语义信息获取不足、梯度消失和梯度爆炸等问题;词级别注意力机制的使用显著地提高了对聊天文本的编码质量, 增加了对文本含义表达有重要贡献的相关词汇的贡献度, 提高了分类性能.图5是attention权重的可视化示例.</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907016_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SMP2017-ECDT数据集中一条用户输入的注意力权重示例" src="Detail/GetImg?filename=images/JFYZ201907016_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 SMP2017-ECDT数据集中一条用户输入的注意力权重示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907016_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Attention based weighting sample for a sentence from SMP2017-ECDT dataset</p>

                </div>
                <h4 class="anchor-tag" id="143" name="143">3.4.3 分类错误原因分析</h4>
                <div class="p1">
                    <p id="144">表4是本文提出方法在测试集上分类错误的代表性示例:</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表4 分类错误示例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Classification Error Samples</b></p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />Input Message</td><td>True Lable</td><td>Predicted Lable</td></tr><tr><td><br />我想看电影台北飘雪</td><td>video</td><td>cinemas</td></tr><tr><td><br />可以帮我买火车票吗</td><td>chat</td><td>train</td></tr><tr><td><br />搜索米聊</td><td>website</td><td>app</td></tr><tr><td><br />鼻息肉</td><td>health</td><td>cookbook</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">分类错误的原因可归纳为3个方面:</p>
                </div>
                <div class="p1">
                    <p id="147">1) 部分类别之间的领域相似度高, 用户输入的上下文信息严重不足.通过对比实验数据集发现, “website”和“app”, “video”和“cinemas”等类别之间的领域差异较小.对于用户输入“搜索米聊”, 其中的上下文信息较少, 用户的确切意图难以区分, 表达的意思有可能是想查找手机中安装的“米聊”软件, 也有可能是想在浏览器中搜索“米聊”软件的信息.在这种情况下, 仅根据已有的文本信息无法做出正确的分类, 需要结合额外的场景信息或上下文信息来做出判断.</p>
                </div>
                <div class="p1">
                    <p id="148">2) 训练数据类别分布不平衡.训练集整体的规模较小, 但类别偏多, 训练集在各类别上的数据分布严重不平衡, 如“chat”, “cookbook”, “video”等类别的训练数据有几百条, 而“bus”, “calc”, “datetime”等类别仅包含十几条.在这种情况下, 对某些类别或者领域来说, 给定的训练数据远远无法覆盖领域所包含的大量专业术语, 如“health”类中的术语“鼻息肉”在该类别的训练数据集中不存在, 从而在测试时模型将“鼻息肉”错分到“cookbook”类别.</p>
                </div>
                <div class="p1">
                    <p id="149">3) 数据集中存在噪声数据.数据集中存在部分如“瓷我要上QQ”、“天最新新闻”和“你是不是”等表达错误或表达不规范的句子, 这样的句子影响模型的训练效果, 也影响测试性能.</p>
                </div>
                <h3 id="150" name="150" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="151">本文提出了一种基于独立循环神经网络和注意力机制的用户意图分类方法, 通过构造一个多层独立循环神经网络模型和融合词级别注意力机制的网络结构实现对用户聊天文本的编码, 解决了传统循环神经网络中出现的“梯度消失”和“梯度爆炸”等问题, 提高了领域相关词汇对用户聊天文本编码的贡献度, 增强了句子表示能力.在SMP2017-ECDT评测数据上的实验表明, 本文方法取得了最好的分类效果, <i>F</i><sub>macro</sub>值达到0.93, 与本文实验中其他4种方法相比显著提高了用户意图分类的效果.</p>
                </div>
                <div class="p1">
                    <p id="152">在后续工作中, 我们将尝试在神经网络结构中引入各类外部语义知识库来进一步提高用户意图分类的性能.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="250" type="formula" href="images/JFYZ201907016_25000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张志昌</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="251" type="formula" href="images/JFYZ201907016_25100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张珍文</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="252" type="formula" href="images/JFYZ201907016_25200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张治满</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="178">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=User intention understanding from scratch">

                                <b>[1]</b>Wang Ziheng, Qi Yonggang, Liu Jun.User intention understanding from scratch[C] //Proc of the 1st Int Workshop on Sensing, Processing and Learning for Intelligent Machines.Piscataway, NJ:IEEE, 2016:1- 4
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention is All You Need">

                                <b>[2]</b>Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C] //Proc of the 31st Conf on Neural Information Processing Systems.New York:Curran Associates, 2017:5998- 6008
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent semantic modeling for slot filling in conversational understanding">

                                <b>[3]</b>Tur G, Celikyilmaz A, Hakkani-Tür D.Latent semantic modeling for slot filling in conversational understanding[C] //Proc of the 38th Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:8307- 8311
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Independently Recurrent Neural Network(IndRNN):Building A Longer and Deeper RNN">

                                <b>[4]</b>Li Shuai, Li Wangqing, Cook C, et al.Independently recurrent neural network (IndRNN) :Building A longer and deeper RNN[C] //Proc of the 2018 Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2018:5457- 5466
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of statistical user simulation techniques for reinforcement- learning of dialogue management strategies">

                                <b>[5]</b>Schatzmann J, Weilhammer K, Stuttle M, et al.A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies[J].Knowledge Engineering Review, 2006, 21 (2) :97- 126
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The first evaluation of Chinese human-computer dialogue technology">

                                <b>[6]</b>Zhang Weinan, Chen Zhigang, Che Wanxiang, et al.The first evaluation of Chinese human-computer dialogue technology[J].arXiv preprint, arXiv:1709.10217, 2017
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identifying user needs from social media">

                                <b>[7]</b>Yang Huahai, Li Yunyao.Identifying user needs from social media[R].San Jose, CA:IBM Research Division, 2013
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weakly-supervised consumption intent detection in microblogs">

                                <b>[8]</b>Fu Bo, Liu Ting.Weakly-supervised consumption intent detection in microblogs[J].Journal of Computational Information Systems, 2013, 6 (9) :2423- 2431
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intent classification using feature sets for domestic violence discourse on social media">

                                <b>[9]</b>Subramani S, Vu H Q, Wang Hua.Intent classification using feature sets for domestic violence discourse on social media[C] //Proc of the 4th Asia-Pacific World Congress on Computer Science and Engineering.Piscataway, NJ:IEEE, 2017:129- 136
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600034477&amp;v=MDY1MzJmYks4SHRmTnFZOUZaT2dMQ0hzK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUlsMGRhUkk9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Chenlo J M, Losada D E.An empirical study of sentence features for subjectivity and polarity classification[J].Information Sciences, 2014, 280:275- 288
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                Pang Bo, Lee L, Vaithyanathan S.Thumbs up?Sentiment classification using machine learning techniques[C] //Proc of EMNLP'02.Stroudsburg, PA:ACL, 2002:79- 86
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Baselines and bigrams:Simple,good sentiment and topic classification">

                                <b>[12]</b>Wang Sida, Manning C D.Baselines and bigrams:Simple, good sentiment and topic classification[C] //Proc of Association for Computational Linguistics.Stroudsburg, PA:ACL, 2012:90- 94
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Consumption Pattern Recognition System Based on SVM">

                                <b>[13]</b>Huang Dazhen, Huang Zhihua.Consumption pattern recognition system based on SVM[C] //Proc of the 4th Int Conf on Intelligent Computation Technology and Automation (ICICTA) .Piscataway, NJ:IEEE, 2011:79- 82
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">

                                <b>[14]</b>Kim Y.Convolutional neural networks for sentence classification[J].arXiv preprint, arXiv:1408.5882, 2014
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201801016&amp;v=MjQzMDJ2U2RMRzRIOW5Ncm85RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5cm1XcnpJTHk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>Gao Yunlong, Zuo Wanli, Wang Ying, et al.Sentence classification model based on sparse and self-taught convolu-tional neural networks[J].Journal of Computer Research and Development, 2018, 55 (1) :179- 187 (in Chinese) (高云龙, 左万利, 王英, 等.基于稀疏自学习卷积神经网络的句子分类模型[J].计算机研究与发展, 2018, 55 (1) :179- 187) 
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating brush movements for Chinese calligraphy:a computer vision based approach">

                                <b>[16]</b>Xu Pengfei, Wang Lei, Guan Ziyu, et al.Evaluating brush movements for Chinese calligraphy:A computer vision based approach[C] //Proc of the 2018 Int Joint Conf on Artificial Intelligence (IJCAI) .San Francisco, CA:Morgan Kaufmann, 2018:1050- 1056
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Query intent detection using convolutional neural networks[C/OL]">

                                <b>[17]</b>Bhardwaj H B, Asiaee A, Kraft R.Query intent detection using convolutional neural networks[C/OL] //Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016[2018-08-22].http://people.cs.pitt.edu/～hashemi/papers/QRUMS2016_HBHashemi.pdf
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=User intent classification using memory networks:A comparative analysis for a limited data scenario">

                                <b>[18]</b>Bhardwaj A, Rudnicky A.User intent classification using memory networks:A comparative analysis for a limited data scenario[J].arXiv preprint, arXiv:1706.06160, 2017
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ONENET:Joint domain intent slot prediction for spoken language understanding">

                                <b>[19]</b>Kim Y B, Lee S, Stratos K.ONENET:Joint domain, intent, slot prediction for spoken language understanding[C] //Proc of Automatic Speech Recognition and Understanding Workshop.Piscataway, NJ:IEEE, 2017:547- 553
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MDI0MTVpclJkR2VycVFUTW53WmVadUh5am1VTHJJSWwwZGFSST1OaWZKWmJLOUh0ak1xbzlGWk9vTERYVXhvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735- 1780
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of gated recurrent neural networks on sequence modeling">

                                <b>[21]</b>Chung J, Gulcehre C, Cho K H, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[J].arXiv preprint, arXiv:1412.3555, 2014
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="4" href="javascript:void(0)">
                            <b>1</b> https://en.wikipedia.org/wiki/Siri
                        </span>
                    </p>
                    <p>
                        <span id="6" href="javascript:void(0)">
                            <b>2</b> https://en.wikipedia.org/wiki/Google_Now
                        </span>
                    </p>
                    <p>
                        <span id="8" href="javascript:void(0)">
                            <b>3</b> https://en.wikipedia.org/wiki/Cortana
                        </span>
                    </p>
                    <p>
                        <span id="10" href="javascript:void(0)">
                            <b>4</b> http://alixiaomi.com
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201907016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907016&amp;v=MDk2MTZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXJtV3J6SUx5dlNkTEc0SDlqTXFJOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSW1PTEZwOVR5TTZQcVNTTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

