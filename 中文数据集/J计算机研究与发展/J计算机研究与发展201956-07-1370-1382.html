

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127891709493750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201907002%26RESULT%3d1%26SIGN%3dJMtmQKTQoHxu4SttarTw5RH1vto%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907002&amp;v=MTQwMjRSTE9lWmVSckZ5dmhXN3pBTHl2U2RMRzRIOWpNcUk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;1 背景知识&lt;/b&gt; "><b>1 背景知识</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="&lt;b&gt;2 共享和私有信息最大化的跨媒体聚类&lt;/b&gt; "><b>2 共享和私有信息最大化的跨媒体聚类</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="&lt;b&gt;2.1 混合单词模型&lt;/b&gt;"><b>2.1 混合单词模型</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;2.2 聚类集成模型&lt;/b&gt;"><b>2.2 聚类集成模型</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;2.3 SPIM算法的目标函数&lt;/b&gt;"><b>2.3 SPIM算法的目标函数</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;2.4 SPIM算法目标函数的优化&lt;/b&gt;"><b>2.4 SPIM算法目标函数的优化</b></a></li>
                                                <li><a href="#221" data-title="&lt;b&gt;2.5 算法分析&lt;/b&gt;"><b>2.5 算法分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#237" data-title="&lt;b&gt;3 实验与性能分析&lt;/b&gt; "><b>3 实验与性能分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#238" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#246" data-title="&lt;b&gt;3.2 评价指标&lt;/b&gt;"><b>3.2 评价指标</b></a></li>
                                                <li><a href="#255" data-title="&lt;b&gt;3.3 对比方法&lt;/b&gt;"><b>3.3 对比方法</b></a></li>
                                                <li><a href="#267" data-title="&lt;b&gt;3.4 实验结果分析&lt;/b&gt;"><b>3.4 实验结果分析</b></a></li>
                                                <li><a href="#280" data-title="&lt;b&gt;3.5 参数分析&lt;/b&gt;"><b>3.5 参数分析</b></a></li>
                                                <li><a href="#283" data-title="&lt;b&gt;3.6 收敛性分析&lt;/b&gt;"><b>3.6 收敛性分析</b></a></li>
                                                <li><a href="#286" data-title="&lt;b&gt;3.7 模型简化测试&lt;/b&gt;"><b>3.7 模型简化测试</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#296" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="图1 典型的跨媒体数据">图1 典型的跨媒体数据</a></li>
                                                <li><a href="#91" data-title="图2 SPIM方法示意图">图2 SPIM方法示意图</a></li>
                                                <li><a href="#102" data-title="图3 IB算法的模型图">图3 IB算法的模型图</a></li>
                                                <li><a href="#113" data-title="图4 不同模态的聚类划分间的互信息">图4 不同模态的聚类划分间的互信息</a></li>
                                                <li><a href="#266" data-title="&lt;b&gt;表1 不同算法的时间复杂度对比&lt;/b&gt;"><b>表1 不同算法的时间复杂度对比</b></a></li>
                                                <li><a href="#273" data-title="图5 IB算法在不同媒体源上的聚类结果">图5 IB算法在不同媒体源上的聚类结果</a></li>
                                                <li><a href="#274" data-title="&lt;b&gt;表2 在6种跨媒体数据集上的&lt;i&gt;ACC&lt;/i&gt;对比结果&lt;/b&gt; %"><b>表2 在6种跨媒体数据集上的<i>ACC</i>对比结果</b> %</a></li>
                                                <li><a href="#275" data-title="&lt;b&gt;表3 在6种跨媒体数据集上的&lt;i&gt;NMI&lt;/i&gt;对比结果&lt;/b&gt; %"><b>表3 在6种跨媒体数据集上的<i>NMI</i>对比结果</b> %</a></li>
                                                <li><a href="#276" data-title="图6 平衡参数&lt;i&gt;λ&lt;/i&gt;对SPIM算法聚类结果的影响">图6 平衡参数<i>λ</i>对SPIM算法聚类结果的影响</a></li>
                                                <li><a href="#282" data-title="图7 SPIM算法在不同数据集上达到收敛时的迭代次数">图7 SPIM算法在不同数据集上达到收敛时的迭代次数</a></li>
                                                <li><a href="#285" data-title="图8 SPIM算法在&lt;i&gt;NMI&lt;/i&gt;指标上的模型简化测试">图8 SPIM算法在<i>NMI</i>指标上的模型简化测试</a></li>
                                                <li><a href="#295" data-title="&lt;b&gt;表4 SPIM算法在&lt;i&gt;ACC&lt;/i&gt;指标上的模型简化测试&lt;/b&gt; %"><b>表4 SPIM算法在<i>ACC</i>指标上的模型简化测试</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="349">


                                    <a id="bibliography_1" title="Kumar A, Daume H.A co-training approach for multi-view spectral clustering[C] //Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:393- 400" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A co-training approach for multi-view spectral clustering">
                                        <b>[1]</b>
                                        Kumar A, Daume H.A co-training approach for multi-view spectral clustering[C] //Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:393- 400
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_2" title="Cai Xiao, Nie Feiping, Huang Heng, et al.Heterogeneous image feature integration via multi-modal spectral clustering[C] //Proc of the 24th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:1977- 1984" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Heterogeneous image feature integration via multi-modal spectral clustering">
                                        <b>[2]</b>
                                        Cai Xiao, Nie Feiping, Huang Heng, et al.Heterogeneous image feature integration via multi-modal spectral clustering[C] //Proc of the 24th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:1977- 1984
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_3" title="Zhang Hong, Wu Fei, Zhuang Yueting.Cross-media correlation reasoning and retrieval[J].Journal of Computer Research and Development, 2008, 45 (5) :869- 876 (in Chinese) (张鸿, 吴飞, 庄越挺.跨媒体相关性推理与检索研究[J].计算机研究与发展, 2008, 45 (5) :869- 876) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200805023&amp;v=MjgyMDl0bk1xbzlIWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2aFc3ekFMeXZTZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Zhang Hong, Wu Fei, Zhuang Yueting.Cross-media correlation reasoning and retrieval[J].Journal of Computer Research and Development, 2008, 45 (5) :869- 876 (in Chinese) (张鸿, 吴飞, 庄越挺.跨媒体相关性推理与检索研究[J].计算机研究与发展, 2008, 45 (5) :869- 876) 
                                    </a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_4" title="Zhang Lei, Zhao Yao, Zhu Zhenfeng.Advances in semantically shared subspace learning for cross-media data[J].Chinese Journal of Computers, 2017, 40 (6) :1394- 1421 (in Chinese) (张磊, 赵耀, 朱振峰.跨媒体语义共享子空间学习研究进展[J].计算机学报, 2017, 40 (6) :1394- 1421) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706011&amp;v=Mjc1NDNCZHJHNEg5Yk1xWTlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2aFc3ekFMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Zhang Lei, Zhao Yao, Zhu Zhenfeng.Advances in semantically shared subspace learning for cross-media data[J].Chinese Journal of Computers, 2017, 40 (6) :1394- 1421 (in Chinese) (张磊, 赵耀, 朱振峰.跨媒体语义共享子空间学习研究进展[J].计算机学报, 2017, 40 (6) :1394- 1421) 
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_5" title="Chaudhuri K, Kakade M, Livescu K, et al.Multi-view clustering via canonical correlation analysis[C] //Proc of the 26th Int Conf on Machine Learning.New York:ACM, 2009:129- 136" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view clustering via canonical correlation analysis">
                                        <b>[5]</b>
                                        Chaudhuri K, Kakade M, Livescu K, et al.Multi-view clustering via canonical correlation analysis[C] //Proc of the 26th Int Conf on Machine Learning.New York:ACM, 2009:129- 136
                                    </a>
                                </li>
                                <li id="359">


                                    <a id="bibliography_6" title="Hardoon D R, Szedmak S, Shawe -Taylor J.Canonical correlation analysis:An overview with application to learning methods[J].Neural Computation, 2004, 16 (12) :2639- 2664" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012263&amp;v=MTQyNTZUTW53WmVadUh5am1VTHJJSTFvY2FSbz1OaWZKWmJLOUh0ak1xbzlGWk9vTkRubzZvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Hardoon D R, Szedmak S, Shawe -Taylor J.Canonical correlation analysis:An overview with application to learning methods[J].Neural Computation, 2004, 16 (12) :2639- 2664
                                    </a>
                                </li>
                                <li id="361">


                                    <a id="bibliography_7" title="Sigal L, Memisevic R, Fleet D J.Shared kernel information embedding for discriminative inference[C] //Proc of the 22nd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2009:2852- 2859" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shared Kernel Information Embedding for discriminative inference">
                                        <b>[7]</b>
                                        Sigal L, Memisevic R, Fleet D J.Shared kernel information embedding for discriminative inference[C] //Proc of the 22nd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2009:2852- 2859
                                    </a>
                                </li>
                                <li id="363">


                                    <a id="bibliography_8" title="Carl H, Phipip P, Lawrence N D.Gaussian process latent variable models for human pose estimation[C] //Proc of the 4th Machine Learning for Multimodal Interaction.Berlin:Springer, 2007:132- 143" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gaussian process latent variable models for human pose estimation">
                                        <b>[8]</b>
                                        Carl H, Phipip P, Lawrence N D.Gaussian process latent variable models for human pose estimation[C] //Proc of the 4th Machine Learning for Multimodal Interaction.Berlin:Springer, 2007:132- 143
                                    </a>
                                </li>
                                <li id="365">


                                    <a id="bibliography_9" title="Barnard K, Forsyth D.Learning the semantics of words and pictures[C] //Proc of the 8th Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2001:408- 415" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning the semantics of words and pictures">
                                        <b>[9]</b>
                                        Barnard K, Forsyth D.Learning the semantics of words and pictures[C] //Proc of the 8th Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2001:408- 415
                                    </a>
                                </li>
                                <li id="367">


                                    <a id="bibliography_10" title="Hofmann T.Learning and representing topic—A hierarchical mixture model for word occurrence in document databases[C] //Proc of the Workshop on Learning from Text and the Web.Pittsburgh, PA:CMU, 1998" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning and Representing Topic--A Hierarchical Mixture for">
                                        <b>[10]</b>
                                        Hofmann T.Learning and representing topic—A hierarchical mixture model for word occurrence in document databases[C] //Proc of the Workshop on Learning from Text and the Web.Pittsburgh, PA:CMU, 1998
                                    </a>
                                </li>
                                <li id="369">


                                    <a id="bibliography_11" title="Barnard K, Duygulu P, Forsyth D, et al.Matching words and pictures[J].Journal of Machine Learning Research, 2003, 3 (2) :1107- 1135" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Matching words and pictures">
                                        <b>[11]</b>
                                        Barnard K, Duygulu P, Forsyth D, et al.Matching words and pictures[J].Journal of Machine Learning Research, 2003, 3 (2) :1107- 1135
                                    </a>
                                </li>
                                <li id="371">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                    Blei D M, Ng A Y, Jordan M I.Latent Dirichlet allocation[J].Journal of Machine Learning Research, 2003, 3 (1) :993- 1022</a>
                                </li>
                                <li id="373">


                                    <a id="bibliography_13" title="Gao Jing, Han Jiawei, Liu Jailu, et al.Multi-view clustering via joint nonnegative matrix factorization[C] //Proc of the 13th SIAM Int Conf on Data Mining.Philadelphia, PA:SIAM, 2013:252- 260" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view clustering via joint nonnegative matrix factorization">
                                        <b>[13]</b>
                                        Gao Jing, Han Jiawei, Liu Jailu, et al.Multi-view clustering via joint nonnegative matrix factorization[C] //Proc of the 13th SIAM Int Conf on Data Mining.Philadelphia, PA:SIAM, 2013:252- 260
                                    </a>
                                </li>
                                <li id="375">


                                    <a id="bibliography_14" title="Cai Xiao, Nie Feiping, Huang Heng.Multi-view k-means clustering on big data[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2013:2598- 2604" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view k-means clustering on big data">
                                        <b>[14]</b>
                                        Cai Xiao, Nie Feiping, Huang Heng.Multi-view k-means clustering on big data[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2013:2598- 2604
                                    </a>
                                </li>
                                <li id="377">


                                    <a id="bibliography_15" title="Strehl A, Ghosh A.Cluster ensembles-a knowledge reuse framework for combining multiple partitions[J].The Journal of Machine Learning Research, 2003 (3) :583- 617" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cluster ensembles - A knowledge reuse framework for combining multiple partitions">
                                        <b>[15]</b>
                                        Strehl A, Ghosh A.Cluster ensembles-a knowledge reuse framework for combining multiple partitions[J].The Journal of Machine Learning Research, 2003 (3) :583- 617
                                    </a>
                                </li>
                                <li id="379">


                                    <a id="bibliography_16" title="Huang Dong, Lai Jianhuang, Wang Changdong.Robust ensemble clustering using probability trajectories[J].IEEE Transactions on Knowledge and Data Engineering, 2016, 28 (5) :1312- 1326" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust ensemble clustering using probability trajectories">
                                        <b>[16]</b>
                                        Huang Dong, Lai Jianhuang, Wang Changdong.Robust ensemble clustering using probability trajectories[J].IEEE Transactions on Knowledge and Data Engineering, 2016, 28 (5) :1312- 1326
                                    </a>
                                </li>
                                <li id="381">


                                    <a id="bibliography_17" title="Huang Dong, Wang Changdong, Lai Jianhuang.Locally weighted ensemble clustering[J].IEEE Transactions on Cybernetics, 2017, 48 (5) :1460- 1473" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locally Weighted Ensemble Clustering">
                                        <b>[17]</b>
                                        Huang Dong, Wang Changdong, Lai Jianhuang.Locally weighted ensemble clustering[J].IEEE Transactions on Cybernetics, 2017, 48 (5) :1460- 1473
                                    </a>
                                </li>
                                <li id="383">


                                    <a id="bibliography_18" title="Lou Zhengzheng, Ye Yangdong, Yan Xiaoqiang.The multi-feature information bottleneck with application to unsupervised image categorization[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2013:1508- 1515" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The multi-feature information bottleneckwith application to unsupervised image categorization">
                                        <b>[18]</b>
                                        Lou Zhengzheng, Ye Yangdong, Yan Xiaoqiang.The multi-feature information bottleneck with application to unsupervised image categorization[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2013:1508- 1515
                                    </a>
                                </li>
                                <li id="385">


                                    <a id="bibliography_19" title="Yan Xiaoqiang, Ye Yangdong, Lou Zhengzheng.Unsuper-vised video categorization based on multivariate information bottleneck method[J].Knowledge-Based Systems, 2015, 84 (C) :34- 45" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES0DF6210F390424192B1AF125B1D626ED&amp;v=MTQ1ODIvckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3eTN3NkE9TmlmT2ZiUE1hTmZPcm84elorSVBDSDQ5emg4Um1ENE1QbjdncVdBMERiU1dROA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        Yan Xiaoqiang, Ye Yangdong, Lou Zhengzheng.Unsuper-vised video categorization based on multivariate information bottleneck method[J].Knowledge-Based Systems, 2015, 84 (C) :34- 45
                                    </a>
                                </li>
                                <li id="387">


                                    <a id="bibliography_20" title="Luo Peng, Peng Jinye, Guan Ziyu, et al.Multi-view semantic learning for data representation[J].IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (11) :3016- 3028" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view semantic learning for data representation">
                                        <b>[20]</b>
                                        Luo Peng, Peng Jinye, Guan Ziyu, et al.Multi-view semantic learning for data representation[J].IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (11) :3016- 3028
                                    </a>
                                </li>
                                <li id="389">


                                    <a id="bibliography_21" title="Zhao Qi, Li Zongmin, Cross-modal social image clustering[J].Chinese Journal of Computers, 2018, 41 (1) :98- 111 (in Chinese) (赵其鲁, 李宗民.跨模态社交图像聚类[J].计算机学报, 2018, 41 (1) :98- 111) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801006&amp;v=MTE1NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2aFc3ekFMejdCZHJHNEg5bk1ybzlGWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Zhao Qi, Li Zongmin, Cross-modal social image clustering[J].Chinese Journal of Computers, 2018, 41 (1) :98- 111 (in Chinese) (赵其鲁, 李宗民.跨模态社交图像聚类[J].计算机学报, 2018, 41 (1) :98- 111) 
                                    </a>
                                </li>
                                <li id="391">


                                    <a id="bibliography_22" title="Peng Yuxin, Qi Jinwei, Huang Xin, et al.CCL:Cross-modal correlation learning with multi-grained fusion by hierarchical network[J].IEEE Transactions on Multimedia, 2018, 20 (2) :405- 420" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CCL:cross-modal correlation learning with multigrained fusion by hierarchical network">
                                        <b>[22]</b>
                                        Peng Yuxin, Qi Jinwei, Huang Xin, et al.CCL:Cross-modal correlation learning with multi-grained fusion by hierarchical network[J].IEEE Transactions on Multimedia, 2018, 20 (2) :405- 420
                                    </a>
                                </li>
                                <li id="393">


                                    <a id="bibliography_23" title="Peng Yuxin, Huang Xin, Qi Jinwei.Cross-media shared representation by hierarchical learning with multiple deep networks[C] //Proc of the 25th Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2016:3846- 3853" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-media shared representation by hierarchical learning with multiple deep networks">
                                        <b>[23]</b>
                                        Peng Yuxin, Huang Xin, Qi Jinwei.Cross-media shared representation by hierarchical learning with multiple deep networks[C] //Proc of the 25th Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2016:3846- 3853
                                    </a>
                                </li>
                                <li id="395">


                                    <a id="bibliography_24" title="Tishby N, Pereira F, Bialek W.The information bottleneck method[C] //Proc of the 37th Allerton Conf on Communication, Control and Computing.Piscataway, NJ:IEEE, 1999:368- 377" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The information bottleneck method">
                                        <b>[24]</b>
                                        Tishby N, Pereira F, Bialek W.The information bottleneck method[C] //Proc of the 37th Allerton Conf on Communication, Control and Computing.Piscataway, NJ:IEEE, 1999:368- 377
                                    </a>
                                </li>
                                <li id="397">


                                    <a id="bibliography_25" title="Slonim N.The information bottleneck:Theory and applications[D].Hebrew, IL:Hebrew University, 2002" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The information bottleneck:Theory and applications">
                                        <b>[25]</b>
                                        Slonim N.The information bottleneck:Theory and applications[D].Hebrew, IL:Hebrew University, 2002
                                    </a>
                                </li>
                                <li id="399">


                                    <a id="bibliography_26" title="Yan Xiaoqiang, Hu Shizhe, Ye Yangdong.Multi-task clustering of human actions by sharing information[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6401- 6409" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-task clustering of human actions by sharing information">
                                        <b>[26]</b>
                                        Yan Xiaoqiang, Hu Shizhe, Ye Yangdong.Multi-task clustering of human actions by sharing information[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6401- 6409
                                    </a>
                                </li>
                                <li id="401">


                                    <a id="bibliography_27" title="Svetlana L, Cordelia S, Jean P.Beyond bags of features:Spatial pyramid matching for recognizing natural scene categories[C] //Proc of the 19th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2006:2169- 2178" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond bags of features:Spatial pyramid matching forrecognizing natural scene categories">
                                        <b>[27]</b>
                                        Svetlana L, Cordelia S, Jean P.Beyond bags of features:Spatial pyramid matching for recognizing natural scene categories[C] //Proc of the 19th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2006:2169- 2178
                                    </a>
                                </li>
                                <li id="403">


                                    <a id="bibliography_28" title="Rasiwasia N, Pereira J C, Coviello E, et al.A new approach to cross-modal multimedia retrieval[C] //Proc of the 18th ACM Int Conf on Multimedia.New York:ACM, 2010:251- 260" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A New Approach to Cross-modal Multimedia Retrieval">
                                        <b>[28]</b>
                                        Rasiwasia N, Pereira J C, Coviello E, et al.A new approach to cross-modal multimedia retrieval[C] //Proc of the 18th ACM Int Conf on Multimedia.New York:ACM, 2010:251- 260
                                    </a>
                                </li>
                                <li id="405">


                                    <a id="bibliography_29" title="Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91- 110" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTE5NDk3cWVidWR0RkMzbFVMdkFJMVk9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                        Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91- 110
                                    </a>
                                </li>
                                <li id="407">


                                    <a id="bibliography_30" title="Rashtchian C, Young P, Hodosh M, et al.Collecting image annotations using Amazon&#39;s Mechanical Turk[C] //Proc of the Workshop on Creating Speech and Language Data with Amazon&#39;s Mechanical Turk.Stroudsburg, PA:ACL, 2010:139- 147" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collecting image annotations using Amazon&amp;#39;&amp;#39;s Mechanical Turk">
                                        <b>[30]</b>
                                        Rashtchian C, Young P, Hodosh M, et al.Collecting image annotations using Amazon&#39;s Mechanical Turk[C] //Proc of the Workshop on Creating Speech and Language Data with Amazon&#39;s Mechanical Turk.Stroudsburg, PA:ACL, 2010:139- 147
                                    </a>
                                </li>
                                <li id="409">


                                    <a id="bibliography_31" title="Everingham M, Gool L V, Williams C K I, et al.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303- 338" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MzIyODQ0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzNsVUx2QUkxWT1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRo&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                        Everingham M, Gool L V, Williams C K I, et al.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303- 338
                                    </a>
                                </li>
                                <li id="411">


                                    <a id="bibliography_32" title="Hwang S J, Grauman K.Accounting for the relative importance of objects in image retrieval[C] //Proc of the British Machine Vision Conf.Berlin:Springer, 2010:1- 12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accounting for the Relative Importance of Objects in Image Re-trieval">
                                        <b>[32]</b>
                                        Hwang S J, Grauman K.Accounting for the relative importance of objects in image retrieval[C] //Proc of the British Machine Vision Conf.Berlin:Springer, 2010:1- 12
                                    </a>
                                </li>
                                <li id="413">


                                    <a id="bibliography_33" title="Peng Yuxin, Zhai Xiaohua, Zhao Yunchao, et al.Semi-supervised cross-media feature learning with unified patch graph regularization[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2016, 26 (3) :583- 596" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-Supervised Cross-Media Feature Learning With Unified Patch Graph Regularization">
                                        <b>[33]</b>
                                        Peng Yuxin, Zhai Xiaohua, Zhao Yunchao, et al.Semi-supervised cross-media feature learning with unified patch graph regularization[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2016, 26 (3) :583- 596
                                    </a>
                                </li>
                                <li id="415">


                                    <a id="bibliography_34" title="Zhai Xiaohua, Peng Yuxin, Xiao Jianguo.Learning cross-media joint representation with sparse and semisupervised regularization[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2014, 24 (6) :965- 978" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Cross-media Joint Representation with Sparse and Semi-supervised Regularization">
                                        <b>[34]</b>
                                        Zhai Xiaohua, Peng Yuxin, Xiao Jianguo.Learning cross-media joint representation with sparse and semisupervised regularization[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2014, 24 (6) :965- 978
                                    </a>
                                </li>
                                <li id="417">


                                    <a id="bibliography_35" title="Peng Yuxin, Huang Xin, Zhao Yunchao.An overview of cross-media retrieval:Concepts, methodologies, benchmarks and challenges[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2017, 28 (9) :2372- 2385" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An overview of cross-media retrieval:concepts,methodologies,benchmarks and challenges">
                                        <b>[35]</b>
                                        Peng Yuxin, Huang Xin, Zhao Yunchao.An overview of cross-media retrieval:Concepts, methodologies, benchmarks and challenges[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2017, 28 (9) :2372- 2385
                                    </a>
                                </li>
                                <li id="419">


                                    <a id="bibliography_36" title="Kuehne H, Jhuang H, Garrote E, et al.HMDB:A large video database for human motion recognition[C] //Proc of the 13th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2011:2556- 2563" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HMDB:A large video database for human motion recognition">
                                        <b>[36]</b>
                                        Kuehne H, Jhuang H, Garrote E, et al.HMDB:A large video database for human motion recognition[C] //Proc of the 13th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2011:2556- 2563
                                    </a>
                                </li>
                                <li id="421">


                                    <a id="bibliography_37" title="Laptev I, Marszalek M, Schmid C, et al.Learning realistic human actions from movies[C] //Proc of the 21st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1- 8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Realistic Human Actions from Movies">
                                        <b>[37]</b>
                                        Laptev I, Marszalek M, Schmid C, et al.Learning realistic human actions from movies[C] //Proc of the 21st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1- 8
                                    </a>
                                </li>
                                <li id="423">


                                    <a id="bibliography_38" title="Karypis G, Kumar V.A fast and high quality multilevel scheme for partitioning irregular graphs[J].SIAM Journal on Scientific Computing, 1998, 20 (1) :359- 392" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and high quality multilevel scheme for partitioning irregular graphs">
                                        <b>[38]</b>
                                        Karypis G, Kumar V.A fast and high quality multilevel scheme for partitioning irregular graphs[J].SIAM Journal on Scientific Computing, 1998, 20 (1) :359- 392
                                    </a>
                                </li>
                                <li id="425">


                                    <a id="bibliography_39" title="Chang Shifu, Wu Xiaoming, Li Zhenguo.Segmentation using superpixels:A bipartite graph partitioning approach[C] //Proc of the 25th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:789- 796" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentation using superpixels:A bipartite graph partitioning approach">
                                        <b>[39]</b>
                                        Chang Shifu, Wu Xiaoming, Li Zhenguo.Segmentation using superpixels:A bipartite graph partitioning approach[C] //Proc of the 25th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:789- 796
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(07),1370-1382 DOI:10.7544/issn1000-1239.2019.20180470            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>共享和私有信息最大化的跨媒体聚类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%97%AB%E5%B0%8F%E5%BC%BA&amp;code=32131236&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闫小强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E9%98%B3%E4%B8%9C&amp;code=09467692&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶阳东</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%83%91%E5%B7%9E%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0048752&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑州大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>近年来, 具有典型多源异构特性的跨媒体数据的快速涌现给数据分析带来巨大挑战.然而, 绝大多数现有跨媒体数据分析方法仅依赖模态间的共享信息发掘跨媒体数据中蕴含的模式结构, 忽略各模态自身的重要信息.针对此问题, 提出共享和私有信息最大化 (share and private information maximization) 的跨媒体聚类算法, 通过兼顾跨媒体数据的共享和私有信息, 以求得更加合理的聚类模式.首先, 提出2种跨媒体数据的共享信息构建模型:1) 混合单词模型, 该模型将各模态的底层特征转换为统一的词频向量表示, 然后使用一种新的自凝聚信息最大化方法自底向上地构建多模态的混合单词空间, 最大化地保持各模态底层特征的统计相似性;2) 聚类集成模型, 构建各模态自身的聚类划分, 通过互信息度量各模态聚类划分间的信息量, 抽取各模态的高层聚类划分之间的相关性.其次, 提出基于信息论的目标函数, 将跨媒体数据的共享和私有信息融合在同一目标函数中, 在抽取聚类模式结构的过程中兼顾跨媒体数据的共享和私有信息.最后, 采用顺序“抽取-合并”过程优化SPIM算法的目标函数, 保证其收敛到局部最优解.在6种跨媒体数据上的实验结果表明SPIM算法的优越性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%A8%E5%AA%92%E4%BD%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跨媒体;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%BA%90%E5%BC%82%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多源异构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B1%E4%BA%AB%E5%92%8C%E7%A7%81%E6%9C%89%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">共享和私有信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E6%81%AF%E6%9C%80%E5%A4%A7%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息最大化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%92%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">互信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类分析;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Yan Xiaoqiang, born in 1989. PhD. Member of CCF. His main research interests include machine learning, computer vision, pattern recognition and  data mining. iexqyan@zzu.edu.cn;
                                </span>
                                <span>
                                    *Ye Yangdong, born in 1962. PhD, professor, PhD supervisor at Zhengzhou University.Senior member of CCF.His main research interests includes machine learning, pattern recognition, knowledge engineering and intelligent system. ieydye@zzu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2018YFB1201403);</span>
                                <span>国家自然科学基金项目 (61772475, 61502434);</span>
                    </p>
            </div>
                    <h1><b>Cross-Media Clustering by Share and Private Information Maximization</b></h1>
                    <h2>
                    <span>Yan Xiaoqiang</span>
                    <span>Ye Yangdong</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering, Zhengzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Recently, the rapid emergence of cross media data with typical multi-source and heterogeneous characteristic brings great challenges to the traditional data analysis approaches. However, the most of existing approaches for cross media data heavily rely on the shared latent feature space to construct the relationships between multiple modalities, while ignoring the private information hidden in each modality. Aiming at this problem, this paper proposes a novel share and private information maximization (SPIM) algorithm for cross media data clustering, which leverages the shared and private information into the clustering process. Firstly, we present two shared information construction models: 1) Hybrid words (H-words) model. In this model, the low-level features in each modality are transformed into words or visual words co-occurrence vector, then a novel agglomerative information maximization is presented to build the hybrid word space for all modalities, which ensures the statistical correlation between the low-level features of multiple modalities. 2) Clustering ensemble (CE) model. This model adopts the mutual information to measure the similarity between the clustering partitions of different modalities, which ensures the semantic correlation of the high-level clustering partitions. Secondly, SPIM algorithm integrates the shared information of multiple modalities and the private information of individual modalities into a unified objective function. Finally, the optimization of SPIM algorithm is performed by a sequential “draw-and-merge” procedure, which guarantees the function converge to a local maximum. The experimental results on 6 cross media datasets show that the proposed approach compares favorably with the existing state-of-the-art cross-media clustering methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cross-media&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cross-media;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-source%20heterogeneous&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-source heterogeneous;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=share%20and%20private%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">share and private information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=information%20maximization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">information maximization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mutual%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mutual information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clustering%20analyse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clustering analyse;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-27</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2018YFB1201403);</span>
                                <span>the National Natural Science Foundation of China (61772475, 61502434);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 典型的跨媒体数据" src="Detail/GetImg?filename=images/JFYZ201907002_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 典型的跨媒体数据  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The typical cross-media data</p>

                </div>
                <div class="p1">
                    <p id="84">聚类方法按照“物以类聚”的原则将数据对象划分为不同的簇, 并保持簇间数据元素间的距离尽可能地大、簇内数据元素间的距离尽可能地小, 进而抽取数据对象中蕴含的模式结构.聚类分析无须借鉴数据的先验知识, 仅根据数据的实际分布情况即可得到自然的数据划分, 在认识数据中的不确定性和价值的隐蔽性方面具有重要的研究价值.然而, 随着信息技术的迅猛发展和广泛应用, 具有典型多源异构特性的跨媒体数据已经遍布生活的各个角落.跨媒体数据是指以不同模态、来源、空间等形式出现, 但具有相似的高层语义的数据.如图1所示, 相同的新闻可以用多种语言进行报道<citation id="427" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>;同一幅图像可以在形状、纹理、颜色等特征空间上获得异构的描述<citation id="428" type="reference"><link href="351" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>;同一概念或事件可用图像、文本、视频、音频等不同类型的媒体共同表达<citation id="429" type="reference"><link href="353" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>;传统聚类方法在做数据分析时仅考虑单模态的数据信息, 已经无法适应跨媒体数据的特征异构性.</p>
                </div>
                <div class="p1">
                    <p id="85">跨媒体聚类 (cross-media clustering, CMC) <citation id="430" type="reference"><link href="355" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>是一种基于数据驱动的分析方法, 旨在根据不同模态数据的分布相似性, 同时聚类多个模态以揭示不同模态间的潜在关联.跨媒体聚类任务的核心问题在于捕捉多模态数据间的关联性, 以实现信息的跨模态共享.针对此问题, 较为直观的解决方法是寻找多模态数据的公共子空间.例如文献<citation id="431" type="reference">[<a class="sup">5</a>]</citation>提出基于典型关联分析 (canonical correlation analysis, CCA) <citation id="432" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的跨媒体聚类算法, 该算法将多个模态的特征投影到低维的子空间上;文献<citation id="433" type="reference">[<a class="sup">7</a>]</citation>使用共享核嵌入、文献<citation id="434" type="reference">[<a class="sup">8</a>]</citation>使用高斯过程隐式变量模型学习多模态间的共享特征.然而, 基于子空间的跨媒体聚类方法将各模态的特征映射到低维空间的同时, 会破坏跨媒体数据的原始结构, 导致一些重要信息的丢失.除了学习多模态共享子空间之外, 近年来相关研究人员也提出了一些行之有效的跨媒体聚类策略.文献<citation id="435" type="reference">[<a class="sup">9</a>]</citation>使用层次模型<citation id="436" type="reference"><link href="367" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> (hierarchical model) 自底向上地构建文本和视觉特征间的关联, 进而在聚类过程中结合跨媒体特征;文献<citation id="437" type="reference">[<a class="sup">11</a>]</citation>首先将图像分割为区域的集合, 此时, 若将图像视为文档, 则每个图像区域类似于文档中的单词, 之后通过主题模型隐含狄利克雷分布 (latent Drichlet allocation, LDA) <citation id="438" type="reference"><link href="371" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将文本与视觉信息转化为一种跨模态的向量表示;文献<citation id="439" type="reference">[<a class="sup">2</a>]</citation>提出多模态谱聚类算法, 自动地结合图像数据的多种异构特征表示;文献<citation id="440" type="reference">[<a class="sup">13</a>]</citation>提出多视角联合矩阵分解方法, 通过学习多视角数据的公共系数矩阵寻求多视角之间兼容的聚类划分;文献<citation id="441" type="reference">[<a class="sup">14</a>]</citation>提出鲁棒的多视角<i>k</i>-means方法, 用来处理大规模数据的多种异构特征表示.然而, 上述跨媒体数据聚类分析方法仅依赖各模态间的共享信息建立多模态数据的关联, 忽略了各模态自身的私有信息, 这显然与实际应用情况不符.</p>
                </div>
                <div class="p1">
                    <p id="86">另外, 机器学习领域中的集成聚类 (consensus clustering, CC) 方法可有效地处理多模态数据, 引起了跨媒体研究人员的关注.集成聚类方法在处理跨媒体数据时, 首先根据单模态的数据分布得到其自身的聚类划分 (基聚类) , 之后按照特定的合并准则将不同模态的聚类划分进行合并, 从而将多个模态的异构信息进行融合, 得到最终的聚类划分.例如文献<citation id="442" type="reference">[<a class="sup">15</a>]</citation>设计基于相似簇、超图、集群3种一致性度量函数合并基聚类;文献<citation id="443" type="reference">[<a class="sup">16</a>]</citation>提出基于稀疏图表示和概率轨迹的聚类集成算法, 同时根据基聚类的局部和全局信息获取最终的聚类划分;文献<citation id="444" type="reference">[<a class="sup">17</a>]</citation>在不考虑原始数据分布的情况下, 通过局部密度估计方法对基聚类进行加权, 进而区分基聚类的可依赖程度.然而, 现有的集成聚类在处理跨媒体数据时忽略原始数据的特征分布, 仅依赖基聚类构建最终的聚类划分, 导致最终的聚类划分过度依赖基聚类的质量.</p>
                </div>
                <div class="p1">
                    <p id="87">针对上述问题, 本文提出共享和私有信息最大化 (share and private information maximization, SPIM) 的跨媒体聚类算法.如图2所示, 该算法通过兼顾跨媒体数据间的共享信息和各媒体数据自身的私有信息进行聚类分析, 以求得更加合理的聚类模式结构.首先, 提出混合单词模型 (hybrid words model, H-words) 和聚类集成模型 (clustering ensemble model, CE) 构建跨媒体数据的2种共享信息, 分别保持各模态底层特征的统计相似性和各模态的高层聚类划分间的相关性.其次, 提出基于信息论的目标函数, 将跨媒体数据的共享和私有信息融合在同一目标函数中.同时处理各模态自身的私有信息 (原始特征) 和聚类集成模型构建的共享信息 (聚类划分) , 有助于克服集成聚类算法对基聚类的过度依赖.最后, 采用顺序“抽取-合并”优化过程, 保证SPIM算法的目标函数收敛到局部最优解.在6种跨媒体数据上的实验结果表明SPIM算法性能优于现有方法.本文的主要贡献总结为3个方面:</p>
                </div>
                <div class="p1">
                    <p id="88">1) 提出共享和私有信息最大化的跨媒体聚类算法SPIM, 该算法通过兼顾跨媒体数据的共享和私有信息, 以求得更加合理的模式结构.</p>
                </div>
                <div class="p1">
                    <p id="89">2) 提出2种跨媒体数据的共享信息构建模型:混合单词模型和聚类集成模型, 分别保持各模态底层特征的统计相似性和各模态的高层聚类划分间的相关性.</p>
                </div>
                <div class="p1">
                    <p id="90">3) 提出基于信息论的目标函数, 并采用顺序“抽取-合并”优化策略对该目标函数进行优化, 保证其收敛到局部最优解.</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SPIM方法示意图" src="Detail/GetImg?filename=images/JFYZ201907002_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 SPIM方法示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The illustration of SPIM method</p>

                </div>
                <div class="p1">
                    <p id="92">本文工作与文献<citation id="449" type="reference">[<a class="sup">18</a>,<a class="sup">19</a>]</citation>中的多特征信息瓶颈 (multi-feature information bottleneck, MfIB) 算法、文献<citation id="445" type="reference">[<a class="sup">20</a>]</citation>中的多视角概念学习 (multi-view concept learning, MCL) 算法、文献<citation id="446" type="reference">[<a class="sup">21</a>]</citation>中的跨媒体社交图像聚类 (cross-media social image clustering, CSIC) 算法关联较为密切.其中MfIB算法是信息瓶颈 (information bottlenede, IB) 算法的扩展, 该算法使用互信息量化聚类模式与多个特征间的信息量, 然而, 该算法要求多种特征表示来自同一数据分布, 无法处理跨模态的异构特征.MCL算法和CSIC算法采用多模态数据的共享特征和独有特征进行跨模态分析.其中, MCL算法学习多模态数据的概念性隐式空间, 并将该隐式空间分解为共享和独有2个部分.然而, 该算法是半监督算法, 无法处理聚类问题.CSIC算法将社交图像中视觉和社交标签信息的共享特征空间学习视为一个共轭词典学习问题, 通过L<sub>1, ∞</sub>范数的正则项保证各模态的词典稀疏化, 这种稀疏化使得各模态的独有特征得以保留.然而, 该算法需要借助WordNet获得辅助的社交语义关系, 仅适用图像和文本2种模态数据, 无法处理更多模态.近年来, 深度神经网络在跨媒体数据的一致性表征中取得了较好的结果, 例如多层次跨模态关联学习<citation id="447" type="reference"><link href="391" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、多网络共享表征<citation id="448" type="reference"><link href="393" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>.深度神经网络需要大量已标注的训练数据, 然而, 数据标签信息的获取是费时费力的过程.</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>1 背景知识</b></h3>
                <div class="p1">
                    <p id="94">IB算法<citation id="450" type="reference"><link href="395" rel="bibliography" /><link href="397" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>是一种典型的基于信息最大化的数据分析方法<citation id="451" type="reference"><link href="383" rel="bibliography" /><link href="385" rel="bibliography" /><link href="399" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">26</a>]</sup></citation>.给定源变量<b><i>X</i></b>与特征变量<b><i>Y</i></b>之间的联合概率分布<i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b>) , IB算法力图寻求源变量<b><i>X</i></b>的最优压缩表示<b><i>T</i></b>, 同时使压缩变量<b><i>T</i></b>最大化地保存特征变量<b><i>Y</i></b>中蕴含的关于源变量<b><i>X</i></b>的信息量.如图3所示, 源变量<b><i>X</i></b>与其特征变量<b><i>Y</i></b>之间的信息通过压缩变量<b><i>T</i></b>进行保存.IB算法可形式化描述为</p>
                </div>
                <div class="p1">
                    <p id="95"><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>R</mtext><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mo stretchy="false">{</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>:</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">) </mo><mo>≥</mo><mi>D</mi><mo stretchy="false">}</mo></mrow></munder><mi>Ι</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>;<b><i>T</i></b>) ,      (1) </p>
                </div>
                <div class="p1">
                    <p id="97">其中, <i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) 是源变量<b><i>X</i></b>到压缩变量<b><i>T</i></b>之间的编码方案, <i>I</i> (<b><i>X</i></b>;<b><i>T</i></b>) 是变量之间的互信息, <i>D</i>是<b><i>X</i></b>到<b><i>T</i></b>之间所有可能的编码方案.从式 (1) 可知, IB算法是在信息保存程度满足<i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b>) ≥<i>D</i>的条件下, 寻找能够最小化<i>I</i> (<b><i>X</i></b>;<b><i>T</i></b>) 的一个编码方案<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) .文献<citation id="452" type="reference">[<a class="sup">22</a>]</citation>给出IB算法的目标函数:</p>
                </div>
                <div class="p1">
                    <p id="98">L<sub>max</sub>[<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) ]=<i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b>) -<i>β</i><sup>-1</sup>·<i>I</i> (<b><i>T</i></b>;<b><i>X</i></b>) ,      (2) </p>
                </div>
                <div class="p1">
                    <p id="99">其中, <i>β</i>是拉格朗日乘数因子, 用来平衡数据对象的压缩与相关信息的保存.在聚类任务中, 簇的个数<i>M</i>往往远远小于原始数据对象的数量, 即<i>M</i>≪|<b><i>X</i></b>|, 这意味着源变量<b><i>X</i></b>与其压缩变量<b><i>T</i></b>之间存在大幅度压缩.因此, 实际应用中通常将<i>β</i>设置为无穷大.这种做法的有效性在多种数据类型的聚类任务中得到验证, 例如文本<citation id="453" type="reference"><link href="397" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、图像<citation id="454" type="reference"><link href="383" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、视频<citation id="455" type="reference"><link href="399" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>.因此, IB算法的目标函数可改写为</p>
                </div>
                <div class="p1">
                    <p id="100">L<sub>max</sub>[<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) ]=<i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b>) ,      (3) </p>
                </div>
                <div class="p1">
                    <p id="101">其中, 互信息<i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b>) 度量压缩变量<b><i>T</i></b>与特征变量<b><i>Y</i></b>之间的信息量.</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 IB算法的模型图" src="Detail/GetImg?filename=images/JFYZ201907002_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 IB算法的模型图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 The model of IB algorithm</p>

                </div>
                <h3 id="103" name="103" class="anchor-tag"><b>2 共享和私有信息最大化的跨媒体聚类</b></h3>
                <div class="p1">
                    <p id="104">本文提出共享和私有信息最大化的跨媒体聚类算法SPIM, 该算法旨在兼顾跨模态数据间的共享信息和各模态自身的私有信息进行跨媒体聚类分析, 以求得更加合理的模式结构.为了清晰地描述, 本节首先给出SPIM算法的问题定义.</p>
                </div>
                <div class="p1">
                    <p id="105"><b>定义1</b>. 使用源变量<b><i>X</i></b>= (<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>) 表示跨媒体数据对象的集合, 使用私有变量<b><i>Y</i></b><sup>1</sup>, <b><i>Y</i></b><sup>2</sup>, …, <b><i>Y</i></b><sup><i>k</i></sup>表示不同模态数据自身的私有信息 (即各模态自身的特征表示, 又称作源变量<b><i>X</i></b>的特征变量) , 使用共有变量<b><i>S</i></b>表示模态间的共享信息, 其中, <i>n</i>是数据对象的数量, <i>k</i>是跨媒体数据的模态数量.SPIM算法的目标是寻求源变量<b><i>X</i></b>到压缩变量<b><i>T</i></b>的一种最优压缩表示<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) , 在压缩过程中使压缩变量<b><i>T</i></b>同时最大化地保存与各模态自身的私有变量<b><i>Y</i></b><sup>1</sup>, <b><i>Y</i></b><sup>2</sup>, …, <b><i>Y</i></b><sup><i>k</i></sup>和共有变量<b><i>S</i></b>间的信息.根据式 (3) , 可给出SPIM目标函数:</p>
                </div>
                <div class="p1">
                    <p id="106">L<sub>max</sub>[<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) ]=<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>;<b><i>Y</i></b><sup><i>i</i></sup>) +<i>λ</i>·<i>I</i> (<b><i>T</i></b>;<b><i>S</i></b>) ,      (4) </p>
                </div>
                <div class="p1">
                    <p id="108">其中, <i>λ</i>是控制私有信息与共享信息之间侧重程度的平衡参数.当<i>λ</i>=0且<i>k</i>=1时, SPIM算法回归至IB算法, 因此, IB算法可视为SPIM算法的特例.</p>
                </div>
                <div class="p1">
                    <p id="109">给定数据对象与其特征表示的联合概率分布, 则压缩变量与各模态自身的私有变量之间的互信息<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>;<b><i>Y</i></b><sup><i>i</i></sup>) 是可计算的.为计算<i>I</i> (<b><i>T</i></b>;<b><i>S</i></b>) , 接下来首先给出2种跨媒体数据的共享信息构建模型.</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>2.1 混合单词模型</b></h4>
                <div class="p1">
                    <p id="112">词库模型<citation id="456" type="reference"><link href="401" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>是一种常用的数据表示方法, 该方法可将跨媒体的各模态数据转换为单词或视觉单词出现频率的向量形式, 例如一幅城市场景的图像可由高楼、街道、红绿灯等视觉单词出现频率表示;一则体育新闻可由比分、队员、场地等文字出现频率表示.在聚类任务中同时考虑不同模态数据的词频表示能在一定程度上刻画多模态间的关联性, 但不同模态数据的词频向量在尺度上具有明显的差异, 且存在较大的样本冗余.因此, 本文提出混合单词H-words模型, 首先将各模态的底层特征转换为统一的词频向量表示, 然后使用自凝聚信息最大化方法自底向上地构建多模态的混合单词空间, 抽取模态间的共享信息, 最大化地保持各模态底层特征的统计相似性.</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同模态的聚类划分间的互信息" src="Detail/GetImg?filename=images/JFYZ201907002_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同模态的聚类划分间的互信息  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 The mutual information between two clusters of different modalities</p>

                </div>
                <div class="p1">
                    <p id="114">给定源变量<b><i>X</i></b>及其<i>k</i>个模态的私有变量<b><i>Y</i></b><sup>1</sup>, <b><i>Y</i></b><sup>2</sup>, …, <b><i>Y</i></b><sup><i>k</i></sup>, 其中, 私有变量<b><i>Y</i></b><sup><i>i</i></sup>的值域为<b><i>Y</i></b><sup><i>i</i></sup>= (<b><i>y</i></b><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, <b><i>y</i></b><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>, …, <b><i>y</i></b><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>i</mi></msubsup></mrow></math></mathml>) , <i>m</i><sub><i>i</i></sub>为第<i>i</i>个模态的特征维度, <b><i>y</i></b><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mi>i</mi></msubsup></mrow></math></mathml>表示特征<b><i>y</i></b><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup></mrow></math></mathml>在数据<b><i>x</i></b><sub><i>j</i></sub>中出现的统计次数.因此, 可以得到<i>k</i>个联合概率分布<i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b><sup>1</sup>) , <i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b><sup>2</sup>) , …, <i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b><sup><i>k</i></sup>) .本文定义目标函数对混合单词空间<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>进行求解:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>F</mtext><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">[</mo><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></mrow><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo>;</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo>;</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">其中, <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo></mrow></math></mathml>是私有变量<b><i>Y</i></b><sup><i>i</i></sup>到混合单词空间<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>的映射.本文采用文献<citation id="457" type="reference">[<a class="sup">26</a>]</citation>中自底向上的层次模型对式 (5) 求解, 该模型首先将每个数据元素视为单独的簇, 即<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">|</mo><mo>=</mo></mrow></math></mathml><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>, 其中<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo>|</mo></mrow></mrow></math></mathml>是混合单词空间的维度, <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>是第<i>i</i>个模态的特征个数.之后通过不断合并最相似的数据元素, 可逐步降低<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo>|</mo></mrow></mrow></math></mathml>的值, 去除冗余特征.由于在自底向上的合并过程中最大化地保持各模态特征间的互信息, 因此, 我们称此过程为自凝聚信息最大化.假设<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>是私有变量<b><i>Y</i></b><sup><i>i</i></sup>中的2个特征向量, 则<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>合并前后式 (5) 的变化量计算为</p>
                </div>
                <div class="p1">
                    <p id="130">ΔF<sub>max</sub> (<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>) =F<sup>bef</sup><sub>max</sub>-F<sup>aft</sup><sub>max</sub>,      (6) </p>
                </div>
                <div class="p1">
                    <p id="131">其中, F<sup>bef</sup><sub>max</sub>, F<sup>aft</sup><sub>max</sub>分别是将特征向量<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>合并前后式 (5) 的值.</p>
                </div>
                <div class="p1">
                    <p id="132"><b>定义2</b>. 合并<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>生成簇<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow></math></mathml>的概率计算为</p>
                </div>
                <div class="p1">
                    <p id="134"><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo><mo>=</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>,      (7) </p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo></mrow></mfrac><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo></mrow></mfrac><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">) </mo><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">给出自凝聚信息最大化的详细执行过程:</p>
                </div>
                <div class="p1">
                    <p id="138">1) 将每个特征点初始化为1个单独簇.</p>
                </div>
                <div class="p1">
                    <p id="139">2) 计算所有特征对合并引起式 (5) 的改变量ΔF<sub>max</sub> (<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>) .</p>
                </div>
                <div class="p1">
                    <p id="140">3) 合并满足arg min ΔF<sub>max</sub> (<b><i>y</i></b><sub><i>h</i></sub>, <b><i>y</i></b><sub><i>g</i></sub>) 的特征对.</p>
                </div>
                <div class="p1">
                    <p id="141">4) 根据定义2更新<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo><mo>, </mo><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">|</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo></mrow></math></mathml>, 直至达到事先给定的簇个数.</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143"><b>2.2 聚类集成模型</b></h4>
                <div class="p1">
                    <p id="144">为了进一步挖掘各模态间的关联关系, 本文提出聚类集成CE模型, 首先为各模态数据构建各自的聚类划分, 然后使用互信息度量各模态高层聚类划分间的相关性, 进而捕捉到多个模态的异构信息.</p>
                </div>
                <div class="p1">
                    <p id="145">SPIM算法的目标是寻求源变量<b><i>X</i></b>中的压缩变量<b><i>T</i></b>, 假设<b><i>T</i></b>中有<i>M</i>个簇<b><i>T</i></b>= (<b><i>t</i></b><sub>1</sub>, <b><i>t</i></b><sub>2</sub>, …, <b><i>t</i></b><sub><i>M</i></sub>) .根据源变量<b><i>X</i></b>的<i>k</i>个模态构建辅助聚类<b><i>C</i></b>= (<b><i>C</i></b><sup>1</sup>, <b><i>C</i></b><sup>2</sup>, …, <b><i>C</i></b><sup><i>k</i></sup>) , 并假设第<i>l</i>个模态的聚类划分<b><i>C</i></b><sup><i>l</i></sup>中同样具有<i>M</i>个簇<b><i>C</i></b><sup><i>l</i></sup>= (<b><i>c</i></b><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>l</mi></msubsup></mrow></math></mathml>, <b><i>c</i></b><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>l</mi></msubsup></mrow></math></mathml>, …, <b><i>c</i></b><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Μ</mi><mi>l</mi></msubsup></mrow></math></mathml>) .假设<i>n</i><sub><i>i</i></sub>为<b><i>C</i></b><sup><i>l</i></sup>中被划分至簇<b><i>c</i></b><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>中的数据元素个数;<i>n</i><sub><i>j</i></sub>为聚类划分<b><i>T</i></b>中划分到簇<b><i>t</i></b><sub><i>j</i></sub>中数据元素个数;<i>n</i><sub><i>ij</i></sub>为同时属于簇<b><i>c</i></b><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>和簇<b><i>t</i></b><sub><i>j</i></sub>中数据元素个数, 则聚类划分<b><i>C</i></b><sup><i>l</i></sup>和<b><i>T</i></b>之间的概率分布可计算为</p>
                </div>
                <div class="p1">
                    <p id="151" class="code-formula">
                        <mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mi>n</mi></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>n</mi></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mi>n</mi></mfrac><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="152">图4举例说明聚类集成模型中使用互信息度量各模态间高层聚类划分的相关性.图4中第<i>i</i>行第<i>j</i>列若为黑框, 表示数据元素<b><i>x</i></b><sub><i>j</i></sub>出现在簇<b><i>c</i></b><sub><i>i</i></sub>中, 否则表示不出现.另外, 为了使展示更加清晰, 该例中数据元素可被划分至多个簇中.图4 (a) 中聚类模式<b><i>C</i></b><sup><i>l</i></sup>和<b><i>T</i></b>高度相似, 得到最高的互信息;图4 (b) 中<b><i>C</i></b><sup><i>l</i></sup>和<b><i>T</i></b>的相关性减弱, 互信息也相应降低;图4 (c) 中<b><i>C</i></b><sup><i>l</i></sup>和<b><i>T</i></b>的相似度最弱, 得到最低的互信息.因此, 使用互信息可有效地度量跨模态高层聚类划分之间的相关性.</p>
                </div>
                <h4 class="anchor-tag" id="153" name="153"><b>2.3 SPIM算法的目标函数</b></h4>
                <div class="p1">
                    <p id="154">根据定义1可知, SPIM算法的目标是寻求源变量<b><i>X</i></b>到压缩变量<b><i>T</i></b>的一种最优压缩表示<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) , 在压缩过程中使压缩变量<b><i>T</i></b>最大化地保存与各模态自身的私有变量<b><i>Y</i></b><sup>1</sup>, <b><i>Y</i></b><sup>2</sup>, …, <b><i>Y</i></b><sup><i>k</i></sup>和共有变量<b><i>S</i></b>的信息量, 其中共有变量由混合单词模型和聚类集成模型共同求得, 因此, SPIM算法的目标函数可改写为</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>L</mtext><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">[</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mo>⋅</mo><mo stretchy="false">[</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mi mathvariant="bold-italic">C</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">其中, <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>;<b><i>Y</i></b><sup><i>i</i></sup>) 度量压缩变量与各模态自身的私有变量之间的信息量;<i>I</i> (<b><i>T</i></b>;<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>) 表示压缩变量与跨模态共享特征之间的信息量;<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>;<b><i>C</i></b><sup><i>i</i></sup>) 度量压缩变量与跨模态的高层聚类划分之间的信息量;<i>λ</i>是控制私有信息与共享信息之间的侧重程度的平衡参数.</p>
                </div>
                <h4 class="anchor-tag" id="160" name="160"><b>2.4 SPIM算法目标函数的优化</b></h4>
                <div class="p1">
                    <p id="161">本节使用顺序“抽取-合并”策略优化SPIM算法的目标函数, 求解源变量<b><i>X</i></b>到压缩变量<b><i>T</i></b>之间的编码方案<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) .顺序“抽取-合并”优化方法包含3个步骤:</p>
                </div>
                <div class="p1">
                    <p id="162">1) 将源变量<b><i>X</i></b>= (<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>n</i></sub>) 随机划分至<i>M</i>个簇<b><i>T</i></b>= (<b><i>t</i></b><sub>1</sub>, <b><i>t</i></b><sub>2</sub>, …, <b><i>t</i></b><sub><i>M</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="163">2) 顺序地将每个数据元素<b><i>x</i></b>从当前簇<b><i>t</i></b><sup>old</sup>中抽取出来, 作为单独簇{<b><i>x</i></b>};</p>
                </div>
                <div class="p1">
                    <p id="164">3) 计算将单独簇{<b><i>x</i></b>}合并至其他簇中时SPIM算法目标函数中信息的损失量, 并选取使得信息损失最小的簇<b><i>t</i></b><sup>new</sup>进行合并.</p>
                </div>
                <div class="p1">
                    <p id="165">顺序抽取合并策略的核心问题是在迭代过程中选取合适的簇<b><i>t</i></b><sup>new</sup>对单独簇{<b><i>x</i></b>}进行合并.使用L<sup>old</sup>表示将<b><i>x</i></b>从簇<b><i>t</i></b><sup>old</sup>中抽取之前式 (10) 的值;使用L<sup>bef</sup>表示抽取<b><i>x</i></b>之后目标函数的值;使用L<sup>aft</sup>表示将单独簇{<b><i>x</i></b>}合并至<b><i>t</i></b><sup>new</sup>之后式 (10) 的值, <b><i>t</i></b><sup>new</sup>满足<b><i>t</i></b><sup>new</sup>=arg min ΔL=L<sup>bef</sup>-L<sup>aft</sup>, 其中, ΔL是{<b><i>x</i></b>}合并前后式 (10) 的改变量, 在此称之为合并代价, 其计算过程为</p>
                </div>
                <div class="area_img" id="482">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907002_48200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="173">其中, Δ<i>I</i><mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>p</mtext><mtext>r</mtext><mtext>i</mtext><mtext>v</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msubsup></mrow></math></mathml>, Δ<i>I</i><sup>com</sup>, Δ<i>I</i><mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>c</mtext><mtext>l</mtext><mtext>u</mtext><mtext>s</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext></mrow></msubsup></mrow></math></mathml>分别是目标函数中<i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b><sup><i>i</i></sup>) , <i>I</i> (<b><i>T</i></b>;<mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>) , <i>I</i> (<b><i>T</i></b>;<b><i>C</i></b><sup><i>i</i></sup>) 的合并代价.我们首先计算Δ<i>I</i><mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>p</mtext><mtext>r</mtext><mtext>i</mtext><mtext>v</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msubsup></mrow></math></mathml>, 假设单独簇{<b><i>x</i></b>}被合并到簇<b><i>t</i></b>形成新簇<mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">t</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow></math></mathml>, 则:</p>
                </div>
                <div class="p1">
                    <p id="179" class="code-formula">
                        <mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">t</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo><mo>=</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">|</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">t</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">t</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo></mrow></mfrac><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">t</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo stretchy="false">) </mo></mrow></mfrac><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo><mo>.</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="180">根据互信息的定义, 可得:</p>
                </div>
                <div class="area_img" id="483">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907002_48300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="186">把式 (12) 代入Δ<i>I</i><mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>p</mtext><mtext>r</mtext><mtext>i</mtext><mtext>v</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msubsup></mrow></math></mathml>中可得:</p>
                </div>
                <div class="area_img" id="484">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907002_48400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="197">其中, <mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><mo>=</mo><mo stretchy="false">{</mo><mi>π</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>π</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">}</mo><mo>=</mo><mo stretchy="false">{</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo></mrow></mfrac><mo stretchy="false">}</mo><mo>, </mo><mi>J</mi><mi>S</mi><msub><mrow></mrow><mi>π</mi></msub></mrow></math></mathml>为2个概率分布之间的Jensen-Shannon距离.同理可知<i>I</i> (<b><i>T</i></b>;<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>) 的合并代价Δ<i>I</i><sup>com</sup>:</p>
                </div>
                <div class="area_img" id="485">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907002_48500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="203">在将{<b><i>x</i></b>}合并至簇<b><i>t</i></b>时, 根据聚类集成模型中的式 (9) 可得到<i>p</i> (<b><i>T</i></b><sup>bef</sup>, <b><i>C</i></b><sup><i>i</i></sup>) , <i>p</i> (<b><i>T</i></b><sup>aft</sup>, <b><i>C</i></b><sup><i>i</i></sup>) , 因此可计算将{<b><i>x</i></b>}合并前后由<i>I</i> (<b><i>T</i></b>;<b><i>C</i></b><sup><i>i</i></sup>) 引起的合并代价.综上所述, 可计算将{<b><i>x</i></b>}合并至簇<b><i>t</i></b>的合并代价:</p>
                </div>
                <div class="p1">
                    <p id="204"><mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mtext>L</mtext><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>Δ</mtext></mstyle><mi>Ι</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>p</mtext><mtext>r</mtext><mtext>i</mtext><mtext>v</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msubsup><mo>+</mo><mi>λ</mi><mtext>Δ</mtext><mi>Ι</mi><msup><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>m</mtext></mrow></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>Δ</mtext></mstyle><mi>Ι</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>c</mtext><mtext>l</mtext><mtext>u</mtext><mtext>s</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext></mrow></msubsup></mrow></math></mathml>.      (14) </p>
                </div>
                <div class="p1">
                    <p id="206">给出SPIM算法的实现过程:</p>
                </div>
                <div class="p1">
                    <p id="207"><b>算法1</b>. SPIM算法.</p>
                </div>
                <div class="p1">
                    <p id="208">输入:源变量<b><i>X</i></b>与多个模态的私有变量<b><i>Y</i></b><sup>1</sup>, <b><i>Y</i></b><sup>2</sup>, …, <b><i>Y</i></b><sup><i>k</i></sup>之间的联合概率分布<i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b><sup>1</sup>) , <i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b><sup>2</sup>) , …, <i>p</i> (<b><i>X</i></b>, <b><i>Y</i></b><sup><i>k</i></sup>) ;簇的个数<i>M</i>;平衡参数<i>λ</i>.</p>
                </div>
                <div class="p1">
                    <p id="209">输出:<b><i>X</i></b>到<b><i>T</i></b>的编码方案<i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) .</p>
                </div>
                <div class="p1">
                    <p id="210">① 根据混合词库模型生成多个模态的混合单词空间<mathml id="211"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>, 并通过式 (|<b><i>Y</i></b><sup>1</sup>|+|<b><i>Y</i></b><sup>2</sup>|+…+|<b><i>Y</i></b><sup><i>k</i></sup>|) /<i>k</i>确定<mathml id="212"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>的维度;</p>
                </div>
                <div class="p1">
                    <p id="213">② 根据聚类集成模型构建多个模态自身的聚类划分<b><i>C</i></b><sup>1</sup>, <b><i>C</i></b><sup>2</sup>, …, <b><i>C</i></b><sup><i>k</i></sup>;</p>
                </div>
                <div class="p1">
                    <p id="214">③ 将源变量<b><i>X</i></b>随机划分为<i>M</i>个簇;</p>
                </div>
                <div class="p1">
                    <p id="215">④ Repeat:</p>
                </div>
                <div class="p1">
                    <p id="216">⑤ For every <b><i>x</i></b>∈<b><i>X</i></b></p>
                </div>
                <div class="p1">
                    <p id="217">⑥  将<b><i>x</i></b>从当前簇<b><i>t</i></b><sup>old</sup>中抽取出来, 作为单独簇{<b><i>x</i></b>};</p>
                </div>
                <div class="p1">
                    <p id="218">⑦  合并{<b><i>x</i></b>}至簇<b><i>t</i></b><sup>new</sup>, 其中<b><i>t</i></b><sup>new</sup>=arg min ΔL, 并根据式 (14) 计算合并代价;</p>
                </div>
                <div class="p1">
                    <p id="219">⑧ End For</p>
                </div>
                <div class="p1">
                    <p id="220">⑨ Until <i>p</i> (<b><i>t</i></b>|<b><i>x</i></b>) 不再发生变化.</p>
                </div>
                <h4 class="anchor-tag" id="221" name="221"><b>2.5 算法分析</b></h4>
                <h4 class="anchor-tag" id="222" name="222">2.5.1 收敛性分析</h4>
                <div class="p1">
                    <p id="223"><b>定理1</b>. SPIM算法可在有限迭代次数内收敛到局部最优解.</p>
                </div>
                <div class="p1">
                    <p id="224">证明. 首先证明每次抽取合并过程都能够增加式 (10) 的值.使用L<sup>old</sup>表示将<b><i>x</i></b>从其原始簇<b><i>t</i></b><sup>old</sup>中抽取之前目标函数的值, 使用L<sup>bef</sup>和L<sup>aft</sup>表示合并单独簇{<b><i>x</i></b>}至<b><i>t</i></b><sup>new</sup>=arg min ΔL前后的目标函数值.合并过程有2种情况:1) <b><i>t</i></b><sup>new</sup>=<b><i>t</i></b><sup>old</sup>, 意味着将{<b><i>x</i></b>}合并至其原本所在的簇<b><i>t</i></b><sup>old</sup>, 此时, 数据分布没有发生改变, 则L<sup>bef</sup>=L<sup>aft</sup>;2) <b><i>t</i></b><sup>new</sup>≠<b><i>t</i></b><sup>old</sup>, 意味着将{<b><i>x</i></b>}合并至其他簇<b><i>t</i></b><sup>new</sup>, 因为<b><i>t</i></b><sup>new</sup>=arg min ΔL, 故将{<b><i>x</i></b>}合并至新簇<b><i>t</i></b><sup>new</sup>的合并代价Δ (<b><i>x</i></b>, <b><i>t</i></b><sup>new</sup>) 一定小于将其合并至原始簇<b><i>t</i></b><sup>old</sup>的合并代价ΔL (<b><i>x</i></b>, <b><i>t</i></b><sup>old</sup>) , 即ΔL (<b><i>x</i></b>, <b><i>t</i></b><sup>new</sup>) &lt;ΔL (<b><i>x</i></b>, <b><i>t</i></b><sup>old</sup>) .又因为ΔL (<b><i>x</i></b>, <b><i>t</i></b><sup>old</sup>) =L<sup>bef</sup>-L<sup>old</sup>, ΔL (<b><i>x</i></b>, <b><i>t</i></b><sup>new</sup>) =L<sup>bef</sup>-L<sup>aft</sup>, 故L<sup>aft</sup>&gt;L<sup>old</sup>.因此, 每次抽取合并过程都有L<sup>aft</sup>≥L<sup>old</sup>.</p>
                </div>
                <div class="p1">
                    <p id="225">证明SPIM算法的目标函数值有上界.压缩变量<b><i>T</i></b>是源变量<b><i>X</i></b>的一种压缩表示, 则必有<i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b><sup><i>i</i></sup>) ≤<i>I</i> (<b><i>X</i></b>;<b><i>Y</i></b><sup><i>i</i></sup>) 和<i>I</i> (<b><i>T</i></b>;<mathml id="226"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo>≤</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>;<mathml id="227"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>) , 当且仅当|<b><i>X</i></b>|=|<b><i>Y</i></b>|时, <i>I</i> (<b><i>T</i></b>;<b><i>Y</i></b><sup><i>i</i></sup>) ≤<i>I</i> (<b><i>X</i></b>;<b><i>Y</i></b><sup><i>i</i></sup>) 和<i>I</i> (<b><i>T</i></b>;<mathml id="228"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo>≤</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>;<mathml id="229"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>) 中的等号成立, 此时<b><i>X</i></b>到<b><i>T</i></b>不存在压缩.另外, 假设源变量<b><i>X</i></b>的正确划分为<b><i>C</i></b>, 则<i>I</i> (<b><i>T</i></b>;<b><i>C</i></b><sup><i>i</i></sup>) ≤<i>I</i> (<b><i>T</i></b>;<b><i>C</i></b>) .综上所述, SPIM算法目标函数值的上界为<mathml id="230"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi></mrow></math></mathml>;<b><i>Y</i></b><sup><i>i</i></sup>) +<i>λ</i>·[<i>I</i> (<b><i>X</i></b>;<mathml id="231"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">) </mo><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>;<b><i>C</i></b>) ].因此SPIM算法可在有限迭代次数内收敛到局部最优解.  证毕.</p>
                </div>
                <h4 class="anchor-tag" id="232" name="232">2.5.2 复杂度分析</h4>
                <div class="p1">
                    <p id="233">预处理中步骤①可在<i>O</i> (|<mathml id="234"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>|<sup>2</sup>) 的时间内构建多模态公共特征空间.预处理中步骤②的时间复杂度由构建基聚类所采用的聚类算法决定, 本文使用顺序信息瓶颈 (sequectial information bottleneck, sIB) 算法, 其时间复杂度为<i>O</i> (<i>n</i> log <i>n</i>) .初始化过程可在<i>O</i> (1) 时间内实现.在SPIM算法主循环中, 抽取合并过程需要计算合并代价, 时间复杂度为<mathml id="235"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo></mrow><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">|</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>k</mi></msup><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>, 因此, SPIM算法的时间复杂度为<mathml id="236"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>Μ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">|</mo><mo>+</mo><mrow><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">|</mo><mo>+</mo></mrow><mo>⋯</mo><mo>+</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>k</mi></msup><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>, 其中<i>M</i>是最终聚类划分中簇的个数.</p>
                </div>
                <h3 id="237" name="237" class="anchor-tag"><b>3 实验与性能分析</b></h3>
                <h4 class="anchor-tag" id="238" name="238"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="239">本文在6种跨媒体数据集上验证SPIM算法的有效性:</p>
                </div>
                <div class="p1">
                    <p id="240">1) Wikipedia数据集<citation id="458" type="reference"><link href="403" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>.该数据集包含2 866个文本图像对共计10个类, 每幅图像的共生文本至少有70个单词.对于图像, 本文采用文献<citation id="459" type="reference">[<a class="sup">28</a>]</citation>提供的128维SIFT<citation id="460" type="reference"><link href="405" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>特征构建BoVW视觉特征表示;对于文本, 首先构建500维BoW表示, 然后通过LDA抽取100个话题的概率分布作为文本特征表示.</p>
                </div>
                <div class="p1">
                    <p id="241">2) Pascal Sentence数据集<citation id="461" type="reference"><link href="407" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>.该数据集包含1 000个文本图像对共计20个类.对于图像, 本文抽取1 024维的SIFT BoVW视觉特征表示;对于文本, 首先构建300维BoW表示, 然后通过LDA抽取100个话题的概率分布作为文本特征表示.</p>
                </div>
                <div class="p1">
                    <p id="242">3) Pascal VOC 2007数据集<citation id="462" type="reference"><link href="409" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>.该数据集包含9 963个文本图像对共计20个类.本文采用文献<citation id="463" type="reference">[<a class="sup">32</a>]</citation>提供的798维基于标签排序的文本特征和776维的BoVW视觉特征.</p>
                </div>
                <div class="p1">
                    <p id="243">4) X-Media<citation id="466" type="reference"><link href="413" rel="bibliography" /><link href="415" rel="bibliography" /><sup>[<a class="sup">33</a>,<a class="sup">34</a>]</sup></citation>是针对检索任务<citation id="464" type="reference"><link href="417" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>构建的跨媒体数据集, 该数据集包含文本、图像、视频、音频、3维模型等多种模态的数据, 共计20个类.我们使用5 000个文本、图像对评估算法性能.关于该数据集的特征描述, 本文采用文献<citation id="465" type="reference">[<a class="sup">33</a>]</citation>提供的10维LDA文本特征、128维BoVW图像特征.</p>
                </div>
                <div class="p1">
                    <p id="244">5) Reuters多语种数据集<citation id="467" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.该数据集由5种语言的新闻文档组成, 包括西班牙语、意大利语、德语、法语和英语, 每种语言的文档被分为6类:C15, CCAT, E21, ECAT, GCAT, M11.本文随机从每种语言类中挑取500个文档, 并使用BoW模型抽取1 000个关键词, 为每个语种构建1 000维BoW表示.</p>
                </div>
                <div class="p1">
                    <p id="245">6) HMDB数据集<citation id="468" type="reference"><link href="419" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>.该数据集由51类共计6 849个人体动作视频序列组成, 主要来源电影片段、网络视频等.本文提取视频序列3种异构描述子<citation id="469" type="reference"><link href="421" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>:梯度直方图 (histogram of oriented gradient, HOG) 、光流直方图 (histogram of optical flow, HOF) 和空间时间特征 (space-time interest points, STIP) , 分别构建视频序列的1 000维BoVW表示.</p>
                </div>
                <h4 class="anchor-tag" id="246" name="246"><b>3.2 评价指标</b></h4>
                <div class="p1">
                    <p id="247">为了公正地对聚类结果进行评估, 本文使用2种指标<citation id="470" type="reference"><link href="377" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>评估算法的聚类性能:</p>
                </div>
                <div class="p1">
                    <p id="248">1) 聚类精度 (clustering accuracy, <i>ACC</i>) :</p>
                </div>
                <div class="p1">
                    <p id="249" class="code-formula">
                        <mathml id="249"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>C</mi><mi>C</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>δ</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">l</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>m</mi><mi>a</mi><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="250">其中, <b><i>l</i></b><sub><i>i</i></sub>和<b><i>t</i></b><sub><i>i</i></sub>分别表示数据对象的真实划分和聚类划分, <i>n</i>是数据集的大小.<i>δ</i> (<b><i>x</i></b>, <b><i>y</i></b>) 为狄克拉函数, 当<b><i>x</i></b>==<b><i>y</i></b>时, <i>δ</i> (<b><i>x</i></b>, <b><i>y</i></b>) =1, 否则<i>δ</i> (<b><i>x</i></b>, <b><i>y</i></b>) =0.<i>map</i> (<b><i>t</i></b><sub><i>i</i></sub>) 是聚类划分<b><i>t</i></b><sub><i>i</i></sub>与真实划分之间的映射函数.</p>
                </div>
                <div class="p1">
                    <p id="251">2) 标准化互信息 (normalized mutual information, <i>NMI</i>) :</p>
                </div>
                <div class="p1">
                    <p id="252"><mathml id="253"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><mi>Μ</mi><mi>Ι</mi><mo>=</mo><mfrac><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mi mathvariant="bold-italic">C</mi><mo stretchy="false">) </mo></mrow><mrow><mi>max</mi><mo stretchy="false">[</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">) </mo><mo>, </mo><mi>Η</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">C</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></mfrac></mrow></math></mathml>,      (16) </p>
                </div>
                <div class="p1">
                    <p id="254">其中, <b><i>T</i></b>和<b><i>C</i></b>分别表示数据对象的聚类划分和真实划分, <i>I</i> (<b><i>T</i></b>;<b><i>C</i></b>) 是<b><i>T</i></b>和<b><i>C</i></b>间的互信息, <i>H</i> (<b><i>T</i></b>) 和<i>H</i> (<b><i>C</i></b>) 分别表示聚类划分和真实划分的信息熵.<i>ACC</i>, <i>NMI</i>的值越大, 聚类结果越好.</p>
                </div>
                <h4 class="anchor-tag" id="255" name="255"><b>3.3 对比方法</b></h4>
                <div class="p1">
                    <p id="256">为验证SPIM算法在跨媒体聚类任务中的有效性, 本文将其与7种算法进行对比:</p>
                </div>
                <div class="p1">
                    <p id="257">1) <i>k</i>-means算法.经典的单模态聚类算法.</p>
                </div>
                <div class="p1">
                    <p id="258">2) IB算法<citation id="471" type="reference"><link href="391" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>.对各模态分别使用IB算法进行聚类, 我们在所有图表中公布各模态中最好的聚类结果.</p>
                </div>
                <div class="p1">
                    <p id="259">3) Concate -IB算法.将各模态的特征直接相连, 然后使用IB算法进行聚类.</p>
                </div>
                <div class="p1">
                    <p id="260">4) 典型关联分析 (canonical correlation analysis, CCA) 算法<citation id="472" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.采用CCA算法学习2个模态间的共享信息.对于Reuters和HMDB数据集, 本文选择2种表现最优的模态作为CCA算法的输入.</p>
                </div>
                <div class="p1">
                    <p id="261">5) CSPA (cluster-based similarity partitioning algorithm) 算法<citation id="473" type="reference"><link href="377" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.该算法是一种典型的聚类集成算法, 首先根据基聚类的co-association矩阵生成图, 其中顶点是数据对象, 边的权重的数据对象间的co-association值, 然后使用METIS算法进行聚类.</p>
                </div>
                <div class="p1">
                    <p id="262">6) PTGP (probability trajectory graph parti-tioning) 算法<citation id="474" type="reference"><link href="379" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.该算法是基于稀疏图表示和概率轨迹的聚类集成算法, 能够同时根据基聚类的局部和全局信息获取最终的聚类划分.</p>
                </div>
                <div class="p1">
                    <p id="263">7) LWGP (locally weighted evidence accumu-lation) 算法<citation id="475" type="reference"><link href="381" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.该算法在不考虑原始数据分布的情况下, 通过局部密度估计方法对基聚类进行加权, 进而区分基聚类的可依赖程度.</p>
                </div>
                <div class="p1">
                    <p id="264">表1给出算法的时间复杂度对比.其中, CCA算法的<i>O</i> (<i>Md</i><sup>2</sup>) 项为计算2个模态的协方差矩阵的时间复杂度, <i>O</i> (<i>d</i><sup>3</sup>) 项是特征分解的时间复杂度, <i>d</i>=max (|<b><i>Y</i></b><sup><i>i</i></sup>|, |<b><i>Y</i></b><sup><i>j</i></sup>|) 是任意2个模态的特征维度的最大值.CSPA算法采用METIS算法<citation id="476" type="reference"><link href="423" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>对基聚类的生成图进行划分, 其时间复杂度为<i>O</i> (|<b><i>X</i></b>|<sup>2</sup>) .LWGP算法首先构建数据对象<b><i>X</i></b>与基聚类之间的二分图 (bipartite graph) , 之后使用TCut算法<citation id="477" type="reference"><link href="425" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>图划分.与LWGP算不同, PTGP算法首先对数据对象<b><i>X</i></b>进行初步聚类生成中间簇<b><i>X</i></b><sup>mcluster</sup>, 构建中间簇<b><i>X</i></b><sup>mcluster</sup>与基聚类的二分图, 最后使用TCut算法进行二分图划分, 其中<mathml id="265"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mrow><mtext>m</mtext><mtext>c</mtext><mtext>l</mtext><mtext>u</mtext><mtext>s</mtext><mtext>t</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>是中间簇的数量.</p>
                </div>
                <div class="area_img" id="266">
                    <p class="img_tit"><b>表1 不同算法的时间复杂度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 The Complexities of Different Baselines</b></p>
                    <p class="img_note"></p>
                    <table id="266" border="1"><tr><td><br />Baselines</td><td>Time Complexity</td></tr><tr><td><br />CCA</td><td><i>O</i> (<i>Md</i><sup>2</sup>) +<i>O</i> (<i>d</i><sup>3</sup>) , <i>d</i>=max (|<b><i>Y</i></b><sup><i>i</i></sup>|, |<b><i>Y</i></b><sup><i>j</i></sup>|) </td></tr><tr><td><br />CSPA</td><td><i>O</i> (|<b><i>X</i></b>|<sup>2</sup>) </td></tr><tr><td><br />PTGP</td><td><i>O</i> (2<i>M</i> (1+<i>k</i>) |<b><i>X</i></b><sup>mcluster</sup>|+<i>kM</i><sup>3/2</sup>) </td></tr><tr><td><br />LWGP</td><td><i>O</i> (2<i>M</i> (1+<i>k</i>) |<b><i>X</i></b>|+<i>kM</i><sup>3/2</sup>) </td></tr><tr><td><br />Our SPIM</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mi>Μ</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mn>1</mn></msup><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">|</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>k</mi></msup><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover><mo stretchy="false">|</mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="267" name="267"><b>3.4 实验结果分析</b></h4>
                <div class="p1">
                    <p id="268">为了验证数据不同模态表示能力的差异, 图5给出IB算法在跨媒体数据不同模态上的聚类结果.图5中的实验结果表明:</p>
                </div>
                <div class="p1">
                    <p id="269">1) IB算法在Wikipedia, Pascal Sentence和Pascal VOC 07数据集的文本、图像2种模态上的聚类结果具有较大的差异, 且IB算法在文本模态上的聚类性能明显优于图像模态.这说明在跨模态的图像聚类任务中仅依赖图像信息很难获得较好的聚类结果, 丰富的语义关系能够带来更加准确的语义相关性.因此, 兼顾语义和视觉等模态信息有助于提升跨模态聚类任务的聚类性能.</p>
                </div>
                <div class="p1">
                    <p id="270">2) IB算法在多语种数据集Reuters的各语种上和视频数据集HMDB的各异构特征空间上得到相近的聚类结果.这说明数据对象往往具有多个侧面的描述, 例如相同的新闻可以用多种语言进行报道;包含相同人体动作的视频序列可以在梯度、光流、时空等特征空间上获得异构的描述.不同侧面的特征信息反映数据不同的内在特性, 有效地组织和使用特征之间的互补作用以便确保高质量的聚类结果.</p>
                </div>
                <div class="p1">
                    <p id="271">表2和表3展示了各种方法的对比结果.对于CCA算法, 本文首先按照相关系数排序, 并选取前<i>d</i>对典型得分作为特征向量进行聚类.我们在表2和表3中公布的是CCA算法聚类结果稳定后的值.为保证CSPA, PTGP, LWGP这3种集成聚类算法中基聚类间的差异性, 我们对数据对象的各模态分别使用IB算法运行20次, 然后使用聚类集成算法对所有模态的聚类成员进行合并, 得到最终的聚类划分.根据原作者的建议, 本文将PTGP算法中节点数量<i>k</i>设置为<mathml id="272"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">X</mi><mo>|</mo></mrow></mrow></msqrt><mo>/</mo><mn>2</mn></mrow></math></mathml>, 将LWEA算法中控制聚类不确定性 (cluster uncertainty) 的参数<i>θ</i>设置为0.4.另外, 除CCA算法外, 其他对比算法及本文提出的SPIM算法均受随机初始化的影响, 因此, 本文将这些算法运行10次, 公布其平均结果 (<i>mean</i>) 及标准差 (<i>std</i>) .从表2和表3可知3点实验现象.</p>
                </div>
                <div class="area_img" id="273">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_273.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 IB算法在不同媒体源上的聚类结果" src="Detail/GetImg?filename=images/JFYZ201907002_273.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 IB算法在不同媒体源上的聚类结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_273.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Clustering results of IB on different media sources</p>

                </div>
                <div class="area_img" id="274">
                    <p class="img_tit"><b>表2 在6种跨媒体数据集上的<i>ACC</i>对比结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 The <i>ACC</i> Comparison Results on 6 Cross-Media Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="274" border="1"><tr><td rowspan="2"><br />Datasets</td><td><br /><i>k</i>-means</td><td>IB</td><td>Concate-IB</td><td rowspan="2">CCA</td><td><br />CSPA</td><td>PTGP</td><td>LWEA</td><td>SPIM</td></tr><tr><td><br /><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><br /><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td></tr><tr><td><br />Wikipedia</td><td>45.22±2.4</td><td>58.27±4.8</td><td>28.58±0.9</td><td>59.67</td><td>60.09±5.1</td><td>62.07±1.0</td><td>62.86±0.8</td><td><b>65.50±2.3</b></td></tr><tr><td><br />Pascal Sentence</td><td>52.93±3.8</td><td>54.10±2.9</td><td>41.75±2.8</td><td>55.20</td><td>55.15±2.0</td><td>57.83±0.8</td><td>59.67±0.7</td><td><b>63.02±1.5</b></td></tr><tr><td><br />Pascal VOC 07</td><td>64.99±3.5</td><td>69.53±3.2</td><td>62.77±2.3</td><td>69.09</td><td>69.84±5.3</td><td>72.89±0.9</td><td>73.64±0.8</td><td><b>76.33±1.2</b></td></tr><tr><td><br />X-Media</td><td>20.55±0.6</td><td>21.16±0.5</td><td>21.31±0.5</td><td>23.58</td><td>18.75±0.7</td><td>24.31±1.1</td><td>21.78±0.6</td><td><b>25.16±1.9</b></td></tr><tr><td><br />Reuters</td><td>53.16±1.4</td><td>53.12±3.1</td><td>53.43±3.4</td><td>50.93</td><td>59.92±0.2</td><td>56.61±3.6</td><td>55.28±4.1</td><td><b>60.59±3.6</b></td></tr><tr><td><br />HMDB</td><td>18.46±0.9</td><td>22.31±2.2</td><td>23.35 ±1.3</td><td>25.34</td><td>20.82±0.5</td><td>25.75±0.9</td><td>26.63±1.4</td><td><b>29.42±0.8</b></td></tr><tr><td><br />Average</td><td>42.55</td><td>46.41</td><td>38.53</td><td>47.30</td><td>47.42</td><td>49.91</td><td>49.97</td><td><b>53.34</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best clustering results in each case is boldfaced.</p>
                </div>
                <div class="area_img" id="275">
                    <p class="img_tit"><b>表3 在6种跨媒体数据集上的<i>NMI</i>对比结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 The <i>NMI</i> Comparison Results on 6 Cross-Media Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="275" border="1"><tr><td rowspan="2"><br />Datasets</td><td><br /><i>k</i>-means</td><td>IB</td><td>Concate-IB</td><td rowspan="2">CCA</td><td><br />CSPA</td><td>PTGP</td><td>LWEA</td><td>SPIM</td></tr><tr><td><br /><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><br /><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td></tr><tr><td><br />Wikipedia</td><td>45.50±1.7</td><td>51.68±1.3</td><td>14.57±0.4</td><td>52.32</td><td>48.12±2.3</td><td>52.98±0.2</td><td>53.01±0.3</td><td><b>54.97±1.6</b></td></tr><tr><td><br />Pascal Sentence</td><td>59.88±1.9</td><td>59.99±1.6</td><td>47.88±1.8</td><td>60.89</td><td>61.99±1.4</td><td>62.12±0.4</td><td>63.75±0.3</td><td><b>66.01±1.8</b></td></tr><tr><td><br />Pascal VOC 07</td><td>65.16±1.5</td><td>65.38±1.2</td><td>65.29±1.0</td><td>66.28</td><td>71.05±2.4</td><td>70.41±0.5</td><td>71.92±0.2</td><td><b>75.21+1.5</b></td></tr><tr><td><br />X-Media</td><td>20.12±0.5</td><td>21.24±0.2</td><td>21.23±0.2</td><td>23.28</td><td>21.45±0.8</td><td>23.94±0.4</td><td>20.03±0.3</td><td><b>26.54±2.2</b></td></tr><tr><td><br />Reuters</td><td>33.94±1.2</td><td>43.29±3.1</td><td>44.33±3.4</td><td>46.77</td><td>48.80±0.1</td><td>47.35±4.9</td><td>46.79±4.0</td><td><b>51.79±2.8</b></td></tr><tr><td><br />HMDB</td><td>26.84±1.0</td><td>33.74±2.2</td><td>34.15 ±2.0</td><td>36.14</td><td>37.83±0.6</td><td>37.25±0.6</td><td>35.41±0.8</td><td><b>41.13±1.1</b></td></tr><tr><td><br />Average</td><td>41.91</td><td>45.89</td><td>37.89</td><td>47.61</td><td>48.21</td><td>49.01</td><td>48.49</td><td><b>52.61</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best clustering results in each case is boldfaced.</p>
                </div>
                <div class="area_img" id="276">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_276.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 平衡参数λ对SPIM算法聚类结果的影响" src="Detail/GetImg?filename=images/JFYZ201907002_276.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 平衡参数<i>λ</i>对SPIM算法聚类结果的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_276.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The impact of <i>λ</i> on the performance of SPIM algorithm</p>

                </div>
                <div class="p1">
                    <p id="277">1) 将各模态的特征直接相连不能有效地提升聚类性能.例如相比IB算法在单一模态上的最优结果, Concate-IB算法在Wikipedia, Pascal Sentence, Pascal VOC 07数据集上的<i>ACC</i>和<i>NMI</i>值出现下降.说明简单地连接多模态的特征信息不能稳定地提升算法的聚类质量.</p>
                </div>
                <div class="p1">
                    <p id="278">2) 结合多模态信息的算法在聚类任务上的表现优于单模态聚类.从表2和表3可知, CCA, CSPA, PTGP, LWEA算法在本文使用的6种跨媒体数据上的平均聚类结果均优于<i>k</i>-means, IB等单模态聚类算法.这种现象验证了多模态之间的互补作用能够有效地提升聚类结果.</p>
                </div>
                <div class="p1">
                    <p id="279">3) 相比于其他单模态和跨模态聚类算法, SPIM算法在本文使用的6种跨媒体数据集上的聚类结果均有提升.</p>
                </div>
                <h4 class="anchor-tag" id="280" name="280"><b>3.5 参数分析</b></h4>
                <div class="p1">
                    <p id="281">SPIM算法使用参数<i>λ</i>控制共享信息与私有信息间的侧重程度, 因此, 本节通过实验来观察不同参数值对聚类结果的影响.从图6可以看出, 当<i>λ</i>取值较小时, SPIM算法得到较低的<i>ACC</i>值.随着<i>λ</i>值的增大, SPIM算法聚类效果逐渐变好, 此时共享信息与私有信息的互补作用得以体现.随着<i>λ</i>值进一步增大, 各模态的私有信息与共享信息达到平衡, SPIM算法的聚类结果也在一定程度上趋于稳定.注意, 本文公布的SPIM算法的聚类结果对应参数取值为<i>λ</i>=60, 聚类结果如图6中星号所示.</p>
                </div>
                <div class="area_img" id="282">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_282.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 SPIM算法在不同数据集上达到收敛时的迭代次数" src="Detail/GetImg?filename=images/JFYZ201907002_282.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 SPIM算法在不同数据集上达到收敛时的迭代次数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_282.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 The iteration number of SPIM on different datasets</p>

                </div>
                <h4 class="anchor-tag" id="283" name="283"><b>3.6 收敛性分析</b></h4>
                <div class="p1">
                    <p id="284">SPIM算法的目标函数只能收敛到一个局部最优解, 因此有必要对其收敛性进行经验性分析.图7给出SPIM算法每次迭代后式 (10) 的值.从图7可以看出, 随着算法的运行, 式 (10) 的值先是迅速上升, 然后上升的幅度趋于平缓, 最终得到目标函数的局部最优值, 说明该算法具有较好的收敛性.</p>
                </div>
                <div class="area_img" id="285">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907002_285.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 SPIM算法在NMI指标上的模型简化测试" src="Detail/GetImg?filename=images/JFYZ201907002_285.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 SPIM算法在<i>NMI</i>指标上的模型简化测试  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907002_285.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Model ablation test of SPIM algorithm in terms of <i>NMI</i></p>

                </div>
                <h4 class="anchor-tag" id="286" name="286"><b>3.7 模型简化测试</b></h4>
                <div class="p1">
                    <p id="287">SPIM算法通过混合单词模型和聚类集成模型对跨模态数据中各模态间的关联进行建模, 分别保持各模态底层的统计相似性和各模态的高层聚类划分间的相关性.本节针对混合单词模型和聚类集成模型设计单独的实验, 以验证单个模型的有效性.根据式 (10) 可知, SPIM算法在仅考虑单一混合单词模型和聚类集成模型时的目标函数可分别简化为</p>
                </div>
                <div class="p1">
                    <p id="288"><mathml id="289"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>L</mtext><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">[</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">x</mi></mrow><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>;<b><i>Y</i></b><sup><i>i</i></sup>) +<i>λ</i>·<i>I</i> (<b><i>T</i></b>;<mathml id="290"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>˜</mo></mover></math></mathml>) ,      (17) </p>
                </div>
                <div class="p1">
                    <p id="291" class="code-formula">
                        <mathml id="291"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext> </mtext><mtext>L</mtext><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">[</mo><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mo>⋅</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>Ι</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Τ</mi><mo>;</mo><mi mathvariant="bold-italic">C</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="292">图8和表4给出分别考虑单一混合单词模型和聚类集成模型时SPIM算法的聚类精度和标准化互信息.从图8和表4可知:</p>
                </div>
                <div class="p1">
                    <p id="293">1) SPIM算法在仅考虑单一模型时的聚类结果优于IB算法在单个模态中最优的聚类结果, 验证SPIM算法中单一模型的有效性;</p>
                </div>
                <div class="p1">
                    <p id="294">2) 在同时使用混合单词模型和聚类集成模型对共享信息建模时, SPIM算法的聚类结果进一步提升.例如, 表4中SPIM算法在6种跨媒体数据集上的平均结果均优于IB算法、聚类集成模型和混合词库模型.</p>
                </div>
                <div class="area_img" id="295">
                    <p class="img_tit"><b>表4 SPIM算法在<i>ACC</i>指标上的模型简化测试</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Model Ablation test of SPIM Algorithm in Terms of <i>ACC</i></b></p>
                    <p class="img_note"></p>
                    <table id="295" border="1"><tr><td rowspan="2"><br />Datasets</td><td><br />IB</td><td>CE</td><td>H-words</td><td>SPIM</td></tr><tr><td><br /><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td><td><i>mean</i>±<i>std</i></td></tr><tr><td><br />Wikipedia</td><td>58.27±4.8</td><td>62.18±4.3</td><td>61.99±1.7</td><td><b>65.50±2.3</b></td></tr><tr><td><br />Pascal Sentence</td><td>54.10±2.9</td><td>56.74±2.8</td><td>55.41±3.7</td><td><b>63.02±1.5</b></td></tr><tr><td><br />Pascal VOC 07</td><td>69.53±3.2</td><td>70.23±2.3</td><td>72.38±1.6</td><td><b>76.33±1.2</b></td></tr><tr><td><br />X-Media</td><td>21.16±0.5</td><td>20.58±0.4</td><td>23.76±0.8</td><td><b>25.16±1.9</b></td></tr><tr><td><br />Reuters</td><td>53.12±3.1</td><td>57.79±3.4</td><td>58.09±2.2</td><td><b>60.59±3.6</b></td></tr><tr><td><br />HMDB</td><td>22.31±2.2</td><td>23.32±2.4</td><td>26.14±1.3</td><td><b>29.42±0.8</b></td></tr><tr><td><br />Average</td><td>46.41</td><td>48.47</td><td>49.63</td><td><b>53.34</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best clustering results in each case is boldfaced.</p>
                </div>
                <h3 id="296" name="296" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="297">本文提出共享和私有信息最大化的跨媒体聚类算法SPIM, 该算法能够使用跨模态数据间的共享信息和各模态自身的私有信息进行聚类分析.首先, 提出2种跨媒体数据的共享信息构建模型, 分别保持各模态底层特征的统计相似性和各模态的高层聚类划分间的相关性, 其次, 提出基于信息论的目标函数, 将跨媒体数据的共享和私有信息融合在同一目标函数中.最后, 采用顺序“抽取-合并”优化过程, 保证SPIM的目标函数收敛到局部最优解.在6种跨媒体数据上的实验结果表明:本文提出的SPIM算法的优越性</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="488" type="formula" href="images/JFYZ201907002_48800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">闫小强</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="489" type="formula" href="images/JFYZ201907002_48900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">叶阳东</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="349">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A co-training approach for multi-view spectral clustering">

                                <b>[1]</b>Kumar A, Daume H.A co-training approach for multi-view spectral clustering[C] //Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:393- 400
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Heterogeneous image feature integration via multi-modal spectral clustering">

                                <b>[2]</b>Cai Xiao, Nie Feiping, Huang Heng, et al.Heterogeneous image feature integration via multi-modal spectral clustering[C] //Proc of the 24th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2011:1977- 1984
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200805023&amp;v=MDIyNzZXN3pBTHl2U2RMRzRIdG5NcW85SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5dmg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Zhang Hong, Wu Fei, Zhuang Yueting.Cross-media correlation reasoning and retrieval[J].Journal of Computer Research and Development, 2008, 45 (5) :869- 876 (in Chinese) (张鸿, 吴飞, 庄越挺.跨媒体相关性推理与检索研究[J].计算机研究与发展, 2008, 45 (5) :869- 876) 
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706011&amp;v=MjM4MTBSckZ5dmhXN3pBTHo3QmRyRzRIOWJNcVk5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Zhang Lei, Zhao Yao, Zhu Zhenfeng.Advances in semantically shared subspace learning for cross-media data[J].Chinese Journal of Computers, 2017, 40 (6) :1394- 1421 (in Chinese) (张磊, 赵耀, 朱振峰.跨媒体语义共享子空间学习研究进展[J].计算机学报, 2017, 40 (6) :1394- 1421) 
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view clustering via canonical correlation analysis">

                                <b>[5]</b>Chaudhuri K, Kakade M, Livescu K, et al.Multi-view clustering via canonical correlation analysis[C] //Proc of the 26th Int Conf on Machine Learning.New York:ACM, 2009:129- 136
                            </a>
                        </p>
                        <p id="359">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012263&amp;v=MDkyNzdUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSTFvY2FSbz1OaWZKWmJLOUh0ak1xbzlGWk9vTkRubzZvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Hardoon D R, Szedmak S, Shawe -Taylor J.Canonical correlation analysis:An overview with application to learning methods[J].Neural Computation, 2004, 16 (12) :2639- 2664
                            </a>
                        </p>
                        <p id="361">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shared Kernel Information Embedding for discriminative inference">

                                <b>[7]</b>Sigal L, Memisevic R, Fleet D J.Shared kernel information embedding for discriminative inference[C] //Proc of the 22nd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2009:2852- 2859
                            </a>
                        </p>
                        <p id="363">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gaussian process latent variable models for human pose estimation">

                                <b>[8]</b>Carl H, Phipip P, Lawrence N D.Gaussian process latent variable models for human pose estimation[C] //Proc of the 4th Machine Learning for Multimodal Interaction.Berlin:Springer, 2007:132- 143
                            </a>
                        </p>
                        <p id="365">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning the semantics of words and pictures">

                                <b>[9]</b>Barnard K, Forsyth D.Learning the semantics of words and pictures[C] //Proc of the 8th Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2001:408- 415
                            </a>
                        </p>
                        <p id="367">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning and Representing Topic--A Hierarchical Mixture for">

                                <b>[10]</b>Hofmann T.Learning and representing topic—A hierarchical mixture model for word occurrence in document databases[C] //Proc of the Workshop on Learning from Text and the Web.Pittsburgh, PA:CMU, 1998
                            </a>
                        </p>
                        <p id="369">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Matching words and pictures">

                                <b>[11]</b>Barnard K, Duygulu P, Forsyth D, et al.Matching words and pictures[J].Journal of Machine Learning Research, 2003, 3 (2) :1107- 1135
                            </a>
                        </p>
                        <p id="371">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                Blei D M, Ng A Y, Jordan M I.Latent Dirichlet allocation[J].Journal of Machine Learning Research, 2003, 3 (1) :993- 1022
                            </a>
                        </p>
                        <p id="373">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view clustering via joint nonnegative matrix factorization">

                                <b>[13]</b>Gao Jing, Han Jiawei, Liu Jailu, et al.Multi-view clustering via joint nonnegative matrix factorization[C] //Proc of the 13th SIAM Int Conf on Data Mining.Philadelphia, PA:SIAM, 2013:252- 260
                            </a>
                        </p>
                        <p id="375">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view k-means clustering on big data">

                                <b>[14]</b>Cai Xiao, Nie Feiping, Huang Heng.Multi-view k-means clustering on big data[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2013:2598- 2604
                            </a>
                        </p>
                        <p id="377">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cluster ensembles - A knowledge reuse framework for combining multiple partitions">

                                <b>[15]</b>Strehl A, Ghosh A.Cluster ensembles-a knowledge reuse framework for combining multiple partitions[J].The Journal of Machine Learning Research, 2003 (3) :583- 617
                            </a>
                        </p>
                        <p id="379">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust ensemble clustering using probability trajectories">

                                <b>[16]</b>Huang Dong, Lai Jianhuang, Wang Changdong.Robust ensemble clustering using probability trajectories[J].IEEE Transactions on Knowledge and Data Engineering, 2016, 28 (5) :1312- 1326
                            </a>
                        </p>
                        <p id="381">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locally Weighted Ensemble Clustering">

                                <b>[17]</b>Huang Dong, Wang Changdong, Lai Jianhuang.Locally weighted ensemble clustering[J].IEEE Transactions on Cybernetics, 2017, 48 (5) :1460- 1473
                            </a>
                        </p>
                        <p id="383">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The multi-feature information bottleneckwith application to unsupervised image categorization">

                                <b>[18]</b>Lou Zhengzheng, Ye Yangdong, Yan Xiaoqiang.The multi-feature information bottleneck with application to unsupervised image categorization[C] //Proc of the 23rd Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2013:1508- 1515
                            </a>
                        </p>
                        <p id="385">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES0DF6210F390424192B1AF125B1D626ED&amp;v=MjQ4MjViUTM1TjFoeDd5M3c2QT1OaWZPZmJQTWFOZk9ybzh6WitJUENINDl6aDhSbUQ0TVBuN2dxV0EwRGJTV1E4L3JDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>Yan Xiaoqiang, Ye Yangdong, Lou Zhengzheng.Unsuper-vised video categorization based on multivariate information bottleneck method[J].Knowledge-Based Systems, 2015, 84 (C) :34- 45
                            </a>
                        </p>
                        <p id="387">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view semantic learning for data representation">

                                <b>[20]</b>Luo Peng, Peng Jinye, Guan Ziyu, et al.Multi-view semantic learning for data representation[J].IEEE Transactions on Knowledge and Data Engineering, 2015, 27 (11) :3016- 3028
                            </a>
                        </p>
                        <p id="389">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801006&amp;v=MDcxNDNCZHJHNEg5bk1ybzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2aFc3ekFMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Zhao Qi, Li Zongmin, Cross-modal social image clustering[J].Chinese Journal of Computers, 2018, 41 (1) :98- 111 (in Chinese) (赵其鲁, 李宗民.跨模态社交图像聚类[J].计算机学报, 2018, 41 (1) :98- 111) 
                            </a>
                        </p>
                        <p id="391">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CCL:cross-modal correlation learning with multigrained fusion by hierarchical network">

                                <b>[22]</b>Peng Yuxin, Qi Jinwei, Huang Xin, et al.CCL:Cross-modal correlation learning with multi-grained fusion by hierarchical network[J].IEEE Transactions on Multimedia, 2018, 20 (2) :405- 420
                            </a>
                        </p>
                        <p id="393">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-media shared representation by hierarchical learning with multiple deep networks">

                                <b>[23]</b>Peng Yuxin, Huang Xin, Qi Jinwei.Cross-media shared representation by hierarchical learning with multiple deep networks[C] //Proc of the 25th Int Joint Conf on Artificial Intelligence.Palo Alto, CA:AAAI, 2016:3846- 3853
                            </a>
                        </p>
                        <p id="395">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The information bottleneck method">

                                <b>[24]</b>Tishby N, Pereira F, Bialek W.The information bottleneck method[C] //Proc of the 37th Allerton Conf on Communication, Control and Computing.Piscataway, NJ:IEEE, 1999:368- 377
                            </a>
                        </p>
                        <p id="397">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The information bottleneck:Theory and applications">

                                <b>[25]</b>Slonim N.The information bottleneck:Theory and applications[D].Hebrew, IL:Hebrew University, 2002
                            </a>
                        </p>
                        <p id="399">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-task clustering of human actions by sharing information">

                                <b>[26]</b>Yan Xiaoqiang, Hu Shizhe, Ye Yangdong.Multi-task clustering of human actions by sharing information[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:6401- 6409
                            </a>
                        </p>
                        <p id="401">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond bags of features:Spatial pyramid matching forrecognizing natural scene categories">

                                <b>[27]</b>Svetlana L, Cordelia S, Jean P.Beyond bags of features:Spatial pyramid matching for recognizing natural scene categories[C] //Proc of the 19th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2006:2169- 2178
                            </a>
                        </p>
                        <p id="403">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A New Approach to Cross-modal Multimedia Retrieval">

                                <b>[28]</b>Rasiwasia N, Pereira J C, Coviello E, et al.A new approach to cross-modal multimedia retrieval[C] //Proc of the 18th ACM Int Conf on Multimedia.New York:ACM, 2010:251- 260
                            </a>
                        </p>
                        <p id="405">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjQxNjlNSDdSN3FlYnVkdEZDM2xVTHZBSTFZPU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hj&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b>Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91- 110
                            </a>
                        </p>
                        <p id="407">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collecting image annotations using Amazon&amp;#39;&amp;#39;s Mechanical Turk">

                                <b>[30]</b>Rashtchian C, Young P, Hodosh M, et al.Collecting image annotations using Amazon's Mechanical Turk[C] //Proc of the Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk.Stroudsburg, PA:ACL, 2010:139- 147
                            </a>
                        </p>
                        <p id="409">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MTA0ODRGQzNsVUx2QUkxWT1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b>Everingham M, Gool L V, Williams C K I, et al.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303- 338
                            </a>
                        </p>
                        <p id="411">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accounting for the Relative Importance of Objects in Image Re-trieval">

                                <b>[32]</b>Hwang S J, Grauman K.Accounting for the relative importance of objects in image retrieval[C] //Proc of the British Machine Vision Conf.Berlin:Springer, 2010:1- 12
                            </a>
                        </p>
                        <p id="413">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-Supervised Cross-Media Feature Learning With Unified Patch Graph Regularization">

                                <b>[33]</b>Peng Yuxin, Zhai Xiaohua, Zhao Yunchao, et al.Semi-supervised cross-media feature learning with unified patch graph regularization[J].IEEE Transactions on Circuits &amp; Systems for Video Technology, 2016, 26 (3) :583- 596
                            </a>
                        </p>
                        <p id="415">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Cross-media Joint Representation with Sparse and Semi-supervised Regularization">

                                <b>[34]</b>Zhai Xiaohua, Peng Yuxin, Xiao Jianguo.Learning cross-media joint representation with sparse and semisupervised regularization[J].IEEE Transactions on Circuits &amp; Systems for Video Technology, 2014, 24 (6) :965- 978
                            </a>
                        </p>
                        <p id="417">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An overview of cross-media retrieval:concepts,methodologies,benchmarks and challenges">

                                <b>[35]</b>Peng Yuxin, Huang Xin, Zhao Yunchao.An overview of cross-media retrieval:Concepts, methodologies, benchmarks and challenges[J].IEEE Transactions on Circuits &amp; Systems for Video Technology, 2017, 28 (9) :2372- 2385
                            </a>
                        </p>
                        <p id="419">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HMDB:A large video database for human motion recognition">

                                <b>[36]</b>Kuehne H, Jhuang H, Garrote E, et al.HMDB:A large video database for human motion recognition[C] //Proc of the 13th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2011:2556- 2563
                            </a>
                        </p>
                        <p id="421">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Realistic Human Actions from Movies">

                                <b>[37]</b>Laptev I, Marszalek M, Schmid C, et al.Learning realistic human actions from movies[C] //Proc of the 21st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1- 8
                            </a>
                        </p>
                        <p id="423">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and high quality multilevel scheme for partitioning irregular graphs">

                                <b>[38]</b>Karypis G, Kumar V.A fast and high quality multilevel scheme for partitioning irregular graphs[J].SIAM Journal on Scientific Computing, 1998, 20 (1) :359- 392
                            </a>
                        </p>
                        <p id="425">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentation using superpixels:A bipartite graph partitioning approach">

                                <b>[39]</b>Chang Shifu, Wu Xiaoming, Li Zhenguo.Segmentation using superpixels:A bipartite graph partitioning approach[C] //Proc of the 25th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:789- 796
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201907002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907002&amp;v=MTQwMjRSTE9lWmVSckZ5dmhXN3pBTHl2U2RMRzRIOWpNcUk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVISFZ1RFQveTV1NlBzSGloMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

