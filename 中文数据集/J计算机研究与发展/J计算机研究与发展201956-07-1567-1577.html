

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127910303556250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201907021%26RESULT%3d1%26SIGN%3d%252fj%252fyODzefXziazj7r4jF%252buXIRNg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907021&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201907021&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907021&amp;v=MTA5NTVxcUJ0R0ZyQ1VSTE9lWmVSckZ5cm5VcnpBTHl2U2RMRzRIOWpNcUk5SFpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#59" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;2 海量并行贝叶斯因子化分析方法 (G&lt;/b&gt; -&lt;b&gt;BF&lt;/b&gt;)  "><b>2 海量并行贝叶斯因子化分析方法 (G</b> -<b>BF</b>) </a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;2.1 贝叶斯分解算法&lt;/b&gt;"><b>2.1 贝叶斯分解算法</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;2.2 海量并行贝叶斯因子化分解方法&lt;/b&gt;"><b>2.2 海量并行贝叶斯因子化分解方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#154" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#157" data-title="&lt;b&gt;3.1 运行效率&lt;/b&gt;"><b>3.1 运行效率</b></a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;3.2 可扩展性&lt;/b&gt;"><b>3.2 可扩展性</b></a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;3.3 处理超大规模张量的性能探究&lt;/b&gt;"><b>3.3 处理超大规模张量的性能探究</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#176" data-title="&lt;b&gt;4 总  结&lt;/b&gt; "><b>4 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="图1 G -BF方法并行加速设计策略图">图1 G -BF方法并行加速设计策略图</a></li>
                                                <li><a href="#149" data-title="图2 G -BF应用H-PARAFAC架构结合设计图">图2 G -BF应用H-PARAFAC架构结合设计图</a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表1 实验环境配置&lt;/b&gt;"><b>表1 实验环境配置</b></a></li>
                                                <li><a href="#160" data-title="图3 G -HALS和G -BF处理大小相同张量单次执行时间 (4-阶) ">图3 G -HALS和G -BF处理大小相同张量单次执行时间 (4-阶) </a></li>
                                                <li><a href="#164" data-title="图4 G -HALS和G -BF处理相同张量迭代终止时间">图4 G -HALS和G -BF处理相同张量迭代终止时间</a></li>
                                                <li><a href="#165" data-title="图5 G -HALS和G -BF分解张量的迭代次数">图5 G -HALS和G -BF分解张量的迭代次数</a></li>
                                                <li><a href="#168" data-title="图6 不同秩下G -HALS和G -BF分解张量的单次执行时间">图6 不同秩下G -HALS和G -BF分解张量的单次执行时间</a></li>
                                                <li><a href="#169" data-title="图7 G -BF处理不同维度张量 (10 的单次执行时间">图7 G -BF处理不同维度张量 (10 的单次执行时间</a></li>
                                                <li><a href="#173" data-title="图8 当数据规模&amp;gt;10时分解大张量总执行时间">图8 当数据规模&gt;10时分解大张量总执行时间</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="206">


                                    <a id="bibliography_1" title="Goldenfeld N.Simple lessons from complexity[J].Science, 1999, 284 (5411) :87- 89" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simple lessons from complexity">
                                        <b>[1]</b>
                                        Goldenfeld N.Simple lessons from complexity[J].Science, 1999, 284 (5411) :87- 89
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_2" title="Wang Lizhe, Zhang Jiabin, Liu Peng, et al.Spectral-spatial multi-feature-based deep learning for hyperspectral remote sensing image classification[J].Soft Computing, 2017, 21 (1) :213- 221" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral-spatial multi-feature-based deep learning for hyperspectral remote sensing image classification">
                                        <b>[2]</b>
                                        Wang Lizhe, Zhang Jiabin, Liu Peng, et al.Spectral-spatial multi-feature-based deep learning for hyperspectral remote sensing image classification[J].Soft Computing, 2017, 21 (1) :213- 221
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_3" title="Cunningham J P, Yu B M.Dimensionality reduction for large-scale neural recordings[J].Nature Neuroscience, 2014, 17 (11) :1500- 1509" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dimensionality reduction for largescale neural recordings">
                                        <b>[3]</b>
                                        Cunningham J P, Yu B M.Dimensionality reduction for large-scale neural recordings[J].Nature Neuroscience, 2014, 17 (11) :1500- 1509
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_4" title="Hyvarinen A, Oja E.Independent component analysis:Algorithms and applications[J].Neural Networks, 2000, 13 (4) :411- 430" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300070568&amp;v=MDk4MjFUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSWx3VmFSbz1OaWZPZmJLN0h0RE9ySTlGWk93UENYb3hvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Hyvarinen A, Oja E.Independent component analysis:Algorithms and applications[J].Neural Networks, 2000, 13 (4) :411- 430
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_5" title="Pearson K F R S.LIII.On lines and planes of closest fit to systems of points in space[J].Philosophical Magazine, 1901, 2 (11) :559- 572" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD910468439&amp;v=MjI4MDhIdFhLcDR0R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5cm5VcnpBTmpuQmFycTU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Pearson K F R S.LIII.On lines and planes of closest fit to systems of points in space[J].Philosophical Magazine, 1901, 2 (11) :559- 572
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_6" title="Gu Yu, Xu Zongben, Sun Jian, et al.An intrusion detection ensemble system based on the features extracted by PCA and ICA[J].Journal of Computer Research and Development, 2006, 43 (4) :633- 638 (in Chinese) (谷雨, 徐宗本, 孙剑, 等.基于PCA与ICA特征提取的入侵检测集成分类系统[J].计算机研究与发展, 2006, 43 (4) :633- 638) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200604009&amp;v=MTM2MzM0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXJuVXJ6QUx5dlNkTEc0SHRmTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Gu Yu, Xu Zongben, Sun Jian, et al.An intrusion detection ensemble system based on the features extracted by PCA and ICA[J].Journal of Computer Research and Development, 2006, 43 (4) :633- 638 (in Chinese) (谷雨, 徐宗本, 孙剑, 等.基于PCA与ICA特征提取的入侵检测集成分类系统[J].计算机研究与发展, 2006, 43 (4) :633- 638) 
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_7" title="Chen Dan, Li Xiaoli, Wang Lizhe, et al.Fast and scalable multi-way analysis of massive neural data[J].IEEE Transactions on Computers, 2015, 64 (3) :707- 719" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and scalable multi-way analysis of massive neural data">
                                        <b>[7]</b>
                                        Chen Dan, Li Xiaoli, Wang Lizhe, et al.Fast and scalable multi-way analysis of massive neural data[J].IEEE Transactions on Computers, 2015, 64 (3) :707- 719
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_8" title="Bro R.PARAFAC-tutorial and applications[J].Chemometrics and Intelligent Laboratory Systems, 1997, 38 (2) :149- 171" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300186560&amp;v=MjE5MjNUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJbHdWYVJvPU5pZk9mYks3SHRET3JJOUZaZU1KQ1hvNW9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Bro R.PARAFAC-tutorial and applications[J].Chemometrics and Intelligent Laboratory Systems, 1997, 38 (2) :149- 171
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_9" title="Wang Juan, Li Xiaoli, Lu Chengbiao, et al.Characteristics of evoked potential multiple EEG recordings in patients with chronic pain by means of parallel factor analysis[J].Computational and Mathematical Methods in Medicine, 2012, 26 (2) :27- 60" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD120615008285&amp;v=MjQ2MDB6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3M0tJVjBjTmlmRGFySzZIdGZOcW85RmJPa0hDUk04&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Wang Juan, Li Xiaoli, Lu Chengbiao, et al.Characteristics of evoked potential multiple EEG recordings in patients with chronic pain by means of parallel factor analysis[J].Computational and Mathematical Methods in Medicine, 2012, 26 (2) :27- 60
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_10" title="Comon P, Luciani X, Almeida A L F D.Tensor decompositions, alternating least squares and other tales[J].Journal of Chemometrics, 2009, 23 (7/8) :393- 405" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensor decompositions, alternating least squares and other tales">
                                        <b>[10]</b>
                                        Comon P, Luciani X, Almeida A L F D.Tensor decompositions, alternating least squares and other tales[J].Journal of Chemometrics, 2009, 23 (7/8) :393- 405
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_11" title="Phan .Fast alternating LS algorithms for high order CANDECOMP/PARAFAC tensor factorizations[J].IEEE Transactions on Signal Processing, 2013, 61 (19) :4834- 4846" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast alternating LS algorithms for high order CAN-DECOMP/PARAFAC tensor factorizations">
                                        <b>[11]</b>
                                        Phan .Fast alternating LS algorithms for high order CANDECOMP/PARAFAC tensor factorizations[J].IEEE Transactions on Signal Processing, 2013, 61 (19) :4834- 4846
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_12" title="Cichocki A, Zdunek R, Amari S I.Hierarchical ALS algorithms for nonnegative matrix and 3D tensor factorization[C] //Proc of the 7th Int Conf on Independent Component Analysis and Signal Separation.Berlin:Springer, 2007:169- 176" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical ALS algorithms for nonnegative matrix and 3D tensor factorizat ion">
                                        <b>[12]</b>
                                        Cichocki A, Zdunek R, Amari S I.Hierarchical ALS algorithms for nonnegative matrix and 3D tensor factorization[C] //Proc of the 7th Int Conf on Independent Component Analysis and Signal Separation.Berlin:Springer, 2007:169- 176
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_13" title="Fang Minquan, Zhang Weimin, Fang Jianbin, et al.GPU Programming and Code Optimization High Performance Computing for the Masses[M].Beijing:Tsinghua University Press, 2016:13- 141 (in Chinese) (方民权, 张卫民, 方建滨, 等.GPU编程与优化——大众高性能计算[M].北京:清华大学出版社, 2016:13- 141) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302446422000&amp;v=MDgwMjhtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZnVTczS0lWMGNYRnF6R2JDNEhOWElxWXRIWnVzUERCTTh6eFVT&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Fang Minquan, Zhang Weimin, Fang Jianbin, et al.GPU Programming and Code Optimization High Performance Computing for the Masses[M].Beijing:Tsinghua University Press, 2016:13- 141 (in Chinese) (方民权, 张卫民, 方建滨, 等.GPU编程与优化——大众高性能计算[M].北京:清华大学出版社, 2016:13- 141) 
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_14" title="Chen Dan, Hu Yangyang, Wang Lizhe, et al.H-PARAFAC:Hierarchical parallel factor analysis of multidimensional big data[J].IEEE Transactions on Parallel &amp;amp; Distributed Systems, 2017, 28 (4) :1091- 1104" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=H-PARAFAC Hierarchical parallel factor analysis of multidimensional big data">
                                        <b>[14]</b>
                                        Chen Dan, Hu Yangyang, Wang Lizhe, et al.H-PARAFAC:Hierarchical parallel factor analysis of multidimensional big data[J].IEEE Transactions on Parallel &amp;amp; Distributed Systems, 2017, 28 (4) :1091- 1104
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_15" title="Bae S H, Choi J Y, Qiu J, et al.Dimension reduction and visualization of large high-dimensional data via interpolation[C] //Proc of the 19th ACM Int Symp on High Performance Distributed Computing.New York:ACM, 2010:203- 214" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dimension reduction and Visualization of large high-dimensional data via interpolation">
                                        <b>[15]</b>
                                        Bae S H, Choi J Y, Qiu J, et al.Dimension reduction and visualization of large high-dimensional data via interpolation[C] //Proc of the 19th ACM Int Symp on High Performance Distributed Computing.New York:ACM, 2010:203- 214
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_16" title="Agullo E, Augonnet C, Dongarra J, et al.QR factorization on a multicore node enhanced with multiple GPU accelerators[C] //Proc of the 25th IEEE Int Parallel &amp;amp; Distributed Processing Symp.Piscataway, NJ:IEEE, 2011:932- 943" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=QR factorization on a multicore node enhanced with multiple GPU accelerators">
                                        <b>[16]</b>
                                        Agullo E, Augonnet C, Dongarra J, et al.QR factorization on a multicore node enhanced with multiple GPU accelerators[C] //Proc of the 25th IEEE Int Parallel &amp;amp; Distributed Processing Symp.Piscataway, NJ:IEEE, 2011:932- 943
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_17" title="Tan Wei, Cao Liangliang, Fong Liana.Faster and cheaper:Parallelizing large-scale matrix factorization on GPUS[C] //Proc of the 25th ACM Int Symp on High-Performance Parallel and Distributed Computing.New York:ACM, 2016:219- 230" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster and cheaper Parallelizing large-scale matrix factorization on GPUS">
                                        <b>[17]</b>
                                        Tan Wei, Cao Liangliang, Fong Liana.Faster and cheaper:Parallelizing large-scale matrix factorization on GPUS[C] //Proc of the 25th ACM Int Symp on High-Performance Parallel and Distributed Computing.New York:ACM, 2016:219- 230
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_18" title="Zou Benyou, Li Cuiping, Tan Liwen, et al.GPUTENSOR:Efficient tensor factorization for context-aware recommenda-tions[J].Information Sciences, 2015, 299 (11) :159- 177" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200009403&amp;v=MDg1MzVpclJkR2VycVFUTW53WmVadUh5am1VTHJJSWx3VmFSbz1OaWZPZmJLOEg5UE5yWTlGWk9zR0NIdzZvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        Zou Benyou, Li Cuiping, Tan Liwen, et al.GPUTENSOR:Efficient tensor factorization for context-aware recommenda-tions[J].Information Sciences, 2015, 299 (11) :159- 177
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_19" title="Shin Kijung, Lee S, Kang U.Fully scalable methods for distributed tensor factorization[J].IEEE Transactions on Knowledge and Data Engineering, 2016, 29 (1) :100- 113" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully scalable methods for distributed tensor factorization">
                                        <b>[19]</b>
                                        Shin Kijung, Lee S, Kang U.Fully scalable methods for distributed tensor factorization[J].IEEE Transactions on Knowledge and Data Engineering, 2016, 29 (1) :100- 113
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_20" title="Zhao Qibin, Zhang Liqing, Cichocki A.Bayesian CP factorization of incomplete tensors with automatic rank determination[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1751- 1763" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bayesian CP factorization of incomplete tensors with automatic rank determination">
                                        <b>[20]</b>
                                        Zhao Qibin, Zhang Liqing, Cichocki A.Bayesian CP factorization of incomplete tensors with automatic rank determination[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1751- 1763
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_21" title="Kang U, Papalexakis E, Harpale A, et al.GigaTensor:Scaling tensor analysis up by 100 times-algorithms and discoveries[C] //Proc of the 21st ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM, 2012:316- 324" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GigaTensor:Scaling Tensor Analysis Up By 100 Times-Algorithms and Discoveries">
                                        <b>[21]</b>
                                        Kang U, Papalexakis E, Harpale A, et al.GigaTensor:Scaling tensor analysis up by 100 times-algorithms and discoveries[C] //Proc of the 21st ACM SIGKDD Int Conf on Knowledge Discovery &amp;amp; Data Mining.New York:ACM, 2012:316- 324
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(07),1567-1577 DOI:10.7544/issn1000-1239.2019.20180792            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向时间序列大数据海量并行贝叶斯因子化分析方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E8%85%BE%E9%A3%9E&amp;code=42264663&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高腾飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8B%87%E7%90%B0&amp;code=42264664&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘勇琰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%A4%E4%BA%91%E6%B3%A2&amp;code=42264665&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汤云波</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%9E%92&amp;code=17649396&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张垒</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E4%B8%B9&amp;code=08981530&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈丹</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>时间序列大数据记录着复杂系统在时间和空间上大尺度的演化过程, 详细描述了系统不同部分之间的相互作用和相互联系.提取时间序列大数据中潜在的低维因子对研究复杂系统的整体机制有着至关重要的作用.大数据的超高维和大尺度导致许多传统因子分析方法难以适应, 先验知识缺乏更增加了研究难度.针对这一巨大挑战, 提出了一种面向时间序列大数据的海量并行贝叶斯因子化分析方法 (the massively parallel Bayesian factorization approach, G-BF) .在缺失先验知识的情况下, 通过贝叶斯算法导出因子矩阵, 将算法映射至CUDA (compute unified device architecture) 模型, 以大规模并行的方式更新因子矩阵.该方法支持对任意维度张量的因子分解.实验结果表明:1) 与通过GPU加速化的因子分解算法G-HALS (GPU-hierarchical alternative least square) 相比, G-BF具有更好的运行性能, 且随着数据规模的增加, 其性能优越性更加明显;2) G-BF在数据处理规模、秩及维度方面都具有良好的可扩展性;3) 将G-BF应用于现有子因子融合框架 (hierarchical-parallel factor analysis, H-PARAFAC) , 可将“巨型”张量作为一个整体进行因子化分解 (在2个节点上处理10<sup>11</sup>个数据元素) , 其能力较常规方法高出2个数量级.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贝叶斯模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%A4%A7%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时间序列大数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%A0%E9%87%8F%E5%88%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张量分解;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%B7%E9%87%8F%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海量并行计算;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%9F%E4%B8%80%E8%AE%A1%E7%AE%97%E8%AE%BE%E5%A4%87%E6%9E%B6%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">统一计算设备架构;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    Gao Tengfei, born in 1993. Master candidate.His main research interests include high performance computing and data engineering. gaotengfei@whu.edu.cn;
                                </span>
                                <span>
                                    Liu Yongyan, born in 1995.Master.His main research interests include parallel computing and data engineering. (201728 2110215@whu.edu.cn) ;
                                </span>
                                <span>
                                    Tang Yunbo, born in 1994.Master.His main research interests include neuroin-formatics and data engineering. (20162021 10072@whu.edu.cn) ;
                                </span>
                                <span>
                                    Zhang  Lei, born  in  1997. Master candidate.His main research interests include  machine  learning  and  bioin-formatics. (leizhsu@gmail.com) ;
                                </span>
                                <span>
                                    *Chen Dan, born in 1973.Professor.Senior member of CCF.He was an HEFCE research fellow with the University of Birmingham, U.K.His main research interests include data science and engineering, neuroinformatics, and complex systems. dan.chen@whu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61772380);</span>
                                <span>湖北省自然科学基金创新群体项目 (2017CFA007);</span>
                    </p>
            </div>
                    <h1><b>A Massively Parallel Bayesian Approach to Factorization-Based Analysis of Big Time Series Data</b></h1>
                    <h2>
                    <span>Gao Tengfei</span>
                    <span>Liu Yongyan</span>
                    <span>Tang Yunbo</span>
                    <span>Zhang Lei</span>
                    <span>Chen Dan</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Big time series data record the evolvement of a complex system (s) in large temporal and spatial scales with great details of the interactions amongst different parts of the system. Extracting the latent low-dimensional factors plays a crucial role in examining the overall mechanism of the underlying complex system (s) . Research challenges arise with the lack of a priori knowledge, and most conventional factorization methods are not able to adapt to the ultra-high dimension and scales of the big data. Aiming at the grand challenge, this study develops a massively parallel Bayesian approach (G-BF) to factorization-based analysis of tensors formed by massive time series. The approach relies on a Bayesian algorithm to derive the factor matrices in the absence of a priori information. Then the algorithm has been mapped to the compute unified device architecture (CUDA) model to update the factor matrices in a massively parallel manner. The proposed approach is designed to support factorization of tensors of arbitrary dimensions. Experimental results indicated that 1) In comparison with GPU-hierarchical alternative least square (G-HALS) , G-BF exhibits much better runtime performance and the superiority becomes more obvious with the increasing data scale; 2) G-BF has excellent scalability in terms of both data volume and rank; 3) Applying G-BF to the existing framework for fusing sub-factors (hierarchical-parallel factor analysis, H-PARAFAC) , it becomes possible to factorize a huge tensor (volume up to 10<sup>11</sup> over two nodes) as a whole with the capability two magnitudes higher than conventional methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bayesian%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bayesian model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=big%20time%20series%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">big time series data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=tensor%20factorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">tensor factorization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=massively%20parallel%20computing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">massively parallel computing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=compute%20unified%20device%20architecture%20(CUDA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">compute unified device architecture (CUDA) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61772380);</span>
                                <span>the Innovation Group Project of Natural Science Foundation of Hubei Province of China (2017CFA007);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="51">从地球、经济到人脑, 在瞬态频率和复杂谐波的演化过程中形成了复杂系统.对于复杂系统的工作机理的探索一直是多学科研究的核心<citation id="248" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.随着数据采集技术和采集设备的飞速发展, 复杂系统研究中对于数据处理的要求越来越高, 所需处理数据的规模、维度及复杂性也在迅速增加.海量规模的时间序列大数据记录了复杂系统在时间和空间尺度上的演化.挖掘时间序列数据的潜在深度信息对于复杂系统过往数据分析和未来行为预测具有非常重要的意义<citation id="249" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.时间序列大数据包含了复杂系统中各个“个体单位”之间以及多个复杂系统间的相互联系.其外部表征具有数据相关性和多重数据属性, 这一特性也决定了时间序列大数据具有较高的维度.</p>
                </div>
                <div class="p1">
                    <p id="52">提取时间序列大数据潜在因子对研究复杂系统的整体机制起着至关重要的作用<citation id="250" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.高维科学数据中的一般潜在低维变量揭示了被观测系统动力学不断变化的整体机制.然而数据元素之间复杂的相互依赖关系、数据的高维度和海量的数据规模阻碍了其对整体机制的揭示.在小数据时代, 时间序列大数据中低维特征的提取被认为是张量分解的数学问题.然而, 直接采用传统的分析方法——独立成分分析<citation id="251" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation> (independent component analysis, ICA) 及主成分分析<citation id="256" type="reference"><link href="214" rel="bibliography" /><link href="216" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation> (principal component analysis, PCA) ——无法完整准确地提取出高维张量数据的潜在信息.这些方法使得维度之间的信息丢失和相关性破坏<citation id="252" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 所提取的线性特征信息物理解释性较差.平行因子分析 (parallel factor analysis, PARAFAC<citation id="253" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>) 和Tucker模型分解<citation id="254" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>适用于高维张量分解, 其中因子化更新通常使用交替最小二乘法 (alternating least squares, ALS) <citation id="255" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>或其他相关的替代算法<citation id="257" type="reference"><link href="226" rel="bibliography" /><link href="228" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>来解决.时间序列大数据具有动态增长的数据特征.数据规模不断增长, 小数据时代的张量因子分解方法难以高效地应用于大规模、高维度的时间序列大数据.为保证原始关键信息提取的高效和完整, 迫切需要开发一种面向大规模高维度的时间序列大数据分解分析方法, 以实现快速有效的因子信息提取.</p>
                </div>
                <div class="p1">
                    <p id="53">时间序列大数据的数据规模是巨大的, 并且在空间中快速增长, 无法保证对问题域有足够稳定的先验知识, 这就导致无法确定张量分解的初始条件.良好的因子化分解初始条件和快速迭代的因子更新过程在大规模高维时间序列大数据分解过程中极其重要的.如何在缺失先验知识的前提下, 适应大规模和超高维的数据特征, 快速获取因子成为了时间序列大数据分析的重要研究挑战.为了解决上述挑战:1) 基于贝叶斯因子化分解方法, 本文提出一种在没有先验信息域和无需精确初始化过程即可快速获取收敛因子的海量并行贝叶斯因子化分析方法 (the massively parallel Bayesian factorization approach, G -BF) ;2) G -BF通过NVIDIA通用并行计算架构 (compute unified device architecture, CUDA) <citation id="258" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>进行核函数映射, 并行加速因子化迭代更新过程;3) G -BF使用高维数据映射方法完成对任意维度的时间序列大数据的分解要求;4) 此外, G -BF结合粗粒度与细粒度2种并行方式应用于子因子融合框架 (hierarchical-parallel factor analysis, H-PARAAC) <citation id="259" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>完成对超大规模张量的因子化分解.</p>
                </div>
                <div class="p1">
                    <p id="54">为了评估G -BF的性能, 本研究进行了一系列高维张量数据分析实验并与经过GPU加速的G -HALS算法对比探究G -BF的性能效率.实验结果表明:1) G -BF在Maxwell架构的GPU运行性能优于G -HALS;2) G -BF在处理高维数据规模、秩和维度等方面具有出色的可扩展性, 可处理任意维度数据;3) 此外, G -BF应用于现有的H-PARAFAC架构在处理超大规模高维张量时表现了优异的处理速度和性能, 打破了可分析数据的规模限制.</p>
                </div>
                <div class="p1">
                    <p id="55">本研究方法主要有3个方面优势:</p>
                </div>
                <div class="p1">
                    <p id="56">1) G -BF能够在无需问题域的先验知识情况下快速准确地分解大规模时间序列大数据, 获得低维特征因子信息;</p>
                </div>
                <div class="p1">
                    <p id="57">2) G -BF针对高维时间序列大数据具有普适性, 可分解数据维度可变, 不限于3维或其他限定维度;</p>
                </div>
                <div class="p1">
                    <p id="58">3) G -BF嵌入到H-PARAFAC架构中作为子张量因子分解过程的解决方案, 在分解大规模张量时具有优越的性能, 打破了处理数据规模大小的限制.</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="60">针对大规模张量因子化分解问题, 许多成功的方法和框架已经得到发展.大型矩阵及张量分解的性能已经成为研究的一大挑战.针对此问题的相关工作大致从优化分解过程和提高分解性能2个方面入手.</p>
                </div>
                <div class="p1">
                    <p id="61">Bae等人<citation id="260" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了一种高效多维并行可视化处理大规模科学数据算法 (SMACOF) .该算法利用集群消息传递接口并行矩阵操作和矩阵划分, 实现了高维矩阵到低维欧氏空间的转换.</p>
                </div>
                <div class="p1">
                    <p id="62">在线性分析问题中, 特征值问题、奇异值问题以及QR分解 (QR decomposition) 问题极其重要.Agullo等人<citation id="261" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了一种将矩阵分解为正交矩阵和上三角矩阵乘积的高效分解方法.该方法利用加速设备节点, 采用CPU/GPU混合架构来加速QR分解过程.</p>
                </div>
                <div class="p1">
                    <p id="63">Tan等人<citation id="262" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了一个矩阵分解库 (cuMF) 来优化交替最小二乘法 (ALS) 方法以求解超大规模矩阵分解.cuMF在单个GPU节点上使用多种策略优化ALS中内存访问, 同时将数据与模型并行化用于最小化GPU通信开销.</p>
                </div>
                <div class="p1">
                    <p id="64">Zou等人<citation id="263" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了一种并行张量分解算法 (GPUTENSOR) 加速大数据集的张量分解.该方法将大张量分成小块, 利用图形处理单元 (GPU) 固有并行性及高存储带宽并行相关操作, 同时使用零散方式优化计算策略, 以避免中间数据爆炸而牺牲性能.</p>
                </div>
                <div class="p1">
                    <p id="65">最近, Shin等人<citation id="264" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了用坐标下降法更新参数的大规模高阶张量分解算法CDTF.该方法具有良好的数据可扩展性, 运行在40个节点 (Xeon 2.4 GHz) 的Hadoop集群上时, 可分解有10亿个可观测条目的5阶张量.</p>
                </div>
                <div class="p1">
                    <p id="66">本研究的目标是在无需先验信息情况下高效分解大规模高维张量形式的时间序列大数据.所提出方法的研究包括:1) 运行效率;2) 分解的维度尺寸;3) 可处理大型张量数据规模限制.</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag"><b>2 海量并行贝叶斯因子化分析方法 (G</b> -<b>BF</b>) </h3>
                <div class="p1">
                    <p id="68">本节的主要内容包括4个部分:1) 介绍贝叶斯分解算法 (Bayesian factorization, BF) 的理论知识;2) 对本文提出的面向时间序列大数据海量并行贝叶斯因子化分析方法 (G -BF) 的算法设计和并行加速设计的描述;3) 简要介绍G -BF和H-PARAFA框架的融合设计;4) 对G -BF为满足处理任意维度分解需求采用的高维数据映射方法进行阐述.</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>2.1 贝叶斯分解算法</b></h4>
                <h4 class="anchor-tag" id="70" name="70">2.1.1 张量分解概率模型及贝叶斯先验</h4>
                <div class="p1">
                    <p id="71">定义张量分解的线性模型表达式为</p>
                </div>
                <div class="area_img" id="72">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907021_07200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="74">其中, <b><i>Y</i></b>表示大小为<i>I</i><sub>1</sub>×<i>I</i><sub>2</sub>×…×<i>I</i><sub><i>N</i></sub>的<i>N</i>维张量, 可以由因子张量<b><i>X</i></b>和噪声张量ϵ组成, 。表示外积, <i>R</i>表示因子矩阵秩, <b><i>U</i></b><sup> (<i>n</i>) </sup>表示大小为<i>I</i><sub><i>n</i></sub>×<i>R</i>的因子矩阵集合, <b><i>U</i></b><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>表示第<i>n</i>个因子矩阵的第<i>r</i>行.</p>
                </div>
                <div class="p1">
                    <p id="76">假设因子向量<b><i>U</i></b><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>和噪声张量ϵ均服从独立同分布的多维高斯分布<i>p</i> (<b><i>U</i></b><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) (1≤<i>n</i>≤<i>N</i>, 1≤<i>i</i><sub><i>n</i></sub>≤<i>I</i><sub><i>n</i></sub>) .因此, 无信息先验的分布表达式为</p>
                </div>
                <div class="area_img" id="79">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907021_07900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="81">其中, 大小为<i>R</i>×<i>R</i>的矩阵<i>σ</i>和标量<i>λ</i>为超参数.</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">2.1.2 基于变分贝叶斯推断的贝叶斯分解</h4>
                <div class="p1">
                    <p id="83">概率模型在变分贝叶斯框架下采用确定性近似推理<citation id="265" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 将自由能 (<i>Free</i>) 定义为<i>p</i> (<b><i>Y</i></b>) 似然估计的边界下限:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext> </mtext><mi>F</mi><mi>r</mi><mi>e</mi><mi>e</mi><mo>=</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mi>q</mi></mrow></mstyle><mo stretchy="false"> (</mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mo stretchy="false">) </mo><mo>×</mo></mtd></mtr><mtr><mtd><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><mo>, </mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mo stretchy="false">) </mo></mrow><mrow><mi>q</mi><mo stretchy="false"> (</mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext>d</mtext><mo stretchy="false"> (</mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">其中, <i>q</i> ({<b><i>U</i></b><sup> (<i>n</i>) </sup>}<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>表示任意概率分布, 近似真实的后验分布<i>p</i> ({<b><i>U</i></b><sup> (<i>n</i>) </sup>}<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>|<b><i>Y</i></b>) .因子矩阵的后验分布<i>p</i> (<b><i>U</i></b><sup> (<i>n</i>) </sup>|<b><i>Y</i></b>) (1≤<i>n</i>≤<i>N</i>) 以及更新的超参数<i>λ</i>, <i>σ</i>在自由能最大化分析时获得.因子矩阵采用行向量的形式表示, 沿每1行独立的行向量推导出其后验分布为<i>p</i> (<b><i>U</i></b><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|<b><i>Y</i></b><sub><i>i</i><sub><i>n</i></sub></sub>) ～N (<b><i>u</i></b><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, <i>Ψ</i><sup> (<i>n</i>) </sup>) , <b><i>Y</i></b><sub><i>i</i><sub><i>n</i></sub></sub>表示张量<b><i>Y</i></b>沿<i>n</i>-mode矩阵化张开矩阵的第<i>i</i>行向量.</p>
                </div>
                <div class="p1">
                    <p id="90">通过变分贝叶斯推断求<i>p</i> (<b><i>U</i></b><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|<b><i>Y</i></b><sub><i>i</i><sub><i>n</i></sub></sub>) 的近似分布<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msub><mo stretchy="false">) </mo><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn><mo>, </mo><mi>m</mi><mo>≠</mo><mi>n</mi></mrow><mi>Ν</mi></msubsup></mrow></msub><mo stretchy="false"> (</mo><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msub><mo>, </mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">其中, <i>E</i><sub>{<b><i>U</i></b><sup> (<i>m</i>) </sup>}</sub> (·) (1≤<i>m</i>≤<i>N</i>, <i>m</i>≠<i>n</i>) 为由后验概率<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msub><mo stretchy="false">) </mo><mtext>的</mtext></mrow></math></mathml>期望.通过计算<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mspace width="0.25em" /><mi>ln</mi><mspace width="0.25em" /><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">Y</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></mfrac></mrow></math></mathml>获得因子矩阵<b><i>u</i></b><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml> (均值) 和方差<i>Ψ</i><sup> (<i>n</i>) </sup>:</p>
                </div>
                <div class="area_img" id="98">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907021_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="102">符号<image id="280" type="formula" href="images/JFYZ201907021_28000.jpg" display="inline" placement="inline"><alt></alt></image>和⊙分别表示矩阵Hadamard和Khatri-Rao乘积, 概率模型采用点估计更新超参数<i>σ</i>, <i>λ</i>.当所述自由能最大化时, 求导计算<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>F</mi><mi>r</mi><mi>e</mi><mi>e</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">σ</mi></mrow></mfrac><mo>, </mo><mfrac><mrow><mo>∂</mo><mi>F</mi><mi>r</mi><mi>e</mi><mi>e</mi></mrow><mrow><mo>∂</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>得到<i>σ</i>, <i>λ</i>更新公式</p>
                </div>
                <div class="area_img" id="104">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907021_10400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="106">符号·表示内积.</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>2.2 海量并行贝叶斯因子化分解方法</b></h4>
                <h4 class="anchor-tag" id="108" name="108">2.2.1 G -BF并行加速设计策略</h4>
                <div class="p1">
                    <p id="109">G -BF是基于BF算法理论可以在GPU节点上运行的大规模并行方法.在无需先验信息及精确因子初始化的情况下, G -BF算法可准确快速迭代地分解大规模张量获取因子矩阵.本方法涉及的大型矩阵运算操作主要包括Khatri-Rao乘积、Hadamard乘积、逐元除法和矩阵乘法等.利用CUDA进行密集矩阵运算并行加速, 实现因子矩阵集的快速更新迭代求解.G -BF将运算过程作为设备并行模块映射到图1中CUDA核函数.G -BF分解大规模张量获取因子矩阵过程根据图1内容包含8个过程:</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 G -BF方法并行加速设计策略图" src="Detail/GetImg?filename=images/JFYZ201907021_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 G -BF方法并行加速设计策略图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 G -BF method parallel acceleration design  strategy diagram</p>

                </div>
                <div class="p1">
                    <p id="111">1) G -BF的第1步发生在主机端.首先, 读取待处理张量<b><i>Y</i></b>, 获取<b><i>Y</i></b>的维度大小<i>N</i>、因子化分析过程中表示因子集大小的参数<i>R</i>以及初始化超参数<i>λ</i>.定义G -BF最大迭代次数<i>ItemMax</i>.当迭代次数达到<i>ItemMax</i>时, 无论是否达到收敛条件都将停止因子矩阵更新.此参数是限制整个因子化过程的总体耗时的重要参数, 当不满足收敛条件时, <i>ItemMax</i>设定值越大, 通过迭代分解获取的因子矩阵越接近最优解, 但耗时也将更大.</p>
                </div>
                <div class="p1">
                    <p id="112">2) 该过程初始化G -BF中各项参数矩阵及参数张量, 此过程发生在主机端.首先将因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>随机初始化大小为<i>I</i><sub><i>n</i></sub>×<i>R</i>的矩阵;<b><i>S</i></b>与<b><i>G</i></b>是3维张量, 大小均为<i>N</i>×<i>R</i>×<i>R</i>, 初始化方法为:当第2维和第3维的索引相同时, 将值设为0.01, 其余元素都设置为0.</p>
                </div>
                <div class="p1">
                    <p id="113">3) 该过程在设备端负责计算初始化3维张量, 其大小为<i>N</i>×<i>R</i>×<i>R</i>.<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>的更新公式为</p>
                </div>
                <div class="p1">
                    <p id="114"><b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>=<b><i>U</i></b><sup> (<i>n</i>) <sup>T</sup></sup><b><i>U</i></b><sup> (<i>n</i>) </sup>+<i>I</i><sub><i>n</i></sub><b><i>G</i></b><sup> (<i>n</i>) </sup>, </p>
                </div>
                <div class="p1">
                    <p id="115">这里涉及的运算操作包括矩阵乘法和矩阵加法.根据<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>的更新公式可知, 各个<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>的更新过程相互独立.上角标<i>n</i>代表更新的不同维度.这里采用并行方式更新, 调用cudaStreams执行多个维度的<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>的并行更新, 每1个cudaStream执行任务相同.第<i>n</i>个cudaStream的执行内容为:将对应第<i>n</i>维的因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>和参数矩阵<b><i>G</i></b><sup> (<i>n</i>) </sup>从主机端复制到设备端, 主机端调用核函数在设备端计算<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>, 获得结果后拷贝回主机端.在完成上述准备后, 开始迭代进行因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>的维度更新, 设置<i>item</i>=1, <i>n</i>=1, <i>item</i>表示G -BF实际迭代周期数, <i>n</i>表示G -BF当前更新维度.</p>
                </div>
                <div class="p1">
                    <p id="116">4) 高维张量<b><i>Y</i></b>的存储映射.G -BF算法设计要求可分解任意维度张量, 但NVIDIA的CUDA计算架构的数据管理固定为1维数据.在G -BF中将张量<b><i>Y</i></b>通过张量矩阵化展开方式映射为1维向量进行存储.当维度更新时, 张量元素坐标根据维度间关系设定映射规则转换.该过程也可以在迭代更新之外完成, 如此可减少维度转换的时间消耗, 但这也将增加存储空间的负担.详细的维度映射方法将在2.2.4节给出.</p>
                </div>
                <div class="p1">
                    <p id="117">5) G -BF的核心内容, 其目的是更新每个迭代循环中各维度的因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>.大致分为3个部分:计算过程参数矩阵<b><i>T</i></b>、更新当前维度下参数矩阵<b><i>G</i></b><sup> (<i>n</i>) </sup>、更新当前维度下因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>.</p>
                </div>
                <div class="p1">
                    <p id="118">① 计算过程参数矩阵的主要内容为在主机端初始化过程参数矩阵<b><i>T</i></b>, 其大小为<i>R</i>×<i>R</i>, 数据值全部设为1/<i>λ</i>.调用核函数更新设备端过程参数矩阵<b><i>T</i></b>, 更新公式为<image id="281" type="formula" href="images/JFYZ201907021_28100.jpg" display="inline" placement="inline"><alt></alt></image>, 主要涉及的矩阵计算为Hadmard乘积.为减少核函数调用和数据传输带来的时间开销, 将除当前维度之外的因子矩阵合并成1维向量与过程参数矩阵<b><i>T</i></b>一同作为Hadmard乘运算核函数的输入, 输出结果为<b><i>T</i></b>的更新结果.</p>
                </div>
                <div class="p1">
                    <p id="119">② 更新当前维度下参数矩阵<b><i>G</i></b><sup> (<i>n</i>) </sup>更新规则为<b><i>G</i></b><sup> (<i>n</i>) </sup>= (<b><i>T</i></b>+<b><i>S</i></b><sup> (<i>n</i>) -1</sup>) <sup>-1</sup>, 需要通过CUDA进行矩阵求逆和矩阵加法的GPU加速.</p>
                </div>
                <div class="p1">
                    <p id="120">③ 更新当前维度下因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>其更新公式为<b><i>U</i></b><sup> (<i>n</i>) </sup>={ (<b><i>Y</i></b><sub> (<i>n</i>) </sub>/<i>λ</i>) <b><i>U</i></b><sup>⊙-<i>n</i></sup>}<b><i>G</i></b><sup> (<i>n</i>) </sup>, 这部分发生在设备端, 调用Khatri-Rao乘积核函数直接计算<b><i>U</i></b><sup>⊙-<i>n</i></sup>, 将<i>N</i>-1个因子矩阵拷贝到1个一维向量作为核函数的输入参数以便一步直接计算<b><i>U</i></b><sup>⊙-<i>n</i></sup>, 这样的方法减少了核函数的调用. (<b><i>Y</i></b><sub> (<i>n</i>) </sub>/<i>λ</i>) <b><i>U</i></b><sup>⊙-<i>n</i></sup>由于矩阵乘法特性以及输入矩阵规模过大导致本过程是整个G -BF中最耗时的步骤.调用设备端矩阵乘法核函数计算得到中间矩阵<b><i>M</i></b>, 再次调用矩阵乘法核函数计算<b><i>M</i></b>×<b><i>G</i></b><sup> (<i>n</i>) </sup>获得当前维度更新后的因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>.</p>
                </div>
                <div class="p1">
                    <p id="121">6) 对当前更新维度的因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>进行非负性和正则化处理.通过遍历将因子矩阵中的负数设为0或某一极小值以及将因子矩阵<b><i>U</i></b><sup> (<i>n</i>) </sup>以列优先方式进行归一化.此操作目的是为减少计算复杂性以及满足H-PARAFAC框架.通过对因子矩阵附加约束, 保证了结合H-PARAFAC框架后结果的唯一性.</p>
                </div>
                <div class="p1">
                    <p id="122">7) 参数矩阵<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>和参数张量<b><i>S</i></b>的更新.该更新计算过程在设备端运行.首先更新当前维度中的参数矩阵<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>, 更新规则为<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>=<b><i>U</i></b><sup> (<i>n</i>) <sup>T</sup></sup><b><i>U</i></b><sup> (<i>n</i>) </sup>+<b><i>G</i></b><sup> (<i>n</i>) </sup>, 使用CUDA调用主机端的核函数, 在设备端进行加速矩阵乘法和矩阵加法运算操作.接下来, 更新参数张量<b><i>S</i></b>.张量<b><i>S</i></b>实际上是1组数量为<i>N</i>且大小为<i>R</i>×<i>R</i>的矩阵集, 其中单个矩阵的更新规则为<b><i>S</i></b><sup> (<i>n</i>) </sup>=<b><i>E</i>_<i>Bar</i></b><sup> (<i>n</i>) </sup>/<i>I</i><sub><i>n</i></sub>+<b><i>G</i></b><sup> (<i>n</i>) </sup>, 任何2个<b><i>S</i></b><sup> (<i>n</i>) </sup>更新过程彼此独立, 调用<i>N</i>个cudaStreams完成到设备端的并行传输并更新矩阵<b><i>S</i></b><sup> (<i>n</i>) </sup>.完成当前维度中的所有更新操作后, <i>n</i>增加1, 更新下一个维度.</p>
                </div>
                <div class="p1">
                    <p id="123">8) 判断G -BF是否达到收敛.通过<i>FIT</i>值大小来判断.<i>FIT</i>值的计算过程如下:<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>Ι</mi><mi>Τ</mi><mo>=</mo><mn>1</mn><mo>-</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Y</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>/</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>.</mo><mtext>G</mtext><mtext> </mtext><mo>-</mo><mtext>B</mtext><mtext>F</mtext></mrow></math></mathml>使用CUDA在设备端快速计算<i>FIT</i>值.G -BF的收敛条件设定为:如果<i>FIT</i>值小于超参数阈值或当迭代次数达到超参数迭代的最大次数时, 迭代更新过程结束;G -BF最终输出张量<b><i>Y</i></b>的因子矩阵集<b><i>U</i></b>, 否则“<i>item</i>+1”, 重置“<i>n</i>=1”并返回4) , 循环迭代更新.</p>
                </div>
                <div class="p1">
                    <p id="125">当G -BF中收敛到全局最小值或迭代次数达到预设最大值时, 输出的因子矩阵集为最优因子.</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">2.2.2 并行加速线程映射策略</h4>
                <div class="p1">
                    <p id="127">G -BF模型中在GPU设备端进行的密集矩阵计算主要包括矩阵的Hadmard乘积、Khatri-Rao乘积、矩阵加减、逐元除法和矩阵乘法.G -BF中的矩阵运算根据线程管理和调用策略的不同分为:</p>
                </div>
                <div class="p1">
                    <p id="128">1) 直接映射.输出矩阵的每个元素直接从输入矩阵中的单个元素确定.例如, 在矩阵Hadmard乘积运算中, 通过直接将2个输入矩阵的第<i>i</i>个元素相乘获得结果矩阵<b><i>C</i></b>的第<i>i</i>个元素, 因此设备端线程直接映射到输入元素.如果输出矩阵有<i>M</i>个元素, 并且核函数总共调用<i>T</i>个线程, 那么每个线程最多负责计算 (<i>M</i>+<i>T</i>-1) /<i>T</i>元素.在G -BF中, Khatri-Rao乘积矩阵、Hadmard乘积矩阵、矩阵加减矩阵和逐元除法都属于直接映射.算法1以Hadmard乘积为例给出直接映射的代码设计.</p>
                </div>
                <div class="p1">
                    <p id="129"><b>算法1</b>. 并行加速——矩阵Hadmard乘积.</p>
                </div>
                <div class="p1">
                    <p id="130">输入:矩阵<b><i>A</i></b>大小 (<i>AX</i>, <i>AY</i>) 、矩阵<b><i>B</i></b>大小 (<i>AX</i>, <i>AY</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="131">输出:矩阵<b><i>C</i></b>大小 (<i>AX</i>, <i>AY</i>) .</p>
                </div>
                <div class="area_img" id="282">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201907021_28200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="142">2) 共享内存.输出矩阵的每个元素由输入矩阵的1行或1列获得.矩阵的乘法就是采用这种策略.矩阵乘法的输出矩阵<b><i>C</i></b>的索引 (<i>i</i>, <i>j</i>) 是输入矩阵<b><i>A</i></b>的第<i>i</i>行和输入矩阵<b><i>B</i></b>的第<i>j</i>列的对应元素乘积的和.若通过直接映射进行矩阵乘法, 则每个线程需要将1列和1行元素从全局内存复制到局部内存, 这会带来较高的时间开销, 大大降低计算性能.</p>
                </div>
                <div class="p1">
                    <p id="143">本文在G -BF中使用共享内存策略加快矩阵乘法的执行效率.设备端Block中共享存储器是线程共享的存储单元, 与设备端的全局内存和局部内存相比, 具有更高的带宽和更低的延迟.使用共享内存能够避免线程将数据从全局内存复制到本地内存的高额时间开销.在矩阵的乘法中, 许多不同的线程使用相同的数据部分, 而共享内存可以满足同一个Block中的线程可以共享数据.因此在对较大矩阵进行乘法运算时可以使用共享内存上的“分块矩阵”, 每个块物理上对应于设备端的Block.假设2个输入矩阵<b><i>A</i></b> (<i>m</i>×<i>l</i>) 和<b><i>B</i></b> (<i>l</i>×<i>n</i>) , 将每个矩阵划分成一系列子矩阵 (例如<b><i>A</i></b><sub><i>ik</i></sub>和<b><i>B</i></b><sub><i>kj</i></sub>) , 确保对应的子矩阵块的列等于行.</p>
                </div>
                <div class="area_img" id="144">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201907021_14400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="146" name="146">2.2.3 G -BF和H-PARAFAC</h4>
                <div class="p1">
                    <p id="147">H-PARFAC框架是以大规模并行方式分解巨大稠密张量的一种解决方案, 它使用现代并行和集群计算技术, 实现了数据的近实时处理, 并且使得提取任意大小和尺寸张量的全部因子成为可能.在大张量分解过程中, H-PARAFAC的分解和融合策略是非常有优势的.</p>
                </div>
                <div class="p1">
                    <p id="148">G -BF在提取大规模高维数据低维信息时, 具有优良的迭代效率和性能.该方法能有效分解一个庞大的稠密张量, 并且不需要精确的初始化过程即可快速迭代出因子的全局最优解.将G -BF嵌入到H-PARAFAC框架中, 它可以完成原来精确初始化模块中并行直接三线性分解 (direct trilinear decomposition, DTLD<citation id="266" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>) 和子张量分解2个部分的工作, 进而加速完成超大规模高维数据的低维因子化分解.图2展示了G -BF结合H-PARAFAC框架的完整设计.利用分解策略将原始高维张量变换为多个子张量;然后使用G -BF方法, 将子张量因子化分解得到子因子矩阵;在子张量经G -BF分解后, 利用H-PARAFAC方法将各维度的子因子矩阵进行融合, 得到完整的原始张量的因子矩阵;最终完成超大规模高维时间序列张量大数据的分解.</p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 G -BF应用H-PARAFAC架构结合设计图" src="Detail/GetImg?filename=images/JFYZ201907021_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 G -BF应用H-PARAFAC架构结合设计图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The design of H-PARAFAC framework with G -BF</p>

                </div>
                <h4 class="anchor-tag" id="150" name="150">2.2.4 高维张量的数据存储映射</h4>
                <div class="p1">
                    <p id="151">由于处理大规模时间序列大数据高维度分解设计目标的限制, 张量<b><i>Y</i></b>的数据存储设计尤为重要.在分解过程中, 几乎所有的运算都是矩阵运算和向量运算.在G -BF算法中, 将张量<b><i>Y</i></b>扩展为矩阵<b><i>Y</i></b><sup> (<i>n</i>) </sup>进行矩阵运算, 其映射策略为:假设<i>N</i>维张量<b><i>Y</i></b>中任意元素的坐标为[<i>i</i><sub>1</sub>, <i>i</i><sub>2</sub>, …, <i>i</i><sub><i>N</i></sub>], 张量每个维度的大小分别对应为<i>I</i><sub>1</sub>, <i>I</i><sub>2</sub>, …, <i>I</i><sub><i>N</i></sub>, 则映射到一维向量的元素位置为</p>
                </div>
                <div class="p1">
                    <p id="152" class="code-formula">
                        <mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>s</mi><mi>i</mi><mi>t</mi><mi>e</mi><mo>=</mo><mi>i</mi><msub><mrow></mrow><mn>1</mn></msub><mo>×</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>Ν</mi></munderover><mi>Ι</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>i</mi><msub><mrow></mrow><mn>1</mn></msub><mo>×</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>3</mn></mrow><mi>Ν</mi></munderover><mi>Ι</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mo>⋯</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub><mo>×</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>Ι</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>i</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="153">由式 (8) 可知, 张量中各元素的<i>N</i>维索引与一维矢量<b><i>Y</i></b>中的坐标是一一对应的, 然后可以直接利用相应的元素进行计算.基于此映射策略来完成4) 中提到的张量维度 (<b><i>Y</i></b><sub> (1) </sub>→<b><i>Y</i></b><sub> (<i>n</i>) </sub>) 的转换.在嵌入G -BF的H-PARAFAC框架中子因子融合过程亦采用相似的维度转换映射规则.</p>
                </div>
                <h3 id="154" name="154" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="155">本次实验对本文提出方法的性能进行了评估, 并与G -HALS方法进行了对比.实验包括:1) 使用CUDA8.0重新编译HALS算法G -HALS;2) 使用CUDA8.0编译本文的G -BF算法.所有的实验都采用了相同的编译工具g++和nvcc.实验在1台装有NVIDIA GeForce TITAN X显卡 (Maxwell archite-cture) 的工作站上进行.具体的实验环境配置如表1所示:</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表1 实验环境配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Configuration of Experimental Environment</b></p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td colspan="2"><br />Host<br /> (Workstation) </td><td colspan="2">Device<br /> (GeForce GTX TITAN X) </td></tr><tr><td><br />Categories</td><td>Details</td><td>Categories</td><td>Details</td></tr><tr><td><br />Equipment<br />Number</td><td>2</td><td>Equipment <br />Number</td><td>2</td></tr><tr><td><br />Compilation</td><td>g++</td><td>Compilation</td><td>nvcc</td></tr><tr><td><br />OS</td><td>Linux/Ubuntu</td><td>CUDA Cores</td><td>3 072</td></tr><tr><td><br />CPU</td><td>Inter<sup>®</sup> Core<sup>TM</sup><br />i7-4790@<br />3.60 GHz</td><td>Memory<br />Bandwidth<br />/GBps</td><td>336.5</td></tr><tr><td><br />RAM Size/GB</td><td>24</td><td>Standard<br />Memory Size/GB</td><td>12</td></tr><tr><td><br />Networking</td><td>1 000 Mbps full<br />duplex</td><td>Base Clock<br />Frequency/MHz</td><td>1 000</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>3.1 运行效率</b></h4>
                <div class="p1">
                    <p id="158">为了保持性能评估的一般性, 本文实验数据均为由随机数组成的多通道数据集.在使用G -BF方法进行分析时, 采用不同大小的数据集来测试该方法的适用性.随机生成不同大小的4阶张量, 张量各个维度大小相同, 包括40 (40<sup>4</sup>=2560000) , 50, 60, 70, 80, 90, 100.</p>
                </div>
                <div class="p1">
                    <p id="159">在所有的实验中, G -BF模型在每个张量上执行20次, 每次实验的迭代次数设置为100次.G -BF模型单次迭代的执行时间如图3所示, G -HALS和G -BF的单次迭代执行时间随着数据量增大的变化趋势大致相同.</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 G -HALS和G -BF处理大小相同张量单次执行时间 (4-阶)" src="Detail/GetImg?filename=images/JFYZ201907021_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 G -HALS和G -BF处理大小相同张量单次执行时间 (4-阶)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 The single execution time of G -HALS and G -BF processing the same tensor (order-4) with different sizes</p>

                </div>
                <div class="p1">
                    <p id="161">G -BF的单次迭代时间范围为[43.3 ms, 645.4 ms], 分别对应于最小体积和最大体积的张量.G -HALS对应的单次迭代时间值为[46.4 ms, 752.5 ms].结果表明, G -BF和G -HALS单次迭代的性能都有相同的趋势.在处理相对小的张量时, 由于GPU的基本通信时间和计算能力没有得到充分利用, G -BF的执行时间接近G -HALS.随着数据规模的增大, G -BF展现出性能上的优势, 在数据规模非常大的情况下, G -BF在一定程度上能够减少执行时间.</p>
                </div>
                <div class="p1">
                    <p id="162">G -BF处理不同尺寸张量的执行效率为 (645.4/43.3-1) ≈14倍, 而数据尺度为38倍.在相同条件下, G -HALS的执行效率为 (752.5/46.4-1) ≈15倍.线性函数能很好地拟合了执行时间和数据大小, 表明G -BF和G -HALS均能适应数据规模的变化.</p>
                </div>
                <div class="p1">
                    <p id="163">使用相同的数据大小和截止条件来测试G -HALS和G -BF的迭代终止执行时间.图4显示了用G -HALS和G -BF过程分解张量的执行时间随数据量增长的变化趋势.在大多数情况下, G -BF的性能优于G -HALS, 与上述单次迭代时的趋势一致.同时与单次迭代时间相比, 性能改善更为明显, 主要原因是G -BF在相同的条件下迭代次数较少.图5显示了张量分解中迭代次数的变化.相同条件下, 在保证分解精度的同时G -BF算法的迭代次数更少, 具有减少迭代次数的优点.</p>
                </div>
                <div class="area_img" id="164">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 G -HALS和G -BF处理相同张量迭代终止时间" src="Detail/GetImg?filename=images/JFYZ201907021_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 G -HALS和G -BF处理相同张量迭代终止时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 The breakdown execution time of G -HALS and G -BF factoring analysis of tensor</p>
                                <p class="img_note">order-4:100×100×100×100</p>

                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 G -HALS和G -BF分解张量的迭代次数" src="Detail/GetImg?filename=images/JFYZ201907021_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 G -HALS和G -BF分解张量的迭代次数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The breakdown iterations of G -HALS and G -BF factoring analysis of tensor</p>
                                <p class="img_note">order-4:100×100×100×100</p>

                </div>
                <h4 class="anchor-tag" id="166" name="166"><b>3.2 可扩展性</b></h4>
                <div class="p1">
                    <p id="167">图6显示了相同数据规模在不同因子数量的分解下单次迭代的执行时间.G -HALS算法的执行时间范围约为[725.3 ms, 771.8 ms], 并且处理时间随着因子数量的增大而有增加的趋势.随着因子数量的增加, G -BF的执行时间[620.5 ms, 673.3 ms]具有相同的变化趋势.以上实验结果表明:G -BF和G -HALS都具有良好的可扩展性.实验的数据集大小是固定的 (100<sup>4</sup>=100 000 000) , 因子数目作为唯一变量设置为4, 8, 12, 16, 20, 24, 28, 32.</p>
                </div>
                <div class="area_img" id="168">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同秩下G -HALS和G -BF分解张量的单次执行时间" src="Detail/GetImg?filename=images/JFYZ201907021_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同秩下G -HALS和G -BF分解张量的单次执行时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The single execution time of G -HALS and G -BF processing the same tensor with different ranks</p>
                                <p class="img_note">order-4:100×100×100×100</p>

                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 G -BF处理不同维度张量 (107) 的单次执行时间" src="Detail/GetImg?filename=images/JFYZ201907021_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 G -BF处理不同维度张量 (10<sup>7</sup>) 的单次执行时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 The single execution time of G -BF processing  the same volume (10<sup>7</sup>) of different dimensionality</p>

                </div>
                <div class="p1">
                    <p id="170">G -BF模型在数据维度上也表现出了良好的可扩展性.图7显示了G -BF模型在处理数据规模相同、维度不同时的单次迭代执行时间.随着张量维数的增加, 单次迭代的执行时间也呈上升趋势.由于算法是基于维度数的迭代更新来求解最优因子, 虽然数据量是固定的, 但是维度数增长带来了更多的时间复杂度.实验表明:该方法适应于处理不同维度的数据, 使得任意维张量的分解成为可能.</p>
                </div>
                <h4 class="anchor-tag" id="171" name="171"><b>3.3 处理超大规模张量的性能探究</b></h4>
                <div class="p1">
                    <p id="172">基于已有工作提出的H-PARAFAC框架<citation id="267" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 探究G -BF可以处理超大规模的高维张量数据性能.G -BF可同时在多个节点上运行, 图8示出了基于H-PARAFAC在2个计算节点使用G -BF进行张量分解的执行时间.</p>
                </div>
                <div class="area_img" id="173">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201907021_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 当数据规模&gt;108时分解大张量总执行时间" src="Detail/GetImg?filename=images/JFYZ201907021_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 当数据规模&gt;10<sup>8</sup>时分解大张量总执行时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201907021_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Total execution time of H-PARAFAC with G -BF deriving sub-factors and full factors of simulated augmenting tensor (order-4) when data scale&gt;10<sup>8</sup></p>

                </div>
                <div class="p1">
                    <p id="174">图8上每个值表示处理张量的总时间.其中, 数据规模从100×100×100×10增加到100×100×100×100 000, 增加了10 000倍;而执行时间从10.3 s增加到16 346.2 s, 时间开销增加了1 587倍.</p>
                </div>
                <div class="p1">
                    <p id="175">GigaTensor<citation id="268" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和CDTF<citation id="269" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>是与这项研究密切相关的2个优秀的张量分解框架.GigaTensor使用100个计算节点可以分解3阶稀疏张量 (体积=10<sup>9</sup>) , 其时间开销比数据量的增加快得多.CDTF使用运行Hadoop的40个计算节点 (Xeon 2.4 GHz) , 可以分解具有10<sup>9</sup>个数据元素的5阶张量, 并且其开销随着数据量线性增加.作为对比, G -BF使用H-PARAFAC框架处理10<sup>9</sup>个数据元素大小的4阶张量具有更高的效率.当时间大小相同时, 结合H-PARAFAC的G -BF能够处理具有10<sup>11</sup>个数据元素的4阶张量.更重要的是, 在处理海量数据时, G -BF打破了数据规模的限制.</p>
                </div>
                <h3 id="176" name="176" class="anchor-tag"><b>4 总  结</b></h3>
                <div class="p1">
                    <p id="177">先验知识的缺乏已成为分析高维大规模时间序列的主要挑战, 大多数传统的因子分解方法不能适应数据的超高维和超大尺度.本文提出了一种海量并行贝叶斯分解方法 (G -BF) 来分析高维张量形式的时间序列大数据.这种方法具有3个优点:1) 在没有先验信息的情况下获得因子矩阵;2) 支持可变维度的张量分解;3) 在处理大规模数据时保持性能和可扩展性的优势.</p>
                </div>
                <div class="p1">
                    <p id="178">本研究使用GPU来加速密集矩阵的并行运算, 高效的存储器配置方案和高维数据映射策略使得程序能够满足任意维度数据的大规模并行需求.同时, 更新过程中的中间数据尽可能在设备存储器中创建和操作, 显著降低了并行编程模型中主机与设备之间的通信消耗.</p>
                </div>
                <div class="p1">
                    <p id="179">本文通过实验探究了G -BF方法在处理不同规模多维张量上的性能, 与同一实验环境下HALS的并行优化算法 (G -HALS) 实验作为对比.由实验结果可得出3个结论:</p>
                </div>
                <div class="p1">
                    <p id="180">1) 与G -HALS (常规因子分解算法) 相比, G -BF具有更好的性能, 并且随着数据量的增加, 其优越性更加明显;</p>
                </div>
                <div class="p1">
                    <p id="181">2) G -BF在数据规模、维度和因子数量等方面具有良好的可扩展性;</p>
                </div>
                <div class="p1">
                    <p id="182">3) 将G -BF方法与已有的H-PARAFAC方法相结合, 可以对超大张量 (2个节点上的体积可达10<sup>11</sup>) 进行因子分解, 其性能相比传统方法显著提高.</p>
                </div>
                <div class="p1">
                    <p id="183">总体而言, G -BF在对大规模多维张量的因子分解方面明显优于同类算法, 针对时间序列大数据的因子降维分析方面具有很大的潜力.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="288" type="formula" href="images/JFYZ201907021_28800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">高腾飞</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="289" type="formula" href="images/JFYZ201907021_28900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘勇琰</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="290" type="formula" href="images/JFYZ201907021_29000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">汤云波</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="291" type="formula" href="images/JFYZ201907021_29100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张垒</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="292" type="formula" href="images/JFYZ201907021_29200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">陈丹</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="206">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simple lessons from complexity">

                                <b>[1]</b>Goldenfeld N.Simple lessons from complexity[J].Science, 1999, 284 (5411) :87- 89
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral-spatial multi-feature-based deep learning for hyperspectral remote sensing image classification">

                                <b>[2]</b>Wang Lizhe, Zhang Jiabin, Liu Peng, et al.Spectral-spatial multi-feature-based deep learning for hyperspectral remote sensing image classification[J].Soft Computing, 2017, 21 (1) :213- 221
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dimensionality reduction for largescale neural recordings">

                                <b>[3]</b>Cunningham J P, Yu B M.Dimensionality reduction for large-scale neural recordings[J].Nature Neuroscience, 2014, 17 (11) :1500- 1509
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300070568&amp;v=MDIzMDdWYVJvPU5pZk9mYks3SHRET3JJOUZaT3dQQ1hveG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUlsdw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Hyvarinen A, Oja E.Independent component analysis:Algorithms and applications[J].Neural Networks, 2000, 13 (4) :411- 430
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD910468439&amp;v=MTY5ODVycTVIdFhLcDR0R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5cm5VcnpBTmpuQmE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Pearson K F R S.LIII.On lines and planes of closest fit to systems of points in space[J].Philosophical Magazine, 1901, 2 (11) :559- 572
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200604009&amp;v=MjI5NjllUnJGeXJuVXJ6QUx5dlNkTEc0SHRmTXE0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Gu Yu, Xu Zongben, Sun Jian, et al.An intrusion detection ensemble system based on the features extracted by PCA and ICA[J].Journal of Computer Research and Development, 2006, 43 (4) :633- 638 (in Chinese) (谷雨, 徐宗本, 孙剑, 等.基于PCA与ICA特征提取的入侵检测集成分类系统[J].计算机研究与发展, 2006, 43 (4) :633- 638) 
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and scalable multi-way analysis of massive neural data">

                                <b>[7]</b>Chen Dan, Li Xiaoli, Wang Lizhe, et al.Fast and scalable multi-way analysis of massive neural data[J].IEEE Transactions on Computers, 2015, 64 (3) :707- 719
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300186560&amp;v=MTk0OTRud1plWnVIeWptVUxySUlsd1ZhUm89TmlmT2ZiSzdIdERPckk5RlplTUpDWG81b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Bro R.PARAFAC-tutorial and applications[J].Chemometrics and Intelligent Laboratory Systems, 1997, 38 (2) :149- 171
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD120615008285&amp;v=MDA4MDZFOWZidm5LcmlmWnU5dUZDdmdVNzNLSVYwY05pZkRhcks2SHRmTnFvOUZiT2tIQ1JNOHp4VVNtRGQ5U0g3bjN4&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Wang Juan, Li Xiaoli, Lu Chengbiao, et al.Characteristics of evoked potential multiple EEG recordings in patients with chronic pain by means of parallel factor analysis[J].Computational and Mathematical Methods in Medicine, 2012, 26 (2) :27- 60
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensor decompositions, alternating least squares and other tales">

                                <b>[10]</b>Comon P, Luciani X, Almeida A L F D.Tensor decompositions, alternating least squares and other tales[J].Journal of Chemometrics, 2009, 23 (7/8) :393- 405
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast alternating LS algorithms for high order CAN-DECOMP/PARAFAC tensor factorizations">

                                <b>[11]</b>Phan .Fast alternating LS algorithms for high order CANDECOMP/PARAFAC tensor factorizations[J].IEEE Transactions on Signal Processing, 2013, 61 (19) :4834- 4846
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical ALS algorithms for nonnegative matrix and 3D tensor factorizat ion">

                                <b>[12]</b>Cichocki A, Zdunek R, Amari S I.Hierarchical ALS algorithms for nonnegative matrix and 3D tensor factorization[C] //Proc of the 7th Int Conf on Independent Component Analysis and Signal Separation.Berlin:Springer, 2007:169- 176
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302446422000&amp;v=MTkzMjYwY1hGcXpHYkM0SE5YSXFZdEhadXNQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZnVTczS0lW&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Fang Minquan, Zhang Weimin, Fang Jianbin, et al.GPU Programming and Code Optimization High Performance Computing for the Masses[M].Beijing:Tsinghua University Press, 2016:13- 141 (in Chinese) (方民权, 张卫民, 方建滨, 等.GPU编程与优化——大众高性能计算[M].北京:清华大学出版社, 2016:13- 141) 
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=H-PARAFAC Hierarchical parallel factor analysis of multidimensional big data">

                                <b>[14]</b>Chen Dan, Hu Yangyang, Wang Lizhe, et al.H-PARAFAC:Hierarchical parallel factor analysis of multidimensional big data[J].IEEE Transactions on Parallel &amp; Distributed Systems, 2017, 28 (4) :1091- 1104
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dimension reduction and Visualization of large high-dimensional data via interpolation">

                                <b>[15]</b>Bae S H, Choi J Y, Qiu J, et al.Dimension reduction and visualization of large high-dimensional data via interpolation[C] //Proc of the 19th ACM Int Symp on High Performance Distributed Computing.New York:ACM, 2010:203- 214
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=QR factorization on a multicore node enhanced with multiple GPU accelerators">

                                <b>[16]</b>Agullo E, Augonnet C, Dongarra J, et al.QR factorization on a multicore node enhanced with multiple GPU accelerators[C] //Proc of the 25th IEEE Int Parallel &amp; Distributed Processing Symp.Piscataway, NJ:IEEE, 2011:932- 943
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster and cheaper Parallelizing large-scale matrix factorization on GPUS">

                                <b>[17]</b>Tan Wei, Cao Liangliang, Fong Liana.Faster and cheaper:Parallelizing large-scale matrix factorization on GPUS[C] //Proc of the 25th ACM Int Symp on High-Performance Parallel and Distributed Computing.New York:ACM, 2016:219- 230
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200009403&amp;v=MDg2NzNSZEdlcnFRVE1ud1plWnVIeWptVUxySUlsd1ZhUm89TmlmT2ZiSzhIOVBOclk5RlpPc0dDSHc2b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>Zou Benyou, Li Cuiping, Tan Liwen, et al.GPUTENSOR:Efficient tensor factorization for context-aware recommenda-tions[J].Information Sciences, 2015, 299 (11) :159- 177
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully scalable methods for distributed tensor factorization">

                                <b>[19]</b>Shin Kijung, Lee S, Kang U.Fully scalable methods for distributed tensor factorization[J].IEEE Transactions on Knowledge and Data Engineering, 2016, 29 (1) :100- 113
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bayesian CP factorization of incomplete tensors with automatic rank determination">

                                <b>[20]</b>Zhao Qibin, Zhang Liqing, Cichocki A.Bayesian CP factorization of incomplete tensors with automatic rank determination[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1751- 1763
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GigaTensor:Scaling Tensor Analysis Up By 100 Times-Algorithms and Discoveries">

                                <b>[21]</b>Kang U, Papalexakis E, Harpale A, et al.GigaTensor:Scaling tensor analysis up by 100 times-algorithms and discoveries[C] //Proc of the 21st ACM SIGKDD Int Conf on Knowledge Discovery &amp; Data Mining.New York:ACM, 2012:316- 324
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201907021" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201907021&amp;v=MTA5NTVxcUJ0R0ZyQ1VSTE9lWmVSckZ5cm5VcnpBTHl2U2RMRzRIOWpNcUk5SFpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVBSjE4TmcxMXFpSEUvc3hFZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

