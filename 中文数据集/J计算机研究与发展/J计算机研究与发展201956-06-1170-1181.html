

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128625674337500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201906006%26RESULT%3d1%26SIGN%3ds3IT%252bLBlmrobUl1DaGBivtYMr5I%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201906006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906006&amp;v=MzExMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEbVVMck9MeXZTZExHNEg5ak1xWTlGWW9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;1 基于运算操作的神经网络特征提取方法&lt;/b&gt; "><b>1 基于运算操作的神经网络特征提取方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="&lt;b&gt;1.1 神经网络模型解析&lt;/b&gt;"><b>1.1 神经网络模型解析</b></a></li>
                                                <li><a href="#49" data-title="&lt;b&gt;1.2 基于算子的模型特征提取&lt;/b&gt;"><b>1.2 基于算子的模型特征提取</b></a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;1.3 算子的特征计算方法&lt;/b&gt;"><b>1.3 算子的特征计算方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;2 基于神经网络特征的运行优化算法&lt;/b&gt; "><b>2 基于神经网络特征的运行优化算法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="&lt;b&gt;3 实验评估&lt;/b&gt; "><b>3 实验评估</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#122" data-title="&lt;b&gt;3.1 经网络特征分析&lt;/b&gt;"><b>3.1 经网络特征分析</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;3.2 神经网络调度优化评测&lt;/b&gt;"><b>3.2 神经网络调度优化评测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#166" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="图1 网络特征提取整体设计">图1 网络特征提取整体设计</a></li>
                                                <li><a href="#54" data-title="图2 基于算子的特征提取流程">图2 基于算子的特征提取流程</a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;表1 典型算子输入参数&lt;/b&gt;"><b>表1 典型算子输入参数</b></a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;表2 典型算子加法特征分析&lt;/b&gt;"><b>表2 典型算子加法特征分析</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表3 6个经典神经网络模型&lt;/b&gt;"><b>表3 6个经典神经网络模型</b></a></li>
                                                <li><a href="#121" data-title="图3 AlexNet可视化模型">图3 AlexNet可视化模型</a></li>
                                                <li><a href="#127" data-title="图4 模型复杂度">图4 模型复杂度</a></li>
                                                <li><a href="#131" data-title="图5 卷积算子占比">图5 卷积算子占比</a></li>
                                                <li><a href="#137" data-title="图6 运算占用内存使用量">图6 运算占用内存使用量</a></li>
                                                <li><a href="#133" data-title="图7 各神经网络模型硬件资源消耗分布雷达图">图7 各神经网络模型硬件资源消耗分布雷达图</a></li>
                                                <li><a href="#140" data-title="图8 AlexNet, Inception, VGG19, VGGish网络资源占比详细分析">图8 AlexNet, Inception, VGG19, VGGish网络资源占比详细分析</a></li>
                                                <li><a href="#163" data-title="图9 硬件资源利用率">图9 硬件资源利用率</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="177">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                    Redmon J, Divvala S, Girshick R, et al.You only look once:Unified, real-time object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2016:779-788</a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_2" title="Redmonand J, Farhadi A.Yolo9000:Better, faster, stronger[C]Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2017:6517-6525" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">
                                        <b>[2]</b>
                                        Redmonand J, Farhadi A.Yolo9000:Better, faster, stronger[C]Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2017:6517-6525
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_3" title="Szegedy C, Liu Wei, Jia Yangqing, et al.Going deeper with convolutions[C]Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2015:1-9" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">
                                        <b>[3]</b>
                                        Szegedy C, Liu Wei, Jia Yangqing, et al.Going deeper with convolutions[C]Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2015:1-9
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_4" title="Liu Sicong, Lin Yingyan, Zhou Zimu, et al.On-demand deep model compression for mobile devices:A usage-driven model selection framework[C]Proc of the 16th Annual Int Conf on Mobile Systems, Applications, and Services (MobiSys) .New York:ACM, 2018:389-400" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On-demand deep model compression for mobile devices:A usage-driven model selection framework">
                                        <b>[4]</b>
                                        Liu Sicong, Lin Yingyan, Zhou Zimu, et al.On-demand deep model compression for mobile devices:A usage-driven model selection framework[C]Proc of the 16th Annual Int Conf on Mobile Systems, Applications, and Services (MobiSys) .New York:ACM, 2018:389-400
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_5" title="Lane N D, Bhattacharya S, Georgiev P, et al.Deepx:Asoftware accelerator for low-power deep learning inference on mobile devices[C]Proc of the 15th ACM/IEEE Int Conf on Information Processing in Sensor Networks (IPSN) .Piscataway, NJ:IEEE, 2016:1-12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep X:A software accelerator for low-power deep learning inference on mobile devices">
                                        <b>[5]</b>
                                        Lane N D, Bhattacharya S, Georgiev P, et al.Deepx:Asoftware accelerator for low-power deep learning inference on mobile devices[C]Proc of the 15th ACM/IEEE Int Conf on Information Processing in Sensor Networks (IPSN) .Piscataway, NJ:IEEE, 2016:1-12
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_6" title="Lane N D, Bhattacharya S.Sparsifying deep learning layers for constrained resource inference on wearables[C]Proc of the 14th ACM Conf on Embedded Network Sensor Systems (SenSys) .New York:ACM, 2016:176-189" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparsifying deep learning layers for constrained resource inference on wearables">
                                        <b>[6]</b>
                                        Lane N D, Bhattacharya S.Sparsifying deep learning layers for constrained resource inference on wearables[C]Proc of the 14th ACM Conf on Embedded Network Sensor Systems (SenSys) .New York:ACM, 2016:176-189
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_7" title="Li Jiajun, Yan Guihai, Lu Wenyan.CCR:A concise convolution rule for sparse neural network accelerators[C]Proc of the 2018Design, Automation&amp;amp;Test in Europe Conf (DATE) .Piscataway, NJ:IEEE, 2018:189-194" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CCR:A concise convolution rule for sparse neural network accelerators">
                                        <b>[7]</b>
                                        Li Jiajun, Yan Guihai, Lu Wenyan.CCR:A concise convolution rule for sparse neural network accelerators[C]Proc of the 2018Design, Automation&amp;amp;Test in Europe Conf (DATE) .Piscataway, NJ:IEEE, 2018:189-194
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_8" title="Liang Yun, Lu Liqiang, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[C]Proc of the 25th IEEE Symp on Field Programmable Custom Computing Machines (FCCM) .Piscataway, NJ:IEEE, 2017:101-108" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating fast algorithms for convolutional neural networks on FPGAs">
                                        <b>[8]</b>
                                        Liang Yun, Lu Liqiang, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[C]Proc of the 25th IEEE Symp on Field Programmable Custom Computing Machines (FCCM) .Piscataway, NJ:IEEE, 2017:101-108
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_9" title="Liu Baoyuan, Wang Min, Foroosh H, et al.Sparse convolutional neural networks[C]Proc of the 28th IEEEConf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2015:806-814" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse Convolutional Neural Networks">
                                        <b>[9]</b>
                                        Liu Baoyuan, Wang Min, Foroosh H, et al.Sparse convolutional neural networks[C]Proc of the 28th IEEEConf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2015:806-814
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_10" title="Howard A G, Zhu Menglong, Chen Bo, et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[J].arXiv preprint arXiv:1704.04861, 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenets:Efficient convolutional neural networks for mobile vision applications">
                                        <b>[10]</b>
                                        Howard A G, Zhu Menglong, Chen Bo, et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[J].arXiv preprint arXiv:1704.04861, 2017
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_11" title="Han Song, Mao Huizi, Dally W J.Deep compression:Compressing deep neural network with pruning, trained quantization and Huffman coding[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:ACM, 2016:Article ID:11" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Compression:Compressing Deep Neural Network with Pruning,Trained Quantization and Coding">
                                        <b>[11]</b>
                                        Han Song, Mao Huizi, Dally W J.Deep compression:Compressing deep neural network with pruning, trained quantization and Huffman coding[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:ACM, 2016:Article ID:11
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_12" title="Changpinyo S, Sandler M, Zhmoginov A.The power of sparsity in convolutional neural networks[C]Proc of the5th Int Conf on Learning Representations (ICLR) .New York:ACM, 2017:Article ID:336" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The power of sparsity in convolutional neural networks">
                                        <b>[12]</b>
                                        Changpinyo S, Sandler M, Zhmoginov A.The power of sparsity in convolutional neural networks[C]Proc of the5th Int Conf on Learning Representations (ICLR) .New York:ACM, 2017:Article ID:336
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_13" title="Venieris S I, Bouganis C.Latency-driven design for FPGA-based convolutional neural networks[C]Proc of the 27th Int Conf on Field Programmable Logic and Applications (FPL) .Piscataway, NJ:IEEE, 2017:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latency-driven design for FPGA-based convolutional neural networks">
                                        <b>[13]</b>
                                        Venieris S I, Bouganis C.Latency-driven design for FPGA-based convolutional neural networks[C]Proc of the 27th Int Conf on Field Programmable Logic and Applications (FPL) .Piscataway, NJ:IEEE, 2017:1-8
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_14" title="Chen Y H, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C]Proc of the ACM/IEEE 43rd Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2016:367-379" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eyeriss:a spatial architecture for energy-efficient dataflow for convolutional neural networks">
                                        <b>[14]</b>
                                        Chen Y H, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C]Proc of the ACM/IEEE 43rd Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2016:367-379
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_15" title="Alwani M, Chen Han, Ferdman M, et al.Fused-layer CNNaccelerators[C]Proc of the 49th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Piscataway, NJ:IEEE, 2016:1-12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fused-layer CNN accelerators">
                                        <b>[15]</b>
                                        Alwani M, Chen Han, Ferdman M, et al.Fused-layer CNNaccelerators[C]Proc of the 49th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Piscataway, NJ:IEEE, 2016:1-12
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_16" title="Iandola F N, Han Song, Moskewicz M W, et al.SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and&amp;lt;1MB model size[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:ACM, 2016:Article ID:465" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SqueezeN et:AlexN et-level accuracy with 50x fewer parameters and&amp;lt;0.5MB model size">
                                        <b>[16]</b>
                                        Iandola F N, Han Song, Moskewicz M W, et al.SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and&amp;lt;1MB model size[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:ACM, 2016:Article ID:465
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_17" title="Altaf M S B, Wood D A.LogCA:A high-level performance model for hardware accelerators[C]Proc of the ACM/IEEE 44th Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2017:375-388" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LogCA:A high-level performance model for hardware accelerators">
                                        <b>[17]</b>
                                        Altaf M S B, Wood D A.LogCA:A high-level performance model for hardware accelerators[C]Proc of the ACM/IEEE 44th Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2017:375-388
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_18" title="Zhang Chen, Li Peng, Sun Guangyu, et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays (FPGA) .New York:ACM, 2015:161-170" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks">
                                        <b>[18]</b>
                                        Zhang Chen, Li Peng, Sun Guangyu, et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays (FPGA) .New York:ACM, 2015:161-170
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_19" title="Motamedi M, Gysel P, Akella V, et al.Design space exploration of FPGA-based deep convolutional neural networks[C]Proc of the 21st Asia and South Pacific Design Automation Conf (ASP-DAC) .Piscataway, NJ:IEEE, 2016:575-580" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design space exploration of FPGA-based deep convolutional neural networks">
                                        <b>[19]</b>
                                        Motamedi M, Gysel P, Akella V, et al.Design space exploration of FPGA-based deep convolutional neural networks[C]Proc of the 21st Asia and South Pacific Design Automation Conf (ASP-DAC) .Piscataway, NJ:IEEE, 2016:575-580
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_20" title="Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2012:1097-1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classifcation with deep convolutional neural networks">
                                        <b>[20]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2012:1097-1105
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_21" title="Shen Yongming, Ferdman M, Milder P.Maximizing CNNaccelerator efficiency through resource partitioning[C]Proc of the ACM/IEEE 44th Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2017:535-547" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximizing CNNaccelerator efficiency through resource partitioning">
                                        <b>[21]</b>
                                        Shen Yongming, Ferdman M, Milder P.Maximizing CNNaccelerator efficiency through resource partitioning[C]Proc of the ACM/IEEE 44th Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2017:535-547
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_22" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[C]Proc of the2nd Int Conf on Learning Representations (ICLR) .New York:ACM, 2014:Article ID:4" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[22]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[C]Proc of the2nd Int Conf on Learning Representations (ICLR) .New York:ACM, 2014:Article ID:4
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_23" title="Liu Wei, Anguelov D, Erhan D, et al.SSD:Single shot multibox detector[C]Proc of the European Conf on Computer Vision (ECCV) .Cham:Springer, 2015:21-37" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[23]</b>
                                        Liu Wei, Anguelov D, Erhan D, et al.SSD:Single shot multibox detector[C]Proc of the European Conf on Computer Vision (ECCV) .Cham:Springer, 2015:21-37
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                    Xu K, Ba J L, Kiros R, et al.Show, attend and tell:Neural image caption generation with visual attention[C]Proc of the 32nd Int Conf on Machine Learning (ICML) .New York:ACM, 2015:37:2048-37:2057</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(06),1170-1181 DOI:10.7544/issn1000-1239.2019.20190111            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>多层神经网络算法的计算特征建模方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%96%B9%E8%8D%A3%E5%BC%BA&amp;code=42096404&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">方荣强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%B6&amp;code=08707351&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晶</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E6%B2%BB%E6%88%90&amp;code=34222009&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚治成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%95%85&amp;code=11107241&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘畅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BC%9F%E5%8A%9F&amp;code=21627418&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张伟功</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%A6%96%E9%83%BD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0044636&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">首都师范大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E6%88%90%E5%83%8F%E7%90%86%E8%AE%BA%E4%B8%8E%E6%8A%80%E6%9C%AF%E9%AB%98%E7%B2%BE%E5%B0%96%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83(%E9%A6%96%E9%83%BD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6)&amp;code=0044636&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京成像理论与技术高精尖创新中心(首都师范大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E6%89%80)&amp;code=0044636&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">体系结构国家重点实验室(中国科学院计算技术研究所)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%AB%98%E5%8F%AF%E9%9D%A0%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E5%8C%97%E4%BA%AC%E5%B8%82%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83(%E9%A6%96%E9%83%BD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6)&amp;code=0142480&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高可靠嵌入式系统技术北京市工程研究中心(首都师范大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着深度学习算法在语音和图像等领域中的成功运用, 能够有效提取目标特征并做出最优决策的神经网络再次得到了广泛的关注.然而随着数据量的增加和识别精度需求的提升, 神经网络模型的复杂度不断提高, 因此采用面向特定领域的专用硬件加速器是高效运行神经网络的有效途径.然而如何根据网络规模设计高能效的加速器, 以及基于有限硬件资源如何提高网络性能并最大化资源利用率是当今体系结构领域研究的重要问题.为此, 提出基于计算特征的神经网络分析和优化方法, 基于“层”的粒度解析典型神经网络模型并提取模型通用表达, 根据通用表达式和基本操作属性提取模型运算量和存储空间需求等特征.提出了基于最大值更替的运行调度算法, 利用所提取的特征分析结果对神经网络在特定硬件资源下的运行调度方案进行优化.实验结果显示:所提方法能够有效分析对比网络特征, 并指导所设计调度算法实现性能和系统资源利用率的提升.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">硬件加速器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机体系结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">资源调度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张伟功, zwg771@cnu.edu.cn, born in 1967.PhD, professor.Member of CCF.His main research interests include computer architecture, high reliable embedded system design.;
                                </span>
                                <span>
                                    *王晶, jwang@cnu.edu.cn, born in 1982.PhD, associate professor.Her main research interests include computer architecture, energy efficient computing, high performance computing, hardware reliability and variability.;
                                </span>
                                <span>
                                    Fang Rongqiang, born in 1996.Master candidate. His main research interests include computer architecture, high performance computing.;
                                </span>
                                <span>
                                    Yao Zhicheng, born in 1989. Master, engineer.His main research interests include computer architecture, high performance computing.;
                                </span>
                                <span>
                                    Liu Chang, born in 1996.Bachelor, engineer.Her main research interests include computer architecture, high performance computing.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61772350);</span>
                                <span>共有信息系统装备预先研究项目 (公开) (JZX2017-0988/Y300);</span>
                                <span>北京市科技新星计划项目 (Z181100006218093);</span>
                                <span>体系结构国家重点实验室开放课题 (CARCH201607);</span>
                                <span>北京未来芯片技术高精尖创新中心科研基金资助项目 (KYJJ2018008);</span>
                                <span>北京市高水平教师队伍建设计划 (CIT&amp;TCD201704082);</span>
                                <span>科技创新服务能力建设-基本科研业务费 (科研类) (19530050173, 02518530500);</span>
                    </p>
            </div>
                    <h1><b>Modeling Computational Feature of Multi-Layer Neural Network</b></h1>
                    <h2>
                    <span>Fang Rongqiang</span>
                    <span>Wang Jing</span>
                    <span>Yao Zhicheng</span>
                    <span>Liu Chang</span>
                    <span>Zhang Weigong</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering, Capital Normal University</span>
                    <span>Beijing Advanced Innovation Center for Imaging Theory and Technology (Capital Normal University)</span>
                    <span>State Key Laboratory of Computer Architecture (Institute of Computing Technology, Chinese Academy of Sciences)</span>
                    <span>Beijing Engineering Research Center of High Reliable Embedded System (Capital Normal University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep neural networks (DNNs) have become increasingly popular as machine learning technique in applications, due to their ability to achieve high accuracy for tasks such as speech/image recognition. However, with the rapid growth on the scale of data and precision of recognition, the topology of neural network is becoming more and more complicated. Thus, how to design the energy-efficiency and programmability, neural or deep learning accelerator plays an essential role in next generation computer. In this paper, we propose a layer granularity analysis method, which could extract computation operations and memory requirement features through general expression and basic operation attributions. We also propose a max value replacement schedule strategy, which schedules the computation hardware resource based on the network feature we extract. Evaluation results show our method can increase computational efficiency and lead to a higher resource utilization.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=features%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">features extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hardware%20accelerator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hardware accelerator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=computer%20architecture&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">computer architecture;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=resource%20scheduling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">resource scheduling;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61772350);</span>
                                <span>the Common Information System Equipment Pre-research Funds (Open Project) (JZX2017-0988/Y300);</span>
                                <span>Beijing Nova Program (Z181100006218093);</span>
                                <span>the Open Project of State Key Laboratory of Computer Architecture (CARCH201607);</span>
                                <span>the Research Fund from Beijing Innovation Center for Future Chips (KYJJ2018008);</span>
                                <span>the Construction Plan of Beijing High-level Teacher Team (CIT&amp;TCD201704082);</span>
                                <span>the Capacity Building for Sci-Tech Innovation Fundamental Scientific Research Funds (19530050173, 025185305000);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="34">随着神经网络越来越广泛地应用于语音识别、计算机视觉、智能机器人、故障检测、市场分析、决策优化等领域<citation id="226" type="reference"><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 人们对网络精度的要求不断提高, 网络层数越来越多, 计算复杂度越来越高, 如2014年GoogLeNet<citation id="225" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>网络已经达到22层.日益增加的计算复杂度使得训练和推理的开销问题逐步凸显出来, 当前GPU和FPGA等专用硬件加速芯片已经成为神经网络运行的重要平台.而随着人工智能技术的进步, 移动设备、嵌入式设备等在计算、体积、功耗等方面受限的设备也需要应用深度学习技术.由于设备资源的约束, 导致现有复杂的深度神经网络无法进行高效的计算.上述场景为计算机体系结构提出了新的挑战:如何在保持现有神经网络精度不变的情况下, 使得网络模型能在资源受限的设备上高效运行, 并最大化系统资源利用率.</p>
                </div>
                <div class="p1">
                    <p id="35">针对上述问题可以从算法角度减小网络计算量, 也可以从硬件角度优化资源利用率.现有研究从算法角度提出了大量优化方案<citation id="227" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>:针对神经网络运算矩阵和矩阵相乘的运算方式, 可以利用奇异值分解来压缩神经网络计算量<citation id="228" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>;针对权重矩阵往往比较稀疏的特性, 可以利用矩阵稀疏编码的方式压缩神经网络<citation id="229" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>;通过改变卷积运算算法可以加速网络执行<citation id="231" type="reference"><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>;权值重载方式也能够有效减少片上存储开销<citation id="230" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.算法的优化可以实现对深度神经网络的压缩, 从而削减网络的计算量和存储需求, 提高网络执行的效率, 但这些方法同时也都是预先静态地对算法进行优化, 无论裁剪到什么程度的网络, 最终都需要在实际的硬件上运行, 因此从体系结构角度, 在有限硬件资源上优化神经网络的运行效率是当前研究的重要问题.</p>
                </div>
                <div class="p1">
                    <p id="36">研究人员从体系结构角度入手, 提出多种提升神经网络执行速度的方案:利用神经网络中数据重用的特点可以优化卷积神经网络<citation id="232" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>;文献<citation id="233" type="reference">[<a class="sup">15</a>]</citation>将网络权值矩阵和输入矩阵进行矢量化操作, 能够大大减小运算量, 提升计算效率.将神经网络从通用处理器CPU的执行平台, 迁移到具有高并行度的GPU和FPGA平台也是有效提高运行效率的手段.利用OpenCL将卷积计算转化为并行度更高的矩阵乘法运算能够进一步优化GPU平台上卷积运算效率.FPGA平台上利用可重构和流水化并行执行的卷积运算过程能够实现对执行速度和资源利用率的同步提升, 基于FPGA的可重构特性能够化简乘法运算, 提高系统能效性<citation id="234" type="reference"><link href="207" rel="bibliography" /><link href="209" rel="bibliography" /><link href="211" rel="bibliography" /><link href="213" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="37">然而, 无论哪一种神经网络加速器的设计, 都需要了解算法的特征, 根据不同算法的计算规模有针对性地分配计算和存储资源, 才能在提高程序运行效率的同时最大化系统资源利用率.此外, 对于硬件加速器设计的验证, 如果运行真实的大规模神经网络将导致验证的周期和成本都大幅增加.而抽取典型计算片段不但能够保证功能验证覆盖率, 还能有效降低验证成本.要达到这2方面目的, 都需要对算法的特征进行分析, 找出模型中频繁出现的层, 本文称为算子 (operator) , 了解算子的计算和访存特点, 从而找到加速优化的切入方向.</p>
                </div>
                <div class="p1">
                    <p id="38">因此本文提出基于基本运算的神经网络特征提取优化方法, 主要贡献包括3个方面:</p>
                </div>
                <div class="p1">
                    <p id="39">1) 针对典型神经网络进行分析, 找出其中核心算子, 对每个算子分析内部包含的基本运算、运算的数量和内存占用量随输入变化的关系.</p>
                </div>
                <div class="p1">
                    <p id="40">2) 在算子的粒度, 根据网络的描述解析网络基本结构, 获得包含算子和算子顺序的模型通用表达式, 并给出图形化描述.根据分析获得的算子内部特征和模型通用表达式, 计算网络模型的乘加运算量、存储占用量等典型特征值.</p>
                </div>
                <div class="p1">
                    <p id="41">3) 基于所获得的运算量等网络特征, 结合硬件资源数量, 提出基于最大值的网络运行调度优化方案, 提高了神经网络的执行效率, 同时最大化硬件资源的利用率.</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>1 基于运算操作的神经网络特征提取方法</b></h3>
                <div class="p1">
                    <p id="43">神经网络通常由不同的层, 如卷积层、池化层、全连层等以特定顺序组合而成, 其中乘法和加法操作 (operation) 是基本的运算.为了分析神经网络运行对硬件资源的需求, 首先从“层”的粒度分析网络结构, 然后基于每层的运算特点分析该层所需要的乘法和加法等基本操作的次数以及所需占用的存储空间.</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>1.1 神经网络模型解析</b></h4>
                <div class="p1">
                    <p id="45">深度神经网络虽然可以通过改变层级结构、神经元个数以及神经元之间的连接衍生出不同的网络模型, 但网络中所涉及的核心算子种类并不多, 通常包括卷积层、池化层、激活函数和全连接层等.尽管输入和参数不同, 但每个算子的运算方式是确定的, 因此可以通过分析得到计算每个算子的基本操作和基本资源需求公式.</p>
                </div>
                <div class="p1">
                    <p id="46">模型解析器可以分析神经网络模型包含的算子以及算子的执行顺序.按照算子的种类和执行顺序, 建立一个仅包含算子种类和其执行顺序但不包含操作次数的图形化通用表达式, 把神经网络中各种算子看作节点, 把其所需要的输入和产生的输出作为连接节点的有向边, 这样便产生了一个有向无环图 (DAG) , 这个图结构作为后续特征提取模块的输入.</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_047.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络特征提取整体设计" src="Detail/GetImg?filename=images/JFYZ201906006_047.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络特征提取整体设计  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_047.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Design of network feature extraction</p>

                </div>
                <div class="p1">
                    <p id="48">模型解析器是本文所提出的网络特征提取方法的第1步, 特征提取方法结构如图1所示.模型解析器基于输入的神经网络模型和其对应的参数, 分析模型所包含的层数和每层的功能, 将网络转化成对应的通用表达式, 然后通过TensorFlow内嵌的TensorBoard可视化组件获得其图形化的显示, 从而获得模型结构和算子种类、算子数量和算子顺序.在模型解析之后, 模型特征提取模块加载所获得的通用表达式以及包含基础算子的特征描述文件, 获得算子内包含的乘加等基本运算操作的类型、次数、存储占用情况等特征.所获得的计算和访存特征作为运行调度模块的输入, 结合系统硬件资源信息, 例如FPGA芯片能够支持的加法器和乘法器个数, 运行调度模块采用最大值更替算法计算出在给定硬件资源下最大化资源利用率的网络运行调度方案.</p>
                </div>
                <h4 class="anchor-tag" id="49" name="49"><b>1.2 基于算子的模型特征提取</b></h4>
                <div class="p1">
                    <p id="50">算子的计算量等特征是可以通过加载提前设定的配置文件 (profile) 来解析统计出加法和乘法等基本操作计算公式.模型特征提取模块负责根据计算公式对输入模型进行整体的特征提取与计算.模块的输入包括2部分:配置文件和模型通用表达式, 其中模型通用表达式是模型解析器传递的包含算子种类及执行顺序的分析结果, 也就是没有神经元具体执行运算和内存占用信息的图结构.算子特征描述文件定义了算法模型中具体算子的特征.</p>
                </div>
                <div class="p1">
                    <p id="51">当模块得到通用模型描述文件和配置信息的输入后, 遍历代表网络模型的图结构, 进行模型特征信息的构建.对每个算子进行单独的特征提取, 通过计算参数配置文件的算子特征公式, 可以获得加法和乘法基本操作次数等神经网络的特征, 并对所有特征进行统计与存储.</p>
                </div>
                <div class="p1">
                    <p id="52">算子特征提取流程如图2所示:首先判断算子对应的类型在配置文件中是否有定义, 如果有, 则从模型算子节点中加载, 按照配置文件中该算子的公式定义来计算出对应的乘法和加法等基本操作的次数, 而后输出该算子的特征;如果配置文件中没有定义, 则进行第2步判断, 检测系统内部是否有内置特征解析方法, 若有, 则调用提取特征的函数对该算子进行解析, 若无, 则输出空特征.</p>
                </div>
                <div class="p1">
                    <p id="53">特征提取模块通过配置文件管理器来管理所有的配置文件, 管理器包含2方面的管理功能:算法模型初始化参数的管理和算法基础算子的特征管理.算法模型初始化参数的管理将每个算子对应的信息做初始化赋值, 算法基础算子的特征管理记录是对每个算子预置基本操作的特定计算公式.在以Conv2D算子为例的算子配置文件内容中, 分为Var和Feature两大部分.当解析该算子的特征时, 首先利用字段Var中指定的方法进行变量初始化, 即按顺序从Var中读取指定的变量提取方法, 并将执行方法得到的结果存放在指定的变量中.对于算法模型, 算子的计算量往往和输入数据大小、维度等有关, 所以在计算“算子的计算量”之前, 需要把算子所涉及的输入输出数据样式确定下来, 例如模型输入图片的大小、批大小 (<i>batch</i>_<i>size</i>) 等参数.神经网络算法模型由不同功能的算子通过不同的连接方式组成, 算子不同则计算类型、计算量、数据处理大小等都不一样, 所以对于“特征提取模块”而言, 它需要知道用于构建模型的每个算子参数、计算特征等, 例如二维卷积算法需要知道卷积核的大小、移动步长、数据维度大小等特征后才能根据具体的算法得出该算子在模型中的特征.为了方便特征提取模块获取每个算子的属性, 参数配置文件管理器基于配置文件的算子管理方式, 当用户需要修改或者添加算子特征时只需修改对应的配置文件.</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于算子的特征提取流程" src="Detail/GetImg?filename=images/JFYZ201906006_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于算子的特征提取流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Operator-based feature extraction process</p>

                </div>
                <h4 class="anchor-tag" id="55" name="55"><b>1.3 算子的特征计算方法</b></h4>
                <div class="p1">
                    <p id="56">本文选取了10种典型神经网络算子, 根据算子的输入参数列表, 如表1所示, 以加法为例给出了每个算子计算资源需求的公式, 如表2所示.</p>
                </div>
                <div class="p1">
                    <p id="57">1) Conv2D算子.它是2维卷积运算, 该算子实际上是用滤波器矩阵在输入图像矩阵上沿水平或竖直方向上按步长数值滑动的同时做加权叠加的操作.卷积核单次计算加法次数为 ( (<i>f</i>_<i>height</i>×<i>f</i>_<i>width</i>-1) ×<i>in</i>_<i>channels</i>) ×<i>out</i>_<i>channels</i>, 所需加法次数为 (<i>f</i>_<i>height</i>×<i>f</i>_<i>width</i>×<i>in</i>_<i>channel</i>-1) ×[ (<i>in</i>_<i>height</i>-<i>f</i>_<i>height</i>) /<i>strides</i>+1]×[ (<i>in</i>_<i>width</i>-<i>f</i>_<i>width</i>) /<i>strides</i>+1], 最后乘以图片的张数和输出结果的通道数就可以得到卷积算子包含的加法次数.而二维卷积所包含的乘法次数, 首先计算单个卷积核单次计算的乘法次数 (<i>f</i>_<i>height</i>×<i>f</i>_<i>width</i>×<i>in</i>_<i>channels</i>) , 再乘以图片张数和移动步数即可获得其包含的乘法次数.</p>
                </div>
                <div class="p1">
                    <p id="58">2) Avg_Pool.它为平均池化算子, 该算子是通过定义一个方形窗口滤波器矩阵, 让该窗口以不重叠的步长<i>strides</i>在输入图像上沿水平或竖直方向上运算的同时记录下该窗口矩阵范围内所有输入图像矩阵元素的平均数作为输出图像矩阵的像素值, 而输出结果的图像矩阵大小取决于定义窗口矩阵的大小.该算子可以将输入的图像进行有效压缩处理以节省资源消耗.关于平均池化的加法计算, 步骤1是计算出它的移动步数, 步骤2则是计算出它的每步所包含的数值的个数.首先, 计算窗口大小<b><i>ksize</i></b>平均值需要Π (<b><i>ksize</i></b>) -1次加法;接着对输出矩阵<b><i>output</i></b>进行求维度运算, 可以获得一个关于输出结果的维度矩阵, 即<b><i>shape</i> (<i>output</i></b>) ;而后对该维度矩阵求连乘积, 即可获得输出矩阵中所包含的元素个数;最后将它们乘在一起就可以得到平均池化算子所包含的加法次数.而该算子所包含的乘法次数与输出矩阵中的元素个数相等, 因为对于每步池化都需要做一次乘法, 又因为该池化核所走的步数等于输出矩阵元素个数, 因此其包含的乘法计算的次数为Π (<b><i>shape</i> (<i>output</i></b>) ) .</p>
                </div>
                <div class="p1">
                    <p id="59">3) Max_Pool.它为最大池化算子, 与平均池化算子相同, 都包含<b><i>ksize</i></b>参数和同样的生成规则, 与之不同的仅在于它不取窗口内的平均值而取其最大值作为输出结果.其加法计算即为在最大池化核的范围内进行数据比较次数, 实为减法操作次数, 故需要比较Π (<b><i>shape</i> (<i>output</i></b>) × (Π (<b><i>ksize</i></b>) -1) ) 次.由于该算子是将池化核内所有数据进行比较, 因此不需要计算乘法.</p>
                </div>
                <div class="area_img" id="60">
                    <p class="img_tit"><b>表1 典型算子输入参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Typical Operation Input Parameters</b></p>
                    <p class="img_note"></p>
                    <table id="60" border="1"><tr><td><br />Operator</td><td>Involved Partial Parameters and Method</td><td>Corresponding Explanation</td></tr><tr><td rowspan="4"><br />Conv2D</td><td><br /><b><i>Input</i></b>[<i>batch</i>_<i>size</i>, <i>in</i>_<i>height</i>, <i>in</i>_<i>width</i>, <i>in</i>_<i>channels</i>]</td><td>The Number/Height/Width and Channels of the Input</td></tr><tr><td><br /><b><i>Filter</i></b>[<i>f</i>_<i>height</i>, <i>f</i>_<i>width</i>, <i>in</i>_<i>channels</i>, <i>out</i>_<i>channels</i>]</td><td>The Height/Width of the Filter, Input/Output Channels</td></tr><tr><td><br /><i>strides</i></td><td>The Stride of the Filter</td></tr><tr><td><br /><i>padding</i></td><td>Padding Pixels</td></tr><tr><td><br />Avg_Pool</td><td><b><i>ksize</i></b></td><td>Pooling Window Size</td></tr><tr><td><br />Max_Pool</td><td><b><i>output</i></b></td><td>Output Matrix</td></tr><tr><td><br />Bias_Add</td><td><b><i>bias</i></b></td><td>Offset Value Vector</td></tr><tr><td rowspan="3"><br />MatMul</td><td><br /><i>x</i></td><td>The Number of Rows in Multiplicand Matrix</td></tr><tr><td><br /><i>y</i></td><td>The Number of Columns in Multiplier Matrix</td></tr><tr><td><br /><i>m</i></td><td>The Number of Rows in Multiplier and the Columns in <br />Multiplicand Matrix</td></tr><tr><td rowspan="7"><br />LRN</td><td><br /><i>n</i>/2</td><td>Normalization Radius</td></tr><tr><td><br /><i>k</i></td><td>Offset Value</td></tr><tr><td><br /><i>a</i></td><td>Input Featuremap</td></tr><tr><td><br /><i>b</i></td><td>Output Featuremap</td></tr><tr><td><br /><i>α</i></td><td>Constant Hyper-parameters</td></tr><tr><td><br /><i>β</i></td><td>Constant Hyper-parameters</td></tr><tr><td><br /><i>dim</i><sub>1</sub>, <i>dim</i><sub>2</sub>, <i>dim</i><sub>3</sub>, <i>dim</i><sub>4</sub></td><td>The Values of Each Dimension in a Four-dimensional Tensor</td></tr><tr><td><br />Softmax</td><td><b><i>logits</i></b></td><td>Non-empty Two-dim-Tensor</td></tr><tr><td><br />Sigmoid</td><td rowspan="3"><i>Output</i>_<i>index</i></td><td rowspan="3">The Index of Hidden Layer Output Vector</td></tr><tr><td><br />tanh</td></tr><tr><td><br />ReLU</td></tr><tr><td rowspan="4"><br />Global Variable<br />Built-in Method</td><td><br /><i>batch</i>_<i>size</i></td><td>The Number of Samples Propagated Through the Network</td></tr><tr><td><br /><i>num</i>_<i>classes</i></td><td>The Total Classes Number</td></tr><tr><td><br /><b><i>shape</i></b> () </td><td>The Dimension of a Matrix</td></tr><tr><td><br />Π () </td><td>Matrix Multiplication</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="61">
                    <p class="img_tit"><b>表2 典型算子加法特征分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Analysis of Add Operation Feature of Typical Operators</b></p>
                    <p class="img_note"></p>
                    <table id="61" border="1"><tr><td><br />Operator</td><td>Formula of Calculating Number of Addition Operation</td></tr><tr><td><br />Conv2D</td><td> ( (<i>f</i>_<i>height</i>×<i>f</i>_<i>width</i>-1) ×<i>in</i>_<i>channels</i>) ×<br /><i>batch</i>_<i>size</i>× ( (<i>in</i>_<i>height</i>-<i>f</i>_<i>height</i>) /<i>strides</i>+1) ×<br /> ( (<i>in</i>_<i>width</i>-<i>f</i>_<i>width</i>) /<i>strides</i>+1) ×<i>out</i>_<i>channels</i></td></tr><tr><td><br />Avg_Pool</td><td>Π (<b><i>shape</i> (<i>output</i></b>) × (Π (<b><i>ksize</i></b>) -1) ) </td></tr><tr><td><br />Max_Pool</td><td>Π (<b><i>shape</i> (<i>output</i></b>) × (Π (<b><i>ksize</i></b>) -1) ) </td></tr><tr><td><br />Softmax</td><td><i>batch</i>_<i>size</i>×<i>num</i>_<i>classes</i>× (<i>num</i>_<i>classes</i>-1) </td></tr><tr><td><br />Bias_Add</td><td><b><i>shape</i> (<i>bias</i></b>) </td></tr><tr><td><br />MatMul</td><td><i>x</i>×<i>y</i>× (<i>m</i>-1) </td></tr><tr><td><br />LRN</td><td> (<i>n</i>+3) ×<i>dim</i><sub>1</sub>×<i>dim</i><sub>2</sub>×<i>dim</i><sub>3</sub>×<i>dim</i><sub>4</sub></td></tr><tr><td><br />Sigmoid</td><td>1</td></tr><tr><td><br />tanh</td><td>2</td></tr><tr><td><br />ReLU</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="63">4) Bias_Add.它是对2维卷积层和全连接层添加偏置值的标准化函数, 该算子所包含的加法次数即为矩阵的维度, 由于是直接相加, 因此乘法计算次数为零.</p>
                </div>
                <div class="p1">
                    <p id="64">5) MatMul.它为矩阵乘法算子, 是全连层主要的运算.其输入为<b><i>A</i></b><sub><i>x</i>×<i>m</i></sub>和<b><i>B</i></b><sub><i>m</i>×<i>y</i></sub>的矩阵, 这2个矩阵经MatMul算子运算后得到<b><i>C</i></b><sub><i>x</i>×<i>y</i></sub>的矩阵, 因此产生的结果中有<i>x</i>×<i>y</i>个新数值, 结果元素可计算为</p>
                </div>
                <div class="p1">
                    <p id="65"><i>c</i><sub><i>ij</i></sub>=<i>a</i><sub><i>i</i>1</sub><i>b</i><sub>1<i>j</i></sub>+<i>a</i><sub><i>i</i>2</sub><i>b</i><sub>2<i>j</i></sub>+…+<i>a</i><sub><i>im</i></sub><i>b</i><sub><i>mj</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="66">每个元素的计算需要<i>m</i>-1个已有数值相加, 因此其加法次数为<i>x</i>×<i>y</i>× (<i>m</i>-1) 次, 而乘法次数也相应为<i>x</i>×<i>y</i>×<i>m</i>次.</p>
                </div>
                <div class="p1">
                    <p id="67">6) LRN.它为局部相应归一化算子, 用来抑制过拟合现象, 并加快收敛速度的算子<citation id="235" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.可计算为</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>b</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mi>i</mi></msubsup><mo>=</mo><mi>a</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mi>i</mi></msubsup><mo>/</mo><mrow><mo> (</mo><mrow><mi>k</mi><mo>+</mo><mi>α</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>i</mi><mo>-</mo><mi>n</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo></mrow><mrow><mi>min</mi><mo stretchy="false"> (</mo><mi>Ν</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>a</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mi>j</mi></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow><msup><mrow></mrow><mi>β</mi></msup><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中, 加法共有<i>n</i>+3次.<i>b</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mi>i</mi></msubsup></mrow></math></mathml>再乘以矩阵的4个维度参数<i>dim</i><sub>1</sub>, <i>dim</i><sub>2</sub>, <i>dim</i><sub>3</sub>和<i>dim</i><sub>4</sub>可以计算出整个算子包含的所有加法次数.在分母处涉及<i>n</i>+<i>β</i>次乘法运算, 因此在同一维内乘法共运算<i>n</i>+<i>β</i>+1次, 同样最后乘以矩阵的4个维度参数便得出LRN算子所包含的乘法次数.</p>
                </div>
                <div class="p1">
                    <p id="71">7) Softmax.它为神经元激活函数算子, 该算子将多分类的输出数值转化为相对概率, 经常被用于分类器后的输出单元做处理, 依据Softmax函数的计算公式:<i>Softmax</i>[<i>i</i>, <i>j</i>]=exp (<i>logits</i>[<i>i</i>, <i>j</i>]) /<i>sum</i>_<i>j</i> (exp (<i>logits</i>[<i>i</i>, <i>j</i>]) ) , 可知其分母由输出层类别个数<i>num</i>_<i>classes</i>相加而成, 因此加法计算次数为<i>num</i>_<i>classes</i>-1.又因为训练一个样本有<i>num</i>_<i>classes</i>个结果需要激活函数Softmax函数进行计算, 所以加法次数为<i>num</i>_<i>classes</i>× (<i>num</i>_<i>classes</i>-1) , 最后乘以每次训练投放的样本个数<i>batch</i>_<i>size</i>可以获得一次完整训练所进行的加法次数.对于分母累加中的每一项, 分别有<i>logits</i>[<i>i</i>, <i>j</i>]-1次乘法, 共有<i>num</i>_<i>classes</i>项相加, 故有 (<i>num</i>_<i>classes</i>) × (<i>logits</i>[<i>i</i>, <i>j</i>]-1) 次乘法.又因为需要对样本中每个数据做Softmax激活, 而每次激活时需要做一次除法.除法在机器运算实为右移操作, 而乘法为左移操作, 在功耗上的消耗相当, 因此可看做是做一次乘法, 故需要做<i>batch</i>_<i>size</i>×<i>num</i>_<i>classes</i>+1次.而Softmax包含的乘法次数为 (<i>batch</i>_<i>size</i>×<i>num</i>_<i>classes</i>) ×<i>num</i>_<i>classes</i>×[ (<i>logits</i>[<i>i</i>, <i>j</i>]-1) +1].</p>
                </div>
                <div class="p1">
                    <p id="72">8) Sigmoid.它也是神经元激活函数算子, 其计算公式为<i>Sigmoid</i> (<i>Output</i>_<i>index</i>) =1/ (1+e<sup>-<i>Output</i>_<i>index</i></sup>) , 其加法次数为1次, 乘法次数在分母运算了<i>Output</i>_<i>index</i>次, 共计<i>Output</i>_<i>index</i>+1次.</p>
                </div>
                <div class="p1">
                    <p id="73">9) tanh.它是常用的激活函数算子, 也称为双曲正切函数.tanh在特征相差明显时效果较好, 在循环过程中会不断扩大特征效果.其计算公式为</p>
                </div>
                <div class="p1">
                    <p id="74">tanh (<i>Output</i>_<i>index</i>) = (e<sup><i>Output</i>_<i>index</i></sup>-e<sup>-<i>Output</i>_<i>index</i></sup>) / (e<sup><i>Output</i>_<i>index</i></sup>+e<sup>-<i>Output</i>_<i>index</i></sup>) , </p>
                </div>
                <div class="p1">
                    <p id="76">其加法次数为2次, 乘法次数由于e<sup><i>Output</i>_<i>index</i></sup>计算一遍后可以多次重用, 故其乘法次数为<i>Output</i>_<i>index</i>+1次.</p>
                </div>
                <div class="p1">
                    <p id="77">10) ReLU.它为激活函数算子, 由于函数解析式简单, 因此能获得更快的收敛速度.其计算公式为<i>ReLU</i> (<i>Output</i>_<i>index</i>) =max{0, <i>Output</i>_<i>index</i>}, 该算子不包含乘法, 仅有一次比较, 故加法次数为1.</p>
                </div>
                <div class="p1">
                    <p id="78">本文针对以VGG和AlexNet为代表的主流神经网络, 在层粒度进行了解析.所提出的静态分析方案不增加动态网络运行延迟和能耗开销.同时, 本文所提出的方案给出了网络层粒度的任务分布, 可以结合对网络数据流的分析, 建立动态网络运行分析模型, 为综合优化性能和能耗提供支持.</p>
                </div>
                <div class="p1">
                    <p id="79">神经网络架构通常计算密度高、存储需求大, 为了适应低功耗的移动等存储环境, 裁剪和量化技术被广泛应用.量化技术通常对所有输入数据采用统一的方法, 如散列和定点化进行处理, 因此对于增加量化技术的神经网络, 量化的计算量与输入数据规模成比例.而对于应用剪枝技术的神经网络, 本方法可以计算出网络的计算量和存储量上限, 结合裁剪技术的具体实现方案, 通过对权值矩阵的分析统计裁剪情况, 得出实际的网络计算特征.</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>2 基于神经网络特征的运行优化算法</b></h3>
                <div class="p1">
                    <p id="81">支持深度神经网络运行的GPU和FPGA平台能够同时并行执行大量乘加运算, 甚至可以支持多个算子的并行执行, 流水化地执行多个输入可以有效提高硬件资源利用率<citation id="236" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.以FPGA为例, FPGA芯片可以划分为多个DSP, 乘法器和加法器都由若干个DSP组成, 此时算子所需硬件计算单元就可以转化为DSP数量, 当算子在FPGA芯片上执行无法占满整个芯片时, 可以根据所需DSP数量计算组合方案, 在同一时间内可以让多个算子同时处理.</p>
                </div>
                <div class="p1">
                    <p id="82">专用硬件加速器和FPGA通常都只能包含固定数量的加法器和乘法器, 具体的数值则由它的结构和硬件工艺所确定.然而, 硬件支持的乘加运算器数量和神经网络不同层次所需要的乘加操作次数难以完美匹配, 也就是可以放入同一块芯片或加速单元中运行的算子有多重组合.如果没有对网络结构和拓扑进行全局性的分析, 系统只能顺序或者随机地选取算子执行, 不但无法保证资源利用率, 往往还会影响加速性能, 加大硬件开销.因此针对网络内部不同层对硬件资源的需求, 结合实际计算资源的情况进行优化调度是必要的.</p>
                </div>
                <div class="p1">
                    <p id="83">本文利用所提取的特征分析结果对神经网络在特定硬件资源下的运行调度方案进行优化, 我们将不同的算子转化成乘法和加法操作次数, 结合系统资源的划分, 如FPGA中DSP的资源数量, 提出最大值更替调度算法.本文以卷积层为例来进行调度的规划, 算法可以推广到神经网络包含的各种算子的硬件资源调度.</p>
                </div>
                <div class="p1">
                    <p id="84">神经网络的基本乘法和加法操作还可以归一化量化为基本单元数, 例如量化为逻辑门的数量或DSP的数量, 根据不同的硬件描述和实现工艺, 不同的层可以用乘法和加法数量乘以乘法器和加法器的门的数量需求得到本层以基本单元数为单位的尺寸.神经网络的调度算法可以建模为不同大小的层, 尽可能放入有限大小的硬件芯片上, 这类似于背包问题.但与背包问题有2方面区别:1) 背包问题只选择部分物品装满一个背包即可, 而神经网络的所有操作都需要执行, 如果总的操作数量大于当前硬件能够支持的计算量, 则需要多次执行, 也就相当于多次背包;2) 背包问题只给出选择物品的数量, 而调度算法需要获得调度方案和具体顺序以指导实际运行, 因此我们改进了背包算法, 提出基于最大值更替的调度算法.算法的描述如算法1所示.</p>
                </div>
                <div class="p1">
                    <p id="85"><b>算法1</b>. 基于最大值更替的调度算法.</p>
                </div>
                <div class="p1">
                    <p id="86">输入:硬件容量<i>C</i>、卷积层种数<i>N</i>、第<i>i</i>种卷积层的总个数<i>k</i><sub><i>i</i></sub>、第<i>i</i>种卷积层综合乘法开销<i>W</i>[<i>i</i>];</p>
                </div>
                <div class="p1">
                    <p id="87">输出:使用的硬件总数、每个硬件分别承载的卷积层种类和其数量.</p>
                </div>
                <div class="area_img" id="175">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201906006_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="112">算法1输入分为2部分:1) 网络模型的各卷积层所包含的乘法和加法次数列表数据, 该部分数据来自特征提取模块所输出的结果;2) 当前所应用的硬件资源包含的基本单元数.设网络模型中卷积层的大小有<i>N</i>种, 第<i>i</i> (0≤<i>i</i>≤<i>N</i>) 种卷积层执行所需的基本单元数<i>W</i>[<i>i</i>]由上一步的特征提取给出.在算法运行过程中记录第<i>i</i>种的3方面信息:该种卷积层已被规划了的次数<i>a</i><sub><i>i</i></sub>、该种卷积层是输入当中的序号<i>b</i><sub><i>i</i></sub>以及该种卷积层当前的个数<i>k</i><sub><i>i</i></sub>;同时将硬件平台所包含的基本单元数量记为<i>C</i>.在规划过程中, 首先判断当前基本单元数<i>C</i><sub><i>i</i></sub>是否大于当前最大卷积操作的资源需求<i>max</i>, 如果大于那么当前卷积可以执行, 则<i>C</i><sub><i>i</i></sub>=<i>C</i><sub><i>i</i></sub>-<i>max</i>且最大值的<i>k</i><sub><i>i</i></sub>=<i>k</i><sub><i>i</i></sub>-1;此时再判断<i>k</i><sub><i>i</i></sub>是否为0, 如果为0那么从当前数据中移除该种卷积, 最大值<i>max</i>重新设为当前数据中的最大值, 如果此时移除的恰好是当前最小卷积操作资源需求<i>min</i>, 那么将该卷积移除后更新<i>min</i>值为当前数据中的最小值.如果当前基本单元数量<i>C</i><sub><i>i</i></sub>比卷积操作的最大值<i>max</i>小, 那么就判断当前的最大值是否跟当前数据中的最小值重合, 如果重合那么将当前基本单元数<i>C</i><sub><i>i</i></sub>重置为<i>C</i>, 最大值<i>max</i>重新设为当前数据中的最大值;而如果最大值和最小值还未重合, 那么最大值设为当前数据中的次大值, 而后再次进行<i>C</i><sub><i>i</i></sub>与<i>max</i>的比较.依次进行上述操作, 直到所有卷积都完成规划.</p>
                </div>
                <h3 id="113" name="113" class="anchor-tag"><b>3 实验评估</b></h3>
                <div class="p1">
                    <p id="114">本文选取了6个被广泛应用于各个领域中的神经网络模型.</p>
                </div>
                <div class="p1">
                    <p id="115">1) 图像识别类模型.通过多层卷积算子提取出图像的特征, 根据提取出的特征对图像进行处理、分析和理解, 以识别各种不同模式的目标和对象.常见的包括:AlexNet模型、VGG模型和Inception模型.</p>
                </div>
                <div class="p1">
                    <p id="116">2) 音频识别类模型.主要作用是区分人声、动物声音或者音乐演奏等声音, 典型的神经网络模型是VGGish.</p>
                </div>
                <div class="p1">
                    <p id="117">3) 视频识别类模型SSD.SSD是基于前向传播的神经网络、无全连接层、参数少、运行速度快、识别精度高.</p>
                </div>
                <div class="p1">
                    <p id="118">4) 文本类模型Attention.Attention模型从网络中某些状态集合中选取与给定状态较为相似的状态, 然后训练一个模型来对输入进行选择性的学习并且在模型输出时将输出序列与之进行关联.</p>
                </div>
                <div class="p1">
                    <p id="119">首先, 通过分析各个模型所包含的算子类型和数量, 如表3所示;然后从模型复杂度和资源需求等方面对算子的特征进行分析;最后, 采用所提出的最大值更替算法给出了调度方案, 并同顺序调度方案对比资源利用率的提升效果.本文用TensorFlow中的图形化组件TensorBoard, 按前面的设计方法, 可以提取出神经网络的图形化表示, 如AlexNet的图形化表示, 如图3所示.通过分析可以清楚地得到AlexNet网络的结构图, 由8层组成, 共有5个卷积层和3个全连接层, 分别是输入层→卷积→池化→卷积→池化→卷积→卷积→卷积→池化→全连接→dropout→全连接→dropout→全连接, 在每一个卷积层后都经过了降采样 (pooling处理) , 同时该模型中加入了dropout操作.我们将6个神经网络模型均按照上述方式进行建模, 进而分析出它们的特征.</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表3 6个经典神经网络模型</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Six Classical Neural Network Models</b></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />Application</td><td>NN Model</td><td>Operators</td></tr><tr><td><br /></td><td>AlexNet<sup>[20]</sup></td><td>Conv2D, Max_Pool, MatMul, Bias_Add, LRN, ReLU</td></tr><tr><td><br />Image Recognition</td><td>Inception</td><td>Conv2D, Max_Pool, Avg_Pool, MatMul, Bias_Add, Softmax</td></tr><tr><td><br /></td><td>VGG19<sup>[22]</sup></td><td>Conv2D, Max_Pool, MatMul, Bias_Add, Softmax</td></tr><tr><td><br />Audio Recognition</td><td>VGGish</td><td>Conv2D, Max_Pool, MatMul, Bias_Add, Softmax</td></tr><tr><td><br />Video Recognition</td><td>SSD<sup>[23]</sup></td><td>Conv2D, Softmax, MatMul, Bias_Add, Max_Pool</td></tr><tr><td><br />Text Recognition</td><td>Attention<sup>[24]</sup></td><td>MatMul, Bias_Add, tanh, Softmax, Sigmoid, Conv2D</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 AlexNet可视化模型" src="Detail/GetImg?filename=images/JFYZ201906006_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 AlexNet可视化模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 The visualization model of AlexNet</p>

                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>3.1 经网络特征分析</b></h4>
                <div class="p1">
                    <p id="123">加速器的选取主要从3个角度进行分析:从模型的总体角度统计运算量, 给出针对模型选用加速器的建议;从算子角度, 通过对各个算子的运算量比较, 得出针对该算子的加速器设计建议;考虑时间因素判断模型各阶段应使用的加速器结构.针对这3个需求, 我们主要从模型复杂程度、卷积算子占算子总数比例、各模型占用内存量情况以及运算量4个角度对网络的特征进行分类分析.</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">1) 模型复杂度分析</h4>
                <div class="p1">
                    <p id="125">由模型复杂程度的对比, 我们统计了所有算子的总数量.由于模型结构这一概念过于抽象, 很难直接对模型结构进行讨论, 因此对模型的复杂度采用操作总数作为比较标准.</p>
                </div>
                <div class="p1">
                    <p id="126">从图4对6种网络的分析结果中可以看出, Attention网络的算子个数最多, 达到了5 905个, 整个网络模型也最复杂;而VGGish的算子个数最少, 仅有178个, 其结构也就最简单.因此在模型结构复杂度角度, 视频识别模型SSD、文本识别类模型的操作数量非常庞大, 因此可以在不影响模型功能的前提下从精简算子数量的角度对模型进行优化.</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_127.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 模型复杂度" src="Detail/GetImg?filename=images/JFYZ201906006_127.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 模型复杂度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_127.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Model complexity</p>

                </div>
                <h4 class="anchor-tag" id="129" name="129">2) 卷积算子占比分析</h4>
                <div class="p1">
                    <p id="130">卷积操作是神经网络众多算子中计算复杂度和能耗开销最高的算子, 卷积的优化对提高网络性能有重要的影响.基于本文所提出的方法, 对6种网络的卷积操作比例进行了统计, 如图5所示:</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 卷积算子占比" src="Detail/GetImg?filename=images/JFYZ201906006_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 卷积算子占比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_131.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Convolution operator proportion</p>

                </div>
                <div class="p1">
                    <p id="132">由图5可知, 图像、视频识别类模型、卷积算子占比明显多于文本类模型和音频模型中的卷积算子占比.这是由于卷积在网络中担当着对输入进行特征提取的工作, 卷积算子越多, 对输入提取出的特征就越详细.音频分析网络由于没有图片特征分析那么高的要求, 故比例相对较低.根据这些分析, 在加速器设计过程中对卷积占比高的神经网络进行加速, 可以通过减少卷积操作方式或个数等技术达到更好的效果.</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">3) 内存需求分析</h4>
                <div class="p1">
                    <p id="135">神经网络的执行过程中要存储大量中间结果, 因此对内存需求量往往较大.我们使用各个模型总内存占用量除以运算次数得到如图6所示的内存占用量.</p>
                </div>
                <div class="p1">
                    <p id="136">针对统计出的模型中平均每次运算占用的内存量设定阈值0.1B/次, 超过阈值的模型体现为访存密集型;小于阈值的模型则体现计算密集型.在这些网络中, AlexNet的平均每次运算占用的内存量远超过其他的神经网络, 它在这些网络模型中访存比例更高, 因此调度运算顺序, 减小运算中间结果的存储等优化方法能够有效提高这类网络的性能.</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_137.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 运算占用内存使用量" src="Detail/GetImg?filename=images/JFYZ201906006_137.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 运算占用内存使用量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_137.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Operational memory usage</p>

                </div>
                <h4 class="anchor-tag" id="138" name="138">4) 模型特征分析</h4>
                <div class="p1">
                    <p id="139">基于对模型的操作数、卷积占比、平均内存量、加法次数、乘法次数5个方面的统计信息, 通过雷达图对比不同模型的特征, 如图7所示.结果显示每个模型都有自己的特点:SSD和Attention计算占用内存方面比较小, 其他资源消耗比较平衡.而AlexNet, Inception, VGG19和VGGish这4个模型都是卷积占比突出, 因此本文针对这4个模型归一比较, 如图8所示.模型在不同的维度展现出不同的特征, 通过雷达图结果的分析, 能够为未来加速器设计技术从加速卷积操作、减少内存占用等方面的选择提供指导.</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_133.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 各神经网络模型硬件资源消耗分布雷达图" src="Detail/GetImg?filename=images/JFYZ201906006_133.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 各神经网络模型硬件资源消耗分布雷达图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_133.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Radar charts of hardware resource consumption distribution for neural network models</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 AlexNet, Inception, VGG19, VGGish网络资源占比详细分析" src="Detail/GetImg?filename=images/JFYZ201906006_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 AlexNet, Inception, VGG19, VGGish网络资源占比详细分析  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_140.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Detailed comparison of AlexNet, Inception, VGG19 and VGGish resources</p>

                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>3.2 神经网络调度优化评测</b></h4>
                <div class="p1">
                    <p id="142">我们对比了所提出的最大值更替调度算法和顺序执行方案的资源利用率.顺序调度方案将可以同时执行的操作按从大到小的顺序依次放入芯片中.若可以放下, 则更新当前芯片容量, 同时减少该种算子的数量;若放不下则启用下一个芯片计数继续放置, 算法描述如算法2所示.</p>
                </div>
                <div class="area_img" id="176">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201906006_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="176">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201906006_17601.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="162">本文参考XC7VX690T系统资源进行调度, 该实验板的DSP数量为3 600.规划方案中芯片的平均利用率如图9所示:</p>
                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201906006_163.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 硬件资源利用率" src="Detail/GetImg?filename=images/JFYZ201906006_163.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 硬件资源利用率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201906006_163.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Hardware resource utilization rate</p>

                </div>
                <div class="p1">
                    <p id="164">结果显示最大值更替调度算法显著地提高了VGG16和Inception网络运行时的硬件资源利用率.VGG16网络中, 相对于传统的顺序调度, 芯片利用率仅有81.31%, 而在相同条件下, 采用最大值更替调度算法, 可以将芯片使用率提升到97.57%.于此类似, 在Inception网络中, 最大值更替调度算法将资源利用率提高了13.89%.而对于AlexNet和VGG19两种算法的调度结果相差不大.从网络特性角度分析, AlexNet网络的雷达图同Inception以及VGG都不同, 而对于AlexNet算法顺序调度得到的结果已经相对较好, 因此最大值更替算法的提升效果不明显.</p>
                </div>
                <div class="p1">
                    <p id="165">而对比VGG16和VGG19, 最大值更替算法能否产生作用, 与输入的数据分布有关.在这2个实验中, 模型上的差异只有层数不一样, 而卷积的相关参数是相同的, 在这种情况下, 同样的输入会使得调度大同小异, 但若改变输入的数据, 其调度结果会因调度算法的选择而产生较大的不同.本文所提出算法的计算复杂度为<i>O</i> (<i>n</i>!) , 顺序调度方式虽然计算复杂较低, 但是其芯片利用率远小于基于最大值更替调度算法所提供的调度效果.</p>
                </div>
                <h3 id="166" name="166" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="167">神经网络技术日益发展的今天, 各个领域对网络运算速度和精度的需求都在不断提高.然而随着应用领域的不同, 网络模型也千差万别.如何根据网络规模设计高能效的加速器, 以及基于有限硬件资源如何提高网络性能并最大化资源利用率是当今体系结构领域研究的重要问题.为此, 本文提出一种基于算子的模型分析方法, 将神经网络的层视为算子, 首先分析模型的算子种类、数量和顺序.然后基于不同算子功能分析统计算子中乘法和加法等基本操作数量以及内存占用量等特征的公式, 实现基于不同输入的模型特征分析.此外, 本文提出基于计算特征的最大值更替调度算法, 实现基于给定硬件资源和不同模型规模的运行调度方案.实验结果显示, 本文所提出的方法为从体系结构角度分析神经网络, 优化硬件加速器的设计提供了参考和指导.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="242" type="formula" href="images/JFYZ201906006_24200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">方荣强</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="243" type="formula" href="images/JFYZ201906006_24300.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王晶</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="244" type="formula" href="images/JFYZ201906006_24400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">姚治成</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="245" type="formula" href="images/JFYZ201906006_24500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘畅</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="246" type="formula" href="images/JFYZ201906006_24600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张伟功</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="177">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                Redmon J, Divvala S, Girshick R, et al.You only look once:Unified, real-time object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2016:779-788
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better,Faster,Stronger">

                                <b>[2]</b>Redmonand J, Farhadi A.Yolo9000:Better, faster, stronger[C]Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2017:6517-6525
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">

                                <b>[3]</b>Szegedy C, Liu Wei, Jia Yangqing, et al.Going deeper with convolutions[C]Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2015:1-9
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On-demand deep model compression for mobile devices:A usage-driven model selection framework">

                                <b>[4]</b>Liu Sicong, Lin Yingyan, Zhou Zimu, et al.On-demand deep model compression for mobile devices:A usage-driven model selection framework[C]Proc of the 16th Annual Int Conf on Mobile Systems, Applications, and Services (MobiSys) .New York:ACM, 2018:389-400
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep X:A software accelerator for low-power deep learning inference on mobile devices">

                                <b>[5]</b>Lane N D, Bhattacharya S, Georgiev P, et al.Deepx:Asoftware accelerator for low-power deep learning inference on mobile devices[C]Proc of the 15th ACM/IEEE Int Conf on Information Processing in Sensor Networks (IPSN) .Piscataway, NJ:IEEE, 2016:1-12
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparsifying deep learning layers for constrained resource inference on wearables">

                                <b>[6]</b>Lane N D, Bhattacharya S.Sparsifying deep learning layers for constrained resource inference on wearables[C]Proc of the 14th ACM Conf on Embedded Network Sensor Systems (SenSys) .New York:ACM, 2016:176-189
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CCR:A concise convolution rule for sparse neural network accelerators">

                                <b>[7]</b>Li Jiajun, Yan Guihai, Lu Wenyan.CCR:A concise convolution rule for sparse neural network accelerators[C]Proc of the 2018Design, Automation&amp;Test in Europe Conf (DATE) .Piscataway, NJ:IEEE, 2018:189-194
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating fast algorithms for convolutional neural networks on FPGAs">

                                <b>[8]</b>Liang Yun, Lu Liqiang, Xiao Qingcheng, et al.Evaluating fast algorithms for convolutional neural networks on FPGAs[C]Proc of the 25th IEEE Symp on Field Programmable Custom Computing Machines (FCCM) .Piscataway, NJ:IEEE, 2017:101-108
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse Convolutional Neural Networks">

                                <b>[9]</b>Liu Baoyuan, Wang Min, Foroosh H, et al.Sparse convolutional neural networks[C]Proc of the 28th IEEEConf on Computer Vision and Pattern Recognition (CVPR) .Piscataway, NJ:IEEE, 2015:806-814
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenets:Efficient convolutional neural networks for mobile vision applications">

                                <b>[10]</b>Howard A G, Zhu Menglong, Chen Bo, et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[J].arXiv preprint arXiv:1704.04861, 2017
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Compression:Compressing Deep Neural Network with Pruning,Trained Quantization and Coding">

                                <b>[11]</b>Han Song, Mao Huizi, Dally W J.Deep compression:Compressing deep neural network with pruning, trained quantization and Huffman coding[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:ACM, 2016:Article ID:11
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The power of sparsity in convolutional neural networks">

                                <b>[12]</b>Changpinyo S, Sandler M, Zhmoginov A.The power of sparsity in convolutional neural networks[C]Proc of the5th Int Conf on Learning Representations (ICLR) .New York:ACM, 2017:Article ID:336
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latency-driven design for FPGA-based convolutional neural networks">

                                <b>[13]</b>Venieris S I, Bouganis C.Latency-driven design for FPGA-based convolutional neural networks[C]Proc of the 27th Int Conf on Field Programmable Logic and Applications (FPL) .Piscataway, NJ:IEEE, 2017:1-8
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eyeriss:a spatial architecture for energy-efficient dataflow for convolutional neural networks">

                                <b>[14]</b>Chen Y H, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C]Proc of the ACM/IEEE 43rd Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2016:367-379
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fused-layer CNN accelerators">

                                <b>[15]</b>Alwani M, Chen Han, Ferdman M, et al.Fused-layer CNNaccelerators[C]Proc of the 49th Annual IEEE/ACM Int Symp on Microarchitecture (MICRO) .Piscataway, NJ:IEEE, 2016:1-12
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SqueezeN et:AlexN et-level accuracy with 50x fewer parameters and&amp;lt;0.5MB model size">

                                <b>[16]</b>Iandola F N, Han Song, Moskewicz M W, et al.SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and&lt;1MB model size[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:ACM, 2016:Article ID:465
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LogCA:A high-level performance model for hardware accelerators">

                                <b>[17]</b>Altaf M S B, Wood D A.LogCA:A high-level performance model for hardware accelerators[C]Proc of the ACM/IEEE 44th Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2017:375-388
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks">

                                <b>[18]</b>Zhang Chen, Li Peng, Sun Guangyu, et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays (FPGA) .New York:ACM, 2015:161-170
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design space exploration of FPGA-based deep convolutional neural networks">

                                <b>[19]</b>Motamedi M, Gysel P, Akella V, et al.Design space exploration of FPGA-based deep convolutional neural networks[C]Proc of the 21st Asia and South Pacific Design Automation Conf (ASP-DAC) .Piscataway, NJ:IEEE, 2016:575-580
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classifcation with deep convolutional neural networks">

                                <b>[20]</b>Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2012:1097-1105
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximizing CNNaccelerator efficiency through resource partitioning">

                                <b>[21]</b>Shen Yongming, Ferdman M, Milder P.Maximizing CNNaccelerator efficiency through resource partitioning[C]Proc of the ACM/IEEE 44th Annual Int Symp on Computer Architecture (ISCA) .Piscataway, NJ:IEEE, 2017:535-547
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[22]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[C]Proc of the2nd Int Conf on Learning Representations (ICLR) .New York:ACM, 2014:Article ID:4
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[23]</b>Liu Wei, Anguelov D, Erhan D, et al.SSD:Single shot multibox detector[C]Proc of the European Conf on Computer Vision (ECCV) .Cham:Springer, 2015:21-37
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                Xu K, Ba J L, Kiros R, et al.Show, attend and tell:Neural image caption generation with visual attention[C]Proc of the 32nd Int Conf on Machine Learning (ICML) .New York:ACM, 2015:37:2048-37:2057
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201906006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201906006&amp;v=MzExMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRmlEbVVMck9MeXZTZExHNEg5ak1xWTlGWW9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4vMkp4bGYzeWRZV1dkYUVsZ05IRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

