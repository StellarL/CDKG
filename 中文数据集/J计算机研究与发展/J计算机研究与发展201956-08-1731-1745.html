

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127885733712500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908016%26RESULT%3d1%26SIGN%3dOyjD1c%252bNlc0%252bBSZbt%252fXnQzxMMVw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908016&amp;v=MjIxOTBSckZ5dmdXNzNMTHl2U2RMRzRIOWpNcDQ5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#54" data-title="&lt;b&gt;1 属性的依存子树及距离信息&lt;/b&gt; "><b>1 属性的依存子树及距离信息</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;1.1 属性的依存子树&lt;/b&gt;"><b>1.1 属性的依存子树</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;1.2 相对距离&lt;/b&gt;"><b>1.2 相对距离</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;1.3 语法距离&lt;/b&gt;"><b>1.3 语法距离</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="&lt;b&gt;2 依存树及距离注意力模型DTDA&lt;/b&gt; "><b>2 依存树及距离注意力模型DTDA</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="&lt;b&gt;2.1 任务定义&lt;/b&gt;"><b>2.1 任务定义</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;2.2 模型结构&lt;/b&gt;"><b>2.2 模型结构</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;2.3 输入层&lt;/b&gt;"><b>2.3 输入层</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;2.4 编码层&lt;/b&gt;"><b>2.4 编码层</b></a></li>
                                                <li><a href="#180" data-title="&lt;b&gt;2.5 注意力层&lt;/b&gt;"><b>2.5 注意力层</b></a></li>
                                                <li><a href="#199" data-title="&lt;b&gt;2.6 输出层&lt;/b&gt;"><b>2.6 输出层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#214" data-title="&lt;b&gt;3 模型训练&lt;/b&gt; "><b>3 模型训练</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#232" data-title="&lt;b&gt;4 实验及分析&lt;/b&gt; "><b>4 实验及分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#233" data-title="&lt;b&gt;4.1 实验准备&lt;/b&gt;"><b>4.1 实验准备</b></a></li>
                                                <li><a href="#248" data-title="&lt;b&gt;4.2 实验1:与相关工作的比较&lt;/b&gt;"><b>4.2 实验1:与相关工作的比较</b></a></li>
                                                <li><a href="#264" data-title="&lt;b&gt;4.3 实验2:判定难度对比&lt;/b&gt;"><b>4.3 实验2:判定难度对比</b></a></li>
                                                <li><a href="#277" data-title="&lt;b&gt;4.4 实验3:依存子树作用分析&lt;/b&gt;"><b>4.4 实验3:依存子树作用分析</b></a></li>
                                                <li><a href="#289" data-title="&lt;b&gt;4.5 实验4:距离信息分析&lt;/b&gt;"><b>4.5 实验4:距离信息分析</b></a></li>
                                                <li><a href="#299" data-title="&lt;b&gt;4.6 实验5:注意力权重可视化&lt;/b&gt;"><b>4.6 实验5:注意力权重可视化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#306" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 属性级别情感分类例子">图1 属性级别情感分类例子</a></li>
                                                <li><a href="#59" data-title="图2 句子的依存树结构">图2 句子的依存树结构</a></li>
                                                <li><a href="#64" data-title="图3 相对距离例子">图3 相对距离例子</a></li>
                                                <li><a href="#68" data-title="图4 属性&lt;i&gt;a&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;的依存树语法距离">图4 属性<i>a</i><sub>1</sub>的依存树语法距离</a></li>
                                                <li><a href="#70" data-title="图5 句子中属性&lt;i&gt;a&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;的依存树语法距离">图5 句子中属性<i>a</i><sub>2</sub>的依存树语法距离</a></li>
                                                <li><a href="#115" data-title="图6 DTDA模型的结构">图6 DTDA模型的结构</a></li>
                                                <li><a href="#158" data-title="图7 BGRU的结构">图7 BGRU的结构</a></li>
                                                <li><a href="#197" data-title="图8 属性相关情感特征计算过程">图8 属性相关情感特征计算过程</a></li>
                                                <li><a href="#208" data-title="图9 输出层的计算过程">图9 输出层的计算过程</a></li>
                                                <li><a href="#242" data-title="&lt;b&gt;表1 Laptop和Restaurant数据集统计&lt;/b&gt;"><b>表1 Laptop和Restaurant数据集统计</b></a></li>
                                                <li><a href="#243" data-title="&lt;b&gt;表2 2个数据集的子集划分结果统计&lt;/b&gt;"><b>表2 2个数据集的子集划分结果统计</b></a></li>
                                                <li><a href="#245" data-title="&lt;b&gt;表3 DSDA模型的超参数设置&lt;/b&gt;"><b>表3 DSDA模型的超参数设置</b></a></li>
                                                <li><a href="#247" data-title="&lt;b&gt;表4 词向量训练参数设置&lt;/b&gt;"><b>表4 词向量训练参数设置</b></a></li>
                                                <li><a href="#257" data-title="&lt;b&gt;表5 各模型的实验结果比较&lt;/b&gt;"><b>表5 各模型的实验结果比较</b></a></li>
                                                <li><a href="#267" data-title="&lt;b&gt;表6 各模型在Word2vec-skipgram词向量上的结果&lt;/b&gt;"><b>表6 各模型在Word2vec-skipgram词向量上的结果</b></a></li>
                                                <li><a href="#271" data-title="&lt;b&gt;表7 各模型在S1, S2和S3上的准确率比较&lt;/b&gt;"><b>表7 各模型在S1, S2和S3上的准确率比较</b></a></li>
                                                <li><a href="#286" data-title="&lt;b&gt;表8 不同属性特征表示方式的结果&lt;/b&gt;"><b>表8 不同属性特征表示方式的结果</b></a></li>
                                                <li><a href="#287" data-title="&lt;b&gt;表9 不同属性特征表示方式在S1, S2和S3上的 准确率结果&lt;/b&gt;"><b>表9 不同属性特征表示方式在S1, S2和S3上的 准确率结果</b></a></li>
                                                <li><a href="#291" data-title="&lt;b&gt;表10 距离信息在Laptop和Restaurant上的实验结果&lt;/b&gt;"><b>表10 距离信息在Laptop和Restaurant上的实验结果</b></a></li>
                                                <li><a href="#292" data-title="&lt;b&gt;表11 距离信息在S1, S2和S3上的准确率结果&lt;/b&gt;"><b>表11 距离信息在S1, S2和S3上的准确率结果</b></a></li>
                                                <li><a href="#296" data-title="图10 距离信息在S3上的准确率对比">图10 距离信息在S3上的准确率对比</a></li>
                                                <li><a href="#297" data-title="图11 距离信息在Laptop的S1和S2上的准确率对比">图11 距离信息在Laptop的S1和S2上的准确率对比</a></li>
                                                <li><a href="#298" data-title="图12 DTDA-sum对“food”和“service”的注意力分布">图12 DTDA-sum对“food”和“service”的注意力分布</a></li>
                                                <li><a href="#300" data-title="图13 DTDA-mean对属性“food”和“service”的注意力可视化">图13 DTDA-mean对属性“food”和“service”的注意力可视化</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="364">


                                    <a id="bibliography_1" title="Pontiki M, Galanis D, Pavlopoulos J, et al.Semeval-2014 task 4:Aspect based sentiment analysis[C] //Proc of the 8th Int Workshop on Semantic Evaluation (SemEval 2014) .Stroudsburg, PA:ACL, 2014:27- 35" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sem Eval-2014 Task 4:Aspect Based Sentiment Analysis">
                                        <b>[1]</b>
                                        Pontiki M, Galanis D, Pavlopoulos J, et al.Semeval-2014 task 4:Aspect based sentiment analysis[C] //Proc of the 8th Int Workshop on Semantic Evaluation (SemEval 2014) .Stroudsburg, PA:ACL, 2014:27- 35
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_2" title="Liu Bin, Liu Quan, Xu Jin, et al.Aspect-based sentiment analysis based on multi-attention CNN[J].Journal of Computer Research and Development, 2017, 54 (8) :1724- 1735 (in Chinese) (梁斌, 刘全, 徐进, 等.基于多注意力卷积神经网络的特定目标情感分析[J].计算机研究与发展, 2017, 54 (8) :1724- 1735) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708009&amp;v=MDE4NDJDVVJMT2VaZVJyRnl2Z1c3M0xMeXZTZExHNEg5Yk1wNDlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Liu Bin, Liu Quan, Xu Jin, et al.Aspect-based sentiment analysis based on multi-attention CNN[J].Journal of Computer Research and Development, 2017, 54 (8) :1724- 1735 (in Chinese) (梁斌, 刘全, 徐进, 等.基于多注意力卷积神经网络的特定目标情感分析[J].计算机研究与发展, 2017, 54 (8) :1724- 1735) 
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_3" title="Ding Xiaowen, Liu Bing, Yu P S, et al.A holistic lexicon-based approach to opinion mining[C] //Proc of the 2008 Int Conf on Web Search and Data Mining.New York:ACM, 2008:231- 240" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Holistic Lexicon-Based Approach to OpinionMining">
                                        <b>[3]</b>
                                        Ding Xiaowen, Liu Bing, Yu P S, et al.A holistic lexicon-based approach to opinion mining[C] //Proc of the 2008 Int Conf on Web Search and Data Mining.New York:ACM, 2008:231- 240
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_4" title="Boiy E, Moens M F.A machine learning approach to sentiment analysis in multilingual Web texts[J].Information Retrieval, 2009, 12 (5) :526- 558" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00004329528&amp;v=MDAyODhTWHFScnhveGNNSDdSN3FlYnVkdEZDM2xVTHJBSWwwPU5qM2Fhck80SHRISXJJMU1ZZWtIWTNrNXpCZGg0ajk5&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Boiy E, Moens M F.A machine learning approach to sentiment analysis in multilingual Web texts[J].Information Retrieval, 2009, 12 (5) :526- 558
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_5" title="Jiang Long, Yu Mo, Zhou Ming, et al.Target-dependent twitter sentiment classification[C] //Proc of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies-Volume 1.Stroudsburg, PA:ACL, 2011:151- 160" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Target-dependent TwitterSentiment Classification">
                                        <b>[5]</b>
                                        Jiang Long, Yu Mo, Zhou Ming, et al.Target-dependent twitter sentiment classification[C] //Proc of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies-Volume 1.Stroudsburg, PA:ACL, 2011:151- 160
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_6" title="Li Dong, Wei Furu, Tan Chuanqi, et al.Adaptive recursive neural network for target-dependent Twitter sentiment classification[C] //Proc of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers) .Stroudsburg, PA:ACL, 2014, 2:49- 54" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive recursive neural network for target-dependent twitter sentiment classification">
                                        <b>[6]</b>
                                        Li Dong, Wei Furu, Tan Chuanqi, et al.Adaptive recursive neural network for target-dependent Twitter sentiment classification[C] //Proc of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers) .Stroudsburg, PA:ACL, 2014, 2:49- 54
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_7" title="Tang Duyu, Qin Bin, Feng Xiaocheng, et al.Effective LSTMs for Target-Dependent Sentiment Classification[C] //Proc of the 26th Int Conf on Computational Linguistics.Stroudsburg, PA:ACL, 2016:3298- 3307" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective LSTMs for target-dependent sentiment classification">
                                        <b>[7]</b>
                                        Tang Duyu, Qin Bin, Feng Xiaocheng, et al.Effective LSTMs for Target-Dependent Sentiment Classification[C] //Proc of the 26th Int Conf on Computational Linguistics.Stroudsburg, PA:ACL, 2016:3298- 3307
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_8" title="Zhang Meishan, Zhang Yue, Vo D T.Gated neural networks for targeted sentiment analysis[C] //Proc of the 13th AAAI Conf on Artificial Intelligence.Cambridge, MA:MIT, 2016:3087- 3093" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated Neural Networks for Targeted Sentiment Analysis">
                                        <b>[8]</b>
                                        Zhang Meishan, Zhang Yue, Vo D T.Gated neural networks for targeted sentiment analysis[C] //Proc of the 13th AAAI Conf on Artificial Intelligence.Cambridge, MA:MIT, 2016:3087- 3093
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_9" title="Nguyen T H, Shirai K.PhraseRNN:Phrase recursive neural network for aspect-based sentiment analysis[C] //Proc of the 2015 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2509- 2514" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PhraseRNN:Phrase Recursive Neural Network for Aspect-based Sentiment Analysis">
                                        <b>[9]</b>
                                        Nguyen T H, Shirai K.PhraseRNN:Phrase recursive neural network for aspect-based sentiment analysis[C] //Proc of the 2015 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2509- 2514
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_10" title="Ruder S, Ghaffari P, Breslin J G.A hierarchical model of reviews for aspect-based sentiment analysis[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:999- 1005" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A hierarchical model of reviews for aspect-based sentiment analysis">
                                        <b>[10]</b>
                                        Ruder S, Ghaffari P, Breslin J G.A hierarchical model of reviews for aspect-based sentiment analysis[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:999- 1005
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_11" title="Wang Yequan, Huang Minlie, Zhu Xiaoyan.Attention-based lstm for aspect-level sentiment classification[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:606- 615" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attentionbased LSTM for aspect-level sentiment classification">
                                        <b>[11]</b>
                                        Wang Yequan, Huang Minlie, Zhu Xiaoyan.Attention-based lstm for aspect-level sentiment classification[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:606- 615
                                    </a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_12" title="Wang Xinbo, Chen Guang.Dependency-attention-based lstm for target-dependent sentiment analysis[C] //Proc of Chinese National Conf on Social Media Processing.Berlin:Springer, 2017:206- 217" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dependency-Attention-Based LSTM for Target-Dependent Sentiment Analysis">
                                        <b>[12]</b>
                                        Wang Xinbo, Chen Guang.Dependency-attention-based lstm for target-dependent sentiment analysis[C] //Proc of Chinese National Conf on Social Media Processing.Berlin:Springer, 2017:206- 217
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_13" title="Tay Y, Luu A T, Hui S C.Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Cambridge, MA:MIT, 2018:5956- 5963" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis">
                                        <b>[13]</b>
                                        Tay Y, Luu A T, Hui S C.Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Cambridge, MA:MIT, 2018:5956- 5963
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_14" title="Tang Duyu, Qin Bing, Liu Ting.Aspect level sentiment classification with deep memory network[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:214- 224" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aspect Level Sentiment Classification with Deep Memory Network">
                                        <b>[14]</b>
                                        Tang Duyu, Qin Bing, Liu Ting.Aspect level sentiment classification with deep memory network[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:214- 224
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_15" title="Chen Peng, Sun Zhongqian, Bin Lidong, et al.Recurrent attention network on memory for aspect sentiment analysis[C] //Proc of the 2017 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2017:452- 461" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent Attention Network on Memory for Aspect Sentiment Analysis">
                                        <b>[15]</b>
                                        Chen Peng, Sun Zhongqian, Bin Lidong, et al.Recurrent attention network on memory for aspect sentiment analysis[C] //Proc of the 2017 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2017:452- 461
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_16" title="Ma Dehong, Li Sujian, Zhang Xiaodong, et al.Interactive attention networks for aspect-level sentiment classification[C] //Proc of the 26th Int Joint Conf on Artificial Intelligence.Cambridge, MA:MIT, 2017:4068- 4074" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interactive attention networks for aspect-level sentiment classification">
                                        <b>[16]</b>
                                        Ma Dehong, Li Sujian, Zhang Xiaodong, et al.Interactive attention networks for aspect-level sentiment classification[C] //Proc of the 26th Int Joint Conf on Artificial Intelligence.Cambridge, MA:MIT, 2017:4068- 4074
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_17" title="Ouyang Zhifan.Research on Aspect-level Sentiment Classification Based on Dependency Tree and Attention Network[D].Guangzhou:South China University of Technology, 2018 (in Chinese) (欧阳志凡.基于依存树和注意力的属性级别情感分类研究[D].广州:华南理工大学, 2018) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018874461.nh&amp;v=MzAxMDZPZVplUnJGeXZnVzczTFZGMjZGcnUvR3RYS3JwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Ouyang Zhifan.Research on Aspect-level Sentiment Classification Based on Dependency Tree and Attention Network[D].Guangzhou:South China University of Technology, 2018 (in Chinese) (欧阳志凡.基于依存树和注意力的属性级别情感分类研究[D].广州:华南理工大学, 2018) 
                                    </a>
                                </li>
                                <li id="398">


                                    <a id="bibliography_18" title="Kingma D P, Ba J L.Adam:A method for stochastic optimization[C/OL] //Proc of ICLR 2015.[2019-05-13].https://arxiv.org/pdf/1412.6980v8.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">
                                        <b>[18]</b>
                                        Kingma D P, Ba J L.Adam:A method for stochastic optimization[C/OL] //Proc of ICLR 2015.[2019-05-13].https://arxiv.org/pdf/1412.6980v8.pdf
                                    </a>
                                </li>
                                <li id="400">


                                    <a id="bibliography_19" title="Srivastava N, Hinton G, Krizhevsky A.Dropout:A simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929- 1958" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">
                                        <b>[19]</b>
                                        Srivastava N, Hinton G, Krizhevsky A.Dropout:A simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929- 1958
                                    </a>
                                </li>
                                <li id="402">


                                    <a id="bibliography_20" title="Ioffe S, Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C] //Proc of the 32nd Int Conf on Machine Learning.New York:ACM, 2015:448- 456" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift">
                                        <b>[20]</b>
                                        Ioffe S, Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C] //Proc of the 32nd Int Conf on Machine Learning.New York:ACM, 2015:448- 456
                                    </a>
                                </li>
                                <li id="404">


                                    <a id="bibliography_21" title="He R, McAuley J.Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering[C] //Proc of The 25th In Conf on World Wide Web.New York:ACM, 2016:507- 517" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering">
                                        <b>[21]</b>
                                        He R, McAuley J.Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering[C] //Proc of The 25th In Conf on World Wide Web.New York:ACM, 2016:507- 517
                                    </a>
                                </li>
                                <li id="406">


                                    <a id="bibliography_22" title="Pennington J, Socher R, Manning C D.Glove:Global vectors for word representation[C] //Proc of the Empirical Methods in Natural Language Processing (EMNLP 2014) .Stroudsburg, PA:ACL, 2014, 14:1532- 1543" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glove:Global vectors for word representation">
                                        <b>[22]</b>
                                        Pennington J, Socher R, Manning C D.Glove:Global vectors for word representation[C] //Proc of the Empirical Methods in Natural Language Processing (EMNLP 2014) .Stroudsburg, PA:ACL, 2014, 14:1532- 1543
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1731-1745 DOI:10.7544/issn1000-1239.2019.20190102            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于依存树及距离注意力的句子属性情感分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8B%8F%E9%94%A6%E9%92%BF&amp;code=07555927&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏锦钿</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AC%A7%E9%98%B3%E5%BF%97%E5%87%A1&amp;code=37752293&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">欧阳志凡</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E7%8F%8A%E7%8F%8A&amp;code=35233829&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余珊珊</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E5%8D%97%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0122765&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华南理工大学计算机科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E4%B8%9C%E8%8D%AF%E7%A7%91%E5%A4%A7%E5%AD%A6%E5%8C%BB%E8%8D%AF%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1699439&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广东药科大学医药信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目前基于注意力机制的句子属性情感分类方法由于忽略句子中属性的上下文信息以及单词与属性间的距离特征, 从而导致注意力机制难以学习到合适的注意力权重.针对该问题, 提出一种基于依存树及距离注意力的句子属性情感分类模型 (dependency tree and distance attention, DTDA) .首先根据句子的依存树得到包含属性的依存子树, 并利用双向GRU学习句子及属性的上下文特征表示;根据句子中单词和属性在依存树中的最短路径确定相应的语法距离及位置权重, 同时结合相对距离构造包含语义信息和距离信息的句子特征表示, 并进一步利用注意力机制生成属性相关的句子情感特征表示;最后, 将句子的上下文信息与属性相关的情感特征表示合并后并通过softmax进行分类输出.实验结果表明:DTDA在国际语义评测SemEval2014的2个基准数据集Laptop和Restaurant上取得与目前最好方法相当的结果.当使用相关领域训练的词向量时, DTDA在Laptop上的精确率为77.01%, 在Restaurant上的准确率为81.68%.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%9E%E6%80%A7%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">属性情感分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BE%9D%E5%AD%98%E6%A0%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">依存树;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然语言处理;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *余珊珊, susyu@139.com;
                                </span>
                                <span>
                                    苏锦钿, sujd@scut.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>广东省自然科学基金项目 (2015A030310318);</span>
                                <span>广东省科技厅应用型科技研发专项资金项目 (20168010124010);</span>
                                <span>广东省医学科学技术研究基金项目 (A2015065);</span>
                    </p>
            </div>
                    <h1><b>Aspect-Level Sentiment Classification for Sentences Based on Dependency Tree and Distance Attention</b></h1>
                    <h2>
                    <span>Su Jindian</span>
                    <span>Ouyang Zhifan</span>
                    <span>Yu Shanshan</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering, South China University of Technology</span>
                    <span>College of Medical Information Engineering, Guangdong Pharmaceutical University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Current attention-based approaches for aspect-level sentiment classification usually neglect the contexts of aspects and the distance feature between words and aspects, which as a result make it difficult for attention mechanism to learn suitable attention weights. To address this problem, a dependency tree and distance attention-based model DTDA for aspect-level sentiment classification is proposed. Firstly, DTDA extracts dependency subtree (aspect sub-sentence) that contains the modification information of the aspect with the help of dependency tree of sentences, and then uses bidirectional GRU networks to learn the contexts of sentence and aspects. After that, the position weights are determined according to the syntactic distance between words and aspect along their path on the dependency tree, which are then further combined with relative distance to build sentence representations that contain semantic and distance information. The aspect-related sentiment feature representations are finally generated via attention mechanism and merged with sentence-related contexts, which are fed to a softmax layer for classification. Experimental results show that DTDA achieves comparable results with those current state-of-the-art methods on the two benchmark datasets of SemEval 2014, Laptop and Restaurant. When using word vectors pre-trained on domain-relative data, DTDA achieves the results with the precision of 77.01% on Laptop and 81.68% on Restaurant.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=aspect-level%20sentiment%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">aspect-level sentiment classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dependency%20tree&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dependency tree;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=natural%20language%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">natural language processing;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Su Jindian, born in 1980.PhD.Associate professor.Member of CCF.His main research interests include natural language processing, artifical intelligence, machine learning. <image id="358" type="" href="images/JFYZ201908016_35800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Ouyang Zhifan, born in 1994.Master.His main research interests include natural language processing and deep learning.<image id="360" type="" href="images/JFYZ201908016_36000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Yu Shanshan, born in 1980.PhD.Senior member of CCF. Her main research interests include machine learning, big data and semantic Web.<image id="362" type="" href="images/JFYZ201908016_36200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-02-26</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Natural Science Foundation of Guangdong Province (2015A030310318);</span>
                                <span>the Applied Scientific and Technology Special Project of the Department of Science and Technology of Guangdong Province (20168010124010);</span>
                                <span>the Medical Scientific Research Foundationof Guangdong Province (A2015065);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="50">句子的属性级别 (aspect-level) 情感分类<citation id="408" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 也称细粒度情感分类或特定目标情感分类<citation id="409" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 是近年来自然语言处理 (natural language processing, NLP) 领域中情感分析任务的一个热点, 主要任务是研究如何在给定一个句子和一个目标属性的前提下判定句子关于该属性的情感极性.早期工作主要侧重于如何将句子的各种语法特征 (如词性或句法结构) 或外部知识 (如情感词典或规则) 引入到传统的机器学习方法中, 如文献<citation id="416" type="reference">[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</citation>.近几年, 由于深度学习方法在自动学习文本特征方面具有明显的优势, 可以避免依赖人工设计特征, 因此在句子属性情感分类中开始得到应用.一些学者陆续提出各种基于神经网络的分类模型, 如递归神经网络 (recursive neural network) 、卷积神经网络 (convolutional neural network, CNN) 或循环神经网络 (recurrent neural Network, RNN) 等, 并在多个数据集上取得突出的成绩.例如Dong等人<citation id="410" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>结合句子的依存树结构和特定目标的位置提出基于自适应递归神经网络的情感分类方法;Tang等人<citation id="411" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在长短期记忆网络 (long short-term memory network, LSTM) 的基础上提出特定目标依赖的情感分类方法TD-LSTM (target dependent LSTM) ;Zhang等人<citation id="412" type="reference"><link href="378" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>将句子划分为左上下文、目标和右上下文3个部分, 然后利用双向GRU (bidirectional gated recurrent unit, BGRU) 学习属性的情感特征表示;Nguyen等人<citation id="413" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>在Dong等人<citation id="414" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的工作基础上指出句法信息对属性情感分类的有效性, 提出利用短语和句法关系将句子的依存树转换为属性相关的短语依存二叉树, 同时结合多种语义组合函数的递归神经网络为该依存二叉树建模并判断属性情感极性;Ruder等人<citation id="415" type="reference"><link href="382" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>指出不同句子间通过互相提供上下文信息可以帮助模型更好地判断评论文本中多个属性的情感倾向, 从而可将其转换为一个句子级别的序列标注任务, 并利用一个层次化LSTM网络进行属性情感分类判别.这些工作较好地利用了句子的语法特征和属性的位置信息等, 但在刻画属性和上下文之间的关系时往往需要根据属性的位置对句子的句法树和依存树结构进行转换, 容易破坏句子中原本的序列信息.</p>
                </div>
                <div class="p1">
                    <p id="51">近2年, 为了更好地刻画不同属性的上下文信息并体现句子中不同单词对情感极性的影响, 一些学者开始将注意力机制引入到属性情感分类任务中.例如Wang等人<citation id="417" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出基于注意力和属性词向量的LSTM (attention based LSTM with aspect embedding, ATAE-LSTM) 模型, 主要以属性词的词向量为注意力目标, 将属性特征表示和句子经过LSTM建模后的隐藏状态矩阵连接, 并利用一个前馈隐藏层求出每个时间步隐藏状态的注意力权重, 从而构造属性相关的情感特征表示.Wang等人<citation id="418" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在Tang<citation id="419" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和Wang<citation id="420" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的工作基础上将句子中各单词对属性词的依赖信息引入到注意力权重的学习中, Tay等人<citation id="421" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>则将注意力权重的计算方式由前馈隐藏层转换为circular correlation和circular convo-lution.Tang等人<citation id="422" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>还将记忆网络 (memory network, MemNet) 与多层注意力机制相结合, 通过利用多层注意力机制生成属性和上下文的特征表示.随后, Chen等人<citation id="423" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>指出MemNet中的属性和上下文特征表示本质是词向量的线性组合, 并提出循环注意力记忆 (recurrent attention memory, RAM) 模型.RAM的主要思路是利用一个堆叠的双向LSTM生成外部记忆矩阵, 然后根据句子中单词和属性词间的距离信息为每个单词产生的记忆片段分配不同的位置权重, 最后利用GRU网络及多层注意力机制构造属性的情感特征表示.Ma等人<citation id="424" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出交互注意力网络 (interactive attention networks, IAN) , 主要是利用LSTM分别对句子和属性建模, 然后通过一个交互式注意力机制分别选择句子和属性中的重要信息, 最后合并结果作为情感特征并预测属性的情感极性.梁斌等人<citation id="425" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>则在多注意力CNN的基础上结合词向量注意力、词性注意力和位置注意力等多种机制学习句子的深层次情感信息, 最后在SemEval 2014的Latop和Restaurant数据集上取得68.04%和79.17%的准确率.</p>
                </div>
                <div class="p1">
                    <p id="52">总地来说, 基于注意力的属性级别情感分类研究已经取得了较好的效果, 但仍存在2点不足:1) 忽略属性词的上下文信息.现有研究在根据属性词计算注意力权重时往往简单地将属性的词向量平均值或线性组合作为其特征表示, 没有考虑属性词中的短语及其修饰词的词序及上下文信息.2) 没有考虑单词的位置信息及单词间的依赖关系信息.部分研究虽然采用相对距离刻画单词与属性词之间的位置信息, 但没有考虑单词间的语法依赖关系, 且容易出现因表达方式的问题而导致远离属性的相关情感词的权重被线性降低, 从而影响模型对注意力权重的学习.作者在前期工作<citation id="426" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>中虽然通过属性依存子树引入语法距离, 但没有进一步考虑单词间的相对距离信息.</p>
                </div>
                <div class="p1">
                    <p id="53">本文在上述工作的基础上利用句子的依存树抽取包含属性及其修饰信息的属性子句, 并结合单词与属性词之间的相对距离及语法距离, 提出一种基于注意力的句子属性情感分类模型DTDA, 最后通过公开的语料库进行验证和分析.</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag"><b>1 属性的依存子树及距离信息</b></h3>
                <h4 class="anchor-tag" id="55" name="55"><b>1.1 属性的依存子树</b></h4>
                <div class="p1">
                    <p id="56">在句子的属性情感分类任务中, 一个句子通常包含多个不同的属性, 并且对应着不同的情感极性.例如, 句子“This French food tastes very well, but the restaurant has poor service.”中包含2个不同的属性“French food”和“service”, 对应的情感极性分别为积极性的 (Pos) 和消极的 (Neg) , 即 (French food, Pos) 和 (service, Neg) , 如图1所示:</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 属性级别情感分类例子" src="Detail/GetImg?filename=images/JFYZ201908016_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 属性级别情感分类例子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 An example of aspect-level sentiment classification</p>

                </div>
                <div class="p1">
                    <p id="58">与普遍的句子情感分类不同, 属性情感分类的判别不仅依赖于句子的上下文信息, 还依赖于属性的特征信息<citation id="427" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.同一个句子的不同属性可能具有完全相反的情感极性.因此, 如何准确地刻画属性及与句子上下文的关系是属性情感分类研究中一个非常关键的问题.现有的许多相关研究主要利用预训练词向量给出属性中各个单词的表示, 然后通过求平均值、求和或其他线性组合的方式得到属性的特征表示.这种表示方式一方面容易丢失短语中单词的序列信息, 另一方面由于目前常用的预训练词向量主要由word2vec或Glove等模型根据一定窗口内的单词共现信息训练而成, 因此导致出现在相近上下文信息中的属性也具有相近的词向量, 难以形成有效的差异化表示.人们在句子中表达属性的情感时往往添加一些形容词及副词等修饰成分, 例如“poor”或“badly”等.这些修饰词很大程度上影响了句子中属性的情感极性, 即修饰词与属性词间存在一定的依赖关系.因此, 可利用句子的依存树进一步分析句子中各单词间的关系.以图1中的句子为例, 其依存树如图2所示:</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 句子的依存树结构" src="Detail/GetImg?filename=images/JFYZ201908016_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 句子的依存树结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_059.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Dependency tree structure for sentence</p>

                </div>
                <div class="p1">
                    <p id="60">由图2可知, 句子<i>S</i>包含2个属性: (<i>a</i><sub>1</sub>, “French food”) 和 (<i>a</i><sub>2</sub>, “service”) .属性<i>a</i><sub>1</sub>为短语, 且存在词性为限定词的修饰词“this”.属性<i>a</i><sub>2</sub>为单词, 且存在词性为形容词的修饰词“poor”.显然, 属性的修饰词一般依存于属性词, 并与属性一起构成以属性词中某个单词为根节点的依存子树.因此利用句子的依存树结构可抽取属性的依存子树 (dependency subtree) , 从而得到包含属性及其修饰信息的属性子句 (aspect clause) <citation id="428" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 如图2中的<i>S</i><sub><i>a</i><sub>1</sub></sub>=“This French food”和<i>S</i><sub><i>a</i><sub>2</sub></sub>=“poor service”.</p>
                </div>
                <div class="p1">
                    <p id="61">相对于简单基于词向量的平均值或线性组的属性特征表示来说, 属性子句不仅包含属性词本身, 还包含属性的修饰信息以及它们之间的位置顺序信息.</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62"><b>1.2 相对距离</b></h4>
                <div class="p1">
                    <p id="63">一般来说, 句子中属性相关的情感描述词往往与该属性的位置非常接近.梁斌等在文献<citation id="429" type="reference">[<a class="sup">2</a>]</citation>中指出属性情感分类中单词与属性词间的位置往往隐含重要的信息, 离目标词更近的词对目标词的影响更大.因此, 可将属性与句子中各个单词间的相对距离作为一种重要的信息引入到属性情感分类模型中.以图1中的句子为例, 单词与属性词间的相对距离如图3所示:</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 相对距离例子" src="Detail/GetImg?filename=images/JFYZ201908016_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 相对距离例子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 An example of relative distance</p>

                </div>
                <div class="p1">
                    <p id="65">文献<citation id="430" type="reference">[<a class="sup">15</a>]</citation>同样也利用相对距离信息刻画单词与属性间的关系, 目的是为了更好地生成属性相关的句子特征表示.对于每一个包含<i>k</i>个单词<i>w</i><sub><i>i</i></sub> (1≤<i>i</i>≤<i>k</i>) 的句子<i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>a</i>, …, <i>w</i><sub><i>k</i></sub>) , 针对每一个属性<i>a</i>可得到相应的相对距离向量<b><i>C</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>) .</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>1.3 语法距离</b></h4>
                <div class="p1">
                    <p id="67">虽然相对距离能够刻画单词与属性词间的位置关系, 但使得句子中各单词的权重以属性词为中心, 并向左右两边线性降低.由于句子的情感表达往往是一个比较复杂的语法结构, 包含对比和否定等, 因此有些情况下相对距离难以全面客观地体现句子中各个单词对属性情感极性判定的影响.例如, 在句子“Although the food is very good, the service in this restaurant is so dreadful”中属性词“service”与情感词“good”的距离要比“dreadful”更近, 但显然“dreadful”才是属性“service”的正确情感表达, 其权重应该要比“good”更大.</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 属性a1的依存树语法距离" src="Detail/GetImg?filename=images/JFYZ201908016_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 属性<i>a</i><sub>1</sub>的依存树语法距离  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Syntax distance of dependency tree for aspect <i>a</i><sub>1</sub></p>

                </div>
                <div class="p1">
                    <p id="69">下面将单词和属性间的距离定义为两者在句子依存树中的路径长度, 即语法距离.在句子的依存树中, 单词间通过依存关系进行连接, 因此越是相关的单词其语法距离越小, 而跟它们在句子中的实际位置无关, 即语法距离能够更好地反映单词与属性间的相关性.以图1为例, 属性<i>a</i><sub>1</sub>=“French food”和<i>a</i><sub>2</sub>=“service”的依存树语法距离分别如图4和图5所示.</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 句子中属性a2的依存树语法距离" src="Detail/GetImg?filename=images/JFYZ201908016_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 句子中属性<i>a</i><sub>2</sub>的依存树语法距离  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Syntax distance of dependency tree for aspect <i>a</i><sub>2</sub></p>

                </div>
                <div class="p1">
                    <p id="71">在基于依存树的语法距离计算过程中, 离属性越近的单词具有越高的权重.若属性为短语, 则以该短语中最后一个单词作为代表计算与句子中各个单词的语法距离.具体算法描述如下:</p>
                </div>
                <div class="area_img" id="363">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201908016_36300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="363">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201908016_36301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="103">对每一个<i>S</i>及属性<i>a</i>, 由算法1可得到相应的语法距离向量<b><i>D</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>d</i><sub>1, <i>a</i></sub>, <i>d</i><sub>2, <i>a</i></sub>, …, <i>d</i><sub><i>n</i>, <i>a</i></sub>) , 其中<i>d</i><sub><i>i</i>, <i>a</i></sub>∈R<sup><i>n</i></sup>为单词<i>w</i><sub><i>i</i></sub>和属性<i>a</i>的语法距离.则句子<i>S</i>中各单词<i>w</i><sub><i>i</i></sub>的权重值<i>l</i><sub><i>i</i>, <i>a</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub></mrow><mrow><mn>2</mn><mi>d</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="106">其中, <i>d</i><sub>max</sub>为<b><i>D</i></b><sub><i>S</i>, <i>a</i></sub>中最大值.</p>
                </div>
                <div class="p1">
                    <p id="107">由式 (1) 中可知:位置权重<i>l</i><sub><i>i</i></sub>∈[0.5, 1]一方面保证句子中远离属性的单词的最小权重在一定范围内, 另一方面使得与属性距离更近的单词有着更高的权重, 从而防止模型在预测属性的情感极性时被与该属性无关的强烈情感词误导.</p>
                </div>
                <div class="p1">
                    <p id="108">根据每一个属性<i>a</i>可计算得到一个与句子<i>S</i>长度相同的基于语法距离的权重向量<b><i>L</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>l</i><sub>1, <i>a</i></sub>, <i>l</i><sub>2, <i>a</i></sub>, …, <i>l</i><sub><i>n</i>, <i>a</i></sub>) , 其中每一个元素<i>l</i><sub><i>i</i>, <i>a</i></sub>∈R (1≤<i>i</i>≤|<i>S</i>|) 为句子中对应位置的单词<i>w</i><sub><i>i</i></sub>在预测特定属性<i>a</i>时的位置权重.</p>
                </div>
                <h3 id="109" name="109" class="anchor-tag"><b>2 依存树及距离注意力模型DTDA</b></h3>
                <h4 class="anchor-tag" id="110" name="110"><b>2.1 任务定义</b></h4>
                <div class="p1">
                    <p id="111">假设句子<i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>a</i><sub><i>i</i></sub>, …, <i>a</i><sub><i>j</i></sub>, …, <i>w</i><sub><i>k</i></sub>) 为一个包含<i>k</i>个单词的序列, <i>w</i><sub><i>t</i></sub>表示<i>S</i>中第<i>t</i>个单词, <i>a</i><sub><i>i</i></sub>和<i>a</i><sub><i>j</i></sub>表示不同的目标属性词, <i>y</i>= (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>m</i></sub>) 为目标情感极性类别.句子属性情感分类任务的目标就是对给定的<i>S</i>及<i>a</i> (如<i>a</i><sub><i>i</i></sub>或<i>a</i><sub><i>j</i></sub>) , 判定<i>S</i>关于<i>a</i>的情感极性, 即计算<i>S</i>和<i>a</i>在<i>y</i>中各极性类别上的似然概率分布:</p>
                </div>
                <div class="p1">
                    <p id="112"><i>f</i>: (<i>S</i>, <i>a</i>) →<i>y</i>= (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>m</i></sub>) . (2) </p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>2.2 模型结构</b></h4>
                <div class="p1">
                    <p id="114">结合依存树及距离信息提出一种基于注意力的句子属性情感分类模型DTDA (dependency tree and distance attention) , 模型结构如图6所示:</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 DTDA模型的结构" src="Detail/GetImg?filename=images/JFYZ201908016_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 DTDA模型的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Structure of DTDA model</p>

                </div>
                <div class="p1">
                    <p id="116">DTDA共包含4层:</p>
                </div>
                <div class="p1">
                    <p id="117">1) 输入层.主要负责对句子<i>S</i>进行一些预处理, 如长度补齐、依存关系分析、计算相对距离及语法距离等, 并根据属性及句子的依存树抽取相应的属性依存子句.在此基础上, 利用预训练词向量表将句子<i>S</i>及属性子句<i>S</i><sub><i>a</i></sub>中的各个单词映射成为低维、连续和实数的向量表示.</p>
                </div>
                <div class="p1">
                    <p id="118">2) 编码层.利用双向GRU层分别对句子<i>S</i>和属性子句<i>S</i><sub><i>a</i></sub>进行建模, 并结合语法距离生成相应的句子上下文特征表示和属性的特征表示.</p>
                </div>
                <div class="p1">
                    <p id="119">3) 注意力层.根据句子的上下文特征和属性特征, 利用注意力机制为句子中的单词分配合适的注意力权重, 针对每一个属性生成特定的情感特征表示.</p>
                </div>
                <div class="p1">
                    <p id="120">4) 输出层.针对各个属性的特定情感特征利用Softmax层预测相应的情感极性并输出.</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>2.3 输入层</b></h4>
                <div class="p1">
                    <p id="122">输入层主要是对句子<i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>k</i></sub>) 进行预处理, 并将其转成为基于词向量的表示.具体包括:</p>
                </div>
                <div class="p1">
                    <p id="123">1) 属性的依存子树抽取.根据1.1节所述对句子<i>S</i>进行依存关系分析, 然后抽取属性<i>a</i>的依存子树<i>S</i><sub><i>a</i></sub>, 并线性化得到一个连续单词序列:</p>
                </div>
                <div class="p1">
                    <p id="124"><i>S</i><sub><i>a</i></sub>= (<i>w</i><sub><i>s</i>+1</sub>, <i>w</i><sub><i>s</i>+2</sub>, …, <i>w</i><sub><i>s</i>+<i>r</i></sub>) , </p>
                </div>
                <div class="p1">
                    <p id="125">0≤<i>s</i>&lt;<i>k</i>∧0≤<i>r</i>&lt;<i>k</i>∧<i>s</i>+<i>r</i>≤<i>k</i>. (3) </p>
                </div>
                <div class="p1">
                    <p id="126">显然, <i>S</i><sub><i>a</i></sub>为<i>S</i>的某个子序列, 即<i>S</i><sub><i>a</i></sub>∈<i>S</i>.下面统称该序列或属性的依存树为属性子句, 并用<i>S</i><sub><i>a</i></sub>表示.</p>
                </div>
                <div class="p1">
                    <p id="127">2) 长度预处理.为了便于模型的处理, 根据句子和属性子句的长度分别进行相应的补齐或截断处理.以句子<i>S</i>为例, 假设句子长度阈值为<i>n</i> (<i>n</i>通常取训练语料库中的最大句子长度或某一具体值) , 对于长度超过<i>n</i>的句子, 只截取前<i>n</i>个词;对于长度少于<i>n</i>的句子, 用特定标志 (如&lt;PAD/&gt;) 进行补齐.例如, 对句子<i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>k</i></sub>) 的长度预处理操作如下:</p>
                </div>
                <div class="area_img" id="128">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908016_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="130">同理, 对属性子句<i>S</i><sub><i>a</i></sub>进行相应的长度预处理.后面若无特别说明, <i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>n</i></sub>) 和<i>S</i><sub><i>a</i></sub>= (<i>w</i><sub><i>s</i>+1</sub>, <i>w</i><sub><i>s</i>+2</sub>, …, <i>w</i><sub><i>s</i>+<i>r</i></sub>) 均表示已进行长度预处理, 其中<i>r</i>为<i>S</i><sub><i>a</i></sub>的长度阈值.</p>
                </div>
                <div class="p1">
                    <p id="131">3) 词向量表示.分别将预处理后的<i>S</i>和<i>S</i><sub><i>a</i></sub>中的每一个词<i>w</i><sub><i>i</i></sub> (1≤<i>i</i>≤<i>n</i>或<i>s</i>+1≤<i>i</i>≤<i>s</i>+<i>r</i>) 映射为一个连续且带语义信息的低维稠密实数向量.</p>
                </div>
                <div class="p1">
                    <p id="132">假设<b><i>N</i></b>∈R<sup>|<i>V</i>|×<i>d</i></sup>为一个预训练或随机初始化的实数词向量表, 其中|<b><i>N</i></b>|表示词汇表<b><i>N</i></b>的大小, <i>d</i>为词向量的维度 (如50维) .对于句子<i>S</i>= (<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>i</i></sub>, <i>w</i><sub><i>i</i>+1</sub>, …, <i>w</i><sub><i>n</i></sub>) , <b><i>N</i></b> (<i>w</i><sub><i>i</i></sub>) ∈R<sup>1×|<b><i>N</i></b>|</sup>为单词<i>w</i><sub><i>i</i></sub>在<b><i>N</i></b>中的位置向量, 即对应<i>w</i><sub><i>i</i></sub>的位置为1, 其他位置均为0.利用<b><i>N</i></b>和矩阵向量积可将每一个词<i>w</i><sub><i>i</i></sub>转化为<b><i>N</i></b>中的一个1×<i>d</i>维向量<b><i>w</i></b><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="134"><b><i>w</i></b><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></math></mathml>=<i>N</i> (<i>w</i><sub><i>i</i></sub>) <b><i>N</i></b>. (5) </p>
                </div>
                <div class="p1">
                    <p id="136">由式 (5) 可将每一个句子<i>S</i>转换成一个<i>n</i>×<i>d</i>维的实数向量矩阵表示<b><i>M</i></b><sub><i>S</i></sub>= (<i>w</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>d</mi></msubsup></mrow></math></mathml>, <i>w</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>d</mi></msubsup></mrow></math></mathml>, …, <i>w</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>d</mi></msubsup></mrow></math></mathml>) , 将每一个属性子句<i>S</i><sub><i>a</i></sub>转换成一个<i>r</i>×<i>d</i>维的实数向量矩阵表示<b><i>M</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>w</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mo>+</mo><mn>1</mn></mrow><mi>d</mi></msubsup></mrow></math></mathml>, <i>w</i><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mo>+</mo><mn>2</mn></mrow><mi>d</mi></msubsup></mrow></math></mathml>, …, <i>w</i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mo>+</mo><mi>r</mi></mrow><mi>d</mi></msubsup></mrow></math></mathml>) .若单词在<b><i>N</i></b>中不存在, 则利用区间[-0.25, 0.25]上的正态分布进行随机初始化.</p>
                </div>
                <div class="p1">
                    <p id="143">对于<i>S</i>, 针对每一个属性<i>a</i>可得到相应的相对距离向量<b><i>C</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>c</i><sub>1, <i>a</i></sub>, <i>c</i><sub>2, <i>a</i></sub>, …, <i>c</i><sub><i>n</i>, <i>a</i></sub>) .用矩阵<b><i>C</i></b>存储数据集中所有句子的位置取值, 并将所有位置值映射为一个多维向量 (<i>c</i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>d</mi></msubsup></mrow></math></mathml>, <i>c</i><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>d</mi></msubsup></mrow></math></mathml>, …, <i>c</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>d</mi></msubsup></mrow></math></mathml>) ∈R<sup><i>n</i>×<i>d</i></sup>, 然后计算句子<i>S</i>的最终表示<b><i>X</i></b><sub><i>S</i></sub>= (<i>x</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>d</mi></msubsup></mrow></math></mathml>, <i>x</i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>d</mi></msubsup></mrow></math></mathml>, …, <i>x</i><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></math></mathml>, <i>x</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>d</mi></msubsup></mrow></math></mathml>, <i>x</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>2</mn></mrow><mi>d</mi></msubsup></mrow></math></mathml>, …, <i>x</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>d</mi></msubsup></mrow></math></mathml>) , 其中:</p>
                </div>
                <div class="p1">
                    <p id="153"><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo>=</mo><mfrac><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo>+</mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mn>2</mn></mfrac></mrow></math></mathml>. (6) </p>
                </div>
                <div class="p1">
                    <p id="155">即句子<i>S</i>的特征表示<b><i>X</i></b><sub><i>S</i></sub>包含了单词的语义信息及位置信息.</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156"><b>2.4 编码层</b></h4>
                <div class="p1">
                    <p id="157">编码层的主要任务是利用BGRU分别学习句子及属性子句的上下文信息, 并得到相应的特征表示.BGRU的结构如图7所示:</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 BGRU的结构" src="Detail/GetImg?filename=images/JFYZ201908016_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 BGRU的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Structure of BGRU</p>

                </div>
                <div class="p1">
                    <p id="159">对于BGRU, 设任意第<i>t</i>个时间步的正向输出为<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>, 逆向输出为<mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover></mrow></math></mathml>, GRU的输出维度为<i>d</i><sub><i>e</i></sub>, 则有</p>
                </div>
                <div class="p1">
                    <p id="162"><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover><mo>=</mo><mover accent="true"><mrow><mtext>G</mtext><mtext>R</mtext><mtext>U</mtext></mrow><mo stretchy="true">→</mo></mover><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>, </mo><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">→</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml>, (7) </p>
                </div>
                <div class="p1">
                    <p id="164"><mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover><mo>=</mo><mover accent="true"><mrow><mtext>G</mtext><mtext>R</mtext><mtext>U</mtext></mrow><mo stretchy="true">←</mo></mover><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>, </mo><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">←</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml>, (8) </p>
                </div>
                <div class="p1">
                    <p id="166">其中, <mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></msup></mrow></math></mathml>和<mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>e</mi></msub></mrow></msup></mrow></math></mathml>分别是前向GRU和逆向GRU在时刻<i>t</i>的隐藏状态的输出结果.将<mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>和<mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover></mrow></math></mathml>串联并作为BGRU在时刻<i>t</i>的隐藏状态输出结果<b><i>h</i></b><sub><i>t</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="171"><mathml id="172"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>;<mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover><mo stretchy="false">]</mo></mrow></math></mathml>. (9) </p>
                </div>
                <div class="p1">
                    <p id="174">分别利用2个独立的<b>BGRU</b>对句子<b><i>S</i></b>和属性子句<b><i>S</i></b><sub><b><i>a</i></b></sub>进行编码, 并将包含所有时间步输出的隐藏状态矩阵<b><i>H</i></b><sub><i>S</i></sub>和<b><i>H</i></b><sub><i>S</i>, <i>a</i></sub>分别记为</p>
                </div>
                <div class="p1">
                    <p id="175"><b><i>H</i></b><sub><i>S</i></sub>=BGRU (<b><i>X</i></b><sub><i>S</i></sub>) , (10) </p>
                </div>
                <div class="p1">
                    <p id="176"><b><i>H</i></b><sub><i>S</i>, <i>a</i></sub>=BGRU (<b><i>X</i></b><sub><i>S</i>, <i>a</i></sub>) , (11) </p>
                </div>
                <div class="p1">
                    <p id="177">其中, <b><i>H</i></b><sub><i>S</i></sub>= (<b><i>h</i></b><sub>1</sub>, <b><i>h</i></b><sub>2</sub>, …, <b><i>h</i></b><sub><i>n</i></sub>) ∈R<sup><i>n</i>×2<i>d</i><sub><i>e</i></sub></sup>为句子<b><i>X</i></b><sub><i>S</i></sub>经BGRU编码后的上下文特征表示, <b><i>H</i></b><sub><i>S</i>, <i>a</i></sub>= (<b><i>h</i></b><sub><i>s</i>+1</sub>, <b><i>h</i></b><sub><i>s</i>+2</sub>, …, <b><i>h</i></b><sub><i>s</i>+<i>r</i></sub>) ∈R<sup><i>r</i>×2<i>d</i><sub><i>e</i></sub></sup>为属性子句<i>S</i><sub><i>a</i></sub>经BGRU编码后的特征表示.</p>
                </div>
                <div class="p1">
                    <p id="178">为了进一步区分句子上下文信息对不同属性的影响, 并降低句子中靠近属性词的无关单词对目标极性的影响, 下面引入属性的语法距离对句子的上下文进行加权, 从而构造属性相关的上下文特征.</p>
                </div>
                <div class="p1">
                    <p id="179">对于句子<i>S</i>及属性<i>a</i>, 由1.2节可得到语法距离向量<b><i>D</i></b><sub><i>S</i>, <i>a</i></sub>和位置权重向量<b><i>L</i></b><sub><i>S</i>, <i>a</i></sub>.将<b><i>L</i></b><sub><i>S</i>, <i>a</i></sub>与<b><i>H</i></b><sub><i>S</i></sub>相乘得到一个根据属性定制的隐藏状态矩阵<b><i>E</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>e</i><sub>1</sub>, <i>e</i><sub>2</sub>, …, <i>e</i><sub><i>i</i></sub>, <i>e</i><sub><i>i</i>+1</sub>, …, <i>e</i><sub><i>n</i></sub>) ∈R<sup><i>n</i>×2<i>d</i><sub><i>e</i></sub></sup>, 其中<i>e</i><sub><i>i</i></sub>=<i>l</i><sub><i>i</i>, <i>a</i></sub>×<b><i>h</i></b><sub><i>i</i></sub>∈R<sup>2<i>d</i><sub><i>e</i></sub></sup>.与<b><i>H</i></b><sub><i>S</i></sub>相比, 利用属性相关的位置权重<b><i>L</i></b><sub><i>S</i>, <i>a</i></sub>进行加权后的<b><i>E</i></b><sub><i>S</i>, <i>a</i></sub>进一步包含了单词和属性<i>a</i>间的语法距离位置信息.</p>
                </div>
                <h4 class="anchor-tag" id="180" name="180"><b>2.5 注意力层</b></h4>
                <div class="p1">
                    <p id="181">注意力层的任务是根据属性和句子的上下文信息, 利用注意力机制为句子中的每一个单词分配合适的注意力权重, 并生成属性相关的情感特征表示.</p>
                </div>
                <div class="p1">
                    <p id="182">具体来说, 对于属性的特征表示<b><i>H</i></b><sub><i>S</i>, <i>a</i></sub>和隐藏状态矩阵<b><i>E</i></b><sub><i>S</i>, <i>a</i></sub>, 注意力计算为</p>
                </div>
                <div class="p1">
                    <p id="183"><b><i>Q</i></b>=<i>relu</i> (<b><i>H</i></b><sub><i>S</i>, <i>a</i></sub><b><i>W</i></b><sub>1</sub>+<b><i>b</i></b><sub>1</sub>) , (12) </p>
                </div>
                <div class="p1">
                    <p id="184"><b><i>K</i></b>=<i>relu</i> (<b><i>E</i></b><sub><i>S</i>, <i>a</i></sub><b><i>W</i></b><sub>2</sub>+<b><i>b</i></b><sub>2</sub>) , (13) </p>
                </div>
                <div class="p1">
                    <p id="185"><mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mi>Κ</mi><mi>Q</mi><msup><mrow></mrow><mi>Τ</mi></msup></mrow><mrow><msqrt><mrow><mi>d</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></msqrt></mrow></mfrac></mrow></math></mathml>, (14) </p>
                </div>
                <div class="p1">
                    <p id="187">其中, <b><i>W</i></b><sub>1</sub>∈R<sup>2<i>d</i><sub><i>e</i></sub>×<i>d</i><sub><i>a</i></sub></sup>和<b><i>W</i></b><sub>2</sub>∈R<sup>2<i>d</i><sub><i>e</i></sub>×<i>d</i><sub><i>a</i></sub></sup>为权值矩阵, 通过线性变化将<b><i>H</i></b><sub><i>S</i>, <i>a</i></sub>和<b><i>E</i></b><sub><i>S</i>, <i>a</i></sub>中的维度由2<i>d</i><sub><i>e</i></sub>转为<i>d</i><sub><i>a</i></sub>, 并利用非线性激活函数<i>relu</i> (rectified linear unit) 将值约束为非负值.<b><i>b</i></b><sub>1</sub>∈R<sup><i>d</i><sub><i>a</i></sub></sup>和<b><i>b</i></b><sub>2</sub>∈R<sup><i>d</i><sub><i>a</i></sub></sup>为偏置值向量.<b><i>Score</i></b>∈R<sup><i>n</i>×<i>r</i></sup>是<b><i>H</i></b><sub><i>S</i>, <i>a</i></sub>和<b><i>E</i></b><sub><i>S</i>, <i>a</i></sub>的注意力权值矩阵, <b><i>Score</i></b><sub><i>i</i>, <i>j</i></sub>表示<i>S</i>中第<i>i</i>个单词和属性子句<i>S</i><sub><i>a</i></sub>中第<i>j</i>个单词间的注意力权值.<mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>/</mo><msqrt><mrow><mi>d</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></msqrt></mrow></math></mathml>为缩放因子, 目的是加快模型的训练.</p>
                </div>
                <div class="p1">
                    <p id="189">属性和句子间的注意力权重由属性子句中所有单词共同决定, 因此需合并属性子句中各个单词与句子所有单词的注意力权值, 并构成最终的注意力权值向量<b><i>g</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>g</i><sub>1, <i>a</i></sub>, <i>g</i><sub>2, <i>a</i></sub>, …, <i>g</i><sub><i>n</i>, <i>a</i></sub>) ∈R<sup><i>n</i></sup>.下面与文献<citation id="431" type="reference">[<a class="sup">17</a>]</citation>类似, 定义2种不同的合并方式:求和sum与取平均值mean.</p>
                </div>
                <div class="p1">
                    <p id="190" class="code-formula">
                        <mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover><mi>S</mi></mstyle><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>, </mo><mi>j</mi></mrow></msub><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mo>=</mo><mtext>s</mtext><mtext>u</mtext><mtext>m</mtext><mo>;</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mi>r</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>r</mi></munderover><mi>S</mi></mstyle><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>, </mo><mi>j</mi></mrow></msub><mo>, </mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mo>=</mo><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo>.</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="191">利用<i>softmax</i>函数对向量<b><i>g</i></b><sub><i>S</i>, <i>a</i></sub>进行归一化, 可得到最终的注意力向量<b><i>q</i></b><sub><i>S</i>, <i>a</i></sub>= (<b><i>q</i></b><sub>1, <i>a</i></sub>, <b><i>q</i></b><sub>2, <i>a</i></sub>, …, <b><i>q</i></b><sub><i>n</i>, <i>a</i></sub>) ∈R<sup><i>n</i></sup>, 其中, </p>
                </div>
                <div class="p1">
                    <p id="192" class="code-formula">
                        <mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>×</mo><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>×</mo><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>8</mn></mrow></msup></mrow></mfrac><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="193">为避免模型训练过程中注意力权重集中分布在补齐标志&lt;PAD/&gt;上, 在对<b><i>g</i></b><sub><i>i</i>, <i>a</i></sub>归一化时利用<i>n</i>维向量 <b><i>mask</i></b>= (<i>mask</i><sub><i>i</i></sub>) <sub>1≤<i>i</i>≤<i>n</i></sub>对<b><i>g</i></b><sub><i>i</i>, <i>a</i></sub>中的&lt;PAD/&gt;进行屏蔽.<i>w</i><sub><i>i</i></sub>为一般的单词时<i>mask</i><sub><i>i</i></sub>=1, <i>w</i><sub><i>i</i></sub>为&lt;PAD/&gt;时<i>mask</i><sub><i>i</i></sub>=-100.同时, 为避免归一化过程中出现分母全为0而导致计算溢出, 将分母加一个非常小的值10<sup>-8</sup>.</p>
                </div>
                <div class="p1">
                    <p id="194">由注意力向量<b><i>q</i></b><sub><i>S</i>, <i>a</i></sub>计算属性相关的情感特征<b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="195"><b><i>V</i></b>=<b><i>H</i></b><sub><i>S</i></sub>+<b><i>X</i></b><sub><i>S</i></sub><i>W</i><sub>3</sub>, (17) </p>
                </div>
                <div class="p1">
                    <p id="196"><b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>=<b><i>q</i></b><sub><i>S</i>, <i>a</i></sub><b><i>V</i></b>, (18) </p>
                </div>
                <div class="area_img" id="197">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_197.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 属性相关情感特征计算过程" src="Detail/GetImg?filename=images/JFYZ201908016_197.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 属性相关情感特征计算过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_197.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Computation process of aspect-related  sentiment feature</p>

                </div>
                <div class="p1">
                    <p id="198">其中, <b><i>W</i></b><sub>3</sub>∈R<sup><i>d</i>×2<i>d</i><sub><i>e</i></sub></sup>为用于进行线性变换的二维权重矩阵, 将<b><i>X</i></b><sub><i>S</i></sub>中的维度<i>d</i>变换为2<i>d</i><sub><i>e</i></sub>.<b><i>V</i></b>∈R<sup><i>n</i>×2<i>d</i><sub><i>e</i></sub></sup>可看成是融合了单词的词向量语义信息、相对距离信息及句子的上下文信息.<b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>∈R<sup>2<i>d</i><sub><i>e</i></sub></sup>表示属性<i>a</i>相关的情感特征, 即结合注意力机制融合了单词的语义信息、属性相关的句子上下文信息、相对距离及语法距离信息.<b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>的计算过程如图8所示:</p>
                </div>
                <h4 class="anchor-tag" id="199" name="199"><b>2.6 输出层</b></h4>
                <div class="p1">
                    <p id="200">输出层的主要任务是根据句子的上下文<b><i>H</i></b><sub><i>S</i></sub>及属性相关的情感特征<b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>预测目标属性的情感极性 (计算过程如图9所示) .<i>S</i>的特征表示<b><i>r</i></b><sub><i>s</i></sub>由<b><i>H</i></b><sub><i>S</i></sub>的最后一个时间步的隐藏状态<mathml id="201"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>和<mathml id="202"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo stretchy="true">←</mo></mover></mrow></math></mathml>构成, 然后利用非线性变换将其和属性相关的情感特征<b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>进行融合并用于预测最终的情感极性:</p>
                </div>
                <div class="p1">
                    <p id="203"><mathml id="204"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>S</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>;<mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo stretchy="true">←</mo></mover><mo stretchy="false">]</mo></mrow></math></mathml>, (19) </p>
                </div>
                <div class="p1">
                    <p id="206"><b><i>h</i></b><sup>*</sup>=<i>relu</i> (<b><i>W</i></b><sub>4</sub><b><i>r</i></b><sub><i>S</i>, <i>a</i></sub>+<b><i>W</i></b><sub>5</sub><b><i>r</i></b><sub><i>S</i></sub>+<b><i>b</i></b><sub>3</sub>) , (20) </p>
                </div>
                <div class="p1">
                    <p id="207">其中, <b><i>W</i></b><sub>4</sub>∈R<sup><i>d</i><sub><i>r</i></sub>×2<i>d</i><sub><i>e</i></sub></sup>和<b><i>W</i></b><sub>5</sub>∈R<sup><i>d</i><sub><i>r</i></sub>×2<i>d</i><sub><i>e</i></sub></sup>为权值矩阵, <b><i>b</i></b><sub>3</sub>∈R<sup><i>d</i><sub><i>r</i></sub></sup>为偏置值向量.</p>
                </div>
                <div class="area_img" id="208">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 输出层的计算过程" src="Detail/GetImg?filename=images/JFYZ201908016_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 输出层的计算过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Computation process of output layer</p>

                </div>
                <div class="p1">
                    <p id="209"><b><i>h</i></b><sup>*</sup>∈R<sup><i>d</i><sub><i>r</i></sub></sup>为句子最终的特征表示, 并通过<i>softmax</i>函数计算<b><i>h</i></b><sup>*</sup>在<i>y</i>中各个极性类别的概率分布:</p>
                </div>
                <div class="p1">
                    <p id="210"><b><i>p</i></b><sub><i>S</i>, <i>a</i></sub>=<i>softmax</i> (<b><i>W</i></b><sub>6</sub><b><i>h</i></b><sup>*</sup>+<b><i>b</i></b><sub>4</sub>) , (21) </p>
                </div>
                <div class="p1">
                    <p id="211">其中, <b><i>W</i></b><sub>6</sub>∈R<sup><i>m</i>×<i>d</i><sub><i>r</i></sub></sup>和<b><i>b</i></b><sub>4</sub>∈R<sup><i>m</i></sup>分别为<i>softmax</i>的权值矩阵和偏置值向量.<b><i>p</i></b><sub><i>S</i>, <i>a</i></sub>= (<i>p</i><sub>1, <i>a</i></sub>, <i>p</i><sub>2, <i>a</i></sub>, …, <i>p</i><sub><i>m</i>, <i>a</i></sub>) 为模型在<i>y</i>中各个极性类别上预测值的归一化结果.</p>
                </div>
                <div class="p1">
                    <p id="212">最后, 取该<i>m</i>维向量中的最大值max (<b><i>p</i></b><sub><i>S</i>, <i>a</i></sub>) 所对应的标签作为模型的最终预测结果, 即:</p>
                </div>
                <div class="p1">
                    <p id="213"><i>p</i>=max (<b><i>p</i></b><sub><i>S</i>, <i>a</i></sub>) . (22) </p>
                </div>
                <h3 id="214" name="214" class="anchor-tag"><b>3 模型训练</b></h3>
                <div class="p1">
                    <p id="215">假设<i>p</i>为模型对输入<i>S</i>和<i>a</i>的预测结果标签值, <i>y</i><sub><i>S</i>, <i>a</i></sub>是一个正确情感类别的元素为1而其他类别的元素为0的one-hot向量.DTDA采用交叉熵 (cross entropy) 作为优化的损失函数, 同时为避免模型在训练中出现过拟合 (overfitting) , 采用<i>L</i><sub>2</sub>范数正则化</p>
                </div>
                <div class="p1">
                    <p id="216"><mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mrow><mi>S</mi><mo>, </mo><mi>a</mi></mrow></msub><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mtext> </mtext><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>a</mi></mrow></msub><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi>θ</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>. (23) </p>
                </div>
                <div class="p1">
                    <p id="218">模型通过迭代求解损失值和梯度下降来优化模型, 使得损失函数的值最小.</p>
                </div>
                <div class="p1">
                    <p id="219">对DTDA的评估采用准确率 (<i>Accuracy</i>) 、精确率<i>P</i> (<i>Precision</i>) 和宏平均<i>F</i>1值 (macro average <i>F</i>1, <i>MF</i>1) 等评价指标, 其中<i>MF</i>1是指所有类别的<i>F</i>1值平均.对单个类别, 设<i>TP</i>是正确预测的样本, <i>FP</i>是其他类别被判定为当前类别的样本, <i>FN</i>是当前类别被错误判定为其他类别的样本, 则精确率<i>P</i>, 召回率 (<i>Recall</i>) 和<i>F</i>1值计算公式为</p>
                </div>
                <div class="p1">
                    <p id="220"><mathml id="221"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mrow></math></mathml>, (24) </p>
                </div>
                <div class="p1">
                    <p id="222"><mathml id="223"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mrow></math></mathml>, (25) </p>
                </div>
                <div class="p1">
                    <p id="224"><mathml id="225"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><mo>×</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></math></mathml>. (26) </p>
                </div>
                <div class="p1">
                    <p id="226">准确率<i>Accuracy</i>和<i>MF</i>1分别定义为</p>
                </div>
                <div class="p1">
                    <p id="227"><i>Accuracy</i>=<mathml id="228"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>Τ</mi></mstyle><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>Τ</mi><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>F</mi><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>F</mi><mi>Ν</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (27) </p>
                </div>
                <div class="p1">
                    <p id="229"><mathml id="230"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>F</mi></mstyle><mn>1</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>. (28) </p>
                </div>
                <div class="p1">
                    <p id="231">模型采用Adam (adaptive moment estimation) <citation id="432" type="reference"><link href="398" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>优化器, 并设其学习率为0.001, 学习率衰减为0.000 4.为防止过拟合, 将dropout<citation id="433" type="reference"><link href="400" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>正则化策略应用于在BGRU层, dropout的值设置0.5.此外, 训练过程中对BGRU层的输出隐藏状态矩阵进行批规范化 (batch normalization) <citation id="434" type="reference"><link href="402" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>.</p>
                </div>
                <h3 id="232" name="232" class="anchor-tag"><b>4 实验及分析</b></h3>
                <h4 class="anchor-tag" id="233" name="233"><b>4.1 实验准备</b></h4>
                <div class="p1">
                    <p id="234">为验证DSDA的有效性, 采用2014年国际语义评测SemEval任务4<citation id="435" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>中2个著名的公开数据集进行实验对比:</p>
                </div>
                <div class="p1">
                    <p id="235">1) Laptop.针对笔记本电脑的文本评论数据集.</p>
                </div>
                <div class="p1">
                    <p id="236">2) Restaurant.针对餐馆的文本评论数据集.</p>
                </div>
                <div class="p1">
                    <p id="237">属性的情感极性主要包含:积极的 (Pos) 、中性的 (Neu) 和消极的 (Neg) , 并有少量情感极性为冲突的 (Conflict) .为避免干扰, 参照文献<citation id="436" type="reference">[<a class="sup">14</a>]</citation>的做法去掉极性为冲突的属性.最终各情感极性对应的句子数量分布如表1所示.</p>
                </div>
                <div class="p1">
                    <p id="238">为了更充分地对比各模型的效果, 参照文献<citation id="437" type="reference">[<a class="sup">9</a>]</citation>和<citation id="438" type="reference">[<a class="sup">17</a>]</citation>, 根据句子中属性的数量和情感极性类别将数据集划分为3种不同判定难度的子集S1, S2和S3, 如表2所示.</p>
                </div>
                <div class="p1">
                    <p id="239">1) S1.同一个句子中只包含有1个属性.</p>
                </div>
                <div class="p1">
                    <p id="240">2) S2.同一个句子包含多个属性, 且均具有相同的情感极性.</p>
                </div>
                <div class="p1">
                    <p id="241">3) S3.同一个句子中包含2个或者2个以上的属性, 且具有不同的情感极性.</p>
                </div>
                <div class="area_img" id="242">
                    <p class="img_tit"><b>表1 Laptop和Restaurant数据集统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Statistic of Laptop and Restaurant Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="242" border="1"><tr><td><br />Dataset</td><td>Positive</td><td>Neutral</td><td>Negative</td></tr><tr><td><br />Laptop-train</td><td>987</td><td>460</td><td>866</td></tr><tr><td><br />Laptop-test</td><td>341</td><td>169</td><td>128</td></tr><tr><td><br />Restaurant-train</td><td>2 164</td><td>633</td><td>805</td></tr><tr><td><br />Restaurant-test</td><td>728</td><td>196</td><td>196</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="243">
                    <p class="img_tit"><b>表2 2个数据集的子集划分结果统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Division Statistics About Subsets of Two Datasets</b></p>
                    <p class="img_note"></p>
                    <table id="243" border="1"><tr><td><br />Dataset</td><td>S1</td><td>S2</td><td>S3</td></tr><tr><td><br />Laptop-train</td><td>906</td><td>952</td><td>455</td></tr><tr><td><br />Laptop-test</td><td>257</td><td>278</td><td>103</td></tr><tr><td><br />Restaurant-train</td><td>983</td><td>1 679</td><td>940</td></tr><tr><td><br />Restaurant-test</td><td>284</td><td>608</td><td>228</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="244">DSDA模型在Laptop和Restaurant上均采用相同的超参数设置, 如表3所示, 并取训练集中的12%的数据进行验证.</p>
                </div>
                <div class="area_img" id="245">
                    <p class="img_tit"><b>表3 DSDA模型的超参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Hyper Parameter Setting for DSDA Model</b></p>
                    <p class="img_note"></p>
                    <table id="245" border="1"><tr><td><br />Parameter</td><td>Value</td><td>Description</td></tr><tr><td><br /><i>n</i></td><td>45</td><td>Length Threshold for Sentence</td></tr><tr><td><br /><i>r</i></td><td>6</td><td>Length Threshold for Aspect Clause</td></tr><tr><td><br /><i>d</i><sub><i>e</i></sub></td><td>120</td><td>Output Dimension for BGRU</td></tr><tr><td><br /><i>d</i><sub><i>a</i></sub></td><td>100</td><td>Dimension of <i>Q</i> and <i>K</i></td></tr><tr><td><br /><i>d</i><sub><i>r</i></sub></td><td>96</td><td>Dimension for Output Layer</td></tr><tr><td><br /><i>λ</i></td><td>0.012</td><td>Weight for <i>L</i><sub>2</sub> Normalization Term</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="246">为便于比较, 采用公开的300维Glove词向量<citation id="439" type="reference"><link href="404" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>.同时, 针对Laptop采用电子产品评论数据集Amazon electronics<citation id="440" type="reference"><link href="406" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>作为词向量的训练语料, 针对Restaurant采用Yelp开放数据集 (https://www.yelp.com/dataset) 作为词向量的训练语料.其中前者共包含约782万条评论, 单词总量为7.7亿.后者共包含约470万条评论, 单词总量为6.4亿.具体训练参数如表4所示:</p>
                </div>
                <div class="area_img" id="247">
                    <p class="img_tit"><b>表4 词向量训练参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Parameter Setting for Word Vector Training</b></p>
                    <p class="img_note"></p>
                    <table id="247" border="1"><tr><td><br />Word Vector</td><td>Trainable Parameter</td><td>Other Parameter</td></tr><tr><td><br />Word2vec-skipgram</td><td rowspan="2"><i>d</i>=300<br />No. of Minimal<br />Occurrence of<br />Word: 100<br />Window Size:8</td><td><br />No. of Negative<br />Sample:3<br />Iteration:3</td></tr><tr><td><br />Glove</td><td><br /><i>x</i><sub>max</sub>:200<br /><i>a</i>:0.75<br />Iteration:20</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="248" name="248"><b>4.2 实验1:与相关工作的比较</b></h4>
                <div class="p1">
                    <p id="249">本实验的主要目的是在公开的Glove词向量的基础上对DSDA与当前部分主流方法进行对比, 特别是部分基于注意力和记忆网络的state-of-the-art方法.此外, 还选取基于属性位置信息的TD-LSTM模型以及笔者前期的工作DSAN<citation id="441" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>一起作为比较的基准方法.各基准方法对应的模型简介如下:</p>
                </div>
                <div class="p1">
                    <p id="250">1) TD-LSTM<citation id="442" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.结合句子中属性的位置将句子分为左右2部分, 然后分别利用前向LSTM和后向LSTM进行建模, 最后将LSTM最后一个时间步的隐藏状态串联作为句子特征, 并预测情感极性.</p>
                </div>
                <div class="p1">
                    <p id="251">2) MemNet<citation id="443" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>.基于记忆网络MemNet及多层注意力机制的情感极性分类方法.</p>
                </div>
                <div class="p1">
                    <p id="252">3) RAM<citation id="444" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.基于堆叠双向LSTM的外部记忆矩阵和GRU网络的循环注意力机制, 并通过非线性组合构成属性相关的句子情感特征表示.</p>
                </div>
                <div class="p1">
                    <p id="253">4) ATAE-LSTM<citation id="445" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>.将属性的特征表示和单词的词向量串联作为LSTM的输入, 在此基础上利用一个简单的注意力机制进行学习并构造句子的表示.</p>
                </div>
                <div class="p1">
                    <p id="254">5) IAN<citation id="446" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>.通过LSTM对句子和属性分别建模, 然后采用交互式注意力分别学习句子和属性中的重要特征, 并将结果串联用于预测情感极性.</p>
                </div>
                <div class="p1">
                    <p id="255">6) DSAN<citation id="447" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>.利用依存树得到属性与句子中各单词的句法距离, 并结合注意力机制生成属性相关的句子特征表示.</p>
                </div>
                <div class="p1">
                    <p id="256">为了便于与前期工作DSAN进行对比, DTDA同样采用sum和mean两个注意力合并方式, 并分别记为DTDA-sum和DTDA-mean.各模型在2种公开的300维Glove词向量Glove 1.9M和Glove 2.2M上的实验结果如表5所示.Glove 1.9M使用的词典大小为1.9M, 单词非大小写敏感.Glove 2.2 M使用的词典大小为2.2 M, 单词大小写敏感.TD-LSTM, MemNet和RAM的数据来自文献<citation id="448" type="reference">[<a class="sup">15</a>]</citation>, ATAE-LSTM和IAN的数据来自文献<citation id="449" type="reference">[<a class="sup">16</a>]</citation>, 而DSAN的数据来自文献<citation id="450" type="reference">[<a class="sup">17</a>]</citation>.表中各列最好的结果用黑体表示.</p>
                </div>
                <div class="area_img" id="257">
                    <p class="img_tit"><b>表5 各模型的实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Experiment Result Comparison for Various Models</b></p>
                    <p class="img_note"></p>
                    <table id="257" border="1"><tr><td rowspan="2"><br />Word<br />Vector</td><td rowspan="2">Model</td><td colspan="2"><br />Laptop</td><td colspan="2">Restaurant</td></tr><tr><td><br /><i>P</i></td><td><i>MF</i>1</td><td><i>P</i></td><td><i>MF</i>1</td></tr><tr><td rowspan="7"><br />Glove<br />1.9M</td><td><br />TD-LSTM</td><td>0.718 3</td><td>0.684 3</td><td>0.780 0</td><td>0.667 3</td></tr><tr><td><br />MemNet</td><td>0.703 3</td><td>0.640 9</td><td>0.781 6</td><td>0.658 3</td></tr><tr><td><br />RAM</td><td><b>0.744</b><b>9</b></td><td><b>0.713</b><b>5</b></td><td>0.802 3</td><td>0.708 0</td></tr><tr><td><br />DSAN-sum</td><td>0.727 3</td><td>0.687 8</td><td>0.807 1</td><td>0.723 8</td></tr><tr><td><br />DSAN-mean</td><td>0.738 2</td><td>0.700 1</td><td>0.808 0</td><td>0.727 3</td></tr><tr><td><br />DTDA-sum</td><td>0.731 2</td><td>0.692 3</td><td>0.807 5</td><td>0.723 9</td></tr><tr><td><br />DTDA-mean</td><td>0.740 1</td><td>0.701 9</td><td><b>0.808</b><b>1</b></td><td><b>0.727</b><b>9</b></td></tr><tr><td rowspan="6"><br />Glove<br />2.2M</td><td><br />ATAE-LSTM</td><td>0.687 0</td><td></td><td>0.772 0</td><td></td></tr><tr><td><br />IAN</td><td>0.7210</td><td></td><td>0.786 0</td><td></td></tr><tr><td><br />DSAN-sum</td><td>0.738 2</td><td>0.696 1</td><td><b>0.800</b><b>9</b></td><td>0.710 9</td></tr><tr><td><br />DSAN-mean</td><td>0.746 1</td><td>0.705 2</td><td>0.796 4</td><td>0.713 6</td></tr><tr><td><br />DTDA-sum</td><td>0.740 2</td><td>0.696 2</td><td><b>0.800</b><b>9</b></td><td>0.727 1</td></tr><tr><td><br />DTDA-mean</td><td><b>0.7471</b></td><td><b>0.708</b><b>9</b></td><td>0.800 8</td><td><b>0.728</b><b>1</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="258">从表5可知:</p>
                </div>
                <div class="p1">
                    <p id="259">1) DTDA在Laptop和Restaurant上均取得与RAM及DSAN相当的结果, 并超过其他4种方法.RAM是当前已知的最好属性级别情感分类方法.与RAM相比, DTDA的2种合并方式在Restaurant上均取得更好的结果, 但在Laptop上DTDA的表现略低于RAM, 主要的原因是RAM采用多层注意力, 而DTDA只采用单层注意力.而且相对于Restaurant来说, Laptop中的大部分句子较长, 而多层注意力机制能够更好地学习到长距离的关系.从模型的结构来看, RAM以属性词的词向量作为其特征表示, 然后利用GRU通过多层注意力逐层提取句子的不同特征, 并将各层的结果进行非线性组合.DTDA和DSAN均利用依存树和GRU生成属性的上下文特征, 并融合单词与属性的距离信息, 因此能够生成更加准确的句子特征表示.虽然RAM, DSAN和DTDA均采用了距离信息, 但RAM基于相对距离, DSAN采用语法距离, 而DTDA进一步将两者结合.从实验的结果来看, DTDA均比DSAN有一定的提升, 说明将更加全面的距离信息能够更好地刻画单词与属性间的关系.</p>
                </div>
                <div class="p1">
                    <p id="260">2) 在第1组实验中, DTDA-sum和DTDA-mean在2个数据集上的结果均超过ATAE-LSTM, IAN和DSAN.从属性的特征构造来看, ATAE-LSTM以属性的词向量作为属性特征表示, IAN利用LSTM对属性建模, 而DTDA和DSAN均使用BGRU对属性和属性的修饰信息进行建模.实验结果一方面说明属性表征表示的重要性, 另一方面也说明结合距离信息及注意力机制能更好地构造属性相关的情感特征信息, 并最终提升模型的效果.</p>
                </div>
                <div class="p1">
                    <p id="261">3) DTDA-sum和DTDA-mean在2个数据集中的表现均优于TD-LSTM和MemNet.TD-LSTM利用LSTM对句子中属性的左右部分分别建模, 容易造成上下文信息不完整.MemNet简单地以属性的词向量作为特征表示, 虽然利用了多层注意力逐层线性累加结果, 没有考虑句子的上下文信息, 而且比较依赖于词向量的质量.</p>
                </div>
                <div class="p1">
                    <p id="262">4) 对DTDA来说, 采用2种不同的注意力计算方式sum和mean时实验结果均比较接近.由于在sum方式中上下文单词的注意力权重相对于mean方式来说更加集中, 而mean方式由于经过再次缩放使得注意力值分布更为均匀.考虑到计算过程中使用因子<mathml id="263"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>/</mo><msqrt><mrow><mi>d</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></msqrt></mrow></math></mathml>进行缩放, 且属性子句的长度较小, 因此实验结果相近.这从<i>DSAN</i>的实验结果也可得到证实.</p>
                </div>
                <h4 class="anchor-tag" id="264" name="264"><b>4.3 实验2:判定难度对比</b></h4>
                <div class="p1">
                    <p id="265">本实验的主要目的是根据领域相关的预训练词向量对DTDA和6种基准方法进行对比, 并分析它们在S1, S2和S3上的结果.由文献<citation id="451" type="reference">[<a class="sup">17</a>]</citation>可知, S1, S2和S3上的情感极性判定难度依次递增.模型在S1和S2上的分类精确率很大程度上反映了模型对上下文信息的利用情况, 而在S3上的分类精确率则更好地即体现模型对属性和上下文的刻画程度.</p>
                </div>
                <div class="p1">
                    <p id="266">表6给出各模型在2个数据集上的实验结果, 表7为各模型在S1, S2和S3上的准确率结果.其中各个基准方法的数据来自于文献<citation id="452" type="reference">[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</citation>.</p>
                </div>
                <div class="area_img" id="267">
                    <p class="img_tit"><b>表6 各模型在Word2vec-skipgram词向量上的结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 Results for Models on Word2vec-skipgram Word Vectors</b></p>
                    <p class="img_note"></p>
                    <table id="267" border="1"><tr><td rowspan="2"><br />Model</td><td colspan="2"><br />Laptop</td><td colspan="2">Restaurant</td></tr><tr><td><br /><i>P</i></td><td><i>MF</i>1</td><td><i>P</i></td><td><i>MF</i>1</td></tr><tr><td><br />TD-LSTM</td><td>0.717 9</td><td>0.666 5</td><td>0.784 8</td><td>0.681 2</td></tr><tr><td><br />MemNet</td><td>0.725 7</td><td>0.676 5</td><td>0.802 7</td><td>0.707 6</td></tr><tr><td><br />RAM</td><td>0.744 5</td><td>0.700 9</td><td>0.7884</td><td>0.6835</td></tr><tr><td><br />ATAE-LSTM</td><td>0.725 7</td><td>0.683 3</td><td>0.786 6</td><td>0.680 2</td></tr><tr><td><br />IAN</td><td>0.719 4</td><td>0.666 4</td><td>0.799 1</td><td>0.704 6</td></tr><tr><td><br />DSAN-sum</td><td>0.769 6</td><td>0.726 5</td><td>0.814 3</td><td>0.731 1</td></tr><tr><td><br />DSAN-mean</td><td>0.763 3</td><td>0.713 6</td><td>0.813 4</td><td>0.723 5</td></tr><tr><td><br />DTDA-sum</td><td><b>0.770</b><b>1</b></td><td><b>0.727</b><b>8</b></td><td><b>0.816</b><b>8</b></td><td><b>0.731</b><b>9</b></td></tr><tr><td><br />DTDA-mean</td><td>0.763 7</td><td>0.714 2</td><td>0.813 4</td><td>0.726 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="268">从表6可知:</p>
                </div>
                <div class="p1">
                    <p id="269">1) 与表5中相比, 使用基于相近领域语料训练所得的词向量后, MemNet, ATAE-LSTM, DSAN和DTDA在Laptop和Restaurant上的效果均有较好的提升.TD-LSTM和IAN在Restaurant上与表5相比也有一定的提升.但RAM因原文献<citation id="453" type="reference">[<a class="sup">15</a>]</citation>中没有提供详细的超参数, 因此本文的实验结果不如原文.</p>
                </div>
                <div class="p1">
                    <p id="270">2) 各个模型使用Word2vec-skipgram词向量后, DTDA-sum取得最好结果.相对于表5, DTDA-sum和DTDA-mean在2个数据集上的<i>P</i>和<i>MF</i>1均有一定的提升, 而MemNet在Laptop和Restaurant上的精确率<i>P</i>均有2%以上的提升.这说明采用相近的语料进行词向量预训练能更好地学习到相关的领域知识和提升模型效果.而且, DTDA比DSAN在2个数据集均有略微的提升, 这说明DTDA中更加全面的距离信息有助于提升模型的效果.</p>
                </div>
                <div class="area_img" id="271">
                    <p class="img_tit"><b>表7 各模型在S1, S2和S3上的准确率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 7 Accuracy Comparison for Various Models on S1, S2 and S3</b></p>
                    <p class="img_note"></p>
                    <table id="271" border="1"><tr><td rowspan="2"><br />Model</td><td colspan="3"><br />Laptop</td><td colspan="3">Restaurant</td></tr><tr><td><br />S1</td><td>S2</td><td>S3</td><td>S1</td><td>S2</td><td>S3</td></tr><tr><td><br />TD-LSTM</td><td>0.723 7</td><td>0.759 0</td><td><b>0.592</b><b>2</b></td><td>0.764 1</td><td>0.861 8</td><td>0.605 3</td></tr><tr><td><br />MemNet</td><td>0.731 5</td><td>0.769 8</td><td><b>0.592</b><b>2</b></td><td><b>0.785</b><b>2</b></td><td>0.888 2</td><td>0.596 5</td></tr><tr><td><br />RAM</td><td>0.758 8</td><td>0.802 2</td><td>0.553 4</td><td>0.757 0</td><td>0.879 9</td><td>0.583 3</td></tr><tr><td><br />ATAE-LSTM</td><td>0.731 5</td><td>0.795 0</td><td>0.524 3</td><td><b>0.785</b><b>2</b></td><td>0.860 2</td><td>0.592 1</td></tr><tr><td><br />IAN</td><td>0.716 0</td><td>0.791 4</td><td>0.534 0</td><td>0.771 1</td><td>0.888 2</td><td>0.596 5</td></tr><tr><td><br />DSAN-sum</td><td>0.766 5</td><td>0.838 1</td><td><b>0.592</b><b>2</b></td><td>0.778 2</td><td>0.901 3</td><td>0.627 2</td></tr><tr><td><br />DSAN-mean</td><td>0.762 6</td><td>0.838 1</td><td>0.563 1</td><td>0.778 2</td><td>0.909 5</td><td>0.600 9</td></tr><tr><td><br />DSDA-sum</td><td><b>0.767</b><b>1</b></td><td><b>0.838</b><b>2</b></td><td>0.592 1</td><td>0.780 4</td><td><b>0.910</b><b>9</b></td><td><b>0.630</b><b>1</b></td></tr><tr><td><br />DSDA-mean</td><td>0.762 7</td><td>0.838 0</td><td>0.564 1</td><td>0.780 1</td><td>0.899 7</td><td>0.600 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="272">从表7可知:</p>
                </div>
                <div class="p1">
                    <p id="273">1) 所有模型在判定难度最高的S3上的准确率表现均低于S1和S2, 在S2上的表现最好, 在S1上的表现次之.主要原因一方面是因为S3所包含的训练数据最少, 导致模型缺乏充分的训练, 而且其判定难度最大.而S2的训练数据最多, 因此虽然判定难度略高于S1, 但总体表现最好.</p>
                </div>
                <div class="p1">
                    <p id="274">2) 各模型在S1, S2和S3上的准确率表现各有优劣.TD-LSTM在2个数据集的S1和S2上的表现较差, 也不如其他几种基准方法, 主要原因是因为句子的切分导致上下文信息不完整.但TD-LSTM在S3上的表现不错, 主要原因是因为对句子局部信息的关注降低了整体对单个属性的情感极性影响.RAM, ATAE-LSTM和IAN在3个子集上的表现各有优劣, 但是在S3上的表现不如TD-LSTM, 主要原因是因为其属性特征表示方法不同.MemNet在3个子集上的表现较好, 主要是因为词向量中包含了相近领域的知识, 因此在线性组合过程中较好地得到知识的迁移.</p>
                </div>
                <div class="p1">
                    <p id="275">3) DTDA在3个子集上均表现良好.相比于其他方法, DTDA在S1和S2上的提升最大, 但在S3上不如DSAN.DSAN-sum在S3上取得最好的表现, 超过了DTDA-sum及其他方法, 我们认为主要是因为sum方式降低了句子整体情感信息对单个属性的情感极性的影响, 而DTDA由于融合了相对距离信息, 造成训练参数更多, 因此随着S3判定难度的增加及训练数据的减少, DTDA-sum的效果反而不如DSAN-sum.</p>
                </div>
                <div class="p1">
                    <p id="276">总体上DTDA均取得较好的表现, 并且在S1和S2上均略高于DSAN, 这说明通过属性子句所得到的属性特征表示能够使注意力机制学习到更加合适的权重, 并且结合距离信息能够更好地区别出句子中不同单词对不同属性的情感极性的影响.</p>
                </div>
                <h4 class="anchor-tag" id="277" name="277"><b>4.4 实验3:依存子树作用分析</b></h4>
                <div class="p1">
                    <p id="278">本实验主要分析依存子树对模型效果的影响, 共包含5个模型:</p>
                </div>
                <div class="p1">
                    <p id="279">1) W-AN.取属性中所有单词的词向量平均值作为属性的特征表示.</p>
                </div>
                <div class="p1">
                    <p id="280">2) A-AN-sum.使用BGRU对属性建模, 并将其隐藏状态矩阵作为属性的特征表示, 不包含属性的修饰信息, 采用sum合并句子的注意力值.</p>
                </div>
                <div class="p1">
                    <p id="281">3) DSAN-sum.使用BGRU对属性子句建模, 并同样采用sum方式.</p>
                </div>
                <div class="p1">
                    <p id="282">4) DTDA-sum-rel.表示使用BGRU对属性子句进行建模, 并采用相对距离和sum方式.</p>
                </div>
                <div class="p1">
                    <p id="283">5) DTDA-sum.表示在DTDA-sum-R的基础上采用语法距离和sum方式.</p>
                </div>
                <div class="p1">
                    <p id="284">所有模型均使用表6中的预训练词向量, 表8和表9分别为上述5种模型在2个数据集及各子集S1, S2和S3上的结果.W-AN, A-AN-sum和DSAN-sum的结果均来自于文献<citation id="454" type="reference">[<a class="sup">17</a>]</citation>.</p>
                </div>
                <div class="p1">
                    <p id="285">从表8中可知, 随着W-AN, A-AN-sum, DSAN-sum和DTDA-sum中所包含的信息越来越丰富, 模型的效果也依次得到提升, 其中DTDA-sum取得最好的效果.从DTDA-sum-rel和DSAN-sum的比较来看, 单纯融合相对距离反而降低了模型的效果, 我们认为主要原因是因为这2个语料集中存在许多长句子, 且句子结构并不是非常规范, 而相对距离强化了与属性距离更近的单词的影响力, 因此在一定程度上反而造成了干扰.从DTDA-sum和DTDA-sum-rel的结果可知, 结合语法距离能够降低离与属性较近的无关单词的影响, 从而提升模型的效果.</p>
                </div>
                <div class="area_img" id="286">
                    <p class="img_tit"><b>表8 不同属性特征表示方式的结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 8 Results for Different Aspect Feature Representations</b></p>
                    <p class="img_note"></p>
                    <table id="286" border="1"><tr><td rowspan="2"><br />Model</td><td colspan="2"><br />Laptop</td><td colspan="2">Restaurant</td></tr><tr><td><br /><i>P</i></td><td><i>MF</i>1</td><td><i>P</i></td><td><i>MF</i>1</td></tr><tr><td><br />W-AN</td><td>0.742 9</td><td>0.706 1</td><td>0.795 5</td><td>0.699 2</td></tr><tr><td><br />A-AN-sum</td><td>0.757 1</td><td>0.711 6</td><td>0.805 4</td><td>0.712 3</td></tr><tr><td><br />DSAN-sum</td><td>0.769 6</td><td>0.726 5</td><td>0.814 3</td><td>0.731 1</td></tr><tr><td><br />DTDA-sum-rel</td><td>0.749 8</td><td>0.708 1</td><td>0.789 9</td><td>0.680 1</td></tr><tr><td><br />DTDA-sum</td><td><b>0.770</b><b>1</b></td><td><b>0.727</b><b>8</b></td><td><b>0.816</b><b>8</b></td><td><b>0.731</b><b>9</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="287">
                    <p class="img_tit"><b>表9 不同属性特征表示方式在S1, S2和S3上的 准确率结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 9 Accuracy Results for Different Aspect Feature Representations on S1, S2 and S3</b></p>
                    <p class="img_note"></p>
                    <table id="287" border="1"><tr><td rowspan="2"><br />Model</td><td colspan="3"><br />Laptop</td><td colspan="3">Restaurant</td></tr><tr><td><br />S1</td><td>S2</td><td>S3</td><td>S1</td><td>S2</td><td>S3</td></tr><tr><td><br />W-AN</td><td>0.735 4</td><td>0.830 9</td><td>0.524 3</td><td><b>0.809</b><b>9</b></td><td>0.894 7</td><td>0.513 2</td></tr><tr><td><br />A-AN-sum</td><td>0.735 4</td><td><b>0.848</b><b>9</b></td><td>0.563 1</td><td>0.771 1</td><td>0.898 0</td><td>0.600 9</td></tr><tr><td><br />DSAN-sum</td><td>0.766 5</td><td>0.838 1</td><td><b>0.592</b><b>2</b></td><td>0.778 2</td><td>0.901 3</td><td>0.627 2</td></tr><tr><td><br />DTDA-sum-rel</td><td>0.734 8</td><td>0.840 1</td><td>0.556 2</td><td>0.777 4</td><td>0.887 6</td><td>0.601 6</td></tr><tr><td><br />DTDA-sum</td><td><b>0.767</b><b>1</b></td><td>0.838 2</td><td>0.592 1</td><td>0.780 4</td><td><b>0.910</b><b>9</b></td><td><b>0.630</b><b>1</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="288">从表9可知, 在属性情感判定难度最大的子集S3上DTDA取得与DSAN-sum相当或更好的准确率效果, 并在S1和S2均超过DSAN-sum, 这证明了更加全面的距离信息有助于生成更加准确的句子特征表示.在实验过程中, 我们发现在注意力计算过程中采用向量mask对&lt;PAD/&gt;进行屏蔽后, 并没有提升模型的效果, 但可加快模型的训练.</p>
                </div>
                <h4 class="anchor-tag" id="289" name="289"><b>4.5 实验4:距离信息分析</b></h4>
                <div class="p1">
                    <p id="290">本实验主要验证2种距离信息对DTDA的影响.实验中使用Glovec 2.2M和Word2vec-skipgram词向量, 并分别对比DTDA-sum和DTDA-mean使用和不使用距离信息时的表现, 实验结果如表10和表11所示, 其中base表示未使用距离信息, rel表示只使用相对距离信息.</p>
                </div>
                <div class="area_img" id="291">
                    <p class="img_tit"><b>表10 距离信息在Laptop和Restaurant上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 10 Experiment Results for Distance Information on Laptop and Restaurant</b></p>
                    <p class="img_note"></p>
                    <table id="291" border="1"><tr><td rowspan="2"><br />Word<br />Vector</td><td rowspan="2">Model</td><td colspan="2"><br />Laptop</td><td colspan="2">Restaurant</td></tr><tr><td><br /><i>P</i></td><td><i>MF</i>1</td><td><i>P</i></td><td><i>MF</i>1</td></tr><tr><td rowspan="6"><br />Glove<br />2.2M</td><td><br />DTDA-sum base</td><td>0.728 5</td><td>0.686 2</td><td>0.797 9</td><td>0.708 8</td></tr><tr><td><br />DTDA-sum rel</td><td>0.749 8</td><td>0.708 1</td><td>0.789 9</td><td>0.680 1</td></tr><tr><td><br />DTDA-sum</td><td>0.740 2</td><td>0.696 2</td><td><b>0.800</b><b>9</b></td><td>0.727 1</td></tr><tr><td><br />DTDA-mean base</td><td>0.734 9</td><td>0.695 8</td><td>0.796 9</td><td>0.708 7</td></tr><tr><td><br />DTDA-mean rel</td><td>0.715 1</td><td>0.678 9</td><td>0.775 6</td><td>0.689 5</td></tr><tr><td><br />DTDA-mean</td><td><b>0.747</b><b>1</b></td><td><b>0.708</b><b>9</b></td><td>0.800 8</td><td><b>0.728</b><b>1</b></td></tr><tr><td rowspan="6"><br />Word2vec-<br />skipgram</td><td><br />DTDA-sum base</td><td>0.766 6</td><td>0.721 9</td><td>0.810 2</td><td>0.720 1</td></tr><tr><td><br />DTDA-sum rel</td><td>0.751 2</td><td>0.711 4</td><td>0.800 1</td><td>0.700 9</td></tr><tr><td><br />DTDA-sum</td><td><b>0.770</b><b>1</b></td><td><b>0.727</b><b>8</b></td><td><b>0.816</b><b>8</b></td><td><b>0.731</b><b>9</b></td></tr><tr><td><br />DTDA-mean base</td><td>0.758 8</td><td>0.710 1</td><td>0.808 9</td><td>0.722 8</td></tr><tr><td><br />DTDA-mean rel</td><td>0.746 9</td><td>0.701 2</td><td>0.798 2</td><td>0.711 3</td></tr><tr><td><br />DTDA-mean</td><td>0.763 7</td><td>0.714 2</td><td>0.813 4</td><td>0.726 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="292">
                    <p class="img_tit"><b>表11 距离信息在S1, S2和S3上的准确率结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 11 Accuracy Results for Distance Information on S1, S2 and S3</b></p>
                    <p class="img_note"></p>
                    <table id="292" border="1"><tr><td rowspan="2"><br />Word Vector</td><td rowspan="2">Model</td><td colspan="3"><br />Laptop</td><td colspan="3">Restaurant</td></tr><tr><td><br />S1</td><td>S2</td><td>S3</td><td>S1</td><td>S2</td><td>S3</td></tr><tr><td rowspan="6"><br />Word2vec-skipgram</td><td><br />DTDA-sum base</td><td>0.751 3</td><td>0.838 8</td><td>0.579 2</td><td>0.777 5</td><td><b>0.904</b><b>3</b></td><td>0.612 3</td></tr><tr><td><br />DTDA-sum rel</td><td>0.740 1</td><td>0.802 3</td><td>0.558 9</td><td>0.758 7</td><td>0.880 7</td><td>0.600 2</td></tr><tr><td><br />DTDA-sum</td><td><b>0.767</b><b>1</b></td><td>0.838 2</td><td><b>0.592</b><b>1</b></td><td>0.780 4</td><td>0.900 9</td><td><b>0.630</b><b>1</b></td></tr><tr><td><br />DTDA-mean base</td><td>0.758 5</td><td><b>0.841</b><b>6</b></td><td>0.541 1</td><td>0.759 8</td><td>0.881 2</td><td>0.594 3</td></tr><tr><td><br />DTDA-mean rel</td><td>0.742 1</td><td>0.800 9</td><td>0.521 3</td><td>0.748 6</td><td>0.867 8</td><td>0.571 5</td></tr><tr><td><br />DTDA-mean</td><td>0.762 7</td><td>0.838 0</td><td>0.564 1</td><td><b>0.780</b><b>1</b></td><td>0.899 7</td><td>0.600 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="293">从表10和表11中可知, DTDA-sum和DTDA-mean同时使用相对距离及语法距离时均比不使用距离时的效果有全面的提升, 这证明了相对距离及语法距离的有效性.从DTDA 在Laptop和Restaurant的子集S3上的表现看, 可发现距离信息在大部分情况都有效.</p>
                </div>
                <div class="p1">
                    <p id="294">图10和图11分别给出DTDA在Laptop和Restaurant的子集S3, 以及在Laptop的子集S1和S2上不使用距离信息 (base) 和使用2种距离信息 (both) 的准确率对比, 词向量均为Word2vec-skipgram.</p>
                </div>
                <div class="p1">
                    <p id="295">从图10可知, 距离信息有效地提升了DTDA在S3上的表现.从图11可知, 距离信息有助于提升DTDA在Laptop S1上的效果, 但却降低了在S2上的效果.我们在文献<citation id="455" type="reference">[<a class="sup">17</a>]</citation>中也证明了这一点.主要的原因可能是因为基于相近领域语料训练所得的词向量本身包含了的领域知识, 因此距离信息反而作用不明显, 甚至会对模型造成干扰.S2中的属性情感受句子的整体情感极性影响较大, 距离信息容易强化与属性距离较近的单词的影响力.综合DTDA及DSDAtt的结果可知, 距离信息均较适合于词向量质量相对较差的情况.</p>
                </div>
                <div class="area_img" id="296">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_296.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 距离信息在S3上的准确率对比" src="Detail/GetImg?filename=images/JFYZ201908016_296.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 距离信息在S3上的准确率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_296.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Accuracy comparisons of distance information on S3</p>

                </div>
                <div class="area_img" id="297">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_297.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 距离信息在Laptop的S1和S2上的准确率对比" src="Detail/GetImg?filename=images/JFYZ201908016_297.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 距离信息在Laptop的S1和S2上的准确率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_297.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Accuracy comparison of distance information on S1 and S2 for Laptop</p>

                </div>
                <div class="area_img" id="298">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_298.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 DTDA-sum对“food”和“service”的注意力分布" src="Detail/GetImg?filename=images/JFYZ201908016_298.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 DTDA-sum对“food”和“service”的注意力分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_298.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Attention distribution for DTDA-sum on “food” and “service”</p>

                </div>
                <h4 class="anchor-tag" id="299" name="299"><b>4.6 实验5:注意力权重可视化</b></h4>
                <div class="area_img" id="300">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908016_300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 DTDA-mean对属性“food”和“service”的注意力可视化" src="Detail/GetImg?filename=images/JFYZ201908016_300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 DTDA-mean对属性“food”和“service”的注意力可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908016_300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Attention distribution for DTDA-mean on “food” and “service”</p>

                </div>
                <div class="p1">
                    <p id="301">本实验的主要目的是通过可视化分析注意力机制的有效性.以Restaurant中的句子“Great food but the service was dreadful!”为例, 属性“food”的情感极性为Pos, “service”的情感极性为Neg.图12和图13分别给出DTDA-sum和DTDA-mean对这2个属性的不同注意力权重分配结果.</p>
                </div>
                <div class="p1">
                    <p id="302">从图12和图13可以看出:</p>
                </div>
                <div class="p1">
                    <p id="303">1) 对于属性“food”, 在DTDA-sum中“Great”和“dreadful”都获得较大的注意力权重, 但相对来说“Great”上的注意力权重更大.对于属性“service”, 在DTDA-sum中“dreadful”得到53%的注意力权重, 明显超过其他单词.DTDA-sum对属性“food”和“service”的预测结果均为正确.</p>
                </div>
                <div class="p1">
                    <p id="304">2) 对于属性“food”和“service”, 在DTDA-mean中注意力权重分布相对更为均匀.例如, 对于“food”, “Great”的注意力权重只比“dreadful”略高一些, 因此最终预测情感为Pos的概率只比Neg的概率略高.同样, 对于“service”也存在类似情况.</p>
                </div>
                <div class="p1">
                    <p id="305">总体来看, 虽然DTDA对于该句子的预测结果均为正确, 但显然在sum方式中模型更关注局部信息, 而在mean方式中模型则更关注全局信息.</p>
                </div>
                <h3 id="306" name="306" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="307">句子的属性情感分类是近年自然语言处理领域的一个研究热点.本文针对目前相关研究中没有考虑属性的上下文信息及单词与属性间的距离信息, 结合依存树及距离信息提出一种基于注意力机制的句子属性情感分类模型DTDA, 通过利用依存树得到更加完整的属性子句信息, 并结合相对距离和语法距离进一步生成包含语义及距离信息的句子特征信息, 从而更好地帮助模型进一步学习注意力权重, 并提升分类的效果.DTDA在SemEval2014的2个基准数据集Laptop和Restaurant上均获得接近当前最好的效果.当使用相关领域语料训练的词向量后, DTDA在Laptop上的准确率77.01%, 在Restaurant上的准确率为81.68%.</p>
                </div>
                <div class="p1">
                    <p id="308">下一步将继续完善本文的工作, 一方面将依存子树的属性特征表示与多注意力机制相结合, 另一方面探讨更加有效的属性位置信息编码方式.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="364">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sem Eval-2014 Task 4:Aspect Based Sentiment Analysis">

                                <b>[1]</b>Pontiki M, Galanis D, Pavlopoulos J, et al.Semeval-2014 task 4:Aspect based sentiment analysis[C] //Proc of the 8th Int Workshop on Semantic Evaluation (SemEval 2014) .Stroudsburg, PA:ACL, 2014:27- 35
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708009&amp;v=MDkwNDBGckNVUkxPZVplUnJGeXZnVzczTEx5dlNkTEc0SDliTXA0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Liu Bin, Liu Quan, Xu Jin, et al.Aspect-based sentiment analysis based on multi-attention CNN[J].Journal of Computer Research and Development, 2017, 54 (8) :1724- 1735 (in Chinese) (梁斌, 刘全, 徐进, 等.基于多注意力卷积神经网络的特定目标情感分析[J].计算机研究与发展, 2017, 54 (8) :1724- 1735) 
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Holistic Lexicon-Based Approach to OpinionMining">

                                <b>[3]</b>Ding Xiaowen, Liu Bing, Yu P S, et al.A holistic lexicon-based approach to opinion mining[C] //Proc of the 2008 Int Conf on Web Search and Data Mining.New York:ACM, 2008:231- 240
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00004329528&amp;v=MDg5NDJxZWJ1ZHRGQzNsVUxyQUlsMD1OajNhYXJPNEh0SElySTFNWWVrSFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Boiy E, Moens M F.A machine learning approach to sentiment analysis in multilingual Web texts[J].Information Retrieval, 2009, 12 (5) :526- 558
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Target-dependent TwitterSentiment Classification">

                                <b>[5]</b>Jiang Long, Yu Mo, Zhou Ming, et al.Target-dependent twitter sentiment classification[C] //Proc of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies-Volume 1.Stroudsburg, PA:ACL, 2011:151- 160
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive recursive neural network for target-dependent twitter sentiment classification">

                                <b>[6]</b>Li Dong, Wei Furu, Tan Chuanqi, et al.Adaptive recursive neural network for target-dependent Twitter sentiment classification[C] //Proc of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers) .Stroudsburg, PA:ACL, 2014, 2:49- 54
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective LSTMs for target-dependent sentiment classification">

                                <b>[7]</b>Tang Duyu, Qin Bin, Feng Xiaocheng, et al.Effective LSTMs for Target-Dependent Sentiment Classification[C] //Proc of the 26th Int Conf on Computational Linguistics.Stroudsburg, PA:ACL, 2016:3298- 3307
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated Neural Networks for Targeted Sentiment Analysis">

                                <b>[8]</b>Zhang Meishan, Zhang Yue, Vo D T.Gated neural networks for targeted sentiment analysis[C] //Proc of the 13th AAAI Conf on Artificial Intelligence.Cambridge, MA:MIT, 2016:3087- 3093
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PhraseRNN:Phrase Recursive Neural Network for Aspect-based Sentiment Analysis">

                                <b>[9]</b>Nguyen T H, Shirai K.PhraseRNN:Phrase recursive neural network for aspect-based sentiment analysis[C] //Proc of the 2015 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2509- 2514
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A hierarchical model of reviews for aspect-based sentiment analysis">

                                <b>[10]</b>Ruder S, Ghaffari P, Breslin J G.A hierarchical model of reviews for aspect-based sentiment analysis[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:999- 1005
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attentionbased LSTM for aspect-level sentiment classification">

                                <b>[11]</b>Wang Yequan, Huang Minlie, Zhu Xiaoyan.Attention-based lstm for aspect-level sentiment classification[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:606- 615
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dependency-Attention-Based LSTM for Target-Dependent Sentiment Analysis">

                                <b>[12]</b>Wang Xinbo, Chen Guang.Dependency-attention-based lstm for target-dependent sentiment analysis[C] //Proc of Chinese National Conf on Social Media Processing.Berlin:Springer, 2017:206- 217
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis">

                                <b>[13]</b>Tay Y, Luu A T, Hui S C.Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Cambridge, MA:MIT, 2018:5956- 5963
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aspect Level Sentiment Classification with Deep Memory Network">

                                <b>[14]</b>Tang Duyu, Qin Bing, Liu Ting.Aspect level sentiment classification with deep memory network[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:214- 224
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent Attention Network on Memory for Aspect Sentiment Analysis">

                                <b>[15]</b>Chen Peng, Sun Zhongqian, Bin Lidong, et al.Recurrent attention network on memory for aspect sentiment analysis[C] //Proc of the 2017 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2017:452- 461
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interactive attention networks for aspect-level sentiment classification">

                                <b>[16]</b>Ma Dehong, Li Sujian, Zhang Xiaodong, et al.Interactive attention networks for aspect-level sentiment classification[C] //Proc of the 26th Int Joint Conf on Artificial Intelligence.Cambridge, MA:MIT, 2017:4068- 4074
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018874461.nh&amp;v=MTkyNjRGcnUvR3RYS3JwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnVzczTFZGMjY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Ouyang Zhifan.Research on Aspect-level Sentiment Classification Based on Dependency Tree and Attention Network[D].Guangzhou:South China University of Technology, 2018 (in Chinese) (欧阳志凡.基于依存树和注意力的属性级别情感分类研究[D].广州:华南理工大学, 2018) 
                            </a>
                        </p>
                        <p id="398">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A Method for Stochastic Optimization[C/OL]">

                                <b>[18]</b>Kingma D P, Ba J L.Adam:A method for stochastic optimization[C/OL] //Proc of ICLR 2015.[2019-05-13].https://arxiv.org/pdf/1412.6980v8.pdf
                            </a>
                        </p>
                        <p id="400">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">

                                <b>[19]</b>Srivastava N, Hinton G, Krizhevsky A.Dropout:A simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929- 1958
                            </a>
                        </p>
                        <p id="402">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift">

                                <b>[20]</b>Ioffe S, Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C] //Proc of the 32nd Int Conf on Machine Learning.New York:ACM, 2015:448- 456
                            </a>
                        </p>
                        <p id="404">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering">

                                <b>[21]</b>He R, McAuley J.Ups and downs:Modeling the visual evolution of fashion trends with one-class collaborative filtering[C] //Proc of The 25th In Conf on World Wide Web.New York:ACM, 2016:507- 517
                            </a>
                        </p>
                        <p id="406">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glove:Global vectors for word representation">

                                <b>[22]</b>Pennington J, Socher R, Manning C D.Glove:Global vectors for word representation[C] //Proc of the Empirical Methods in Natural Language Processing (EMNLP 2014) .Stroudsburg, PA:ACL, 2014, 14:1532- 1543
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908016&amp;v=MjIxOTBSckZ5dmdXNzNMTHl2U2RMRzRIOWpNcDQ5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3JxSjJtQ1ppS3Q2ZlluST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

