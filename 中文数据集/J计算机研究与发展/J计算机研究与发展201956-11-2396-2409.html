

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127116529676250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJFYZ201911010%26RESULT%3d1%26SIGN%3dssXAveYboO1nghaoXNgbiw9GiBQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201911010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201911010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201911010&amp;v=MzI2NzZxQnRHRnJDVVJMT2VaZVJzRnl6bVZiM0tMeXZTZExHNEg5ak5ybzlFWklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;1 神经网络及分布式框架&lt;/b&gt; "><b>1 神经网络及分布式框架</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="&lt;b&gt;1.1 深度学习算法&lt;/b&gt;"><b>1.1 深度学习算法</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;1.2 参数服务器&lt;/b&gt;"><b>1.2 参数服务器</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;1.3 服务器参数更新策略&lt;/b&gt;"><b>1.3 服务器参数更新策略</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="&lt;b&gt;2 性能感知模型与DBS-SGD算法设计&lt;/b&gt; "><b>2 性能感知模型与DBS-SGD算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#138" data-title="&lt;b&gt;2.1 节点性能不一致情况下的梯度过时分析&lt;/b&gt;"><b>2.1 节点性能不一致情况下的梯度过时分析</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;2.2 &lt;/b&gt;&lt;i&gt;&lt;b&gt;batch&lt;/b&gt;&lt;/i&gt;&lt;b&gt;_&lt;/b&gt;&lt;i&gt;&lt;b&gt;size&lt;/b&gt;&lt;/i&gt;&lt;b&gt;对梯度下降算法的影响&lt;/b&gt;"><b>2.2 </b><i><b>batch</b></i><b>_</b><i><b>size</b></i><b>对梯度下降算法的影响</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;2.3 性能感知模型及DBS-SGD算法&lt;/b&gt;"><b>2.3 性能感知模型及DBS-SGD算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#181" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#203" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#375" data-title="附录A ">附录A</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="图1 模型并行">图1 模型并行</a></li>
                                                <li><a href="#71" data-title="图2 数据并行">图2 数据并行</a></li>
                                                <li><a href="#98" data-title="图3 参数服务器架构">图3 参数服务器架构</a></li>
                                                <li><a href="#117" data-title="图4 同步更新策略">图4 同步更新策略</a></li>
                                                <li><a href="#125" data-title="图5 异步更新策略">图5 异步更新策略</a></li>
                                                <li><a href="#126" data-title="图6 全局参数更新过程">图6 全局参数更新过程</a></li>
                                                <li><a href="#134" data-title="图7 top-&lt;i&gt;k&lt;/i&gt;更新策略">图7 top-<i>k</i>更新策略</a></li>
                                                <li><a href="#140" data-title="图8 采用2-soft和15-soft策略的梯度分布">图8 采用2-soft和15-soft策略的梯度分布</a></li>
                                                <li><a href="#141" data-title="图9 采用2-soft和15-soft策略的梯度分布(节点性能不一致)">图9 采用2-soft和15-soft策略的梯度分布(节点性能不一致)</a></li>
                                                <li><a href="#191" data-title="&lt;b&gt;表1 性能分析模型测试结果&lt;/b&gt;"><b>表1 性能分析模型测试结果</b></a></li>
                                                <li><a href="#192" data-title="图10 Mnist数据集loss函数值整体下降趋势">图10 Mnist数据集loss函数值整体下降趋势</a></li>
                                                <li><a href="#194" data-title="图11 Mnist数据集loss函数值下降约60%">图11 Mnist数据集loss函数值下降约60%</a></li>
                                                <li><a href="#195" data-title="图12 Mnist数据集算法的扩展性">图12 Mnist数据集算法的扩展性</a></li>
                                                <li><a href="#196" data-title="图13 Mnist数据集上分类精度的收敛趋势">图13 Mnist数据集上分类精度的收敛趋势</a></li>
                                                <li><a href="#198" data-title="图14 cifar数据集10 000次迭代算法达到的准确率">图14 cifar数据集10 000次迭代算法达到的准确率</a></li>
                                                <li><a href="#199" data-title="图15 cifar数据集10 000次迭代算法达到的loss函数值">图15 cifar数据集10 000次迭代算法达到的loss函数值</a></li>
                                                <li><a href="#200" data-title="图16 cifar数据集算法的扩展性">图16 cifar数据集算法的扩展性</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="264">


                                    <a id="bibliography_1" title="Yu Kai,Jia Lei,Chen Yuqiang,et al.Deep learning:Yesterday,today,and tomorrow[J].Journal of Computer Research and Development,2013,50(9):1799- 1804 (in Chineses)(余凯,贾磊,陈雨强,等.深度学习的昨天、今天和明天[J].计算机研究与发展,2013,50(9):1799- 1804)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201309002&amp;v=MzE5NzBSc0Z5em1WYjNLTHl2U2RMRzRIOUxNcG85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Yu Kai,Jia Lei,Chen Yuqiang,et al.Deep learning:Yesterday,today,and tomorrow[J].Journal of Computer Research and Development,2013,50(9):1799- 1804 (in Chineses)(余凯,贾磊,陈雨强,等.深度学习的昨天、今天和明天[J].计算机研究与发展,2013,50(9):1799- 1804)
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_2" title="Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436- 444" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Learning&amp;quot;">
                                        <b>[2]</b>
                                        Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436- 444
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_3" title="Chen Kai,Huo Qiang.Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering[C] //Proc of IEEE Int Conf on Acoustics Speech and Signal Processing.Piscataway,NJ:IEEE,2016:5880- 5884" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering">
                                        <b>[3]</b>
                                        Chen Kai,Huo Qiang.Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering[C] //Proc of IEEE Int Conf on Acoustics Speech and Signal Processing.Piscataway,NJ:IEEE,2016:5880- 5884
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_4" title="He Kaiming,Zhang Xiangyu,Ren Shaoqing,et al.Deep residual learning for image recognition[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770- 778" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[4]</b>
                                        He Kaiming,Zhang Xiangyu,Ren Shaoqing,et al.Deep residual learning for image recognition[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770- 778
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_5" title="Hoermann S,Bach M,Dietmayer K.Dynamic occupancy grid prediction for urban autonomous driving:A deep learning approach with fully automatic labeling[C] //Proc of 2018 IEEE Int Conf on Robotics and Automation.Piscataway,NJ:IEEE,2018:2056- 2063" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic occupancy grid prediction for urban autonomous driving:A deep learning approach with fully automatic labeling">
                                        <b>[5]</b>
                                        Hoermann S,Bach M,Dietmayer K.Dynamic occupancy grid prediction for urban autonomous driving:A deep learning approach with fully automatic labeling[C] //Proc of 2018 IEEE Int Conf on Robotics and Automation.Piscataway,NJ:IEEE,2018:2056- 2063
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_6" title="Dean J,Greg S C,Rajat M,et al.Large scale distributed deep networks[C] //Proc of the 25th Int Conf on Neural Information Processing Systems.New York:ACM,2012:1223- 1231" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Large Scale Distributed Deep Networks &amp;quot;">
                                        <b>[6]</b>
                                        Dean J,Greg S C,Rajat M,et al.Large scale distributed deep networks[C] //Proc of the 25th Int Conf on Neural Information Processing Systems.New York:ACM,2012:1223- 1231
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_7" title="Chilimbi T,Suzue Y,Apacible J,et al.Project adam:Building an efficient and scalable deep learning training system[C] //Proc of the 11th USENIX Conf on Operating Systems Design and Implementation.Berkeley,CA:USENIX Association,2014:571- 582" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Project Adam:Building an Efficient and Scalable Deep Learning Training System">
                                        <b>[7]</b>
                                        Chilimbi T,Suzue Y,Apacible J,et al.Project adam:Building an efficient and scalable deep learning training system[C] //Proc of the 11th USENIX Conf on Operating Systems Design and Implementation.Berkeley,CA:USENIX Association,2014:571- 582
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_8" title="Seide F,Fu Hao,Droppo J,et al.1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs[C] //Proc of the 15th Annual Conf of the Int Speech Communication Association.Piscataway,NJ:IEEE,2014:1058- 1062" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs">
                                        <b>[8]</b>
                                        Seide F,Fu Hao,Droppo J,et al.1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs[C] //Proc of the 15th Annual Conf of the Int Speech Communication Association.Piscataway,NJ:IEEE,2014:1058- 1062
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_9" title="Strom K.Scalable distributed DNN training using commodity GPU cloud computing[C] //Proc of the 16th Annual Conf of the Int Speech Communication Association.Piscataway,NJ:IEEE,2015:1488- 1492" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable distributed DNN training using commodity GPU cloud computing">
                                        <b>[9]</b>
                                        Strom K.Scalable distributed DNN training using commodity GPU cloud computing[C] //Proc of the 16th Annual Conf of the Int Speech Communication Association.Piscataway,NJ:IEEE,2015:1488- 1492
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_10" title="Gupta S,Zhang Wei,Wang Fei.Model accuracy and runtime tradeoff in distributed deep learning:A systematic study[C] //Proc of the 16th IEEE Int Conf on Data Mining.Piscataway,NJ:IEEE,2016:171- 180" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model accuracy and runtime tradeoff in distributed deep learning:A systematic study">
                                        <b>[10]</b>
                                        Gupta S,Zhang Wei,Wang Fei.Model accuracy and runtime tradeoff in distributed deep learning:A systematic study[C] //Proc of the 16th IEEE Int Conf on Data Mining.Piscataway,NJ:IEEE,2016:171- 180
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_11" title="Coates A,Huval B,Wang Tao,et al.Deep learning with cots HPC systems[C] //Proc of the 30th Int Conf on Machine Learning.New York:ACM,2013:1337- 1345" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep learning with COTS HPC systems.&amp;quot;">
                                        <b>[11]</b>
                                        Coates A,Huval B,Wang Tao,et al.Deep learning with cots HPC systems[C] //Proc of the 30th Int Conf on Machine Learning.New York:ACM,2013:1337- 1345
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_12" title="Faerber P,Asanovic K.Parallel neural network training on Multi-Spert[C] //Proc of the 3rd Int Conf on Algorithms and Architectures for Parallel Processing.Piscataway,NJ:IEEE,1997:659- 666" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel neural network training on Multi-Spert">
                                        <b>[12]</b>
                                        Faerber P,Asanovic K.Parallel neural network training on Multi-Spert[C] //Proc of the 3rd Int Conf on Algorithms and Architectures for Parallel Processing.Piscataway,NJ:IEEE,1997:659- 666
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_13" title="Ben-Nun T,Torsten H.Demystifying parallel and distributed deep learning:An in-depth concurrency analysis[J].arXiv preprint arXiv:1802.09941,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Demystifying parallel and distributed deep learning:An in-depth concurrency analysis">
                                        <b>[13]</b>
                                        Ben-Nun T,Torsten H.Demystifying parallel and distributed deep learning:An in-depth concurrency analysis[J].arXiv preprint arXiv:1802.09941,2018
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_14" title="Bottou L.Large-scale machine learning with stochastic gradient descent[C] //Proc of the 19th Int Conf on Computational Statistics.Berlin:Springer,2010:177- 186" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale machine learning with stochastic gradient descent">
                                        <b>[14]</b>
                                        Bottou L.Large-scale machine learning with stochastic gradient descent[C] //Proc of the 19th Int Conf on Computational Statistics.Berlin:Springer,2010:177- 186
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_15" title="Lian Xiangru,Zhang Ce,Zhang Huan,et al.Can decentralized algorithms outperform centralized algorithms?A case study for decentralized parallel stochastic gradient descent[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.New York:ACM,2017:5336- 5346" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Can decentralized algorithms outperform centralized algorithms?A case study for decentralized parallel stochastic gradient descent">
                                        <b>[15]</b>
                                        Lian Xiangru,Zhang Ce,Zhang Huan,et al.Can decentralized algorithms outperform centralized algorithms?A case study for decentralized parallel stochastic gradient descent[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.New York:ACM,2017:5336- 5346
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_16" title="Luo Qinyi,Lin Jinkun,Zhuo Youwei,et al.Hop:Heterogeneity-aware decentralized training[J].arXiv preprint arXiv:1902.01064,2019" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hop:Heterogeneity-aware decentralized training">
                                        <b>[16]</b>
                                        Luo Qinyi,Lin Jinkun,Zhuo Youwei,et al.Hop:Heterogeneity-aware decentralized training[J].arXiv preprint arXiv:1902.01064,2019
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_17" title="Li Mu,Andersen D G,Park J W,et al.Scaling distributed machine learning with the parameter server[C] //Proc of the 11th USENIX Conf on Operating Systems Design and Implementation.Berkeley,CA:USENIX Association,2014:583- 598" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scaling distributed machine learning with the parameter server">
                                        <b>[17]</b>
                                        Li Mu,Andersen D G,Park J W,et al.Scaling distributed machine learning with the parameter server[C] //Proc of the 11th USENIX Conf on Operating Systems Design and Implementation.Berkeley,CA:USENIX Association,2014:583- 598
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_18" title="Cui Henggang,Zhang Hao,Ganger G R,et al.GeePS:Scalable deep learning on distributed GPUs with a GPU-specialized parameter server[C] //Proc of the 11th European Conf on Computer Systems.New York:ACM,Piscataway,NJ:IEEE,2016:4" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GeePS:Scalable deep learning on distributed GPUs with a GPU-specialized parameter server">
                                        <b>[18]</b>
                                        Cui Henggang,Zhang Hao,Ganger G R,et al.GeePS:Scalable deep learning on distributed GPUs with a GPU-specialized parameter server[C] //Proc of the 11th European Conf on Computer Systems.New York:ACM,Piscataway,NJ:IEEE,2016:4
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_19" title="Langford J,Smola A,Zinkevich M.Slow learners are fast[C] //Proc of the 22nd Int Conf on Neural Information Processing Systems.New York:ACM,2009:2331- 2339" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Slow learners are aast">
                                        <b>[19]</b>
                                        Langford J,Smola A,Zinkevich M.Slow learners are fast[C] //Proc of the 22nd Int Conf on Neural Information Processing Systems.New York:ACM,2009:2331- 2339
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_20" title="Iandola F N,Moskewicz M W,Ashraf K,et al.FireCaffe:Near-linear acceleration of deep neural network training on compute clusters[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2592- 2600" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FireCaffe:Near-linear acceleration of deep neural network training on compute clusters">
                                        <b>[20]</b>
                                        Iandola F N,Moskewicz M W,Ashraf K,et al.FireCaffe:Near-linear acceleration of deep neural network training on compute clusters[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2592- 2600
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_21" title="Zheng Shuxin,Qi Meng,Wang Taifeng,et al.Asynchronous stochastic gradient descent with delay compensation[C] //Proc of the 34th Int Conf on Machine Learning.New York:ACM,2017:4120- 4129" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asynchronous stochastic gradient descent with delay compensation">
                                        <b>[21]</b>
                                        Zheng Shuxin,Qi Meng,Wang Taifeng,et al.Asynchronous stochastic gradient descent with delay compensation[C] //Proc of the 34th Int Conf on Machine Learning.New York:ACM,2017:4120- 4129
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_22" title="Huo Qirong,James C,Jin K,et al.More effective distributed ML via a stale synchronous parallel parameter server[C] //Proc of the 26th Int Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2013:1223- 1231" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=More effective distributed ml via a stale synchronous parallel parameter server">
                                        <b>[22]</b>
                                        Huo Qirong,James C,Jin K,et al.More effective distributed ML via a stale synchronous parallel parameter server[C] //Proc of the 26th Int Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2013:1223- 1231
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_23" title="Zhang Wei,Gupta S,Lian Xiangru,et al.Staleness-aware async-SGD for distributed deep learning[C] //Proc of the 25th Int Joint Conf on Artificial Intelligence.New York:ACM,2016:2350- 2356" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staleness-aware async-SGD for distributed deep learning">
                                        <b>[23]</b>
                                        Zhang Wei,Gupta S,Lian Xiangru,et al.Staleness-aware async-SGD for distributed deep learning[C] //Proc of the 25th Int Joint Conf on Artificial Intelligence.New York:ACM,2016:2350- 2356
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_24" title="Chen Jianmin,Pan Xinghao,Monga R,et al.Revisiting distributed synchronous SGD[J].arXiv preprint arXiv:1604.00981,2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Revisiting distributed synchronous SGD">
                                        <b>[24]</b>
                                        Chen Jianmin,Pan Xinghao,Monga R,et al.Revisiting distributed synchronous SGD[J].arXiv preprint arXiv:1604.00981,2017
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_25" title="Jiang Jiawei,Cui Bin,Zhang Ce,et al.Heterogeneity-aware distributed parameter servers[C] //Proc of the 2017 ACM Int Conf on Management of Data.New York:ACM,2017:463- 478" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Heterogeneity-aware distributed parameter servers">
                                        <b>[25]</b>
                                        Jiang Jiawei,Cui Bin,Zhang Ce,et al.Heterogeneity-aware distributed parameter servers[C] //Proc of the 2017 ACM Int Conf on Management of Data.New York:ACM,2017:463- 478
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_26" title="Su Hang,Chen Haoyu.Experiments on parallel training of deep neural network using model averaging[J].arXiv preprint arXiv:1507.01239,2018" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Experiments on parallel training of deep neural network using model averaging">
                                        <b>[26]</b>
                                        Su Hang,Chen Haoyu.Experiments on parallel training of deep neural network using model averaging[J].arXiv preprint arXiv:1507.01239,2018
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_27" title="Balles L,Romero J,Hennig P.Coupling adaptive batch sizes with learning rates[C] //Proc of the 33rd Conf on Uncertainty in Artificial Intelligence.Vabcouver,BC:AUAI,2017:410- 419" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupling adaptive batch sizes with learning rates">
                                        <b>[27]</b>
                                        Balles L,Romero J,Hennig P.Coupling adaptive batch sizes with learning rates[C] //Proc of the 33rd Conf on Uncertainty in Artificial Intelligence.Vabcouver,BC:AUAI,2017:410- 419
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_28" title="Nemirovski S,Juditsky B,Lan G,et al.Robust stochastic approximation approach to stochastic programming[J].SIAM Journal on Optimization,2008,19(4):1574- 1609" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Stochastic Approximation Approach to Stochastic Programming">
                                        <b>[28]</b>
                                        Nemirovski S,Juditsky B,Lan G,et al.Robust stochastic approximation approach to stochastic programming[J].SIAM Journal on Optimization,2008,19(4):1574- 1609
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(11),2396-2409 DOI:10.7544/issn1000-1239.2019.20180880            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>分布式深度学习框架下基于性能感知的DBS-SGD算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BA%AA%E6%B3%BD%E5%AE%87&amp;code=43335588&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">纪泽宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%85%B4%E5%86%9B&amp;code=09088180&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张兴军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E5%93%B2&amp;code=43335589&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付哲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E6%9F%8F%E6%9D%BE&amp;code=43335590&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高柏松</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%9D%96%E6%B3%A2&amp;code=40918582&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李靖波</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0189085&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安交通大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>通过增加模型的深度以及训练数据的样本数量,深度神经网络模型能够在多个机器学习任务中获得更好的性能,然而这些必要的操作会使得深度神经网络模型训练的开销相应增大.因此为了更好地应对大量的训练开销,在分布式计算环境中对深度神经网络模型的训练过程进行加速成为了研发人员最常用的手段.随机梯度下降(stochastic gradient descent, SGD)算法是当前深度神经网络模型中最常见的训练算法之一,然而SGD在进行并行化的时候容易产生梯度过时问题,从而影响算法的整体收敛性.现有解决方案大部分针对的是各节点性能差别较小的高性能计算(high performance computing, HPC)环境,很少有研究考虑过各节点性能差别较大的集群环境.针对上述问题进行研究并提出了一种基于性能感知技术的动态batch size随机梯度下降算法(dynamic batch size SGD, DBS-SGD).该算法通过分析各节点的计算能力,对各节点的minibatch进行动态分配,从而保证了节点间每次迭代更新的时间基本一致,进而降低了节点的平均梯度过时值.提出的算法能够有效优化异步更新策略中存在的梯度过时问题.选用常用的图像分类基准Mnist和cifar10作为训练数据集,将该算法与异步随机梯度下降(asynchronous SGD, ASGD)算法、<i>n</i>-soft算法进行了对比.实验结果表明:在不损失加速比的情况下,Mnist数据集的loss函数值降低了60%,cifar数据集的准确率提升了约10%,loss函数值降低了10%,其性能高于ASGD算法和<i>n</i>-soft算法,接近同步策略下的收敛曲线.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">参数服务器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E6%AD%A5%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异步随机梯度下降算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%AF%E5%BA%A6%E8%BF%87%E6%97%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梯度过时;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%80%A7%E8%83%BD%E6%84%9F%E7%9F%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">性能感知;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据并行;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *张兴军(xjzhang@xjtu.edu.cn);
                                </span>
                                <span>
                                    纪泽宇,zeyu.ji@stu.xjtu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2016YFB0200902);</span>
                    </p>
            </div>
                    <h1><b>Performance-Awareness Based Dynamic Batch Size SGD for Distributed Deep Learning Framework</b></h1>
                    <h2>
                    <span>Ji Zeyu</span>
                    <span>Zhang Xingjun</span>
                    <span>Fu Zhe</span>
                    <span>Gao Bosong</span>
                    <span>Li Jingbo</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Xi&apos;an Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>By increasing the depth of neural network and the size of datasets, the deep neural networks are currently widely used for many artificial intelligence applications including computer vision, speech recognition and natural language processing. It can deliver the state of the art accuracy on these tasks. However, these operations will increase the overhead of training process in deep neural network algorithm. Stochastic gradient descent(SGD) has been used for accelerating the training of deep neural networks in a distributed computing environment. Nevertheless, parallel SGD can easily generate the problem of stale gradient, which affects the overall convergence. Most of the existing solutions are suit for high performance computing(HPC) environment where the performance of each node is similar. Few studies have researched cluster environment where the performance of each node is quite different. This paper proposes a variant of ASGD(asynchronous SGD) algorithm in which the batch size is modulated according to the runtime performance of each node. Experimental verification is performed on commonly-used image classification benchmarks: Mnist and cifar10 to demonstrate the effectiveness of the approach. Compared with ASGD and <i>n</i>-soft, the loss function of Mnist is reduced by 60% and the accuracy of the cifar10 is increased about 10% without reducing the speed-up.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=parameter%20server&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">parameter server;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=synchronous%20stochastic%20gradient%20descent(SSGD)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">synchronous stochastic gradient descent(SSGD);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stale%20gradient&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stale gradient;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=performance%20awareness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">performance awareness;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20parallelism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data parallelism;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Ji Zeyu,born in 1986.PhD candidate.Student member of CCF.His main research interests include computer architecture,high performance computing and deep learning.<image id="365" type="formula" href="images/JFYZ201911010_36500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhang Xingjun,born in 1969.PhD,professor,PhD supervisor.Member of CCF.His main research interests include computer architecture,high performance computing,big data storage system and computer networks.<image id="366" type="formula" href="images/JFYZ201911010_36600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Fu Zhe,born in 1992.Master candidate.Student member of CCF.His main research interests include deep learning combined with wireless networks,and wireless powered mobile-edge computing.<image id="367" type="formula" href="images/JFYZ201911010_36700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Gao Bosong,born in 1994.Master candidate.Her main research interests include deep high performance computing and deep learning.<image id="368" type="formula" href="images/JFYZ201911010_36800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Li Jingbo,born in 1993.PhD candidate.Student member of CCF.His main research interests include computer architecture and high performance computing.<image id="369" type="formula" href="images/JFYZ201911010_36900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China(2016YFB0200902);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="66">在过去10年间,深度学习<citation id="320" type="reference"><link href="264" rel="bibliography" /><link href="266" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>(deep learning, DL)算法在图像分类、语音识别、自然语言处理以及无人驾驶等领域得到了广泛应用并产生了巨大的影响<citation id="321" type="reference"><link href="268" rel="bibliography" /><link href="270" rel="bibliography" /><link href="272" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>,这在很大程度上归功于更深层次的神经网络模型以及相对海量的训练数据.然而,复杂的网络模型和海量的训练数据大大提高了神经网络的训练开销.</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 模型并行" src="Detail/GetImg?filename=images/JFYZ201911010_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 模型并行  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Model parallelism</p>

                </div>
                <div class="p1">
                    <p id="68">由于大规模深度神经网络训练受制于有限计算资源,研究人员进行了诸多利用分布式计算框架加速神经网络训练的工作.例如:Dean等人<citation id="322" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>以及Chilimbi等人<citation id="323" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>分别建立了1个集成多个商用CPU的集群,并在此之上对数据进行并行训练;Seide等人<citation id="324" type="reference"><link href="278" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>以及Strom<citation id="325" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>均使用由集成多个GPU的节点构成的异构计算平台进行并行加速;Gupta等人<citation id="326" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>使用高性能计算集群进行训练等.</p>
                </div>
                <div class="p1">
                    <p id="69">为了使神经网络算法能够在分布式计算框架下运行,研究人员通常会采用相应的并行模型.目前流行的深度神经网络并行模型主要分为2种:模型并行(model parallelism)<citation id="327" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和数据并行(data parallelism)<citation id="328" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="70">模型并行是指分布式系统中的不同硬件(CPU/GPU)负责神经网络模型中的不同部分,如图1所示,神经网络模型中不同网络层被分配到不同的硬件上执行,或者同一层内不同的参数被分配到不同的硬件<citation id="329" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>.数据并行是指分布式系统中每个节点都有同一个模型的副本,如图2所示,每个节点被分配了不同的训练数据,然后由参数服务器将所有机器的计算结果按照一定的方式合并,最后再完成参数的更新和广播.</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数据并行" src="Detail/GetImg?filename=images/JFYZ201911010_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 数据并行  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Data parallelism</p>

                </div>
                <div class="p1">
                    <p id="72">在大多数情况下,模型并行所带来的通信和同步开销远远超过数据并行,加速比较低,因此目前大多数的研究工作都集中在数据并行领域.随机梯度下降(stochastic gradient descent, SGD)算法<citation id="330" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>具有实现简单、收敛速度快、运行效率高等诸多优点,因此在深度神经网络算法中得到了广泛的应用.深度学习分布式策略可以分为基于参数分布形式的研究以及基于模型一致性的研究.参数分布形式研究包括去中心化<citation id="335" type="reference"><link href="292" rel="bibliography" /><link href="294" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>和中心化<citation id="336" type="reference"><link href="274" rel="bibliography" /><link href="296" rel="bibliography" /><link href="298" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>2类.在去中心化的方法中,每个工作节点都维护1个本地的参数并根据指定的通信图进行参数传递和更新.在中心化的方法中,由被称为参数服务器<citation id="331" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的中央节点维护全局参数,而其余的工作节点进行计算工作.而基于模型一致性的研究则包含了同步更新模型<citation id="337" type="reference"><link href="284" rel="bibliography" /><link href="300" rel="bibliography" /><link href="302" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>、异步更新模型<citation id="338" type="reference"><link href="274" rel="bibliography" /><link href="304" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">21</a>]</sup></citation>和过时同步更新模型<citation id="339" type="reference"><link href="282" rel="bibliography" /><link href="306" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">22</a>]</sup></citation>.在同步更新模型中SSGD(synchronization-SGD, SSGD)算法是基于SGD算法最简单直接的分布式实现,最早是由Langford等人<citation id="332" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出.在该算法中,主机简单地将数据划分并映射到相应的工作节点上,通过使用显示栅栏进行强制同步,能够确保主机各工作节点都使用相同的模型参数来进行梯度计算,然而各节点在每次迭代中会被强制要求等待其中工作最慢的节点.这种同步策略所付出的代价是降低了SSGD算法的可扩展性以及运行时性能.为了解决这个问题,Zheng等人<citation id="333" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出了基于异步更新模型的ASGD(asynchronization-SGD, ASGD)算法,该算法通过消除各节点间的同步栅栏来克服上述缺点.然而这种异步行为又不可避免地给整个系统引入了“梯度过时”:由于计算梯度是需要时间的,因此当某个节点计算完梯度值并提交给服务器更新参数时,全局参数可能已经被更新了数次,那么此次更新的参数就会存在一定的过时量,因而不是最新的收敛方向,这种现象被称为“梯度过时”.因此在确定迭代次数的前提下,ASGD训练的模型往往比SSGD训练的模型效果差很多,严重的梯度值过时会明显减慢网络模型的收敛速度,甚至完全停止收敛.而过时同步更新模型<citation id="334" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>则是在同步更新模型与异步更新模型之间做权衡,当运行快的工作节点在达到一定梯度过时量时将被强制进行全局同步操作,从而缓解梯度过时问题带来的影响.</p>
                </div>
                <div class="p1">
                    <p id="73">目前有很多针对异步梯度下降算法中出现的梯度过时问题的研究<citation id="343" type="reference"><link href="274" rel="bibliography" /><link href="306" rel="bibliography" /><link href="308" rel="bibliography" /><link href="310" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">22</a>,<a class="sup">23</a>,<a class="sup">24</a>]</sup></citation>,其中大多数异步随机梯度下降算法的基本思路大致相同,主要差异是采用了不同的优化策略来减少过时梯度所造成的影响.Dean等人<citation id="344" type="reference"><link href="274" rel="bibliography" /><link href="306" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">22</a>]</sup></citation>在参数服务器端使用了延时同步技术,每隔一定迭代轮数,服务器将进行1次强制同步,从而消除梯度过时的影响.Zhang等人<citation id="340" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提出了一种ASGD算法的变体,该算法根据不同时刻的梯度过时值动态调整学习速率,同时具有很强的收敛性.Chen等人<citation id="341" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提出在每次训练时多使用5%～10%的节点,以随时替换那些性能较慢的节点,从而尽可能保证系统中所有节点性能的一致性.虽然这些研究通过不同的方式消除了梯度过时带来的影响,但是相关算法均运行在各节点性能较为平均的同构平台上,没有充分考虑到商业级云计算环境.Jiang等人<citation id="342" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>提到了商业级云计算平台中3种典型的应用场景:</p>
                </div>
                <div class="p1">
                    <p id="74">1) 网络和计算异质性.在1个由不同数据中心提供的数千台商用机器组成的集群中,各机器很难保证拥有同代处理器,这样就使节点间的性能存在差异.</p>
                </div>
                <div class="p1">
                    <p id="75">2) 资源竞争.在1个面向多用户的集群中,通常需要同时处理多个应用请求,那么这些请求将不可避免地在同一个节点上竞争稀缺的硬件资源.因此1个应用的不同实例通常具有不同的运行时间.</p>
                </div>
                <div class="p1">
                    <p id="76">3) Spot Instance.Amazon EC2通过使用Spot Instance来为用户提供服务.为了大量节省成本,不同资源需求的实例能够运行在同一个集群上(例如m4.large具有2个核,a1.4xlarge具有16个核).那么当1个用户在这样的集群上提交任务时,与被分配了高性能硬件的工作节点相比,被分配了低性能硬件的工作节点需要花费更多的时间计算等量的数据.</p>
                </div>
                <div class="p1">
                    <p id="77">这3种应用场景导致不同节点提供的算力各不相同,此时节点间计算性能的差异导致了梯度过时问题的产生.基于异质性环境中的计算性能差异问题,Jiang等人<citation id="345" type="reference"><link href="312" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>通过测试发现SSP模型策略在异质性集群上的运行时间比正常慢了约80%,同时在SSP模型的基础上动态优化学习率以解决异质性带来的问题,而Luo等人<citation id="346" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>则基于去中心化的研究并结合Chen等人<citation id="347" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提出的backup works方法来解决异质问题.本文针对异质性的分布式运行环境提出了一种基于性能感知技术的动态批大小(batch size)随机梯度下降算法(dynamic batch size SGD, DBS-SGD),该算法可以有效地根据各节点间不同的性能,合理地分配与之相适应的数据量,最终使得各节点在异步更新的情况下达到基本相同的迭代次数,从而消除由性能差异引起的梯度过时问题带来的影响.</p>
                </div>
                <div class="p1">
                    <p id="78">本文主要工作有2个方面:</p>
                </div>
                <div class="p1">
                    <p id="79">1) 提出了一种可以快速量化各节点性能的模型.此模型通过对节点最近<i>N</i>次运行时间进行统计和计算,对节点当前性能进行量化,以方便对各节点性能进行直接评估.</p>
                </div>
                <div class="p1">
                    <p id="80">2) 基于上述的节点性能量化模型,提出了一种基于性能感知技术的动态batch size随机梯度下降算法DBS-SGD.在得到各节点性能的量化值后,该算法能够实时地调配各节点的工作量,使得各节点每次迭代的运行时间基本一致,从而解决了由性能差异引起的梯度过时问题,消除了梯度过时对系统的影响.</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>1 神经网络及分布式框架</b></h3>
                <div class="p1">
                    <p id="82">本节我们将对分布式深度学习的相关理论、参数服务器整体框架及分布式系统中的同步策略进行分析.</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>1.1 深度学习算法</b></h4>
                <div class="p1">
                    <p id="84">神经网络算法通过调整自身的参数对1组输入及其相应输出进行非线性的变换<i>f</i><sub><i>θ</i></sub>:<i>X</i>→<i>Y</i>,其中<i>θ</i>为1组可调整的参数(或者称为权重).例如在监督学习的环境下的图像分类任务中,<i>X</i>为输入的图片,<i>Y</i>为与输入图片相应的分类标签.深度神经网络将参数集<i>θ</i>分为多个层,每层均由线性变换和之后相应的非线性变换函数组成,其中非线性变换函数包括sigmoid,tanh函数等.在前馈深度神经网络中,各层按照顺序使得<i>L</i>-1层的输出传递到<i>L</i>层的输入中,最后1层则管理整个网络的输出结果<mathml id="206"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>Y</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi mathvariant="bold-italic">θ</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>.</mo></mrow><mn>1</mn></mrow></math></mathml>个神经网络的训练算法试图找到一组最优的参数<i>θ</i><sup>*</sup>,其能够将真实结果<i>Y</i>与预测结果<mathml id="207"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>Y</mi></mstyle><mrow><mspace width="0.25em" /><mo>⌒</mo></mrow></mover></mrow></math></mathml>之间的差异最小化.其中采用的优化算法通常为梯度下降法,它被广泛应用于神经网络模型中.梯度下降算法将梯度的负方向作为搜索方向,通过不断地迭代以求解目标函数或损失函数的最优值或者局部最优值.由于需要对目标函数进行求导,因此理论上梯度下降算法能够达到线性收敛,同时只能求解可微的凸函数.损失函数的表达式为</p>
                </div>
                <div class="p1">
                    <p id="85"><mathml id="208"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mrow><mo>{</mo><mrow><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>f</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi mathvariant="bold-italic">ε</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><mo>}</mo></mrow></mrow></math></mathml>,(1)</p>
                </div>
                <div class="p1">
                    <p id="86">其中,<i>ε</i><sub><i>n</i></sub>表示数据集中第<i>n</i>个数据,<i><b>w</b></i>为整个神经网络的参数,<i>f</i>(<i><b>w</b></i>,<i>ε</i><sub><i>n</i></sub>)表示使用了参数<i><b>w</b></i>和<i>ε</i><sub><i>n</i></sub>的复合损失函数.</p>
                </div>
                <div class="p1">
                    <p id="87">神经网络模型采用反向传播算法对网络中的各参数进行求导并依次更新,每次迭代对参数进行更新的表达式为</p>
                </div>
                <div class="area_img" id="370">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="89"><i><b>w</b></i><sub><i>j</i></sub><sub>+1</sub>=<i><b>w</b></i><sub><i>j</i></sub>-<i>η</i><image href="images/JFYZ201911010_212.jpg" type="" display="inline" placement="inline"><alt></alt></image><i>F</i>(<i><b>w</b></i><sub><i>j</i></sub>),(3)</p>
                </div>
                <div class="p1">
                    <p id="90">其中,<i>η</i>为学习率,<image href="images/JFYZ201911010_213.jpg" type="" display="inline" placement="inline"><alt></alt></image><i>F</i>为损失函数的1阶导数.</p>
                </div>
                <div class="p1">
                    <p id="91">由于梯度下降算法需要在整个数据集上进行计算,其计算时间较长,同时比较容易陷入局部最优解.因此出现了很多关于梯度下降的变种算法,如随机梯度下降算法、批量梯度下降算法等.随机梯度下降算法和小批量梯度下降算法的表达式为</p>
                </div>
                <div class="p1">
                    <p id="92"><i><b>w</b></i><sub><i>j</i></sub><sub>+1</sub>=<i><b>w</b></i><sub><i>j</i></sub>-<image href="images/JFYZ201911010_214.jpg" type="" display="inline" placement="inline"><alt></alt></image><i>F</i>(<i><b>w</b></i><sub><i>j</i></sub>)=<i><b>w</b></i><sub><i>j</i></sub>-<i>η</i><image href="images/JFYZ201911010_215.jpg" type="" display="inline" placement="inline"><alt></alt></image><i>f</i>(<i><b>w</b></i><sub><i>j</i></sub>,<i>ε</i><sub><i>n</i></sub>).(4)</p>
                </div>
                <div class="p1">
                    <p id="93"><i><b>w</b></i><sub><i>j</i></sub><sub>+1</sub>=<i><b>w</b></i><sub><i>j</i></sub>-<image href="images/JFYZ201911010_216.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><image href="images/JFYZ201911010_218.jpg" type="" display="inline" placement="inline"><alt></alt></image><i>f</i>(<i><b>w</b></i><sub><i>j</i></sub>,<i>ε</i><sub><i>n</i></sub>).(5)</p>
                </div>
                <div class="p1">
                    <p id="94">1) 随机梯度下降算法.每1次只选取1个数据进行梯度的计算,然后进行迭代.</p>
                </div>
                <div class="p1">
                    <p id="95">2) 小批量梯度下降算法(mini batch gradient descent, MBGD).它是对梯度下降算法和随机梯度下降算法的折中,取<i>k</i>个数据进行梯度的计算,1&lt;<i>k</i>≪<i>n</i>.</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>1.2 参数服务器</b></h4>
                <div class="p1">
                    <p id="97">在集群环境中,参数服务器架构<citation id="348" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>(如图3所示)是目前主流的支持分布式并行深度学习的架构,其最早原型为谷歌团队开发的Distbelief<citation id="349" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,是大规模深度学习在不断使用过程中抽象出来的架构.</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 参数服务器架构" src="Detail/GetImg?filename=images/JFYZ201911010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 参数服务器架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Parameter server framework</p>

                </div>
                <div class="p1">
                    <p id="99">在此架构中,工作节点相关参数及操作定义如下.</p>
                </div>
                <div class="p1">
                    <p id="100">1) <i>λ</i>.工作节点个数.</p>
                </div>
                <div class="p1">
                    <p id="101">2) <i>μ</i>.每个工作节点在计算梯度时所用minibatch的大小.</p>
                </div>
                <div class="p1">
                    <p id="102">3) <i>η</i>.学习率,也称为步长.</p>
                </div>
                <div class="p1">
                    <p id="103">4) 时间戳.使用标量时钟的计数值来表示梯度权重时间戳<i>i</i>,从<i>i</i>=0开始.每次权重更新将会使相应时间戳的值增加1,梯度的时间戳与权重的时间戳相同.</p>
                </div>
                <div class="p1">
                    <p id="104">5) <i>τ</i><sub><i>i</i></sub><sub>,</sub><sub><i>l</i></sub>.工作节点<i>l</i>的梯度过时值.1个工作节点将带有时间戳<i>j</i>的梯度值发送到带有时间戳<i>i</i>的参数服务器上,<i>i</i>≥<i>j</i>.以<i>i</i>-<i>j</i>的形式来计算梯度的过时量<i>τ</i><sub><i>i</i></sub><sub>,</sub><sub><i>l</i></sub>,对于任意的<i>i</i>和<i>l</i>,<i>τ</i><sub><i>i</i></sub><sub>,</sub><sub><i>l</i></sub>≥0.</p>
                </div>
                <div class="p1">
                    <p id="105">每个工作节点顺序的执行操作有4个:</p>
                </div>
                <div class="p1">
                    <p id="106">1) getMiniatch.从训练集中随机选择1份小批量的样本.</p>
                </div>
                <div class="p1">
                    <p id="107">2) pullWeights.向参数服务器发出请求,申请当前的参数集.</p>
                </div>
                <div class="p1">
                    <p id="108">3) calcGradient.根据选择的小批量样本产生的训练误差来计算梯度值.</p>
                </div>
                <div class="p1">
                    <p id="109">4) pushGradient.将计算好的梯度发送给参数服务器.</p>
                </div>
                <div class="p1">
                    <p id="110">参数服务器对模型的全体参数进行统一维护并执行2个操作:</p>
                </div>
                <div class="p1">
                    <p id="111">1) sumGradients.接收各工作节点计算好的梯度值并进行累加操作.</p>
                </div>
                <div class="p1">
                    <p id="112">2) applyUpdate.用计算好的梯度值乘以学习率并更新全局的参数.</p>
                </div>
                <div class="p1">
                    <p id="113">在服务器端,可以使用不同的更新策略,包括同步更新策略和异步更新策略.同步更新策略也可被称为硬同步策略(synchronous-SGD),异步更新策略称也可被称为软同步策略(asynchronous-SGD).第2节将会讨论不同更新策略对梯度产生的不同影响.</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>1.3 服务器参数更新策略</b></h4>
                <div class="p1">
                    <p id="115">SSGD是梯度下降算法在分布式系统上最直观的实现,该算法在服务端采用参数平均法<citation id="350" type="reference"><link href="314" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>来处理权重的更新值,同时可证明分布式环境下的SSGD算法能够在数学上完全等同于单机环境中的该算法,证明过程见附录A.由附录A中式(A1)(A2)可知:参数服务器端通过聚合各节点在第<i>i</i>步计算完成后提交的各梯度变化值来更新第<i>i</i>+1步的权重,从而保证第<i>i</i>步产生的梯度过时量为0,即没有梯度过时问题,因此这种算法会生成具有最佳准确度的模型.</p>
                </div>
                <div class="p1">
                    <p id="116">通过每次迭代进行强制同步来保证参数更新的一致性,在工作节点较少时,该算法扩展性表现较好,但是在大规模集群中,强制同步的机制会造成木桶效应从而使整个系统的效率变得很低,如图4所示:</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 同步更新策略" src="Detail/GetImg?filename=images/JFYZ201911010_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 同步更新策略  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Synchronous method</p>

                </div>
                <div class="p1">
                    <p id="118">为了解决上述问题,软同步策略提出将参数计算的过程从参数服务器端转移到节点端,参数的更新就会变成:</p>
                </div>
                <div class="p1">
                    <p id="119"><i>Δ</i><i><b>w</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>=<i>α</i><image href="images/JFYZ201911010_219.jpg" type="" display="inline" placement="inline"><alt></alt></image><i><b>L</b></i><sub><i>j</i></sub>,(6)</p>
                </div>
                <div class="area_img" id="371">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="122">当服务器端依旧采用同步方式更新参数时,该方法完全等价于参数平均法.然而服务器放松同步约束后,当服务器接收到任意1个节点计算得到的新参数时就立刻进行更新全局参数的操作,而非等待所有节点的计算结果.该策略就变成了ASGD算法,如图5所示.ASGD算法有2个主要的优势:</p>
                </div>
                <div class="p1">
                    <p id="123">1) 增加了整个分布式系统的数据吞吐量,各工作节点可以把更多的时间投入到计算中,而非等待其他节点;</p>
                </div>
                <div class="p1">
                    <p id="124">2) 相比于同步更新方式,各节点可以更快得到其他节点的信息(参数更新值).</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 异步更新策略" src="Detail/GetImg?filename=images/JFYZ201911010_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 异步更新策略  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 5 <i>Asynchronous method</i></p>

                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 全局参数更新过程" src="Detail/GetImg?filename=images/JFYZ201911010_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 全局参数更新过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Fig</i>. 6 <i>The process of updating global parameter</i></p>

                </div>
                <div class="p1">
                    <p id="127">异步更新策略在获得以上优势的同时也带来了新的开销.当全局参数采用异步更新策略时,梯度过时问题也随之而来.因为计算梯度本身是需要消耗时间的,所以当某个节点计算完梯度值向参数服务器提交时,参数服务器端维护的全局参数可能已经被更新了多次,具体流程如图6所示.</p>
                </div>
                <div class="p1">
                    <p id="128">从图6中展示的参数更新流程可以看到,<i><b>w</b></i><sub>101</sub>的值是基于<i><b>w</b></i><sub>97</sub>计算得到的,而非最近的<i><b>w</b></i><sub>100</sub>的值.2个权重的下标之差大于1,这意味着梯度的更新不是基于最新的参数,于是产生了梯度过时.Gupta等人<citation id="351" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>论述了此模型下梯度的最小平均过时值为工作节点的数量<i>n</i>,事实上由于各节点的迭代时钟周期数不同,梯度过时的值一般大于<i>n</i>,同时由于系统中节点数量的增加以及节点间性能差异的增大,梯度过时问题更加严重.</p>
                </div>
                <div class="p1">
                    <p id="129">此外,还有一种基于ASGD策略的<i>n</i>-soft同步更新策略,如图7所示,每个工作节点从参数服务器取到最新的参数值进行梯度计算,并将计算后的结果再推送到服务器,而参数服务器在收集至少<i>c</i>=-<i>λ</i>/<i>n</i>-个更新后进行全局更新(<i>n</i>的取值范围为1～<i>λ</i>),那么<i>n</i>-soft同步策略的参数更新策略为</p>
                </div>
                <div class="area_img" id="372">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="133"><i><b>w</b></i><sub><i>i</i></sub><sub>+1</sub>=<i><b>w</b></i><sub><i>i</i></sub>-<i>α</i><image href="images/JFYZ201911010_225.jpg" type="" display="inline" placement="inline"><alt></alt></image><i><b>w</b></i><sub><i>i</i></sub>.(10)</p>
                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 top-k更新策略" src="Detail/GetImg?filename=images/JFYZ201911010_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 top-<i>k</i>更新策略  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 The top-<i>k</i> method</p>

                </div>
                <div class="p1">
                    <p id="135">这种更新策略能较好地解决异步更新策略所引发的梯度过时问题,但仅适用于各工作节点性能较为平均的情况下.第2节将会对不同策略的梯度过时问题进行量化分析.</p>
                </div>
                <h3 id="136" name="136" class="anchor-tag"><b>2 性能感知模型与DBS-SGD算法设计</b></h3>
                <div class="p1">
                    <p id="137">在第1节中提到的SSGD算法采用了显式的同步栅栏,在每一步向前迭代的过程中,参数服务器会做一个强制的聚合运算,因此每个工作节点从服务器端得到的参数都是最新的,即梯度过时值为0.但是当更新策略变为异步更新时,不可避免地引入了梯度过时问题.为了解决这个问题,首先要对梯度过时产生的原因进行分析.</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138"><b>2.1 节点性能不一致情况下的梯度过时分析</b></h4>
                <div class="p1">
                    <p id="139">假设参数服务器系统中有30个工作节点即<i>λ</i>=30,那么对于1-soft更新策略,参数服务器需要收集30次来自工作节点的梯度值,然后再进行参数的归约和更新操作.同样地,对于2-soft更新策略,参数服务器需要收集30/2=15次来自工作节点的梯度值后再进行参数更新.根据Zhang等人<citation id="352" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和Gupta等人<citation id="353" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>的工作,对于系统中的每个工作节点,无论其<i>batch</i>_<i>size</i>的大小和训练模型的大小如何变化,均可得到图8相似的经验性实验结果.从图8(a)看到,对1-soft更新策略来说,梯度过时值为0,1,2,对于图8(b)中的15-soft更新策略,平均的梯度过时值变为0～30.如果是30-soft同步策略,那么参数服务器从任意工作节点收到计算结果后即对参数进行更新操作,这时等价于ASGD算法,该算法下梯度的过时值为0～60,并呈高斯分布.可以看出在ASGD算法中,工作节点数量的增加导致了梯度过时值的增加.从图8(a)(b)中可以看到,对于<i>n</i>-soft策略来说,平均的梯度过时值接近于<i>n</i>.这是基于各工作节点性能完全一致的情况下得到的结果.如果各工作节点的性能差异性较大,那么该算法下产生的平均梯度过时值就会变大,而且梯度过时值变化会从较为稳定的高斯分布变成1个随机性更大的分布.如图9(a)(b)所示,梯度值在<i>n</i>附近的概率下降,同时原本2侧发生概率较低的地方,它们发生的概率反而得到了上升.这会导致损失函数在收敛的过程中出现剧烈的抖动,从而影响最终的收敛效率和结果.DBM-SGD算法的思想就是根据各节点的实时性能动态地分配给各节点不同的工作量,即<i>batch</i>_<i>size</i>的值,通过这样的操作,使得各节点每个迭代的时间基本一致,那么在这种情况下,整个系统梯度过时值的分布情况将再次回归到正常的高斯模型上,平均的梯度过时量减少到<i>n</i>,趋向稳定,最终收敛恢复正常.</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 采用2-soft和15-soft策略的梯度分布" src="Detail/GetImg?filename=images/JFYZ201911010_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 采用2-soft和15-soft策略的梯度分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Gradient distribution with 2-soft synchronous and 15-soft synchronous</p>

                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 采用2-soft和15-soft策略的梯度分布(节点性能不一致)" src="Detail/GetImg?filename=images/JFYZ201911010_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 采用2-soft和15-soft策略的梯度分布(节点性能不一致)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Gradient distribution with 2-soft and 15-soft(different performance)</p>

                </div>
                <h4 class="anchor-tag" id="142" name="142"><b>2.2 </b><i><b>batch</b></i><b>_</b><i><b>size</b></i><b>对梯度下降算法的影响</b></h4>
                <div class="p1">
                    <p id="143">在之前的研究<citation id="356" type="reference"><link href="290" rel="bibliography" /><link href="294" rel="bibliography" /><link href="306" rel="bibliography" /><link href="308" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">16</a>,<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>中,<i>batch</i>_<i>size</i>的值<i>m</i>通常设置为固定值,因此各节点训练的数据量是一样的,<i>m</i>一般是临时选择或通过经验测试选择.Balles等人<citation id="354" type="reference"><link href="316" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>提出在神经网络训练的过程中动态地调整<i>batch</i>_<i>size</i>的值可以加速算法的收敛,这就说明<i>batch</i>_<i>size</i>的值实际上是1个关键变量,它的变化会对神经网络性能产生错综复杂的影响.一方面,随机梯度的方差(即收敛程度)随<i>m</i>的取值线性减小,那么较大的批量值能够得到较具体的梯度信息,从而减少了整体的收敛步骤数;另一方面,每次迭代的计算成本随<i>m</i>的取值线性增加,因此可以线性地权衡方差和时间成本.从随机梯度下降算法每次更新参数的公式中可以看出,随机梯度下降算法每次迭代只使用从1个样本数据计算出的值进行参数更新,而梯度下降算法使用全部数据计算的值进行更新.Nemirovski等人<citation id="355" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>从数学上证明了两者的最终收敛趋势是一致的.不同的是使用了全部样本进行更新的梯度下降算法每次收敛的方向是最优的(梯度最陡),而使用1个样本数据的随机梯度下降算法每次收敛的方向有较大的波动.因为不是走的最优路径,所以这种抖动的结果一方面会导致函数总体迭代次数的增加,另一方面使函数有很大的概率跳出局部最优的区域,从而找到更好的解.</p>
                </div>
                <div class="p1">
                    <p id="144">而mini batch SGD算法则对各节点<i>batch</i>_<i>size</i>的值进行了折中(1&lt;<i>k</i>≪<i>n</i>),每个节点不必计算整个训练样本集,既节省了时间,又不会因为单个数据的随机性使得每次收敛的方向出现太大的波动.由此,可以得到DMB-SGD算法的基本思想为:首选统计各节点最近<i>N</i>次运行的性能指标(在该算法中,以每个节点运行1次神经网络计算的墙上时间作为节点的性能参数),然后实时地对各节点分配不同的工作量,最终使得各节点在异步更新的策略下,每个节点进行梯度计算的墙上时间大致相同,这样可以使得整体梯度过时的值一直保持在一个较低范围(Gupta等人<citation id="357" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>已经证明,理想状态下,分布式环境中,梯度过时值最小等于<i>n</i>,<i>n</i>为分布式系统中工作节点的数量),从而在最大化利用各工作节点计算能力的同时不会对整个模型的收敛趋势和迭代次数引起较大的影响.</p>
                </div>
                <div class="p1">
                    <p id="145">根据分析,可以得出的结论为:</p>
                </div>
                <div class="p1">
                    <p id="146">在每次神经网络训练的迭代中,训练数据的大小,即<i>batch</i>_<i>size</i>的值决定了这次迭代收敛方向的准确率.<i>batch</i>_<i>size</i>的值越小,收敛的波动性越大,如果<i>batch</i>_<i>size</i>=1,等价于SGD算法.<i>batch</i>_<i>size</i>的值越大,其收敛的方向越准确,如果<i>batch</i>_<i>size</i>的值为<i>n</i>,等价于GD算法.因此在每次迭代的过程中动态地去调整<i>batch</i>_<i>size</i>的值(无论是分布式还是单机)并不会影响整体的收敛趋势.</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147"><b>2.3 性能感知模型及DBS-SGD算法</b></h4>
                <div class="p1">
                    <p id="148">结合2.2节对<i>batch</i>_<i>size</i>取值的分析,在采用处理器作为计算资源的集群中,性能感知模型可通过分析节点的工作状态预测并得到节点当前的性能数据.在单个节点中影响程序运行的因素有很多,包括硬件架构、操作系统、编译环境、编程框架、算法等.模型中涉及的参数越多,其预测的结果越准确,但同时采集数据带来的开销也将增大.首先本模型的目标是实时地预测工作节点运行效率,因此应优先考虑数据的时效性;其次在异质性的应用场景中,其底层的硬件架构往往不同,因此数据采集的开销也随之增大.可见,性能感知模型中选取的参数应兼具时效性和易取性.对于单个节点的性能评估主要有较为直观的参数进行表示:迭代次数、运行时间.而梯度过时出现的主要原因是节点数量过多以及节点间的计算性能不一致,使得节点进行参数更新的时间出现间隔,本文主要的应用场景为后一种情况.为了尽可能减少梯度过时带来的影响,应该保证各节点每次迭代的时间一致,而非最终的迭代次数一致,同时运行时间易获取且能够较为直观地反映该节点当前的运行效率,所以选择以节点迭代1次所运行的时间为量化标准更为合理.从第<i>N</i>轮次的迭代开始(即最初运行时不计算其平均运行时间),之后每轮迭代取该节点最近<i>N</i>次的平均运行时间(经多次试验结果分析,<i>N</i>的值设置为5～10时统计效果较好.)作为各节点当前的性能指标,以此来对所有的节点进行计算性能的量化评估.</p>
                </div>
                <div class="p1">
                    <p id="149">结合上述的性能感知模型,DBS-SGD算法进行的操作为:各节点使用1组长度为<i>N</i>的向量来保存本地节点的工作状态,向量保存本地节点最近<i>N</i>次迭代的运行时间序列,每轮迭代完成后将最新1轮的运行时间加入向量以替换最旧1轮的运行时间.如果本地节点为基准节点,那么在统计自身最近<i>N</i>次迭代运行时间的同时还需要将此值发送至服务器端,通过服务器将基准的运行时间广播给其他工作节点.在深度学习算法中,节点每次计算1个实例过程的计算量是一样的,因此每轮迭代所使用的时间与训练的数据量成正比关系.算法中以0号节点作为基准,本地节点每次迭代都与0号节点的最近<i>N</i>轮平均运行时间进行对比,若该比例落在一定范围内,则说明该节点的性能与基准节点相似,它的<i>batch</i>_<i>size</i>不需要进行调整,否则需要对该节点的<i>batch</i>_<i>size</i>进行等比例的放大或缩小.通过算法的调度,整个系统中所有节点的运行时间都和0号节点的运行时间大致保持在同一个层次,最终避免了由于节点间性能差异过大而引发的梯度过时问题.该算法还需要在参数服务器端新增加1个1维向量来保存0号节点的运行时间.因为每轮迭代的运行时间这个参数较为直观,对其获取和统计很方便,很容易保证其时效性,同时对节点进行量化的计算操作也并不复杂,所以即便在每次迭代中都需要进行性能量化,所增加的计算开销相对于深度学习本身的计算过程来说也可以忽略不计.基于这些理论和模型,可以得到算法1.</p>
                </div>
                <div class="area_img" id="373">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="373">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="374">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="179">注:<i>X</i>和<i>m</i>的值需要经过实验给定比较优的值,并不是越小或者越大越好.<i>A</i>为1个常量.</p>
                </div>
                <div class="p1">
                    <p id="180">在理想情况下,该算法可以使各节点产生1个隐式的栅栏,在显式同步栅栏的约束下接近或达到同步更新的效果.</p>
                </div>
                <h3 id="181" name="181" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="182">为了测试该算法的效率,本文分别评估了单机运行并使用ASGD模型以及经过算法优化后的方法在Mnist和cifar10 benchmark上的效率.</p>
                </div>
                <div class="p1">
                    <p id="183">实验的测试环境为:Intel<sup>®</sup> Core<sup>TM</sup> i7-6700 CPU核心频率3.40 GHz,内存8 GB.实验用的软件为tenserflow1.6-CPU版.为了模拟性能差异较大的分布式运行环境,本实验采用了容器的机制,通过-<i>c</i>--<i>cpu</i>-<i>share</i>或者--<i>cpus</i>来限制容器能使用的CPU资源或者CPU核数.在实验测试中,分别模拟了4节点和8节点的分布式训练场景,为了模拟异质性环境,一半的节点CPU配置为1个CPU负载(docker配置参数为--<i>cpus</i>=1),另一半的节点CPU配置为1.5倍CPU负载(docker配置参数为--<i>cpus</i>=1.5),表示前一半节点的CPU负载为后一半节点的约70%.内存配置为各容器平均分配5 GB内存.容器系统默认为每个容器创建1个唯一的IPC命名空间,在默认情况下容器之间无共享内存,实验中为了更真实模拟分布式环境,也没有使用共享内存机制.</p>
                </div>
                <div class="p1">
                    <p id="184">Mnist(mixed National Institute of Standards and Technology)数据集来自美国国家标准与技术研究所,它是手写体数据集,是1个非常通用的进行分类测试的通用数据集,其包括1个训练图片集、1个训练标签集、1个测试图片集和1个测试标签集,其中训练数据由5万张28×28像素的手写体数字图片组成,测试数据包含10 000张图片.</p>
                </div>
                <div class="p1">
                    <p id="185">cifar10数据集共有60 000张彩色图像,这些图像是32×32的像素,分为10个类,每类6 000张图片.每类中的50 000张用于训练,构成了5个训练批,每一批10 000张图片;另外10 000张用于测试,单独构成1批.测试批的数据里,取自10类中的每一类,每一类随机取1 000张.抽取后剩下的随机排列组成了训练批.1个训练批中的各类图像并不一定数量相同,总的来看训练批,每一类都有5 000张图.需要说明的是,这10类都是各自独立的,不会出现重叠.</p>
                </div>
                <div class="p1">
                    <p id="186">对于Mnist数据集,构建了全相接的神经网络,其中包括2个隐藏层和1个输出层,隐藏层包含了200个神经元,输出层是1个10路的输出函数<i>softmax</i>(),该函数在10个输出类上生成了相应的概率分布,学习率设置为0.001,并对损失函数进行优化,优化函数为<i>adamoptimizer</i>().对于<i>batch</i>_<i>size</i>的设置,设置为256.每个工作节点按访问顺序从数据集中顺序地读取<i>batch</i>_<i>size</i>个数据作为子数据集.</p>
                </div>
                <div class="p1">
                    <p id="187">对于cifar10数据集,首先对原始数据进行了预处理,将32×32的图片重新从中心取24×24的尺寸,再进行反转、调整亮度、对比度等操作,增加数据集的容量,以保证更好的学习效果.随后同样采用LeNet卷积神经网络,有2个卷积层,每个卷积层接1个池化层,其中每个卷积层卷积核的大小为5,核数为32,池化层的核尺寸为2,步长为2,采用的方式是取区域最大值,随后全相连的2层神经网络,包含128个神经元,输出层1个10路的输出函数<i>softmax</i>(),该函数在10个输出类上生成了概率分布,学习率设置为0.000 1,并对损失函数进行优化,优化函数为<i>adamoptimizer</i>().对于<i>batch</i>_<i>size</i>的设置,类似于Mnist数据集,只是每个工作节点按照顺序从数据集中随机地读取<i>batch</i>_<i>size</i>个数据作为子数据集.</p>
                </div>
                <div class="p1">
                    <p id="188">Mnist数据集大小约为55 MB(压缩后约为11 MB),cifar10数据集约为180 MB(压缩后约为140 MB),同时LeNet模型包含约6万个参数,数据大小约为240 KB.分配给docker的内存空间远大于数据集和网络模型所需的存储空间,训练的全过程都可以在内存中进行,因此不存在内存与硬盘之间数据访问的延迟.</p>
                </div>
                <div class="p1">
                    <p id="189">节点间的模拟网络是1个2层交换网络,其网络延迟在0.04～0.08 μs之间.同时神经网络1次迭代的计算时间至少需要0.15 s(实验时对2个数据集均采用LeNet模型),因此容器间由于网络通信而造成的开销相对于计算来说是可以忽略不计的.</p>
                </div>
                <div class="p1">
                    <p id="190">首先对性能预测模型进行测试,在节点上对于Mnist数据集使用了全相联神经网络和LeNet网络进行测试,对于cifar10数据集采取LeNet网络进行测试,测试结果如表1所示.在cifar数据集上,设置迭代次数为1 000,可以看到程序的运行时间与<i>batch</i>_<i>size</i>的取值呈线性关系,且<i>batch</i>_<i>size</i>的值翻倍,程序运行时间也随之翻倍,同样的现象Mnist数据集上也可以观测到.Mnist数据集上,当使用全相联网络测试,迭代次数设置为1 000时的测试结果不是很准确.造成这个现象的原因是由于Mnist数据集为黑白图片,同时使用了结构较为简单的2层全相联神经网络,与卷积神经网络(convolutional neural network, CNN)网络相比,其计算量相对较小,最后设置的迭代次数量级也较低,所以程序的运行时间很短,结果不稳定.当其迭代次数设置为2 000之后,可以看到结果已经恢复正常.此测试结果与之前性能预测模型的理论分析相符.</p>
                </div>
                <div class="area_img" id="191">
                    <p class="img_tit"><b>表1 性能分析模型测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Test on Performance Predict Model</b></p>
                    <p class="img_note"></p>
                    <table id="191" border="1"><tr><td rowspan="3"><br /><i>batch</i>_<i>size</i></td><td><br />Running Time/min</td><td colspan="3">Running Time/s</td></tr><tr><td><br />cifar 10 LeNet</td><td>Mnist LeNet</td><td colspan="2">Mnist full</td></tr><tr><td><br /><i>iter</i>=1000</td><td><i>iter</i>=200</td><td><i>iter</i>=2 000</td><td><i>itre</i>=1 000</td></tr><tr><td>100</td><td>2.28</td><td>33.26</td><td>4.34</td><td>2.14</td></tr><tr><td><br />200</td><td>4.50</td><td>63.50</td><td>6.27</td><td>3.16</td></tr><tr><td><br />400</td><td>9.26</td><td>122.76</td><td>11.84</td><td>6.08</td></tr><tr><td><br />600</td><td>14.10</td><td>186.19</td><td>16.65</td><td>8.43</td></tr><tr><td><br />800</td><td>18.42</td><td>246.79</td><td>21.06</td><td>10.45</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="192">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_192.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 Mnist数据集loss函数值整体下降趋势" src="Detail/GetImg?filename=images/JFYZ201911010_192.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 Mnist数据集loss函数值整体下降趋势  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_192.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 Value of loss function on Mnist</p>

                </div>
                <div class="p1">
                    <p id="193">图10～13中使用的数据集为Mnist,是在4个节点的情况下使用ASGD算法、SSGD算法和<i>n</i>-soft与本算法进行对比,其中<i>n</i>-soft算法中参数设置为2,即服务器每收到2个节点的更新值更新1次全局参数,而基于同步更新策略的SSGD算法,其梯度过时量为0,作为基准参考以测试其余算法最终的收敛精度.4个算法中<i>batch</i>_<i>size</i>的值设置为400,均使用LeNet模型进行训练.在图10,11中,ASGD-0.78表示在4个节点中性能最低的节点和性能最高的节点进行性能量化后的百分比为78%.图10中可以看到经过本算法优化后,在相同迭代步数内,其收敛速度更快,快于ASGD算法和<i>n</i>-soft算法,略低于SSGD.图11中可以看到经过算法优化,loss函数值在相同迭代次数下降低了60%.图13中可以看出在异质性环境中,经算法优化后,神经网络模型的精度收敛曲线最陡并在更少的迭代步数内达到了最优,收敛趋势接近基于同步策略的SSGD算法.图12中则展示该算法在扩展到8个节点时所达到的加速比.</p>
                </div>
                <div class="area_img" id="194">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 Mnist数据集loss函数值下降约60%" src="Detail/GetImg?filename=images/JFYZ201911010_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 Mnist数据集loss函数值下降约60%  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Value of loss function decreased by 
 about 60% on Mnist</p>

                </div>
                <div class="area_img" id="195">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 Mnist数据集算法的扩展性" src="Detail/GetImg?filename=images/JFYZ201911010_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 Mnist数据集算法的扩展性  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_195.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 12 Speed-up on Mnist</p>

                </div>
                <div class="area_img" id="196">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 Mnist数据集上分类精度的收敛趋势" src="Detail/GetImg?filename=images/JFYZ201911010_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 Mnist数据集上分类精度的收敛趋势  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_196.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 13 Accuracy on Mnist</p>

                </div>
                <div class="p1">
                    <p id="197">对于cifar数据集,同样做了与ASGD算法、SSGD算法以及<i>n</i>-soft算法的对比测试,其中<i>n</i>-soft算法中参数设置为2,即服务器每收到任意2个节点的更新值更新1次全局参数.同Mnist数据集的实验一样,也以基于同步策略的SSGD算法作为衡量基准以测试其余算法最终的收敛精度.4个算法中<i>batch</i>_<i>size</i>的值设置为400,均使用LeNet模型进行训练.从图14中可以看出,在4个工作节点的情况下,节点间性能差异越大,收敛过程中抖动的情况出现得就越频繁,抖动幅度就越大,这种情况出现的原因已经在第2节中进行了分析.严重的梯度过时问题导致了收敛过程中出现频繁的抖动,并影响最终的收敛精度.而经过算法优化后,神经网络的收敛过程就显得较为平稳,收敛精度也提高了10%,其效果优于ASGD算法和<i>n</i>-soft算法,略低于基于同步策略的SSGD算法.图15中同样可以看出在异质环境中,经算法优化后神经网络的loss函数值也有10%的下降,其收敛趋势同样快于ASGD算法和<i>n</i>-soft算法,接近基于同步策略SSGD算法的收敛曲线.在图16中,展示了算法扩展到8个节点时所达到的加速比,接近线性加速.</p>
                </div>
                <div class="area_img" id="198">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_198.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 cifar数据集10 000次迭代算法达到的准确率" src="Detail/GetImg?filename=images/JFYZ201911010_198.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 cifar数据集10 000次迭代算法达到的准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_198.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 14 Accuracy on cifar10 after 10 000 iterations</p>

                </div>
                <div class="area_img" id="199">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_199.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 cifar数据集10 000次迭代算法达到的loss函数值" src="Detail/GetImg?filename=images/JFYZ201911010_199.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 cifar数据集10 000次迭代算法达到的loss函数值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_199.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 15 Value of loss function on cifar10 after 
 10 000 iterations</p>

                </div>
                <div class="area_img" id="200">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201911010_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 cifar数据集算法的扩展性" src="Detail/GetImg?filename=images/JFYZ201911010_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图16 cifar数据集算法的扩展性  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201911010_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 16 Speed-up on cifar10</p>

                </div>
                <div class="p1">
                    <p id="201">对于使用该算法所产生的的开销,在第2节进行了描述,仅在每次节点与参数服务器端进行通信的时候多传输了1组关于0号节点最近<i>m</i>次迭代时间的向量,其数据量的大小相对于全局参数来说不到1%.同时在工作节点上进行动态<i>batch</i>_<i>size</i>的计算量相对于深度神经网络算法的计算量同样不到1%.因此该算法所产生的开销非常小,可以忽略不计,在2个数据集上进行的测试结果也说明了这一点.Mnist数据集使用的图片尺寸很小,进行分类的神经网络结构也比较简单.在数据集和计算量都不大的情况下,该算法依然可以达到近似线性的加速,说明了算法产生的开销对整体的收敛影响很小.对于数据集较大的cifar10以及更复杂的卷积神经网络来,该算法产生的开销占比会更小,从而不会影响算法的收敛.</p>
                </div>
                <div class="p1">
                    <p id="202">从实验中可看出:同异步梯度下降算法以及<i>n</i>-soft算法相比,算法都去掉了等待时间,充分利用了全部的计算资源,但是本算法通过合理的任务分配,减少了梯度过时的影响,因此可以看到在相同的时间内收敛得更快,性能更好.</p>
                </div>
                <h3 id="203" name="203" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="204">为了解决深度学习数据并行过程中产生的梯度过时问题,本文设计的性能分析模型解决了分布式系统下各节点性能不易量化的问题.同时本文根据该模型提出了基于性能感知的DBS -SGD算法并从理论上证明了算法的收敛性.该算法通过性能感知模型实时检测各工作节点的计算性能,并为计算性能不同的工作节点动态分配相应工作量,从而解决了梯度过时问题,使神经网络收敛速度加快.实验结果表明,该算法使得分布式系统中各节点的单次迭代时间达到一致,解决了梯度过时问题.与传统的ASGD算法以及改进的<i>n</i>-soft算法相比,本文提出的DBS-SGD算法在相同的迭代次数下神经网络的收敛速度更快,同时该算法能够获得接近线性的加速比.</p>
                </div>
                <div class="p1">
                    <p id="205">未来的工作包括2个方向:1)如何为各节点的不等量任务分配相应的权值,以更好地量化各节点在每次提交梯度更新时所做出的贡献;2)结合第1个研究方向将本算法应用在同步更新策略上,可以使神经网络收敛的次数达到理论上的最低.</p>
                </div>
                <h3 id="375" name="375" class="anchor-tag">附录A</h3>
                <div class="p1">
                    <p id="376">采用参数平均法策略的梯度下降算法在单节点和分布式条件下具有一致性．</p>
                </div>
                <div class="p1">
                    <p id="377">证明：假设1个集群有n个工作节点，每个节点处理m个训练样本，则总的样本数为m×n个，所以如果在单机上训练m×n个样本，学习率设为η，权值的更新公式为</p>
                </div>
                <div class="area_img" id="378">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201911010_37800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="379">在分布式训练的情况下，假设有n个节点，每个节点处理m个训练样本，则得到：</p>
                </div>
                <div class="area_img" id="380">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201911010_38000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="381">证毕．</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="264">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201309002&amp;v=MDcwNjNVUkxPZVplUnNGeXptVmIzS0x5dlNkTEc0SDlMTXBvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Yu Kai,Jia Lei,Chen Yuqiang,et al.Deep learning:Yesterday,today,and tomorrow[J].Journal of Computer Research and Development,2013,50(9):1799- 1804 (in Chineses)(余凯,贾磊,陈雨强,等.深度学习的昨天、今天和明天[J].计算机研究与发展,2013,50(9):1799- 1804)
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Learning&amp;quot;">

                                <b>[2]</b>Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436- 444
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering">

                                <b>[3]</b>Chen Kai,Huo Qiang.Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering[C] //Proc of IEEE Int Conf on Acoustics Speech and Signal Processing.Piscataway,NJ:IEEE,2016:5880- 5884
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[4]</b>He Kaiming,Zhang Xiangyu,Ren Shaoqing,et al.Deep residual learning for image recognition[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:770- 778
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic occupancy grid prediction for urban autonomous driving:A deep learning approach with fully automatic labeling">

                                <b>[5]</b>Hoermann S,Bach M,Dietmayer K.Dynamic occupancy grid prediction for urban autonomous driving:A deep learning approach with fully automatic labeling[C] //Proc of 2018 IEEE Int Conf on Robotics and Automation.Piscataway,NJ:IEEE,2018:2056- 2063
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Large Scale Distributed Deep Networks &amp;quot;">

                                <b>[6]</b>Dean J,Greg S C,Rajat M,et al.Large scale distributed deep networks[C] //Proc of the 25th Int Conf on Neural Information Processing Systems.New York:ACM,2012:1223- 1231
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Project Adam:Building an Efficient and Scalable Deep Learning Training System">

                                <b>[7]</b>Chilimbi T,Suzue Y,Apacible J,et al.Project adam:Building an efficient and scalable deep learning training system[C] //Proc of the 11th USENIX Conf on Operating Systems Design and Implementation.Berkeley,CA:USENIX Association,2014:571- 582
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs">

                                <b>[8]</b>Seide F,Fu Hao,Droppo J,et al.1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs[C] //Proc of the 15th Annual Conf of the Int Speech Communication Association.Piscataway,NJ:IEEE,2014:1058- 1062
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable distributed DNN training using commodity GPU cloud computing">

                                <b>[9]</b>Strom K.Scalable distributed DNN training using commodity GPU cloud computing[C] //Proc of the 16th Annual Conf of the Int Speech Communication Association.Piscataway,NJ:IEEE,2015:1488- 1492
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model accuracy and runtime tradeoff in distributed deep learning:A systematic study">

                                <b>[10]</b>Gupta S,Zhang Wei,Wang Fei.Model accuracy and runtime tradeoff in distributed deep learning:A systematic study[C] //Proc of the 16th IEEE Int Conf on Data Mining.Piscataway,NJ:IEEE,2016:171- 180
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep learning with COTS HPC systems.&amp;quot;">

                                <b>[11]</b>Coates A,Huval B,Wang Tao,et al.Deep learning with cots HPC systems[C] //Proc of the 30th Int Conf on Machine Learning.New York:ACM,2013:1337- 1345
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel neural network training on Multi-Spert">

                                <b>[12]</b>Faerber P,Asanovic K.Parallel neural network training on Multi-Spert[C] //Proc of the 3rd Int Conf on Algorithms and Architectures for Parallel Processing.Piscataway,NJ:IEEE,1997:659- 666
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Demystifying parallel and distributed deep learning:An in-depth concurrency analysis">

                                <b>[13]</b>Ben-Nun T,Torsten H.Demystifying parallel and distributed deep learning:An in-depth concurrency analysis[J].arXiv preprint arXiv:1802.09941,2018
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale machine learning with stochastic gradient descent">

                                <b>[14]</b>Bottou L.Large-scale machine learning with stochastic gradient descent[C] //Proc of the 19th Int Conf on Computational Statistics.Berlin:Springer,2010:177- 186
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Can decentralized algorithms outperform centralized algorithms?A case study for decentralized parallel stochastic gradient descent">

                                <b>[15]</b>Lian Xiangru,Zhang Ce,Zhang Huan,et al.Can decentralized algorithms outperform centralized algorithms?A case study for decentralized parallel stochastic gradient descent[C] //Proc of the 31st Int Conf on Neural Information Processing Systems.New York:ACM,2017:5336- 5346
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hop:Heterogeneity-aware decentralized training">

                                <b>[16]</b>Luo Qinyi,Lin Jinkun,Zhuo Youwei,et al.Hop:Heterogeneity-aware decentralized training[J].arXiv preprint arXiv:1902.01064,2019
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scaling distributed machine learning with the parameter server">

                                <b>[17]</b>Li Mu,Andersen D G,Park J W,et al.Scaling distributed machine learning with the parameter server[C] //Proc of the 11th USENIX Conf on Operating Systems Design and Implementation.Berkeley,CA:USENIX Association,2014:583- 598
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GeePS:Scalable deep learning on distributed GPUs with a GPU-specialized parameter server">

                                <b>[18]</b>Cui Henggang,Zhang Hao,Ganger G R,et al.GeePS:Scalable deep learning on distributed GPUs with a GPU-specialized parameter server[C] //Proc of the 11th European Conf on Computer Systems.New York:ACM,Piscataway,NJ:IEEE,2016:4
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Slow learners are aast">

                                <b>[19]</b>Langford J,Smola A,Zinkevich M.Slow learners are fast[C] //Proc of the 22nd Int Conf on Neural Information Processing Systems.New York:ACM,2009:2331- 2339
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FireCaffe:Near-linear acceleration of deep neural network training on compute clusters">

                                <b>[20]</b>Iandola F N,Moskewicz M W,Ashraf K,et al.FireCaffe:Near-linear acceleration of deep neural network training on compute clusters[C] //Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE,2016:2592- 2600
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asynchronous stochastic gradient descent with delay compensation">

                                <b>[21]</b>Zheng Shuxin,Qi Meng,Wang Taifeng,et al.Asynchronous stochastic gradient descent with delay compensation[C] //Proc of the 34th Int Conf on Machine Learning.New York:ACM,2017:4120- 4129
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=More effective distributed ml via a stale synchronous parallel parameter server">

                                <b>[22]</b>Huo Qirong,James C,Jin K,et al.More effective distributed ML via a stale synchronous parallel parameter server[C] //Proc of the 26th Int Conf on Neural Information Processing Systems.Cambridge,MA:MIT Press,2013:1223- 1231
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staleness-aware async-SGD for distributed deep learning">

                                <b>[23]</b>Zhang Wei,Gupta S,Lian Xiangru,et al.Staleness-aware async-SGD for distributed deep learning[C] //Proc of the 25th Int Joint Conf on Artificial Intelligence.New York:ACM,2016:2350- 2356
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Revisiting distributed synchronous SGD">

                                <b>[24]</b>Chen Jianmin,Pan Xinghao,Monga R,et al.Revisiting distributed synchronous SGD[J].arXiv preprint arXiv:1604.00981,2017
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Heterogeneity-aware distributed parameter servers">

                                <b>[25]</b>Jiang Jiawei,Cui Bin,Zhang Ce,et al.Heterogeneity-aware distributed parameter servers[C] //Proc of the 2017 ACM Int Conf on Management of Data.New York:ACM,2017:463- 478
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Experiments on parallel training of deep neural network using model averaging">

                                <b>[26]</b>Su Hang,Chen Haoyu.Experiments on parallel training of deep neural network using model averaging[J].arXiv preprint arXiv:1507.01239,2018
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupling adaptive batch sizes with learning rates">

                                <b>[27]</b>Balles L,Romero J,Hennig P.Coupling adaptive batch sizes with learning rates[C] //Proc of the 33rd Conf on Uncertainty in Artificial Intelligence.Vabcouver,BC:AUAI,2017:410- 419
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Stochastic Approximation Approach to Stochastic Programming">

                                <b>[28]</b>Nemirovski S,Juditsky B,Lan G,et al.Robust stochastic approximation approach to stochastic programming[J].SIAM Journal on Optimization,2008,19(4):1574- 1609
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201911010" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201911010&amp;v=MzI2NzZxQnRHRnJDVVJMT2VaZVJzRnl6bVZiM0tMeXZTZExHNEg5ak5ybzlFWklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDZ3NE9EZVZqSUNKR21CbkRKMGpjST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

