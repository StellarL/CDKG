<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133238111846250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201903019%26RESULT%3d1%26SIGN%3dDNUMA7JwDll8Nbx%252bYg2vgA7%252fkbc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201903019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201903019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201903019&amp;v=MjM3NTdyL0pMeXZTZExHNEg5ak1ySTlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;1 背景和相关工作&lt;/b&gt; "><b>1 背景和相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;1.1 随机多臂赌博机问题&lt;/b&gt;"><b>1.1 随机多臂赌博机问题</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;1.2 估计值函数&lt;/b&gt;"><b>1.2 估计值函数</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;1.3 动作选择方法&lt;/b&gt;"><b>1.3 动作选择方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="&lt;b&gt;2 算法描述&lt;/b&gt; "><b>2 算法描述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#119" data-title="&lt;b&gt;2.1 CNAME算法描述&lt;/b&gt;"><b>2.1 CNAME算法描述</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;2.2 CNAME算法分析&lt;/b&gt;"><b>2.2 CNAME算法分析</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;2.3 CNAME算法的悔值上界证明&lt;/b&gt;"><b>2.3 CNAME算法的悔值上界证明</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#224" data-title="&lt;b&gt;3 实验部分&lt;/b&gt; "><b>3 实验部分</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#226" data-title="&lt;b&gt;3.1 实验1:CNAME算法中参数&lt;i&gt;w&lt;/i&gt;的影响&lt;/b&gt;"><b>3.1 实验1:CNAME算法中参数<i>w</i>的影响</b></a></li>
                                                <li><a href="#244" data-title="&lt;b&gt;3.2 实验2:随机数据集上的比较&lt;/b&gt;"><b>3.2 实验2:随机数据集上的比较</b></a></li>
                                                <li><a href="#265" data-title="&lt;b&gt;3.3 实验3:内容分发网络数据集上的比较&lt;/b&gt;"><b>3.3 实验3:内容分发网络数据集上的比较</b></a></li>
                                                <li><a href="#276" data-title="&lt;b&gt;3.4 实验4:雅虎公司 R6A数据集上的比较&lt;/b&gt;"><b>3.4 实验4:雅虎公司 R6A数据集上的比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#282" data-title="&lt;b&gt;4 总 结&lt;/b&gt; "><b>4 总 结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#233" data-title="图1 不同&lt;i&gt;w&lt;/i&gt;值下CNAME算法的平均奖赏和平均悔值">图1 不同<i>w</i>值下CNAME算法的平均奖赏和平均悔值</a></li>
                                                <li><a href="#236" data-title="&lt;b&gt;表1 CNAME算法的平均奖赏和平均悔值 (&lt;i&gt;w&lt;/i&gt;∈[0.1, 1&lt;/b&gt;]) "><b>表1 CNAME算法的平均奖赏和平均悔值 (<i>w</i>∈[0.1, 1</b>]) </a></li>
                                                <li><a href="#248" data-title="&lt;b&gt;表2 6种算法的参数设置及实验结果&lt;/b&gt;"><b>表2 6种算法的参数设置及实验结果</b></a></li>
                                                <li><a href="#252" data-title="图2 CNAME算法与其他3种算法的平均奖赏">图2 CNAME算法与其他3种算法的平均奖赏</a></li>
                                                <li><a href="#253" data-title="图3 CNAME算法与其他3种算法的平均每轮的悔值">图3 CNAME算法与其他3种算法的平均每轮的悔值</a></li>
                                                <li><a href="#272" data-title="&lt;b&gt;表3 6种算法的平均延时&lt;/b&gt;"><b>表3 6种算法的平均延时</b></a></li>
                                                <li><a href="#279" data-title="图4 CNAME算法与其他3种算法的累计奖赏">图4 CNAME算法与其他3种算法的累计奖赏</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="338">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                    Sutton R, Barto G. Reinforcement Learning: An Introduction[M]. Cambridge, MA: MIT Press, 1998: 1- 20</a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_2" title="Gao Yang, Zhou Ruyi, Wang Hao, et al. Study on an average reward reinforcement learning algorithm[J]. Chinese Journal of Computers, 2007, 30 (8) : 1372- 1378 (in Chinese) (高阳, 周如益, 王皓, 等. 平均奖赏强化学习算法研究[J]. 计算机学报, 2007, 30 (8) : 1372- 1378) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX200708023&amp;v=MDk4MzJGeTduVXIvSkx6N0Jkckc0SHRiTXA0OUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Gao Yang, Zhou Ruyi, Wang Hao, et al. Study on an average reward reinforcement learning algorithm[J]. Chinese Journal of Computers, 2007, 30 (8) : 1372- 1378 (in Chinese) (高阳, 周如益, 王皓, 等. 平均奖赏强化学习算法研究[J]. 计算机学报, 2007, 30 (8) : 1372- 1378) 
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_3" title="Zhao Fengfei, Qin Zheng. A multi-motive reinforcement learning framework[J]. Journal of Computer Research and Development, 2013, 50 (2) : 240- 247 (in Chinese) (赵凤飞, 覃征. 一种多动机强化学习框架[J]. 计算机研究与发展, 2013, 50 (2) : 240- 247) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201302003&amp;v=MzA5MzJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci9KTHl2U2RMRzRIOUxNclk5Rlo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Zhao Fengfei, Qin Zheng. A multi-motive reinforcement learning framework[J]. Journal of Computer Research and Development, 2013, 50 (2) : 240- 247 (in Chinese) (赵凤飞, 覃征. 一种多动机强化学习框架[J]. 计算机研究与发展, 2013, 50 (2) : 240- 247) 
                                    </a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_4" title="Liu Quan, Fu Qiming, Yang Xudong, et al. A scalable parallel reinforcement learning method based on intelligent scheduling[J]. Journal of Computer Research and Development, 2013, 50 (4) : 843- 851 (in Chinese) (刘全, 傅启明, 杨旭东, 等. 一种基于智能调度的可扩展并行强化学习方法[J]. 计算机研究与发展, 2013, 50 (4) : 843- 851) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201304023&amp;v=MDQxNjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci9KTHl2U2RMRzRIOUxNcTQ5SFo0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Liu Quan, Fu Qiming, Yang Xudong, et al. A scalable parallel reinforcement learning method based on intelligent scheduling[J]. Journal of Computer Research and Development, 2013, 50 (4) : 843- 851 (in Chinese) (刘全, 傅启明, 杨旭东, 等. 一种基于智能调度的可扩展并行强化学习方法[J]. 计算机研究与发展, 2013, 50 (4) : 843- 851) 
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_5" title="Liu Zhibin, Zeng Xiaoqin, Liu Huiyi, et al. A heuristic two-layer reinforcement learning algorithm based on BP neural networks[J]. Journal of Computer Research and Development, 2015, 52 (3) : 579- 587 (in Chinese) (刘智斌, 曾晓勤, 刘惠义, 等. 基于BP神经网络的双层启发式强化学习方法[J]. 计算机研究与发展, 2015, 52 (3) : 579- 587) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201503004&amp;v=MDIxNTQ3blVyL0pMeXZTZExHNEg5VE1ySTlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Liu Zhibin, Zeng Xiaoqin, Liu Huiyi, et al. A heuristic two-layer reinforcement learning algorithm based on BP neural networks[J]. Journal of Computer Research and Development, 2015, 52 (3) : 579- 587 (in Chinese) (刘智斌, 曾晓勤, 刘惠义, 等. 基于BP神经网络的双层启发式强化学习方法[J]. 计算机研究与发展, 2015, 52 (3) : 579- 587) 
                                    </a>
                                </li>
                                <li id="348">


                                    <a id="bibliography_6" title="Robbins H. Some aspects of the sequential design of experiments[J]. American Mathematical Society, 1952, 58 (5) : 527- 535" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524057654&amp;v=MjU3MjE3bktJVjRWTmlmS1k3SzZIdFRPcTQ5QVkrMEtDQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDcmtV&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Robbins H. Some aspects of the sequential design of experiments[J]. American Mathematical Society, 1952, 58 (5) : 527- 535
                                    </a>
                                </li>
                                <li id="350">


                                    <a id="bibliography_7" title="Devanur N R, Kakade S M. The price of truthfulness for pay-per-click auctions[C] //Proc of ACM Conf on Electronic Commerce. New York: ACM, 2009: 99- 106" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The price of truthfulness for pay-per-click auctions">
                                        <b>[7]</b>
                                        Devanur N R, Kakade S M. The price of truthfulness for pay-per-click auctions[C] //Proc of ACM Conf on Electronic Commerce. New York: ACM, 2009: 99- 106
                                    </a>
                                </li>
                                <li id="352">


                                    <a id="bibliography_8" title="Cheng Shi, Mao Luhong, Chang Peng. Multi-armed bandit recommender algorithm with matrix factorization[J]. Journal of Chinese Computer Systems, 2017, 38 (12) : 2754- 2758 (in Chinese) (成石, 毛陆虹, 常鹏. 融合矩阵分解的多臂赌博机推荐算法[J]. 小型微型计算机系统, 2017, 38 (12) : 2754- 2758) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201712024&amp;v=MTcxODlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci9KUFRYY2RyRzRIOWJOclk5SFlJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Cheng Shi, Mao Luhong, Chang Peng. Multi-armed bandit recommender algorithm with matrix factorization[J]. Journal of Chinese Computer Systems, 2017, 38 (12) : 2754- 2758 (in Chinese) (成石, 毛陆虹, 常鹏. 融合矩阵分解的多臂赌博机推荐算法[J]. 小型微型计算机系统, 2017, 38 (12) : 2754- 2758) 
                                    </a>
                                </li>
                                <li id="354">


                                    <a id="bibliography_9" title="Yu Yonghong, Gao Yang, Wang Hao, et al. Integrating user social status and matrix factorization for item recommendation[J]. Journal of Computer Research and Development, 2018, 55 (1) : 113- 124 (in Chinese) (余永红, 高阳, 王皓, 等. 融合用户社会地位和矩阵分解的推荐算法[J]. 计算机研究与发展, 2018, 55 (1) : 113- 124) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201801009&amp;v=MTk5NzZVci9KTHl2U2RMRzRIOW5Ncm85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N24=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Yu Yonghong, Gao Yang, Wang Hao, et al. Integrating user social status and matrix factorization for item recommendation[J]. Journal of Computer Research and Development, 2018, 55 (1) : 113- 124 (in Chinese) (余永红, 高阳, 王皓, 等. 融合用户社会地位和矩阵分解的推荐算法[J]. 计算机研究与发展, 2018, 55 (1) : 113- 124) 
                                    </a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_10" title="Jain S, Gujar S, Zoeter O, et al. A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning[C] //Proc of the 13th Int Conf on Autonomous Agents and Multi-Agent Systems. New York: ACM, 2014: 1609- 1610" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning">
                                        <b>[10]</b>
                                        Jain S, Gujar S, Zoeter O, et al. A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning[C] //Proc of the 13th Int Conf on Autonomous Agents and Multi-Agent Systems. New York: ACM, 2014: 1609- 1610
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_11" title="Jain S, Narayanaswamy B, Narahari Y. A multi-armed bandit incentive mechanism for crowdsourcing demand response in smart grids[C] //Proc of the 28th AAAI Conf on Artificial Intelligence and the 26th Innovative Applications of Artificial Intelligence Conf. Menlo Park, CA: AAAI, 2014: 721- 727" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multi-armed bandit incentive mechanism for crowdsourcing demand response in smart grids">
                                        <b>[11]</b>
                                        Jain S, Narayanaswamy B, Narahari Y. A multi-armed bandit incentive mechanism for crowdsourcing demand response in smart grids[C] //Proc of the 28th AAAI Conf on Artificial Intelligence and the 26th Innovative Applications of Artificial Intelligence Conf. Menlo Park, CA: AAAI, 2014: 721- 727
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_12" title="Sun Xinxin. Research on task assignment technology in crowdsourcing environment[D]. Yangzhou: Yangzhou University, 2016 (in Chinese) (孙信昕. 众包环境下的任务分配技术研究[D]. 扬州: 扬州大学, 2016) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016290369.nh&amp;v=MzEzMjl0TEtwcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0pWRjI2R0xHeEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Sun Xinxin. Research on task assignment technology in crowdsourcing environment[D]. Yangzhou: Yangzhou University, 2016 (in Chinese) (孙信昕. 众包环境下的任务分配技术研究[D]. 扬州: 扬州大学, 2016) 
                                    </a>
                                </li>
                                <li id="362">


                                    <a id="bibliography_13" title="Zhou Baojian. Research on search engine keyword optimal selection strategy based on multi-armed bandit[D]. Harbin: Harbin Institute of Technology, 2014 (in Chinese) (周保健. 基于多臂赌博机的搜索引擎关键字最优选择策略研究[D]. 哈尔滨: 哈尔滨工业大学, 2014) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014082590.nh&amp;v=MDIzNjg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVXIvSlZGMjZHck93SE5URnI1RWJQSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        Zhou Baojian. Research on search engine keyword optimal selection strategy based on multi-armed bandit[D]. Harbin: Harbin Institute of Technology, 2014 (in Chinese) (周保健. 基于多臂赌博机的搜索引擎关键字最优选择策略研究[D]. 哈尔滨: 哈尔滨工业大学, 2014) 
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_14" title="Chen Hongcui. Research on channel selection mechanism based on multi-armed bandit in cognitive network[D]. Chongqing: Chongqing University of Posts and Telecommu-nications, 2016 (in Chinese) (陈红翠. 认知网络中基于赌博机模型的信道选择机制研究[D]. 重庆: 重庆邮电大学, 2016) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017700729.nh&amp;v=Mjg2OTJwcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0pWRjI2R2JTNEh0Yk8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Chen Hongcui. Research on channel selection mechanism based on multi-armed bandit in cognitive network[D]. Chongqing: Chongqing University of Posts and Telecommu-nications, 2016 (in Chinese) (陈红翠. 认知网络中基于赌博机模型的信道选择机制研究[D]. 重庆: 重庆邮电大学, 2016) 
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_15" title="Bubeck S, Cesabianchi N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems[J]. Foundations and Trends in Machine Learning, 2012, 5 (1) : 1- 22" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Regret analysis of stochastic and nonstochastic multi-armed bandit problems">
                                        <b>[15]</b>
                                        Bubeck S, Cesabianchi N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems[J]. Foundations and Trends in Machine Learning, 2012, 5 (1) : 1- 22
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_16" title="Slivkins A. Contextual bandits with similarity information[J]. Journal of Machine Learning Research, 2014, 15 (1) : 2533- 2568" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contextual bandits with similarity information">
                                        <b>[16]</b>
                                        Slivkins A. Contextual bandits with similarity information[J]. Journal of Machine Learning Research, 2014, 15 (1) : 2533- 2568
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_17" title="Watkins C J C H. Learning from delayed rewards[J]. Robotics &amp;amp; Autonomous Systems, 1989, 15 (4) : 233- 235" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011502000458&amp;v=MjE0MjViSzdIdEROcW85SFpPc1BDSGt4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSmx3VmFoTT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        Watkins C J C H. Learning from delayed rewards[J]. Robotics &amp;amp; Autonomous Systems, 1989, 15 (4) : 233- 235
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_18" title="Luce R D. Individual choice behavior[J]. American Economic Review, 1959, 67 (1) : 1- 15" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Individual choice behavior.">
                                        <b>[18]</b>
                                        Luce R D. Individual choice behavior[J]. American Economic Review, 1959, 67 (1) : 1- 15
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_19" title="Auer P, Cesa-Bianchi N, Fischer P. Finite-time analysis of the multiarmed bandit problem[J]. Machine Learning, 2002, 47 (2/3) : 235- 256" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340413&amp;v=MTA1NTVKSUY4PU5qN0Jhck80SHRITnJJdEZZT29NWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU25sVmIz&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        Auer P, Cesa-Bianchi N, Fischer P. Finite-time analysis of the multiarmed bandit problem[J]. Machine Learning, 2002, 47 (2/3) : 235- 256
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_20" title="Li Lihong, Chu Wei, Langford J, et al. A contextual-bandit approach to personalized news article recommendation[C] //Proc of ACM Int Conf on World Wide Web. New York: ACM, 2010: 661- 670" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A contextual-bandit approach to personalized news article recom-mendation">
                                        <b>[20]</b>
                                        Li Lihong, Chu Wei, Langford J, et al. A contextual-bandit approach to personalized news article recommendation[C] //Proc of ACM Int Conf on World Wide Web. New York: ACM, 2010: 661- 670
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_21" title="Chu Hongmin, Lin Hsuan Tien. Can active learning experience be transferred?[C] //Proc of IEEE Int Conf on Data Mining. Piscataway, NJ: IEEE, 2017: 841- 846" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Can active learning experience be transferred?">
                                        <b>[21]</b>
                                        Chu Hongmin, Lin Hsuan Tien. Can active learning experience be transferred?[C] //Proc of IEEE Int Conf on Data Mining. Piscataway, NJ: IEEE, 2017: 841- 846
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_22" title="Krishnamurthy B, Wills C, Zhang Yin. On the use and performance of content distribution networks[C] //Proc of the 1st ACM SIGCOMM Internet Measurement Workshop. New York: ACM, 2001: 169- 182" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the use and performance of content distribution networks">
                                        <b>[22]</b>
                                        Krishnamurthy B, Wills C, Zhang Yin. On the use and performance of content distribution networks[C] //Proc of the 1st ACM SIGCOMM Internet Measurement Workshop. New York: ACM, 2001: 169- 182
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_23" title="Thathachar M A L, Sastry P S. A new approach to the design of reinforcement schemes for learning automata[J]. IEEE Transactions on Systems Man &amp;amp; Cybernetics, 1985, SMC-15 (1) : 168- 175" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new approach to the design of reinforcement schemes for learning automata">
                                        <b>[23]</b>
                                        Thathachar M A L, Sastry P S. A new approach to the design of reinforcement schemes for learning automata[J]. IEEE Transactions on Systems Man &amp;amp; Cybernetics, 1985, SMC-15 (1) : 168- 175
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_24" title="Even-Dar E, Mannor S, Mansour Y. PAC bounds for multi-armed bandit and Markov decision processes[C] //Proc of the 15th Annual Conf on Computational Learning Theory (COLT) . Berlin: Springer, 2002: 255- 270" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PAC bounds for multi-armed bandit andMarkov decision processes">
                                        <b>[24]</b>
                                        Even-Dar E, Mannor S, Mansour Y. PAC bounds for multi-armed bandit and Markov decision processes[C] //Proc of the 15th Annual Conf on Computational Learning Theory (COLT) . Berlin: Springer, 2002: 255- 270
                                    </a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_25" title="Cesa-Bianchi N, Fischer P. Finite-time regret bounds for the multi-armed bandit problem[C] //Proc of the Int Conf on Machine Learning. New York: ACM, 1998: 100- 108" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finite-time regret bounds for the multi-armed bandit problem">
                                        <b>[25]</b>
                                        Cesa-Bianchi N, Fischer P. Finite-time regret bounds for the multi-armed bandit problem[C] //Proc of the Int Conf on Machine Learning. New York: ACM, 1998: 100- 108
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_26" title="Strens M. Learning Cooperation and feedback in pattern recognition[D]. London: Physics Department, King’s College London, 1999" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Cooperation and feedback in pattern recognition">
                                        <b>[26]</b>
                                        Strens M. Learning Cooperation and feedback in pattern recognition[D]. London: Physics Department, King’s College London, 1999
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_27" title="Vermorel J, Mohri M. Multi-armed bandit algorithms and empirical evaluation[C] //Proc of the European Conf on Machine Learning. Berlin: Springer, 2005: 437- 448" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-armed bandit algorithms and empirical evaluation">
                                        <b>[27]</b>
                                        Vermorel J, Mohri M. Multi-armed bandit algorithms and empirical evaluation[C] //Proc of the European Conf on Machine Learning. Berlin: Springer, 2005: 437- 448
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_28" title="Kirkpatrick B S, Gelatt C, Vecchi D. Optimization by simulated annealing[J]. Science, 1983, 220 (4598) : 671- 680" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimization by simulated annealing">
                                        <b>[28]</b>
                                        Kirkpatrick B S, Gelatt C, Vecchi D. Optimization by simulated annealing[J]. Science, 1983, 220 (4598) : 671- 680
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_29" title="Lai T L, Robbins H. Asymptotically efficient adaptive allocation rules[J]. Advances in Applied Mathematics, 1985, 6 (1) : 4- 22" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asymptotically efficient adaptive allocation rules">
                                        <b>[29]</b>
                                        Lai T L, Robbins H. Asymptotically efficient adaptive allocation rules[J]. Advances in Applied Mathematics, 1985, 6 (1) : 4- 22
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_30" title="Xia Yingce, Qin Tao, Ding Wenkui, et al. Finite budget analysis of multi-armed bandit problems[J]. Neurocomputing, 2017, 258: 13- 29" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD5FF7A03C039C43F122734C4919A852E&amp;v=MDM0MzQxNlMzdVJxQnMwY01PY1FManFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh3cnErd0trPU5pZk9mY2U5YUtmTDNvOUdGK3NNQlE4OXpHQVM2RA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                        Xia Yingce, Qin Tao, Ding Wenkui, et al. Finite budget analysis of multi-armed bandit problems[J]. Neurocomputing, 2017, 258: 13- 29
                                    </a>
                                </li>
                                <li id="398">


                                    <a id="bibliography_31" title="Sz&#246;r&#233;nyi B, Busa-Fekete R, Weng P. Qualitative multi-armed bandits: A quantile-based approach[C] //Proc of the 32nd Int Conf on Machine Learning. New York: ACM, 2015: 1660- 1668" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Qualitative multi-armed bandits: A quantile-based approach">
                                        <b>[31]</b>
                                        Sz&#246;r&#233;nyi B, Busa-Fekete R, Weng P. Qualitative multi-armed bandits: A quantile-based approach[C] //Proc of the 32nd Int Conf on Machine Learning. New York: ACM, 2015: 1660- 1668
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(03),643-654 DOI:10.7544/issn1000-1239.2019.20180019            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种自适应的多臂赌博机算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AB%A0%E6%99%93%E8%8A%B3&amp;code=21762538&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">章晓芳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E5%80%A9&amp;code=08849075&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周倩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A2%81%E6%96%8C&amp;code=28427035&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梁斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E8%BF%9B&amp;code=21991331&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐进</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0240077&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E4%BB%B6%E6%96%B0%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%AD%A6)&amp;code=0069758&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机软件新技术国家重点实验室(南京大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>多臂赌博机问题是强化学习中研究探索和利用两者平衡的经典问题, 其中, 随机多臂赌博机问题是最经典的一类多臂赌博机问题, 是众多新型多臂赌博机问题的基础.针对现有多臂赌博机算法未能充分使用环境反馈信息以及泛化能力较弱的问题, 提出一种自适应的多臂赌博机算法.该算法利用当前估计值最小的动作被选择的次数来调整探索和利用的概率 (chosen number of arm with minimal estimation, CNAME) , 有效缓解了探索和利用不平衡的问题.同时, 该算法不依赖于上下文信息, 在不同场景的多臂赌博机问题中有更好的泛化能力.通过理论分析给出了该算法的悔值 (regret) 上界, 并通过不同场景的实验结果表明:CNAME算法可以高效地获得较高的奖赏和较低的悔值, 并且具有更好的泛化能力.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%87%82%E8%B5%8C%E5%8D%9A%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多臂赌博机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">探索和利用;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9B%B8%E5%85%B3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上下文相关;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    章晓芳, xfzhang@suda.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61772263, 61772014, 61572375);</span>
                                <span>苏州市科技发展计划基金项目 (SYG201807);</span>
                    </p>
            </div>
                    <h1><b>An Adaptive Algorithm in Multi-Armed Bandit Problem</b></h1>
                    <h2>
                    <span>Zhang Xiaofang</span>
                    <span>Zhou Qian</span>
                    <span>Liang Bin</span>
                    <span>Xu Jin</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Soochow University</span>
                    <span>State Key Laboratory for Novel Software Technology (Nanjing University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>As an important ongoing field in machine learning, reinforcement learning has received extensive attention in recent years. The multi-armed bandit (MAB) problem is a typical problem of the exploration and exploitation dilemma in reinforcement learning. As a classical MAB problem, the stochastic multi-armed bandit (SMAB) problem is the base of many new MAB problems. To solve the problems of insufficient use of information and poor generalization ability in existing MAB methods, this paper presents an adaptive SMAB algorithm to balance exploration and exploitation based on the chosen number of arm with minimal estimation, namely CNAME in short. CNAME makes use of the chosen times and the estimations of an action at the same time, so that an action is chosen according to the exploration probability, which is updated adaptively. In order to control the decline rate of exploration probability, the parameter <i>w</i> is introduced to adjust the influence degree of feedback during the selection process. Furthermore, CNAME does not depend on contextual information, hence it has better generalization ability. The upper bound of CNAME's regret is theoretically proved and analyzed. Our experimental results in different scenarios show that CNAME can yield greater reward and smaller regret with high efficiency than commonly used methods. In addition, its generalization ability is very strong.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-armed%20bandit&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-armed bandit;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=exploration%20and%20exploitation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">exploration and exploitation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=contextual&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">contextual;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Zhang Xiaofang, born in 1980.PhD, associate professor, member of CCF.Her main research interests include reinforcement learning, machine learning, and software testing and analysis.<image id="330" type="formula" href="images/JFYZ201903019_33000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhou Qian, born in 1992.Master.Her main research interests include reinforcement learning, and deep reinforcement learning. (20154227029@stu.suda.edu.cn) <image id="331" type="formula" href="images/JFYZ201903019_33100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liang Bin, born in 1993.Master.His main research interests include sentiment analysis, natural language processing, and deep learning. (bliang@stu.suda.edu.cn) <image id="332" type="formula" href="images/JFYZ201903019_33200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Xu Jin, born in 1992.Master.His main research interests include reinforcement learning, deep learning and deep reinforcement learning. (20154227016@stu.suda.edu.cn) <image id="333" type="formula" href="images/JFYZ201903019_33300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-01-09</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61772263, 61772014, 61572375);</span>
                                <span>the Suzhou Science and Technology Development Project (SYG201807);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="72">强化学习 (reinforcement learning, RL) 是智能体从环境状态到动作映射的学习, 以使动作从环境中获得的累积奖赏值最大<citation id="400" type="reference"><link href="338" rel="bibliography" /><link href="340" rel="bibliography" /><link href="342" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>.强化学习通过试错与环境交互获得策略的改进, 这就导致一个问题:哪一种试错策略可以产生最有效的学习.因此强化学习面临探索 (exploration) 和利用 (exploitation) 的两难问题:长期来看, 探索可能获得更高的累积奖赏, 帮助智能体收敛到最优策略;而利用总是最大化短期奖赏, 但可能收敛到次优解上<citation id="401" type="reference"><link href="344" rel="bibliography" /><link href="346" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>.多臂赌博机 (multi-armed bandit, MAB) 问题是强化学习中研究探索与利用平衡的经典问题.</p>
                </div>
                <div class="p1">
                    <p id="73">随机多臂赌博机 (stochastic multi-armed bandit, SMAB) 问题是一类经典的MAB问题, 该问题最早由Robbins提出<citation id="402" type="reference"><link href="348" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.一个MAB问题中包括<i>K</i>个臂, 玩家每次只能选择其中一个臂, 并获得相应的奖赏.<i>n</i>轮游戏后, 玩家的目标是最大化奖赏的期望值.其中, 玩家选择某个臂之后只能知道该臂的奖赏, 且各个臂的奖赏是相互独立的, 服从某种未知的分布.为了达到目标, 玩家需要在获得奖赏的同时去学习该奖赏的分布.因此, 每个时间步, 玩家都必须在以下两者做出选择:利用, 选择目前为止已知奖赏最高的臂;探索, 尝试其他未来奖赏可能更高的臂.这样, 在游戏过程中玩家面临着探索与利用间的平衡问题.</p>
                </div>
                <div class="p1">
                    <p id="74">作为一个序列化的决策问题, MAB被应用于很多实际场景中, 最早的应用就是Robbins提出的诊治试验<citation id="403" type="reference"><link href="348" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.诊治试验中各个患者的治疗方案对应MAB问题中的各个臂.诊治试验的目标是最小化患者的健康损失.近来, MAB问题有了更多广泛的应用, 比如推荐算法<citation id="409" type="reference"><link href="350" rel="bibliography" /><link href="352" rel="bibliography" /><link href="354" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>、众包机制<citation id="410" type="reference"><link href="356" rel="bibliography" /><link href="358" rel="bibliography" /><link href="360" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>、搜索引擎关键字选择<citation id="404" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、网络信道选择<citation id="405" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等.除了SMAB问题, MAB问题还存在多个变种, 例如Markov赌博机 (Markovian bandit) <citation id="406" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 对抗式赌博机 (adversarial bandit) <citation id="407" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和上下文相关的赌博机 (contextual bandit) <citation id="408" type="reference"><link href="368" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>等.在上下文相关的赌博机问题中, 玩家选择一个臂之后, 除了获得相应的奖赏之外, 还可以获得该臂的上下文信息.本文将研究SMAB问题, 提出一种自适应的MAB算法, 并将其推广到上下文相关的场景中.</p>
                </div>
                <div class="p1">
                    <p id="75">目前已有很多策略来平衡MAB问题中探索与利用的难题.Watkins首次引入<i>ε</i> -贪心 (<i>ε</i> -greedy) 算法<citation id="411" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, <i>ε</i> -greedy算法以一个很小的概率<i>ε</i>进行探索, 以1-<i>ε</i>的概率利用.但是<i>ε</i> -greedy算法将在所有臂中均等选择进行探索, 没有利用选择臂之后获得的信息, 比如各个臂的平均奖赏和选择次数.软最大化 (SoftMax) 算法最早由Luce提出<citation id="412" type="reference"><link href="372" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, SoftMax算法是基于当前已知臂的平均奖赏来对探索和利用的程度进行折中.然而SoftMax算法只考虑了平均奖赏, 这种较为单一的信息可能导致错误的概率分配, 比如某个臂第一次被选择, 随机得到一个很低的奖赏, SoftMax算法根据这一次的奖赏为该臂分配了很低的选择概率, 但该臂有可能是初期奖赏低而实际平均奖赏很高的臂.置信上界 (upper-confidence-bound, UCB) 动作选择方法<citation id="413" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>则根据各个臂已知的平均奖赏和选择次数直接计算出要选择的臂, 不利用随机性来选择臂.使用UCB算法时必须在初期轮流选择所有臂各一次, 那么当实验次数小于等于臂的数量时, 难以使用UCB方法, 因此UCB方法泛化能力较弱.LinUCB算法<citation id="414" type="reference"><link href="376" rel="bibliography" /><link href="378" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>是经典的上下文相关的MAB算法, 利用上下文信息最大化奖赏的期望值.然而LinUCB算法只适用于上下文可用的场景中.</p>
                </div>
                <div class="p1">
                    <p id="76">本文针对上述随机多臂赌博机算法未能充分使用反馈信息以及泛化能力较弱的问题, 提出了一种自适应的多臂赌博机算法, 利用当前估计值最小的动作被选择的次数 (chosen number of arm with minimal estimation, CNAME) 来调整探索和利用的概率, 记为CNAME算法.本文的主要贡献有3个方面:</p>
                </div>
                <div class="p1">
                    <p id="77">1) 提出了一种自适应的随机多臂赌博机算法CNAME, 其探索概率由一个简单分式给出, 该分式由当前平均奖赏最小的臂被选择的次数和参数<i>w</i>构成, 充分利用了选择臂之后的信息.该算法不依赖上下文信息, 因此在各个应用场景中具有更强的推广能力和泛化能力.</p>
                </div>
                <div class="p1">
                    <p id="78">2) 通过理论分析给出了CNAME算法的悔值 (regret) 上界, 并与其他算法的悔值上界进行对比, 为CNAME算法提供了有力的效果保证.</p>
                </div>
                <div class="p1">
                    <p id="79">3) 通过实验研究给出了CNAME算法中关键参数的参考取值范围.在无上下文的随机数据集和内容分发网络 (content distribution network, CDN) <citation id="415" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>数据集以及带有上下文的雅虎公司R6A数据集上的实验结果表明:利用估计值最小的动作被选择的次数能有效平衡探索和利用, 并且在上下文相关的场景中依然表现良好.</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>1 背景和相关工作</b></h3>
                <h4 class="anchor-tag" id="81" name="81"><b>1.1 随机多臂赌博机问题</b></h4>
                <div class="p1">
                    <p id="82">随机多臂赌博机问题, 又称为<i>K</i>-臂赌博机问题, <i>K</i>即为臂的数量, 下文把各个臂统称为动作.<i>K</i>-臂赌博机的目标是最大化获得的累积奖赏.若时刻<i>t</i>动作<i>i</i>被选择, 则获得随机奖赏<i>X</i><sub><i>i</i>, <i>t</i></sub>.各个动作的奖赏相互独立且服从均值为<i>μ</i>=[<i>μ</i><sub>1</sub>, <i>μ</i><sub>2</sub>, …, <i>μ</i><sub><i>K</i></sub>]的某种分布, <i>μ</i><sub><i>i</i></sub>是动作<i>i</i>的真实值.假设已知各个动作的真实值, 只要一直选择有最高真实值的动作便可以获得最大的累积奖赏, 这种情况下多臂赌博机问题自然得解.然而, 事实上随机多臂赌博机问题可以通过获得的奖赏来估计各个动作的值, 但动作的真实值是未知的.</p>
                </div>
                <div class="p1">
                    <p id="83">悔值是<i>K</i>-臂赌博机的一种典型的度量标准<citation id="416" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>.在动作真实值未知的情况下, 任何算法都不可能每次都选到奖赏最高的动作, 悔值就是实际得到的奖赏与期望得到的最大奖赏之间的差值.给定<i>n</i>次操作后的悔值定义为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>n</mi><mtext> </mtext><mi>μ</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>E</mi></mstyle><mo stretchy="false">[</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo></mtd></mtr><mtr><mtd><mi>μ</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>Κ</mi></mrow></munder><mspace width="0.25em" /><mi>μ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">其中, <i>N</i><sub><i>n</i></sub> (<i>i</i>) 是前<i>n</i>次操作中动作<i>i</i>被选择的次数.悔值越小, 则该算法产生的策略越接近最优策略.</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>1.2 估计值函数</b></h4>
                <div class="p1">
                    <p id="87">动作的真实值是选择动作后期望得到的平均奖赏, 因此一种估计动作值的简单方法是计算选择动作之后实际获得的平均奖赏.假设第<i>n</i>次操作之后, 动作<i>i</i>已经被选择<i>N</i><sub><i>n</i></sub> (<i>i</i>) 次, 得到的奖赏分别是<i>r</i><sub>1</sub>, <i>r</i><sub>2</sub>, …, <i>r</i><sub><i>N</i><sub><i>n</i></sub> (<i>i</i>) </sub>, 这时动作<i>i</i>的估计值为</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>r</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>.      (2) </p>
                </div>
                <div class="p1">
                    <p id="90"><i>Q</i><sub><i>n</i>+1</sub> (<i>i</i>) 是到第<i>n</i>次操作之后的动作<i>i</i>的估计值, 即动作<i>i</i>实际获得的平均奖赏.根据式 (2) , <i>Q</i><sub><i>n</i>+1</sub> (<i>i</i>) 的更新需要记录每次选择动作<i>i</i>之后获得的奖赏, 为了减小需要的存储空间, 估计值的更新可以转化为增量式的更新:</p>
                </div>
                <div class="p1">
                    <p id="91"><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Q</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>+</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></mfrac><mo stretchy="false">[</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub><mo>-</mo><mi>Q</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></math></mathml>.      (3) </p>
                </div>
                <div class="p1">
                    <p id="93">根据式 (3) , 估计值的更新只需要存储立即奖赏和上一时刻的估计值.若<i>N</i><sub><i>n</i></sub> (<i>i</i>) =0, <i>Q</i><sub><i>n</i></sub> (<i>i</i>) 为初始值;当<i>N</i><sub><i>n</i></sub> (<i>i</i>) →∞时, 根据大数定理可知<i>Q</i><sub><i>n</i></sub> (<i>i</i>) 收敛到动作<i>i</i>的真实值.这种方法叫作抽样平均 (sample average) <citation id="417" type="reference"><link href="382" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 每个估计值都是奖赏的简单平均值.因此, 通过以上方法得到动作的估计值后, 可以根据估计值作出动作选择.</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>1.3 动作选择方法</b></h4>
                <div class="p1">
                    <p id="95">本节主要介绍已有的3类经典随机多臂赌博机算法.</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">1.3.1 <i>ε</i> -greedy方法及其变种</h4>
                <div class="p1">
                    <p id="97">最简单的动作选择方法是一直选择估计值最高的动作, 即贪心动作<i>a</i><sup>*</sup>, 这种方法称为贪心 (greedy) 方法<citation id="418" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.贪心方法利用当前知识最大化立即奖赏, 没有探索的过程.</p>
                </div>
                <div class="p1">
                    <p id="98">一个简单的代替贪心的办法是在大多数时间内采用<i>a</i><sup>*</sup>, 但是以一个很小的概率<i>ε</i>进行探索, 即与估计动作值无关地随机均匀地选择一个动作, 该方法为<i>ε</i> -greedy<citation id="419" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, <i>ε</i>位于开区间 (0, 1) 上.</p>
                </div>
                <div class="p1">
                    <p id="99"><i>ε</i> -greedy算法的一个变种是<i>ε</i> -优先 (<i>ε</i> -first) 算法<citation id="420" type="reference"><link href="384" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, <i>ε</i> -first算法在实验初期先做完所有的探索工作.例如对于一个给定实验次数为<i>n</i>的实验, 在最初的<i>εn</i>次实验中随机选择动作, 即纯探索阶段, 在余下的 (1-<i>ε</i>) <i>n</i>次实验中, 选择平均奖赏最高的动作, 即纯利用阶段.</p>
                </div>
                <div class="p1">
                    <p id="100"><i>ε</i> -greedy算法的另一个变种是<i>ε</i> -递减 (<i>ε</i> -decreasing) 算法<citation id="421" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.<i>ε</i> -decreasing算法用一个递减的概率来接近最优策略, 递减变量<i>ε</i><sub><i>t</i></sub>由<i>ε</i><sub><i>t</i></sub>=min{1, <i>ε</i><sub>0</sub>/<i>t</i>}给出, 其中<i>t</i>是时间步, 初始参数<i>ε</i><sub>0</sub>&gt;0.此外, Cesa-Bianchi等人<citation id="422" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>提出了混合贪心 (greedy mix) 算法, 用递减因子1/ln <i>t</i>代替了<i>ε</i> -decreasing算法的递减因子1/<i>t</i>.类似地, Strens<citation id="423" type="reference"><link href="388" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>提出了Least Taken算法, Vermorel等人<citation id="424" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>则在Least Taken算法的基础上添加了初始参数<i>ε</i><sub>0</sub>.</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">1.3.2 SoftMax方法及其变种</h4>
                <div class="p1">
                    <p id="102">尽管<i>ε</i> -greedy算法是一类有效的方法, 但是<i>ε</i> -greedy算法在探索的时候是在所有的动作中均等选择.这就意味着<i>ε</i> -greedy算法选择下一个动作时有可能选到最差的动作.为了避免这种情况, SoftMax方法将探索概率更改为与估计值相关的一个分级函数:贪心动作仍然具有最高的选择概率, 其他动作则根据其估计值进行分级并分配权重.SoftMax算法广泛使用Gibbs或Boltzmann分布, 时刻<i>t</i>选择动作<i>i</i>的概率为</p>
                </div>
                <div class="p1">
                    <p id="103"><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>/</mo><mi>τ</mi></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo>/</mo><mi>τ</mi></mrow></msup></mrow></mfrac></mrow></math></mathml>,      (4) </p>
                </div>
                <div class="p1">
                    <p id="105">其中, <i>τ</i>是温度 (temperature) 参数<citation id="425" type="reference"><link href="392" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>.高温可以使得各动作的选择概率趋于相等, 低温则使具有不同估计值的动作的选择概率趋于不同.当<i>τ</i>→0时, SoftMax算法的作用与greedy方法一致.</p>
                </div>
                <div class="p1">
                    <p id="106">与<i>ε</i> -greedy算法相似, SoftMax算法可以变形为递减的软最大化 (decreasing SoftMax) 算法<citation id="426" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>, decreasing SoftMax算法的温度随时间步的增加而递减, 即<i>τ</i><sub><i>t</i></sub>=<i>τ</i><sub>0</sub>/<i>t</i>.类似地, 可以用递减因子1/ln <i>t</i>代替decreasing SoftMax算法的递减因子1/<i>t</i><citation id="427" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.作为一个更复杂的SoftMax算法变种, Exp3算法的关键在于利用了累计奖赏与动作选择概率的比值<citation id="428" type="reference"><link href="386" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">1.3.3 UCB方法</h4>
                <div class="p1">
                    <p id="108">由于动作值的估计具有不确定性, 因此, 需要在动作选择过程中引入探索.然而, 非贪心动作中有接近贪心的动作, 也有很差的动作.如果能根据非贪心动作成为最优动作的可能性进行探索, 探索的效果往往会更好.</p>
                </div>
                <div class="p1">
                    <p id="109">基于上述的思想, UCB算法动作选择方法同时考虑非贪心动作估计值对最优动作估计值的接近程度和估计值的不确定性:</p>
                </div>
                <div class="p1">
                    <p id="110"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>i</mi></munder><mrow><mo>[</mo><mrow><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>+</mo><mi>c</mi><msqrt><mrow><mfrac><mrow><mi>ln</mi><mtext> </mtext><mi>t</mi></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt></mrow><mo>]</mo></mrow></mrow></math></mathml>,      (5) </p>
                </div>
                <div class="p1">
                    <p id="112">其中, <i>t</i>是时间步, <i>A</i><sub><i>t</i></sub>是时刻<i>t</i>应该被选择的动作, <i>N</i><sub><i>t</i></sub> (<i>i</i>) 表示动作<i>i</i>在时间步<i>t</i>之前已经被选择的次数, 参数<i>c</i>&gt;0控制探索的程度.若<i>N</i><sub><i>t</i></sub> (<i>i</i>) =0, 则把动作<i>i</i>看作最优动作<i>A</i><sub><i>t</i></sub>.参数<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><msqrt><mn>2</mn></msqrt></mrow></math></mathml>时, 为UCB1算法<citation id="429" type="reference"><link href="374" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>.本文后续对比实验中采用的是UCB1算法.</p>
                </div>
                <div class="p1">
                    <p id="114">雅虎公司的科学家在UCB算法的基础上引入了上下文信息, 提出了LinUCB算法<citation id="430" type="reference"><link href="376" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 并应用于雅虎公司的新闻推荐中.LinUCB算法假设一个玩家选择一个动作之后, 其奖赏和上下文信息成线性关系.于是学习过程就变成:用玩家和动作的上下文信息预估奖赏及其置信区间, 选择置信区间上界最大的动作, 观察奖赏后更新线性关系的参数, 以此达到最大化奖赏期望值的目的.</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">1.3.4 已有算法存在的问题</h4>
                <div class="p1">
                    <p id="116">基于上述讨论可知:<i>ε</i> -greedy算法是一个简单有效的方法, 其存在的问题是在所有动作中均等探索, 没有利用选择动作后获得的反馈信息.SoftMax算法仅基于当前已知动作的估计值对各动作的选择概率进行分级, 因此SoftMax算法容易被误导.<i>ε</i> -greedy算法和SoftMax算法都仅需要设置一个参数, 计算相对简单.与上述2种方法不同的是, UCB动作选择方法充分利用了动作的估计值和被选择次数, 每次都根据已有信息直接计算出要选择的动作, 其计算开销相对较大.另外, UCB动作选择方法必须在实验初期轮流选择所有动作各一次, 因此当实验次数小于等于动作的数量时, UCB动作选择方法将不适用.LinUCB算法则只适用于上下文信息可用的场景中, 实际应用中存在着大量没有先验信息的环境, 此时LinUCB算法的使用将受限.</p>
                </div>
                <h3 id="117" name="117" class="anchor-tag"><b>2 算法描述</b></h3>
                <div class="p1">
                    <p id="118">本文针对已有的随机多臂赌博机算法存在的不足, 提出了一种自适应的随机多臂赌博机算法 (CNAME) .</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>2.1 CNAME算法描述</b></h4>
                <div class="p1">
                    <p id="120">探索概率的变化将直接影响探索和利用的平衡.针对该问题, 本文提出一种新的探索概率计算方法:</p>
                </div>
                <div class="p1">
                    <p id="121"><i>p</i>=<i>w</i> / (<i>w</i>+<i>m</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup></mrow></math></mathml>) , </p>
                </div>
                <div class="p1">
                    <p id="123">其中, <i>m</i><sub><i>t</i></sub>是当前平均奖赏最小的动作被选择的次数, 参数<i>w</i>&gt;0用来控制探索概率衰减的速率, 起到宏观调控的作用.<i>w</i>和<i>m</i><sub><i>t</i></sub>共同控制探索概率.该计算方法可以充分利用选择动作之后的信息, 根据环境自适应地调整探索概率, 平衡探索和利用.本文3.1节将通过实验讨论参数<i>w</i>对CNAME算法效果的影响及其参考取值范围.</p>
                </div>
                <div class="p1">
                    <p id="124">CNAME算法以<i>w</i>/ (<i>w</i>+<i>m</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup></mrow></math></mathml>) 的概率选择目前最少被选择的动作, 即探索;或者选择当前估计值最大的动作, 即利用.具体如算法1所示:</p>
                </div>
                <div class="p1">
                    <p id="126"><b>算法1</b>. CNAME算法.</p>
                </div>
                <div class="p1">
                    <p id="127">输入:动作集合;</p>
                </div>
                <div class="p1">
                    <p id="128">输出:估计值.</p>
                </div>
                <div class="p1">
                    <p id="129">① 设置参数<i>w</i>;</p>
                </div>
                <div class="p1">
                    <p id="130">② for 每个动作<i>i</i>∈[1, 2, …, <i>K</i>] do</p>
                </div>
                <div class="p1">
                    <p id="131">③ <i>Q</i><sub>0</sub> (<i>i</i>) =0;</p>
                </div>
                <div class="p1">
                    <p id="132">④ <i>N</i><sub>0</sub> (<i>i</i>) =0;</p>
                </div>
                <div class="p1">
                    <p id="133">⑤ end for</p>
                </div>
                <div class="p1">
                    <p id="134">⑥ for 每个时间步<i>t</i>≤<i>n</i> do</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>⑦</mi><mspace width="0.25em" /><mtext> </mtext><mi>m</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>Ν</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>i</mi></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>⑧</mi><mspace width="0.25em" /><mtext> </mtext><mi>p</mi><mo>=</mo><mfrac><mi>w</mi><mrow><mi>w</mi><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup></mrow></mfrac><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">⑨ 在开区间 (0, 1) 上产生一个随机数<i>x</i>;</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>⑩</mi><mspace width="0.25em" /><mtext> </mtext><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo>←</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>i</mi></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>x</mi><mo>&gt;</mo><mi>p</mi><mo>;</mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>i</mi></munder><mspace width="0.25em" /><mi>Ν</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mo>;</mo></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139"> (11) 获得立即奖赏<i>X</i><sub><i>a</i><sub><i>t</i></sub>, <i>t</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="140"> (12) <i>N</i><sub><i>t</i></sub> (<i>a</i><sub><i>t</i></sub>) =<i>N</i><sub><i>t</i>-1</sub> (<i>a</i><sub><i>t</i></sub>) +1;</p>
                </div>
                <div class="p1">
                    <p id="141"> (13) <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Q</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo stretchy="false">[</mo><mi>X</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>t</mi></mrow></msub><mo>-</mo></mrow></math></mathml><i>Q</i><sub><i>t</i>-1</sub> (<i>a</i><sub><i>t</i></sub>) ];</p>
                </div>
                <div class="p1">
                    <p id="144"> (14) end for</p>
                </div>
                <div class="p1">
                    <p id="145">根据算法1, 步骤①首先设置CNAME算法的关键参数<i>w</i>, 参数<i>w</i>影响着探索概率衰减的速率.步骤②～⑤初始化动作的估计值和被选择的次数, 即<i>Q</i><sub>0</sub> (<i>i</i>) =0, <i>N</i><sub>0</sub> (<i>i</i>) =0.这里也可以使用乐观初始值, 比如<i>Q</i><sub>0</sub> (<i>i</i>) =1, 乐观初始值在实验初期可以临时地促进探索<citation id="431" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.步骤⑥～⑩为动作选择的过程, <i>n</i>是实验次数, <i>m</i><sub><i>t</i></sub>是时刻<i>t</i>估计值最小的动作已经被选择的次数, <i>p</i>是当前的探索概率.步骤⑩在选择动作时, 同时考虑了实际获得的平均奖赏和动作选择的次数.步骤 (11) 是选择动作<i>a</i><sub><i>t</i></sub>后获得立即奖赏<i>X</i><sub><i>a</i><sub><i>t</i></sub>, <i>t</i></sub>, 该奖赏服从某种概率分布.步骤 (12) (13) 对动作被选择的次数和估计值进行更新, 其中, 估计值更新方法采用的是增量式抽样平均方法.</p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>2.2 CNAME算法分析</b></h4>
                <div class="p1">
                    <p id="147">在任何一个特定环境下, 探索的效果更优还是利用的效果更优取决于估计的精确值、环境不确定性和剩余操作次数等复杂的具体情况<citation id="432" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="148">CNAME算法充分利用了用户反馈, 即动作估计值和动作选择的次数, 自适应地根据实际情况来调整探索概率.同时, 通过参数<i>w</i>控制探索概率下降的速率, 并根据<i>m</i><sub><i>t</i></sub>来改变探索概率.具体从3个方面分析:</p>
                </div>
                <div class="p1">
                    <p id="149">1) 当参数<i>w</i>的取值较大时, <i>m</i><sub><i>t</i></sub>对探索概率的影响较小;当参数<i>w</i>的取值较小时, <i>m</i><sub><i>t</i></sub>对探索概率的影响则较大.由于不同环境中<i>m</i><sub><i>t</i></sub>的实际影响也不同, 本文设计参数<i>w</i>来削弱或者增强<i>m</i><sub><i>t</i></sub>在调整探索概率时起到的作用.</p>
                </div>
                <div class="p1">
                    <p id="150">2) 每当<i>m</i><sub><i>t</i></sub>增加一次, 说明估计值最小的动作又被选择了一次, 估计值最小的动作是对整个累积奖赏贡献最少的动作, 应尽量减少该动作被选择的次数.随着<i>m</i><sub><i>t</i></sub>的增加, 探索概率降低, 贪心动作的选择概率增加, 这样有助于提高实际获得的累积奖赏.</p>
                </div>
                <div class="p1">
                    <p id="151">3) CNAME算法在探索时选取的是当前最少被选择的动作, 这样就保证每个动作都有被选择到的机会, 同时避免了随机选择动作的盲目性.</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>2.3 CNAME算法的悔值上界证明</b></h4>
                <div class="p1">
                    <p id="153">基于2.1～2.2节的算法描述和分析, 本节将通过证明给出CNAME算法的悔值上界.悔值用来评价算法的好坏, 悔值上界越小, 说明该算法的效果越好.</p>
                </div>
                <div class="p1">
                    <p id="154">Lai等人<citation id="433" type="reference"><link href="394" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>发现, 对于均值为单一参数的奖赏分布, 所有算法都满足:</p>
                </div>
                <div class="p1">
                    <p id="155"><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>≥</mo><mfrac><mrow><mi>ln</mi><mtext> </mtext><mi>n</mi></mrow><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mi>p</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>,      (6) </p>
                </div>
                <div class="p1">
                    <p id="157">其中, <mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mo>‖</mo><mrow><mi>p</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></mrow><mo stretchy="false">) </mo><mo>=</mo></mrow></math></mathml>∫<sub><i>X</i></sub><i>p</i><sub><i>i</i></sub> (<i>x</i>) ln <i>p</i><sub><i>i</i></sub> (<i>x</i>) /<i>p</i><sup>*</sup> (<i>x</i>) d<i>x</i>是<i>p</i><sub><i>i</i></sub>和<i>p</i><sup>*</sup>之间的KL散度, <i>p</i><sub><i>i</i></sub>指任意次优动作<i>i</i>在随机变量<i>X</i>上的概率分布, <i>p</i><sup>*</sup>指最优动作在随机变量<i>X</i>上的概率分布, <i>x</i>∈<i>X</i>.KL散度可度量<i>p</i><sub><i>i</i></sub>和<i>p</i><sup>*</sup>的接近程度, 2个概率分布越接近, KL散度值越小, 则式 (1) 中的悔值也越小, <i>p</i><sub><i>i</i></sub>越接近最优策略<i>p</i><sup>*</sup>.</p>
                </div>
                <div class="p1">
                    <p id="159">对于某个算法A, 根据式 (1) (6) , 若已知算法中<i>p</i><sub><i>i</i></sub>的上界, 便可得到算法A的悔值上界.为了方便证明, 给出已知的2个不等式:</p>
                </div>
                <div class="p1">
                    <p id="160"><b>引理1</b>. Chernoff-Hoeffding不等式.</p>
                </div>
                <div class="p1">
                    <p id="161">设<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>为[0, 1]之间的随机变量, 且<i>E</i>[<i>X</i><sub><i>t</i></sub>|<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>t</i>-1</sub>]=<i>μ</i>, <i>S</i><sub><i>n</i></sub>=<i>X</i><sub>1</sub>+<i>X</i><sub>2</sub>+…+<i>X</i><sub><i>n</i></sub>.那么对于所有的<i>a</i>≥0都有:</p>
                </div>
                <div class="p1">
                    <p id="162" class="code-formula">
                        <mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mi>S</mi><msub><mrow></mrow><mi>n</mi></msub><mo>≥</mo><mi>n</mi><mtext> </mtext><mi>μ</mi><mo>+</mo><mi>a</mi><mo stretchy="false">}</mo><mo>≤</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mn>2</mn><mi>a</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>n</mi></mfrac></mrow><mo>) </mo></mrow><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="163"><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mi>S</mi><msub><mrow></mrow><mi>n</mi></msub><mo>≤</mo><mi>n</mi><mtext> </mtext><mi>μ</mi><mo>-</mo><mi>a</mi><mo stretchy="false">}</mo><mo>≤</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mn>2</mn><mi>a</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>n</mi></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="165"><b>引理2</b>. Bernstein不等式.</p>
                </div>
                <div class="p1">
                    <p id="166">设<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>n</i></sub>为[0, 1]之间的随机变量, 且<mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mrow><mo>[</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>X</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>]</mo></mrow><mo>=</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mi>S</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mrow><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mtext>那</mtext></mrow></mrow></math></mathml>么对于所有的<i>a</i>≥0都有:</p>
                </div>
                <div class="p1">
                    <p id="169" class="code-formula">
                        <mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mi>S</mi><msub><mrow></mrow><mi>n</mi></msub><mo>≥</mo><mi>E</mi><mo stretchy="false">[</mo><mi>S</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">]</mo><mo>+</mo><mi>a</mi><mo stretchy="false">}</mo><mo>≤</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>a</mi><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mn>2</mn></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>a</mi><mo>/</mo><mn>2</mn></mrow></mfrac></mrow><mo>) </mo></mrow><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="170"><b>定理1</b>. 第<i>n</i>次操作时, CNAME算法的次优动作选择概率<i>p</i><sub><i>i</i></sub>不超过:</p>
                </div>
                <div class="area_img" id="172">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903019_17200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="173">证明. 首先, 对于所有的动作1≤<i>i</i>≤<i>K</i>, 定义<i>X</i><sub><i>i</i>, <i>n</i></sub>为动作<i>i</i>在第<i>n</i>次操作得到的立即奖赏, 显然<i>E</i>[<i>X</i><sub><i>i</i>, <i>n</i></sub>]=<i>μ</i><sub><i>i</i></sub>.另外, 拥有<i>μ</i><sup>*</sup>的动作叫作最优动作.类似地, <i>N</i><sup>*</sup><sub><i>n</i></sub>表示最优动作在前<i>n</i>次操作被选择的次数, <mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup></mrow></math></mathml>表示最优动作在第<i>n</i>次操作时的平均奖赏.最优动作值与次优动作值的差值为<i>Δ</i><sub><i>i</i></sub>=<i>μ</i><sup>*</sup>-<i>μ</i><sub><i>i</i></sub>.动作<i>i</i>在第<i>n</i>次操作时的平均奖赏定义为</p>
                </div>
                <div class="p1">
                    <p id="175" class="code-formula">
                        <mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>X</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="176">令前<i>n</i>次操作中动作<i>i</i>被选择次数的期望为:<mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">{</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>i</mi><mo stretchy="false">}</mo></mrow></math></mathml>, 其中<i>P</i>{<i>a</i><sub><i>t</i></sub>=<i>i</i>}为时刻<i>t</i>动作<i>i</i>的选择概率.</p>
                </div>
                <div class="p1">
                    <p id="178">令<i>ε</i><sub><i>n</i></sub>表示第<i>n</i>次操作的探索概率, 即</p>
                </div>
                <div class="p1">
                    <p id="179" class="code-formula">
                        <mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mfrac><mi>w</mi><mrow><mi>w</mi><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mi>n</mi><mn>2</mn></msubsup></mrow></mfrac><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="180">其中, <i>w</i>&gt;0.同时, 令<mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>Κ</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>ε</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>.则动作<i>i</i>在第<i>n</i>次操作的选择概率满足:</p>
                </div>
                <div class="p1">
                    <p id="182" class="code-formula">
                        <mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mi>a</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>i</mi><mo stretchy="false">}</mo><mo>≤</mo><mfrac><mrow><mi>ε</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mi>Κ</mi></mfrac><mo>+</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>ε</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mi>Κ</mi></mfrac></mrow><mo>) </mo></mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub><mo>≥</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mrow><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup></mrow><mo>*</mo></msubsup><mo stretchy="false">}</mo><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="183">根据式 (7) , 要求解<i>P</i>{<i>a</i><sub><i>t</i></sub>=<i>i</i>}, 只需要求出概率<mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub></mrow><mo>≥</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mrow><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup></mrow><mo>*</mo></msubsup><mo stretchy="false">}</mo></mrow></math></mathml>即可.已知:</p>
                </div>
                <div class="p1">
                    <p id="185" class="code-formula">
                        <mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub><mo>≥</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mrow><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup></mrow><mo>*</mo></msubsup><mo stretchy="false">}</mo><mo>≤</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="186"><mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>+</mo><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mrow><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup></mrow><mo>*</mo></msubsup><mo>≤</mo><mi>μ</mi><mo>*</mo><mo>-</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="188">令<i>N</i><mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup></mrow></math></mathml> (<i>i</i>) 为动作<i>i</i>在前<i>n</i>次操作中随机被选到的次数, 结合Chernoff-Hoeffding不等式 (引理1) , 可得:</p>
                </div>
                <div class="p1">
                    <p id="190" class="code-formula">
                        <mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">{</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mi>t</mi><mo>∧</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">{</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mi>t</mi><mo stretchy="false">|</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>×</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false">{</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>≤</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">{</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mi>t</mi><mo stretchy="false">|</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>×</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mi>t</mi></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>≤</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">{</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mi>t</mi><mo stretchy="false">|</mo><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>2</mn><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>≤</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow></mrow><mrow></mrow><mo>-</mo><mrow></mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mrow></mrow><mrow></mrow><mo>-</mo><mrow></mrow></mrow></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">{</mo><mi>Ν</mi><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>≤</mo><mi>t</mi><mo stretchy="false">|</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>X</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>≥</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mrow><mi>Δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">}</mo><mo>+</mo><mfrac><mn>2</mn><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mrow></mrow><mrow></mrow><mo>-</mo><mrow></mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mrow></mrow><mrow></mrow><mo>-</mo><mrow></mrow></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>≤</mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>×</mo><mi>Ρ</mi><mo stretchy="false">{</mo><mi>Ν</mi><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>≤</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">}</mo><mo>+</mo><mfrac><mn>2</mn><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mrow></mrow><mrow></mrow><mo>-</mo><mrow></mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mrow></mrow><mrow></mrow><mo>-</mo><mrow></mrow></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="191">根据式 (7) (8) , 求解<i>P</i>{<i>a</i><sub><i>n</i></sub>=<i>i</i>}的问题可以转化为求解<i>x</i><sub>0</sub>和<i>P</i>{<i>N</i><mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup></mrow></math></mathml> (<i>i</i>) ≤<i>x</i><sub>0</sub>}的问题.其中, <i>P</i>{<i>N</i><mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup></mrow></math></mathml> (<i>i</i>) ≤<i>x</i><sub>0</sub>}的求解过程为:</p>
                </div>
                <div class="p1">
                    <p id="194">已知<mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>Ν</mi><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>ε</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>, 而且<i>N</i><mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup></mrow></math></mathml> (<i>i</i>) 的方差</p>
                </div>
                <div class="p1">
                    <p id="197" class="code-formula">
                        <mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>V</mtext><mtext>a</mtext><mtext>r</mtext><mspace width="0.25em" /><mrow><mo>[</mo><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mrow><mi>ε</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>Κ</mi></mfrac></mrow></mstyle><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>ε</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>Κ</mi></mfrac></mrow><mo>) </mo></mrow><mo>≤</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>ε</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="198">根据Bernstein不等式 (引理2) 可以得到:</p>
                </div>
                <div class="p1">
                    <p id="199"><mathml id="200"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">{</mo><mi>Ν</mi><msubsup><mrow></mrow><mi>n</mi><mi>R</mi></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>≤</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">}</mo><mo>≤</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mn>5</mn></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>.      (9) </p>
                </div>
                <div class="p1">
                    <p id="201">求解<i>x</i><sub>0</sub>, 具体过程为</p>
                </div>
                <div class="area_img" id="202">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903019_20200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="203">当<i>n</i>≥<i>K</i>时, 结合式 (7) ～ (10) 可得:</p>
                </div>
                <div class="area_img" id="334">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903019_33400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="207">那么对于CNAME算法, <i>w</i>&gt;0, 若<i>n</i>≥<i>K</i>, 那么在第<i>n</i>次操作时动作<i>i</i>的选择概率<i>p</i><sub><i>i</i></sub>有:</p>
                </div>
                <div class="area_img" id="335">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903019_33500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="211">证毕.</p>
                </div>
                <div class="p1">
                    <p id="212"><b>定理2</b>. 第<i>n</i>次操作时, CNAME算法的悔值满足:</p>
                </div>
                <div class="area_img" id="336">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903019_33600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="216">证明. 结合式 (6) (11) 可得到<i>E</i>[<i>N</i><sub><i>n</i></sub> (<i>i</i>) ]的下界为</p>
                </div>
                <div class="p1">
                    <p id="217" class="code-formula">
                        <mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">[</mo><mi>Ν</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>≥</mo><mo stretchy="false"> (</mo><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>n</mi><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mfrac><mi>w</mi><mrow><mo stretchy="false"> (</mo><mi>w</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mi>Κ</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>w</mi><mtext> </mtext><mi>n</mi><mo>-</mo><mn>2</mn></mrow><mrow><mi>Κ</mi><mi>w</mi></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>w</mi><mtext> </mtext><mi>n</mi><mo>-</mo><mn>2</mn></mrow><mrow><mn>1</mn><mn>0</mn><mi>Κ</mi><mi>w</mi></mrow></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mfrac><mn>4</mn><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>Δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>w</mi><mtext> </mtext><mi>n</mi><mo>-</mo><mn>2</mn><mo stretchy="false">) </mo></mrow><mrow><mn>4</mn><mi>Κ</mi><mi>w</mi></mrow></mfrac></mrow><mo>) </mo></mrow><mrow><mo>‖</mo><mrow><mi>p</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></mrow><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="218">最后, 根据式 (1) (12) 可得出CNAME算法的悔值上界:</p>
                </div>
                <div class="area_img" id="221">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201903019_22100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="337">证毕.</p>
                </div>
                <div class="p1">
                    <p id="222">值得注意的是, 不同于其他算法的悔值上界, 例如<i>ε</i> -decreasing算法的悔值上界<i>O</i> (log <i>n</i>) 和Exp3算法的悔值上界<mathml id="223"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><msqrt><mrow><mi>Κ</mi><mi>n</mi><mtext> </mtext><mi>log</mi><mtext> </mtext><mi>Κ</mi></mrow></msqrt><mo stretchy="false">) </mo></mrow></math></mathml>, 本节给出了CNAME算法的次优动作选择概率的上界, 基于此可以得到某一时刻动作选择概率的分布, 进而得到该时刻的悔值上界, 即瞬时的悔值上界.此时, 若要进一步知道某个时刻的悔值, 只需要知道最优动作与次优动作之间的差值.</p>
                </div>
                <h3 id="224" name="224" class="anchor-tag"><b>3 实验部分</b></h3>
                <div class="p1">
                    <p id="225">本节通过4组实验来分别研究CNAME算法中参数<i>w</i>的影响以及CNAME算法与其他算法的效果比较.在对比实验中, 使用3个数据集:服从正态分布的随机数据集、内容分发网络数据集<citation id="434" type="reference"><link href="380" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和带有上下文信息的雅虎公司R6A数据集<citation id="325" type="note"><link href="4" rel="footnote" /><sup>①</sup></citation>.</p>
                </div>
                <h4 class="anchor-tag" id="226" name="226"><b>3.1 实验1:CNAME算法中参数<i>w</i>的影响</b></h4>
                <div class="p1">
                    <p id="227">本节的实验对象是一组有1 000个随机产生的<i>K</i>-臂赌博机的任务, 其中<i>K</i>=10.具体实验设置如下:</p>
                </div>
                <div class="p1">
                    <p id="228">1) 每个任务的真实值<i>μ</i>=[<i>μ</i><sub>1</sub>, <i>μ</i><sub>2</sub>, …, <i>μ</i><sub><i>K</i></sub>]服从均值为0、方差为1的正态分布.</p>
                </div>
                <div class="p1">
                    <p id="229">2) 每个动作<i>i</i>的奖赏服从一个均值为<i>μ</i><sub><i>i</i></sub>、方差为1的正态分布.</p>
                </div>
                <div class="p1">
                    <p id="230">3) 1 000个<i>K</i>-臂赌博机任务通过1 000次重复随机选择<i>μ</i>产生, 选取1 000个不同的<i>K</i>-臂赌博机任务是为了克服任务和环境本身的随机性.</p>
                </div>
                <div class="p1">
                    <p id="231">在上述实验条件下取不同的<i>w</i>值, 分别记录1 000个随机任务的平均奖赏和平均每轮的悔值.</p>
                </div>
                <div class="p1">
                    <p id="232">首先, 在闭区间[0.01, 100]以10的倍率分别取5组不同的<i>w</i>值, 图1给出了相应的平均奖赏和平均每轮的悔值.</p>
                </div>
                <div class="area_img" id="233">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903019_233.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同w值下CNAME算法的平均奖赏和平均悔值" src="Detail/GetImg?filename=images/JFYZ201903019_233.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不同<i>w</i>值下CNAME算法的平均奖赏和平均悔值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903019_233.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Average rewards and average regrets of algorithm CNAME with different values of parameter <i>w</i></p>

                </div>
                <div class="p1">
                    <p id="234">从图1可以看出:当<i>w</i>∈<citation id="435" type="reference">[<a class="sup">1</a>]</citation>时, 随着<i>w</i>的减小, 平均奖赏呈增加趋势, 而平均每轮的悔值呈下降趋势.然而当<i>w</i>=0.1和<i>w</i>=0.01时获得的平均奖赏却小于<i>w</i>=1时的平均奖赏, 即平均奖赏并不是随着<i>w</i>的减小而无限增加的.同时, 平均每轮的悔值的下降也存在下限.基于图1的数据, 可以猜测, <i>w</i>=1是CNAME算法效果的分界线, <i>w</i>&lt;1或<i>w</i>&gt;1时, 效果均下降.较之其他<i>w</i>的取值, 当<i>w</i>取值为0.1和1时, CNAME算法的平均奖赏较高, 平均每轮的悔值较低.</p>
                </div>
                <div class="p1">
                    <p id="235">为进一步研究当<i>w</i>∈[0.1, 1]时, CNAME算法的性能, 下面以0.1为间隔分别取10组不同的<i>w</i>值展开实验.表1给出了对应的实验结果数据, 加粗部分是效果最好的参数及结果.</p>
                </div>
                <div class="area_img" id="236">
                    <p class="img_tit"><b>表1 CNAME算法的平均奖赏和平均悔值 (<i>w</i>∈[0.1, 1</b>])  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Average Rewards and Average Regrets of CNAME (<i>w</i>∈[0.1, 1</b>]) </p>
                    <p class="img_note"></p>
                    <table id="236" border="1"><tr><td><br /><i>w</i></td><td>Average Rewards</td><td>Average Regrets</td></tr><tr><td><br />0.1</td><td>1.451</td><td>0.085</td></tr><tr><td><br />0.2</td><td>1.443</td><td>0.089</td></tr><tr><td><br />0.3</td><td>1.453</td><td>0.081</td></tr><tr><td><br />0.4</td><td>1.448</td><td>0.087</td></tr><tr><td><br />0.5</td><td>1.457</td><td>0.079</td></tr><tr><td><br />0.6</td><td>1.446</td><td>0.088</td></tr><tr><td><br />0.7</td><td>1.469</td><td>0.074</td></tr><tr><td><br />0.8</td><td>1.472</td><td>0.071</td></tr><tr><td><br /><b>0.9</b></td><td><b>1.478</b></td><td><b>0.068</b></td></tr><tr><td><br />1.0</td><td>1.468</td><td>0.075</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best result is highlighted in bold.</p>
                </div>
                <div class="p1">
                    <p id="237">基于图1和表1的数据, 可以确认当<i>w</i>∈[0.1, 1]时, CNAME算法能够获得较高的平均奖赏和较低的平均悔值.此外, 根据表1可知, 当<i>w</i>取值较为接近1时, 比如<i>w</i>=0.9时, 获得的平均奖赏更大, 平均每轮的悔值更小, 即CNAME算法的效果更好.</p>
                </div>
                <div class="p1">
                    <p id="238">仅以平均奖赏为例对图1和表1的实验结果进行分析, 主要有2个原因:</p>
                </div>
                <div class="p1">
                    <p id="239">1) 当<i>w</i>的取值过小时, 探索概率的衰减速率过快, 导致没有足够的探索, 无法得到最高的累积奖赏.</p>
                </div>
                <div class="p1">
                    <p id="240">初始时刻<i>m</i><sub>0</sub>=0, CNAME算法探索的概率为1, 此时接近随机策略.一定时间步之后, 当<i>m</i><sub><i>t</i></sub>≠1时, 探索概率开始衰减, <i>w</i>越小, 衰减的速率越高.例如当<i>w</i>=1, <i>m</i><sub><i>t</i></sub>=1时, 探索概率为0.5;当<i>m</i><sub><i>t</i></sub>=2时, 探索概率降为0.2, 算法效果近似于<i>ε</i> -decreasing.而当<i>w</i>=0.01, <i>m</i><sub><i>t</i></sub>=1时, 探索概率为0.009 9;当<i>m</i><sub><i>t</i></sub>=2时, 探索概率衰减到0.002 5, 这时算法效果类似于greedy, 可能会忽略更优的动作, 从而不能获得最高的平均奖赏.因此, <i>w</i>取值过小, 平均奖赏反而会有所下降.</p>
                </div>
                <div class="p1">
                    <p id="241">2) 当<i>w</i>的取值较大时, 探索概率的衰减速率过慢, 导致过度探索, 平均奖赏相对较小.</p>
                </div>
                <div class="p1">
                    <p id="242">例如当<i>w</i>=100, <i>m</i><sub><i>t</i></sub>=1时, 探索概率为0.990;<i>m</i><sub><i>t</i></sub>=2, 探索概率为0.962.<i>m</i><sub><i>t</i></sub>的增加对探索概率的影响过小, 不能及时地减少探索概率, 导致了过多的探索, 使得平均奖赏较小.因此<i>w</i>取值过大, 平均奖赏相对较小.</p>
                </div>
                <div class="p1">
                    <p id="243">综上所述, 参数<i>w</i>应根据实际环境适当取值, 不宜过大也不宜过小.根据实验结果, 该环境下CNAME算法的参数<i>w</i>取闭区间[0.1, 1]中的值较好.</p>
                </div>
                <h4 class="anchor-tag" id="244" name="244"><b>3.2 实验2:随机数据集上的比较</b></h4>
                <div class="p1">
                    <p id="245">本节基于随机数据集, 比较CNAME算法和其他3类经典算法及其变种的算法性能.</p>
                </div>
                <div class="p1">
                    <p id="246">与实验1相似, 本实验同样采用随机产生的服从正态分布的数据集:1 000个随机产生的多臂赌博机任务.具体地, <i>K</i>=10, <i>n</i>=2 000;<i>μ</i>=[<i>μ</i><sub>1</sub>, <i>μ</i><sub>2</sub>, …, <i>μ</i><sub><i>K</i></sub>]服从均值为0、方差为1的正态分布;每个动作<i>i</i>的奖赏服从一个均值为<i>μ</i><sub><i>i</i></sub>、方差为1的正态分布.根据实验1的结果, 本次实验中, 参数<i>w</i>=0.95.</p>
                </div>
                <div class="p1">
                    <p id="247">在上述实验条件下, 分别记录各个算法的平均奖赏和平均每轮的悔值.表2列出了实验中6种算法的参数设置、对应1 000个任务在2 000个时间步下的平均奖赏和平均每轮的悔值.各个算法均采用的是该实验条件下效果较好的参数, 表2中加粗部分是效果最好的结果.</p>
                </div>
                <div class="area_img" id="248">
                    <p class="img_tit"><b>表2 6种算法的参数设置及实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Parameters and Experimental Results of Six Algorithms</b></p>
                    <p class="img_note"></p>
                    <table id="248" border="1"><tr><td><br />Algorithms</td><td>Parameters</td><td>Average<br />Rewards</td><td>Average<br />Regrets</td></tr><tr><td><br /><i>ε</i> -greedy</td><td><i>ε</i>=0.1</td><td>1.338</td><td>0.185</td></tr><tr><td><br /><i>ε</i> -decreasing</td><td>ε<sub>0</sub>=10</td><td>1.378</td><td>0.143</td></tr><tr><td><br />SoftMax</td><td><i>τ</i>=0.2</td><td>1.470</td><td>0.081</td></tr><tr><td><br />decreasing SoftMax</td><td><i>τ</i><sub>0</sub>=20</td><td>1.478</td><td>0.060</td></tr><tr><td><br />UCB1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><msqrt><mn>2</mn></msqrt></mrow></math></td><td>1.419</td><td>0.088</td></tr><tr><td><br />CNAME</td><td><i>w</i>=0.95</td><td><b>1.491</b></td><td><b>0.054</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best result is highlighted in bold.</p>
                </div>
                <div class="p1">
                    <p id="250">从表2可以看出, 该实验条件下, CNAME算法获得的平均奖赏最高, 平均每轮的悔值最低.变种<i>ε</i> -decreasing算法的效果明显优于<i>ε</i> -greedy算法的效果;类似地, 变种decreasing SoftMax算法的效果优于SoftMax算法.</p>
                </div>
                <div class="p1">
                    <p id="251">为了简洁清晰地表示这几种算法的对比效果, 图2和图3仅分别展示了<i>ε</i> -decreasing, decreasing SoftMax, UCB1, CNAME这4种算法运行至1 000个时间步时的实验数据.图2是CNAME算法与其他3种算法平均奖赏的结果比较.图3为CNAME算法与其他3种算法的悔值的比较, 同时给出了CNAME算法平均悔值的上界 (upper bound) .基于2.3节得出的悔值上界, upper bound是平均每轮的悔值上界.由图3可知, 随着时间步的增加, upper bound的值保持平稳, 可见CNAME算法非常稳定.另一方面, CNAME算法各个时间步的平均悔值均在upper bound之下, 符合2.3节的理论结论.</p>
                </div>
                <div class="area_img" id="252">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903019_252.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNAME算法与其他3种算法的平均奖赏" src="Detail/GetImg?filename=images/JFYZ201903019_252.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CNAME算法与其他3种算法的平均奖赏  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903019_252.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Average rewards of CNAME and the other three algorithms</p>

                </div>
                <div class="area_img" id="253">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903019_253.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CNAME算法与其他3种算法的平均每轮的悔值" src="Detail/GetImg?filename=images/JFYZ201903019_253.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 CNAME算法与其他3种算法的平均每轮的悔值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903019_253.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Average regrets of CNAME and the other three algorithms</p>

                </div>
                <div class="p1">
                    <p id="254">表2、图2和图3中的数据均是1 000个随机任务的平均值.结合图表, 本文从3方面进行比较:</p>
                </div>
                <h4 class="anchor-tag" id="255" name="255">1) CNAME算法和<i>ε</i> -greedy算法及其变种</h4>
                <div class="p1">
                    <p id="256">CNAME算法的收敛速度略低于<i>ε</i> -decreasing算法, 但其平均奖赏明显高于<i>ε</i> -greedy算法和<i>ε</i> -decreasing算法, 平均每轮悔值也明显低于<i>ε</i> -greedy算法和<i>ε</i> -decreasing算法.CNAME算法的效果与<i>ε</i> -greedy算法和<i>ε</i> -decreasing算法相比有了很大的提高.</p>
                </div>
                <div class="p1">
                    <p id="257">这是因为CNAME算法利用了用户反馈, 虽然收敛速度有所减小, 但效果比探索概率固定不变的<i>ε</i> -greedy算法好很多.同理, <i>ε</i> -decreasing算法的探索概率虽然是随着时间降低的, 但是没有利用选择动作之后的信息, 因此其效果比CNAME算法差.</p>
                </div>
                <h4 class="anchor-tag" id="258" name="258">2) CNAME算法和SoftMax算法及其变种</h4>
                <div class="p1">
                    <p id="259">CNAME算法的收敛速度略低于decreasing SoftMax算法, 但其平均奖赏高于SoftMax算法和decreasing SoftMax算法, 且平均每轮的悔值较低.因此, CNAME算法的效果与SoftMax算法和decreasing SoftMax算法相比有所提高.</p>
                </div>
                <div class="p1">
                    <p id="260">这是因为与SoftMax算法相比, CNAME算法除了考虑动作估计值, 还考虑了动作被选择的次数, 比SoftMax算法和decreasing SoftMax算法利用的信息多, 虽然收敛速度略有降低, 但不易被前期的估计值误导.因此CNAME算法最终获得的平均奖赏和平均每轮悔值均优于SoftMax算法和decreasing SoftMax算法.</p>
                </div>
                <h4 class="anchor-tag" id="261" name="261">3) CNAME算法和UCB1算法</h4>
                <div class="p1">
                    <p id="262">CNAME算法的收敛速度与UCB1算法相当, 但其平均奖赏高于UCB1算法, 且平均每轮的悔值低于UCB1算法.值得注意的是, 通过分析实验数据的变化, 我们发现:CNAME算法的数据波动较小, 而UCB1算法在实验初期的数据会出现尖峰.</p>
                </div>
                <div class="p1">
                    <p id="263">这是因为CNAME算法和UCB1算法均充分利用了选择动作之后的信息, 两者收敛速度相当.但是UCB1算法在初期的前<i>K</i>次操作中, 先将每个动作轮流选择一次, 在第<i>K</i>+1次操作时选择贪心动作, 因此UCB1算法会在第<i>K</i>+1操作时出现尖峰.而CNAME算法在第<i>K</i>+1次操作时, 依然根据当前的探索概率选择动作, 因此CNAME算法的数据不会出现尖峰.</p>
                </div>
                <div class="p1">
                    <p id="264">综上所述, CNAME算法是一种有效的多臂赌博机算法.</p>
                </div>
                <h4 class="anchor-tag" id="265" name="265"><b>3.3 实验3:内容分发网络数据集上的比较</b></h4>
                <div class="p1">
                    <p id="266">实验3采用的数据集对应于真实环境中可用资源冗余情况下的数据检索问题.例如在内容分发网络中, agent必须通过一个可用资源冗余的网络来检索数据.假设每次检索agent只选择一个资源, 直到检索到数据, agent的目标是最小化检索的累积延时.该问题可转换为随机多臂赌博机问题进行处理<citation id="436" type="reference"><link href="390" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>, 3项具体说明如下:</p>
                </div>
                <div class="p1">
                    <p id="267">1) 实验3用超过700个学校的主页作为资源, 每一个主页对应多臂赌博机中的一个臂 (动作) .</p>
                </div>
                <div class="p1">
                    <p id="268">2) 这些主页约每隔10 min被检索一次, 共记录了10 d的检索延迟信息 (约1 300条) , 单位为ms.</p>
                </div>
                <div class="p1">
                    <p id="269">3) 每一个延时对应一个负奖赏, 因此, 平均延时越小, 对应算法越有效.</p>
                </div>
                <div class="p1">
                    <p id="270">上述延时数据来源于实际问题, 数据范围广、波动大, 而且隐含了实际环境中的各种干扰和影响.本文分别利用各个算法对该数据集进行处理, 更能体现各个算法性能的差异和各自的稳定性.</p>
                </div>
                <div class="p1">
                    <p id="271">表3是6种算法平均每轮的检索延时, 这些数据是1 000次模拟的平均结果.考虑到实际环境的随机性, 每次模拟中延时的顺序都是随机的.</p>
                </div>
                <div class="area_img" id="272">
                    <p class="img_tit"><b>表3 6种算法的平均延时</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Average Latency of Six Algorithms</b></p>
                    <p class="img_note"></p>
                    <table id="272" border="1"><tr><td><br />Algorithms</td><td>Parameters</td><td>Average Latency/ms</td></tr><tr><td><br /></td><td><i>ε</i>=0.05</td><td>427</td></tr><tr><td><br /><i>ε</i> -greedy</td><td><i>ε</i>=0.1</td><td>494</td></tr><tr><td><br /></td><td><i>ε</i>=0.15</td><td>475</td></tr><tr><td><br /></td><td><i>ε</i><sub>0</sub>=5</td><td>424</td></tr><tr><td><br /><i>ε</i> -decreasing</td><td><i>ε</i><sub>0</sub>=10</td><td>397</td></tr><tr><td><br /></td><td><i>ε</i><sub>0</sub>=15</td><td>389</td></tr><tr><td><br /></td><td><i>τ</i>=0.1</td><td>411</td></tr><tr><td><br />SoftMax</td><td><i>τ</i>=0.15</td><td>410</td></tr><tr><td><br /></td><td><i>τ</i>=0.2</td><td>409</td></tr><tr><td><br /></td><td><i>τ</i><sub>0</sub>=15</td><td>401</td></tr><tr><td><br />decreasing SoftMax</td><td><i>τ</i><sub>0</sub>=20</td><td>392</td></tr><tr><td><br /></td><td><i>τ</i><sub>0</sub>=25</td><td>388</td></tr><tr><td><br />UCB1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><mspace width="0.25em" /><msqrt><mn>2</mn></msqrt></mrow></math></td><td>373</td></tr><tr><td><br /></td><td><i>w</i>=0.1</td><td><b>372</b></td></tr><tr><td><br />CNAME</td><td><i>w</i>=0.5</td><td>374</td></tr><tr><td><br /></td><td><i>w</i>=1</td><td>376</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: The best result is highlighted in bold.</p>
                </div>
                <div class="p1">
                    <p id="273">从表3可以看出, 使用<i>ε</i> -greedy, <i>ε</i> -decreasing, SoftMax, decreasing SoftMax这4种算法得到的平均延时明显高于UCB1和CNAME算法.这主要是因为关于延时的网络数据通常波动很大, 而且延时数据的取值范围很广, 可能是10 ms, 也可能是1 000 ms.对于这样的数据, 需要非常小心地探索, 因为尝试一个动作有可能得到很小的延时, 但也有可能得到一个突增的延时.这种情况下, UCB1算法和CNAME算法的优势很明显, 这2种算法同时根据动作的估计值和被选择的次数自适应地调整探索和利用, 充分利用了反馈信息.</p>
                </div>
                <div class="p1">
                    <p id="274">虽然UCB1算法和CNAME算法最后得到的平均延时相当, 但CNAME算法只需要约2 min就可以完成计算, 而UCB1算法则需要约6 h.这主要是因为UCB1算法相对复杂, 计算负担较大, 所以需要较长的计算时间;而CNAME算法的计算则相对简单, 计算负担较小, 极大地节约了计算时间.</p>
                </div>
                <div class="p1">
                    <p id="275">可见, CNAME算法可以更加高效地平衡随机多臂赌博机问题中的探索和利用.</p>
                </div>
                <h4 class="anchor-tag" id="276" name="276"><b>3.4 实验4:雅虎公司 R6A数据集上的比较</b></h4>
                <div class="p1">
                    <p id="277">在线内容推荐是交互式机器学习问题的重要内容, 需要在探索和利用之间进行有效的平衡.实验4将推荐系统建模为多臂赌博机模型.雅虎公司R6A数据集包含45 811 883个用户访问雅虎公司首页Today模块的点击日志.对于每次访问, 用户和每个候选文章都有关联的6维的特征向量.</p>
                </div>
                <div class="p1">
                    <p id="278">本节对比了CNAME算法与其他3种算法在雅虎公司R6A数据集上的推荐效果, 实验结果如图4所示, 纵坐标表示累计奖赏, 对应推荐系统中的点击次数.其中, Random算法作为基准算法, 每次为用户随机推荐物品, LinUCB算法为上下文相关的多臂赌博机算法.</p>
                </div>
                <div class="area_img" id="279">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201903019_279.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CNAME算法与其他3种算法的累计奖赏" src="Detail/GetImg?filename=images/JFYZ201903019_279.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 CNAME算法与其他3种算法的累计奖赏  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201903019_279.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Cumulative rewards of CNAME and the other three algorithms</p>

                </div>
                <div class="p1">
                    <p id="280">从图4可以看出, Random算法的累计奖赏最低, 这是因为Random算法只是随机产生推荐, 没有利用用户反馈信息.CNAME算法获得的累计奖赏明显高于UCB1算法.UCB1算法在初期将各个物品轮流推荐给用户, 在物品数量较大的情况下不能及时地学习到用户的兴趣.因此, UCB1算法在初期获得的累计奖赏接近Random算法, 后期获得的累计奖赏则快速增加.</p>
                </div>
                <div class="p1">
                    <p id="281">LinUCB算法作为经典的上下文相关的多臂赌博机算法, 主要结合数据集中的6维特征向量进行推荐, 在100个时间步后获得的累计奖赏为78.由于LinUCB算法依赖用户和物品的特征向量, 而不同推荐系统中的特征向量维度和含义不同, LinUCB算法无法方便地推广到不同的应用场景.CNAME算法基于探索概率, 在利用用户反馈进行推荐的同时, 探索可能为用户带来惊喜的物品.因此CNAME算法获得的累计奖赏很高且稳定增长, 100个时间步后获得的累计奖赏高达98, 累计奖赏高于基于上下文信息的LinUCB算法.CNAME算法在推荐的过程中, 一方面根据用户反馈, 自适应地调整探索概率, 另一方面不依赖上下文信息, 可以方便地应用到各个场景中.</p>
                </div>
                <h3 id="282" name="282" class="anchor-tag"><b>4 总 结</b></h3>
                <div class="p1">
                    <p id="283">本文提出了一种自适应的随机多臂赌博机算法, 即CNAME算法.该算法充分利用选择动作之后获得的信息, 同时考虑动作的估计值和被选择的次数, 简便高效地解决了随机多臂赌博机中探索和利用的平衡问题.CNAME算法根据当前估计值最小的动作 (即实际获得的平均奖赏最小的动作) 已经被选择的次数<i>m</i><sub><i>t</i></sub>和参数<i>w</i>共同控制探索的概率, 自适应地根据实际环境调整探索和利用的程度.同时, CNAME算法在探索时选择目前最少被选择的动作, 避免了随机选择动作的盲目性.</p>
                </div>
                <div class="p1">
                    <p id="284">一方面, 本文通过理论分析和证明给出了CNAME算法的悔值上界, 为CNAME算法的性能提供了有力的理论支持.另一方面, 本文通过4组实验讨论了CNAME算法的关键参数<i>w</i>的影响及参考取值范围并分别在随机数据集, 内容分发网络数据集和带有上下文信息的雅虎公司 R6A数据集上与其他3类经典算法及其变种进行了对比实验.实验表明CNAME算法与<i>ε</i> -greedy算法和<i>ε</i> -decreasing算法相比有了很大的提高, 效果比SoftMax算法和decreasing SoftMax算法略好, 比UCB1算法更高效.此外, CNAME算法的泛化能力较强, 在雅虎公司R6A数据集上获得了与LinUCB算法相当甚至更好的效果.综上所述, CNAME算法是一种效果稳定、高效且泛化能力强的多臂赌博机算法.</p>
                </div>
                <div class="p1">
                    <p id="285">未来工作尝试将CNAME算法应用到更加复杂的多臂赌博机问题中, 比如带有预算的多臂赌博机问题<citation id="437" type="reference"><link href="396" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、定性的多臂赌博机问题等<citation id="438" type="reference"><link href="398" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>;同时, 探讨CNAME算法解决复杂问题的可能性, 尝试提出有实际应用的新型多臂赌博机问题.此外, 考虑与深度学习相结合, 生成深度网络多臂赌博机, 以解决更加复杂的问题.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="338">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                Sutton R, Barto G. Reinforcement Learning: An Introduction[M]. Cambridge, MA: MIT Press, 1998: 1- 20
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX200708023&amp;v=MTY1NDllVnZGeTduVXIvSkx6N0Jkckc0SHRiTXA0OUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Gao Yang, Zhou Ruyi, Wang Hao, et al. Study on an average reward reinforcement learning algorithm[J]. Chinese Journal of Computers, 2007, 30 (8) : 1372- 1378 (in Chinese) (高阳, 周如益, 王皓, 等. 平均奖赏强化学习算法研究[J]. 计算机学报, 2007, 30 (8) : 1372- 1378) 
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201302003&amp;v=MDc0NDF5dlNkTEc0SDlMTXJZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVXIvSkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Zhao Fengfei, Qin Zheng. A multi-motive reinforcement learning framework[J]. Journal of Computer Research and Development, 2013, 50 (2) : 240- 247 (in Chinese) (赵凤飞, 覃征. 一种多动机强化学习框架[J]. 计算机研究与发展, 2013, 50 (2) : 240- 247) 
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201304023&amp;v=MDk1MTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci9KTHl2U2RMRzRIOUxNcTQ5SFo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Liu Quan, Fu Qiming, Yang Xudong, et al. A scalable parallel reinforcement learning method based on intelligent scheduling[J]. Journal of Computer Research and Development, 2013, 50 (4) : 843- 851 (in Chinese) (刘全, 傅启明, 杨旭东, 等. 一种基于智能调度的可扩展并行强化学习方法[J]. 计算机研究与发展, 2013, 50 (4) : 843- 851) 
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201503004&amp;v=MzAxNDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVXIvSkx5dlNkTEc0SDlUTXJJOUZZSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Liu Zhibin, Zeng Xiaoqin, Liu Huiyi, et al. A heuristic two-layer reinforcement learning algorithm based on BP neural networks[J]. Journal of Computer Research and Development, 2015, 52 (3) : 579- 587 (in Chinese) (刘智斌, 曾晓勤, 刘惠义, 等. 基于BP神经网络的双层启发式强化学习方法[J]. 计算机研究与发展, 2015, 52 (3) : 579- 587) 
                            </a>
                        </p>
                        <p id="348">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJAM&amp;filename=SJAM120524057654&amp;v=MTgwNjBxNDlBWSswS0NCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNya1U3bktJVjRWTmlmS1k3SzZIdFRP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Robbins H. Some aspects of the sequential design of experiments[J]. American Mathematical Society, 1952, 58 (5) : 527- 535
                            </a>
                        </p>
                        <p id="350">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The price of truthfulness for pay-per-click auctions">

                                <b>[7]</b>Devanur N R, Kakade S M. The price of truthfulness for pay-per-click auctions[C] //Proc of ACM Conf on Electronic Commerce. New York: ACM, 2009: 99- 106
                            </a>
                        </p>
                        <p id="352">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201712024&amp;v=MjE3MTgvSlBUWGNkckc0SDliTnJZOUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVXI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Cheng Shi, Mao Luhong, Chang Peng. Multi-armed bandit recommender algorithm with matrix factorization[J]. Journal of Chinese Computer Systems, 2017, 38 (12) : 2754- 2758 (in Chinese) (成石, 毛陆虹, 常鹏. 融合矩阵分解的多臂赌博机推荐算法[J]. 小型微型计算机系统, 2017, 38 (12) : 2754- 2758) 
                            </a>
                        </p>
                        <p id="354">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201801009&amp;v=MDM3NTMzenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0pMeXZTZExHNEg5bk1ybzlGYllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Yu Yonghong, Gao Yang, Wang Hao, et al. Integrating user social status and matrix factorization for item recommendation[J]. Journal of Computer Research and Development, 2018, 55 (1) : 113- 124 (in Chinese) (余永红, 高阳, 王皓, 等. 融合用户社会地位和矩阵分解的推荐算法[J]. 计算机研究与发展, 2018, 55 (1) : 113- 124) 
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning">

                                <b>[10]</b>Jain S, Gujar S, Zoeter O, et al. A quality assuring multi-armed bandit crowdsourcing mechanism with incentive compatible learning[C] //Proc of the 13th Int Conf on Autonomous Agents and Multi-Agent Systems. New York: ACM, 2014: 1609- 1610
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multi-armed bandit incentive mechanism for crowdsourcing demand response in smart grids">

                                <b>[11]</b>Jain S, Narayanaswamy B, Narahari Y. A multi-armed bandit incentive mechanism for crowdsourcing demand response in smart grids[C] //Proc of the 28th AAAI Conf on Artificial Intelligence and the 26th Innovative Applications of Artificial Intelligence Conf. Menlo Park, CA: AAAI, 2014: 721- 727
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016290369.nh&amp;v=MTYxMzlKVkYyNkdMR3hIdExLcHBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25Vci8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Sun Xinxin. Research on task assignment technology in crowdsourcing environment[D]. Yangzhou: Yangzhou University, 2016 (in Chinese) (孙信昕. 众包环境下的任务分配技术研究[D]. 扬州: 扬州大学, 2016) 
                            </a>
                        </p>
                        <p id="362">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014082590.nh&amp;v=MTI0MjM2R3JPd0hOVEZyNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blVyL0pWRjI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>Zhou Baojian. Research on search engine keyword optimal selection strategy based on multi-armed bandit[D]. Harbin: Harbin Institute of Technology, 2014 (in Chinese) (周保健. 基于多臂赌博机的搜索引擎关键字最优选择策略研究[D]. 哈尔滨: 哈尔滨工业大学, 2014) 
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017700729.nh&amp;v=MTYyNzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVXIvSlZGMjZHYlM0SHRiT3BwRWJQSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Chen Hongcui. Research on channel selection mechanism based on multi-armed bandit in cognitive network[D]. Chongqing: Chongqing University of Posts and Telecommu-nications, 2016 (in Chinese) (陈红翠. 认知网络中基于赌博机模型的信道选择机制研究[D]. 重庆: 重庆邮电大学, 2016) 
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Regret analysis of stochastic and nonstochastic multi-armed bandit problems">

                                <b>[15]</b>Bubeck S, Cesabianchi N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems[J]. Foundations and Trends in Machine Learning, 2012, 5 (1) : 1- 22
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contextual bandits with similarity information">

                                <b>[16]</b>Slivkins A. Contextual bandits with similarity information[J]. Journal of Machine Learning Research, 2014, 15 (1) : 2533- 2568
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011502000458&amp;v=MjU5OTZDSGt4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSmx3VmFoTT1OaWZPZmJLN0h0RE5xbzlIWk9zUA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>Watkins C J C H. Learning from delayed rewards[J]. Robotics &amp; Autonomous Systems, 1989, 15 (4) : 233- 235
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Individual choice behavior.">

                                <b>[18]</b>Luce R D. Individual choice behavior[J]. American Economic Review, 1959, 67 (1) : 1- 15
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340413&amp;v=MTY2ODdlYnVkdEZTbmxWYjNKSUY4PU5qN0Jhck80SHRITnJJdEZZT29NWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3Ujdx&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>Auer P, Cesa-Bianchi N, Fischer P. Finite-time analysis of the multiarmed bandit problem[J]. Machine Learning, 2002, 47 (2/3) : 235- 256
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A contextual-bandit approach to personalized news article recom-mendation">

                                <b>[20]</b>Li Lihong, Chu Wei, Langford J, et al. A contextual-bandit approach to personalized news article recommendation[C] //Proc of ACM Int Conf on World Wide Web. New York: ACM, 2010: 661- 670
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Can active learning experience be transferred?">

                                <b>[21]</b>Chu Hongmin, Lin Hsuan Tien. Can active learning experience be transferred?[C] //Proc of IEEE Int Conf on Data Mining. Piscataway, NJ: IEEE, 2017: 841- 846
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the use and performance of content distribution networks">

                                <b>[22]</b>Krishnamurthy B, Wills C, Zhang Yin. On the use and performance of content distribution networks[C] //Proc of the 1st ACM SIGCOMM Internet Measurement Workshop. New York: ACM, 2001: 169- 182
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new approach to the design of reinforcement schemes for learning automata">

                                <b>[23]</b>Thathachar M A L, Sastry P S. A new approach to the design of reinforcement schemes for learning automata[J]. IEEE Transactions on Systems Man &amp; Cybernetics, 1985, SMC-15 (1) : 168- 175
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PAC bounds for multi-armed bandit andMarkov decision processes">

                                <b>[24]</b>Even-Dar E, Mannor S, Mansour Y. PAC bounds for multi-armed bandit and Markov decision processes[C] //Proc of the 15th Annual Conf on Computational Learning Theory (COLT) . Berlin: Springer, 2002: 255- 270
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finite-time regret bounds for the multi-armed bandit problem">

                                <b>[25]</b>Cesa-Bianchi N, Fischer P. Finite-time regret bounds for the multi-armed bandit problem[C] //Proc of the Int Conf on Machine Learning. New York: ACM, 1998: 100- 108
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Cooperation and feedback in pattern recognition">

                                <b>[26]</b>Strens M. Learning Cooperation and feedback in pattern recognition[D]. London: Physics Department, King’s College London, 1999
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-armed bandit algorithms and empirical evaluation">

                                <b>[27]</b>Vermorel J, Mohri M. Multi-armed bandit algorithms and empirical evaluation[C] //Proc of the European Conf on Machine Learning. Berlin: Springer, 2005: 437- 448
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimization by simulated annealing">

                                <b>[28]</b>Kirkpatrick B S, Gelatt C, Vecchi D. Optimization by simulated annealing[J]. Science, 1983, 220 (4598) : 671- 680
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asymptotically efficient adaptive allocation rules">

                                <b>[29]</b>Lai T L, Robbins H. Asymptotically efficient adaptive allocation rules[J]. Advances in Applied Mathematics, 1985, 6 (1) : 4- 22
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD5FF7A03C039C43F122734C4919A852E&amp;v=MTI1Mjkrc01CUTg5ekdBUzZEMTZTM3VScUJzMGNNT2NRTGpxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod3JxK3dLaz1OaWZPZmNlOWFLZkwzbzlHRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b>Xia Yingce, Qin Tao, Ding Wenkui, et al. Finite budget analysis of multi-armed bandit problems[J]. Neurocomputing, 2017, 258: 13- 29
                            </a>
                        </p>
                        <p id="398">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Qualitative multi-armed bandits: A quantile-based approach">

                                <b>[31]</b>Szörényi B, Busa-Fekete R, Weng P. Qualitative multi-armed bandits: A quantile-based approach[C] //Proc of the 32nd Int Conf on Machine Learning. New York: ACM, 2015: 1660- 1668
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="4" href="javascript:void(0)">
                            <b>1</b> https://webscope.sandbox.yahoo.com
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201903019" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201903019&amp;v=MjM3NTdyL0pMeXZTZExHNEg5ak1ySTlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3blU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
