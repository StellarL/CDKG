

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127885180275000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908014%26RESULT%3d1%26SIGN%3dY0BSK37P4iGdHY8zBHLJwiPe%252bvQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908014&amp;v=MTc2NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnV3IzUEx5dlNkTEc0SDlqTXA0OUVZSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="&lt;b&gt;1 背景知识&lt;/b&gt; "><b>1 背景知识</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="&lt;b&gt;1.1 强化学习和随机行动者-评论家算法&lt;/b&gt;"><b>1.1 强化学习和随机行动者-评论家算法</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;1.2 深度确定性策略梯度算法&lt;/b&gt;"><b>1.2 深度确定性策略梯度算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="&lt;b&gt;2 算  法&lt;/b&gt; "><b>2 算  法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#139" data-title="&lt;b&gt;2.1 多行动者-评论家模型&lt;/b&gt;"><b>2.1 多行动者-评论家模型</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;2.2 基于经验的指导&lt;/b&gt;"><b>2.2 基于经验的指导</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#243" data-title="&lt;b&gt;3 实  验&lt;/b&gt; "><b>3 实  验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#244" data-title="&lt;b&gt;3.1 实验平台及实验介绍&lt;/b&gt;"><b>3.1 实验平台及实验介绍</b></a></li>
                                                <li><a href="#259" data-title="&lt;b&gt;3.2 参数设置&lt;/b&gt;"><b>3.2 参数设置</b></a></li>
                                                <li><a href="#261" data-title="&lt;b&gt;3.3 优秀经验筛选方法的效果&lt;/b&gt;"><b>3.3 优秀经验筛选方法的效果</b></a></li>
                                                <li><a href="#264" data-title="&lt;b&gt;3.4 基于经验的指导和多行动者机制的优势&lt;/b&gt;"><b>3.4 基于经验的指导和多行动者机制的优势</b></a></li>
                                                <li><a href="#270" data-title="&lt;b&gt;3.5 对比不同算法的性能&lt;/b&gt;"><b>3.5 对比不同算法的性能</b></a></li>
                                                <li><a href="#275" data-title="&lt;b&gt;3.6 使用专家经验的EGDDAC-MA&lt;/b&gt;"><b>3.6 使用专家经验的EGDDAC-MA</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#281" data-title="&lt;b&gt;4 结束语&lt;/b&gt; "><b>4 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="图1 行动者-评论家算法">图1 行动者-评论家算法</a></li>
                                                <li><a href="#138" data-title="图2 EGDDAC-MA结构示意图">图2 EGDDAC-MA结构示意图</a></li>
                                                <li><a href="#253" data-title="图3 四足蚂蚁形态机器人">图3 四足蚂蚁形态机器人</a></li>
                                                <li><a href="#257" data-title="图4 Bullet中的2D行走任务">图4 Bullet中的2D行走任务</a></li>
                                                <li><a href="#263" data-title="图5 正态分布模拟情节回报值的结果">图5 正态分布模拟情节回报值的结果</a></li>
                                                <li><a href="#266" data-title="图6 在Inverte-dDoublePendulum中的平均回报对比">图6 在Inverte-dDoublePendulum中的平均回报对比</a></li>
                                                <li><a href="#274" data-title="图7 4种方法在8个不同连续任务中的平均回报">图7 4种方法在8个不同连续任务中的平均回报</a></li>
                                                <li><a href="#274" data-title="图7 4种方法在8个不同连续任务中的平均回报">图7 4种方法在8个不同连续任务中的平均回报</a></li>
                                                <li><a href="#279" data-title="图8 使用专家经验的效果">图8 使用专家经验的效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="311">


                                    <a id="bibliography_1" title="Lange S, Riedmiller M, Voigtlander A.Autonomous reinforcement learning on raw visual input data in real world application[C] //Proc of the 9th Int Joint Conf Neural Network.Piscataway, NJ:IEEE, 2012:1- 8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Autonomous reinforcement learning on raw visual input data in real world application">
                                        <b>[1]</b>
                                        Lange S, Riedmiller M, Voigtlander A.Autonomous reinforcement learning on raw visual input data in real world application[C] //Proc of the 9th Int Joint Conf Neural Network.Piscataway, NJ:IEEE, 2012:1- 8
                                    </a>
                                </li>
                                <li id="313">


                                    <a id="bibliography_2" title="Zhang Rubo, Tang Pingpeng, Yang Ge, et al.Convergence analysis of adaptive obstacle avoidance decision processes for unmanned surface vehicle[J].Journal of Computer Research and Development, 2014, 51 (12) :2644- 2652 (in Chinese) (张汝波, 唐平鹏, 杨歌, 等.水面无人艇自适应危险规避决策过程收敛性分析[J].计算机研究与发展, 2014, 51 (12) :2644- 2652) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201412010&amp;v=MTY2NjhaZVJyRnl2Z1dyM1BMeXZTZExHNEg5WE5yWTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Zhang Rubo, Tang Pingpeng, Yang Ge, et al.Convergence analysis of adaptive obstacle avoidance decision processes for unmanned surface vehicle[J].Journal of Computer Research and Development, 2014, 51 (12) :2644- 2652 (in Chinese) (张汝波, 唐平鹏, 杨歌, 等.水面无人艇自适应危险规避决策过程收敛性分析[J].计算机研究与发展, 2014, 51 (12) :2644- 2652) 
                                    </a>
                                </li>
                                <li id="315">


                                    <a id="bibliography_3" title="Liu Quan, Yan Qicui, Fu Yuchen, et al.A hierarchical reinforcement learning method based on heuristic reward function[J].Journal of Computer Research and Development, 2011, 48 (12) :2352- 2358 (in Chinese) (刘全, 闫其粹, 伏玉琛, 等.一种基于启发式奖赏函数的分层强化学习方法[J].计算机研究与发展, 2011, 48 (12) :2352- 2358) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201112024&amp;v=MjI4NjNTZExHNEg5RE5yWTlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1dyM1BMeXY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Liu Quan, Yan Qicui, Fu Yuchen, et al.A hierarchical reinforcement learning method based on heuristic reward function[J].Journal of Computer Research and Development, 2011, 48 (12) :2352- 2358 (in Chinese) (刘全, 闫其粹, 伏玉琛, 等.一种基于启发式奖赏函数的分层强化学习方法[J].计算机研究与发展, 2011, 48 (12) :2352- 2358) 
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_4" title="Zhu Fei, Wu Wen, Liu Quan, et al.A deep Q-Network method based on upper confidence bound experience sampling[J].Journal of Computer Research and Development, 2018, 55 (8) :1694- 1705 (in Chinese) (朱斐, 吴文, 刘全, 等.一种最大置信上界经验采样的深度Q网络方法[J].计算机研究与发展, 2018, 55 (8) :1694- 1705) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201808011&amp;v=MDc2MTZXcjNQTHl2U2RMRzRIOW5NcDQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5dmc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Zhu Fei, Wu Wen, Liu Quan, et al.A deep Q-Network method based on upper confidence bound experience sampling[J].Journal of Computer Research and Development, 2018, 55 (8) :1694- 1705 (in Chinese) (朱斐, 吴文, 刘全, 等.一种最大置信上界经验采样的深度Q网络方法[J].计算机研究与发展, 2018, 55 (8) :1694- 1705) 
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_5" title="Silver D, Huang A, Maddison CJ, et al.Mastering the game of go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484- 489" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mastering the game of Go with deep neural networks and tree search">
                                        <b>[5]</b>
                                        Silver D, Huang A, Maddison CJ, et al.Mastering the game of go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484- 489
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_6" title="Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 1998:6-22" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">
                                        <b>[6]</b>
                                        Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 1998:6-22
                                    </a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_7" title="Rummery G A, Niranjan M.On-line Q-learning using connectionist systems[R].Cambridge, UK:Engineering Department, Cambridge University, 1994" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On-line Q-learning using connectionist systems">
                                        <b>[7]</b>
                                        Rummery G A, Niranjan M.On-line Q-learning using connectionist systems[R].Cambridge, UK:Engineering Department, Cambridge University, 1994
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_8" title="Christopher J C H Watkins, Peter Dayan.Q-learning[J].Machine Learning, 1992, 8 (3) :279- 292" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338435&amp;v=MjA4MDI3QmFyTzRIdEhOckl4TllPZ0tZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDM2xVTHJCSWxrPU5q&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Christopher J C H Watkins, Peter Dayan.Q-learning[J].Machine Learning, 1992, 8 (3) :279- 292
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_9" title="Singh S P, Sutton R S.Reinforcement learning with replacing eligibility traces[J].Machine Learning, 1996, 22 (1) :123- 158" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339335&amp;v=MzE4NTRxZWJ1ZHRGQzNsVUxyQklsaz1OajdCYXJPNEh0SE5ySXhNWitnS1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        Singh S P, Sutton R S.Reinforcement learning with replacing eligibility traces[J].Machine Learning, 1996, 22 (1) :123- 158
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_10" title="Williams R J.Simple statistical gradient-fellowing algorithms for connectionist reinforcement learning[J].Machine Learning, 1992, 8 (3) :229- 256" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338429&amp;v=MDQ0MTZOajdCYXJPNEh0SE5ySXhOWU9rR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFVMckJJbGs9&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Williams R J.Simple statistical gradient-fellowing algorithms for connectionist reinforcement learning[J].Machine Learning, 1992, 8 (3) :229- 256
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_11" title="Andrew G B, Sutton R S, Anderson C W.Neuronlike adaptive elements that can solve difficult learning control problems[J].IEEE Transactions on Systems, Man and Cybernetics, 2012, 13 (5) :834- 846" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neuronlike adaptive elements that can solve difficult learning control problems">
                                        <b>[11]</b>
                                        Andrew G B, Sutton R S, Anderson C W.Neuronlike adaptive elements that can solve difficult learning control problems[J].IEEE Transactions on Systems, Man and Cybernetics, 2012, 13 (5) :834- 846
                                    </a>
                                </li>
                                <li id="333">


                                    <a id="bibliography_12" title="Peters J, Schaal S.Natural actor-critic[J].Neurocomputing, 2008, 71 (7) :1080- 1090" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501912639&amp;v=MDY1MjBJMXNkYUJVPU5pZk9mYks3SHRETnFvOUViZW9OQ244d29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        Peters J, Schaal S.Natural actor-critic[J].Neurocomputing, 2008, 71 (7) :1080- 1090
                                    </a>
                                </li>
                                <li id="335">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                    Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529- 533</a>
                                </li>
                                <li id="337">


                                    <a id="bibliography_14" title="Wang Z, Schaul T, Hessel M, et al.Dueling network architectures for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:1995- 2003" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dueling Network Architectures for Deep Reinforcement Learning">
                                        <b>[14]</b>
                                        Wang Z, Schaul T, Hessel M, et al.Dueling network architectures for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:1995- 2003
                                    </a>
                                </li>
                                <li id="339">


                                    <a id="bibliography_15" title="Gu Shixiang, Lillicrap, Timothy, et al.Continuous deep Q-Learning with model-based acceleration[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:2829- 2838" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous deep Q-Learning with model-based acceleration">
                                        <b>[15]</b>
                                        Gu Shixiang, Lillicrap, Timothy, et al.Continuous deep Q-Learning with model-based acceleration[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:2829- 2838
                                    </a>
                                </li>
                                <li id="341">


                                    <a id="bibliography_16" title="Lillicrap T P, Hunt J J, Pritzel A, et al.Continuous control with deep reinforcement learning[J].Computer Science, 2015, 8 (6) :A187" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous control with deep reinforcement learning">
                                        <b>[16]</b>
                                        Lillicrap T P, Hunt J J, Pritzel A, et al.Continuous control with deep reinforcement learning[J].Computer Science, 2015, 8 (6) :A187
                                    </a>
                                </li>
                                <li id="343">


                                    <a id="bibliography_17" title="Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:1928- 1937" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">
                                        <b>[17]</b>
                                        Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:1928- 1937
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_18" title="Yarosav Ganin, Tejas Kulkarni, Igor Babuschkin, et al.Synthesizing programs for images using reinforced adversarial learning[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM, 2018:1652- 1661" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synthesizing programs for images using reinforced adversarial learning">
                                        <b>[18]</b>
                                        Yarosav Ganin, Tejas Kulkarni, Igor Babuschkin, et al.Synthesizing programs for images using reinforced adversarial learning[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM, 2018:1652- 1661
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_19" title="Jun Feng, Li Zhao, Xiaoyan Zhu.Reinforcement learning for relation classification from noisy data[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2018:5779- 5786" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning for relation classification from noisy data">
                                        <b>[19]</b>
                                        Jun Feng, Li Zhao, Xiaoyan Zhu.Reinforcement learning for relation classification from noisy data[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2018:5779- 5786
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_20" title="Li Jiwei, Monroe W, Ritter A, et al.Deep reinforcement learning for dialogue generation[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:1192- 1202" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Reinforcement Learning for Dialogue Generation">
                                        <b>[20]</b>
                                        Li Jiwei, Monroe W, Ritter A, et al.Deep reinforcement learning for dialogue generation[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:1192- 1202
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_21" title="Tangkaratt V, Abdolmaleki A, Sugiyama M.Guide actor-critic for continuous control[C] //Proc of the 6th Int Conf on Learning Representations.San Juan, CA:ICLR, 2018:1925- 1937" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Guide actor-critic for continuous control">
                                        <b>[21]</b>
                                        Tangkaratt V, Abdolmaleki A, Sugiyama M.Guide actor-critic for continuous control[C] //Proc of the 6th Int Conf on Learning Representations.San Juan, CA:ICLR, 2018:1925- 1937
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_22" title="Sutton R S, David A M, et al.Policy gradient methods for reinforcement learning with function approximation[C] //Proc of the 12th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2000:1057- 1063" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Policy Gradient Methods for Reinforcement Learning with Function Approximation">
                                        <b>[22]</b>
                                        Sutton R S, David A M, et al.Policy gradient methods for reinforcement learning with function approximation[C] //Proc of the 12th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2000:1057- 1063
                                    </a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_23" title="Silver D, Lever G, Heess N, et al.Deterministic policy gradient algorithms[C] //Proc of the 31st Int Conf on Machine Learning.New York:ACM, 2014:387- 395" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deterministic policy gradient algorithms">
                                        <b>[23]</b>
                                        Silver D, Lever G, Heess N, et al.Deterministic policy gradient algorithms[C] //Proc of the 31st Int Conf on Machine Learning.New York:ACM, 2014:387- 395
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_24" title="Schulman J, Levine S, Abbeel P, et al.Trust region policy optimization[C] //Proc of the 32nd Int Conf on Machine Learning.New York:ACM, 2015:1889- 1897" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trust region policy optimization">
                                        <b>[24]</b>
                                        Schulman J, Levine S, Abbeel P, et al.Trust region policy optimization[C] //Proc of the 32nd Int Conf on Machine Learning.New York:ACM, 2015:1889- 1897
                                    </a>
                                </li>
                                <li id="359">


                                    <a id="bibliography_25" title="Uhlenbeck G E, Ornstein L S.On the theory of the {Brownian} motion[J].Revista Latinoamericana De Microbiolog&#237;a, 1973, 15 (1) :No.29" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the theory of the {Brownian} motion">
                                        <b>[25]</b>
                                        Uhlenbeck G E, Ornstein L S.On the theory of the {Brownian} motion[J].Revista Latinoamericana De Microbiolog&#237;a, 1973, 15 (1) :No.29
                                    </a>
                                </li>
                                <li id="361">


                                    <a id="bibliography_26" title="Lowe R, Wu Yi, Tamar A, et al.Multi-agent actor-critic for mixed cooperative-competitive environments[C] //Proc of the 30th Annual Conf on Neural Information Processing System.Cambridge, MA:MIT Press, 2017:6382- 6393" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-agent actor-critic for mixed cooperative-competitive environments">
                                        <b>[26]</b>
                                        Lowe R, Wu Yi, Tamar A, et al.Multi-agent actor-critic for mixed cooperative-competitive environments[C] //Proc of the 30th Annual Conf on Neural Information Processing System.Cambridge, MA:MIT Press, 2017:6382- 6393
                                    </a>
                                </li>
                                <li id="363">


                                    <a id="bibliography_27" title="Brockman G, Cheung V, Pettersson L, et al.OpenAI gym[CP].CoRR, abs/1606.01540, 2016.https://arxiv.org/pdf/1606.0154" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OpenAI gym[CP]">
                                        <b>[27]</b>
                                        Brockman G, Cheung V, Pettersson L, et al.OpenAI gym[CP].CoRR, abs/1606.01540, 2016.https://arxiv.org/pdf/1606.0154
                                    </a>
                                </li>
                                <li id="365">


                                    <a id="bibliography_28" title="Todorov E, Erez T, Tassa Y.Mujoco:A physics engine for model-based control[C] //Proc of 2012 IEEE/RSJ Int Conf on Intelligent Robots and Systems.Piscataway, NJ:IEEE, 2012:5026- 5033" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MuJoCo:A physics engine for model-based control">
                                        <b>[28]</b>
                                        Todorov E, Erez T, Tassa Y.Mujoco:A physics engine for model-based control[C] //Proc of 2012 IEEE/RSJ Int Conf on Intelligent Robots and Systems.Piscataway, NJ:IEEE, 2012:5026- 5033
                                    </a>
                                </li>
                                <li id="367">


                                    <a id="bibliography_29" title="Dhariwal P, Hesse N, Manning C, et al.OpenAI baselines[CP].GitHub, 2017.https://github.com/openai/baselines" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OpenAI baselines[CP]">
                                        <b>[29]</b>
                                        Dhariwal P, Hesse N, Manning C, et al.OpenAI baselines[CP].GitHub, 2017.https://github.com/openai/baselines
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1708-1720 DOI:10.7544/issn1000-1239.2019.20190155            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于经验指导的深度确定性多行动者-评论家算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%BA%A2%E5%90%8D&amp;code=42527954&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈红名</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%85%A8&amp;code=09886962&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘全</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%97%AB%E5%B2%A9&amp;code=27933643&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闫岩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E6%96%8C&amp;code=26851467&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E7%8E%89%E6%96%8C&amp;code=41590362&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜玉斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%90%B3%E7%90%B3&amp;code=10756912&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张琳琳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0240077&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E7%9C%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6)&amp;code=1046463&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏省计算机信息处理技术重点实验室(苏州大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%AC%A6%E5%8F%B7%E8%AE%A1%E7%AE%97%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B%E6%95%99%E8%82%B2%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%90%89%E6%9E%97%E5%A4%A7%E5%AD%A6)&amp;code=0069758&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">符号计算与知识工程教育部重点实验室(吉林大学)</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BD%AF%E4%BB%B6%E6%96%B0%E6%8A%80%E6%9C%AF%E4%B8%8E%E4%BA%A7%E4%B8%9A%E5%8C%96%E5%8D%8F%E5%90%8C%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">软件新技术与产业化协同创新中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>连续控制问题一直是强化学习研究的一个重要方向.近些年深度学习的发展以及确定性策略梯度 (deterministic policy gradients, DPG) 算法的提出, 为解决连续控制问题提供了很多好的思路.这类方法大多在动作空间中加入外部噪声源进行探索, 但是它们在一些连续控制任务中的表现并不是很好.为更好地解决探索问题, 提出了一种基于经验指导的深度确定性多行动者-评论家算法 (experience-guided deep deterministic actor-critic with multi-actor, EGDDAC-MA) , 该算法不需要外部探索噪声, 而是从自身优秀经验中学习得到一个指导网络, 对动作选择和值函数的更新进行指导.此外, 为了缓解网络学习的波动性, 算法使用多行动者-评论家模型, 模型中的多个行动者网络之间互不干扰, 各自执行情节的不同阶段.实验表明:相比于DDPG, TRPO和PPO算法, EGDDAC-MA算法在GYM仿真平台中的大多数连续任务中有更好的表现.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A1%AE%E5%AE%9A%E6%80%A7%E8%A1%8C%E5%8A%A8%E8%80%85-%E8%AF%84%E8%AE%BA%E5%AE%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">确定性行动者-评论家;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%8F%E9%AA%8C%E6%8C%87%E5%AF%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">经验指导;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%93%E5%AE%B6%E6%8C%87%E5%AF%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">专家指导;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%A1%8C%E5%8A%A8%E8%80%85&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多行动者;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *刘全, quanliu@suda.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61772355, 61702055, 61472262, 61502323, 61502329);</span>
                                <span>江苏省高等学校自然科学研究重大项目 (18KJA520011, 17KJA520004);</span>
                                <span>苏州市应用基础研究计划工业部分项目 (SYG201422);</span>
                    </p>
            </div>
                    <h1><b>An Experience -Guided Deep Deterministic Actor -Critic Algorithm with Multi -Actor</b></h1>
                    <h2>
                    <span>Chen Hongming</span>
                    <span>Liu Quan</span>
                    <span>Yan Yan</span>
                    <span>He Bin</span>
                    <span>Jiang Yubin</span>
                    <span>Zhang Linlin</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Soochow University</span>
                    <span>Provincial Key Laboratory for Computer Information Processing Technology (Soochow University)</span>
                    <span>Key Laboratory of Symbolic Computation and Knowledge Engineering (Jilin University) , Ministry of Education</span>
                    <span>Collaborative Innovation Center of Novel Software Technology and Industrialization</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The continuous control task has always been an important research direction in reinforce-ment learning. In recent years, the development of deep learning (DL) and the advent of deterministic policy gradients algorithm (DPG) , provide many good ideas for solving continuous control problems. The main difficulty faced by these methods is the exploration in the continuous action space. And some of them engage in exploratory behavior through external noise injection in the action space. However, this exploration method does not perform well in some continuous control tasks. This paper proposes an experience-guided deep deterministic actor-critic algorithm with multi-actor (EGDDAC-MA) without external noise, which learns a guiding network from excellent experiences to guide the updates of the actor network and the critic network. Besides, it uses a multi-actor actor-critic (AC) model which configures different actors for each phase in an episode. These actors are independent of each other and do not interfere with each other. Finally, the experimental results show that compared with DDPG, TRPO and PPO algorithms, the proposed algorithm has better performance in most continuous tasks in GYM simulation platform.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deterministic%20actor-critic&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deterministic actor-critic;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=experience%20guiding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">experience guiding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=expert%20guiding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">expert guiding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-actor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-actor;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Chen Hongming, born in 1994.Master candidate. His main research interests include reinforcement learning and deep reinforcement learning.<image id="396" type="" href="images/JFYZ201908014_39600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Quan, born in 1969. PhD and professor.Member of CCF.His main research interests include reinforcement learning, and automated reasoning. <image id="398" type="" href="images/JFYZ201908014_39800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Yan Yan, born in 1994.Master candidate. His  main  research  interests  include reinforce-ent learning, deep learning and deep reinforcement learning. (20165222005@ stu.suda.edu.cn) <image id="400" type="" href="images/JFYZ201908014_40000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    He Bin, born in 1991.Master candidate. His  main  research  interest  include reinforcement learning. (20175227047@ stu.suda.edu.cn) <image id="402" type="" href="images/JFYZ201908014_40200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Jiang Yubin, born in 1994. Master candidate. His main research interests include  reinforcement  learning, deep learning and deep reinforcement learning. (20164227021@stu.suda.edu.cn) <image id="404" type="" href="images/JFYZ201908014_40400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhang Linlin, born in 1994. Master candidate.Her main research interests include reinforcement learning and deep reinforcement learning. (20175227007@ stu.suda.edu.cn) <image id="406" type="" href="images/JFYZ201908014_40600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-19</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61772355, 61702055, 61472262, 61502323, 61502329);</span>
                                <span>the Natural Science Research University Major Projects of Jiangsu Province (18KJA520011, 17KJA520004);</span>
                                <span>Suzhou Industrial Application of Basic Research Program (SYG201422);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="67">目前, 强化学习已经广泛应用于仿真模拟、工业控制和博弈游戏等领域<citation id="370" type="reference"><link href="311" rel="bibliography" /><link href="313" rel="bibliography" /><link href="315" rel="bibliography" /><link href="317" rel="bibliography" /><link href="319" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>.强化学习 (reinforcement learning) 的目标是学习一个最优策略使得智能体 (agent) 能够获得最大的累积奖赏<citation id="369" type="reference"><link href="321" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.强化学习方法大体上可以分为3类:基于值函数的方法、策略搜索方法 (或策略梯度方法) 和行动者-评论家方法.</p>
                </div>
                <div class="p1">
                    <p id="68">基于值函数的方法通过学习一个值函数获得一个最优策略, 这种方法适用于离散动作空间的任务, 对于连续动作空间来说是并不适用的.例如Rummery和Niranjan<citation id="371" type="reference"><link href="323" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的Sarsa算法、Watkins等人<citation id="372" type="reference"><link href="325" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的Q-Learning算法.对比基于值函数的方法, 策略搜索方法并没有学习值函数而是直接学习一个策略, 使得累积奖赏最大化.例如Williams提出的基于蒙特卡洛方法<citation id="373" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation> (Monte Carlo methods, MC) 的强化 (reinforce) 算法和使用基线的强化 (reinforce with baseline) 算法<citation id="374" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 后者是前者的泛化.行动者-评论家算法结合了基于值的方法和策略搜索方法, 其中参数化的策略称为行动者, 学习到的值函数称为评论家.例如Barto和Sutton等人<citation id="375" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出的行动者-评论家算法 (actor-critic, AC) , Peters和Schaal提出的自然行动者-评论家方法<citation id="376" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> (natural actor-critic, NAC) .</p>
                </div>
                <div class="p1">
                    <p id="69">传统强化学习面临的问题是对于高维状态动作空间感知能力不足.最近几年随着深度学习 (deep learning, DL) 的流行, 由于其对高维状态动作空间有很好的表示能力, 因此深度学习与传统强化学习的结合产生了深度强化学习 (deep reinforcement learning, DRL) 这一研究热点.这一类方法在一些游戏和机器人控制任务上取得了不错的成果.比如基于Q-Learning的深度Q网络 (deep Q-network, DQN) <citation id="377" type="reference"><link href="335" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>算法在49个Atari 2600游戏中的实验结果超过以往所有算法, 并且可以媲美职业人类玩家的水平.在DQN之上有很多改进的算法版本, 例如在此基础上提出的竞争网络结构<citation id="378" type="reference"><link href="337" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation> (dueling network architecture, DNA) 和可用于连续动作空间的归一化优势函数连续Q学习<citation id="379" type="reference"><link href="339" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation> (continuous Q-Learning with normalized advantage functions, NAF) 等, 还有基于行动者-评论家方法的深度确定性策略梯度<citation id="380" type="reference"><link href="341" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> (deep deterministic policy gradient, DDPG) 方法, 以及异步优势行动者-评论家<citation id="381" type="reference"><link href="343" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation> (asynchronous advantage actor-critic, A3C) 方法等.此外深度强化学习在其他研究方向比如图像处理、自然语言处理等都有一些重要应用<citation id="382" type="reference"><link href="345" rel="bibliography" /><link href="347" rel="bibliography" /><link href="349" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="70">基于值函数的方法像深度Q网络等, 大多是根据值函数通过<i>ε</i>-greedy策略来选择动作, 即以<i>ε</i>的概率随机选择动作, 以1-<i>ε</i>的概率选择具有最大值的动作.这类方法在离散动作空间任务中具有很好的效果, 而对于连续控制任务却不是很适用<citation id="383" type="reference"><link href="351" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 这是因为连续动作空间中具有最大值的动作不易确定.基于策略梯度的方法可以分为随机策略梯度<citation id="384" type="reference"><link href="353" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation> (stochastic policy gradients, SPG) 和确定性策略梯度<citation id="385" type="reference"><link href="355" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation> (deterministic policy gradients, DPG) .随机策略梯度在选择动作时输出是每个可能的动作的概率, 这类方法也不太适用于连续动作空间任务.而确定性策略梯度方法在选择行动时, 策略的输出是一个确定的动作, 因此可以很好地应用于连续控制任务.确定性策略梯度与AC方法的结合形成了确定性AC方法<citation id="386" type="reference"><link href="355" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation> (deterministic actor-critic, DAC) , 比如DDPG.这类方法虽然可以很好地适用于连续动作空间, 但是其性能很大程度上取决于探索方法的好坏.它们一般通过在动作中加入外部噪声实现探索或者使用高斯策略实现探索, 但这些探索方式实际上是盲目的, 因此在一些连续控制任务表现不是很好.</p>
                </div>
                <div class="p1">
                    <p id="71">为了提高确定性AC方法在连续控制问题上的性能, 本文提出了基于经验指导的深度确定性多行动者-评论家算法 (experience-guided deep deter-ministic actor-critic with multi-actor, EGDDAC-MA) . EGDDAC-MA并不需要外部探索噪声源, 而是从自身优秀经验中学习一个指导网络, 对行动的选择和评论家网络的更新进行指导.此外为了缓解单个网络的学习压力, EGDDAC-MA使用了多个行动者网络, 各个行动者网络之间互不干扰, 执行情节的不同阶段.</p>
                </div>
                <div class="p1">
                    <p id="72">实验上, 本文首先对比基于经验的指导相比于外部探索噪声的优势, 证明了多行动者机制可以有效缓解网络学习波动, 然后比较了深度确定性策略梯度算法 (deep deterministic policy gradient,  DDPG) 、置信区域策略优化算法<citation id="387" type="reference"><link href="357" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> (trust region policy optimization, TRPO) 、对TRPO进行改进的近端策略优化算法 (proximal policy optimization Algorithms, PPO) 和EGDDAC-MA在多个连续任务上的性能.本文还使用了专家经验来取代自身优秀经验进行实验, 发现在提供专家经验条件下, EGDDAC-MA可以快速学到一个不错的策略.</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag"><b>1 背景知识</b></h3>
                <h4 class="anchor-tag" id="74" name="74"><b>1.1 强化学习和随机行动者-评论家算法</b></h4>
                <div class="p1">
                    <p id="75">强化学习问题通常使用Markov决策过程 (Markov decision process, MDP) 进行建模.一个MDP问题可以用一个四元组 (<i>S</i>, <i>A</i>, <i>R</i>, <i>P</i>) 表示, 其中<i>S</i>为状态集合, <i>A</i>为动作集合, <i>R</i>为奖赏函数, <i>P</i>为状态转移函数.在与环境<i>E</i>交互过程中, 每个时间步agent在状态<i>s</i><sub><i>t</i></sub>执行动作<i>a</i><sub><i>t</i></sub>, 获得奖赏<i>r</i><sub><i>t</i>+1</sub>并到达下一个状态<i>s</i><sub><i>t</i>+1</sub>, 这里<i>s</i><sub><i>t</i></sub>∈<i>S</i>, <i>a</i><sub><i>t</i></sub>∈<i>A</i>, <i>r</i><sub><i>t</i></sub>=<i>R</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) , <i>S</i>⊆R<sup><i>n</i><sub><i>s</i></sub></sup>, <i>A</i>⊆R<sup><i>n</i><sub><i>a</i></sub></sup>.Agent的目标是最大化累积奖赏:</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>t</mi><mo>′</mo></msup><mo>=</mo><mi>t</mi></mrow><mi>Τ</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mrow><msup><mi>t</mi><mo>′</mo></msup><mo>-</mo><mi>t</mi></mrow></msup><mi>r</mi><msub><mrow></mrow><msup><mi>t</mi><mo>′</mo></msup></msub></mrow></math></mathml>. (1) </p>
                </div>
                <div class="p1">
                    <p id="78">作为强化学习中的一种重要方法, 随机行动者-评论家算法 (stochastic actor-critic) 使用随机策略梯度来更新策略, 其中行动者 (actor) 和评论家 (critic) 进行了参数化处理, 这里用<i>π</i> (<i>a</i>|<i>s</i>, <i>θ</i><sup><i>π</i></sup>) :<i>S</i>→<i>P</i> (<i>A</i>) 和<i>Q</i> (<i>s</i>, <i>a</i>|<i>θ</i><sup><i>q</i></sup>) 分别表示行动者 (策略) 和评论家 (动作值函数) , 其中, <i>θ</i><sup><i>π</i></sup>和<i>θ</i><sup><i>q</i></sup>是参数, <i>P</i> (<i>A</i>) 表示动作空间概率分布.策略和动作值函数可以是线性的, 也可以使用神经网络表示.行动者-评论家算法的目标是寻找一个最优策略使得累积奖赏最大化.</p>
                </div>
                <div class="p1">
                    <p id="79">在强化学习中, 无论是状态值函数还是动作值函数都满足贝尔曼方程:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>Q</i><sup><i>π</i></sup> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) =<i>E</i><sub><i>s</i><sub><i>t</i></sub>～<i>E</i>, <i>a</i><sub><i>t</i></sub>～<i>π</i>, <i>r</i><sub><i>t</i>+1</sub>=<i>R</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) </sub>[<i>r</i><sub><i>t</i>+1</sub>+</p>
                </div>
                <div class="p1">
                    <p id="81"><i>γE</i><sub><i>a</i><sub><i>t</i>+1</sub>～<i>π</i></sub>[<i>Q</i><sup><i>π</i></sup> (<i>s</i><sub><i>t</i>+1</sub>, <i>a</i><sub><i>t</i>+1</sub>) ]]. (2) </p>
                </div>
                <div class="p1">
                    <p id="82">式 (2) 中, 由于期望回报是不可知的, 所以值函数在随机行动者-评论家算法中是用来做评估的, 用于计算TD 误差 (TD error) :</p>
                </div>
                <div class="p1">
                    <p id="83"><i>δ</i>=<i>r</i><sub><i>t</i>+1</sub>+<i>γQ</i> (<i>s</i><sub><i>t</i>+1</sub>, <i>a</i><sub><i>t</i>+1</sub>|<i>θ</i><sup><i>q</i></sup>) -<i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>|<i>θ</i><sup><i>q</i></sup>) , (3) </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <i>γ</i>是折扣因子, 根据随机策略梯度理论<citation id="388" type="reference"><link href="321" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation> (stochastic policy gradient theorem) , 策略<i>π</i> (<i>a</i>|<i>s</i>, <i>θ</i><sup><i>π</i></sup>) 参数更新所使用的梯度可以表示为</p>
                </div>
                <div class="area_img" id="407">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908014_40700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="91">其中, <i>θ</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>π</mi></msubsup></mrow></math></mathml>=<i>θ</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>π</mi></msubsup></mrow></math></mathml>+<i>α</i><sub><i>θ</i><sup><i>π</i></sup></sub><i>γ</i><sup><i>t</i></sup><i>δ</i><image id="94" type="" href="images/JFYZ201908014_09400.jpg" display="inline" placement="inline"><alt></alt></image><sub><i>θ</i><sup><i>π</i></sup></sub> (ln (<i>π</i> (<i>a</i><sub><i>t</i></sub>|<i>s</i><sub><i>t</i></sub>, <i>θ</i><sup><i>π</i></sup>) ) ) , <i>b</i> (<i>s</i>) 是与动作无关的量, 作为基线使用可以降低方差并加速学习.最终, 行动者和评论家的参数更新为</p>
                </div>
                <div class="area_img" id="409">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908014_40900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="103">其中, <i>α</i><sub><i>θ</i><sup><i>q</i></sup></sub>, <i>α</i><sub><i>θ</i><sup><i>π</i></sup></sub>是梯度更新的步长参数.</p>
                </div>
                <div class="p1">
                    <p id="104">行动者-评论家算法的模型如图1所示:</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 行动者-评论家算法" src="Detail/GetImg?filename=images/JFYZ201908014_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 行动者-评论家算法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The diagram of Actor-Critic framework</p>

                </div>
                <div class="p1">
                    <p id="106">根据图1, 算法首先初始化策略和值函数, 进入循环, 在每个时间步<i>t</i>, 策略在状态<i>s</i><sub><i>t</i></sub>选择动作<i>a</i><sub><i>t</i></sub>并执行, 环境给出下一个状态<i>s</i><sub><i>t</i>+1</sub>和奖赏<i>r</i><sub><i>t</i>+1</sub>作为反馈, 然后使用式 (3) 计算出TD误差, 最后使用式 (5) 和 (6) 来更新策略和值函数参数, 重复执行以上步骤直至收敛.</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>1.2 深度确定性策略梯度算法</b></h4>
                <div class="p1">
                    <p id="108">强化学习使用深度神经网络来逼近值函数时, 会表现得不稳定甚至会发散.因此同DQN中一样, 深度确定性策略梯度算法 (deep deterministic policy gradient, DDPG) 使用了目标网络和经验重放2个机制.</p>
                </div>
                <div class="p1">
                    <p id="109">深度确定性策略梯度算法是确定性策略梯度算法与行动者-评论家算法的结合.与随机策略梯度中定义的策略形式不同.在确定性行动者-评论家方法中用<i>π</i> (<i>s</i>|<i>θ</i><sup><i>π</i></sup>) :<i>S</i>→<i>A</i>表示行动者网络, 注意<i>S</i>指向的是动作空间而不是动作空间的概率分布, 用<i>Q</i> (<i>s</i>, <i>a</i>|<i>θ</i><sup><i>q</i></sup>) 来表示评论家网络, 这里<i>θ</i><sup><i>π</i></sup>和<i>θ</i><sup><i>q</i></sup>表示网络参数.同时使用<i>π</i> (<i>s</i>|<i>θ</i><sup><i>π</i>′</sup>) 和<i>Q</i> (<i>s</i>, <i>a</i>|<i>θ</i><sup><i>q</i>′</sup>) 表示目标行动者网络和目标评论家网络.</p>
                </div>
                <div class="p1">
                    <p id="110">根据确定性策略梯度理论<citation id="389" type="reference"><link href="355" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation> (deterministic policy gradient theorem) , 确定性策略的策略梯度可以表示为</p>
                </div>
                <div class="area_img" id="408">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908014_40800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="117">DDPG评论家的优化目标是最小化损失函数:</p>
                </div>
                <div class="p1">
                    <p id="118"><i>L</i> (<i>θ</i><sup><i>q</i></sup>) =<i>E</i><sub><i>s</i><sub><i>t</i></sub>～<i>E</i>, <i>a</i><sub><i>t</i></sub>=<i>π</i> (<i>s</i><sub><i>t</i></sub>|<i>θ</i><sup><i>π</i></sup>) , <i>r</i><sub><i>t</i>+1</sub>=<i>R</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) </sub></p>
                </div>
                <div class="p1">
                    <p id="119">[ (<i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>|<i>θ</i><sup><i>q</i></sup>) -<i>y</i><sub><i>t</i></sub>) <sup>2</sup>], (8) </p>
                </div>
                <div class="p1">
                    <p id="120">其中:</p>
                </div>
                <div class="p1">
                    <p id="121"><i>y</i><sub><i>t</i></sub>=<i>r</i><sub><i>t</i>+1</sub>+<i>γQ</i> (<i>s</i><sub><i>t</i>+1</sub>, <i>π</i> (<i>s</i><sub><i>t</i>+1</sub>|<i>θ</i><sup><i>π</i>′</sup>) |<i>θ</i><sup><i>q</i>′</sup>) , (9) </p>
                </div>
                <div class="p1">
                    <p id="122">注意在<i>y</i><sub><i>t</i></sub>中, 动作是由目标行动者网络选择的, 状态-动作对的值是由目标评论家网络评估的.</p>
                </div>
                <div class="p1">
                    <p id="123">为解决探索问题, DDPG中使用的噪声是通过奥恩斯坦-乌伦贝克 (Ornstein-Uhlenbeck, OU) 过程<citation id="390" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>生成的时间相关噪声.这里使用参数<i>φ</i>和<i>σ</i>表示为</p>
                </div>
                <div class="p1">
                    <p id="124"><i>n</i><sub><i>t</i></sub>←-<i>n</i><sub><i>t</i>-1</sub><i>φ</i>+<i>N</i> (0, <i>σI</i>) . (10) </p>
                </div>
                <div class="p1">
                    <p id="125">最终动作为</p>
                </div>
                <div class="p1">
                    <p id="126"><i>a</i><sub><i>t</i></sub>=<i>π</i> (<i>s</i><sub><i>t</i></sub>|<i>θ</i><sup><i>π</i></sup>) +<i>n</i><sub><i>t</i></sub>. (11) </p>
                </div>
                <div class="p1">
                    <p id="127">目标网络使用了“soft”的更新方式</p>
                </div>
                <div class="p1">
                    <p id="128"><i>θ</i>′←<i>τ θ</i>+ (1-<i>τ</i>) <i>θ</i>′. (12) </p>
                </div>
                <div class="p1">
                    <p id="129">DDPG中使用的经验重放机制要求算法在每个时间步将得到的经验放入经验池.在训练时, 算法从经验池中随机抽取批量经验用于训练.</p>
                </div>
                <h3 id="130" name="130" class="anchor-tag"><b>2 算  法</b></h3>
                <div class="p1">
                    <p id="131">与DDPG结构不同, EGDDAC-MA定义有一个值函数网络 (critic network) 和目标值函数网络 (target critic network) , 有多个行动者网络 (actor network) 和多个目标行动者网络 (target actor network) .此外, EGDDAC-MA中还定义有多个编码层和多个目标编码层, 分别嵌入在行动者网络和目标行动者网络之中.在EGDDAC-MA中, 行动者网络用<i>π</i> (<i>s</i>|<i>θ</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>π</mi></msubsup></mrow></math></mathml>) 表示, 目标行动者网络用<i>π</i> (<i>s</i>|<i>θ</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><msup><mi>π</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>) 表示, 这里<i>θ</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>π</mi></msubsup></mrow></math></mathml>和<i>θ</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><msup><mi>π</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>表示网络参数, <i>n</i>表示第<i>n</i>个行动者网络或目标行动者网络, 需要注意的是<i>θ</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>π</mi></msubsup></mrow></math></mathml>和<i>θ</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><msup><mi>π</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>同时也包含了对应的编码层和目标编码层的网络参数.EGDDAC-MA中使用<i>B</i><sub><i>n</i></sub>表示第<i>n</i>个行动者网络的经验池.此外还定义了一个专门存放优秀情节经验的经验池<i>B</i>′和一个指导网络<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) .图2显示的是整个模型结构.</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 EGDDAC-MA结构示意图" src="Detail/GetImg?filename=images/JFYZ201908014_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 EGDDAC-MA结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 An overview of EGDDAC-MA</p>

                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>2.1 多行动者-评论家模型</b></h4>
                <div class="p1">
                    <p id="140">一般来说AC方法中只会在同一个情节中使用一个行动者网络比如DDPG, 或者是多个行动者网络分别并行执行不同的情节比如像A3C, 又或者像MAAC<citation id="391" type="reference"><link href="361" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation> (multi-agent actor-critic) 一样通过多个agent之间合作学习.而EGDDAC-MA中定义的多个行动者网络, 在情节之中不是并行的, 也没有交流与合作, 而是在同一个情节中针对不同阶段使用不同的行动者网络.对于学习任务, EGDDAC-MA将任务情节进行阶段划分, 每个阶段配置单独的行动者网络和经验池.</p>
                </div>
                <div class="p1">
                    <p id="141">某些任务在情节的不同阶段可能由于状态空间和动作空间之间的映射变化造成学习波动, 对于单个行动者网络来说就会很难学习 (3.4节部分进行实验说明) .而多个行动者网络, 它们在各自所控制的阶段学习, 互不干扰, 在一定程度上缓解了学习的波动.此外, 单个行动者网络学习率是固定, 但是对于多个行动者网络来说, 其每个阶段的学习率是可以不同的.这样实际上对于每一个情节, EGDDAC-MA使用了多个策略进行控制.</p>
                </div>
                <div class="p1">
                    <p id="142">本算法中采用的是每隔<i>N</i>步设置一个阶段.即在一个情节中, 第1个<i>N</i>步为第1个阶段, 对应第1个行动者网络;第2个<i>N</i>步为第2个阶段, 对应第2个行动者网络;以次类推.若情节长度是<i>T</i>, 则EGDDAC-MA的行动者网络数是-<i>T</i>/<i>N</i>-.但是这样进行阶段划分存在一个问题就是, 如果情节长度不断增长, 那么需要的行动者网络数量也会随着增长.针对这个问题, 可以做一个周期性变动, 也就是EGDDAC-MA可以以<i>N</i>个时间步作为一个阶段, 同时以<i>M</i>个阶段作为一个周期.这样的设置使得无论情节长度是多少, 模型的行动者网络数一直是<i>M</i>个.</p>
                </div>
                <div class="p1">
                    <p id="143">模型中还有一个编码过程, 这个过程是嵌入到行动者网络中的.其输入的是状态<i>s</i><sub><i>t</i></sub>, 输出状态信号<i>ss</i><sub><i>t</i></sub>, 对于具有较高维度的状态空间, <i>ss</i><sub><i>t</i></sub>的维度要比<i>s</i><sub><i>t</i></sub>的维度要低, <i>ss</i><sub><i>t</i></sub>会作为行动者网络和指导网络的输入.实际上在状态空间中, 有许多状态是相似的, 那么在进行动作选择时, 它们的最优动作很可能是相同的.通过编码过程的降维, 将状态空间映射到低维空间中, 这样就可以使得相似的状态在一定程度上重合, 减小了状态空间的大小.并且优秀的经验会以元组 (<i>ss</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) 的形式放入经验池<i>B</i>′用于训练指导网络, 从而加速<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) 的学习.</p>
                </div>
                <div class="p1">
                    <p id="144">要注意的是, EGDDAC-MA中并没有把状态信号<i>ss</i><sub><i>t</i></sub>用于评论家网络, 这是因为编码层的参数在不断更新, 因此同一个状态在编码层中得到的状态信号会不断变化, 这样就不利于评论家网络进行评估.此外, 评论家网络也没有创建多个, 因为评论家网络在进行网络参数更新时, 使用了当前状态-动作对的值函数<i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>|<i>θ</i><sup><i>q</i></sup>) 作为预测值, 使用后继状态来计算目标值, 然后通过反向传播进行参数更新.在预测值和目标值的计算之中不仅使用了前一个状态, 还使用了后一个状态, 而样本是从经验池中随机抽样的, 无法判断样本中后继状态是否属于下一个阶段, 这样在阶段连接处的状态, 用其来进行训练时就不好计算评论家训练所需的目标值, 因此模型中就没有使用多个评论家的结构.</p>
                </div>
                <div class="p1">
                    <p id="145">EGDDAC-MA在学习过程中, 每个时间步, 首先判断该时间步属于的阶段, 使用对应阶段的行动者网络来生成原始动作.更新网络参数时, 只有对应的那一个行动者网络会被更新, 并通过确定性策略梯度理论计算梯度:</p>
                </div>
                <div class="area_img" id="410">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908014_41000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="154">EGDDAC-MA的目标行动者网络的更新也是使用“soft”更新方式.其评论家网络的更新由于受到指导网络的影响, 将在2.2节详细介绍.</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155"><b>2.2 基于经验的指导</b></h4>
                <div class="p1">
                    <p id="156">连续动作空间的一个挑战是探索问题的解决.一般来说, 离散动作空间问题的探索是通过改变动作的选择概率来实现的.而连续动作空间中由于动作的连续性不方便为每个动作分配相应的概率, 因此通过改变选择概率来实现探索就不适用于连续动作空间.由于动作是连续变化的, 因此可以通过直接改变动作来实现探索, 通常是直接在动作空间中加上外部探索噪声, 比如DDPG中使用的OU噪声.但是这种探索是盲目的, 并不能有效地学习到优秀经验.与DDPG使用外部噪声不同, 本文提出的EGDDAC-MA并不需要额外的噪声源, 而是通过自身优秀经验指导学习.</p>
                </div>
                <div class="p1">
                    <p id="157">在DDPG算法的学习过程中, agent会遇到一些具有高回报的轨迹, 这些轨迹中包含有许多有用的信息, 但是这些信息并没有被有效利用.因此, 为了利用这些经验, EGDDAC-MA中定义了一个存储优秀经验的经验池<i>B</i>′.<i>B</i>′的大小是一定的, 不同于普通经验池的是, 其存放经验的过程是其本身的进化过程.此外基于这个经验池, 定义了一个指导网络<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) .</p>
                </div>
                <div class="p1">
                    <p id="158">对于普通的经验池, 在每个时间步, 根据该时间步所属的阶段, 经验会以五元组 (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>, <i>r</i><sub><i>t</i>+1</sub>, <i>s</i><sub><i>t</i>+1</sub>, <i>ss</i><sub><i>t</i>+1</sub>) 的形式放入相应的经验池中, 这里的<i>ss</i><sub><i>t</i>+1</sub>在式 (19) 中用于指导评论家更新.对于经验池<i>B</i>′, 并不是在每个时间步放入经验, 而是在每个情节结束时, 先判断该情节是否是优秀的情节, 若是, 则放入<i>B</i>′中, 否则舍去.注意放入经验时, 是以情节经验[ (<i>ss</i><sub>0</sub>, <i>a</i><sub>0</sub>) , (<i>ss</i><sub>1</sub>, <i>a</i><sub>1</sub>) , …, <i>T</i>]的形式放入的, <i>T</i>是情节结束标志.每个情节是否优秀是相对的, 会随着学习进程而变化.其判断标准如下:</p>
                </div>
                <div class="p1">
                    <p id="159"><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mi>r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>r</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>r</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>≥</mo><mover accent="true"><mi>G</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msub></mrow></math></mathml>, (14) </p>
                </div>
                <div class="p1">
                    <p id="161">其中:</p>
                </div>
                <div class="p1">
                    <p id="162"><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>G</mi><mo>¯</mo></mover><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>G</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>G</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>G</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mi>k</mi></mfrac></mrow></math></mathml>      (15) </p>
                </div>
                <div class="p1">
                    <p id="164">表示已经放入经验池<i>B</i>′中的最近<i>k</i>个优秀情节的回报均值, <i>m</i>表示第<i>m</i>个情节, <i>G</i><sub><i>m</i></sub>表示其回报.</p>
                </div>
                <div class="p1">
                    <p id="165">根据式 (14) , 当整个情节具有好的回报时, 情节经验才放入<i>B</i>′中.假设当前为第<i>m</i>个情节, 若当前情节的回报超过或者等于<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>G</mi><mo>¯</mo></mover></math></mathml><sub><i>k</i>, <i>m</i></sub>, 则将当前情节经验放入经验池<i>B</i>′中, 然后在第<i>m</i>+1个情节判断时, <mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>G</mi><mo>¯</mo></mover></math></mathml><sub><i>k</i>, <i>m</i>+1</sub>的计算会将式 (15) 分子中第1个情节回报舍去, 而将第<i>m</i>个情节的回报包含进去.要注意的是, 经验池<i>B</i>′的容量要远小于普通经验池的容量, 这是要保证经验池中的经验及时更新, 使得其中保存的都是最近的优秀经验.本文在3.3节使用了一个模拟实验来说明经验池<i>B</i>′中的经验是不断进化的.</p>
                </div>
                <div class="p1">
                    <p id="168">指导网络<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) 是在<i>B</i>′上通过监督学习而来的, 其训练所用的损失函数为</p>
                </div>
                <div class="p1">
                    <p id="169"><i>L</i> (<i>θ</i><sup><i>g</i></sup>) =<i>E</i>[ (<i>G</i> (<i>ss</i><sub><i>t</i></sub>|<i>θ</i><sup><i>g</i></sup>) -<i>a</i><sub><i>t</i></sub>) <sup>2</sup>]. (16) </p>
                </div>
                <div class="p1">
                    <p id="170">指导网络<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) 在进化的经验池<i>B</i>′上通过监督学习学习, 这样<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) 学习到的经验也是不断进化的.并且经验池<i>B</i>′中的经验是比普通经验池中的经验更好的, 而经验网和行动网事实上都是状态到动作的映射, 因此在进行动作选择时, 可以使用经验网为动作加上一个指导项</p>
                </div>
                <div class="p1">
                    <p id="171"><i>a</i><sub><i>t</i></sub>=<i>π</i> (<i>s</i><sub><i>t</i></sub>|<i>θ</i><mathml id="172"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>π</mi></msubsup></mrow></math></mathml>) +<i>ξ</i> (<i>G</i> (<i>ss</i><sub><i>t</i></sub>|<i>θ</i><sup><i>g</i></sup>) -<i>π</i> (<i>s</i><sub><i>t</i></sub>|<i>θ</i><mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>π</mi></msubsup></mrow></math></mathml>) ) , (17) </p>
                </div>
                <div class="p1">
                    <p id="174">其中, <i>ξ</i>是干扰系数, 0&lt;<i>ξ</i>≪1.通过式 (17) , 指导网络会引导行动者网络向具有高回报的动作方向进行选择.</p>
                </div>
                <div class="p1">
                    <p id="175">由梯度式 (13) 可知评论家网络也影响着行动者网络的学习.根据式 (11) , DDPG只在动作空间中加入探索噪声, 而EGDDAC-MA中的指导网络<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) 不仅指导动作的选择而且还对评论家网络的更新进行指导.评论家网络更新所使用的标签值式 (9) 的改写为</p>
                </div>
                <div class="p1">
                    <p id="176"><i>q</i><sub><i>t</i>+1</sub>=<i>Q</i> (<i>s</i><sub><i>t</i>+1</sub>, <i>π</i> (<i>s</i><sub><i>t</i>+1</sub>|<i>θ</i><mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><msup><mi>π</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>) |<i>θ</i><sup><i>q</i>′</sup>) , (18) </p>
                </div>
                <div class="p1">
                    <p id="178"><i>q</i><sub><i>t</i>+1</sub>=<i>q</i><sub><i>t</i>+1</sub>+<i>ξ</i> (<i>Q</i> (<i>s</i><sub><i>t</i>+1</sub>, </p>
                </div>
                <div class="p1">
                    <p id="179"><i>G</i> (<i>ss</i><sub><i>t</i>+1</sub>|<i>θ</i><sup><i>g</i></sup>) |<i>θ</i><sup><i>q</i>′</sup>) -<i>q</i><sub><i>t</i>+1</sub>) , (19) </p>
                </div>
                <div class="p1">
                    <p id="180"><i>y</i><sub><i>t</i></sub>=<i>r</i><sub><i>t</i>+1</sub>+<i>γq</i><sub><i>t</i>+1</sub>ϕ, (20) </p>
                </div>
                <div class="p1">
                    <p id="181">其中, ϕ是情节是否结束的标志, 若情节结束则其值为0, 否则为1.最终评论家网络更新所使用的损失函数表示为</p>
                </div>
                <div class="p1">
                    <p id="182"><i>L</i> (<i>θ</i><sup><i>q</i></sup>) =<i>E</i><sub><i>s</i><sub><i>t</i></sub>～<i>E</i>, <i>a</i><sub><i>t</i></sub>=<i>π</i> (<i>s</i><sub><i>t</i></sub>|<i>θ</i><sup><i>π</i></sup><sub><i>n</i></sub>) , <i>r</i><sub><i>t</i>+1</sub>=<i>R</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) </sub>[ (<i>Q</i> (<i>s</i><sub><i>t</i></sub>, </p>
                </div>
                <div class="p1">
                    <p id="183"><i>a</i><sub><i>t</i></sub>|<i>θ</i><sup><i>q</i></sup>) -<i>y</i><sub><i>t</i></sub>) <sup>2</sup>]. (21) </p>
                </div>
                <div class="p1">
                    <p id="184">每次在进行动作选择时, 指导网络<i>G</i> (<i>ss</i>|<i>θ</i><sup><i>g</i></sup>) 通过式 (17) 指导动作的选择, 并且通过式 (19) 指导评论家网络的学习.由于经验池<i>B</i>′中的经验集合是普通经验池中经验集合的子集, 所以当行动者网络收敛时, 指导网络也就自然收敛了.此时, 式 (17) 和式 (19) 中的指导也就不存在了.可以看出EGDDAC-MA与DDPG一样都属于异策略 (off-policy) 学习, 即学习的策略和执行的策略是不一样的.整个算法的过程如算法1所示.</p>
                </div>
                <div class="area_img" id="411">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201908014_41100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="411">
                                <img alt="" src="Detail/GetImg?filename=images/JFYZ201908014_41101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="243" name="243" class="anchor-tag"><b>3 实  验</b></h3>
                <h4 class="anchor-tag" id="244" name="244"><b>3.1 实验平台及实验介绍</b></h4>
                <div class="p1">
                    <p id="245">本文采用了OpenAI GYM平台<citation id="392" type="reference"><link href="363" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>上Mujoco物理模拟器<citation id="393" type="reference"><link href="365" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>中的6个连续性控制任务作为实验环境.GYM是开发和对比强化学习的一个开源工具包, 其提供了各种连续控制性任务的环境接口, 旨在促进机器人、生物力学、图形和动画以及其他需要快速精确仿真的领域的研究和开发, 为人工智能研究者提供了丰富的模拟实验环境.此外, 为了进一步说明算法的适用性, 本文还额外增加了2个PyBullet连续任务.PyBullet强化学习环境也是基于GYM平台的, 使用的是Bullet物理模拟器.整体来说PyBullet强化学习环境要比Mujoco环境更难.</p>
                </div>
                <div class="p1">
                    <p id="246">本文使用的6个Mujoco连续任务包括:</p>
                </div>
                <div class="p1">
                    <p id="247">1) Ant.使3D四足蚂蚁形态机器人学会快速向前行走, 如图3所示;</p>
                </div>
                <div class="p1">
                    <p id="248">2) HumanoidStandup.使3D人形态机器人学会快速站立;</p>
                </div>
                <div class="p1">
                    <p id="249">3) Humanoid.使3D人形态机器人学会行走;</p>
                </div>
                <div class="p1">
                    <p id="250">4) HalfCheetah.使2D-猎豹形态机器人学会快速奔跑;</p>
                </div>
                <div class="p1">
                    <p id="251">5) InvertedDoublePendulum.平衡具有2个关节的平衡杆;</p>
                </div>
                <div class="p1">
                    <p id="252">6) Reacher.使2D手臂形态机器人伸向指定位置.</p>
                </div>
                <div class="area_img" id="253">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_253.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 四足蚂蚁形态机器人" src="Detail/GetImg?filename=images/JFYZ201908014_253.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 四足蚂蚁形态机器人  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_253.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Ant</p>

                </div>
                <div class="p1">
                    <p id="254">使用的2个PyBullet连续任务包括:</p>
                </div>
                <div class="p1">
                    <p id="255">1) AntBullet.是类似于Mujoco中Ant的连续任务, 只是加重了Ant的重量, 来鼓励其行走过程中以更多条腿接触地面;</p>
                </div>
                <div class="p1">
                    <p id="256">2) Walker2DBullet.任务是使双足机器人学会行走, 产生更加逼真的慢节奏动作, 如图4所示:</p>
                </div>
                <div class="area_img" id="257">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_257.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Bullet中的2D行走任务" src="Detail/GetImg?filename=images/JFYZ201908014_257.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Bullet中的2D行走任务  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_257.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Walker2DBullet</p>

                </div>
                <div class="p1">
                    <p id="258">本文首先说明优秀经验筛选方法的效果, 以及基于经验的指导机制和多行动者机制的效果, 然后对比了EGDDAC-MA, DDPG, TRPO和PPO这4种算法的性能, 最后研究使用专家经验取代自身优秀经验对EGDDAC-MA的影响.本文实验使用Intel<sup>®</sup> Xeon<sup>®</sup> CPU E5-2680处理器, 使用NVIDIA Tesla P40图形处理器对深度学习运算进行辅助加速计算.</p>
                </div>
                <h4 class="anchor-tag" id="259" name="259"><b>3.2 参数设置</b></h4>
                <div class="p1">
                    <p id="260">本文实验中所使用的DDPG算法其网络结构和参数设置与参考文献中设置一样, TRPO和PPO算法来自是OpenAI baselines<citation id="394" type="reference"><link href="367" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>的深度强化学习算法集.在EGDDAC-MA中, 其使用的评论家网络与DDPG算法中的评论家网络设置一样.EGDDAC-MA的每一个编码层包含2层, 第1层有300个神经网络节点, 第2层有100个网络节点.每个行动者网络包含2层, 第1层有200个神经网络节点, 第2层有100个神经网络节点.EGDDAC-MA的指导网络也包含2层, 第1层200个神经网络节点, 第2层有100个神经网络节点.EGDDAC-MA的每一个普通经验池容量是300 000, 而经验池<i>B</i>′的容量是100 000, 是普通经验池1/3.干扰系数<i>ξ</i>=1×10<sup>-5</sup>, mini-bach的大小是64, <i>τ</i>=0.001, 学习率<i>γ</i>=0.99, <i>k</i>=50.评论家网络的学习率是1×10<sup>-3</sup>, 指导网络的学习率是0.000 2, 行动者网络的学习率是1×10<sup>-5</sup>.每个Mujoco环境下每个算法训练的总时间步数是250万步, 除了Reacher中是100万步, 这是因为Reacher在100万步内可以学习到一个稳定的策略.此外, 2个PyBullet环境下每个算法训练的总时间步数是400万步.由于实验中使用了多个不同环境, 为了统一参数, EGDDAC-MA都是以60个时间步作为一个阶段来设置的.每个情节的最长时间步设置为1 000.</p>
                </div>
                <h4 class="anchor-tag" id="261" name="261"><b>3.3 优秀经验筛选方法的效果</b></h4>
                <div class="p1">
                    <p id="262">为了说明通过式 (14) 和式 (15) 的筛选, 经验池<i>B</i>′中的经验在变好, 这里通过均值<i>μ</i>=1, 方差<i>σ</i><sup>2</sup>分别为0.1, 1, 5, 10, 20的正态分布来做一个模拟实验.实验中会定义一个经验池<i>B</i>′, 每个情节只是正态分布生成的一个随机变量, 然后使用这个随机变量作为该情节的回报, 同时情节经验也用这个随机变量表示, 在<i>B</i>′存放经验时, 就使用式 (14) 和式 (15) 来判断情节是否优秀, 若优秀则将该情节经验也就是对应的随机变量 (也表示情节回报) 放入<i>B</i>′.模拟中<i>k</i>取的10, 经验池容量为100, 情节总数为100 000, 模拟结果如图5所示.第1幅图显示的是放入经验池中的最近<i>k</i>个情节的平均回报随时间步数的变化.第2幅图显示的是放入经验池中的所有情节的平均回报随时间步数的变化.从图5可以看出, 虽然均值是1, 但是通过使用式 (14) 和式 (15) 对优秀经验进行判定, 无论方差多大, 最近<i>k</i>个情节的均值, 和整个经验池的均值都是向着大于1的方向进化的, 这说明经验池中的经验整体上在变得越来越好.</p>
                </div>
                <div class="area_img" id="263">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_263.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 正态分布模拟情节回报值的结果" src="Detail/GetImg?filename=images/JFYZ201908014_263.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 正态分布模拟情节回报值的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_263.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The results of normal distribution simulating  episodic return</p>

                </div>
                <h4 class="anchor-tag" id="264" name="264"><b>3.4 基于经验的指导和多行动者机制的优势</b></h4>
                <div class="p1">
                    <p id="265">为了说明基于经验的指导和多行动者机制的优势, 本文在InvertedDoublePendulum任务上对比了EGDDAC-MA, EGDDAC-One actor和DDPG的性能.首先为了说明基于经验指导的方法优于外部噪声探索, 这里对比只使用一个行动者网络的EGDDAC-One actor和使用外部OU噪声探索的DDPG.对比结果如图6所示, 图6中横坐标为训练时间步, 纵坐标为平均回报.</p>
                </div>
                <div class="area_img" id="266">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_266.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 在Inverte-dDoublePendulum中的平均回报对比" src="Detail/GetImg?filename=images/JFYZ201908014_266.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 在Inverte-dDoublePendulum中的平均回报对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_266.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The average return in InvertedDoublePendulum</p>

                </div>
                <div class="p1">
                    <p id="267">可以看见DDPG在整个训练过程中, 平均回报处于较低值, 而且没有上升趋势, 这说明基于外部OU噪声的探索, 并没有探索到好的经验供agent学习.但EGDDAC-One actor可以获得更高的平均回报, 即使整个训练过程中波动很大.这是因为基于经验的指导机制可以引导agent倾向选择具有高回报轨迹, 而外部的OU噪声探索是没有方向性的盲目探索.</p>
                </div>
                <div class="p1">
                    <p id="268">为了缓解图6中出现的波动, 我们在基于经验的指导的基础上加上多行动者机制, 如图6所示.这里对比EGDDAC-MA和EGDDAC-One actor 在InvertedDoublePendulum中的学习表现.可以看出EGDDAC-MA没有出现像EGDDAC-One actor中的剧烈波动, 而是在一定程度波动内稳步上升.这说明多行动者机制可以缓解单个网络学习压力.而且可以看到经过200万步后网络学习的波动被控制在一定范围之内.</p>
                </div>
                <h4 class="anchor-tag" id="270" name="270"><b>3.5 对比不同算法的性能</b></h4>
                <div class="p1">
                    <p id="271">本文在Mujoco的6个连续任务和PyBullet 的2个连续任务中对比了EGDDAC-MA, DDPG, TRPO和PPO这4种算法的性能.其中TRPO和PPO也都是基于AC方法的改进, 与DDPG和EGDDAC-MA不同的是, TRPO和PPO两者都使用高斯策略实现探索, 并且没有使用经验回放, 而是在线进行更新.实验结果如图7所示.</p>
                </div>
                <div class="p1">
                    <p id="272">在Ant环境中, DDPG整体上呈现先上升后下降的学习趋势.TRPO和PPO的平均回报虽然随着学习的进行会有增长趋势, 但是两者最终的平均回报值都维持在一个较低值.而EGDDAC-MA 的表现比其他3种算法都好, 最终平均回报值维持在4 000左右.此外在HalfCheetah, HumanoidStandup和Reacher中, EGDDAC-MA也是明显优于其他方法.这是因为DDPG以及TRPO和PPO算法的探索是盲目的, 而基于优秀经验的指导机制, 会引导agent去选择具有高回报的轨迹, 因此EGDDAC-MA会表现的更好.在Humanoid中, EGDDAC-MA前期表现不如TRPO和PPO, 可能是因为TRPO和PPO这类方法直接是在线学习的不需要经验累积, 而EGDDAC-MA的经验池<i>B</i>′中的优秀经验需要经历一定的时间步去收集, 这个问题在Inverted-DoublePendulum中也可以看到.但是EGDDAC-MA最终在Humanoid和 InvertedDoubleP-endulum两个环境中都超过TRPO和PPO.</p>
                </div>
                <div class="p1">
                    <p id="273">从图7的实验结果可以看出:EGDDAC-MA的性能很大程度上取决于经验池<i>B</i>′中经验的优秀程度, 因此下一个实验我们将展现使用专家经验取代自身优秀经验后, EGDDAC-MA的学习效果.</p>
                </div>
                <div class="area_img" id="274">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_27400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 4种方法在8个不同连续任务中的平均回报" src="Detail/GetImg?filename=images/JFYZ201908014_27400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 4种方法在8个不同连续任务中的平均回报  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_27400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig．7 The average returns of four approaches in eight continues control tasks</p>

                </div>
                <div class="area_img" id="274">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_27401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 4种方法在8个不同连续任务中的平均回报" src="Detail/GetImg?filename=images/JFYZ201908014_27401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 4种方法在8个不同连续任务中的平均回报  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_27401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig．7 The average returns of four approaches in eight continues control tasks</p>

                </div>
                <h4 class="anchor-tag" id="275" name="275"><b>3.6 使用专家经验的EGDDAC-MA</b></h4>
                <div class="p1">
                    <p id="276">相比基于自身优秀经验的EGDDAC-MA, 基于专家经验的EGDDAC-MA其指导网络的输入不再是状态信号而是状态, 而且<i>B</i>′中直接装入的是预先训练得到的专家经验.</p>
                </div>
                <div class="p1">
                    <p id="277">整个实验是在InvertedDoublePendulum环境中进行的, 探索了不同干扰因子下, 基于专家经验的EGDDAC-MA的学习效果.实验结果如图8所示, 图8中的前5幅图片, 分别是在<i>ξ</i>=0.9, 0.7, 0.5, 0.3, 0.1下, 基于专家经验的EGDDAC-MA的行动者网络的学习表现.要注意的是, 为了体现行动者网络的学习效果, 图8中的每一个绿点都表示没有指导网络的指导时, 只使用行动者网络来生成的情节回报.也就是在1 000 000时间步的训练过程中, 每隔500时间步就会单独使用行动者网络来生成一个情节, 因此每一个<i>ξ</i>下, 都有2 000个绿点.图8中红线表示专家水平.图8中最后一幅图片对比训练过程中不同<i>ξ</i>下的只使用行动者网络生成的平均情节回报随时间步数的变化.</p>
                </div>
                <div class="p1">
                    <p id="278">从图8可以看出, 在<i>ξ</i>=0.9时, 行动者网络的情节回报虽然有向上趋势, 但是最终基本上维持在一个较低值.在 <i>ξ</i>=0.7时, 情节回报的向上趋势更加明显, 但大部分回报值都很低.在<i>ξ</i>=0.5时, 可以看见200 000时间步后, 行动者网络很快学到一个不错策略, 情节回报值基本上达到专家水平, 只有少数情节回报值较低.在<i>ξ</i>为0.1和0.3时, 随着<i>ξ</i>值的降低, 情节回报值上升趋势会下降, 而且大多数情节回报值在专家水平之下.</p>
                </div>
                <div class="area_img" id="279">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908014_279.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 使用专家经验的效果" src="Detail/GetImg?filename=images/JFYZ201908014_279.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 使用专家经验的效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908014_279.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 The effect of using expert experiences</p>

                </div>
                <div class="p1">
                    <p id="280">整体上来看, 在<i>ξ</i>=0.5时, 在基于专家经验的指导下, 行动者网络可以快速学习到一个不错的策略, 而在<i>ξ</i>高于0.5或低于0.5时, 行动者网络学习的并不好.出现这种现象的原因是, 若<i>ξ</i>值过高, 高于0.5, 此时行动的选择, 主要取决于指导网络, 而行动者网络对行动决策贡献较小, 这样得到的经验是不利于行动者网络学习的.若<i>ξ</i>值过低, 低于0.5, 此时行动的选择, 主要取决于行动者网络自身, 指导网络对行动决策贡献较小, 而行动者网络学习的方向并不一定是专家策略的方向, 这样行动者网络的决策与指导网络的决策就可能出现冲突, 而且指导网络同样会影响评论家网络的学习, 因此也不利于行动者网络学习.只有<i>ξ</i>=0.5时, 行动的决策受行动者网络和指导网络均等程度上的控制, 评论家的学习也是这样, 从而行动者网络可以快速的学习一个不错的策略.与基于专家经验的EGDDAC-MA在<i>ξ</i>=0.5时表现最好不同, 基于自身优秀经验的EGDDAC-MA的<i>ξ</i>一定要是一个较小的值.这是因为指导网络所使用的经验是自身优秀经验, 是行动者网络学习所使用的自身经验的子集, 从而指导网络和行动者网络学习的策略的方向是一致的.因此, 在选择动作和更新评论家网络时, 指导网络只需要做一个微弱的指导就可以.</p>
                </div>
                <h3 id="281" name="281" class="anchor-tag"><b>4 结束语</b></h3>
                <div class="p1">
                    <p id="282">连续控制问题一直是强化学习研究的一个重要方向.确定性策略梯度方法和深度学习结合可以在一定程度上解决这类问题.但是这类方法在一些连续任务中的表现并不是很好, 这很大程度上是由于探索不当造成的.本文提出了一种不需要外部探索噪声的基于经验指导的深度确定性多行动者-评论家算法 (EGDDAC-MA) .</p>
                </div>
                <div class="p1">
                    <p id="283">EGDDAC-MA中通过定义多个行动者网络来应对情节的不同阶段, 这样可以缓解情节内部波动对单个网络学习造成的压力, 并通过在自身优秀经验上学习得来的指导网络为动作执行以及评论家网络更新提供指导.此外本文不仅使用自身优秀经验来训练指导网络, 也尝试了使用专家经验来训练, 并且发现使用专家经验, 在<i>ξ</i>=0.5时EGDDAC-MA可以更快的学到一个不错的策略.最终, Mujoco上的模拟实验结果显示:相比于DDPG, TRPO和PPO, EGDDAC-MA在大多数连续控制任务上均取得更好的效果.</p>
                </div>
                <div class="p1">
                    <p id="284">实际上, 本文采用的是简单的阶段划分方法, 也取得了不错的效果, 未来的一些工作可以使用无监督方法来对情节进行自适应的阶段划分来提高阶段划分的有效性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="311">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Autonomous reinforcement learning on raw visual input data in real world application">

                                <b>[1]</b>Lange S, Riedmiller M, Voigtlander A.Autonomous reinforcement learning on raw visual input data in real world application[C] //Proc of the 9th Int Joint Conf Neural Network.Piscataway, NJ:IEEE, 2012:1- 8
                            </a>
                        </p>
                        <p id="313">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201412010&amp;v=MjA0MTJyWTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1dyM1BMeXZTZExHNEg5WE4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Zhang Rubo, Tang Pingpeng, Yang Ge, et al.Convergence analysis of adaptive obstacle avoidance decision processes for unmanned surface vehicle[J].Journal of Computer Research and Development, 2014, 51 (12) :2644- 2652 (in Chinese) (张汝波, 唐平鹏, 杨歌, 等.水面无人艇自适应危险规避决策过程收敛性分析[J].计算机研究与发展, 2014, 51 (12) :2644- 2652) 
                            </a>
                        </p>
                        <p id="315">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201112024&amp;v=MDU5Nzh0R0ZyQ1VSTE9lWmVSckZ5dmdXcjNQTHl2U2RMRzRIOUROclk5SFlJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Liu Quan, Yan Qicui, Fu Yuchen, et al.A hierarchical reinforcement learning method based on heuristic reward function[J].Journal of Computer Research and Development, 2011, 48 (12) :2352- 2358 (in Chinese) (刘全, 闫其粹, 伏玉琛, 等.一种基于启发式奖赏函数的分层强化学习方法[J].计算机研究与发展, 2011, 48 (12) :2352- 2358) 
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201808011&amp;v=MTUyNzZxQnRHRnJDVVJMT2VaZVJyRnl2Z1dyM1BMeXZTZExHNEg5bk1wNDlFWllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Zhu Fei, Wu Wen, Liu Quan, et al.A deep Q-Network method based on upper confidence bound experience sampling[J].Journal of Computer Research and Development, 2018, 55 (8) :1694- 1705 (in Chinese) (朱斐, 吴文, 刘全, 等.一种最大置信上界经验采样的深度Q网络方法[J].计算机研究与发展, 2018, 55 (8) :1694- 1705) 
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mastering the game of Go with deep neural networks and tree search">

                                <b>[5]</b>Silver D, Huang A, Maddison CJ, et al.Mastering the game of go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484- 489
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">

                                <b>[6]</b>Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 1998:6-22
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On-line Q-learning using connectionist systems">

                                <b>[7]</b>Rummery G A, Niranjan M.On-line Q-learning using connectionist systems[R].Cambridge, UK:Engineering Department, Cambridge University, 1994
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338435&amp;v=MjgxMjlYcVJyeG94Y01IN1I3cWVidWR0RkMzbFVMckJJbGs9Tmo3QmFyTzRIdEhOckl4TllPZ0tZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Christopher J C H Watkins, Peter Dayan.Q-learning[J].Machine Learning, 1992, 8 (3) :279- 292
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339335&amp;v=MDE5NzBkdEZDM2xVTHJCSWxrPU5qN0Jhck80SHRITnJJeE1aK2dLWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>Singh S P, Sutton R S.Reinforcement learning with replacing eligibility traces[J].Machine Learning, 1996, 22 (1) :123- 158
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338429&amp;v=MTYxNTk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzNsVUxyQklsaz1OajdCYXJPNEh0SE5ySXhOWU9rR1kzazV6QmRoNGo5&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Williams R J.Simple statistical gradient-fellowing algorithms for connectionist reinforcement learning[J].Machine Learning, 1992, 8 (3) :229- 256
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neuronlike adaptive elements that can solve difficult learning control problems">

                                <b>[11]</b>Andrew G B, Sutton R S, Anderson C W.Neuronlike adaptive elements that can solve difficult learning control problems[J].IEEE Transactions on Systems, Man and Cybernetics, 2012, 13 (5) :834- 846
                            </a>
                        </p>
                        <p id="333">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501912639&amp;v=MDE5NThqbVVMcklJMXNkYUJVPU5pZk9mYks3SHRETnFvOUViZW9OQ244d29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>Peters J, Schaal S.Natural actor-critic[J].Neurocomputing, 2008, 71 (7) :1080- 1090
                            </a>
                        </p>
                        <p id="335">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529- 533
                            </a>
                        </p>
                        <p id="337">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dueling Network Architectures for Deep Reinforcement Learning">

                                <b>[14]</b>Wang Z, Schaul T, Hessel M, et al.Dueling network architectures for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:1995- 2003
                            </a>
                        </p>
                        <p id="339">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous deep Q-Learning with model-based acceleration">

                                <b>[15]</b>Gu Shixiang, Lillicrap, Timothy, et al.Continuous deep Q-Learning with model-based acceleration[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:2829- 2838
                            </a>
                        </p>
                        <p id="341">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous control with deep reinforcement learning">

                                <b>[16]</b>Lillicrap T P, Hunt J J, Pritzel A, et al.Continuous control with deep reinforcement learning[J].Computer Science, 2015, 8 (6) :A187
                            </a>
                        </p>
                        <p id="343">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">

                                <b>[17]</b>Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C] //Proc of the 33rd Int Conf on Machine Learning.New York:ACM, 2016:1928- 1937
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synthesizing programs for images using reinforced adversarial learning">

                                <b>[18]</b>Yarosav Ganin, Tejas Kulkarni, Igor Babuschkin, et al.Synthesizing programs for images using reinforced adversarial learning[C] //Proc of the 35th Int Conf on Machine Learning.New York:ACM, 2018:1652- 1661
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning for relation classification from noisy data">

                                <b>[19]</b>Jun Feng, Li Zhao, Xiaoyan Zhu.Reinforcement learning for relation classification from noisy data[C] //Proc of the 32nd AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2018:5779- 5786
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Reinforcement Learning for Dialogue Generation">

                                <b>[20]</b>Li Jiwei, Monroe W, Ritter A, et al.Deep reinforcement learning for dialogue generation[C] //Proc of the 2016 Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2016:1192- 1202
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Guide actor-critic for continuous control">

                                <b>[21]</b>Tangkaratt V, Abdolmaleki A, Sugiyama M.Guide actor-critic for continuous control[C] //Proc of the 6th Int Conf on Learning Representations.San Juan, CA:ICLR, 2018:1925- 1937
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Policy Gradient Methods for Reinforcement Learning with Function Approximation">

                                <b>[22]</b>Sutton R S, David A M, et al.Policy gradient methods for reinforcement learning with function approximation[C] //Proc of the 12th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2000:1057- 1063
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deterministic policy gradient algorithms">

                                <b>[23]</b>Silver D, Lever G, Heess N, et al.Deterministic policy gradient algorithms[C] //Proc of the 31st Int Conf on Machine Learning.New York:ACM, 2014:387- 395
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trust region policy optimization">

                                <b>[24]</b>Schulman J, Levine S, Abbeel P, et al.Trust region policy optimization[C] //Proc of the 32nd Int Conf on Machine Learning.New York:ACM, 2015:1889- 1897
                            </a>
                        </p>
                        <p id="359">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the theory of the {Brownian} motion">

                                <b>[25]</b>Uhlenbeck G E, Ornstein L S.On the theory of the {Brownian} motion[J].Revista Latinoamericana De Microbiología, 1973, 15 (1) :No.29
                            </a>
                        </p>
                        <p id="361">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-agent actor-critic for mixed cooperative-competitive environments">

                                <b>[26]</b>Lowe R, Wu Yi, Tamar A, et al.Multi-agent actor-critic for mixed cooperative-competitive environments[C] //Proc of the 30th Annual Conf on Neural Information Processing System.Cambridge, MA:MIT Press, 2017:6382- 6393
                            </a>
                        </p>
                        <p id="363">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OpenAI gym[CP]">

                                <b>[27]</b>Brockman G, Cheung V, Pettersson L, et al.OpenAI gym[CP].CoRR, abs/1606.01540, 2016.https://arxiv.org/pdf/1606.0154
                            </a>
                        </p>
                        <p id="365">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MuJoCo:A physics engine for model-based control">

                                <b>[28]</b>Todorov E, Erez T, Tassa Y.Mujoco:A physics engine for model-based control[C] //Proc of 2012 IEEE/RSJ Int Conf on Intelligent Robots and Systems.Piscataway, NJ:IEEE, 2012:5026- 5033
                            </a>
                        </p>
                        <p id="367">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OpenAI baselines[CP]">

                                <b>[29]</b>Dhariwal P, Hesse N, Manning C, et al.OpenAI baselines[CP].GitHub, 2017.https://github.com/openai/baselines
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908014" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908014&amp;v=MTc2NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnV3IzUEx5dlNkTEc0SDlqTXA0OUVZSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR3J5K3prbGZ0eHpGbFdWdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

