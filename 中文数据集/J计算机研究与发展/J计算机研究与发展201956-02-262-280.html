<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243360440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902004%26RESULT%3d1%26SIGN%3ddX9IF36DRvj%252bSFwl9K12ITsbB1A%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902004&amp;v=MjY5MjRSTE9lWmVWdkZ5N25XcnZNTHl2U2RMRzRIOWpNclk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#583" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;相关工作&lt;/b&gt; "><b>1</b><b>相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#584" data-title="&lt;b&gt;1.1&lt;/b&gt;&lt;b&gt;强化学习&lt;/b&gt;"><b>1.1</b><b>强化学习</b></a></li>
                                                <li><a href="#595" data-title="&lt;b&gt;1.2&lt;/b&gt;&lt;b&gt;深度Q网络&lt;/b&gt;"><b>1.2</b><b>深度Q网络</b></a></li>
                                                <li><a href="#609" data-title="&lt;b&gt;1.3&lt;/b&gt;&lt;b&gt;优先经验回放&lt;/b&gt;"><b>1.3</b><b>优先经验回放</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#616" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;基于TD-error校正的主动采样模型&lt;/b&gt; "><b>2</b><b>基于TD-error校正的主动采样模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#619" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;TD-error偏差分析&lt;/b&gt;"><b>2.1</b><b>TD-error偏差分析</b></a></li>
                                                <li><a href="#677" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;ATDC-PER模型&lt;/b&gt;"><b>2.2</b><b>ATDC-PER模型</b></a></li>
                                                <li><a href="#774" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;算法描述和复杂度分析&lt;/b&gt;"><b>2.3</b><b>算法描述和复杂度分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#836" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;实验结果与分析&lt;/b&gt; "><b>3</b><b>实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#838" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;实验平台描述&lt;/b&gt;"><b>3.1</b><b>实验平台描述</b></a></li>
                                                <li><a href="#844" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;ATDC-PER参数选择&lt;/b&gt;"><b>3.2</b><b>ATDC-PER参数选择</b></a></li>
                                                <li><a href="#853" data-title="&lt;b&gt;3.3&lt;/b&gt;&lt;b&gt;对比实验与分析&lt;/b&gt;"><b>3.3</b><b>对比实验与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#892" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;结&lt;/b&gt;&lt;b&gt;论&lt;/b&gt; "><b>4</b><b>结</b><b>论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#610" data-title="图1 本文方法 (ATDC-PER) 的总体结构">图1 本文方法 (ATDC-PER) 的总体结构</a></li>
                                                <li><a href="#622" data-title="图2 倒立摆的状态表示">图2 倒立摆的状态表示</a></li>
                                                <li><a href="#630" data-title="图3 训练周期为420时经验池中样本的&lt;i&gt;τ&lt;/i&gt;的分布">图3 训练周期为420时经验池中样本的<i>τ</i>的分布</a></li>
                                                <li><a href="#649" data-title="图4 CartPole训练周期为180时&lt;i&gt;p&lt;/i&gt;和&lt;i&gt;p&lt;/i&gt;&lt;sub&gt;real&lt;/sub&gt;分布图">图4 CartPole训练周期为180时<i>p</i>和<i>p</i><sub>real</sub>分布图</a></li>
                                                <li><a href="#655" data-title="图5 CartPole训练周期为180时|Δ&lt;i&gt;p&lt;/i&gt;|的分布图">图5 CartPole训练周期为180时|Δ<i>p</i>|的分布图</a></li>
                                                <li><a href="#663" data-title="图6 使用&lt;i&gt;p&lt;/i&gt;和&lt;i&gt;p&lt;/i&gt;&lt;sub&gt;real&lt;/sub&gt;作为优先级的训练曲线对比">图6 使用<i>p</i>和<i>p</i><sub>real</sub>作为优先级的训练曲线对比</a></li>
                                                <li><a href="#683" data-title="图7 ATDC-PER算法的分段更新策略示意图">图7 ATDC-PER算法的分段更新策略示意图</a></li>
                                                <li><a href="#714" data-title="图8 Pong和Breakout在&lt;i&gt;t&lt;/i&gt;=10时样本特征&lt;mathml id=&quot;715&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mtext&gt;p&lt;/mtext&gt;&lt;mo&gt;˜&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;&lt;/mathml&gt;, &lt;mathml id=&quot;716&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mtext&gt;τ&lt;/mtext&gt;&lt;mo&gt;˜&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;&lt;/mathml&gt;和样本标签Δ&lt;mathml id=&quot;717&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mtext&gt;p&lt;/mtext&gt;&lt;mo&gt;˜&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;&lt;/mathml&gt;的分布">图8 Pong和Breakout在<i>t</i>=10时样本特征<mathml id="715"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml>, <mathml id="716"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>τ</mtext><mo>˜</mo></mover></math></mathml>和样本标签Δ<mathml id="717"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml>的分布</a></li>
                                                <li><a href="#766" data-title="图9 &lt;i&gt;Breakout&lt;/i&gt;训练中t=10时&lt;mathml id=&quot;767&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mtext&gt;p&lt;/mtext&gt;&lt;mo&gt;˜&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;&lt;/mathml&gt;, &lt;mathml id=&quot;768&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mtext&gt;p&lt;/mtext&gt;&lt;mo&gt;˜&lt;/mo&gt;&lt;/mover&gt;&lt;/math&gt;&lt;/mathml&gt;&lt;sub&gt;&lt;i&gt;real&lt;/i&gt;&lt;/sub&gt;和&lt;mathml id=&quot;769&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mtext&gt;p&lt;/mtext&gt;&lt;mo&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;msubsup&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt; (&lt;/mo&gt;&lt;mtext&gt;i&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;) &lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/math&gt;&lt;/mathml&gt;分布">图9 <i>Breakout</i>训练中t=10时<mathml id="767"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml>, <mathml id="768"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sub><i>real</i></sub>和<mathml id="769"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>p</mtext><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>a</mi><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mtext>i</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分布</a></li>
                                                <li><a href="#841" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;19个Atari游戏的简要介绍&lt;/b&gt;"><b>表1</b><b>19个Atari游戏的简要介绍</b></a></li>
                                                <li><a href="#842" data-title="图10 19个Atari游戏的状态表示">图10 19个Atari游戏的状态表示</a></li>
                                                <li><a href="#848" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;比较不同&lt;i&gt;D&lt;/i&gt;值下时刻&lt;i&gt;t&lt;/i&gt;训练的偏差模型在时刻&lt;i&gt;t&lt;/i&gt;+&lt;i&gt;D&lt;/i&gt;测试的平均损失&lt;/b&gt;"><b>表2</b><b>比较不同<i>D</i>值下时刻<i>t</i>训练的偏差模型在时刻<i>t</i>+<i>D</i>测试的平均损失</b></a></li>
                                                <li><a href="#852" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;比较不同&lt;i&gt;K&lt;/i&gt;值下处于偏差模型更新周期中心的经验池平均损失&lt;/b&gt;"><b>表3</b><b>比较不同<i>K</i>值下处于偏差模型更新周期中心的经验池平均损失</b></a></li>
                                                <li><a href="#877" data-title="图11 本文方法在19个Atari游戏中与PER和DDQN算法的训练曲线对比">图11 本文方法在19个Atari游戏中与PER和DDQN算法的训练曲线对比</a></li>
                                                <li><a href="#877" data-title="图11 本文方法在19个Atari游戏中与PER和DDQN算法的训练曲线对比">图11 本文方法在19个Atari游戏中与PER和DDQN算法的训练曲线对比</a></li>
                                                <li><a href="#877" data-title="图11 本文方法在19个Atari游戏中与PER和DDQN算法的训练曲线对比">图11 本文方法在19个Atari游戏中与PER和DDQN算法的训练曲线对比</a></li>
                                                <li><a href="#886" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;b&gt;全部游戏的规约得分评价表&lt;/b&gt;"><b>表4</b><b>全部游戏的规约得分评价表</b></a></li>
                                                <li><a href="#887" data-title="&lt;b&gt;表5&lt;/b&gt;&lt;b&gt;全部游戏的实际得分和规约得分表&lt;/b&gt;"><b>表5</b><b>全部游戏的实际得分和规约得分表</b></a></li>
                                                <li><a href="#890" data-title="&lt;b&gt;表6&lt;/b&gt;&lt;b&gt;全部游戏的综合评价表&lt;/b&gt;"><b>表6</b><b>全部游戏的综合评价表</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="968">


                                    <a id="bibliography_1" title="Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 1998:100-107" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">
                                        <b>[1]</b>
                                        Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 1998:100-107
                                    </a>
                                </li>
                                <li id="970">


                                    <a id="bibliography_2" title="Bertsekas D P, Tsitsiklis J N.Neuro-dynamic programming:An overview[C]Proc of the 34th IEEE Conf on Decision and Control.Piscataway, NJ:IEEE, 1995:560-564" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neuro-dynamic programming:an overview">
                                        <b>[2]</b>
                                        Bertsekas D P, Tsitsiklis J N.Neuro-dynamic programming:An overview[C]Proc of the 34th IEEE Conf on Decision and Control.Piscataway, NJ:IEEE, 1995:560-564
                                    </a>
                                </li>
                                <li id="972">


                                    <a id="bibliography_3" title="Tesauro G.Td-gammon:A self-teaching backgammon program[M]Applications of Neural Networks.New York:Springer, 1995:267-285" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TD-Gammon:A Self-Teaching Backgammon Program">
                                        <b>[3]</b>
                                        Tesauro G.Td-gammon:A self-teaching backgammon program[M]Applications of Neural Networks.New York:Springer, 1995:267-285
                                    </a>
                                </li>
                                <li id="974">


                                    <a id="bibliography_4" title="Tesauro G.Programming backgammon using self-teaching neural nets[J].Artificial Intelligence, 2002, 134 (1/2) :181-199" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702006969&amp;v=MTkwNzJRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSmx3ZGJoWT1OaWZPZmJLN0h0RE5xSTlIWk9zSkJYb3dvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Tesauro G.Programming backgammon using self-teaching neural nets[J].Artificial Intelligence, 2002, 134 (1/2) :181-199
                                    </a>
                                </li>
                                <li id="976">


                                    <a id="bibliography_5" title="Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 26th Int Conf on neural information processing systems (NIPS) .Cambridge, MA:MIT Press, 2012:1097-1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">
                                        <b>[5]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 26th Int Conf on neural information processing systems (NIPS) .Cambridge, MA:MIT Press, 2012:1097-1105
                                    </a>
                                </li>
                                <li id="978">


                                    <a id="bibliography_6" title="Liu Quan, Zhai Jianwei, Zhang Zongchang, et al.A survey on deep reinforcement learning[J].Chinese Journal of Computers, 2018, 41 (1) :1-27 (in Chinese) (刘全, 翟建伟, 章宗长, 等.深度强化学习综述[J].计算机学报, 2018, 41 (1) :1-27) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801001&amp;v=MDYyNzA1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydk1MejdCZHJHNEg5bk1ybzlGWllRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Liu Quan, Zhai Jianwei, Zhang Zongchang, et al.A survey on deep reinforcement learning[J].Chinese Journal of Computers, 2018, 41 (1) :1-27 (in Chinese) (刘全, 翟建伟, 章宗长, 等.深度强化学习综述[J].计算机学报, 2018, 41 (1) :1-27) 
                                    </a>
                                </li>
                                <li id="980">


                                    <a id="bibliography_7" title="Li Yuxi.Deep reinforcement learning:An overview[EB/OL].New York:Cornell University, 2017[2018-03-04].https:arxiv.org/abs/1701.07274" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning:an overview">
                                        <b>[7]</b>
                                        Li Yuxi.Deep reinforcement learning:An overview[EB/OL].New York:Cornell University, 2017[2018-03-04].https:arxiv.org/abs/1701.07274
                                    </a>
                                </li>
                                <li id="982">


                                    <a id="bibliography_8" title="Mnih V, Kavukcuoglu K, Silver D, et al.Playing Atari with deep reinforcement learning[EB/OL].New York:Cornell University, 2013[2017-10-04].https:arxiv.org/abs/1312.5602" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Playing atari with deep reinforcement learning">
                                        <b>[8]</b>
                                        Mnih V, Kavukcuoglu K, Silver D, et al.Playing Atari with deep reinforcement learning[EB/OL].New York:Cornell University, 2013[2017-10-04].https:arxiv.org/abs/1312.5602
                                    </a>
                                </li>
                                <li id="984">


                                    <a id="bibliography_9" title="Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529-533" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">
                                        <b>[9]</b>
                                        Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529-533
                                    </a>
                                </li>
                                <li id="986">


                                    <a id="bibliography_10" title="Maei H R, Szepesvari C, Bhatnagar S, et al.Convergent temporal-difference learning with arbitrary smooth function approximation[C]Proc of the 23rd Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2009:1204-1212" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convergent temporal-difference learningwith arbitrary smooth function approximation">
                                        <b>[10]</b>
                                        Maei H R, Szepesvari C, Bhatnagar S, et al.Convergent temporal-difference learning with arbitrary smooth function approximation[C]Proc of the 23rd Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2009:1204-1212
                                    </a>
                                </li>
                                <li id="988">


                                    <a id="bibliography_11" title="Schaul T, Quan J, Antonoglou I, et al.Prioritized experience replay[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:256-265" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Prioritized experience replay">
                                        <b>[11]</b>
                                        Schaul T, Quan J, Antonoglou I, et al.Prioritized experience replay[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:256-265
                                    </a>
                                </li>
                                <li id="990">


                                    <a id="bibliography_12" title="Nair A, Srinivasan P, Blackwell S, et al.Massively parallel methods for deep reinforcement learning[EB/OL].New York:Cornell University, 2015[2018-03-02].https:arxiv.org/abs/1507.04296" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Massively parallel methods for deep reinforcement learning">
                                        <b>[12]</b>
                                        Nair A, Srinivasan P, Blackwell S, et al.Massively parallel methods for deep reinforcement learning[EB/OL].New York:Cornell University, 2015[2018-03-02].https:arxiv.org/abs/1507.04296
                                    </a>
                                </li>
                                <li id="992">


                                    <a id="bibliography_13" title="Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:JMLR, 2016:1928-1937" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">
                                        <b>[13]</b>
                                        Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:JMLR, 2016:1928-1937
                                    </a>
                                </li>
                                <li id="994">


                                    <a id="bibliography_14" title="Andre D, Friedman N, Parr R.Generalized prioritized sweeping[C]Proc of the 12th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 1998:1001-1007" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized prioritized sweeping">
                                        <b>[14]</b>
                                        Andre D, Friedman N, Parr R.Generalized prioritized sweeping[C]Proc of the 12th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 1998:1001-1007
                                    </a>
                                </li>
                                <li id="996">


                                    <a id="bibliography_15" title="Bellman R.A Markovian decision process[J].Journal of Mathematics and Mechanics, 1957, 6 (4) :679-684" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Markovian decision process">
                                        <b>[15]</b>
                                        Bellman R.A Markovian decision process[J].Journal of Mathematics and Mechanics, 1957, 6 (4) :679-684
                                    </a>
                                </li>
                                <li id="998">


                                    <a id="bibliography_16" title="Bertsekas D P.Dynamic programming and suboptimal control:A survey from ADP to MPC[J].European Journal of Control, 2005, 11 (4/5) :310-334" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES43FE6DC313BF4DA251866CE09A930EA8&amp;v=MDU4MjlneVhyQnRFY0xHVU1NdVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGh3cnEyeEt3PU5pZk9mYmU3YUtUSzIveEdaZWg5ZW5oTnZoUVc2emQ3VA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Bertsekas D P.Dynamic programming and suboptimal control:A survey from ADP to MPC[J].European Journal of Control, 2005, 11 (4/5) :310-334
                                    </a>
                                </li>
                                <li id="1000">


                                    <a id="bibliography_17" title="Browne C B, Powley E, Whitehouse D, et al.A survey of Monte Carlo tree search methods[J].IEEE Transactions on Computational Intelligence and AI in Games, 2012, 4 (1) :1-43" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey of Monte Carlo Tree Search Methods">
                                        <b>[17]</b>
                                        Browne C B, Powley E, Whitehouse D, et al.A survey of Monte Carlo tree search methods[J].IEEE Transactions on Computational Intelligence and AI in Games, 2012, 4 (1) :1-43
                                    </a>
                                </li>
                                <li id="1002">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                    Silver D, Huang Shijie, Maddison C J, et al.Mastering the game of Go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484-489</a>
                                </li>
                                <li id="1004">


                                    <a id="bibliography_19" title="Degris T, Pilarski P M, Sutton R S.Model-Free reinforcement learning with continuous action in practice[C]Proc of the 52nd American Control Conf.Piscataway, NJ:IEEE, 2012:2177-2182" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model-Free reinforcement learning with continuous action in practice">
                                        <b>[19]</b>
                                        Degris T, Pilarski P M, Sutton R S.Model-Free reinforcement learning with continuous action in practice[C]Proc of the 52nd American Control Conf.Piscataway, NJ:IEEE, 2012:2177-2182
                                    </a>
                                </li>
                                <li id="1006">


                                    <a id="bibliography_20" title="Zhu Fei, Zhu Haijun, Fu Yuchen, et al.A kernel based true online Sarsa (λ) for continuous space control problems[J].Computer Science&amp;amp;Information Systems, 2017, 14 (3) :789-804" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A kernel based true online Sarsa (λ)for continuous space control problems">
                                        <b>[20]</b>
                                        Zhu Fei, Zhu Haijun, Fu Yuchen, et al.A kernel based true online Sarsa (λ) for continuous space control problems[J].Computer Science&amp;amp;Information Systems, 2017, 14 (3) :789-804
                                    </a>
                                </li>
                                <li id="1008">


                                    <a id="bibliography_21" title="Zhu Fei, Liu Quan, Fu Qiming, et al.A least square actorcritic approach for continuous action space[J].Journal of Computer Research and Development, 2014, 51 (3) :548-558 (in Chinese) (朱斐, 刘全, 傅启明, 等.一种用于连续动作空间的最小二乘行动者-评论家方法[J].计算机研究与发展, 2014, 51 (3) :548-558) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201403009&amp;v=MjY3NzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XcnZNTHl2U2RMRzRIOVhNckk5RmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Zhu Fei, Liu Quan, Fu Qiming, et al.A least square actorcritic approach for continuous action space[J].Journal of Computer Research and Development, 2014, 51 (3) :548-558 (in Chinese) (朱斐, 刘全, 傅启明, 等.一种用于连续动作空间的最小二乘行动者-评论家方法[J].计算机研究与发展, 2014, 51 (3) :548-558) 
                                    </a>
                                </li>
                                <li id="1010">


                                    <a id="bibliography_22" title="Liu Quan, Zhang Peng, Zhong Shan, et al.An improved actor-critic algorithm in continuous spaces with action weighting[J].Chinese Journal of Computers, 2017, 40 (6) :1252-1264 (in Chinese) (刘全, 章鹏, 钟珊, 等.连续空间中的一种动作加权行动者评论家算法[J].计算机学报, 2017, 40 (6) :1252-1264) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706002&amp;v=MTU1NzMzenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydk1MejdCZHJHNEg5Yk1xWTlGWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        Liu Quan, Zhang Peng, Zhong Shan, et al.An improved actor-critic algorithm in continuous spaces with action weighting[J].Chinese Journal of Computers, 2017, 40 (6) :1252-1264 (in Chinese) (刘全, 章鹏, 钟珊, 等.连续空间中的一种动作加权行动者评论家算法[J].计算机学报, 2017, 40 (6) :1252-1264) 
                                    </a>
                                </li>
                                <li id="1012">


                                    <a id="bibliography_23" title="Barto A, Duff M.Monte Carlo matrix inversion and reinforcement learning[C]Proc of the 8th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 1994:687-694" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monte Carlo matrix inversion and reinforcement learning">
                                        <b>[23]</b>
                                        Barto A, Duff M.Monte Carlo matrix inversion and reinforcement learning[C]Proc of the 8th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 1994:687-694
                                    </a>
                                </li>
                                <li id="1014">


                                    <a id="bibliography_24" title="Watkins C J C H, Dayan P.Q learning[J].Machine Learning, 1992, 8 (3/4) :279-292" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338455&amp;v=MDUwOTdNSDdSN3FlYnVkdEZTbmxWYjNCSkZvPU5qN0Jhck80SHRITnJJeE5ZTzRLWTNrNXpCZGg0ajk5U1hxUnJ4b3hj&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        Watkins C J C H, Dayan P.Q learning[J].Machine Learning, 1992, 8 (3/4) :279-292
                                    </a>
                                </li>
                                <li id="1016">


                                    <a id="bibliography_25" title="Liu Quan, Zhou Xin, Zhu Fei, et al.Experience replay for least-squares policy iteration[J].IEEE/CAA Journal of Automatica Sinica, 2015, 1 (3) :274-281" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Experience replay for least-squares policy iteration">
                                        <b>[25]</b>
                                        Liu Quan, Zhou Xin, Zhu Fei, et al.Experience replay for least-squares policy iteration[J].IEEE/CAA Journal of Automatica Sinica, 2015, 1 (3) :274-281
                                    </a>
                                </li>
                                <li id="1018">


                                    <a id="bibliography_26" title="Hasselt H V.Double Q-learning[C]Proc of the 24th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2010:2613-2621" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Double Q-learning">
                                        <b>[26]</b>
                                        Hasselt H V.Double Q-learning[C]Proc of the 24th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2010:2613-2621
                                    </a>
                                </li>
                                <li id="1020">


                                    <a id="bibliography_27" title="Hasselt H V, Guez A, Silver D.Deep reinforcement learning with double Q-learning[C]Proc of the 30th AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:2094-2100" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Reinforcement Learning with Double Q-Learning">
                                        <b>[27]</b>
                                        Hasselt H V, Guez A, Silver D.Deep reinforcement learning with double Q-learning[C]Proc of the 30th AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:2094-2100
                                    </a>
                                </li>
                                <li id="1022">


                                    <a id="bibliography_28" title="Osband I, Blundell C, Pritzel A, et al.Deep exploration via bootstrapped DQN[C]Proc of the 30th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:4026-4034" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep exploration via bootstrapped DQN">
                                        <b>[28]</b>
                                        Osband I, Blundell C, Pritzel A, et al.Deep exploration via bootstrapped DQN[C]Proc of the 30th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:4026-4034
                                    </a>
                                </li>
                                <li id="1024">


                                    <a id="bibliography_29" title="Tang Haoran, Houthooft R, Foote D, et al.Exploration:Astudy of count-based exploration for deep reinforcement learning[C]Proc of the 31st Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2017:2750-2759" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploration:Astudy of count-based exploration for deep reinforcement learning">
                                        <b>[29]</b>
                                        Tang Haoran, Houthooft R, Foote D, et al.Exploration:Astudy of count-based exploration for deep reinforcement learning[C]Proc of the 31st Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2017:2750-2759
                                    </a>
                                </li>
                                <li id="1026">


                                    <a id="bibliography_30" title="Bellemare M, Srinivasan S, Ostrovski G, et al.Unifying count-based exploration and intrinsic motivation[C]Proc of the 30th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:1471-1479" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unifying count-based exploration and intrinsic motivation">
                                        <b>[30]</b>
                                        Bellemare M, Srinivasan S, Ostrovski G, et al.Unifying count-based exploration and intrinsic motivation[C]Proc of the 30th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:1471-1479
                                    </a>
                                </li>
                                <li id="1028">


                                    <a id="bibliography_31" title="Stadie B C, Levine S, Abbeel P.Incentivizing exploration in reinforcement learning with deep predictive models[EB/OL].New York:Cornell University, 2015[2018-03-01].https:arxiv.org/abs/1507.00814" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incentivizing exploration in reinforcement learning with deep predictive models">
                                        <b>[31]</b>
                                        Stadie B C, Levine S, Abbeel P.Incentivizing exploration in reinforcement learning with deep predictive models[EB/OL].New York:Cornell University, 2015[2018-03-01].https:arxiv.org/abs/1507.00814
                                    </a>
                                </li>
                                <li id="1030">


                                    <a id="bibliography_32" title="Wang Ziyu, Schaul T, Hessel M, et al.Dueling network architectures for deep reinforcement learning[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:JMLR, 2016:1995-2003" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dueling network architectures for deep reinforcement learning">
                                        <b>[32]</b>
                                        Wang Ziyu, Schaul T, Hessel M, et al.Dueling network architectures for deep reinforcement learning[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:JMLR, 2016:1995-2003
                                    </a>
                                </li>
                                <li id="1032">


                                    <a id="bibliography_33" title="Anschel O, Baram N, Shimkin N.Averaged-DQN:Variance reduction and stabilization for deep reinforcement learning[C]Proc of the 34th Int Conf on Machine Learning (ICML) .New York:JMLR, 2017:176-185" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Averaged-DQN:Variance reduction and stabilization for deep reinforcement learning">
                                        <b>[33]</b>
                                        Anschel O, Baram N, Shimkin N.Averaged-DQN:Variance reduction and stabilization for deep reinforcement learning[C]Proc of the 34th Int Conf on Machine Learning (ICML) .New York:JMLR, 2017:176-185
                                    </a>
                                </li>
                                <li id="1034">


                                    <a id="bibliography_34" title="Rusu A, Colmenarejo S G, Gulcehre C, et al.Policy distillation[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:343-352" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Policy distillation">
                                        <b>[34]</b>
                                        Rusu A, Colmenarejo S G, Gulcehre C, et al.Policy distillation[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:343-352
                                    </a>
                                </li>
                                <li id="1036">


                                    <a id="bibliography_35" title="Yin Haiyan, Pan Jialin.Knowledge transfer for deep reinforcement learning with hierarchical experience replay[C]Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2017:1640-1646" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Knowledge transfer for deep reinforcement learning with hierarchical experience replay">
                                        <b>[35]</b>
                                        Yin Haiyan, Pan Jialin.Knowledge transfer for deep reinforcement learning with hierarchical experience replay[C]Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2017:1640-1646
                                    </a>
                                </li>
                                <li id="1038">


                                    <a id="bibliography_36" title="Liu Quan, Zhai Jianwei, Zhong Shan, et al.A Deep recurrent Q-network based on visual attention mechanism[J].Chinese Journal of Computers, 2017, 40 (6) :1353-1366 (in Chinese) (刘全, 翟建伟, 钟珊, 等.一种基于视觉注意力机制的深度循环Q网络模型[J].计算机学报, 2017, 40 (6) :1353-1366) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706008&amp;v=MTY1MTJxWTlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydk1MejdCZHJHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[36]</b>
                                        Liu Quan, Zhai Jianwei, Zhong Shan, et al.A Deep recurrent Q-network based on visual attention mechanism[J].Chinese Journal of Computers, 2017, 40 (6) :1353-1366 (in Chinese) (刘全, 翟建伟, 钟珊, 等.一种基于视觉注意力机制的深度循环Q网络模型[J].计算机学报, 2017, 40 (6) :1353-1366) 
                                    </a>
                                </li>
                                <li id="1040">


                                    <a id="bibliography_37" title="Hou Yuenan, Liu Lifeng, Wei Qing, et al.A novel DDPGmethod with prioritized experience replay[C]Proc of the13th IEEE Int Conf on Systems, Man, and Cybernetics (SMC) .Piscataway, NJ:IEEE, 2017:316-321" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel DDPGmethod with prioritized experience replay">
                                        <b>[37]</b>
                                        Hou Yuenan, Liu Lifeng, Wei Qing, et al.A novel DDPGmethod with prioritized experience replay[C]Proc of the13th IEEE Int Conf on Systems, Man, and Cybernetics (SMC) .Piscataway, NJ:IEEE, 2017:316-321
                                    </a>
                                </li>
                                <li id="1042">


                                    <a id="bibliography_38" title="Bruin T, Kober J, Tuyls K, et al.The importance of experience replay database composition in deep reinforcement learning[EB/OL].2015[2017-11-04].http:www.jenskober.de/deBruinNIPS_WS2015.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The importance of experience replay database composition in deep reinforcement learning">
                                        <b>[38]</b>
                                        Bruin T, Kober J, Tuyls K, et al.The importance of experience replay database composition in deep reinforcement learning[EB/OL].2015[2017-11-04].http:www.jenskober.de/deBruinNIPS_WS2015.pdf
                                    </a>
                                </li>
                                <li id="1044">


                                    <a id="bibliography_39" title="Silver D, Lever G, Heess N, et al.Deterministic policy gradient algorithms[C]Proc of the 31st Int Conf on Machine Learning (ICML) .New York:JMLR, 2014:387-395" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deterministic policy gradient algorithms">
                                        <b>[39]</b>
                                        Silver D, Lever G, Heess N, et al.Deterministic policy gradient algorithms[C]Proc of the 31st Int Conf on Machine Learning (ICML) .New York:JMLR, 2014:387-395
                                    </a>
                                </li>
                                <li id="1046">


                                    <a id="bibliography_40" title="Lillicrap T P, Hunt J J, Pritzel A, et al.Continuous control with deep reinforcement learning[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:232-241" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous control with deep reinforcement learning">
                                        <b>[40]</b>
                                        Lillicrap T P, Hunt J J, Pritzel A, et al.Continuous control with deep reinforcement learning[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:232-241
                                    </a>
                                </li>
                                <li id="1048">


                                    <a id="bibliography_41" title="Barto A G, Sutton R S, Anderson C W.Neuronlike adaptive elements that can solve difficult learning control problems[J].IEEE Transactions on Systems, Man, and Cybernetics, 1983, 13 (5) :834-846" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neuronlike adaptive elements that can solve difficult learning control problems">
                                        <b>[41]</b>
                                        Barto A G, Sutton R S, Anderson C W.Neuronlike adaptive elements that can solve difficult learning control problems[J].IEEE Transactions on Systems, Man, and Cybernetics, 1983, 13 (5) :834-846
                                    </a>
                                </li>
                                <li id="1050">


                                    <a id="bibliography_42" title="Mahmood A R, Hasselt H V, Sutton R S.Weighted importance sampling for off-policy learning with linear function approximation[C]Proc of the 24th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:3014-3022" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weighted importance sampling for off-policy learning with linear function approximation">
                                        <b>[42]</b>
                                        Mahmood A R, Hasselt H V, Sutton R S.Weighted importance sampling for off-policy learning with linear function approximation[C]Proc of the 24th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:3014-3022
                                    </a>
                                </li>
                                <li id="1052">


                                    <a id="bibliography_43" title="Bellmare M G, Naddaf Y, Veness J, et al.The arcade learning environment:An evaluation platform for general agents[J].Journal of Artificial Intelligence Research, 2013, 47 (1) :253-279" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The arcade learning environment:an evaluation platform for general agents">
                                        <b>[43]</b>
                                        Bellmare M G, Naddaf Y, Veness J, et al.The arcade learning environment:An evaluation platform for general agents[J].Journal of Artificial Intelligence Research, 2013, 47 (1) :253-279
                                    </a>
                                </li>
                                <li id="1054">


                                    <a id="bibliography_44" title="Brockman G, Cheung V, Pettersson L, et al.OpenAI gym[EB/OL].New York:Cornell University, 2016[2017-10-04].https:arxiv.org/abs/1606.01540" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OpenAI gym">
                                        <b>[44]</b>
                                        Brockman G, Cheung V, Pettersson L, et al.OpenAI gym[EB/OL].New York:Cornell University, 2016[2017-10-04].https:arxiv.org/abs/1606.01540
                                    </a>
                                </li>
                                <li id="1056">


                                    <a id="bibliography_45" title="Abadi M, Agarwal A, Barham P, et al.TensorFlow:Largescale machine learning on heterogeneous distributed systems[EB/OL].New York:Cornell University, 2016[2017-10-04].https:arxiv.org/abs/1603.04467" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensor Flow:large-scale machine learning on heterogeneous distributed systems">
                                        <b>[45]</b>
                                        Abadi M, Agarwal A, Barham P, et al.TensorFlow:Largescale machine learning on heterogeneous distributed systems[EB/OL].New York:Cornell University, 2016[2017-10-04].https:arxiv.org/abs/1603.04467
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),262-280 DOI:10.7544/issn1000-1239.2019.20170812            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于TD-error自适应校正的深度Q学习主动采样方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%99%BD%E8%BE%B0%E7%94%B2&amp;code=38037602&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">白辰甲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E9%B9%8F&amp;code=06989271&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%B7%8D&amp;code=07004232&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵巍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E9%99%8D%E9%BE%99&amp;code=06997946&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐降龙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%93%88%E5%B0%94%E6%BB%A8%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%99%BA%E8%83%BD%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0149444&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈尔滨工业大学计算机科学与技术学院模式识别与智能系统研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>强化学习中智能体与环境交互的成本较高.针对深度Q学习中经验池样本利用效率的问题, 提出基于TD-error自适应校正的主动采样方法.深度Q学习训练中样本存储优先级的更新滞后于Q网络参数的更新, 存储优先级不能准确反映经验池中样本TD-error的真实分布.提出的TD-error自适应校正主动采样方法利用样本回放周期和Q网络状态建立优先级偏差模型, 估计经验池中样本的真实优先级.在Q网络迭代中使用校正后的优先级选择样本, 偏差模型在学习过程中分段更新.分析了Q网络学习性能与偏差模型阶数和模型更新周期之间的依赖关系, 并对算法复杂度进行了分析.方法在Atari 2600平台进行了实验, 结果表明, 使用TD-error自适应校正的主动采样方法选择样本提高了智能体的学习速度, 减少了智能体与环境的交互次数, 同时改善了智能体的学习效果, 提升了最优策略的质量.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%B7%E6%9C%AC%E4%BC%98%E5%85%88%E7%BA%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">样本优先级;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=TD-error%E6%A0%A1%E6%AD%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">TD-error校正;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E5%8A%A8%E9%87%87%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主动采样;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6Q%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度Q学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *赵巍 (zhaowei@hit.edu.cn) ;
                                </span>
                                <span>
                                    白辰甲 bai_chenjia@stu.hit.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-10-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61671175, 61672190);</span>
                    </p>
            </div>
                    <h1><b>Active Sampling for Deep Q-Learning Based on TD-error Adaptive Correction</b></h1>
                    <h2>
                    <span>Bai Chenjia</span>
                    <span>Liu Peng</span>
                    <span>Zhao Wei</span>
                    <span>Tang Xianglong</span>
            </h2>
                    <h2>
                    <span>Pattern Recognition and Intelligent System Research Center, School of Computer Science and Technology, Harbin Institute of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep reinforcement learning (DRL) is one of research hotspots in artificial intelligence. Deep Q-learning is one of the representative achievements of DRL. In some fields, its performance has met or exceeded the level of human expert. It is necessary for training deep Q-learning to acquire lots of samples. These samples are obtained by the interaction between agent and environment. However, it is usually computationally intensive and sometimes impossible to keep away from interaction risk. We propose an active sampling method based on TD-error adaptive correction in order to solve sample efficiency problem in deep Q-learning. In various deep Q-learning methods, the updating of storage priority in experience memory lags behind the updating of Q-network parameters. It causes that a lot of samples are not selected to apply in Q-network training because the storage priority cannot reflect the true distribution of TD-error in experience memory. The TD-error adaptive correction active sampling method proposed in this paper uses the replay periods of samples and Q-network state to establish a priority bias model to estimate the real priority of each sample in experience memory during the Q-network iteration. The samples are selected from experience memory according to the corrected priority and the bias model parameters are adaptively updated by a segmented form. We analyze the complexity of the algorithm and the relationship between learning performance and the order of polynomial feature and updating period of model parameters. Our method is verified on the platform of Atari 2600. The experimental results show that proposed method improves the learning speed and reduces the number of interaction between agent and environment. Meanwhile, it ameliorates the quality of optimal policy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sample%20priority&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sample priority;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=TD-error%20correction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">TD-error correction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaption&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaption;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=active%20sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">active sampling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20Q-learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep Q-learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Bai Chenjia, born in 1993.PhD candidate of Pattern Recognition and Intelligent System Research Center, Harbin Institute of Technology.Student member of CCF.His main research interests include reinforcement learning and neural network.<image id="948" type="formula" href="images/JFYZ201902004_94800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Liu Peng, born in 1975.Received his PhDdegree of microelectronics and solid-state electronics from Harbin Institute of Technology in 2007.Associate professor at the School of Computer Science and Technology, Harbin Institute of Technology.His main research interest covers image processing, video analysis, pattern recognition, and design of very large scale integrated (VLSI) circuit.<image id="950" type="formula" href="images/JFYZ201902004_95000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Zhao Wei, born in 1972.Associate professor of School of the Computer Science and Technology, Harbin Institute of Technology.Her research fields include pattern recognition, machine learning and computer vision.<image id="952" type="formula" href="images/JFYZ201902004_95200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Tang Xianglong, born in 1960.Received his PhD degree of computer application technology from Harbin Institute of Technology in 1995.Professor at the School of Computer Science and Technology, Harbin Institute of Technology.His main research interest covers pattern recognition, image processing, and machine learning.<image id="954" type="formula" href="images/JFYZ201902004_95400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-10-27</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61671175, 61672190);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="578">强化学习<sup><a class="sup">[1]</a></sup> (reinforcement learning, RL) 是机器学习的重要分支.智能体在与环境的交互过程中根据当前状态选择动作, 执行动作后转移到下一个状态并获得奖励.从智能体执行动作到获得奖励的过程产生了一条“经验”, 智能体从大量经验中学习, 不断改进策略, 最大化累计奖励.传统强化学习方法适用于离散状态空间, 不能有效处理高维连续状态空间中的问题.线性值函数近似或非线性值函数近似法将状态与值函数之间的关系进行参数化表示<sup><a class="sup">[2]</a></sup>, 可以处理高维状态空间中的问题.其中, 线性值函数近似有良好的收敛性保证, 但往往需要人为提取状态特征, 且模型表达能力有限;非线性值函数近似没有严格的收敛性证明, 但具有强大的模型表达能力.Tesauro<sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup>将时序差分法 (temporal difference, TD) 与非线性值函数近似相结合, 提出了TD-Gammon算法.该算法用前馈神经网络对状态值函数进行估计, 在西洋双陆棋中达到了人类顶尖水平.</p>
                </div>
                <div class="p1">
                    <p id="579">近年来, 深度学习<sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup>与强化学习相结合<sup><a class="sup">[7]</a></sup>的非线性值函数近似方法在大规模强化学习中表现出良好性能.谷歌DeepMind团队提出了“深度Q网络”<sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup> (deep Q-network, DQN) , DQN以图像序列作为输入, 输出对动作值函数的估计, 策略表示为卷积神经网络的参数.DQN在Atari 2600的大多数视频游戏上的性能超过了人类专家水平.</p>
                </div>
                <div class="p1">
                    <p id="580">对经验样本的存储和采样是DQN的关键问题.DQN在训练中使用误差梯度反向传播算法, 只有训练样本集合满足或近似满足独立同分布时才能正常收敛<sup><a class="sup">[10]</a></sup>.然而, 智能体与环境交互产生的样本前后具有强相关性.为了消除样本相关性, DQN使用经验池来存储和管理样本, 在训练中从经验池中随机取样.DQN需要容量巨大的经验池 (约10<sup>6</sup>) , 为了填充经验池, 智能体需要频繁地与环境进行交互.然而, 智能体与环境交互的代价非常昂贵的, 不仅表现在时间的消耗上, 更表现在安全性、可控性、可恢复性等诸多方面<sup><a class="sup">[11]</a></sup>.因此, 研究如何对样本进行高效采样、减少智能体与环境的交互次数、改善智能体的学习效果, 对深度强化学习研究有重要意义.</p>
                </div>
                <div class="p1">
                    <p id="581">目前针对DQN样本采样问题的研究主要有2个方面:1) 并行采样和训练.使用多个智能体并行采样和训练, 用异步梯度法更新Q网络<sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup>, 其思想是通过并行采样和训练来打破样本之间的相关性.2) 主动采样.强调样本之间的差异性<sup><a class="sup">[14]</a></sup>, 为每个样本设定优先级, 按优先级进行采样.Schaul等人<sup><a class="sup">[11]</a></sup>提出了优先经验回放法 (prioritized experience replay, PER) , 经验池中样本被采样的概率与样本的存储优先级成正比, 存储优先级由经验池中样本上次参与训练的TD-error决定.TD-error是样本在使用TD算法更新时目标值函数与当前状态值函数的差值, 其中目标值函数是立即奖励与下一个状态值函数之和.尽管PER算法在一定程度上提高了样本的利用效率, 但在包含大量样本的经验池中, 每个时间步仅有少量样本可以参与Q网络迭代, 其余样本无法参与训练, 这些样本的TD-error没有跟随Q网络的更新而变化, 导致样本的存储优先级无法准确反映经验池中TD-error的真实分布.</p>
                </div>
                <div class="p1">
                    <p id="582">为了进一步提高样本的采样效率, 应当以更准确的采样优先级在经验池中选择样本.本文研究发现, 经验池中样本距离上次被采样的时间间隔越长, 样本的存储TD-error偏离真实值越远.由于长时间未被采样的样本数量巨大, 导致存储优先级与真实优先级的偏差较大.本文提出一种基于TD-error自适应校正的主动采样模型 (active sampling method based on adaptive TD-error correction, ATDC-PER) .该模型基于样本回放周期和网络状态在样本与优先级偏差之间建立映射关系, 实现对存储优先级的校正, 在Q网络每次训练中都使用校正后的优先级在经验池中选择样本.本文提出的偏差模型参数是自适应的, 在训练中模型始终跟随Q网络的变化, 对采样优先级的估计更为合理.实验结果表明, 校正后的优先级符合经验池中样本TD-error的分布, 利用校正后的优先级选择样本提高了样本的利用效率, 减少了智能体与环境的交互, 同时改善了智能体的学习效果, 获得了更大的累计奖励.</p>
                </div>
                <h3 id="583" name="583" class="anchor-tag"><b>1</b><b>相关工作</b></h3>
                <h4 class="anchor-tag" id="584" name="584"><b>1.1</b><b>强化学习</b></h4>
                <div class="p1">
                    <p id="585">强化学习的目标是最大化累计奖励.智能体通过从经验中学习, 不断优化状态与动作之间的映射关系, 最终找到最优策略 (policy) .智能体与环境的交互过程可以用马尔可夫决策过程<sup><a class="sup">[15]</a></sup> (Markov decision processes, MDP) 来建模.在周期型任务中, MDP包括一系列离散的时间步0, 1, 2, …, <i>t</i>, …, <i>T</i>, 其中<i>T</i>为终止时间步.在时间步<i>t</i>, 智能体观察环境得到状态的表示<i>S</i><sub><i>t</i></sub>, 根据现有策略<i>π</i>选择动作<i>A</i><sub><i>t</i></sub>并执行.执行动作后MDP到达下一个时间步<i>t</i>+1, 智能体收到奖励<i>R</i><sub><i>t</i>+1</sub>并转移到下一个状态<i>S</i><sub><i>t</i>+1</sub>.回报定义为折扣奖励之和, 智能体通过调整策略来最大化回报.动作状态值函数是指智能体处于状态<i>s</i>时执行动作<i>a</i>, 随后按照策略<i>π</i>与环境交互直到周期结束所获得的期望回报, 记为<i>Q</i><sub><i>π</i></sub> (<i>s</i>, <i>a</i>) .</p>
                </div>
                <div class="p1">
                    <p id="586">最优策略求解一般遵循“广泛策略迭代”的思想, 包含策略评价和策略提升<sup><a class="sup">[1]</a></sup>.策略评价是已知策略计算值函数的过程, 策略提升是已知值函数选择最优动作的过程.策略评价中值函数<i>Q</i><sub><i>π</i></sub> (<i>s</i>, <i>a</i>) 的求解可以使用贝尔曼方程<sup><a class="sup">[15]</a></sup>转换为递归形式</p>
                </div>
                <div class="p1">
                    <p id="587"><mathml id="588"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><mi>r</mi></mrow></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><mi>r</mi><mo stretchy="false">|</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><msup><mi>a</mi><mo>′</mo></msup></munder><mspace width="0.25em" /><mi>Q</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="589">其中, <i>r</i>是立即奖励, <i>γ</i>是折扣因子.值函数定义了在策略空间上的偏序关系, 存在最优策略<i>π</i><sup>*</sup>优于 (或等同于) 其他所有策略.</p>
                </div>
                <div class="p1">
                    <p id="590">贝尔曼方程中<i>p</i> (<i>s</i>′, <i>r</i>|<i>s</i>, <i>a</i>) 代表状态转移概率, 由智能体所处的环境决定.如果已知状态转移概率, 可以使用动态规划<sup><a class="sup">[16]</a></sup>、树搜索<sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup>等求解最优策略, 此类方法为基于模型 (model-base) 的强化学习.然而, 在多数问题中无法获取环境的状态转移概率, 智能体需要通过与环境交互进行学习, 此类方法为模型无关 (model-free) 的强化学习, 主要包括策略梯度法和Q学习2种类型.策略梯度法<sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup>对策略<i>π</i>进行建模, 奖励均值的期望作为策略<i>π</i>的评价函数, 策略<i>π</i>跟随策略梯度的方向更新来最大化评价函数.Q学习法主要包括蒙特卡洛法<sup></sup><sup><a class="sup">[23]</a></sup> (Monte Carlo, MC) 、时序差分法<sup><a class="sup">[24]</a></sup> (temporal difference, TD) 等.其中, MC方法需在一个周期结束后更新值函数, 而TD方法在交互过程的每个时间步均可更新值函数, 因而适用范围更广.TD方法包括在策略 (on-policy) 和离策略 (off-policy) 2种类型.Q-learning<sup><a class="sup">[24]</a></sup>是一种典型的离策略算法, 使用智能体与环境交互产生的样本<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>, <i>r</i><sub><i>t</i>+1</sub>, <i>s</i><sub><i>t</i>+1</sub>进行值函数更新.离散状态空间下Q-learning算法中TD-error是目标动作值函数与当前动作值函数之差, 其中目标动作值函数是智能体获得的立即奖励与下一个状态的最大动作值函数之和, 目标动作值函数表示智能体在未来的期望回报.TD-error用<mathml id="591"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>-</mo><mrow><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo></mrow><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>表示, 其中<i>γ</i>是折扣因子.</p>
                </div>
                <div class="p1">
                    <p id="592">在大多数实际问题中无法用表格化的方式穷举所有的 (<i>s</i>, <i>a</i>) 组合, 因此使用参数化方式对值函数进行表达, 记为<i>Q</i> (<i>s</i>, <i>a</i>;<i>θ</i><sub><i>t</i></sub>) , 其中<i>θ</i><sub><i>t</i></sub>为参数向量.值函数近似条件下Q-learning算法中TD-error为<mathml id="593"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>a</mi></mrow></math></mathml>;<i>θ</i><sub><i>t</i></sub>) -<i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>;<i>θ</i><sub><i>t</i></sub>) , 参数向量按照<i>θ</i><sub><i>t</i></sub>=<i>θ</i><sub><i>t</i></sub>+<i>α δ</i><image href="images/JFYZ201902004_594.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>θ</i><sub><i>t</i></sub></sub><i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>, <i>θ</i><sub><i>t</i></sub>) 进行更新.</p>
                </div>
                <h4 class="anchor-tag" id="595" name="595"><b>1.2</b><b>深度Q网络</b></h4>
                <div class="p1">
                    <p id="596">深度Q网络<sup><a class="sup">[9]</a></sup> (DQN) 遵循Q-learning算法的更新法则, 与线性值函数近似的不同之处在于DQN使用Q网络来表示动作值函数, 进而表示策略.Q网络是由卷积层和全连接层组成的卷积神经网络 (CNN) .DQN的主要特点有2个:</p>
                </div>
                <div class="p1">
                    <p id="597">1) 使用2个独立的Q网络.分别使用<i>θ</i>和<i>θ</i><sup>-</sup>代表Q网络和目标Q网络的参数, 每隔<i>L</i>个时间步将Q网络的参数复制到目标Q网络中, 即<i>θ</i><sup>-</sup>←<i>θ</i>, 随后<i>θ</i><sup>-</sup>在<i>L</i>个时间步内保持不变.DQN中TD-error定义为</p>
                </div>
                <div class="p1">
                    <p id="598"><mathml id="599"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>D</mtext><mtext>Q</mtext><mtext>Ν</mtext></mrow></msubsup><mo>=</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>a</mi></mrow></math></mathml>;<i>θ</i><sup>-</sup><sub><i>t</i></sub>) -</p>
                </div>
                <div class="p1">
                    <p id="600"><i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>;<i>θ</i><sub><i>t</i></sub>) . (1) </p>
                </div>
                <div class="p1">
                    <p id="601">使用2个独立的Q网络可以使学习的目标值函数分段保持稳定, 使训练过程更加鲁棒.Q网络的损失函数为平方误差损失, <i>L</i> (<i>θ</i><sub><i>t</i></sub>) =Ε<sub><i>s</i>, <i>a</i></sub>[ (<i>δ</i><mathml id="602"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>D</mtext><mtext>Q</mtext><mtext>Ν</mtext></mrow></msubsup></mrow></math></mathml>) <sup>2</sup>], 用误差反向传播算法迭代更新Q网络的参数, 直到Q网络收敛.</p>
                </div>
                <div class="p1">
                    <p id="603">2) 使用经验池存储和管理样本, 使用经验回放<sup><a class="sup">[25]</a></sup>选择样本.样本存储于经验池中, 从经验池中随机抽取批量样本训练Q网络.使用经验回放可以消除样本之间的相关性, 使参与训练的样本满足或近似满足独立同分布.</p>
                </div>
                <div class="p1">
                    <p id="604">在DQN的基础上, Hasselt等人<sup>[<a class="sup">26</a>,<a class="sup">27</a>]</sup>指出, DQN在计算TD-error时使用相同的Q网络来选择动作和计算值函数会导致值函数的过高估计, 进而提出了DDQN (double DQN) 算法.该算法用Q网络来选择动作, 用目标Q网络来估计值函数, 从而消除了对值函数的过高估计, 并提升了智能体的累计奖励.DDQN中TD-error由式 (2) 计算:</p>
                </div>
                <div class="p1">
                    <p id="605"><mathml id="606"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>D</mtext><mtext>D</mtext><mtext>Q</mtext><mtext>Ν</mtext></mrow></msubsup><mspace width="0.25em" /><mo>=</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>a</mi></mrow></math></mathml>;<i>θ</i><sub><i>t</i></sub>) ;<i>θ</i><sup>-</sup><sub><i>t</i></sub>) -</p>
                </div>
                <div class="p1">
                    <p id="607"><i>Q</i> (<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>;<i>θ</i><sub><i>t</i></sub>) . (2) </p>
                </div>
                <div class="p1">
                    <p id="608">此外, 一些研究从不同角度提出了DQN的改进方法, 主要思路可以分为:1) 从探索和利用的角度<sup>[<a class="sup">28</a>,<a class="sup">29</a>,<a class="sup">30</a>,<a class="sup">31</a>]</sup>, 将传统强化学习中使用的探索与利用策略扩展到深度强化学习中, 改进DQN中的<i>ε</i>-贪心策略.2) 从网络结构的角度, 使用竞争式网络结构<sup><a class="sup">[32]</a></sup>, 分别估计动作值函数和优势函数.3) 从并行训练的角度<sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup>, 消除样本之间的相关性, 同时加速训练.4) 从减少训练中奖励方差的角度<sup><a class="sup">[33]</a></sup>, 使用多个迭代步的延迟估计Q值, 使其更加稳定.5) 从模型之间知识迁移和多任务学习的角度<sup>[<a class="sup">34</a>,<a class="sup">35</a>]</sup>, 扩展DQN的使用范围.6) 从视觉注意力的角度, 使智能体在训练中将注意力集中于有价值的图像区域<sup><a class="sup">[36]</a></sup>.7) 从主动采样, 减少与环境交互次数的角度<sup><a class="sup">[11]</a></sup>, 使用基于TD-error的优先经验回放来提高样本的利用效率.</p>
                </div>
                <h4 class="anchor-tag" id="609" name="609"><b>1.3</b><b>优先经验回放</b></h4>
                <div class="area_img" id="610">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_610.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法 (ATDC-PER) 的总体结构" src="Detail/GetImg?filename=images/JFYZ201902004_610.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法 (ATDC-PER) 的总体结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_610.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The main architecture of ATDC-PER model</p>

                </div>
                <div class="p1">
                    <p id="611">优先经验回放是一种有效的管理经验池的方法.原始的DQN算法在训练时不考虑经验池中样本之间的差异, 随机抽取样本进行训练, 导致样本的利用效率较低, 智能体需要频繁地与环境进行交互.针对这一问题, Schaul等人<sup><a class="sup">[11]</a></sup>提出了优先经验回放法 (PER) 来估计每个样本的优先级, 按照优先级选择样本训练Q网络.样本优先级由样本的TD-error分布决定.样本的TD-error绝对值越大, 在训练中造成的Q网络损失越大, 表明该样本给Q网络带来的信息量越大, 在Q网络后续迭代中应“优先”选择该样本参与训练.PER算法的主要特点有3个:</p>
                </div>
                <div class="p1">
                    <p id="612">1) 样本优先级基于样本上次参与训练时的TD-error值.长时间未参与训练的样本在经验池中存储的TD-error长时间未更新, 这些样本的TD-error无法跟随Q网络的变化.只有上一个时间步参与训练的批量样本的TD-error值被更新, 然而这部分样本仅占经验池中样本的少数.</p>
                </div>
                <div class="p1">
                    <p id="613">2) 采样时在优先级的基础上引入随机性.完全按照优先级进行采样会降低样本的多样性, 因此PER算法中引入随机性, 使样本被抽取的概率与优先级成正比, 同时所有样本都有机会被采样.</p>
                </div>
                <div class="p1">
                    <p id="614">3) 优先级的引入相对于均匀抽样改变了样本的分布, 引入了误差.PER算法使用重要性采样权重对误差进行补偿.在计算损失函数梯度时, 需在原有梯度的基础上乘以重要性采样权重, 按补偿后的梯度进行更新.</p>
                </div>
                <div class="p1">
                    <p id="615">PER算法<sup><a class="sup">[11]</a></sup>具有高度的灵活性, 可与离策略的深度强化学习方法进行广泛结合.例如, 文献[37-38]将PER算法和策略梯度法结合, 提高了确定性策略梯度算法<sup>[<a class="sup">39</a>,<a class="sup">40</a>]</sup> (deep deterministic policy gradient, DDPG) 在仿真机器人控制中的数据利用效率;PER算法与深度Q学习的结合提高了经验池中样本的利用效率, 显著减少了智能体与环境的交互次数, 提升了智能体的最优策略得分<sup><a class="sup">[11]</a></sup>.</p>
                </div>
                <h3 id="616" name="616" class="anchor-tag"><b>2</b><b>基于TD-error校正的主动采样模型</b></h3>
                <div class="p1">
                    <p id="617">PER算法的问题在于, 经验池中存储的TD-error值相对于Q网络来说更新不及时, 无法准确反映样本的优先级.在每次迭代中Q网络都会更新自身参数, 而经验池仅可更新参与训练的批量样本的TD-error值, 此类样本的数量约占经验池容量的0.1%.这导致大多数样本的存储TD-error无法跟随Q网络的变化, 并进一步导致样本在经验池中的存储优先级与“真实”优先级之间存在偏差.本文分析了这种偏差对学习的影响, 并阐述了依据真实或逼近真实的TD-error分布产生的优先级来选择样本能够使深度Q网络以更高的效率收敛到最优策略.</p>
                </div>
                <div class="p1">
                    <p id="618">本文提出基于TD-error自适应校正的主动采样模型 (ATDC-PER) 来校正样本的存储优先级, 使用校正后的优先级分布进行主动采样, 能够提高经验池中样本的利用效率.图1是本文方法ATDC-PER的总体结构, 主要包括“偏差模型”和“优先级校正”两部分.其中, 优先级校正需在所有时间步进行, 偏差模型则每隔<i>D</i>个时间步更新1次.2.1节使用引例说明TD-error及样本优先级分布对学习的作用, 2.2节阐述ATDC-PER的建模过程, 2.3节给出算法描述和复杂度分析.</p>
                </div>
                <h4 class="anchor-tag" id="619" name="619"><b>2.1</b><b>TD-error偏差分析</b></h4>
                <div class="p1">
                    <p id="620">本节用强化学习中的经典问题平衡车<sup><a class="sup">[41]</a></sup> (CartPole) 为引例<sup>①</sup>, 说明PER算法中经验池的存储TD-error与真实TD-error之间存在偏差, 并分析TD-error分布对学习的作用.</p>
                </div>
                <div class="p1">
                    <p id="621">CartPole的状态表示为4维向量, 包括杆的角度、杆运动的角速度、车的位置和车运动的速度, 如图2所示:</p>
                </div>
                <div class="area_img" id="622">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_622.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 倒立摆的状态表示" src="Detail/GetImg?filename=images/JFYZ201902004_622.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 倒立摆的状态表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_622.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The state of CartPole</p>

                </div>
                <div class="p1">
                    <p id="623">智能体通过向小车施加+1或-1的力来尽力保持杆的平衡.在时间步<i>t</i>, 如果杆与垂直方向的角度不超过15度且小车与中心的距离不超过2.4个单元, 则奖励为+1, 否则奖励为-1同时周期结束.设1个周期的时间步不超过200步, 则智能体在1个周期内所获得的最大奖励不超过200.用基于DDQN的PER算法来训练Q网络, Q网络是包含64个隐含层节点的3层全连接网络, 经验池容量为50 000.</p>
                </div>
                <div class="p1">
                    <p id="624">PER算法中经验池用样本集合<i>E</i>={<i>e</i><sup> (1) </sup>, <i>e</i><sup> (2) </sup>, …, <i>e</i><sup> (<i>m</i>) </sup>}表示, 其中序号为<i>i</i>的样本<i>e</i><sup> (<i>i</i>) </sup>=&lt;<i>s</i><sup> (<i>i</i>) </sup>, <i>a</i><sup> (<i>i</i>) </sup>, <i>r</i><sup> (<i>i</i>) </sup>, <i>s</i>′<sup> (<i>i</i>) </sup>&gt;∈E.PER算法在每次迭代时抽取<i>E</i>中的1个批量样本进行训练.本文将样本<i>e</i><sup> (<i>i</i>) </sup>上次参与训练的TD-error值称为样本的存储TD-error, 记为<i>δ</i><sup> (<i>i</i>) </sup>.设当前时间步为<i>t</i>, <i>e</i><sup> (<i>i</i>) </sup>上次参与训练的时间步为<i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup>, 则<i>δ</i><sup> (<i>i</i>) </sup>由式 (3) 计算<sup><a class="sup">[27]</a></sup>:</p>
                </div>
                <div class="p1">
                    <p id="625"><mathml id="626"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi>r</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi>a</mi></mrow></math></mathml>;<i>θ</i><sub><i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="627"><i>θ</i><sup>-</sup><sub><i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup></sub>) -<i>Q</i> (<i>s</i><sup> (<i>i</i>) </sup>, <i>a</i><sup> (<i>i</i>) </sup>;<i>θ</i><sub><i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup></sub>) , (3) </p>
                </div>
                <div class="p1">
                    <p id="628">其中<i>θ</i><sub><i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup></sub>和<i>θ</i><sup>-</sup><sub><i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup></sub>分别是<i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup>时间步的Q网络和目标Q网络参数.</p>
                </div>
                <div class="p1">
                    <p id="629">由式 (3) 可知, 存储TD-error计算中使用的参数<i>θ</i><sub><i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup></sub>不是当前Q网络参数, 而是<i>τ</i><sup> (<i>i</i>) </sup>个时间步之前的参数.在<i>τ</i><sup> (<i>i</i>) </sup>个时间步内Q网络经历了<i>τ</i><sup> (<i>i</i>) </sup>次参数迭代, 而存储TD-error值却保持不变.这导致样本的存储TD-error与当前Q网络不匹配, 且<i>τ</i><sup> (<i>i</i>) </sup>的值越大, 这种不匹配程度越高.新样本加入经验池时<i>τ</i><sup> (<i>i</i>) </sup>=1, 每经历1个时间步, 参与该时间步训练的样本<i>τ</i><sup> (<i>i</i>) </sup>置1, 未参与该时间步训练的样本<i>τ</i><sup> (<i>i</i>) </sup>加1.由于经验池满后新样本会覆盖老样本, 因此所有样本<i>τ</i><sup> (<i>i</i>) </sup>的取值范围均在1到经验池样本总量之间.图3显示了CartPole在训练周期为420时经验池样本的<i>τ</i><sup> (<i>i</i>) </sup>的分布, 样本按<i>τ</i><sup> (<i>i</i>) </sup>由小到大排序, 经验池中样本总数为50 000.图3表明, 经验池中有大量样本的<i>τ</i><sup> (<i>i</i>) </sup>值较大, 这些样本长期没有参与训练, <i>δ</i><sup> (<i>i</i>) </sup>长期未更新.因此, 存储优先级与当前Q网络的不匹配程度较高.</p>
                </div>
                <div class="area_img" id="630">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_630.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 训练周期为420时经验池中样本的τ (i) 的分布" src="Detail/GetImg?filename=images/JFYZ201902004_630.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 训练周期为420时经验池中样本的<i>τ</i><sup> (<i>i</i>) </sup>的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_630.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Distribution of <i>τ</i><sup> (<i>i</i>) </sup> in experience memory when  training episode is 420</p>

                </div>
                <div class="p1">
                    <p id="631">引入一种理想的TD-error计算方式, 称为样本“真实”的TD-error, 记为<i>δ</i><mathml id="632"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>.计算<i>δ</i><mathml id="633"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的分布需在当前时间步将经验池的全部样本送入Q网络和目标Q网络, 更新包括上次参与训练的样本在内的经验池中全部样本的TD-error值.在这种计算方式下, 经验池中任意样本的TD-error均由最新的Q网络和目标Q网络计算, 均与当前最新的网络参数<i>θ</i><sub><i>t</i></sub>和<i>θ</i><sup>-</sup><sub><i>t</i></sub>相对应, 如式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="634"><mathml id="635"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi>r</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi>a</mi></mrow></math></mathml>;<i>θ</i><sub><i>t</i></sub>) ;<i>θ</i><sup>-</sup><sub><i>t</i></sub>) -</p>
                </div>
                <div class="p1">
                    <p id="636"><i>Q</i> (<i>s</i><sup> (<i>i</i>) </sup>, <i>a</i><sup> (<i>i</i>) </sup>;<i>θ</i><sub><i>t</i></sub>) . (4) </p>
                </div>
                <div class="p1">
                    <p id="637">用<i>p</i><sup> (<i>i</i>) </sup>和<i>p</i><mathml id="638"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分别表示样本在经验池中的存储优先级和真实优先级, 分别与存储TD-error值<i>δ</i><sup> (<i>i</i>) </sup>和真实TD-error值<i>δ</i><mathml id="639"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>相对应, 用式 (5) (6) 计算.</p>
                </div>
                <div class="p1">
                    <p id="640"><i>p</i><sup> (<i>i</i>) </sup>= (|<i>δ</i><sup> (<i>i</i>) </sup>|+<i>ε</i>) <sup><i>α</i></sup>, (5) </p>
                </div>
                <div class="p1">
                    <p id="641"><i>p</i><mathml id="642"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>= (|<i>δ</i><mathml id="643"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|+<i>ε</i>) <sup><i>α</i></sup>, (6) </p>
                </div>
                <div class="p1">
                    <p id="644">其中<i>ε</i>和<i>α</i>均为常数.样本TD-error的绝对值越大, 在采样中样本的优先级就越高.</p>
                </div>
                <div class="p1">
                    <p id="645">真实优先级<i>p</i><mathml id="646"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>与当前最新的Q网络相对应, 而存储优先级<i>p</i><sup> (<i>i</i>) </sup>相对于当前Q网络滞后了<i>τ</i><sup> (<i>i</i>) </sup>个时间步, 二者的分布存在差异.图4比较了CartPole训练周期为180时<i>p</i><sup> (<i>i</i>) </sup>和<i>p</i><mathml id="647"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的分布, 样本按<i>p</i><sup> (<i>i</i>) </sup>进行排序.图4中坐标系右侧的<i>p</i><sup> (<i>i</i>) </sup>和<i>p</i><mathml id="648"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的分布偏差较大, 表明在PER算法中用于选择样本的存储优先级并没有反映经验池中样本的真实情况.</p>
                </div>
                <div class="area_img" id="649">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_649.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CartPole训练周期为180时p (i) 和p (i) real分布图" src="Detail/GetImg?filename=images/JFYZ201902004_649.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 CartPole训练周期为180时<i>p</i><sup> (<i>i</i>) </sup>和<i>p</i><sup> (<i>i</i>) </sup><sub>real</sub>分布图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_649.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Distribution of <i>p</i><sup> (<i>i</i>) </sup> and <i>p</i><sup> (<i>i</i>) </sup><sub>real</sub> in CartPole training  when training episode is 180</p>

                </div>
                <div class="p1">
                    <p id="650">设Δ<i>p</i><sup> (<i>i</i>) </sup>为真实优先级<i>p</i><mathml id="651"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>与存储优先级<i>p</i><sup> (<i>i</i>) </sup>之间的优先级偏差:</p>
                </div>
                <div class="p1">
                    <p id="652">Δ<i>p</i><sup> (<i>i</i>) </sup>=<i>p</i><mathml id="653"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>-<i>p</i><sup> (<i>i</i>) </sup>. (7) </p>
                </div>
                <div class="p1">
                    <p id="654">图5显示了优先级偏差的绝对值|Δ<i>p</i><sup> (<i>i</i>) </sup>|的分布, 样本仍按照<i>p</i><sup> (<i>i</i>) </sup>由大到小排序.</p>
                </div>
                <div class="area_img" id="655">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_655.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 CartPole训练周期为180时|Δp (i) |的分布图" src="Detail/GetImg?filename=images/JFYZ201902004_655.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 CartPole训练周期为180时|Δ<i>p</i><sup> (<i>i</i>) </sup>|的分布图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_655.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Distribution of |Δ<i>p</i><sup> (<i>i</i>) </sup>| in CartPole training  when training episode is 180</p>

                </div>
                <div class="p1">
                    <p id="656">图4和图5表明, PER算法中使用基于<i>δ</i><sup> (<i>i</i>) </sup>的存储优先级进行主动采样存在2个问题:</p>
                </div>
                <div class="p1">
                    <p id="657">1) 约1/3样本的优先级被显著降低.如图4所示, <i>p</i><sup> (<i>i</i>) </sup>和<i>p</i><mathml id="658"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的差异主要集中在坐标系右侧约占全体样本1/3的区域.经统计, 按照<i>p</i><sup> (<i>i</i>) </sup>采样时, 该区域样本优先级之和占全体样本的比例为6.9%, 而按照<i>p</i><mathml id="659"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>采样时这一比例可以达到19.5%, 提高近3倍.因此, 按照存储优先级进行采样显著降低了约1/3的样本被采样的概率.</p>
                </div>
                <div class="p1">
                    <p id="660">2) <i>δ</i><sup> (<i>i</i>) </sup>的分布不能准确反映经验池的真实情况.其中, 当|<i>δ</i><sup> (<i>i</i>) </sup>|较大时, 样本被抽取的概率高, <i>τ</i><sup> (<i>i</i>) </sup>值较小, 期间Q网络变化不明显, 优先级偏差较小, 此时可以使用存储优先级作为真实优先级的近似估计;但当|<i>δ</i><sup> (<i>i</i>) </sup>|较小时, <i>τ</i><sup> (<i>i</i>) </sup>值较大, Q网络在<i>τ</i><sup> (<i>i</i>) </sup>个时间步内参数变化较大, 导致存储优先级偏离真实优先级的分布, 此时基于<i>δ</i><sup> (<i>i</i>) </sup>的存储优先级不能准确反映经验池的情况.</p>
                </div>
                <div class="p1">
                    <p id="661">对CartPole问题分别使用基于存储优先级<i>p</i><sup> (<i>i</i>) </sup>和真实优先级<i>p</i><mathml id="662"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的PER算法进行训练.图6对比了2种优先级下的训练曲线.其中, 横轴代表迭代时间步, 每个时间步智能体与环境交互一次, 纵轴代表智能体在一个周期内获得的累计奖励.</p>
                </div>
                <div class="area_img" id="663">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_663.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 使用p (i) 和p (i) real作为优先级的训练曲线对比" src="Detail/GetImg?filename=images/JFYZ201902004_663.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 使用<i>p</i><sup> (<i>i</i>) </sup>和<i>p</i><sup> (<i>i</i>) </sup><sub>real</sub>作为优先级的训练曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_663.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Training curve comparison using experience  replay with <i>p</i><sup> (<i>i</i>) </sup> and <i>p</i><sup> (<i>i</i>) </sup><sub>real</sub></p>

                </div>
                <div class="p1">
                    <p id="664">图6表明, 使用真实优先级<i>p</i><mathml id="665"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>进行主动采样的效果更好.具体体现为:</p>
                </div>
                <div class="p1">
                    <p id="666">1) 收敛性.使用存储优先级<i>p</i><sup> (<i>i</i>) </sup>和使用真实优先级<i>p</i><mathml id="667"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>进行采样都可以收敛到最大奖励值.</p>
                </div>
                <div class="p1">
                    <p id="668">2) 智能体与环境的交互次数.使用真实优先级<i>p</i><mathml id="669"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>采样达到最大奖励需约74 000个时间步;而使用存储优先级<i>p</i><sup> (<i>i</i>) </sup>则需约100 000个时间步, 且学习过程中波动明显.</p>
                </div>
                <div class="p1">
                    <p id="670">以上数据和分析表明, 使用真实优先级<i>p</i><mathml id="671"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>进行主动采样可以提高样本的利用效率, 显著减少智能体与环境的交互次数.</p>
                </div>
                <div class="p1">
                    <p id="672">虽然<i>p</i><mathml id="673"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>是一种更为理想的优先级分布, 但仅在小规模问题 (如CartPole) 中可以使用.在大规模强化学习中, 计算<i>p</i><mathml id="674"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的分布需在每个时间步将经验池的全部样本送入Q网络和目标Q网络, 由于迭代时间步较多且经验池容量较大, 计算会造成大量的时间成本和存储成本.本文提出一种TD-error自适应校正方法来估计真实优先级和存储优先级偏差的分布, 进而得到真实优先级的估计<mathml id="675"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>.本文并不直接计算真实优先级<i>p</i><mathml id="676"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 能够以极小的代价获得跟随Q网络更新的采样优先级.</p>
                </div>
                <h4 class="anchor-tag" id="677" name="677"><b>2.2</b><b>ATDC-PER模型</b></h4>
                <div class="p1">
                    <p id="678">本文提出的基于TD-error自适应校正的主动采样算法 (ATDC-PER) 包括“偏差模型”和“优先级校正”2部分, 使用分段更新策略将二者结合.偏差模型是经验池中样本与真实优先级和存储优先级的偏差之间的回归模型.在时刻<i>t</i>提取当前经验池的样本特征和优先级偏差求解偏差模型的参数, 作为时刻<i>t</i>的偏差模型, 在时刻<i>t</i>～ (<i>t</i>+<i>D</i>) 内均使用时刻<i>t</i>的偏差模型对当前经验池样本的存储优先级进行校正.校正后得到样本真实优先级的估计<mathml id="679"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 在Q网络训练时使用<mathml id="680"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>选择样本.在时刻<i>t</i>+<i>D</i>再一次使用该时刻的经验池求解偏差模型的参数, 偏差模型被更新, 在 (<i>t</i>+<i>D</i>) ～ (<i>t</i>+2<i>D</i>) 时间步内使用时刻<i>t</i>+<i>D</i>求解的偏差模型进行优先级校正.以此类推, 偏差模型将在时刻<i>t</i>+3<i>D</i>, <i>t</i>+4<i>D</i>, <i>t</i>+5<i>D</i>, …更新, 在所有时间步均根据最新的偏差模型进行优先级校正, <i>D</i>为偏差模型的更新周期.ATDC-PER算法在存储优先级的基础上估计真实优先级, 既使得优先级可以跟随Q网络更新, 又避免了直接计算真实优先级所需的巨大开销.</p>
                </div>
                <div class="p1">
                    <p id="681">真实优先级和存储优先级的偏差分布具有稳定性, 所以ATDC-PER方法对偏差模型更新周期<i>D</i>的选取是不敏感的.真实优先级与存储优先级的不同之处在于计算时使用的Q网络参数不同, 真实优先级使用时刻<i>t</i>的Q网络参数, 而存储优先级使用<i>t</i>-<i>τ</i><sup> (<i>i</i>) </sup>时刻的Q网络参数, 二者的区别在于<i>τ</i><sup> (<i>i</i>) </sup>的分布.如图3所示, <i>τ</i><sup> (<i>i</i>) </sup>在经验池中呈现长尾分布, 最小值为1, 最大值为经验池样本总量.在训练中经验池填充满后容量不再发生变化, <i>τ</i><sup> (<i>i</i>) </sup>分布具有稳定性, 因此偏差模型不需要频繁更新.</p>
                </div>
                <div class="p1">
                    <p id="682">ATDC-PER算法在训练过程中偏差模型分段更新的示意图如图7所示, 横轴代表训练时间步, 纵轴代表智能体在周期内的累计奖励.偏差模型更新后的<i>D</i>个时间步内均使用该模型进行优先级校正, 可以极大地减少时间开销.</p>
                </div>
                <div class="area_img" id="683">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_683.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 ATDC-PER算法的分段更新策略示意图" src="Detail/GetImg?filename=images/JFYZ201902004_683.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 ATDC-PER算法的分段更新策略示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_683.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Interval update policy in ATDC-PER algorithm</p>

                </div>
                <h4 class="anchor-tag" id="684" name="684">2.2.1 偏差模型</h4>
                <div class="p1">
                    <p id="685">从经验池中提取的样本特征是偏差模型的输入, 真实优先级和存储优先级之间的优先级偏差作为样本标签, 是偏差模型的输出.使用线性回归模型对样本特征与样本标签之间的关系进行建模, 通过最小化平方损失函数来求解模型参数<b><i>w</i></b>.</p>
                </div>
                <div class="p1">
                    <p id="686">1) 样本特征.样本特征是针对单个样本而言的.特征在训练中的经验池中保存且与优先级偏差相关.样本特征包括:</p>
                </div>
                <div class="p1">
                    <p id="687">① 样本在经验池中的存储优先级.由图4可知, 总体上, 样本的存储优先级<i>p</i><sup> (<i>i</i>) </sup>越低, 优先级偏差越大.为了使模型在不同环境和不同训练阶段具有通用性, 将存储优先级的定义在式 (5) 的基础上进行改造, 用经验池中样本优先级的最大值进行规约, 记为<mathml id="688"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, 则</p>
                </div>
                <div class="p1">
                    <p id="689"><mathml id="690"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mfrac><mrow><mi>p</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>k</mi></munder><mspace width="0.25em" /><mi>p</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac></mrow></math></mathml>. (8) </p>
                </div>
                <div class="p1">
                    <p id="691">② 样本的回放周期.样本的回放周期用样本距离上次参与训练的时间步<i>τ</i><sup> (<i>i</i>) </sup>来表示, <i>τ</i><sup> (<i>i</i>) </sup>在经验池中的分布形式如图3所示.训练中样本不断填充经验池, 新样本和刚刚参与训练的样本<i>τ</i><sup> (<i>i</i>) </sup>=1, 所有样本的<i>τ</i><sup> (<i>i</i>) </sup>不超过经验池样本总量, 因此所有样本<i>τ</i><sup> (<i>i</i>) </sup>的取值范围均在1到经验池样本总量之间.类似地, 将<i>τ</i><sup> (<i>i</i>) </sup>用全部样本的最大值作规约, 定义为回放周期, 记为<mathml id="692"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>τ</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, 则</p>
                </div>
                <div class="p1">
                    <p id="693"><mathml id="694"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>τ</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mfrac><mrow><mi>τ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>k</mi></munder><mspace width="0.25em" /><mi>τ</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac></mrow></math></mathml>. (9) </p>
                </div>
                <div class="p1">
                    <p id="695">2) 样本标签.样本标签为样本的优先级偏差, 是真实优先级和存储优先级之差.其中真实优先级的计算基于TD-error值<i>δ</i><mathml id="696"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 按式 (4) 在当前时间步将经验池中所有样本送入Q网络和目标Q网络进行计算.将优先级偏差重新记为Δ<mathml id="697"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, 则</p>
                </div>
                <div class="p1">
                    <p id="698"><mathml id="699"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>-</mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="700">其中, <mathml id="701"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>是存储优先级, 由式 (8) 计算.<mathml id="702"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><mathml id="703"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>是真实优先级, 其值是在式 (6) 的<i>p</i><mathml id="704"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的基础上规约得到的:</p>
                </div>
                <div class="p1">
                    <p id="705"><mathml id="706"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>p</mi><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>k</mi></munder><mspace width="0.25em" /><mi>p</mi><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></mfrac></mrow></math></mathml>. (11) </p>
                </div>
                <div class="p1">
                    <p id="707">Atari游戏Pong<sup>①</sup>和Breakout<sup>②</sup>在训练时间步为10<sup>5</sup>时经验池中所有样本的样本特征<mathml id="708"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, <mathml id="709"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>τ</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>和样本标签Δ<mathml id="710"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>的分布如图8所示, 样本按照存储优先级排序, 具体的设置将在实验部分介绍.图8表明, 样本的存储优先级<mathml id="711"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>和回放周期<mathml id="712"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>τ</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>都与优先级偏差Δ<mathml id="713"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>密切相关.</p>
                </div>
                <div class="area_img" id="714">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_714.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 Pong和Breakout在t=105时样本特征p˜ (i) , τ˜ (i) 和样本标签Δp˜ (i) 的分布" src="Detail/GetImg?filename=images/JFYZ201902004_714.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 Pong和Breakout在<i>t</i>=10<sup>5</sup>时样本特征<mathml id="715"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, <mathml id="716"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>τ</mtext><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>和样本标签Δ<mathml id="717"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_714.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Distribution of feature <mathml id="718"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, <mathml id="719"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>τ</mtext><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup> and label Δ<mathml id="720"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup> in Pong and Breakout when <i>t</i>=10<sup>5</sup></p>

                </div>
                <div class="p1">
                    <p id="721">3) 偏差模型参数求解.使用回归模型来对样本特征和样本标签之间的关系进行建模.由于样本特征与样本标签之间的关系明显, 因此使用线性回归模型.同时, 线性回归模型在当前的输入输出条件下存在闭式解, 不需要使用梯度法迭代求解, 时间开销很小.</p>
                </div>
                <div class="p1">
                    <p id="722">在特征<mathml id="723"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>和<mathml id="724"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>τ</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>的基础上提取<i>K</i>阶多项式特征作为偏差模型的输入.以<i>K</i>=2为例, 特征向量<b><i>x</i></b><sup> (<i>i</i>) </sup>为6维, 表示为</p>
                </div>
                <div class="p1">
                    <p id="725"><mathml id="726"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mover accent="true"><mi>τ</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mo stretchy="false"> (</mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mo stretchy="false"> (</mo><mover accent="true"><mi>τ</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mover accent="true"><mi>τ</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>. (12) </p>
                </div>
                <div class="p1">
                    <p id="727">权重向量<b><i>w</i></b>的维度与特征<b><i>x</i></b><sup> (<i>i</i>) </sup>的维度相同.根据线性回归模型的定义, 假设函数<i>h</i><sub><b><i>w</i></b></sub> (<b><i>x</i></b><sup> (<i>i</i>) </sup>) 表示为</p>
                </div>
                <div class="p1">
                    <p id="728"><mathml id="729"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msub><mrow></mrow><mi mathvariant="bold-italic">w</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>5</mn></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow></math></mathml>. (13) </p>
                </div>
                <div class="p1">
                    <p id="730">设当前经验池中样本数为<i>m</i>, 则全体样本组成的特征矩阵<b><i>X</i></b>∈R<sup><i>m</i>×6</sup>, 表示为</p>
                </div>
                <div class="p1">
                    <p id="731" class="code-formula">
                        <mathml id="731"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">X</mi><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="732">. (14) </p>
                </div>
                <div class="p1">
                    <p id="733">样本标签为优先级偏差Δ<mathml id="734"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, 经验池中所有样本组成的标签向量表示为</p>
                </div>
                <div class="p1">
                    <p id="735"><mathml id="736"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mo stretchy="false"> (</mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>. (15) </p>
                </div>
                <div class="p1">
                    <p id="737">损失函数为平方误差损失, 记为<i>J</i> (<b><i>w</i></b>) .经验池中所有样本的平均损失表示为</p>
                </div>
                <div class="p1">
                    <p id="738"><mathml id="739"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo></mrow></math></mathml>. (16) </p>
                </div>
                <div class="p1">
                    <p id="740">参数向量<b><i>w</i></b>为使得损失<i>J</i> (<b><i>w</i></b>) 取最小值的解.由于特征矩阵<b><i>X</i></b>的元素均大于0, <b><i>w</i></b>存在闭式解.经推导可得, 最小化损失函数<i>J</i> (<b><i>w</i></b>) 的<b><i>w</i></b>的解为</p>
                </div>
                <div class="p1">
                    <p id="741"><b><i>w</i></b>= (<b><i>X</i></b><sup>T</sup><b><i>X</i></b>) <sup>-1</sup><b><i>X</i></b><sup>T</sup><b><i>y</i></b>. (17) </p>
                </div>
                <div class="p1">
                    <p id="742">偏差模型表征了算法在训练过程中的存储优先级、回放周期与优先级偏差之间的关系.偏差模型建立后, 只需利用最新的模型参数就可预测当前经验池中每个样本的优先级偏差, 进而得到真实优先级的估计<mathml id="743"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>.</mo></mrow></math></mathml></p>
                </div>
                <h4 class="anchor-tag" id="744" name="744">2.2.2 优先级校正</h4>
                <div class="p1">
                    <p id="745">优先级校正是根据偏差模型参数<b><i>w</i></b>估计当前经验池样本真实优先级的过程.输入当前时间步<i>t</i>经验池中样本组成的特征矩阵<b><i>X</i></b><sub><i>t</i></sub>, 预测每个样本的优先级偏差Δ<mathml id="746"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><mathml id="747"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 进而得到真实优先级的估计.主要过程分为样本特征提取和优先级预测.</p>
                </div>
                <div class="p1">
                    <p id="748">对时间步<i>t</i>的经验池样本提取特征, 特征仍表示为存储优先级<mathml id="749"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><mathml id="750"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>和回放周期<mathml id="751"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>τ</mi><mo>˜</mo></mover></math></mathml><mathml id="752"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 并在此基础上进行<i>K</i>阶多项式特征提取, 如式 (12) 所示, 样本<i>e</i><sup> (<i>i</i>) </sup>的特征向量记为<b><i>x</i></b><mathml id="753"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>.经验池中全部样本组成特征矩阵<b><i>X</i></b><sup>T</sup><sub><i>t</i></sub> = ( (<b><i>x</i></b><mathml id="754"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) <sup>T</sup>, (<b><i>x</i></b><mathml id="755"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) <sup>T</sup>, …, (<b><i>x</i></b><mathml id="756"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>) <sup>T</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="757">根据当前最新的偏差模型参数<b><i>w</i></b>和特征矩阵<b><i>X</i></b><sub><i>t</i></sub>可以预测时间步<i>t</i>所有样本的优先级偏差<mathml id="758"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 如式 (18) 所示:</p>
                </div>
                <div class="p1">
                    <p id="759"><mathml id="760"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">w</mi></mrow></math></mathml>. (18) </p>
                </div>
                <div class="p1">
                    <p id="761">在此基础上, 将<mathml id="762"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>与存储优先级求和即可得到样本真实优先级的估计<mathml id="763"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 如式 (19) 所示:</p>
                </div>
                <div class="p1">
                    <p id="764"><mathml id="765"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>. (19) </p>
                </div>
                <div class="area_img" id="766">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_766.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 Breakout训练中t=107时p˜ (i) , p˜ (i) real和p^real (i) 分布" src="Detail/GetImg?filename=images/JFYZ201902004_766.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 <i>Breakout</i>训练中t=10<sup>7</sup>时<mathml id="767"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sup> (i) </sup>, <mathml id="768"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover></math></mathml><sup> (i) </sup><sub><i>real</i></sub>和<mathml id="769"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>p</mtext><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>a</mi><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mtext>i</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_766.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>
                                <p class="img_note"><mathml id="770"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>i</mi><mi>g</mi><mo>.</mo><mspace width="0.25em" /><mn>9</mn><mtext> </mtext><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>b</mi><mi>u</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mspace width="0.25em" /><mi>o</mi><mi>f</mi><mspace width="0.25em" /><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>i</mtext><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mover accent="true"><mtext>p</mtext><mo>˜</mo></mover><msubsup><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>a</mi><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mtext>i</mtext><mo stretchy="false">) </mo></mrow></msubsup><mspace width="0.25em" /><mi>a</mi><mi>n</mi><mi>d</mi><mspace width="0.25em" /><mover accent="true"><mtext>p</mtext><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>a</mi><mi>l</mi></mrow><mrow><mo stretchy="false"> (</mo><mtext>i</mtext><mo stretchy="false">) </mo></mrow></msubsup><mspace width="0.25em" /><mi>a</mi><mi>t</mi><mspace width="0.25em" /><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mspace width="0.25em" /><mtext>t</mtext><mo>=</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mn>7</mn></msup><mspace width="0.25em" /></mrow></math></mathml><i>in Breakout training</i></p>

                </div>
                <div class="p1">
                    <p id="771">图9显示了<i>Breakout</i>在训练过程中t=10<sup>7</sup>时间步的存储优先级、真实优先级和真实优先级的估计值<mathml id="772"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的分布.图9表明, 相比于存储优先级, 校正后的优先级<mathml id="773"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>更加逼近于真实优先级的分布.</p>
                </div>
                <h4 class="anchor-tag" id="774" name="774"><b>2.3</b><b>算法描述和复杂度分析</b></h4>
                <h4 class="anchor-tag" id="775" name="775">2.3.1 算法描述</h4>
                <div class="p1">
                    <p id="776">ATDC-PER的算法流程如算法1所示.</p>
                </div>
                <div class="p1">
                    <p id="777"><b>算法1</b>. ATDC-PER算法.</p>
                </div>
                <div class="p1">
                    <p id="778">输入:偏差模型特征阶数<i>K</i>、偏差模型更新周期<i>D</i>、奖励折扣因子<i>γ</i>、样本批量大小<i>k</i>、学习率<i>η</i>、经验池容量<i>N</i>、偏差模型参数<i>α</i>、重要性权重<i>β</i>、训练终止时间步<i>T</i>、目标Q网络更新频率<i>L</i>;</p>
                </div>
                <div class="p1">
                    <p id="779">输出:Q网络参数;</p>
                </div>
                <div class="p1">
                    <p id="780">初始化:经验池为空, 批量梯度<i>Δ</i>=<b>0</b>, 初始化Q网络和目标Q网络参数为<i>θ</i>和<i>θ</i><sup>-</sup>.</p>
                </div>
                <div class="p1">
                    <p id="781">① 根据初始状态<i>S</i><sub>0</sub>, 以<i>ε</i>-贪心选择动作<i>A</i><sub>0</sub>～<i>π</i><sub><i>θ</i></sub> (<i>S</i><sub>0</sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="782">② for <i>t</i>=1 to <i>T</i> do</p>
                </div>
                <div class="p1">
                    <p id="783">③ 观察到状态<i>S</i><sub><i>t</i></sub>并获得奖励<i>R</i><sub><i>t</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="784">④ 将经验序列 (<i>S</i><sub><i>t</i>-1</sub>, <i>A</i><sub><i>t</i>-1</sub>, <i>R</i><sub><i>t</i></sub>, <i>S</i><sub><i>t</i></sub>) 存储到经验池中, 并赋予当前经验池中最大的优先级;</p>
                </div>
                <div class="p1">
                    <p id="785">⑤ if <i>t</i>%<i>D</i>==0</p>
                </div>
                <div class="p1">
                    <p id="786">⑥ 样本特征提取, 构造如式 (14) 所示的特征矩阵<b><i>X</i></b>; /*2.2.1节*/</p>
                </div>
                <div class="p1">
                    <p id="787">⑦ 样本标签提取, 根据式 (10) 计算Δ<mathml id="788"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sup> (<i>i</i>) </sup>, 得到如式 (15) 所示的样本标签向量<b><i>y</i></b>; /*2.2.1节*/</p>
                </div>
                <div class="p1">
                    <p id="789">⑧ 偏差模型求解, 将偏差模型参数更新为<b><i>w</i></b>= (<b><i>X</i></b><sup>T</sup><b><i>X</i></b>) <sup>-1</sup><b><i>X</i></b><sup>T</sup><b><i>y</i></b>; /*2.2.1节*/</p>
                </div>
                <div class="p1">
                    <p id="790">⑨ end if</p>
                </div>
                <div class="p1">
                    <p id="791">⑩ 构造样本特征<b><i>X</i></b><sub><i>t</i></sub>, 预测经验池样本优先级偏差;<mathml id="792"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>t</mi></msub><mi mathvariant="bold-italic">w</mi></mrow></math></mathml>; /*2.2.2节*/</p>
                </div>
                <div class="p1">
                    <p id="793"> (11) 得到真实优先级的估计<mathml id="794"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mtext>Δ</mtext><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>+</mo><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>; /*2.2.2节*/</p>
                </div>
                <div class="p1">
                    <p id="795"> (12) 根据校正后的优先级提取<i>k</i>个样本<mathml id="796"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>j</mi><mo>~</mo><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo></mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>/</mo></mrow></math></mathml><mathml id="797"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mrow></mrow></mstyle></mrow></math></mathml><mathml id="798"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="799"> (13) for <i>j</i>=1 to <i>k</i></p>
                </div>
                <div class="p1">
                    <p id="800"> (14) 计算重要性权重<mathml id="801"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>Ν</mi><mo>×</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mi>β</mi></mrow></msup><mo>/</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>k</mi></munder><mspace width="0.25em" /><mtext>ϕ</mtext><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>;</p>
                </div>
                <div class="p1">
                    <p id="802"> (15) 计算TD-error</p>
                </div>
                <div class="p1">
                    <p id="803" class="code-formula">
                        <mathml id="803"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>δ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>R</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false"> (</mo><mi>S</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>a</mi></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>a</mi><mo>;</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">) </mo><mo>;</mo><mi mathvariant="bold-italic">θ</mi><msup><mrow></mrow><mo>-</mo></msup><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="804"><i>Q</i> (<i>S</i><sub><i>j</i>-1</sub>, <i>A</i><sub><i>j</i>-1</sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="805"> (16) 根据TD-error更新经验池中样本的优先级;</p>
                </div>
                <div class="p1">
                    <p id="806"> (17) 累计批量的梯度<i>Δ</i>=<i>Δ</i>+ϕ<sub><i>j</i></sub>×<i>δ</i><sub><i>j</i></sub>×</p>
                </div>
                <div class="p1">
                    <p id="807"><image href="images/JFYZ201902004_808.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>θ</i></sub><i>Q</i> (<i>S</i><sub><i>j</i>-1</sub>, <i>A</i><sub><i>j</i>-1</sub>, <i>θ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="809"> (18) end for</p>
                </div>
                <div class="p1">
                    <p id="810"> (19) 更新Q网络权重<i>θ</i>=<i>θ</i>+<i>η</i><i>Δ</i>, 重置<i>Δ</i>=<b>0</b>;</p>
                </div>
                <div class="p1">
                    <p id="811"> (20) 如果<i>t</i>%<i>L</i>==0, 更新目标Q网络参数<i>θ</i><sup>-</sup>=<i>θ</i>;</p>
                </div>
                <div class="p1">
                    <p id="812"> (21) 以<i>ε</i>-贪心选择下一个动作<i>A</i><sub><i>t</i></sub>～<i>π</i><sub><i>θ</i></sub> (<i>S</i><sub><i>t</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="813"> (22) end for</p>
                </div>
                <div class="p1">
                    <p id="814">其中行⑤～⑨是偏差模型更新的过程, 需在<i>t</i>, <i>t</i>+<i>D</i>, <i>t</i>+2<i>D</i>, …等时间步计算经验池的样本特征和优先级偏差进行训练, 求解模型参数<b><i>w</i></b>.行⑩ (11) 是优先级校正的过程, 通过偏差模型得到样本真实优先级的估计<mathml id="815"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>.行 (12) 在优先级的基础上加入随机性, 样本被采样的概率与优先级成正比, 同时所有样本都有机会被采样.行 (13) ～ (18) 计算损失函数的梯度, 计算梯度时需在TD-error的基础上乘以重要性权重 (importance sampling weight) 系数<sup><a class="sup">[42]</a></sup>, 原因是按照优先级<mathml id="816"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>采样与均匀采样相比会给梯度的计算带来误差, 因此使用采样概率的比值ϕ进行补偿<sup><a class="sup">[11]</a></sup>, 并使用全部样本的ϕ的最大值进行规约, 如式 (20) 所示.</p>
                </div>
                <div class="p1">
                    <p id="817"><mathml id="818"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>Ν</mi><mo>×</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mi>β</mi></mrow></msup><mo>/</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>k</mi></munder><mspace width="0.25em" /><mtext>ϕ</mtext><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>, (20) </p>
                </div>
                <div class="p1">
                    <p id="819">其中, <i>k</i>=1, 2, …, <i>N</i>, <i>N</i>为经验池容量, <i>β</i>是一个常数, <i>P</i> (<i>j</i>) 为样本<i>e</i><sup> (<i>j</i>) </sup>的采样优先级在全体样本中所占的比例, 如式 (21) 所示.</p>
                </div>
                <div class="p1">
                    <p id="820"><mathml id="821"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mi>α</mi></msup></mrow></math></mathml><mathml id="822"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mo stretchy="false"> (</mo></mstyle><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mi>α</mi></msup></mrow></math></mathml>. (21) </p>
                </div>
                <h4 class="anchor-tag" id="823" name="823">2.3.2 复杂度分析</h4>
                <div class="p1">
                    <p id="824">ATDC-PER算法包括偏差模型和优先级校正两部分.根据分段更新策略, 在执行<i>D</i>次优先级校正的同时会执行一次偏差模型更新.由于偏差模型更新的时间复杂度较高, 而优先级校正仅执行矩阵乘法和加法, 时间复杂度低, 所以分段更新策略可以使复杂度高的部分以极低的频率执行, 从而减少时间开销.</p>
                </div>
                <div class="p1">
                    <p id="825">1) 偏差模型的复杂度分析.偏差模型更新包括样本特征提取、样本标签提取和模型求解3个阶段, 分析如下:</p>
                </div>
                <div class="p1">
                    <p id="826">① 样本特征提取.原始的样本特征维度为2, 提取<i>K</i>阶多项式后的特征向量<mathml id="827"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mi>Κ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>Κ</mi><mo>+</mo><mn>2</mn></mrow><mn>2</mn></mfrac><mo>×</mo><mn>1</mn></mrow></msup></mrow></math></mathml>, 特征矩阵<mathml id="828"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">X</mi><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mrow><mi>m</mi><mo>×</mo><mfrac><mrow><mi>Κ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>Κ</mi><mo>+</mo><mn>2</mn></mrow><mn>2</mn></mfrac></mrow></msup></mrow></math></mathml>.其中, <i>m</i>的值不超过经验池总容量, 且在经验池填充满之后不再变化, 是一个常数.因此特征提取的时间复杂度和空间复杂度均为<i>O</i> (<i>K</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="829">② 样本标签提取.得到样本的优先级偏差需计算样本真实的TD-error值<i>δ</i><mathml id="830"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 需将经验池的全部样本送入Q网络和目标Q网络进行前向传播.由于经验池大小为常数, 因此样本组成的输入矩阵维度固定, 计算的时间复杂度和空间复杂度都为<i>Ο</i> (1) .</p>
                </div>
                <div class="p1">
                    <p id="831">③ 偏差模型求解.模型求解阶段的时间消耗集中在式 (17) 中对参数向量<b><i>w</i></b>的求解上.<mathml id="832"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo>∈</mo><mrow><mtext>R</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mi>Κ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>Κ</mi><mo>+</mo><mn>2</mn></mrow><mn>2</mn></mfrac><mo>×</mo><mfrac><mrow><mi>Κ</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>Κ</mi><mo>+</mo><mn>2</mn></mrow><mn>2</mn></mfrac></mrow></msup></mrow></mrow></math></mathml>, 在此基础上求逆的时间复杂度为<i>Ο</i> (<i>K</i><sup>6</sup>) , 空间复杂度为<i>Ο</i> (<i>K</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="833">2) 优先级校正的复杂度分析.优先级校正阶段在提取样本特征<b><i>X</i></b><sub><i>t</i></sub>后与参数向量<b><i>w</i></b>执行矩阵乘法, 时间复杂度为<i>Ο</i> (<i>K</i><sup>4</sup>) , 空间复杂度为<i>Ο</i> (<i>K</i><sup>2</sup>) .</p>
                </div>
                <div class="p1">
                    <p id="834">由于ATDC-EPR算法中间隔<i>D</i>个时间步偏差模型才更新一次, <i>D</i>的取值较大, 其余时间步都仅执行优先级校正的操作, 因此总体时间复杂度为<i>Ο</i> (<i>K</i><sup>4</sup>) , 空间复杂度为<i>Ο</i> (<i>K</i><sup>2</sup>) .另外, 一般<i>K</i>=2时偏差模型就可以很好地估计优先级偏差, 此时求解参数<b><i>w</i></b>所需的逆矩阵 (<b><i>X</i></b><sup>T</sup><b><i>X</i></b>) <sup>-1</sup>∈R<sup>6×6</sup>, 规模较小, 可以快速求解.</p>
                </div>
                <div class="p1">
                    <p id="835">经过复杂度分析, 本文提出的ATDC-PER算法以极小的时间和空间代价就可以在Q网络每次迭代中校正经验池中样本的存储优先级, 使用校正后的优先级选择样本.</p>
                </div>
                <h3 id="836" name="836" class="anchor-tag"><b>3</b><b>实验结果与分析</b></h3>
                <div class="p1">
                    <p id="837">本节首先介绍实验平台和环境, 随后介绍ATDC-PER算法的参数选择, 通过对比实验说明参数对性能的影响, 最后从智能体的学习速度和最优策略的质量2方面对实验结果进行分析.</p>
                </div>
                <h4 class="anchor-tag" id="838" name="838"><b>3.1</b><b>实验平台描述</b></h4>
                <div class="p1">
                    <p id="839">实验使用基于Atari 2600游戏平台Arcade Learning Environment (ALE) <sup><a class="sup">[43]</a></sup>的OpenAI集成环境<sup><a class="sup">[44]</a></sup>.ALE平台包含数十个视频游戏, 并对外提供接口.ALE平台的挑战在于游戏种类和数量丰富, 包含益智类、体育类、策略类、射击类、搏斗类等多种类型的游戏, 需多个游戏中使用同一算法, 在超参数固定的情况下, 仅以原始屏幕图像作为输入进行学习和训练.</p>
                </div>
                <div class="p1">
                    <p id="840">本文选择ALE平台中的19个游戏进行实验, 游戏的简要介绍列于表1, 各游戏界面如图10所示.可以看出, 游戏类型多样, 游戏的界面风格不同, 在训练中Q网络的输入不同.同时, 每个游戏中智能体需完成的任务不同, 即最终学习的策略也不同.</p>
                </div>
                <div class="area_img" id="841">
                    <p class="img_tit"><b>表1</b><b>19个Atari游戏的简要介绍</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1</b><b>A Brief Introduction to 19 Atari Games</b></p>
                    <p class="img_note"></p>
                    <table id="841" border="1"><tr><td><br />Game</td><td>Type</td><td>Action Number</td><td>Brief Introduction</td></tr><tr><td><br />Alien</td><td>puzzle</td><td>18</td><td>Eat beans and extra bonus while avoiding enemies to chase</td></tr><tr><td><br />Amidar</td><td>strategy</td><td>10</td><td>The agent passes all tracks without touching the enemy in the maze</td></tr><tr><td><br />BankHeist</td><td>strategy</td><td>18</td><td>Reaches the prey to gain points, and the prey becomes an enemy chase</td></tr><tr><td><br />BeamRider</td><td>shooter</td><td>9</td><td>The agent launches artillery shells and hits each other</td></tr><tr><td><br />Boxing</td><td>wrestling</td><td>18</td><td>The agent avoid hitting the opponent while fist hitting the opponent</td></tr><tr><td><br />Breakout</td><td>strategy</td><td>4</td><td>The agent receives and ejects the ball, destroying the upper box in the shortest time</td></tr><tr><td><br />Centipede</td><td>strategy</td><td>18</td><td>The agent avoids the rapid emergence of bullets while hitting the top of the prey</td></tr><tr><td><br />ChopperCommand</td><td>shooter</td><td>18</td><td>Driving helicopters to shoot down opponent helicopters while avoiding ground</td></tr><tr><td><br />CrazyClimber</td><td>puzzle</td><td>9</td><td>Escape the enemy’s blows, avoid obstacles and climb high-rise buildings</td></tr><tr><td><br />DoubleDunk</td><td>sports</td><td>19</td><td>2 vs 2 basketball game, one side with the ball and the other defensive</td></tr><tr><td><br />Enduro</td><td>driving</td><td>9</td><td>The agent need to avoid collisions while driving a car on the highway</td></tr><tr><td><br />NameThisGame</td><td>shooter</td><td>6</td><td>Shooting fast-moving sharks in the sea to avoid enemy artillery shells</td></tr><tr><td><br />Pong</td><td>sports</td><td>6</td><td>Agent prevents the opponent from receiving the ball in the table tennis game</td></tr><tr><td><br />PrivateEye</td><td>strategy</td><td>18</td><td>The agent jumps from the ground to meet the enemy’s score in the changing scene</td></tr><tr><td><br />Riverraid</td><td>shooter</td><td>18</td><td>The agent drives aircraft to hit different targets and score different goals.</td></tr><tr><td><br />RoadRunner</td><td>strategy</td><td>18</td><td>The agent walks on fast moving roads, avoids obstacles and destroys enemies.</td></tr><tr><td><br />Robotank</td><td>shooter</td><td>18</td><td>Driving tanks move their own field of vision, and score quickly</td></tr><tr><td><br />TimePilot</td><td>shooter</td><td>10</td><td>In air combat, we need to drive planes, avoid obstacles and strike enemy planes</td></tr><tr><td><br />UpNDown</td><td>action</td><td>6</td><td>The agent hits the rear vehicle and reaches the destination as fast as possible</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="842">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_842.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 19个Atari游戏的状态表示" src="Detail/GetImg?filename=images/JFYZ201902004_842.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 19个Atari游戏的状态表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_842.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 State representations of 19 Atari games</p>

                </div>
                <div class="p1">
                    <p id="843">本文使用的计算平台为戴尔PowerEdge T630塔式工作站, 工作站配置2颗Intel Xeon E5-2609 CPU、4块NVIDIA GTX-1080Ti显卡 (GPU) 和64 GB内存, 每个游戏使用单独的一块显卡进行训练.卷积神经网络基于Tensorflow<sup><a class="sup">[45]</a></sup>开源库实现, 本文核心代码、实验结果和测试视频均可下载<sup>①</sup>.</p>
                </div>
                <h4 class="anchor-tag" id="844" name="844"><b>3.2</b><b>ATDC-PER参数选择</b></h4>
                <div class="p1">
                    <p id="845">ATDC-PER算法在全部游戏中使用相同的超参数集合, 以验证模型的通用性.ATDC-PER算法在PER算法的基础上增加了2个超参数, 分别是偏差模型更新周期<i>D</i>和偏差模型特征阶数<i>K</i>, 其余参数的设置与PER算法的设置相同.状态表示为原始图像预处理并叠加4帧组成的84×84×4的矩阵;Q网络为3层卷积神经网络, 输出每个动作的值函数, 目标Q网络的更新频率设为40 000;鉴于不同游戏的分值计算方式不同, 使用符号函数对奖励进行规约;为了使训练更加稳定, 对回传的梯度进行限制, 使其不超过10.使用基于平方误差损失的Adam优化器进行训练, 批量大小<i>k</i>=32, 学习率设为10<sup>-4</sup>, 折扣因子<i>γ</i>=0.99;探索因子在前400万帧从1.0线性递减至0.1, 在随后的时间步内以之前速度的1/10线性递减至0.01后保持不变;经验池容量设为10<sup>6</sup>;优先级计算中使用的参数<i>α</i>=0.6;重要性权重计算中使用的参数<i>β</i>初始值设为0.4, 在训练中线性增加至1.0.</p>
                </div>
                <h4 class="anchor-tag" id="846" name="846">3.2.1 偏差模型更新周期</h4>
                <div class="p1">
                    <p id="847">通过在Alien, Breakout, Pong的训练过程中使用不同的偏差模型更新周期<i>D</i>来测试参数<i>D</i>对Q网络学习的影响.在训练过程中的时刻<i>t</i>训练偏差模型得偏差模型参数<b><i>w</i></b><sub><i>t</i></sub>, 使用不同的参数<i>D</i>, 测试模型<b><i>w</i></b><sub><i>t</i></sub>在位于时刻<i>t</i>+<i>D</i>的经验池样本的平均损失, 损失的计算如式 (16) 所示.不同的偏差模型更新周期<i>D</i>下的平均损失对比列如表2所示:</p>
                </div>
                <div class="area_img" id="848">
                    <p class="img_tit"><b>表2</b><b>比较不同<i>D</i>值下时刻<i>t</i>训练的偏差模型在时刻<i>t</i>+<i>D</i>测试的平均损失</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2</b><b>Average Loss of Bias Model Trained in <i>t</i> Steps and Tested in <i>t</i>+<i>D</i> Steps Under Different <i>D</i> Values</b></p>
                    <p class="img_note"></p>
                    <table id="848" border="1"><tr><td rowspan="2"><br />10<sup>5</sup>×<i>t</i></td><td colspan="5"><br />Alien, 10<sup>-5</sup>×<i>D</i></td><td colspan="5">Breakout, 10<sup>-5</sup>×<i>D</i></td><td colspan="5">Pong, 10<sup>-5</sup>×D</td></tr><tr><td><br />0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td></tr><tr><td><br />15</td><td>0.003 3</td><td>0.003 1</td><td>0.003 1</td><td>0.002 8</td><td>0.003 2</td><td>0.001 2</td><td>0.001 1</td><td>0.001 6</td><td>0.001 6</td><td>0.003 3</td><td>0.004 2</td><td>0.004 2</td><td>0.004 4</td><td>0.005 0</td><td>0.004 3</td></tr><tr><td><br />20</td><td>0.002 8</td><td>0.003 8</td><td>0.005 5</td><td>0.010 6</td><td>0.004 8</td><td>0.002 9</td><td>0.003 7</td><td>0.003 8</td><td>0.003 3</td><td>0.008 5</td><td>0.004 2</td><td>0.004 4</td><td>0.005 2</td><td>0.004 4</td><td>0.004 8</td></tr><tr><td><br />25</td><td>0.002 6</td><td>0.002 1</td><td>0.002 1</td><td>0.002 6</td><td>0.004 5</td><td>0.000 9</td><td>0.000 5</td><td>0.001 1</td><td>0.002 1</td><td>0.0017</td><td>0.004 0</td><td>0.004 2</td><td>0.004 5</td><td>0.004 1</td><td>0.004 2</td></tr><tr><td><br />30</td><td>0.001 5</td><td>0.001 7</td><td>0.001 6</td><td>0.001 4</td><td>0.003 9</td><td>0.000 4</td><td>0.000 4</td><td>0.000 7</td><td>0.001 1</td><td>0.001 1</td><td>0.003 0</td><td>0.003 7</td><td>0.003 2</td><td>0.002 8</td><td>0.003 6</td></tr><tr><td><br />35</td><td>0.003 2</td><td>0.018 2</td><td>0.103 3</td><td>0.024 9</td><td>0.005 1</td><td>0.000 7</td><td>0.000 9</td><td>0.000 9</td><td>0.000 8</td><td>0.000 9</td><td>0.002 6</td><td>0.004 6</td><td>0.003 0</td><td>0.003 2</td><td>0.002 9</td></tr><tr><td><br />40</td><td>0.003 9</td><td>0.018 1</td><td>0.012 1</td><td>0.014 9</td><td>0.024 1</td><td>0.000 4</td><td>0.001 2</td><td>0.001 8</td><td>0.002 5</td><td>0.0013</td><td>0.001 5</td><td>0.003 1</td><td>0.002 7</td><td>0.002 0</td><td>0.002 9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="849">由表2可知, 随着偏差模型更新周期<i>D</i>的增加, 损失总体升高.但从数值上看, 损失的升高并不明显, 特别是在10<sup>5</sup>个时间步内, 因此实验中将<i>D</i>的值设置为10<sup>5</sup>.表2数据表明本文提出的偏差模型具有较强的泛化能力, 对偏差模型更新周期<i>D</i>的选择不敏感, 偏差模型可以在训练后较长的时间步内适用, 同时也验证了图7所示的分段更新策略的合理性.</p>
                </div>
                <h4 class="anchor-tag" id="850" name="850">3.2.2 偏差模型特征阶数</h4>
                <div class="p1">
                    <p id="851">偏差模型的特征阶数<i>K</i>越大, 模型复杂度越高, 训练误差越小, 但计算复杂度也随之升高, 同时可能引起过拟合.通过在游戏Alien, Breakout, Pong的训练过程中进行粗搜索来选择<i>K</i>的值.实验使用不同的<i>K</i>值进行训练, 记录多个位于偏差模型更新周期中心的经验池样本, 比较经验池样本的平均损失, 结果列于表3.表3中数据表明, 随着<i>K</i>值的增加, 损失总体下降, 但在<i>K</i>&gt;2时损失的变化已经不显著, 同时在Alien中<i>K</i>&gt;3时有过拟合的趋势.因此, 综合考虑损失和计算消耗, 本文实验将多项式特征的阶数设为<i>K</i>=2.</p>
                </div>
                <div class="area_img" id="852">
                    <p class="img_tit"><b>表3</b><b>比较不同<i>K</i>值下处于偏差模型更新周期中心的经验池平均损失</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3</b><b>Average Loss at the Center of Renewal Period of Correction Model Under Different <i>K</i> Values</b></p>
                    <p class="img_note"></p>
                    <table id="852" border="1"><tr><td rowspan="3">
K</td><td colspan="5">
Alien</td><td colspan="5">
Breakout</td><td colspan="5">
Pong</td></tr><tr><td colspan="4">
10<font class="font7"><sup>-5</sup></font><font class="font5">×</font><font class="font6">t</font></td><td rowspan="2">
AVG</td><td colspan="4">
10<font class="font7"><sup>-5</sup></font><font class="font5">×</font><font class="font6">t</font></td><td rowspan="2">
AVG</td><td colspan="4">
10<font class="font7"><sup>-5</sup></font><font class="font5">×</font><font class="font6">t</font></td><td rowspan="2">
AVG</td></tr><tr><td>4.5</td><td>
9.5</td><td>
14.5</td><td>
19.5</td><td>
4.5</td><td>
9.5</td><td>
14.5</td><td>
19.5</td><td>
4.5</td><td>
9.5</td><td>
14.5</td><td>
19.5</td></tr><tr><td>1</td><td>0.010 8</td><td>0.008 8</td><td>0.006 9</td><td>0.004 2</td><td>0.007 <font class="font8">7</font></td><td>0.001 5</td><td>0.003 5</td><td>0.003 2</td><td>0.003 6</td><td>0.003 <font class="font8">0</font></td><td>0.004 3</td><td>0.005 9</td><td>0.005 4</td><td>0.003 4</td><td>0.004 <font class="font8">8</font></td></tr><tr><td>2</td><td>0.010 5</td><td>0.008 2</td><td>0.003 5</td><td>0.002 8</td><td>0.006 <font class="font8">3</font></td><td>0.001 4</td><td>0.003 3</td><td>0.003 0</td><td>0.003 2</td><td>0.002 <font class="font8">7</font></td><td>0.003 8</td><td>0.005 2</td><td>0.004 9</td><td>0.003 2</td><td>0.004 <font class="font8">3</font></td></tr><tr><td>3</td><td>0.009 6</td><td>0.007 1</td><td>0.002 5</td><td>0.002 2</td><td>0.005 <font class="font8">4</font></td><td>0.001 3</td><td>0.003 2</td><td>0.002 8</td><td>0.003 2</td><td>0.002 <font class="font8">6</font></td><td>0.003 7</td><td>0.005 1</td><td>0.004 8</td><td>0.003 1</td><td>0.004 <font class="font8">2</font></td></tr><tr><td>4</td><td>0.009 5</td><td>0.007 2</td><td>0.002 8</td><td>0.002 1</td><td>0.005 <font class="font8">4</font></td><td>0.001 3</td><td>0.003 2</td><td>0.002 8</td><td>0.003 2</td><td>0.002 <font class="font8">6</font></td><td>0.003 7</td><td>0.005 1</td><td>0.004 8</td><td>0.003 1</td><td>0.004 <font class="font8">2</font></td></tr><tr><td>5</td><td>0.009 5</td><td>0.007 0</td><td>0.003 5</td><td>0.002 2</td><td>0.005 <font class="font8">6</font></td><td>0.001 3</td><td>0.003 2</td><td>0.002 7</td><td>0.003 2</td><td>
0.0026</td><td>0.003 6</td><td>0.005 1</td><td>0.004 8</td><td>0.003 1</td><td>0.004 <font class="font8">2</font></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="853" name="853"><b>3.3</b><b>对比实验与分析</b></h4>
                <div class="p1">
                    <p id="854">本文选择2个基线算法进行对比, 分别为DDQN<sup><a class="sup">[27]</a></sup>和优先经验回放<sup><a class="sup">[11]</a></sup> (PER) .其中, DDQN算法中样本没有优先级, 在训练时从经验池中随机取样;PER算法在DDQN的基础上为每个样本设置存储优先级, 存储优先级与样本上次参与训练时的TD-error绝对值成正比, 在采样时根据存储优先级依概率选择样本.本文通过偏差模型对存储优先级进行校正, 用校正后的真实优先级的估计进行采样, 提高了样本的利用效率.</p>
                </div>
                <div class="p1">
                    <p id="855">算法的评价标准包括智能体的学习速度和智能体学习到的最优策略的质量.</p>
                </div>
                <h4 class="anchor-tag" id="856" name="856">3.3.1 智能体的学习速度</h4>
                <div class="p1">
                    <p id="857">图11显示了19个Atari游戏的训练曲线, 横轴代表时间步, 每个时间步智能体与ALE环境交互一次, ALE环境会产生一帧新的图像.纵轴代表智能体在周期内获得的累计奖励.在训练开始后, 随着迭代的进行, 智能体的水平不断提高, 奖励值不断增大, 达到峰值后神经网络的训练可能趋于过拟合, 因此训练后期奖励值可能会有波动.</p>
                </div>
                <div class="p1">
                    <p id="858">ATDC-PER算法在19个游戏中的15个游戏收敛速度更快, 能够以更少的交互次数达到最大奖励, 如图11所示.特别是在Breakout, DoubleDunk, Enduro, Pong, Robotank, UpNDown这6个游戏中速度提升明显, 分别如图11 (f) (j) (k) (m) (q) (s) 所示, 在训练的相同时间步, ATDC-PER算法中智能体的得分水平总体优于DDQN和PER算法.图11中的曲线表明, 本文提出的ATDC-PER算法通过在训练中校正经验池样本的存储优先级, 提高了经验池样本的利用效率, 提升了智能体的学习速度, 减少了智能体与环境的交互次数, 使智能体以更少的迭代步收敛.</p>
                </div>
                <div class="area_img" id="877">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_87700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 本文方法在19个Atari游戏中与PER[11]和DDQN[27]算法的训练曲线对比" src="Detail/GetImg?filename=images/JFYZ201902004_87700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 本文方法在19个Atari游戏中与PER<sup><a class="sup">[11]</a></sup>和DDQN<sup><a class="sup">[27]</a></sup>算法的训练曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_87700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Comparison of our method with PER<sup></sup><sup>[11]</sup> and DDQN<sup></sup><sup>[27]</sup> algorithms in training curve of 19 Atari games</p>

                </div>
                <div class="area_img" id="877">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_87701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 本文方法在19个Atari游戏中与PER[11]和DDQN[27]算法的训练曲线对比" src="Detail/GetImg?filename=images/JFYZ201902004_87701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 本文方法在19个Atari游戏中与PER<sup><a class="sup">[11]</a></sup>和DDQN<sup><a class="sup">[27]</a></sup>算法的训练曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_87701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Comparison of our method with PER<sup></sup><sup>[11]</sup> and DDQN<sup></sup><sup>[27]</sup> algorithms in training curve of 19 Atari games</p>

                </div>
                <div class="area_img" id="877">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902004_87702.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 本文方法在19个Atari游戏中与PER[11]和DDQN[27]算法的训练曲线对比" src="Detail/GetImg?filename=images/JFYZ201902004_87702.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 本文方法在19个Atari游戏中与PER<sup><a class="sup">[11]</a></sup>和DDQN<sup><a class="sup">[27]</a></sup>算法的训练曲线对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902004_87702.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 11 Comparison of our method with PER<sup></sup><sup>[11]</sup> and DDQN<sup></sup><sup>[27]</sup> algorithms in training curve of 19 Atari games</p>

                </div>
                <h4 class="anchor-tag" id="879" name="879">3.3.2 最优策略的质量</h4>
                <div class="p1">
                    <p id="880">最优策略反映智能体最终的学习结果, 训练结束后使用最优策略可以指导智能体在ALE环境中获得良好表现.ATDC-PER算法中策略表示为Q网络参数, 训练结束后智能体可以得到完整可复用的策略.训练过程中保存每个时间步所在周期的奖励值, 同时每隔10<sup>5</sup>个时间步保存一次Q网络参数, 训练结束后选择奖励曲线平滑后处于峰值点的Q网络参数作为最优策略, 通过测试最优策略的质量对学习效果进行评价.</p>
                </div>
                <div class="p1">
                    <p id="881">最优策略的评价方法与DDQN<sup><a class="sup">[27]</a></sup>中相同, 首先, 周期开始时智能体与环境随机进行1～31次无动作交互, 为测试提供随机的初始状态.随后, 智能体按照最优策略的<i>ε</i>-贪心策略选择动作, 探索因子为0.05.每个周期智能体与环境最多进行18 000个时间步的交互, 策略的最终得分为100个测试周期得分的平均值.鉴于每个游戏的分值计算方式不同, 使用式 (22) 在实际得分的基础上计算规约得分.</p>
                </div>
                <div class="p1">
                    <p id="882"><mathml id="883"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>r</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext><mtext>i</mtext><mtext>z</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>g</mtext><mtext>e</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mo>-</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>d</mtext><mtext>o</mtext><mtext>m</mtext></mrow></msub></mrow><mrow><mo stretchy="false">|</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>u</mtext><mtext>m</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo>-</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>d</mtext><mtext>o</mtext><mtext>m</mtext></mrow></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>, (22) </p>
                </div>
                <div class="p1">
                    <p id="884">其中, <i>score</i><sub>random</sub>为随机智能体的得分, <i>score</i><sub>human</sub>为人类专家的得分, 规约后的分值大于100%表明智能体的水平已经超过了人类专家水平.</p>
                </div>
                <div class="p1">
                    <p id="885">19个Atari游戏总体的最优策略评价结果比较列于表4, 单个游戏的评价结果列于表5.表5中随机智能体 (random) 、人类专家 (human) 和DDQN算法的得分来源于文献[27], PER算法的得分来源于文献[11].</p>
                </div>
                <div class="area_img" id="886">
                    <p class="img_tit"><b>表4</b><b>全部游戏的规约得分评价表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4</b><b>Summary of Normalized Score on All Games</b></p>
                    <p class="img_note">%</p>
                    <table id="886" border="1"><tr><td><br />Indicators</td><td>DDQN<sup>[27]</sup></td><td>PER<sup>[11]</sup></td><td>ATDC-PER</td></tr><tr><td><br />Median</td><td>139.0</td><td>392.5</td><td><b>404.5</b></td></tr><tr><td><br />Average</td><td>335.6</td><td>505.6</td><td><b>588.9</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="887">
                    <p class="img_tit"><b>表5</b><b>全部游戏的实际得分和规约得分表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5</b><b>Raw Scores and Normalized Scores on All Games</b></p>
                    <p class="img_note"></p>
                    <table id="887" border="1"><tr><td rowspan="2"><br />Game</td><td rowspan="2">random</td><td rowspan="2">human</td><td colspan="3"><br />Score</td><td colspan="3">Normalized Score/%</td></tr><tr><td><br />DDQN<sup>[27]</sup></td><td>PER<sup>[11]</sup></td><td>ATDC-PER</td><td>DDQN<sup>[27]</sup></td><td>PER<sup>[11]</sup></td><td>ATDC-PER</td></tr><tr><td>Alien</td><td>227.8</td><td>6 875.4</td><td>2 907.3</td><td>3 583.3</td><td><b>3</b><b>856.1</b></td><td>40.3</td><td>50.5</td><td><b>54.6</b></td></tr><tr><td><br />Amidar</td><td>5.8</td><td>1 675.8</td><td>702.1</td><td>2 051.8</td><td><b>2</b><b>361.3</b></td><td>41.7</td><td>122.5</td><td><b>141.0</b></td></tr><tr><td><br />BankHeist</td><td>14.2</td><td>734.4</td><td>728.3</td><td>1 126.8</td><td><b>1</b><b>196</b></td><td>99.2</td><td>154.5</td><td><b>164.1</b></td></tr><tr><td><br />BeamRider</td><td>363.9</td><td>5 774.7</td><td>7 654</td><td><b>22</b><b>430.7</b></td><td>22 250.6</td><td>134.7</td><td><b>407.8</b></td><td>404.5</td></tr><tr><td><br />Boxing</td><td>0.1</td><td>4.3</td><td>81.7</td><td>98.8</td><td><b>99.5</b></td><td>1 942.9</td><td>2 350.0</td><td><b>2</b><b>366.7</b></td></tr><tr><td><br />Breakout</td><td>1.7</td><td>31.8</td><td>375.0</td><td>381.5</td><td><b>560.6</b></td><td>1 240.2</td><td>1 261.8</td><td><b>1</b><b>856.8</b></td></tr><tr><td><br />Centipede</td><td>2 090.9</td><td>11 963.2</td><td>4 139</td><td>5 175.4</td><td><b>5</b><b>738.7</b></td><td>20.7</td><td>31.2</td><td><b>36.9</b></td></tr><tr><td><br />ChopperCommand</td><td>811</td><td>9 881.8</td><td>4 653.0</td><td>5 135.0</td><td><b>6</b><b>581.0</b></td><td>42.4</td><td>47.7</td><td><b>63.6</b></td></tr><tr><td><br />CrazyClimber</td><td>10 780.5</td><td>35 410.5</td><td>101 874</td><td>183 137</td><td><b>191</b><b>354.9</b></td><td>369.8</td><td>699.8</td><td><b>733.1</b></td></tr><tr><td><br />DoubleDunk</td><td>-18.6</td><td>-15.5</td><td>-6.3</td><td>4.8</td><td><b>12.7</b></td><td>396.8</td><td>754.8</td><td><b>1</b><b>009.7</b></td></tr><tr><td><br />Enduro</td><td>0</td><td>309.6</td><td>319.5</td><td>2 155.0</td><td><b>3</b><b>827.0</b></td><td>103.2</td><td>696.1</td><td><b>1</b><b>236.1</b></td></tr><tr><td><br />NameThisGame</td><td>2 292.3</td><td>4 076.2</td><td>6 997.1</td><td><b>13</b><b>439.4</b></td><td>12 713.9</td><td>263.7</td><td><b>624.9</b></td><td>584.2</td></tr><tr><td><br />Pong</td><td>-20.7</td><td>9.3</td><td>21</td><td>20.7</td><td><b>21</b></td><td>139.0</td><td>138.0</td><td><b>139.0</b></td></tr><tr><td><br />PrivateEye</td><td>24.9</td><td>69 571.3</td><td><b>670.0</b></td><td>200.0</td><td>361.5</td><td><b>0.9</b></td><td>0.3</td><td>0.5</td></tr><tr><td><br />Riverraid</td><td>1 338.5</td><td>13 513.3</td><td>12 015.3</td><td>20 494</td><td><b>24</b><b>355.3</b></td><td>87.7</td><td>157.3</td><td><b>189.1</b></td></tr><tr><td><br />RoadRunner</td><td>11.5</td><td>7 845.0</td><td>48 377.0</td><td><b>62</b><b>785</b></td><td>60 344.3</td><td>617.4</td><td><b>801.3</b></td><td>770.2</td></tr><tr><td><br />Robotank</td><td>2.2</td><td>11.9</td><td>46.7</td><td>58.6</td><td><b>66.5</b></td><td>458.8</td><td>581.4</td><td><b>662.9</b></td></tr><tr><td><br />TimePilot</td><td>3 568.0</td><td>5 925.0</td><td>7 964.0</td><td>11 448.0</td><td><b>11</b><b>654.4</b></td><td>186.5</td><td>334.3</td><td><b>343.1</b></td></tr><tr><td><br />UpNDown</td><td>533.4</td><td>9 082.0</td><td>16 769.9</td><td>34 082.7</td><td><b>37</b><b>550</b></td><td>189.9</td><td>392.5</td><td><b>433.0</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="888" name="888">3.3.3 综合评价</h4>
                <div class="p1">
                    <p id="889">表6显示了19个Atari游戏中智能体学习速度和最优策略质量的综合评价.结果表明, 本文使用偏差模型校正后的真实优先级的估计来选择样本能够在训练中提升智能体的学习速度, 同时提高最优策略的质量.本文算法在2种评价指标下都取得最好成绩的游戏达14/19个, 其中在15/19个游戏中提升了智能体的学习速度, 减少了智能体与环境的交互次数;在15/19个游戏中改善了智能体的学习效果, 提升了最优策略的质量.Boxing游戏比较简单, 3种算法均能收敛到接近最大奖励值, 本文算法和PER算法的收敛速度均略低于DDQN算法, 但本文方法的最优策略得分优于其余2种算法, 如图11 (e) 所示.BeamRider, NameThisGame这2个游戏不同方法的训练曲线交错上升, 最后各算法达到的最优策略质量相似, 均明显超过了人类专家水平.RoadRunner中本文方法的收敛速度优于DDQN和PER算法, 最优策略得分略低于PER算法.PrivateEye游戏较难, 3种算法的最优策略得分均不足人类专家得分的1%, 均没有得到较好的结果.</p>
                </div>
                <div class="area_img" id="890">
                    <p class="img_tit"><b>表6</b><b>全部游戏的综合评价表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6</b><b>Comprehensive Evaluation on All Games</b></p>
                    <p class="img_note"></p>
                    <table id="890" border="1"><tr><td rowspan="2"><br />Game</td><td colspan="3"><br />Learning Speed</td><td colspan="3">Quality of Optimal Policy</td></tr><tr><td><br />DDQN<sup>[27]</sup></td><td>PER<sup>[11]</sup></td><td>ATDC-PER</td><td>DDQN<sup>[27]</sup></td><td>PER<sup>[11]</sup></td><td>ATDC-PER</td></tr><tr><td><br />Alien</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />Amidar</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />BankHeist</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />BeamRider</td><td></td><td>■</td><td></td><td></td><td>■</td><td></td></tr><tr><td><br />Boxing</td><td>■</td><td></td><td></td><td></td><td></td><td>■</td></tr><tr><td><br />Breakout</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />Centipede</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />ChopperCommand</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />CrazyClimber</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />DoubleDunk</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />Enduro</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />NameThisGame</td><td>■</td><td></td><td></td><td></td><td>■</td><td></td></tr><tr><td><br />Pong</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />PrivateEye</td><td>■</td><td></td><td></td><td>■</td><td></td><td></td></tr><tr><td><br />Riverraid</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />RoadRunner</td><td></td><td></td><td>■</td><td></td><td>■</td><td></td></tr><tr><td><br />Robotank</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />TimePilot</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr><tr><td><br />UpNDown</td><td></td><td></td><td>■</td><td></td><td></td><td>■</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note: ■ shows this method is the best among all of the comparison methods.</p>
                </div>
                <div class="p1">
                    <p id="891">由表4和表5数据可知, ATDC-PER算法能够在全部19个游戏中的15个游戏获得更好的最优策略.最优策略规约得分的中位数相对于PER算法<sup><a class="sup">[11]</a></sup>提高12%, 平均数提高83.3%.实验结果表明, 本文方法通过提高经验池中样本的采样效率, 可以改善智能体的最终学习效果, 使智能体收敛到更好的最优策略.</p>
                </div>
                <h3 id="892" name="892" class="anchor-tag"><b>4</b><b>结</b><b>论</b></h3>
                <div class="p1">
                    <p id="893">深度Q学习中, 基于优先经验回放的方法在训练中样本的存储优先级不能跟随Q网络迭代而更新, 存储优先级不能准确反映经验池中样本TD-error分布的真实情况, 从而降低了大量样本的利用效率.使用样本的真实优先级对经验池样本进行采样能够明显提高经验池样本的利用效率和学习的效果.</p>
                </div>
                <div class="p1">
                    <p id="894">本文提出的基于TD-error自适应校正的主动采样方法, 利用样本的回放周期和Q网络状态能够校正样本的优先级偏差, 得到样本真实优先级的估计值, 以极小的代价逼近真实优先级的分布.偏差模型在训练过程中分段更新, 且对更新周期不敏感, 偏差模型特征阶数较小时就可以很好地估计优先级偏差, 分段更新策略显著降低了本文方法的计算复杂度.在Q网络迭代的每个时间步使用校正后的优先级进行采样, 能够提升Q网络训练中经验池样本的利用效率.在Atari 2600中的实验结果表明, 使用本文方法进行主动采样提升了智能体的学习速度, 减少了智能体与环境的交互次数, 同时改善了智能体的学习效果, 提升了最优策略的质量.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="968">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">

                                <b>[1]</b>Sutton R S, Barto A G.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 1998:100-107
                            </a>
                        </p>
                        <p id="970">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neuro-dynamic programming:an overview">

                                <b>[2]</b>Bertsekas D P, Tsitsiklis J N.Neuro-dynamic programming:An overview[C]Proc of the 34th IEEE Conf on Decision and Control.Piscataway, NJ:IEEE, 1995:560-564
                            </a>
                        </p>
                        <p id="972">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TD-Gammon:A Self-Teaching Backgammon Program">

                                <b>[3]</b>Tesauro G.Td-gammon:A self-teaching backgammon program[M]Applications of Neural Networks.New York:Springer, 1995:267-285
                            </a>
                        </p>
                        <p id="974">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702006969&amp;v=MjM5ODZadUh5am1VYjdJSmx3ZGJoWT1OaWZPZmJLN0h0RE5xSTlIWk9zSkJYb3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Tesauro G.Programming backgammon using self-teaching neural nets[J].Artificial Intelligence, 2002, 134 (1/2) :181-199
                            </a>
                        </p>
                        <p id="976">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with DeepConvolutional Neural Networks">

                                <b>[5]</b>Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 26th Int Conf on neural information processing systems (NIPS) .Cambridge, MA:MIT Press, 2012:1097-1105
                            </a>
                        </p>
                        <p id="978">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801001&amp;v=MTMyOTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydk1MejdCZHJHNEg5bk1ybzlGWllRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Liu Quan, Zhai Jianwei, Zhang Zongchang, et al.A survey on deep reinforcement learning[J].Chinese Journal of Computers, 2018, 41 (1) :1-27 (in Chinese) (刘全, 翟建伟, 章宗长, 等.深度强化学习综述[J].计算机学报, 2018, 41 (1) :1-27) 
                            </a>
                        </p>
                        <p id="980">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep reinforcement learning:an overview">

                                <b>[7]</b>Li Yuxi.Deep reinforcement learning:An overview[EB/OL].New York:Cornell University, 2017[2018-03-04].https:arxiv.org/abs/1701.07274
                            </a>
                        </p>
                        <p id="982">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Playing atari with deep reinforcement learning">

                                <b>[8]</b>Mnih V, Kavukcuoglu K, Silver D, et al.Playing Atari with deep reinforcement learning[EB/OL].New York:Cornell University, 2013[2017-10-04].https:arxiv.org/abs/1312.5602
                            </a>
                        </p>
                        <p id="984">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">

                                <b>[9]</b>Mnih V, Kavukcuoglu K, Silver D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529-533
                            </a>
                        </p>
                        <p id="986">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convergent temporal-difference learningwith arbitrary smooth function approximation">

                                <b>[10]</b>Maei H R, Szepesvari C, Bhatnagar S, et al.Convergent temporal-difference learning with arbitrary smooth function approximation[C]Proc of the 23rd Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2009:1204-1212
                            </a>
                        </p>
                        <p id="988">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Prioritized experience replay">

                                <b>[11]</b>Schaul T, Quan J, Antonoglou I, et al.Prioritized experience replay[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:256-265
                            </a>
                        </p>
                        <p id="990">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Massively parallel methods for deep reinforcement learning">

                                <b>[12]</b>Nair A, Srinivasan P, Blackwell S, et al.Massively parallel methods for deep reinforcement learning[EB/OL].New York:Cornell University, 2015[2018-03-02].https:arxiv.org/abs/1507.04296
                            </a>
                        </p>
                        <p id="992">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Asynchronous methods for deep reinforcement learning">

                                <b>[13]</b>Mnih V, Badia A P, Mirza M, et al.Asynchronous methods for deep reinforcement learning[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:JMLR, 2016:1928-1937
                            </a>
                        </p>
                        <p id="994">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized prioritized sweeping">

                                <b>[14]</b>Andre D, Friedman N, Parr R.Generalized prioritized sweeping[C]Proc of the 12th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 1998:1001-1007
                            </a>
                        </p>
                        <p id="996">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Markovian decision process">

                                <b>[15]</b>Bellman R.A Markovian decision process[J].Journal of Mathematics and Mechanics, 1957, 6 (4) :679-684
                            </a>
                        </p>
                        <p id="998">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES43FE6DC313BF4DA251866CE09A930EA8&amp;v=MDkxNjFTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxod3JxMnhLdz1OaWZPZmJlN2FLVEsyL3hHWmVoOWVuaE52aFFXNnpkN1RneVhyQnRFY0xHVU1NdVhDT052Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Bertsekas D P.Dynamic programming and suboptimal control:A survey from ADP to MPC[J].European Journal of Control, 2005, 11 (4/5) :310-334
                            </a>
                        </p>
                        <p id="1000">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey of Monte Carlo Tree Search Methods">

                                <b>[17]</b>Browne C B, Powley E, Whitehouse D, et al.A survey of Monte Carlo tree search methods[J].IEEE Transactions on Computational Intelligence and AI in Games, 2012, 4 (1) :1-43
                            </a>
                        </p>
                        <p id="1002">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                Silver D, Huang Shijie, Maddison C J, et al.Mastering the game of Go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484-489
                            </a>
                        </p>
                        <p id="1004">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model-Free reinforcement learning with continuous action in practice">

                                <b>[19]</b>Degris T, Pilarski P M, Sutton R S.Model-Free reinforcement learning with continuous action in practice[C]Proc of the 52nd American Control Conf.Piscataway, NJ:IEEE, 2012:2177-2182
                            </a>
                        </p>
                        <p id="1006">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A kernel based true online Sarsa (λ)for continuous space control problems">

                                <b>[20]</b>Zhu Fei, Zhu Haijun, Fu Yuchen, et al.A kernel based true online Sarsa (λ) for continuous space control problems[J].Computer Science&amp;Information Systems, 2017, 14 (3) :789-804
                            </a>
                        </p>
                        <p id="1008">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201403009&amp;v=MTg2MzZxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydk1MeXZTZExHNEg5WE1ySTlGYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Zhu Fei, Liu Quan, Fu Qiming, et al.A least square actorcritic approach for continuous action space[J].Journal of Computer Research and Development, 2014, 51 (3) :548-558 (in Chinese) (朱斐, 刘全, 傅启明, 等.一种用于连续动作空间的最小二乘行动者-评论家方法[J].计算机研究与发展, 2014, 51 (3) :548-558) 
                            </a>
                        </p>
                        <p id="1010">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706002&amp;v=MjIzNTVxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XcnZNTHo3QmRyRzRIOWJNcVk5RlpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>Liu Quan, Zhang Peng, Zhong Shan, et al.An improved actor-critic algorithm in continuous spaces with action weighting[J].Chinese Journal of Computers, 2017, 40 (6) :1252-1264 (in Chinese) (刘全, 章鹏, 钟珊, 等.连续空间中的一种动作加权行动者评论家算法[J].计算机学报, 2017, 40 (6) :1252-1264) 
                            </a>
                        </p>
                        <p id="1012">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monte Carlo matrix inversion and reinforcement learning">

                                <b>[23]</b>Barto A, Duff M.Monte Carlo matrix inversion and reinforcement learning[C]Proc of the 8th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 1994:687-694
                            </a>
                        </p>
                        <p id="1014">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001338455&amp;v=MjQ3Njc9Tmo3QmFyTzRIdEhOckl4TllPNEtZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTbmxWYjNCSkZv&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>Watkins C J C H, Dayan P.Q learning[J].Machine Learning, 1992, 8 (3/4) :279-292
                            </a>
                        </p>
                        <p id="1016">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Experience replay for least-squares policy iteration">

                                <b>[25]</b>Liu Quan, Zhou Xin, Zhu Fei, et al.Experience replay for least-squares policy iteration[J].IEEE/CAA Journal of Automatica Sinica, 2015, 1 (3) :274-281
                            </a>
                        </p>
                        <p id="1018">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Double Q-learning">

                                <b>[26]</b>Hasselt H V.Double Q-learning[C]Proc of the 24th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2010:2613-2621
                            </a>
                        </p>
                        <p id="1020">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Reinforcement Learning with Double Q-Learning">

                                <b>[27]</b>Hasselt H V, Guez A, Silver D.Deep reinforcement learning with double Q-learning[C]Proc of the 30th AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2016:2094-2100
                            </a>
                        </p>
                        <p id="1022">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep exploration via bootstrapped DQN">

                                <b>[28]</b>Osband I, Blundell C, Pritzel A, et al.Deep exploration via bootstrapped DQN[C]Proc of the 30th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:4026-4034
                            </a>
                        </p>
                        <p id="1024">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploration:Astudy of count-based exploration for deep reinforcement learning">

                                <b>[29]</b>Tang Haoran, Houthooft R, Foote D, et al.Exploration:Astudy of count-based exploration for deep reinforcement learning[C]Proc of the 31st Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2017:2750-2759
                            </a>
                        </p>
                        <p id="1026">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unifying count-based exploration and intrinsic motivation">

                                <b>[30]</b>Bellemare M, Srinivasan S, Ostrovski G, et al.Unifying count-based exploration and intrinsic motivation[C]Proc of the 30th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:1471-1479
                            </a>
                        </p>
                        <p id="1028">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incentivizing exploration in reinforcement learning with deep predictive models">

                                <b>[31]</b>Stadie B C, Levine S, Abbeel P.Incentivizing exploration in reinforcement learning with deep predictive models[EB/OL].New York:Cornell University, 2015[2018-03-01].https:arxiv.org/abs/1507.00814
                            </a>
                        </p>
                        <p id="1030">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dueling network architectures for deep reinforcement learning">

                                <b>[32]</b>Wang Ziyu, Schaul T, Hessel M, et al.Dueling network architectures for deep reinforcement learning[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:JMLR, 2016:1995-2003
                            </a>
                        </p>
                        <p id="1032">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Averaged-DQN:Variance reduction and stabilization for deep reinforcement learning">

                                <b>[33]</b>Anschel O, Baram N, Shimkin N.Averaged-DQN:Variance reduction and stabilization for deep reinforcement learning[C]Proc of the 34th Int Conf on Machine Learning (ICML) .New York:JMLR, 2017:176-185
                            </a>
                        </p>
                        <p id="1034">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Policy distillation">

                                <b>[34]</b>Rusu A, Colmenarejo S G, Gulcehre C, et al.Policy distillation[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:343-352
                            </a>
                        </p>
                        <p id="1036">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Knowledge transfer for deep reinforcement learning with hierarchical experience replay">

                                <b>[35]</b>Yin Haiyan, Pan Jialin.Knowledge transfer for deep reinforcement learning with hierarchical experience replay[C]Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2017:1640-1646
                            </a>
                        </p>
                        <p id="1038">
                            <a id="bibliography_36" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706008&amp;v=MjM2NTVGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydk1MejdCZHJHNEg5Yk1xWTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[36]</b>Liu Quan, Zhai Jianwei, Zhong Shan, et al.A Deep recurrent Q-network based on visual attention mechanism[J].Chinese Journal of Computers, 2017, 40 (6) :1353-1366 (in Chinese) (刘全, 翟建伟, 钟珊, 等.一种基于视觉注意力机制的深度循环Q网络模型[J].计算机学报, 2017, 40 (6) :1353-1366) 
                            </a>
                        </p>
                        <p id="1040">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel DDPGmethod with prioritized experience replay">

                                <b>[37]</b>Hou Yuenan, Liu Lifeng, Wei Qing, et al.A novel DDPGmethod with prioritized experience replay[C]Proc of the13th IEEE Int Conf on Systems, Man, and Cybernetics (SMC) .Piscataway, NJ:IEEE, 2017:316-321
                            </a>
                        </p>
                        <p id="1042">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The importance of experience replay database composition in deep reinforcement learning">

                                <b>[38]</b>Bruin T, Kober J, Tuyls K, et al.The importance of experience replay database composition in deep reinforcement learning[EB/OL].2015[2017-11-04].http:www.jenskober.de/deBruinNIPS_WS2015.pdf
                            </a>
                        </p>
                        <p id="1044">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deterministic policy gradient algorithms">

                                <b>[39]</b>Silver D, Lever G, Heess N, et al.Deterministic policy gradient algorithms[C]Proc of the 31st Int Conf on Machine Learning (ICML) .New York:JMLR, 2014:387-395
                            </a>
                        </p>
                        <p id="1046">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous control with deep reinforcement learning">

                                <b>[40]</b>Lillicrap T P, Hunt J J, Pritzel A, et al.Continuous control with deep reinforcement learning[C]Proc of the 4th Int Conf on Learning Representations (ICLR) .New York:Springer, 2016:232-241
                            </a>
                        </p>
                        <p id="1048">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neuronlike adaptive elements that can solve difficult learning control problems">

                                <b>[41]</b>Barto A G, Sutton R S, Anderson C W.Neuronlike adaptive elements that can solve difficult learning control problems[J].IEEE Transactions on Systems, Man, and Cybernetics, 1983, 13 (5) :834-846
                            </a>
                        </p>
                        <p id="1050">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weighted importance sampling for off-policy learning with linear function approximation">

                                <b>[42]</b>Mahmood A R, Hasselt H V, Sutton R S.Weighted importance sampling for off-policy learning with linear function approximation[C]Proc of the 24th Int Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:3014-3022
                            </a>
                        </p>
                        <p id="1052">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The arcade learning environment:an evaluation platform for general agents">

                                <b>[43]</b>Bellmare M G, Naddaf Y, Veness J, et al.The arcade learning environment:An evaluation platform for general agents[J].Journal of Artificial Intelligence Research, 2013, 47 (1) :253-279
                            </a>
                        </p>
                        <p id="1054">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OpenAI gym">

                                <b>[44]</b>Brockman G, Cheung V, Pettersson L, et al.OpenAI gym[EB/OL].New York:Cornell University, 2016[2017-10-04].https:arxiv.org/abs/1606.01540
                            </a>
                        </p>
                        <p id="1056">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensor Flow:large-scale machine learning on heterogeneous distributed systems">

                                <b>[45]</b>Abadi M, Agarwal A, Barham P, et al.TensorFlow:Largescale machine learning on heterogeneous distributed systems[EB/OL].New York:Cornell University, 2016[2017-10-04].https:arxiv.org/abs/1603.04467
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="1110" href="javascript:void(0)">
                            <b>1</b> https://gym.openai.com/envs/Cartpole-v0
                        </span>
                    </p>
                    <p>
                        <span id="1112" href="javascript:void(0)">
                            <b>2</b> https://gym.openai.com/envs/Pong-v0
                        </span>
                    </p>
                    <p>
                        <span id="1114" href="javascript:void(0)">
                            <b>3</b> https://gym.openai.com/envs/Breakout-v0
                        </span>
                    </p>
                    <p>
                        <span id="1116" href="javascript:void(0)">
                            <b>4</b> http://pr-ai.hit.edu.cn/2018/0510/c1049a207703/page.htm
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902004&amp;v=MjY5MjRSTE9lWmVWdkZ5N25XcnZNTHl2U2RMRzRIOWpNclk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
