<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243346533750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902003%26RESULT%3d1%26SIGN%3dfSkUdpnS0JnJ6YSBPhgYNApe8eA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902003&amp;v=MDYzMzZxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydktMeXZTZExHNEg5ak1yWTlGWjRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#362" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;基本概念&lt;/b&gt; "><b>1</b><b>基本概念</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#365" data-title="&lt;b&gt;1.1&lt;/b&gt;&lt;b&gt;值函数&lt;/b&gt;"><b>1.1</b><b>值函数</b></a></li>
                                                <li><a href="#381" data-title="&lt;b&gt;1.2&lt;/b&gt;&lt;b&gt;策略搜索&lt;/b&gt;"><b>1.2</b><b>策略搜索</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#391" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;逆强化学习&lt;/b&gt; "><b>2</b><b>逆强化学习</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#394" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;基于确定基函数组合的反馈信号函数&lt;/b&gt;"><b>2.1</b><b>基于确定基函数组合的反馈信号函数</b></a></li>
                                                <li><a href="#423" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;基于参数化模型的反馈信号函数&lt;/b&gt;"><b>2.2</b><b>基于参数化模型的反馈信号函数</b></a></li>
                                                <li><a href="#439" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;其他函数表示形式&lt;/b&gt;"><b>2.3</b><b>其他函数表示形式</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#442" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;基于逆强化学习的示教学习方法&lt;/b&gt; "><b>3</b><b>基于逆强化学习的示教学习方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#445" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;学徒学习&lt;/b&gt;"><b>3.1</b><b>学徒学习</b></a></li>
                                                <li><a href="#477" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;代价指导学习&lt;/b&gt;"><b>3.2</b><b>代价指导学习</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#495" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;结束语&lt;/b&gt; "><b>4</b><b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#355" data-title="图1 强化学习过程">图1 强化学习过程</a></li>
                                                <li><a href="#480" data-title="图2 代价指导学习过程">图2 代价指导学习过程</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="543">


                                    <a id="bibliography_1" title="Sutton R, Barto A.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 2017" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning:An Introduction">
                                        <b>[1]</b>
                                        Sutton R, Barto A.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 2017
                                    </a>
                                </li>
                                <li id="545">


                                    <a id="bibliography_2" title="Bagnell J.An invitation to imitation[OL].2015[2017-02-12].https:pdfs.semanticscholar.org/f04d/3ddee335927186-b012a1bee765c142ddce57.pdf/_ga=2.181329124.233418181.1525172550-397727813.1525172550" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An invitation to imitation[OL]">
                                        <b>[2]</b>
                                        Bagnell J.An invitation to imitation[OL].2015[2017-02-12].https:pdfs.semanticscholar.org/f04d/3ddee335927186-b012a1bee765c142ddce57.pdf/_ga=2.181329124.233418181.1525172550-397727813.1525172550
                                    </a>
                                </li>
                                <li id="547">


                                    <a id="bibliography_3" title="Abbeel P, Ganapathi V, Ng A.Learning vehicular dynamics, with application to modeling helicopters[C]Proc of the 20th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2006:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning vehicular dynamics with application tomodeling helicopters">
                                        <b>[3]</b>
                                        Abbeel P, Ganapathi V, Ng A.Learning vehicular dynamics, with application to modeling helicopters[C]Proc of the 20th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2006:1-8
                                    </a>
                                </li>
                                <li id="549">


                                    <a id="bibliography_4" title="Abbeel P, Coates A, Quigley M, et al.An application of reinforcement learning to aerobatic helicopter flight[C]Proc of the 21st Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2007:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An application of reinforcement learning to aerobatic helicopter flight">
                                        <b>[4]</b>
                                        Abbeel P, Coates A, Quigley M, et al.An application of reinforcement learning to aerobatic helicopter flight[C]Proc of the 21st Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2007:1-8
                                    </a>
                                </li>
                                <li id="551">


                                    <a id="bibliography_5" title="Ng A, Coates A, Diel M, et al.Inverted autonomous helicopter flight via reinforcement learning[C]Proc of the 4th Int Symp on Experimental Robotics.New York:Springer, 2004:799-806" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverted autonomous helicopter flight via reinforcement learning">
                                        <b>[5]</b>
                                        Ng A, Coates A, Diel M, et al.Inverted autonomous helicopter flight via reinforcement learning[C]Proc of the 4th Int Symp on Experimental Robotics.New York:Springer, 2004:799-806
                                    </a>
                                </li>
                                <li id="553">


                                    <a id="bibliography_6" title="Coates A, Abbeel P, Ng A.Learning for control from multiple demonstrations[C]Proc of the 25th Int Conf on Machine Learning (ICML) .New York:ACM, 2008:144-151" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning for control from multiple demonstrations">
                                        <b>[6]</b>
                                        Coates A, Abbeel P, Ng A.Learning for control from multiple demonstrations[C]Proc of the 25th Int Conf on Machine Learning (ICML) .New York:ACM, 2008:144-151
                                    </a>
                                </li>
                                <li id="555">


                                    <a id="bibliography_7" title="Abbeel P, Coates A, Hunter T, et al.Autonomous autorotation of an RC helicopter[C]Proc of the 8th Int Symp on Robotics.New York:Springer, 2008:385-394" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Autonomous autorotation of an RC helicopter">
                                        <b>[7]</b>
                                        Abbeel P, Coates A, Hunter T, et al.Autonomous autorotation of an RC helicopter[C]Proc of the 8th Int Symp on Robotics.New York:Springer, 2008:385-394
                                    </a>
                                </li>
                                <li id="557">


                                    <a id="bibliography_8" title="Abbeel P, Coates A, Ng A.Autonomous helicopter aerobatics through apprenticeship learning[J].The International Journal of Robotics Research, 2010, 29 (13) :1608-1639" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Autonomous Helicopter Aerobatics through Apprenticeship Learning">
                                        <b>[8]</b>
                                        Abbeel P, Coates A, Ng A.Autonomous helicopter aerobatics through apprenticeship learning[J].The International Journal of Robotics Research, 2010, 29 (13) :1608-1639
                                    </a>
                                </li>
                                <li id="559">


                                    <a id="bibliography_9" title="Ratliff N, Bagnell J, Zinkevich M.Maximum margin planning[C]Proc of the 23rd Int Conf on Machine Learning (ICML) .New York:ACM, 2006:729-736" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximum Margin Planning">
                                        <b>[9]</b>
                                        Ratliff N, Bagnell J, Zinkevich M.Maximum margin planning[C]Proc of the 23rd Int Conf on Machine Learning (ICML) .New York:ACM, 2006:729-736
                                    </a>
                                </li>
                                <li id="561">


                                    <a id="bibliography_10" title="Abbeel P, Dolov D, Ng A, et al.Apprenticeship learning for motion planning with application to parking lot navigation[C]Proc of the 21st IEEE/RSJ Int Conf on Intelligent Robots and Systems.Piscataway, NJ:IEEE, 2008:1083-1090" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Apprenticeship Learning for Motion Planning with Application to Parking Lot Navigation">
                                        <b>[10]</b>
                                        Abbeel P, Dolov D, Ng A, et al.Apprenticeship learning for motion planning with application to parking lot navigation[C]Proc of the 21st IEEE/RSJ Int Conf on Intelligent Robots and Systems.Piscataway, NJ:IEEE, 2008:1083-1090
                                    </a>
                                </li>
                                <li id="563">


                                    <a id="bibliography_11" title="Ziebart B, Maas A, Bagnell J, et al.Maximum entropy inverse reinforcement learning[C]Proc of the 23rd AAAIConf on Artificial Intelligence.Menlo Park:AAAI Press, 2008:1433-1438" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximum entropy inverse reinforcement learning">
                                        <b>[11]</b>
                                        Ziebart B, Maas A, Bagnell J, et al.Maximum entropy inverse reinforcement learning[C]Proc of the 23rd AAAIConf on Artificial Intelligence.Menlo Park:AAAI Press, 2008:1433-1438
                                    </a>
                                </li>
                                <li id="565">


                                    <a id="bibliography_12" title="Ziebart B, Bagnell J, Dey A.Modeling interaction via the principle of maximum causal entropy[C]Proc of the 27th Int Conf on Machine Learning (ICML) .New York:ACM, 2010:1255-1262" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling interaction via the principle of maximum causal entropy">
                                        <b>[12]</b>
                                        Ziebart B, Bagnell J, Dey A.Modeling interaction via the principle of maximum causal entropy[C]Proc of the 27th Int Conf on Machine Learning (ICML) .New York:ACM, 2010:1255-1262
                                    </a>
                                </li>
                                <li id="567">


                                    <a id="bibliography_13" title="Barrett E, Linder S.Autonomous HVAC control, a reinforcement learning approach[C]Proc of the 25th European Conf on Machine Learning and Knowledge Discovery in Databases.Porto:CEUR-WS, 2015:3-19" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Autonomous HVAC control,a reinforcement learning approach">
                                        <b>[13]</b>
                                        Barrett E, Linder S.Autonomous HVAC control, a reinforcement learning approach[C]Proc of the 25th European Conf on Machine Learning and Knowledge Discovery in Databases.Porto:CEUR-WS, 2015:3-19
                                    </a>
                                </li>
                                <li id="569">


                                    <a id="bibliography_14" title="Sammut C, Hurst S, Kedzier D, et al.Learning to fly[C]Proc of the 9th Int Conf on Machine Learning (ICML) .New York:ACM, 1992:385-393" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to fly">
                                        <b>[14]</b>
                                        Sammut C, Hurst S, Kedzier D, et al.Learning to fly[C]Proc of the 9th Int Conf on Machine Learning (ICML) .New York:ACM, 1992:385-393
                                    </a>
                                </li>
                                <li id="571">


                                    <a id="bibliography_15" title="Kunigoshi Y, Inaba M, Inoue H.Learning by watching:Extracting reusable task knowledge from visual observation of human performance[J].IEEE Transactions on Robotics and Automation, 1994, 10 (6) :799-822" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning by watching: extracting reusable task knowledge from visual observation of human performance">
                                        <b>[15]</b>
                                        Kunigoshi Y, Inaba M, Inoue H.Learning by watching:Extracting reusable task knowledge from visual observation of human performance[J].IEEE Transactions on Robotics and Automation, 1994, 10 (6) :799-822
                                    </a>
                                </li>
                                <li id="573">


                                    <a id="bibliography_16" title="Ross S, Bagnell D.Efficient reductions for imitation learning[C]Proc of the 13th Int Conf on Artificial Intelligence and Statistics (AISTATS) .Cambridge, MA:MIT Press, 2010:661-668" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient reductions for imitation learning">
                                        <b>[16]</b>
                                        Ross S, Bagnell D.Efficient reductions for imitation learning[C]Proc of the 13th Int Conf on Artificial Intelligence and Statistics (AISTATS) .Cambridge, MA:MIT Press, 2010:661-668
                                    </a>
                                </li>
                                <li id="575">


                                    <a id="bibliography_17" title="Ross S, Gordon G, Bagnell J.A reduction of imitation learning and structured prediction to no-regret online learning[C]Proc of the 14th Int Conf on Artificial Intelligence and Statistics (AISTATS) .Cadiz:JMLR, 2011:627-635" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning">
                                        <b>[17]</b>
                                        Ross S, Gordon G, Bagnell J.A reduction of imitation learning and structured prediction to no-regret online learning[C]Proc of the 14th Int Conf on Artificial Intelligence and Statistics (AISTATS) .Cadiz:JMLR, 2011:627-635
                                    </a>
                                </li>
                                <li id="577">


                                    <a id="bibliography_18" title="Abbeel P, Ng A.Apprenticeship learning via inverse reinforcement learning[C]Proc of the 21st Int Conf on Machine Learning (ICML) .Cadiz:JMLR, 2004:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Apprenticeship learning via inversereinforcement learning">
                                        <b>[18]</b>
                                        Abbeel P, Ng A.Apprenticeship learning via inverse reinforcement learning[C]Proc of the 21st Int Conf on Machine Learning (ICML) .Cadiz:JMLR, 2004:1-8
                                    </a>
                                </li>
                                <li id="579">


                                    <a id="bibliography_19" title="Finn C, Levine S, Abbeel P.Guided cost learning:Deep inverse optimal control via policy optimization[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:ACM, 2016:49-58" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Guided cost learning:deep inverse optimal control via policy optimization">
                                        <b>[19]</b>
                                        Finn C, Levine S, Abbeel P.Guided cost learning:Deep inverse optimal control via policy optimization[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:ACM, 2016:49-58
                                    </a>
                                </li>
                                <li id="581">


                                    <a id="bibliography_20" title="Ho J, Ermon S.Generative adversarial imitation learning[C]Proc of the 30th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:4565-4573" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Generative adversarial imitation learning,&amp;quot;">
                                        <b>[20]</b>
                                        Ho J, Ermon S.Generative adversarial imitation learning[C]Proc of the 30th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:4565-4573
                                    </a>
                                </li>
                                <li id="583">


                                    <a id="bibliography_21" title="Freund Y, Schapire R.Adaptive game playing using multiplicative weights[J].Games and Economic Behavior, 1999, 29 (1/2) :79-103" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300002264&amp;v=MTYwMjBlcnFRVE1ud1plWnVIeWptVWI3SUpsd2RiaGM9TmlmT2ZiSzdIdERPckk5RlpPc05Ebm85b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Freund Y, Schapire R.Adaptive game playing using multiplicative weights[J].Games and Economic Behavior, 1999, 29 (1/2) :79-103
                                    </a>
                                </li>
                                <li id="585">


                                    <a id="bibliography_22" title="Syed U, Schapire R.A game-theoretic approach to apprenticeship learning[C]Proc of the 22nd Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2008:1449-1456" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A game-theoretic approach to apprenticeship learning">
                                        <b>[22]</b>
                                        Syed U, Schapire R.A game-theoretic approach to apprenticeship learning[C]Proc of the 22nd Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2008:1449-1456
                                    </a>
                                </li>
                                <li id="587">


                                    <a id="bibliography_23" title="Goodfellow I, Abdaie J, Mirza M, et al.Generative adversarial nets[C]Proc of the 28th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:2672-2680" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Nets">
                                        <b>[23]</b>
                                        Goodfellow I, Abdaie J, Mirza M, et al.Generative adversarial nets[C]Proc of the 28th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:2672-2680
                                    </a>
                                </li>
                                <li id="589">


                                    <a id="bibliography_24" title="Finn C, Christiano P, Abbeel P, et al.A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models[OL]. (2016-11-11) [2016-11-25].https:arxiv.org/abs/1611.03852, 2016" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A connection between generative adversarial networks,inverse reinforcement learning,and energy-based models[OL]">
                                        <b>[24]</b>
                                        Finn C, Christiano P, Abbeel P, et al.A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models[OL]. (2016-11-11) [2016-11-25].https:arxiv.org/abs/1611.03852, 2016
                                    </a>
                                </li>
                                <li id="591">


                                    <a id="bibliography_25" title="Ng A, Russell S.Algorithms for inverse reinforcement learning[C]Proc of the 17th Int Conf on Machine Learning (ICML) .Cambridge, MA:MIT Press, 2000:663-670" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Algorithms for inverse reinforcement learning">
                                        <b>[25]</b>
                                        Ng A, Russell S.Algorithms for inverse reinforcement learning[C]Proc of the 17th Int Conf on Machine Learning (ICML) .Cambridge, MA:MIT Press, 2000:663-670
                                    </a>
                                </li>
                                <li id="593">


                                    <a id="bibliography_26" title="Syed U, Bowling M, Schapire R.Apprenticeship learning using linear programming[C]Proc of the 25th Int Conf on Machine Learning (ICML) .New York:ACM, 2008:1032-1039" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Apprenticeship learning using linear programming">
                                        <b>[26]</b>
                                        Syed U, Bowling M, Schapire R.Apprenticeship learning using linear programming[C]Proc of the 25th Int Conf on Machine Learning (ICML) .New York:ACM, 2008:1032-1039
                                    </a>
                                </li>
                                <li id="595">


                                    <a id="bibliography_27" title="Levine S, Popvic Z.Feature construction for inverse reinforcement learning[C]Proc of the 24th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2010:1342-1350" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature construction for inverse reinforcement learning">
                                        <b>[27]</b>
                                        Levine S, Popvic Z.Feature construction for inverse reinforcement learning[C]Proc of the 24th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2010:1342-1350
                                    </a>
                                </li>
                                <li id="597">


                                    <a id="bibliography_28" title="Levine S, Popvic Z.Nonlinear inverse reinforcement learning with gaussian processes[C]Proc of the 25th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2011:19-27" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlinear inverse reinforcement learning with gaussian processes">
                                        <b>[28]</b>
                                        Levine S, Popvic Z.Nonlinear inverse reinforcement learning with gaussian processes[C]Proc of the 25th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2011:19-27
                                    </a>
                                </li>
                                <li id="599">


                                    <a id="bibliography_29" title="Rasmussen C, Williams C.Guassian Processes for Machine Learning.Cambridge, MA:MIT Press, 2006" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Guassian Processes for Machine Learning">
                                        <b>[29]</b>
                                        Rasmussen C, Williams C.Guassian Processes for Machine Learning.Cambridge, MA:MIT Press, 2006
                                    </a>
                                </li>
                                <li id="601">


                                    <a id="bibliography_30" title="Jin M, Spanos C.Inverse reinforcement learning via deep gaussian process[OL]. (2015-12-26) [2017-03-04].https:arxiv.org/abs/1512.08065" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverse reinforcement learning via deep gaussian process[OL]">
                                        <b>[30]</b>
                                        Jin M, Spanos C.Inverse reinforcement learning via deep gaussian process[OL]. (2015-12-26) [2017-03-04].https:arxiv.org/abs/1512.08065
                                    </a>
                                </li>
                                <li id="603">


                                    <a id="bibliography_31" title="LuCun Y, Chopra S, Hadsell R.A tutorial on energy-based learning[J].Predicting Structured Data, 2006, 1 (1) :1-59" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Tutorial on EnergyBased Learning">
                                        <b>[31]</b>
                                        LuCun Y, Chopra S, Hadsell R.A tutorial on energy-based learning[J].Predicting Structured Data, 2006, 1 (1) :1-59
                                    </a>
                                </li>
                                <li id="605">


                                    <a id="bibliography_32" title="Wulfmeier M, Ondruska P, Posner I.Maximum entropy deep inverse reinforcement learning[OL]. (2015-07-17) [2016-03-11].https:arxiv.org/abs/1507.04888" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximum entropy deep inverse reinforcement learning[OL]">
                                        <b>[32]</b>
                                        Wulfmeier M, Ondruska P, Posner I.Maximum entropy deep inverse reinforcement learning[OL]. (2015-07-17) [2016-03-11].https:arxiv.org/abs/1507.04888
                                    </a>
                                </li>
                                <li id="607">


                                    <a id="bibliography_33" title="Ramachandran D, Amir E.Bayesian inverse reinforcement learning[C]Proc of the 20th Int Joint Conf on Artificial Intelligence (IJCAI) .Burlington:Morgan Kaufmann, 2007:2586-2591" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bayesian inverse reinforcement learning">
                                        <b>[33]</b>
                                        Ramachandran D, Amir E.Bayesian inverse reinforcement learning[C]Proc of the 20th Int Joint Conf on Artificial Intelligence (IJCAI) .Burlington:Morgan Kaufmann, 2007:2586-2591
                                    </a>
                                </li>
                                <li id="609">


                                    <a id="bibliography_34" title="Choi J, Kim K.Bayesain nonparametric feature construction for inverse reinforcement learning[C]Proc of the 27th Int Joint Conf on Artificial Intelligence (IJCAI) .San Francisco, MA:Morgan Kaufmann, 2013:1287-1293" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bayesain nonparametric feature construction for inverse reinforcement learning">
                                        <b>[34]</b>
                                        Choi J, Kim K.Bayesain nonparametric feature construction for inverse reinforcement learning[C]Proc of the 27th Int Joint Conf on Artificial Intelligence (IJCAI) .San Francisco, MA:Morgan Kaufmann, 2013:1287-1293
                                    </a>
                                </li>
                                <li id="611">


                                    <a id="bibliography_35" title="Nguyen Q, Low K, Jaillet P.Inverse reinforcement learning with locally consistent reward functions[C]Proc of the29th Conf on Advances in Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2015:1747-1755" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverse reinforcement learning with locally consistent reward functions">
                                        <b>[35]</b>
                                        Nguyen Q, Low K, Jaillet P.Inverse reinforcement learning with locally consistent reward functions[C]Proc of the29th Conf on Advances in Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2015:1747-1755
                                    </a>
                                </li>
                                <li id="613">


                                    <a id="bibliography_36" title="Choi J, Kim K.Inverse reinforcement learning in partially observable environments[J].Journal of Machine Learning Research, 2011, 12 (2) :1028-1033" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverse reinforcement learning in partially observable environments">
                                        <b>[36]</b>
                                        Choi J, Kim K.Inverse reinforcement learning in partially observable environments[J].Journal of Machine Learning Research, 2011, 12 (2) :1028-1033
                                    </a>
                                </li>
                                <li id="615">


                                    <a id="bibliography_37" title="Abbeel P, Ng A.Exploration and apprenticeship learning in reinforcement learning[C]Proc of the 22nd Int Conf on Machine Learning (ICML) .Cambridge, MA:MIT Press, 2005:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploration and apprenticeship learning in reinforcement learning">
                                        <b>[37]</b>
                                        Abbeel P, Ng A.Exploration and apprenticeship learning in reinforcement learning[C]Proc of the 22nd Int Conf on Machine Learning (ICML) .Cambridge, MA:MIT Press, 2005:1-8
                                    </a>
                                </li>
                                <li id="617">


                                    <a id="bibliography_38" title="Levine S, Abbeel P.Learning neural network policies with guided policy search under unknown dynamics[C]Proc of the 28th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:1071-1079" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning neural network policies with guided policy search under unknown dynamics">
                                        <b>[38]</b>
                                        Levine S, Abbeel P.Learning neural network policies with guided policy search under unknown dynamics[C]Proc of the 28th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:1071-1079
                                    </a>
                                </li>
                                <li id="619">


                                    <a id="bibliography_39" title="Finn C, Abbeel P, Levine S.Model-agnostic meta-learning for fast adaptation of deep networks[OL]. (2017-03-09) [2017-07-18].https:arxiv.org/abs/1703.03400" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model-agnostic meta-learning for fast adaptation of deep networks[OL]">
                                        <b>[39]</b>
                                        Finn C, Abbeel P, Levine S.Model-agnostic meta-learning for fast adaptation of deep networks[OL]. (2017-03-09) [2017-07-18].https:arxiv.org/abs/1703.03400
                                    </a>
                                </li>
                                <li id="621">


                                    <a id="bibliography_40" title="Duan Y, Andrychowicz M, Stadie B, et al.One-shot imitation learning[OL]. (2017-05-21) [2017-12-04].https:arxiv.org/abs/1703.07326" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One-shot imitation learning[OL]">
                                        <b>[40]</b>
                                        Duan Y, Andrychowicz M, Stadie B, et al.One-shot imitation learning[OL]. (2017-05-21) [2017-12-04].https:arxiv.org/abs/1703.07326
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-29 13:15</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),254-261 DOI:10.7544/issn1000-1239.2019.20170578            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于逆强化学习的示教学习方法综述</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%87%AF%E5%B3%B0&amp;code=41253832&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张凯峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BF%9E%E6%89%AC&amp;code=08720944&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">俞扬</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E4%BB%B6%E6%96%B0%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%AD%A6)&amp;code=0069758&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机软件新技术国家重点实验室(南京大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着强化学习在自动机器人控制、复杂决策问题上的广泛应用, 强化学习逐渐成为机器学习领域中的一大研究热点.传统强化学习算法是一种通过不断与所处环境进行自主交互并从中得到策略的学习方式.然而, 大多数多步决策问题难以给出传统强化学习所需要的反馈信号.这逐渐成为强化学习在更多复杂问题中实现应用的瓶颈.逆强化学习是基于专家决策轨迹最优的假设, 在马尔可夫决策过程中逆向求解反馈函数的一类算法.目前, 通过将逆强化学习和传统正向强化学习相结合设计的一类示教学习算法已经在机器人控制等领域取得了一系列成果.对强化学习、逆强化学习以及示教学习方法做一定介绍, 此外还介绍了逆强化学习在应用过程中所需要解决的问题以及基于逆强化学习的示教学习方法.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A4%BA%E6%95%99%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">示教学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">逆强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马尔可夫决策过程;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%AD%A5%E5%86%B3%E7%AD%96%E9%97%AE%E9%A2%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多步决策问题;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *俞扬 (yuy@nju.edu.cn) ;
                                </span>
                                <span>
                                    张凯峰 zhangkf@lamda.nju.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-08-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>江苏省自然科学基金项目 (BK20160066);</span>
                    </p>
            </div>
                    <h1><b>Methodologies for Imitation Learning via Inverse Reinforcement Learning: A Review</b></h1>
                    <h2>
                    <span>Zhang Kaifeng</span>
                    <span>Yu Yang</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory for Novel Software Technology (Nanjing University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Motivated by applying reinforcement learning methods into autonomous robotic systems and complex decision making problems, reinforcement learning is becoming more and more popular in the community of machine learning. Traditional reinforcement learning is one kind of learning paradigm in machine learning field which is learning from the interactions between the agent and the environment. However, for the vast majority of cases, the environments for sequential decision making problems cannot provide an explicit reward signal immediately or the reward signal can be much delayed. This becomes the bottleneck for applying reinforcement learning methods into more complex tasks. So inverse reinforcement learning is proposed to recover the reward function from expert demonstrations in the Markov decision process (MDP) by assuming that the expert demonstrations is optimal. So far, the imitation learning algorithms which combines direct reinforcement learning approaches and inverse reinforcement learning approaches have already made a great progress. This paper briefly introduces the basic concepts of reinforcement learning, inverse reinforcement learning and imitation learning. And this paper also gives an introduction to the existing problems concerning with inverse reinforcement learning and some other methods in imitation learning. In addition, we also introduce some existing bottlenecks once applying the above methods into real world applications.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=imitation%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">imitation learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=inverse%20reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">inverse reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Markov%20decision%20process&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Markov decision process;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-step%20decision%20problem&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-step decision problem;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Zhang Kaifeng, born in 1994.Master candidate of the Department of Computer Science and Technology, Nanjing University.His main research interests include machine learning and optimization.<image id="540" type="formula" href="images/JFYZ201902003_54000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Yu Yang, born in 1982.PhD and associate professor in the Department of Computer Science and Technology, Nanjing University.His research interests mainly include machine learning and reinforcement learning.<image id="542" type="formula" href="images/JFYZ201902003_54200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-08-11</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the Natural Science Foundation of Jiangsu Province (BK20160066);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="354">强化学习 (reinforcement learning, RL) <sup><a class="sup">[1]</a></sup>是机器学习的重要分支之一.在强化学习中, 智能体 (agent) 通过不断与其所处环境 (environment) 自主交互从而进行学习并完成任务.在交互过程中, 智能体将基于最大化累积反馈奖赏的目标对自身策略不断进行优化更新.该过程可以被认为是 (正向) 强化学习过程.与传统监督学习不同的是, 强化学习天生具有一定的“自学”能力, 可以自主地对环境进行探索学习.因此, 强化学习能够被有效地应用到许多标记数据代价高昂的自主学习问题当中去, 这包括:推荐系统、自动驾驶、智能机器人、Atari游戏等.在强化学习中, 如图1所示, 智能体通过观测所处环境的状态, 在动作空间选取合适动作予以执行.环境将依据相应状态转换概率转换至新的状态, 并给予智能体一定反馈奖赏.这个过程可以始终执行下去, 也可以在智能体观测到终止状态后停止.</p>
                </div>
                <div class="area_img" id="355">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902003_355.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 强化学习过程" src="Detail/GetImg?filename=images/JFYZ201902003_355.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 强化学习过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902003_355.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Reinforcement learning procedure</p>

                </div>
                <div class="p1">
                    <p id="356">然而对于绝大多数决策问题而言, 环境将难以给出准确的即时反馈信号或者环境给出的反馈信号将具有很高的延迟性.例如在自动驾驶问题中, 对于行驶过程中的车辆, 环境很难在车辆每执行一个动作后即时地给出反馈信号;而在围棋这一类游戏之中, 在每一步的落子后环境也很难立即评价该步的好坏, 而往往需要经过多步之后才能来判断之前一步的好坏, 这也就是环境所给予的反馈信号延迟性较高的情况.在上述情况下, 更为直接的方式是利用大量人类专家的决策数据进行学习从而得到智能体的策略.这样的学习方式被称为示教学习或模仿学习 (imitation learning) <sup><a class="sup">[2]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="357">示教学习的目标是模仿专家的决策轨迹进行决策, 其中每条专家决策轨迹{<i>ζ</i><sub>1</sub>, <i>ζ</i><sub>2</sub>, …, <i>ζ</i><sub><i>m</i></sub>}包括了一系列的状态-动作对<i>ζ</i><sub><i>i</i></sub>=&lt;<i>s</i><sub><i>i</i>1</sub>, <i>a</i><sub><i>i</i>1</sub>, <i>s</i><sub><i>i</i>2</sub>, <i>a</i><sub><i>i</i>2</sub>, …, <i>s</i><sub><i>in</i></sub>, <i>a</i><sub><i>in</i></sub>&gt;.近年来, 示教学习先后通过学习人类飞行员的飞行操作数据、道路导航数据以及自动系统控制数据等, 在Stanford自动直升机<sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup>、导航<sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup>以及HVAC控制<sup><a class="sup">[13]</a></sup>等项目中取得了一系列成果.</p>
                </div>
                <div class="p1">
                    <p id="358">根据模拟专家行为的不同实现过程, 示教学习可以被划分为以下3种实现方式:</p>
                </div>
                <div class="p1">
                    <p id="359">1) 行为克隆 (behavioral cloning) <sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup>.通过传统监督学习方法建立状态-动作之间的分类模型 (针对离散动作空间) 或回归模型 (针对连续动作空间) , 从而实现决策, 也即动作的预测.然而, 由于该类方法在大规模状态空间下所得到的策略存在严重的复合误差 (compounding errors) <sup><a class="sup">[16]</a></sup>并且难以有效学习到专家决策行为的动机.因此, 该类方法需要设计人工标记数据的方法进行矫正, 例如DAgger等<sup><a class="sup">[17]</a></sup>, 且仅适用于状态空间较小的情况.</p>
                </div>
                <div class="p1">
                    <p id="360">2) 基于逆强化学习的示教学习方法.逆强化学习的目标是通过在马尔可夫决策过程上建立合适的优化模型, 逆向求解得到决策问题的反馈函数.通过结合传统的正向强化学习方法设计的一系列示教学习方法, 例如学徒学习 (apprenticeship learning) <sup><a class="sup">[18]</a></sup>、代价指导学习 (guided cost learning) <sup><a class="sup">[19]</a></sup>等, 能够更好地解决大规模状态空间所带来的问题, 因而在众多机器人项目中得到了广泛的应用.值得说明的是, 部分研究工作也认为逆强化学习是示教学习方法的一种<sup><a class="sup">[20]</a></sup>, 这是由于该类方法在工作过程中通过不断的正向策略搜索进而优化算法所需要的反馈信号, 因此整个系统 (逆强化学习) 可以被认为是一类示教学习方法.</p>
                </div>
                <div class="p1">
                    <p id="361">3) 基于博弈的示教学习方法.经典的示教学习过程可以看作是智能体和所处环境进行博弈的过程.其中系统依据其混合策略<i>P</i><sub><i>t</i></sub>在动作空间选取动作, 环境依据相应混合策略<i>Q</i><sub><i>t</i></sub>选取状态, 同时系统将观测到自身在执行决策之后所得到的损失值.相关的经典工作包括通过已有自适应博弈方法<sup><a class="sup">[21]</a></sup>来优化学徒学习的MWAL算法<sup><a class="sup">[22]</a></sup>, 以及生成式对抗性示教学习方法<sup>[<a class="sup">20</a>,<a class="sup">23</a>,<a class="sup">24</a>]</sup>, 通过生成器 (generator) 生成策略, 由判别器 (discriminator) 判断其是否是来自专家决策数据抑或是生成器生成的策略数据, 通过训练2个学习器, 寻找最优策略.</p>
                </div>
                <h3 id="362" name="362" class="anchor-tag"><b>1</b><b>基本概念</b></h3>
                <div class="p1">
                    <p id="363">在强化学习中, 马尔可夫决策过程<sup><a class="sup">[1]</a></sup>可以形式化为一个五元组&lt;<i>S</i>, <i>A</i>, <i>T</i>, <i>R</i>, <i>γ</i>&gt;表示.其中, <i>S</i>表示强化学习智能体所处环境的状态空间;<i>A</i>表示智能体可选取动作的动作空间;<i>T</i>表示状态转换概率模型;<i>R</i>表示环境在某个状态-动作对下所给予的反馈信号;<i>γ</i>表示反馈奖赏折扣系数.通常, 强化学习所面对的任务的状态转换模型以及反馈量需要通过智能体不断地探索 (exploration) 从而获取相关信息.</p>
                </div>
                <div class="p1">
                    <p id="364">智能体的目标是通过和环境的不断交互最大化自身策略的未来累计反馈奖赏值.其交互过程为:智能体在某个状态<i>s</i><sub>0</sub>出发, 根据策略在动作空间选取动作<i>a</i><sub>1</sub>执行, 此时环境将依据其状态转换模型转换到下一个状态, 同时将给予智能体一个确定的反馈奖赏.该过程将不断进行直到终止状态.其中智能体的策略<i>π</i>是指状态空间到动作空间的映射.</p>
                </div>
                <h4 class="anchor-tag" id="365" name="365"><b>1.1</b><b>值函数</b></h4>
                <div class="p1">
                    <p id="366">与动态规划算法类似的是, 我们可以为每个状态定义一个值函数 (value function) , 这将为强化学习的实现带来很大方便.值函数根据其自变量的不同可以分为:状态值函数<i>V</i> (<i>s</i>) 和状态-动作对值函数<i>Q</i> (<i>s</i>, <i>a</i>) .其表述形式分别为</p>
                </div>
                <div class="p1">
                    <p id="367"><mathml id="368"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>|</mo><mrow><mi>s</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></mrow><mo>=</mo><mi>s</mi><mo stretchy="false">]</mo></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="369"><mathml id="370"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mi>π</mi></msub><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mi>r</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>|</mo><mrow><mi>s</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></mrow><mo>=</mo><mi>s</mi><mo>, </mo><mi>a</mi><msub><mrow></mrow><mn>0</mn></msub><mo>=</mo><mi>a</mi><mo stretchy="false">]</mo></mrow></math></mathml>. (2) </p>
                </div>
                <div class="p1">
                    <p id="371">可以看出:状态值函数或者状态-动作对值函数分别是某个状态、状态-动作对下的累计未来反馈奖赏.因此只需要通过最大化值函数就可以最大化累计反馈奖赏, 这使得强化学习策略求解更加方便.</p>
                </div>
                <div class="p1">
                    <p id="372">基于最优策略, 我们不难得到以下2个定理:</p>
                </div>
                <div class="p1">
                    <p id="373"><b>定理1</b>. Bellman等式.假设马尔可夫决策过程为<i>M</i>=&lt;<i>S</i>, <i>A</i>, <i>T</i>, <i>R</i>, <i>γ</i>&gt;, 智能体策略为<i>π</i>:<i>S</i>→<i>A</i>, 对于任意状态<i>s</i>、动作<i>a</i>, 其价值函数可以表示为</p>
                </div>
                <div class="p1">
                    <p id="374"><mathml id="375"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>+</mo><mi>γ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo>′</mo></msup><mo>∈</mo><mi>S</mi></mrow></munder><mi>Τ</mi></mstyle><msub><mrow></mrow><mrow><mi>s</mi><mi>π</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mi>V</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>, (3) </p>
                </div>
                <div class="p1">
                    <p id="376"><mathml id="377"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>+</mo><mi>γ</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo>′</mo></msup><mo>∈</mo><mi>S</mi></mrow></munder><mi>Τ</mi></mstyle><msub><mrow></mrow><mrow><mi>s</mi><mtext> </mtext><mi>a</mi></mrow></msub><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mi>V</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>. (4) </p>
                </div>
                <div class="p1">
                    <p id="378"><b>定理2</b>. Bellman最优定理.假设马尔可夫决策过程为<i>M</i>=&lt;<i>S</i>, <i>A</i>, <i>T</i>, <i>R</i>, <i>γ</i>&gt;, 智能体策略为<i>π</i>:<i>S</i>→<i>A</i>, 则策略<i>π</i>是最优策略当且仅当对任意状态<i>s</i>:</p>
                </div>
                <div class="p1">
                    <p id="379"><mathml id="380"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>π</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>∈</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mspace width="0.25em" /><mi>Q</mi><msup><mrow></mrow><mi>π</mi></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mrow></math></mathml>. (5) </p>
                </div>
                <h4 class="anchor-tag" id="381" name="381"><b>1.2</b><b>策略搜索</b></h4>
                <div class="p1">
                    <p id="382">经典的正向强化学习研究是智能体基于最大化累计未来反馈奖赏求解策略的过程.而求解策略可以通过求解值函数实现.</p>
                </div>
                <div class="p1">
                    <p id="383">根据1.1节所述, 求解值函数可以通过式 (6) 和式 (7) 展开进行:</p>
                </div>
                <div class="p1">
                    <p id="384" class="code-formula">
                        <mathml id="384"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mspace width="0.25em" /><mi>E</mi><mo stretchy="false">[</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>V</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo stretchy="false">]</mo><mo>=</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="385"><mathml id="386"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo>′</mo></msup><mo>∈</mo><mi>S</mi></mrow></munder><mi>Τ</mi></mstyle><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>[<i>R</i> (<i>s</i>, <i>a</i>) +<i>γV</i><sup>*</sup> (<i>s</i>′) ], (6) </p>
                </div>
                <div class="p1">
                    <p id="387" class="code-formula">
                        <mathml id="387"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><msup><mi>a</mi><mo>′</mo></msup><mo>∈</mo><mi>A</mi></mrow></munder><mspace width="0.25em" /><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">|</mo></mtd></mtr><mtr><mtd><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="388"><mathml id="389"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>s</mi><mo>′</mo></msup><mo>∈</mo><mi>S</mi></mrow></munder><mi>Τ</mi></mstyle><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo>, </mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">[</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>+</mo><mi>γ</mi><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mspace width="0.25em" /><mi>Q</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></math></mathml>. (7) </p>
                </div>
                <div class="p1">
                    <p id="390">通过式 (6) (7) 求解值函数从而获得最优策略的方法可以理解为策略迭代过程, 也即通过不断迭代以下2个交互过程:策略评估 (policy evaluation) 和策略改进 (policy improvement) , 从而获取最优策略.其中, 策略评估是指通过当前的策略评估值函数, 而策略改进是指通过当前值函数优化得到新的策略.这个过程就是经典的正向强化学习过程.</p>
                </div>
                <h3 id="391" name="391" class="anchor-tag"><b>2</b><b>逆强化学习</b></h3>
                <div class="p1">
                    <p id="392">逆强化学习是通过大量专家决策数据在马尔可夫决策过程中逆向求解环境反馈信号函数的一类方法.其基本原则是寻找一个或多个反馈信号函数能够很好地描述专家决策行为.这也就是说, 逆强化学习算法将基于专家决策最优的假设进行设计.</p>
                </div>
                <div class="p1">
                    <p id="393">然而, 由于在函数空间中可能存在多个函数能够同时满足专家策略最优的假设, 例如每一步决策所带来的反馈始终为0的情况.因此, 算法设计的模型应能够解决反馈信号的模糊性 (ambiguity) .目前, 我们可以通过3类反馈信号函数的形式实现反馈信号求解过程, 它们分别是:1) 基于大间隔 (max-margin) 的反馈信号;2) 基于确定基函数组合的反馈信号函数;3) 基于参数化的反馈信号函数, 例如神经网络.</p>
                </div>
                <h4 class="anchor-tag" id="394" name="394"><b>2.1</b><b>基于确定基函数组合的反馈信号函数</b></h4>
                <div class="p1">
                    <p id="395">逆强化学习发展初期, 大多工作均建立在环境反馈信号函数为确定基函数组合的情况下.该类方法通过状态特征构建基函数, 从而将求解反馈信号函数的任务转化为求解各个基函数权重的任务.其能够较好地克服反馈信号搜索过程中存在的函数歧义性的问题.</p>
                </div>
                <div class="p1">
                    <p id="396">为了建立合适的优化模型求解相关决策问题的反馈信号, 该类方法从专家决策轨迹最优的假设出发, 通过以下2种方法建立相关模型:</p>
                </div>
                <div class="p1">
                    <p id="397">1) 根据1.1节中的Bellman最优等式, 我们可以推知:最优策略相对于其他任何策略, 在状态空间上具有最大动作价值总和, 即对于在动作集中任意非<i>a</i><sub>1</sub>的动作都有:<mathml id="398"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>-</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mi>A</mi><mo>\</mo><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub><mspace width="0.25em" /></mrow></munder><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>的值最大, 其中为了便于表示, 我们假设<i>a</i><sub>1</sub>表示为最优决策动作.因此, 逆强化学习的目标也就是使得专家决策数据所确定的策略具有最高<i>Q</i>总和.类似的目标函数还包括:<mathml id="399"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></munder><mrow></mrow></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml> (<i>Q</i><sup><i>π</i></sup> (<i>s</i>, <i>a</i><sub>1</sub>) -<i>Q</i><sup><i>π</i></sup> (<i>s</i>, <i>a</i>) ) 等<sup><a class="sup">[25]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="400">在上述优化目标的基础上, 我们可以考虑逆强化学习问题的约束条件还应包括:<i>a</i><sub>1</sub>为最优决策动作, 根据定理2可以得知, 该条件等价于<i>a</i><sub>1</sub>动作在相应状态下的<i>Q</i>值将大于其余动作的<i>Q</i>值.此外, 约束条件中还应保证立即反馈信号值始终是有限值.当考虑到对模型进行正则化时, 我们可以得到Ng等人<sup><a class="sup">[25]</a></sup>提出的针对专家决策轨迹的优化模型, 如式 (8) 所示:</p>
                </div>
                <div class="p1">
                    <p id="401" class="code-formula">
                        <mathml id="401"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext><mtext>i</mtext><mtext>m</mtext><mtext>i</mtext><mtext>z</mtext><mtext>e</mtext><mspace width="0.25em" /><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mspace width="0.25em" /></mstyle><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>a</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>a</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">}</mo></mrow></munder><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mo stretchy="false"> (</mo><mi>Ι</mi><mo>-</mo><mi>γ</mi><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">}</mo><mi mathvariant="bold-italic">R</mi><mo>-</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">R</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>Ι</mi><mo>-</mo><mi>γ</mi><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">R</mi><mo>≻</mo><mn>0</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="402" class="code-formula">
                        <mathml id="402"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>≤</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="403">其中, <b><i>P</i></b><sub><i>a</i></sub> (<i>i</i>) 表示状态转换概率矩阵.矩阵<b><i>R</i></b>表示反馈量矩阵.其中模型约束条件</p>
                </div>
                <div class="p1">
                    <p id="404"> (<b><i>P</i></b><sub><i>a</i><sub>1</sub></sub> (<i>i</i>) -<b><i>P</i></b><sub><i>a</i></sub> (<i>i</i>) ) (<i>I</i>-<i>γ</i>·<b><i>P</i></b><sub><i>a</i><sub>1</sub></sub>) <sup>-1</sup><b><i>R</i></b>≻0, </p>
                </div>
                <div class="p1">
                    <p id="405">表示左侧矩阵各项元素均大于0, 以保证<i>a</i><sub>1</sub>为最优决策.|<i>R</i><sub><i>i</i></sub>|≤<i>R</i><sub>max</sub>亦表示矩阵中各项元素均小于某个有限值.当考虑到决策问题的反馈函数可以由一组确定的基函数线性拟合时, 该优化模型可以很好地通过线性规划 (linear programming) 求解得到相应环境的反馈函数.</p>
                </div>
                <div class="p1">
                    <p id="406">2) 根据强化学习基于动态规划算法最大化未来反馈量的经典研究我们可以得知:最优策略相对于其他策略而言将获得最大的未来奖赏, 即:</p>
                </div>
                <div class="p1">
                    <p id="407" class="code-formula">
                        <mathml id="407"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msup><mrow></mrow><mi>π</mi></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></munderover><mi>R</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="408">将取得最大值.</p>
                </div>
                <div class="p1">
                    <p id="409">当决策问题的反馈信号可以由一系列确定的基函数<i>φ</i><sub>1</sub>, <i>φ</i><sub>2</sub>, …, <i>φ</i><sub><i>k</i></sub>线性组合而成时, 我们可以定义策略的特征期望<sup><a class="sup">[18]</a></sup>为</p>
                </div>
                <div class="p1">
                    <p id="410" class="code-formula">
                        <mathml id="410"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>π</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mspace width="0.25em" /></mstyle><mi>γ</mi><msup><mrow></mrow><mi>t</mi></msup><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">φ</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mi>π</mi></mrow><mo stretchy="false">]</mo><mo>∈</mo><mi>R</mi><msup><mrow></mrow><mi>k</mi></msup><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="411">由此, 我们可以得到对于任意策略特征期望<i>μ</i>, 可以得到:</p>
                </div>
                <div class="p1">
                    <p id="412"><b><i>w</i></b><sup>T</sup><i>μ</i><sub>E</sub>≥<b><i>w</i></b><sup>T</sup><i>μ</i>.</p>
                </div>
                <div class="p1">
                    <p id="413">其中, <i>μ</i><sub>E</sub>表示专家决策数据所确定的专家策略特征期望, 其值可以通过蒙特卡洛算法进行估算:</p>
                </div>
                <div class="p1">
                    <p id="414"><mathml id="415"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">μ</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>E</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle></mrow></mstyle><msup><mrow></mrow><mi>t</mi></msup><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>s</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="416">通过建立优化模型:</p>
                </div>
                <div class="p1">
                    <p id="417" class="code-formula">
                        <mathml id="417"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>t</mi><mo>, </mo><mi mathvariant="bold-italic">w</mi></mrow></munder><mspace width="0.25em" /><mi>t</mi></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mtext>E</mtext></msub><mo>≥</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo>+</mo><mi>t</mi><mo>, </mo><mi>j</mi><mo>=</mo><mn>0</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="418" class="code-formula">
                        <mathml id="418"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn><mo>.</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="419">我们可以得到以下结论:当优化变量<i>t</i>不大于拟合误差<i>ε</i>时, 算法将得到决策问题反馈信号优化变量<b><i>w</i></b>, 也即得到未来总反馈函数<i>R</i>=<b><i>w</i></b><sup>T</sup><i>μ</i>.此时, 由于<i>t</i>≤<i>ε</i>, 也将得到相应策略, 其未来奖赏值<b><i>w</i></b><sup>T</sup><i>μ</i> (<i>i</i>) ≥<b><i>w</i></b><sup>T</sup><i>μ</i><sub>E</sub>-<i>ε</i>, 也即结合不同正向强化学习策略搜索方法设计的示教学习方法得到的策略将不低于专家策略减去某小量的水平.</p>
                </div>
                <div class="p1">
                    <p id="420">通过上述2种方式建立的逆强化学习优化模型可以帮助求解得到相关问题的反馈信号.该类方法通过比较专家策略和其他策略的价值, 从而建立逆强化学习优化模型, 能够较好地实现对专家决策轨迹的学习, 并获取环境反馈信号函数.</p>
                </div>
                <div class="p1">
                    <p id="421">但是由于在实际结合正向强化学习设计的大多示教学习算法中, 逆强化学习模型将被嵌套在策略搜索的循环中, 因此降低逆强化学习算法的复杂度得到极大的关注.其中较为经典的工作有2008年Syed等人<sup><a class="sup">[26]</a></sup>提出的LPAL算法, 通过设计学徒学习的对偶模型, 并通过线性规划方法求解该线性模型的占有度 (occupancy measure) , 其算法最坏时间复杂度仅为<mathml id="422"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false"> (</mo><mo stretchy="false">|</mo><mi>S</mi><mrow><mo>‖</mo><mi>A</mi></mrow><mo stretchy="false">|</mo><mo>+</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 相对于此前2004年Abbeel等人<sup><a class="sup">[18]</a></sup>提出的学徒学习方法以及2007年通过自适应博弈理论优化后的MWAL算法<sup><a class="sup">[22]</a></sup>在时间效率上都有了极大的提高.</p>
                </div>
                <h4 class="anchor-tag" id="423" name="423"><b>2.2</b><b>基于参数化模型的反馈信号函数</b></h4>
                <div class="p1">
                    <p id="424">随着逆强化学习面对的决策问题复杂度的提升, 研究人员开始关注于提升反馈信号函数的表达能力.其中较为有效的是通过参数化模型对环境反馈信号进行建模.</p>
                </div>
                <div class="p1">
                    <p id="425">早期的致力于扩大决策问题反馈信号表达能力的工作包括:2010年Levine等人<sup><a class="sup">[27]</a></sup>提出的FIRL (feature construction for IRL) 算法, 其方法通过构建一组基于逻辑联结的合成特征, 从而间接实现了非线性反馈信号的建模.2011年, Levine等人<sup><a class="sup">[28]</a></sup>又提出了GP-IRL, 其方法采用了基于高斯过程<sup><a class="sup">[29]</a></sup>的反馈信号, 通过高斯过程极大地增强了反馈函数的表示能力.2015年, Jin等人<sup><a class="sup">[30]</a></sup>又在GP-IRL算法基础上结合了深度信念网络, 实现了深度高斯过程在逆强化学习上的应用 (DGP-IRL) .其中GP-IRL和DGP-IRL在众多开源环境测试, 例如经典的Grid-world测试实验以及gym下的强化学习基准测试实验中都取得了”state-of-the-art”的效果.</p>
                </div>
                <div class="p1">
                    <p id="426">随着深度学习的蓬勃发展, 通过神经网络对反馈函数进行建模逐渐称为逆强化学习的一大主流方向.其中较为知名的是2008年, Ziebart等人<sup><a class="sup">[11]</a></sup>提出的最大熵逆强化学习方法 (maximum entropy IRL) , 通过优化专家决策数据集的似然函数实现反馈信号的优化, 很好地解决了专家决策数据中可能存在的噪声以及专家数据本身并不是最优的问题.</p>
                </div>
                <div class="p1">
                    <p id="427">最大熵逆强化学习方法是经典的基于“能量”的模型 (energy-based model) <sup><a class="sup">[31]</a></sup>.其中能量函数ε为环境的代价函数 (即反馈信号函数的相反数) .根据“能量”模型的假设, 可以知道专家在策略轨迹空间的采样概率密度为</p>
                </div>
                <div class="p1">
                    <p id="428"><i>p</i> (<i>τ</i>) =<mathml id="429"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>τ</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>τ</mi></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>τ</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="430">其中, <i>τ</i>为策略轨迹, 分母为划分函数<i>Z</i> (partition function) .式 (10) 可以简单地理解为:当2条决策轨迹具有相同的反馈奖赏时, 其具有相同的概率别“专家”采样获得, 而当某条轨迹具有更高的反馈奖赏时, “专家”将更有机会能够采样到这条轨迹.</p>
                </div>
                <div class="p1">
                    <p id="431">为了让专家决策数据 (训练数据) 更能够被“专家”采样到, 逆强化学习的优化目标是最大化专家轨迹的似然函数, 可以表述为</p>
                </div>
                <div class="p1">
                    <p id="432">maximize<mathml id="433"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mtext> </mtext><mi>p</mi><mo stretchy="false"> (</mo><mi>τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="434"><mathml id="435"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo>-</mo></mstyle><mrow><mi>log</mi></mrow><mtext> </mtext><mi>Ζ</mi><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false"> (</mo><mi>τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>. (11) </p>
                </div>
                <div class="p1">
                    <p id="436">因此, 通过随机梯度方法优化模型式 (11) 就可以求解得到环境的反馈信号函数.此处需要注意的是:当我们面对的是离散且规模较小的状态空间时, 划分函数<i>Z</i>可以通过动态规划算法求得;而当我们面对大规模状态空间时, 则需要通过采样等方法实现<sup>[<a class="sup">19</a>,<a class="sup">32</a>]</sup>.</p>
                </div>
                <div class="p1">
                    <p id="437">最大熵逆强化学习方法通过优化专家决策数据的似然函数从而获得环境反馈信号, 该方法引入了一定的随机性, 可以处理专家决策数据本身不是最优或含有一定噪声的情况.</p>
                </div>
                <div class="p1">
                    <p id="438">类似能够处理专家决策数据本身不是最优的方法还包括一系列概率模型.其中包括:2007年, Ramachandran和Amir<sup><a class="sup">[33]</a></sup>提出贝叶斯非参数化方法去构建反馈函数特征来实现逆强化学习, 该方法称作贝叶斯逆强化学习 (Bayesian IRL) .其后2013年, Choi等人<sup><a class="sup">[34]</a></sup>通过构建了一组合成特征上的先验概率优化了该算法.</p>
                </div>
                <h4 class="anchor-tag" id="439" name="439"><b>2.3</b><b>其他函数表示形式</b></h4>
                <div class="p1">
                    <p id="440">对于某些复杂决策问题, 环境反馈信号难以通过单一的一个函数进行表示, 也就是说是通过单一函数拟合过程中会出现决策数据和反馈函数严重不一致的情况.通过基于每条专家决策轨迹都能够被多个局部一致的反馈函数所生成的假设, Nguyen等人<sup><a class="sup">[35]</a></sup>提出了通过期望最大化 (expectation-max-imization, EM) 方法来学习不同的反馈信号以及它们之间动态的转换过程.通过该方法, 可以实现针对专家决策轨迹的分割, 使得各个部分 (segments) 均能对应合适的局部一致的反馈函数.基准数据测试 (Grid-world以及gym等开源强化学习环境测试) 表明该方法也取得了”state-of-the-art”的效果.</p>
                </div>
                <div class="p1">
                    <p id="441">此外, 逆强化学习领域仍有很多问题需要进行研究解决.例如, 在考虑到部分可观察的环境 (partially observable environments) <sup><a class="sup">[36]</a></sup>时, 如何有效地将逆强化学习或示教学习方法迁移到这样的环境之中、如何设计实验来提高反馈函数的可识别性 (identifiablity) 等问题.</p>
                </div>
                <h3 id="442" name="442" class="anchor-tag"><b>3</b><b>基于逆强化学习的示教学习方法</b></h3>
                <div class="p1">
                    <p id="443">示教学习的目标是通过专家决策轨迹去模仿专家的决策行为.本文第2节介绍了逆强化学习的方法和所需解决的问题, 逆强化学习是通过学习专家决策轨迹从而获得环境反馈信号的一类方法.本节将介绍通过结合逆强化学习、正向强化学习策略搜索算法所设计的示教学习方法, 也即基于逆强化学习的示教学习方法.</p>
                </div>
                <div class="p1">
                    <p id="444">目前, 基于逆强化学习的示教学习主要的2个框架分别是:1) 在经典的正向强化学习算法内循环中使用逆强化学习算法优化问题的反馈信号, 基于反馈信号函数继续实现策略的优化, 不断迭代实现示教学习过程.其核心在于将逆强化学习方法置于正向策略搜索方法的内循环之中, 经典的方法包括学徒学习方法等.2) 基于不断优化得到的反馈信号去实现正向强化学习过程, 通过采样数据和专家数据相结合实现逆强化学习过程, 同时将正向强化学习过程置于逆强化学习的内循环中, 经典的方法有代价指导学习等.本节将主要介绍学徒学习方法和代价指导学习方法.</p>
                </div>
                <h4 class="anchor-tag" id="445" name="445"><b>3.1</b><b>学徒学习</b></h4>
                <div class="p1">
                    <p id="446">学徒学习方法是通过在马尔可夫决策过程中, 模仿专家行为, 最终得到不差于专家行为策略的方法.其核心的思想是通过匹配专家期望特征实现模仿学习过程.</p>
                </div>
                <div class="p1">
                    <p id="447">在线性假设下, 反馈信号可以由一组确定基函数<i>φ</i><sub>1</sub>, <i>φ</i><sub>2</sub>, …, <i>φ</i><sub><i>k</i></sub>进行线性组合.因此, 策略的价值可以表示为</p>
                </div>
                <div class="p1">
                    <p id="448"><i>E</i><sub><i>s</i><sub>0</sub>～<i>D</i></sub>[<i>V</i><sup><i>π</i></sup> (<i>s</i><sub>0</sub>) ]=<mathml id="449"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mi>R</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mi>π</mi></mrow><mo stretchy="false">]</mo><mo>=</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="450"><mathml id="451"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mi mathvariant="bold-italic">w</mi><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mi>π</mi></mrow><mo stretchy="false">]</mo><mo>=</mo><mi mathvariant="bold-italic">w</mi></mrow></math></mathml>·<mathml id="452"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mi>π</mi></mrow><mo stretchy="false">]</mo></mrow></math></mathml>. (12) </p>
                </div>
                <div class="p1">
                    <p id="453">我们可以发现如果有这样一个策略<i>π</i>, 其特征期望满足<mathml id="454"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>π</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mtext>E</mtext></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>ε</mi></mrow></math></mathml>时, 有:</p>
                </div>
                <div class="p1">
                    <p id="455" class="code-formula">
                        <mathml id="455"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>|</mo><mrow><mi>E</mi><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mi>R</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mrow><mi>π</mi><msub><mrow></mrow><mtext>E</mtext></msub></mrow></mrow><mo stretchy="false">]</mo><mo>-</mo><mi>E</mi><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>∞</mi></munderover><mi>γ</mi></mstyle><msup><mrow></mrow><mi>t</mi></msup><mi>R</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mi>π</mi></mrow><mo stretchy="false">]</mo></mrow><mo>|</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>π</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mtext>E</mtext></msub></mrow><mo>|</mo></mrow><mo>≤</mo></mtd></mtr><mtr><mtd><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>π</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mtext>E</mtext></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>ε</mi><mo>.</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="456">因此, 我们可以得到以下结论:对于某个策略<i>π</i>, 若其特征期望接近专家策略特征期望, 则该策略是学徒学习的一个解.算法1描述了由Abbeel等人<sup><a class="sup">[18]</a></sup>提出的通过结合策略迭代和式 (9) 的逆强化学习算法所设计的学徒学习方式.</p>
                </div>
                <div class="p1">
                    <p id="457"><b>算法1</b>. 学徒学习算法.</p>
                </div>
                <div class="p1">
                    <p id="458">输入:专家决策行为数据;</p>
                </div>
                <div class="p1">
                    <p id="459">输出:算法得到的策略以及相应的反馈函数.</p>
                </div>
                <div class="p1">
                    <p id="460">① 随机初始化一个策略, 计算其特征期望:</p>
                </div>
                <div class="p1">
                    <p id="461"><i>μ</i> (0) =<i>μ</i> (<i>π</i> (0) ) , 设置<i>i</i>=1;</p>
                </div>
                <div class="p1">
                    <p id="462">② 计算:</p>
                </div>
                <div class="p1">
                    <p id="463" class="code-formula">
                        <mathml id="463"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>:</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn></mrow></munder><mtext> </mtext><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>j</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">}</mo></mrow></munder><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mtext>E</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="464">并且获得相应<b><i>w</i></b>值为<b><i>w</i></b> (<i>i</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="465">③ IF <i>t</i> (<i>i</i>) ≤<i>ε</i> THEN</p>
                </div>
                <div class="p1">
                    <p id="466">④ 算法终止;</p>
                </div>
                <div class="p1">
                    <p id="467">End If</p>
                </div>
                <div class="p1">
                    <p id="468">⑤ 使用强化学习算法, 计算最优策略<i>π</i> (<i>i</i>) 未来累计奖赏为<i>R</i>= (<b><i>w</i></b> (<i>i</i>) ) <sup>T</sup><i>φ</i>;</p>
                </div>
                <div class="p1">
                    <p id="469">⑥ 计算策略特征期望<i>μ</i> (<i>i</i>) =<i>μ</i> (<i>π</i> (<i>i</i>) ) ;</p>
                </div>
                <div class="p1">
                    <p id="470">⑦ 设置<i>i</i>=<i>i</i>+1, 并返回步骤②.</p>
                </div>
                <div class="p1">
                    <p id="471">算法1中步骤①初始化随机策略并计算其相应的特征期望;步骤②即为逆强化学习算法 (式 (9) ) , 其中<mathml id="472"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow></mrow></math></mathml>表示<b><i>w</i></b>的二阶范数;步骤③④判断专家策略和目前学得策略的最大策略值的差值, 若满足要求则算法终止;步骤⑤基于得到的反馈信号通过正向强化学习算法实现策略优化;步骤⑥⑦计算新策略的特征期望, 并迭代.学徒学习方法通过结合逆强化学习 (模型2) 和正向强化学习方法实现最优策略的搜索, 算法将得到一组策略, 通过以下二次规划算法我们可以相应用于决策的混合策略:</p>
                </div>
                <div class="p1">
                    <p id="473" class="code-formula">
                        <mathml id="473"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mi>min</mi></mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">μ</mi><msub><mrow></mrow><mtext>E</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">μ</mi></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="474" class="code-formula">
                        <mathml id="474"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">μ</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>λ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>λ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≥</mo><mn>0</mn><mo>, </mo><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>λ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>.</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="475">其中, <i>λ</i><sub><i>i</i></sub>可以看作是以<i>λ</i><sub><i>i</i></sub>的概率选择<i>μ</i> (<i>i</i>) 策略.</p>
                </div>
                <div class="p1">
                    <p id="476">为了将学徒学习方法应用到高性能机器人系统之中, Abbeel等人<sup><a class="sup">[37]</a></sup>通过将学徒学习和探索策略 (exploration policies) 方法结合解决动态未知的机器人环境.其后, 该项工作也被应用到了著名的Stanford自动直升机之中.</p>
                </div>
                <h4 class="anchor-tag" id="477" name="477"><b>3.2</b><b>代价指导学习</b></h4>
                <div class="p1">
                    <p id="478">代价指导学习<sup><a class="sup">[19]</a></sup>是通过结合正向强化学习中的策略优化<sup><a class="sup">[38]</a></sup> (policy optimization) 方法和最大熵逆强化学习方法<sup><a class="sup">[11]</a></sup>实现的示教学习方法.</p>
                </div>
                <div class="p1">
                    <p id="479">如图2所示, 系统通过初始化策略在机器人或设备上进行轨迹采样, 并将采样得到的轨迹和专家决策数据进行合并, 共同用于实现逆强化学习过程, 优化反馈信号函数.基于得到的反馈信号函数, 在内循环中实现策略优化.不断迭代上述过程, 最终实现示教学习过程.其实现过程如算法2所示.</p>
                </div>
                <div class="area_img" id="480">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902003_480.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 代价指导学习过程" src="Detail/GetImg?filename=images/JFYZ201902003_480.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 代价指导学习过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902003_480.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Guided cost learning procedure</p>

                </div>
                <div class="p1">
                    <p id="481"><b>算法2</b>. 代价指导学习算法.</p>
                </div>
                <div class="p1">
                    <p id="482">输入:专家决策行为数据;</p>
                </div>
                <div class="p1">
                    <p id="483">输出:算法得到的策略以及相应的反馈函数.</p>
                </div>
                <div class="p1">
                    <p id="484">① 随机初始化一个策略;</p>
                </div>
                <div class="p1">
                    <p id="485">② FOR <i>i</i>=1 to <i>I</i> DO</p>
                </div>
                <div class="p1">
                    <p id="486">③ 通过目前策略采样生成采样数据集;</p>
                </div>
                <div class="p1">
                    <p id="487">④ 扩展数据样本集:<i>D</i><sub>samp</sub>=<i>D</i><sub>samp</sub>∪<i>D</i><sub>traj</sub>;</p>
                </div>
                <div class="p1">
                    <p id="488">⑤ 通过<i>D</i><sub>samp</sub>优化问题反馈信号函数;</p>
                </div>
                <div class="p1">
                    <p id="489">⑥ 通过正向强化学习更新策略;</p>
                </div>
                <div class="p1">
                    <p id="490">⑦ END FOR</p>
                </div>
                <div class="p1">
                    <p id="491">⑧ 返回优化后的策略和相应反馈信号.</p>
                </div>
                <div class="p1">
                    <p id="492">算法2步骤①实现随机初始化策略;步骤③通过当前策略进行采样;步骤④实现采样数据集和专家决策数据集的合并;步骤⑤实现逆强化学习过程 (最大熵算法) ;步骤⑥实现策略优化;不断迭代步骤③～⑥, 实现示教学习过程.</p>
                </div>
                <div class="p1">
                    <p id="493">此外, 由于强化学习系统采样的样本有限, 一般可以通过将专家决策数据集合进行分组, 通过多组数据循环优化反馈信号, 实现逆强化学习过程.目前, 代价指导学习算法在机器人多项智能操作, 例如倒水、叠盘子等实验<sup><a class="sup">[19]</a></sup>中取得了”state-of-the-art”的效果.</p>
                </div>
                <div class="p1">
                    <p id="494">通过上述介绍的学徒学习方法和代价指导学习方法两大类实现框架, 我们能够将不同的逆强化学习和正向强化学习方法进行结合从而设计一系列示教学习算法.通过采用不同的逆强化学习方法, 我们可以处理专家决策数据本身存在的各种问题, 例如数据存在噪声、其决策过程本身并不是最优的以及反馈信号表示能力受到限制等.同样地, 通过采用不同的正向强化学习方法, 我们可以解决许多由环境所带来的问题, 例如实现在高维连续系统中的示教学习应用等.</p>
                </div>
                <h3 id="495" name="495" class="anchor-tag"><b>4</b><b>结束语</b></h3>
                <div class="p1">
                    <p id="496">本文不仅介绍了建立逆强化学习优化模型的方法以及逆强化学习方法发展回顾, 还介绍了如何通过结合逆强化学习、正向强化学习方法设计新的示教学习方法, 重点介绍了2种框架以及其具有代表性2种的经典方法:学徒学习以及代价指导学习方法.</p>
                </div>
                <div class="p1">
                    <p id="497">示教学习是通过模仿专家行为实现专家决策的学习方法.而其中, 基于逆强化学习的示教学习方法不仅能够实现针对决策数据的学习, 还能够较好地学习到专家行为的动机.</p>
                </div>
                <div class="p1">
                    <p id="498">目前, 示教学习的主要应用领域为智能机器人操控.在应用过程中, 目前示教学习方法也遇到了很多问题, 这包括:如何将示教学习算法在不同机器人之间进行迁移<sup><a class="sup">[39]</a></sup>;如何采用更少量的专家决策数据来学得较好的反馈信号<sup><a class="sup">[40]</a></sup>等.此外, 将示教学习方法应用到更多的强化学习场景当中也是我们未来的研究方向之一.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="543">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning:An Introduction">

                                <b>[1]</b>Sutton R, Barto A.Reinforcement Learning:An Introduction[M].Cambridge, MA:MIT Press, 2017
                            </a>
                        </p>
                        <p id="545">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An invitation to imitation[OL]">

                                <b>[2]</b>Bagnell J.An invitation to imitation[OL].2015[2017-02-12].https:pdfs.semanticscholar.org/f04d/3ddee335927186-b012a1bee765c142ddce57.pdf/_ga=2.181329124.233418181.1525172550-397727813.1525172550
                            </a>
                        </p>
                        <p id="547">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning vehicular dynamics with application tomodeling helicopters">

                                <b>[3]</b>Abbeel P, Ganapathi V, Ng A.Learning vehicular dynamics, with application to modeling helicopters[C]Proc of the 20th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2006:1-8
                            </a>
                        </p>
                        <p id="549">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An application of reinforcement learning to aerobatic helicopter flight">

                                <b>[4]</b>Abbeel P, Coates A, Quigley M, et al.An application of reinforcement learning to aerobatic helicopter flight[C]Proc of the 21st Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2007:1-8
                            </a>
                        </p>
                        <p id="551">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverted autonomous helicopter flight via reinforcement learning">

                                <b>[5]</b>Ng A, Coates A, Diel M, et al.Inverted autonomous helicopter flight via reinforcement learning[C]Proc of the 4th Int Symp on Experimental Robotics.New York:Springer, 2004:799-806
                            </a>
                        </p>
                        <p id="553">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning for control from multiple demonstrations">

                                <b>[6]</b>Coates A, Abbeel P, Ng A.Learning for control from multiple demonstrations[C]Proc of the 25th Int Conf on Machine Learning (ICML) .New York:ACM, 2008:144-151
                            </a>
                        </p>
                        <p id="555">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Autonomous autorotation of an RC helicopter">

                                <b>[7]</b>Abbeel P, Coates A, Hunter T, et al.Autonomous autorotation of an RC helicopter[C]Proc of the 8th Int Symp on Robotics.New York:Springer, 2008:385-394
                            </a>
                        </p>
                        <p id="557">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Autonomous Helicopter Aerobatics through Apprenticeship Learning">

                                <b>[8]</b>Abbeel P, Coates A, Ng A.Autonomous helicopter aerobatics through apprenticeship learning[J].The International Journal of Robotics Research, 2010, 29 (13) :1608-1639
                            </a>
                        </p>
                        <p id="559">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximum Margin Planning">

                                <b>[9]</b>Ratliff N, Bagnell J, Zinkevich M.Maximum margin planning[C]Proc of the 23rd Int Conf on Machine Learning (ICML) .New York:ACM, 2006:729-736
                            </a>
                        </p>
                        <p id="561">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Apprenticeship Learning for Motion Planning with Application to Parking Lot Navigation">

                                <b>[10]</b>Abbeel P, Dolov D, Ng A, et al.Apprenticeship learning for motion planning with application to parking lot navigation[C]Proc of the 21st IEEE/RSJ Int Conf on Intelligent Robots and Systems.Piscataway, NJ:IEEE, 2008:1083-1090
                            </a>
                        </p>
                        <p id="563">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximum entropy inverse reinforcement learning">

                                <b>[11]</b>Ziebart B, Maas A, Bagnell J, et al.Maximum entropy inverse reinforcement learning[C]Proc of the 23rd AAAIConf on Artificial Intelligence.Menlo Park:AAAI Press, 2008:1433-1438
                            </a>
                        </p>
                        <p id="565">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling interaction via the principle of maximum causal entropy">

                                <b>[12]</b>Ziebart B, Bagnell J, Dey A.Modeling interaction via the principle of maximum causal entropy[C]Proc of the 27th Int Conf on Machine Learning (ICML) .New York:ACM, 2010:1255-1262
                            </a>
                        </p>
                        <p id="567">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Autonomous HVAC control,a reinforcement learning approach">

                                <b>[13]</b>Barrett E, Linder S.Autonomous HVAC control, a reinforcement learning approach[C]Proc of the 25th European Conf on Machine Learning and Knowledge Discovery in Databases.Porto:CEUR-WS, 2015:3-19
                            </a>
                        </p>
                        <p id="569">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to fly">

                                <b>[14]</b>Sammut C, Hurst S, Kedzier D, et al.Learning to fly[C]Proc of the 9th Int Conf on Machine Learning (ICML) .New York:ACM, 1992:385-393
                            </a>
                        </p>
                        <p id="571">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning by watching: extracting reusable task knowledge from visual observation of human performance">

                                <b>[15]</b>Kunigoshi Y, Inaba M, Inoue H.Learning by watching:Extracting reusable task knowledge from visual observation of human performance[J].IEEE Transactions on Robotics and Automation, 1994, 10 (6) :799-822
                            </a>
                        </p>
                        <p id="573">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient reductions for imitation learning">

                                <b>[16]</b>Ross S, Bagnell D.Efficient reductions for imitation learning[C]Proc of the 13th Int Conf on Artificial Intelligence and Statistics (AISTATS) .Cambridge, MA:MIT Press, 2010:661-668
                            </a>
                        </p>
                        <p id="575">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning">

                                <b>[17]</b>Ross S, Gordon G, Bagnell J.A reduction of imitation learning and structured prediction to no-regret online learning[C]Proc of the 14th Int Conf on Artificial Intelligence and Statistics (AISTATS) .Cadiz:JMLR, 2011:627-635
                            </a>
                        </p>
                        <p id="577">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Apprenticeship learning via inversereinforcement learning">

                                <b>[18]</b>Abbeel P, Ng A.Apprenticeship learning via inverse reinforcement learning[C]Proc of the 21st Int Conf on Machine Learning (ICML) .Cadiz:JMLR, 2004:1-8
                            </a>
                        </p>
                        <p id="579">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Guided cost learning:deep inverse optimal control via policy optimization">

                                <b>[19]</b>Finn C, Levine S, Abbeel P.Guided cost learning:Deep inverse optimal control via policy optimization[C]Proc of the 33rd Int Conf on Machine Learning (ICML) .New York:ACM, 2016:49-58
                            </a>
                        </p>
                        <p id="581">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Generative adversarial imitation learning,&amp;quot;">

                                <b>[20]</b>Ho J, Ermon S.Generative adversarial imitation learning[C]Proc of the 30th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2016:4565-4573
                            </a>
                        </p>
                        <p id="583">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300002264&amp;v=MzE2NTFQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SUpsd2RiaGM9TmlmT2ZiSzdIdERPckk5RlpPc05Ebm85b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Freund Y, Schapire R.Adaptive game playing using multiplicative weights[J].Games and Economic Behavior, 1999, 29 (1/2) :79-103
                            </a>
                        </p>
                        <p id="585">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A game-theoretic approach to apprenticeship learning">

                                <b>[22]</b>Syed U, Schapire R.A game-theoretic approach to apprenticeship learning[C]Proc of the 22nd Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2008:1449-1456
                            </a>
                        </p>
                        <p id="587">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Nets">

                                <b>[23]</b>Goodfellow I, Abdaie J, Mirza M, et al.Generative adversarial nets[C]Proc of the 28th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:2672-2680
                            </a>
                        </p>
                        <p id="589">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A connection between generative adversarial networks,inverse reinforcement learning,and energy-based models[OL]">

                                <b>[24]</b>Finn C, Christiano P, Abbeel P, et al.A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models[OL]. (2016-11-11) [2016-11-25].https:arxiv.org/abs/1611.03852, 2016
                            </a>
                        </p>
                        <p id="591">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Algorithms for inverse reinforcement learning">

                                <b>[25]</b>Ng A, Russell S.Algorithms for inverse reinforcement learning[C]Proc of the 17th Int Conf on Machine Learning (ICML) .Cambridge, MA:MIT Press, 2000:663-670
                            </a>
                        </p>
                        <p id="593">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Apprenticeship learning using linear programming">

                                <b>[26]</b>Syed U, Bowling M, Schapire R.Apprenticeship learning using linear programming[C]Proc of the 25th Int Conf on Machine Learning (ICML) .New York:ACM, 2008:1032-1039
                            </a>
                        </p>
                        <p id="595">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature construction for inverse reinforcement learning">

                                <b>[27]</b>Levine S, Popvic Z.Feature construction for inverse reinforcement learning[C]Proc of the 24th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2010:1342-1350
                            </a>
                        </p>
                        <p id="597">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlinear inverse reinforcement learning with gaussian processes">

                                <b>[28]</b>Levine S, Popvic Z.Nonlinear inverse reinforcement learning with gaussian processes[C]Proc of the 25th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2011:19-27
                            </a>
                        </p>
                        <p id="599">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Guassian Processes for Machine Learning">

                                <b>[29]</b>Rasmussen C, Williams C.Guassian Processes for Machine Learning.Cambridge, MA:MIT Press, 2006
                            </a>
                        </p>
                        <p id="601">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverse reinforcement learning via deep gaussian process[OL]">

                                <b>[30]</b>Jin M, Spanos C.Inverse reinforcement learning via deep gaussian process[OL]. (2015-12-26) [2017-03-04].https:arxiv.org/abs/1512.08065
                            </a>
                        </p>
                        <p id="603">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Tutorial on EnergyBased Learning">

                                <b>[31]</b>LuCun Y, Chopra S, Hadsell R.A tutorial on energy-based learning[J].Predicting Structured Data, 2006, 1 (1) :1-59
                            </a>
                        </p>
                        <p id="605">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximum entropy deep inverse reinforcement learning[OL]">

                                <b>[32]</b>Wulfmeier M, Ondruska P, Posner I.Maximum entropy deep inverse reinforcement learning[OL]. (2015-07-17) [2016-03-11].https:arxiv.org/abs/1507.04888
                            </a>
                        </p>
                        <p id="607">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bayesian inverse reinforcement learning">

                                <b>[33]</b>Ramachandran D, Amir E.Bayesian inverse reinforcement learning[C]Proc of the 20th Int Joint Conf on Artificial Intelligence (IJCAI) .Burlington:Morgan Kaufmann, 2007:2586-2591
                            </a>
                        </p>
                        <p id="609">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bayesain nonparametric feature construction for inverse reinforcement learning">

                                <b>[34]</b>Choi J, Kim K.Bayesain nonparametric feature construction for inverse reinforcement learning[C]Proc of the 27th Int Joint Conf on Artificial Intelligence (IJCAI) .San Francisco, MA:Morgan Kaufmann, 2013:1287-1293
                            </a>
                        </p>
                        <p id="611">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverse reinforcement learning with locally consistent reward functions">

                                <b>[35]</b>Nguyen Q, Low K, Jaillet P.Inverse reinforcement learning with locally consistent reward functions[C]Proc of the29th Conf on Advances in Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2015:1747-1755
                            </a>
                        </p>
                        <p id="613">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverse reinforcement learning in partially observable environments">

                                <b>[36]</b>Choi J, Kim K.Inverse reinforcement learning in partially observable environments[J].Journal of Machine Learning Research, 2011, 12 (2) :1028-1033
                            </a>
                        </p>
                        <p id="615">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploration and apprenticeship learning in reinforcement learning">

                                <b>[37]</b>Abbeel P, Ng A.Exploration and apprenticeship learning in reinforcement learning[C]Proc of the 22nd Int Conf on Machine Learning (ICML) .Cambridge, MA:MIT Press, 2005:1-8
                            </a>
                        </p>
                        <p id="617">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning neural network policies with guided policy search under unknown dynamics">

                                <b>[38]</b>Levine S, Abbeel P.Learning neural network policies with guided policy search under unknown dynamics[C]Proc of the 28th Conf on Neural Information Processing Systems (NIPS) .Cambridge, MA:MIT Press, 2014:1071-1079
                            </a>
                        </p>
                        <p id="619">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model-agnostic meta-learning for fast adaptation of deep networks[OL]">

                                <b>[39]</b>Finn C, Abbeel P, Levine S.Model-agnostic meta-learning for fast adaptation of deep networks[OL]. (2017-03-09) [2017-07-18].https:arxiv.org/abs/1703.03400
                            </a>
                        </p>
                        <p id="621">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One-shot imitation learning[OL]">

                                <b>[40]</b>Duan Y, Andrychowicz M, Stadie B, et al.One-shot imitation learning[OL]. (2017-05-21) [2017-12-04].https:arxiv.org/abs/1703.07326
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902003&amp;v=MDYzMzZxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydktMeXZTZExHNEg5ak1yWTlGWjRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
