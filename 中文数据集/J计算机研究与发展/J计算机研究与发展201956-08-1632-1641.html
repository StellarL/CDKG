

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127882826212500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201908006%26RESULT%3d1%26SIGN%3dgQX97Ez%252fAjNhOA9cfpAGaExEpPc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201908006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908006&amp;v=MjY4ODk5ak1wNDlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1ZycklMeXZTZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#72" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;1.1 生成对抗网络&lt;/b&gt;"><b>1.1 生成对抗网络</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;1.2 重排序&lt;/b&gt;"><b>1.2 重排序</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;2 基于DCGAN和拓展近邻重排序的行人重识别&lt;/b&gt; "><b>2 基于DCGAN和拓展近邻重排序的行人重识别</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;2.1 卷积神经网络&lt;/b&gt;"><b>2.1 卷积神经网络</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;2.2 标签平滑正则化&lt;/b&gt;"><b>2.2 标签平滑正则化</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;2.3 拓展近邻重排序&lt;/b&gt;"><b>2.3 拓展近邻重排序</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="&lt;b&gt;3 实  验&lt;/b&gt; "><b>3 实  验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#128" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;3.2 实验设置&lt;/b&gt;"><b>3.2 实验设置</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;3.3 参数分析&lt;/b&gt;"><b>3.3 参数分析</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;3.4 生成图片的数量对Re-ID性能的影响&lt;/b&gt;"><b>3.4 生成图片的数量对Re-ID性能的影响</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;3.5 标签平滑正则化对Re-ID性能的改进&lt;/b&gt;"><b>3.5 标签平滑正则化对Re-ID性能的改进</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;3.6 拓展近邻重排序对Re-ID性能的改进&lt;/b&gt;"><b>3.6 拓展近邻重排序对Re-ID性能的改进</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#158" data-title="&lt;b&gt;4 总  结&lt;/b&gt; "><b>4 总  结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="图1 公共数据集行人图像对">图1 公共数据集行人图像对</a></li>
                                                <li><a href="#80" data-title="图2 本文方法的整体框架">图2 本文方法的整体框架</a></li>
                                                <li><a href="#83" data-title="图3 结合生成图片的网络训练框架">图3 结合生成图片的网络训练框架</a></li>
                                                <li><a href="#136" data-title="图4 在Market1501数据集上参数&lt;i&gt;N&lt;/i&gt;对行人重识别性能的影响">图4 在Market1501数据集上参数<i>N</i>对行人重识别性能的影响</a></li>
                                                <li><a href="#137" data-title="图5 在Market1501数据集上参数&lt;i&gt;M&lt;/i&gt;对行人重识别性能的影响">图5 在Market1501数据集上参数<i>M</i>对行人重识别性能的影响</a></li>
                                                <li><a href="#138" data-title="图6 CUHK03数据集的生成图片">图6 CUHK03数据集的生成图片</a></li>
                                                <li><a href="#141" data-title="图7 Market-1501数据集的生成图片">图7 Market-1501数据集的生成图片</a></li>
                                                <li><a href="#143" data-title="图8 DukeMTMC-reID数据集的生成图片">图8 DukeMTMC-reID数据集的生成图片</a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;表1 生成图片的数量对Re-ID性能的影响&lt;/b&gt;"><b>表1 生成图片的数量对Re-ID性能的影响</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表2 在Market-1501数据集上的性能比较&lt;/b&gt;"><b>表2 在Market-1501数据集上的性能比较</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表3 single-shot情况下多种方法在CUHK03 数据集上的比较&lt;/b&gt;"><b>表3 single-shot情况下多种方法在CUHK03 数据集上的比较</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表4 multi-short情况下CUHK03数据集上的性能比较&lt;/b&gt;"><b>表4 multi-short情况下CUHK03数据集上的性能比较</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表5 DukeMTMC-reID数据集上的性能比较&lt;/b&gt;"><b>表5 DukeMTMC-reID数据集上的性能比较</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表6 生成图片与真实图片对行人重识别性能的影响&lt;/b&gt;"><b>表6 生成图片与真实图片对行人重识别性能的影响</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表7 不同重排序方法的比较&lt;/b&gt;"><b>表7 不同重排序方法的比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="189">


                                    <a id="bibliography_1" title="Zheng Liang, Yang Yi, Hauptmann Alexander G.Person re-identification:Past, present and future[EB/OL].2016[2019-05-10].https://arxiv.org/pdf/1610.02984.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification:Past,present and future">
                                        <b>[1]</b>
                                        Zheng Liang, Yang Yi, Hauptmann Alexander G.Person re-identification:Past, present and future[EB/OL].2016[2019-05-10].https://arxiv.org/pdf/1610.02984.pdf
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_2" title="LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436- 444" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[2]</b>
                                        LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436- 444
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_3" title="Wang Hongyuan, Ding Zongyuan, Zhang Ji, et al.Person reidentification by semisupervised dictionary rectification learning with retraining module[J].Journal of Electronic Imaging, 2018, 27 (4) :No.043043" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEGF3B664FA3FDBE4D126303ADEF12C24B3&amp;v=MTE3ODhxL2swWjUxN2Znazl1eGNSN0R4OVN3NlcyV1EwZThHV1FjaWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzI2eGFnPU5pZk9hY1c3Yk5mSw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Wang Hongyuan, Ding Zongyuan, Zhang Ji, et al.Person reidentification by semisupervised dictionary rectification learning with retraining module[J].Journal of Electronic Imaging, 2018, 27 (4) :No.043043
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_4" title="NI Tongguang, Ding Zongyuan, Chen Fuhua, et al.Relative distance metric leaning based on clustering centralization and projection vectors learning for person re-identification[J].IEEE Access, 2018, 6 (1) :11405- 11411" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relative distance metric leaning based on clustering centralization and projection vectors learning for person re-identification">
                                        <b>[4]</b>
                                        NI Tongguang, Ding Zongyuan, Chen Fuhua, et al.Relative distance metric leaning based on clustering centralization and projection vectors learning for person re-identification[J].IEEE Access, 2018, 6 (1) :11405- 11411
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_5" title="Ding Zongyuan, Wang Hongyuan, Chen Fuhua, et al.Pedestrian weight recognition based on distance centralization and projection vector learning[J].Journal of Computer Research and Development, 2017, 54 (8) :1785- 1794 (in Chinese) (丁宗元, 王洪元, 陈付华, 等.基于距离中心化与投影向量学习的行人重识别[J].计算机研究与发展, 2017, 54 (8) :1785- 1794) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708015&amp;v=MTQ5MTE0TzN6cXFCdEdGckNVUkxPZVplUnJGeXZnVnJySUx5dlNkTEc0SDliTXA0OUVZWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Ding Zongyuan, Wang Hongyuan, Chen Fuhua, et al.Pedestrian weight recognition based on distance centralization and projection vector learning[J].Journal of Computer Research and Development, 2017, 54 (8) :1785- 1794 (in Chinese) (丁宗元, 王洪元, 陈付华, 等.基于距离中心化与投影向量学习的行人重识别[J].计算机研究与发展, 2017, 54 (8) :1785- 1794) 
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_6" title="Liong V E, Lu J, Ge Y.Regularized local metric learning for person re-identification[J].Pattern Recognition Letters, 2015, 68 (2015) :288- 296" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122500448808&amp;v=MTA5NzVUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklJMXNSYnhJPU5pZk9mYks5SDlQT3FvOUZZTzhIQkh3eG9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        Liong V E, Lu J, Ge Y.Regularized local metric learning for person re-identification[J].Pattern Recognition Letters, 2015, 68 (2015) :288- 296
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_7" title="Zheng Liang, Shen Liyue, Tian Lu, et al.Scalable person re-identification:a benchmark[C] //Proc of IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1116- 1124" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">
                                        <b>[7]</b>
                                        Zheng Liang, Shen Liyue, Tian Lu, et al.Scalable person re-identification:a benchmark[C] //Proc of IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1116- 1124
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_8" title="Li Wei, Zhao Rui, Xiao Tong, et al.Deepreid:Deep filter pairing neural network for person re-identification[C] //Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:152- 159" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepReID:Deep filter pairing neural network for person re-identification">
                                        <b>[8]</b>
                                        Li Wei, Zhao Rui, Xiao Tong, et al.Deepreid:Deep filter pairing neural network for person re-identification[C] //Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:152- 159
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_9" title="Ristani E, Solera F, Zou R, et al.Performance measures and a data set for multi-target, multi-camera tracking[C] //Proc of European Conf on Computer Vision.Berlin:Springer, 2016:17- 35" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance Measures and a Data Set for Multi-target">
                                        <b>[9]</b>
                                        Ristani E, Solera F, Zou R, et al.Performance measures and a data set for multi-target, multi-camera tracking[C] //Proc of European Conf on Computer Vision.Berlin:Springer, 2016:17- 35
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_10" title="Ni Tongguang, Gu Xiaoqing, Wang Hongyuan, et al.Discirminative deep trasfer metric learning for cross-scenario person re-identification[J].Journal of Electronic Imaging, 2018, 27 (4) :No.043026" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEG2F42C2CB5ED91F63DEA377A0B00E13F5&amp;v=MjU3MzJhQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjZ4YWc9TmlmT2FiSE9HdE8vcmZ3M1laNTdCWDFQeVJWbm4wNStUM2lUckdBMWVjZVZSc3lhQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Ni Tongguang, Gu Xiaoqing, Wang Hongyuan, et al.Discirminative deep trasfer metric learning for cross-scenario person re-identification[J].Journal of Electronic Imaging, 2018, 27 (4) :No.043026
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_11" title="Xin Xiaomeng, Wang Jinjun, Xie Ruji, et al.Semi-supervised person Re-Identification using multi-view clustering[J].Pattern Recognition, 2019, 88 (2019) :285- 297" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA4893570C6A35DB3A1347252839AC421&amp;v=Mjk1NTlpNnp4NVQzM25yaG8yY01QblFiaWVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzI2eGFnPU5pZk9mY0s4RnRqUHFvaEZGKzErRDNsTnZSVg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        Xin Xiaomeng, Wang Jinjun, Xie Ruji, et al.Semi-supervised person Re-Identification using multi-view clustering[J].Pattern Recognition, 2019, 88 (2019) :285- 297
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_12" title="Shen Xiaohui, Lin Zhe, Brandt J, et al.Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:3013- 3020" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object retrieval and localization with spatially-constrained similarity measure and knn re-ranking">
                                        <b>[12]</b>
                                        Shen Xiaohui, Lin Zhe, Brandt J, et al.Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:3013- 3020
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_13" title="Ye Mang, Chen Jun, Leng Qingming, et al.Coupled-view based ranking optimization for person re-identification[C] //Proc of the 21st Int Conf on Multimedia Modeling.Berlin:Springer, 2015:105- 117" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Coupled-view based ranking optimization for person re-identification&amp;quot;">
                                        <b>[13]</b>
                                        Ye Mang, Chen Jun, Leng Qingming, et al.Coupled-view based ranking optimization for person re-identification[C] //Proc of the 21st Int Conf on Multimedia Modeling.Berlin:Springer, 2015:105- 117
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_14" title="Zhong Zhun, Zheng Liang, Cao Donglin, et al.Re-ranking person re-identification with k-reciprocal encoding[C] //Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:1318- 1327" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Re-ranking Person Re-identification with k-reciprocal Encoding">
                                        <b>[14]</b>
                                        Zhong Zhun, Zheng Liang, Cao Donglin, et al.Re-ranking person re-identification with k-reciprocal encoding[C] //Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:1318- 1327
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_15" title="Bai Song, Bai Xiang.Sparse contextual activation for efficient visual reranking[J].IEEE Transactions on Image Processing, 2016, 25 (3) :1056- 1069" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse Contextual Activation for Efficient Visual Re-ranking.">
                                        <b>[15]</b>
                                        Bai Song, Bai Xiang.Sparse contextual activation for efficient visual reranking[J].IEEE Transactions on Image Processing, 2016, 25 (3) :1056- 1069
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_16" title="Jarvis R A, Patrick E A.Clustering using a similarity measure based on shared near neighbors[J].IEEE Transactions on Computers, 1973, 100 (11) :1025- 1034" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CLUSTERING USING A SIMILARITY MEASURE BASED ON SHARED NEAR NEIGHBORS">
                                        <b>[16]</b>
                                        Jarvis R A, Patrick E A.Clustering using a similarity measure based on shared near neighbors[J].IEEE Transactions on Computers, 1973, 100 (11) :1025- 1034
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_17" title="Radford A, Metz L, Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].2016 [2019-05-10].https://arxiv.org/abs/1511.06434.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">
                                        <b>[17]</b>
                                        Radford A, Metz L, Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].2016 [2019-05-10].https://arxiv.org/abs/1511.06434.pdf
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_18" title="Szegedy C, Vanhoucke V, Ioffe S, et al.Rethinking the inception architecture for computer vision[C] //Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2818- 2826" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">
                                        <b>[18]</b>
                                        Szegedy C, Vanhoucke V, Ioffe S, et al.Rethinking the inception architecture for computer vision[C] //Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2818- 2826
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_19" title="Schroff F, Treibitz T, Kriegman D, et al.Pose, illumination and expression invariant pairwise face-similarity measure via doppelg&#228;nger list comparison[C] //Proc of the 29th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2011:2494- 2501" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pose,illumination and expression invariant pairwise face-similarity measure via doppelg?nger list comparison">
                                        <b>[19]</b>
                                        Schroff F, Treibitz T, Kriegman D, et al.Pose, illumination and expression invariant pairwise face-similarity measure via doppelg&#228;nger list comparison[C] //Proc of the 29th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2011:2494- 2501
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_20" title="Zhang Li, Xiang Tao, Gong Shaogang.Learning a discriminative null space for person re-identification[C] //Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1239- 1248" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Null Space for Person Re-identification">
                                        <b>[20]</b>
                                        Zhang Li, Xiang Tao, Gong Shaogang.Learning a discriminative null space for person re-identification[C] //Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1239- 1248
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_21" title="Zheng Zhedong, Zheng Liang, Yang Yi.A discriminatively learned cnn embedding for person reidentification[J].ACM Transactions on Multimedia Computing, Communications, and Applications, 2018, 14 (1) :13:1- 13:20" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM68E09C1E98C844456DEFD79E7048C559&amp;v=MTQyNTAxaHg3MjZ4YWc9TmlmSVk3V3dhOUhGM0k0d2JlTjhCSGc5eXhNVm5rb0xQSGpyMlJVMWZicm5RTCtXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                        Zheng Zhedong, Zheng Liang, Yang Yi.A discriminatively learned cnn embedding for person reidentification[J].ACM Transactions on Multimedia Computing, Communications, and Applications, 2018, 14 (1) :13:1- 13:20
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_22" title="Barbosa I B, Cristani M, Caputo B, et al.Looking beyond appearances:Synthetic training data for deep cnns in re-identification[J].Computer Vision and Image Understanding, 2018, 167:50- 62" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE3E2EA8B65341A4DDFB5F68EA6F7D4D9&amp;v=MjcxMDJpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh4NzI2eGFnPU5pZk9mY2E3YTlPNTNvYzNZdTRNQ0gxSXkySm5uRTE0UG5ucTJXTXpEN1hnUWM2V0NPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        Barbosa I B, Cristani M, Caputo B, et al.Looking beyond appearances:Synthetic training data for deep cnns in re-identification[J].Computer Vision and Image Understanding, 2018, 167:50- 62
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_23" title="Liao Shengcai, Hu Yang, Zhu Xiangyu, et al.Person re-identification by local maximal occurrence representation and metric learning[C] //Proc of the 33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:2197- 2206" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Person re-identification by Local Maximal Occurrence representation and metric learning.&amp;quot;">
                                        <b>[23]</b>
                                        Liao Shengcai, Hu Yang, Zhu Xiangyu, et al.Person re-identification by local maximal occurrence representation and metric learning[C] //Proc of the 33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:2197- 2206
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_24" title="Ustinova E, Ganin Y, Lempitsky V.Multi-region bilinear convolutional neural networks for person re-identification[C] //Proc of the 14th IEEE Int Conf on Advanced Video and Signal Based Surveillance (AVSS) .Piscataway, NJ:IEEE, 2017:1- 6" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-region bilinear convolutional neural networks for person re-identification">
                                        <b>[24]</b>
                                        Ustinova E, Ganin Y, Lempitsky V.Multi-region bilinear convolutional neural networks for person re-identification[C] //Proc of the 14th IEEE Int Conf on Advanced Video and Signal Based Surveillance (AVSS) .Piscataway, NJ:IEEE, 2017:1- 6
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_25" title="Varior R R, Haloi M, Wang G.Gated siamese convolutional neural network architecture for human re-identification[C] //Proc of European Conf on Computer Vision.Berlin:Springer, 2016:791- 808" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification">
                                        <b>[25]</b>
                                        Varior R R, Haloi M, Wang G.Gated siamese convolutional neural network architecture for human re-identification[C] //Proc of European Conf on Computer Vision.Berlin:Springer, 2016:791- 808
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_26" title="Geng Mengyue, Wang Yaowei, Xiang Tao.Deep transfer learning for person re-identification[EB/OL].2016[2019-05-10].https://arxiv.org/pdf/1611.05244.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep transfer learning for person re-identification">
                                        <b>[26]</b>
                                        Geng Mengyue, Wang Yaowei, Xiang Tao.Deep transfer learning for person re-identification[EB/OL].2016[2019-05-10].https://arxiv.org/pdf/1611.05244.pdf
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_27" title="Koestinger M, Hirzer M, Wohlhart P, et al.Large scale metric learning from equivalence constraints[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:2288- 2295" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large scale metric learning from equivalence constraints">
                                        <b>[27]</b>
                                        Koestinger M, Hirzer M, Wohlhart P, et al.Large scale metric learning from equivalence constraints[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:2288- 2295
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_28" title="Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C] //Proc of Advances in Neural Information Processing Systems.San Francisco, CA:Morgan Kaufmann, 2012:1097- 1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[28]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C] //Proc of Advances in Neural Information Processing Systems.San Francisco, CA:Morgan Kaufmann, 2012:1097- 1105
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_29" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL].2015[2019-05-10].https://arxiv.org/pdf/1409.1556.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[29]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL].2015[2019-05-10].https://arxiv.org/pdf/1409.1556.pdf
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_30" title="Xiao Tong, Li Shuang, Wang Baochao, et al.Joint detection and identification feature learning for person search[C] //Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3415- 3424" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint detection and identification feature learning for person search">
                                        <b>[30]</b>
                                        Xiao Tong, Li Shuang, Wang Baochao, et al.Joint detection and identification feature learning for person search[C] //Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3415- 3424
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_31" title="Hermans A, Beyer L, Leibe B.In defense of the triplet loss for person re-identification[EB/OL].2017[2019-05-10].https://arxiv.org/pdf/1703.07737.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of the triplet loss for person re-identification">
                                        <b>[31]</b>
                                        Hermans A, Beyer L, Leibe B.In defense of the triplet loss for person re-identification[EB/OL].2017[2019-05-10].https://arxiv.org/pdf/1703.07737.pdf
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(08),1632-1641 DOI:10.7544/issn1000-1239.2019.20190195            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度卷积生成对抗网络和拓展近邻重排序的行人重识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%88%B4%E8%87%A3%E8%B6%85&amp;code=42588554&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">戴臣超</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B4%AA%E5%85%83&amp;code=24489626&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王洪元</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%80%AA%E5%BD%A4%E5%85%89&amp;code=27225291&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">倪彤光</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E9%A6%96%E5%85%B5&amp;code=40175111&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈首兵</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B8%B8%E5%B7%9E%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0268985&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常州大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>行人重识别任务旨在识别不相交摄像头视图下的相同行人.这项任务极具挑战性, 尤其是当数据集中每个行人仅仅有几张图片时.针对行人重识别数据集中行人图片数量不足的问题, 提出一个从原始数据集中生成额外训练数据的方法.在这项工作之中存在2个挑战:1) 如何从原始数据集之中获取更多的训练数据;2) 如何处理这些新生成的训练数据.使用深度卷积生成对抗网络来生成额外的无标签行人图片, 并采用标签平滑正则化来处理这些新生成的无标签行人图片.为了进一步提升行人重识别准确度, 提出了一种新的无监督重排序框架.此框架既不需要为每组图像对重新计算新的排序列表, 也不需要任何人工交互或标签信息.在Market-1501, CUHK03和DukeMTMC-reID数据集上的实验验证了所提方法的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人重识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%87%8D%E6%8E%92%E5%BA%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重排序;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91%E6%AD%A3%E5%88%99%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标签平滑正则化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E7%9B%91%E7%9D%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无监督;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *王洪元, hywang@cczu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61572085, 61502058, 61806026);</span>
                                <span>江苏省自然科学基金项目 (BK20180956);</span>
                    </p>
            </div>
                    <h1><b>Person Re-Identification Based on Deep Convolutional Generative Adversarial Network and Expanded Neighbor Reranking</b></h1>
                    <h2>
                    <span>Dai Chenchao</span>
                    <span>Wang Hongyuan</span>
                    <span>Ni Tongguang</span>
                    <span>Chen Shoubing</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Engineering, Changzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Person Re-Identification (Re-ID) focuses on identifying the same person among disjoint camera views. This task is highly challenging, especially when there exists only several images per person in the database. Aiming at the problem of insufficient number of person images in person re-identification dataset, a method that generates extra training data from the original dataset is proposed. There are two challenges in this work, one is how to get more training data from the original training set, and the other is how to deal with these newly generated training data. The deep convolutional generative adversarial network is used to generate extra unlabeled person images and label smoothing regularization is used to process these newly generated unlabeled person images. In order to further improve the accuracy of person re-identification, a new unsupervised reranking framework is proposed. This framework neither requires to recalculate a new sorted list for each image pairs nor requires any human interaction or label information. Experiments on the datasets Market-1501, CUHK03, and DukeMTMC-reID verify the effectiveness of the proposed method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20convolutional%20generative%20adversarial%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep convolutional generative adversarial network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reranking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reranking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20smoothing%20regularization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label smoothing regularization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=unsupervised&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">unsupervised;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Dai Chenchao, born in 1995. Master candidate at Changzhou University.His main research interests include computer vision and person re-identification.<image id="281" type="" href="images/JFYZ201908006_28100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wang Hongyuan, born in 1960.Received his PhD degree from Nanjing University of Science and Technology in 2004.Professor and master supervisor at Changzhou University.His main research interests include  image  processing, artificial intelligence and pattern recognition. <image id="283" type="" href="images/JFYZ201908006_28300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Ni Tongguan, born in 1978.Received his PhD degree from Jiangnan University in 2015.Currently associate professor in the School of Information Science and Engineering, Changzhou University, Chang-zhou, China.His main research interests include pattern recognition, intelligent computation and their application. <image id="285" type="" href="images/JFYZ201908006_28500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Chen Shoubing, born in 1991. Master candidate at Changzhou University.His main research interests include computer vision and person re-identification.<image id="287" type="" href="images/JFYZ201908006_28700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-21</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61572085, 61502058, 61806026);</span>
                                <span>the Natural Science Foundation of Jiangsu Province of China (BK20180956);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="69">行人重识别<citation id="251" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>旨在自动匹配同一个行人在不同摄像机视图下的行人图片, 该任务在公共安全方面具有很大的应用潜力.因为光照、遮挡、姿势改变、背景混乱等因素, 不同摄像机视图下同一个行人的图片往往有很大不同, 如图1所示 (同一列的图片属于同一个行人) , 所以该任务具有很大的挑战性, 是目前计算机视觉领域的研究热点.近几年在行人重识别领域的进步主要归功于高性能的深度学习算法<citation id="252" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.和传统方法相比<citation id="258" type="reference"><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 这类算法往往需要大量标注好的行人图片.尽管目前已经发布了一些规模较大的行人重识别数据集, 但是在这些数据集中每个行人的图片仍然是有限的.据统计, 在Market1501数据集<citation id="253" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>之中平均每个行人只有17.2张图片, 在CUHK03数据集<citation id="254" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>之中平均每个行人只有9.6张图片, 在DukeMTMC-reID数据集<citation id="255" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>之中平均每个行人只有23.5张图片.为解决这个问题, 有部分研究者试图使用其他场景下标注好的行人图片, 例如Ni等人<citation id="256" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出一个迁移学习模型试图学习不同场景下行人图片的公共特征.另外一些研究者试图利用无标签行人图片, 例如Xin等人<citation id="257" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出多视图聚类的方法来为无标签图片分配一个伪标签, 然后在训练期间使用具有真实标签和伪标签的数据参与训练, 进而提高模型的泛化性能.然而这些方法往往只在特定场合下有用, 有时候甚至会导致模型性能下降.本文从另外一个角度出发, 使用改进的生成对抗网络, 从现有训练集中生成类似行人图片参与训练, 从而引入更多的颜色、光照以及姿势变化等信息来正则化模型, 提升模型的鲁棒性.</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 公共数据集行人图像对" src="Detail/GetImg?filename=images/JFYZ201908006_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 公共数据集行人图像对  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Pedestrian image pairs of public datasets</p>

                </div>
                <div class="p1">
                    <p id="71">另一方面, 行人重识别在本质上就是一个图片检索任务, 给定一张查询图片, 在对应的图库之中检索出与这张查询图片身份相同的行人图片.在这个过程之中经常会出现匹配不吻合的现象, 导致行人重识别第一匹配率下降.为缓解这个问题, 越来越多的研究者在行人重识别任务中加入重排序<citation id="260" type="reference"><link href="211" rel="bibliography" /><link href="213" rel="bibliography" /><link href="215" rel="bibliography" /><link href="217" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>.简单来说, 重排序就是重新计算查询图片与图库图片之间的距离, 从而使得更多与查询图片身份一致的图库图片排在排序列表更靠前的位置.然而, 目前流行的重排序方法往往需要根据每一组图片对的<i>k</i>近邻或者<i>k</i>互邻重新计算新的排序列表, 这使得重排序操作复杂度极高.为了解决这个问题, 本文通过引入拓展近邻距离<citation id="259" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的概念, 提出了一种新的重排序方法, 它不需要为每组图像对重新计算新的排序列表.经实验发现, 本文方法可以达到的最高Rank-1和mAP在Market-1501数据集上是89.70% 和82.86%, 在CUHK03数据集上是87.60% 和87.46%, 在DukeMTMC-reID数据集上是76.44%, 67.59%, 并成功用于2018全球 (南京) 人工智能应用大赛多目标跨摄像头跟踪赛题.</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="73" name="73"><b>1.1 生成对抗网络</b></h4>
                <div class="p1">
                    <p id="74">生成对抗网络由2个子网络组成:生成网络和判别网络.在训练过程中二者相互博弈直到进入一个均衡和谐的状态.针对普通的生成对抗网络经常会面临训练不稳定、网络难以收敛以及生成图片质量太差等问题, 本文采用的深度卷积生成对抗网络<citation id="261" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation> (deep convolutional generative adversarial network, DCGAN) 在3个方面作出改进:1) 去除掉网络中卷积层后面的全连接层, 此时网络变成了全卷积网络, 实验发现去掉该全连接层, 网络可以收敛的更快;2) 在生成网络的输入层之后作批归一化操作, 使得训练过程变得更加稳定;3) 使用步幅卷积来替换网络中的所有池化层, 使得网络可以学习自身的空间下采样, 进而生成质量更高的行人图片.</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>1.2 重排序</b></h4>
                <div class="p1">
                    <p id="76">最近几年, 重排序在行人重识别领域受到了越来越多的关注, Shen等人<citation id="262" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>使用<i>k</i>近邻生成新的排序列表, 并且基于这些排序列表重新计算图片对之间的距离.Ye等人<citation id="263" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>将<i>k</i>近邻的全局特征与局部特征组合为一个新的查询特征, 并且根据这些信息修正初始排名列表.不同于普通的<i>k</i>近邻, Zhong等人<citation id="264" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>利用<i>k</i>互邻来计算杰卡德距离, 融合杰卡德距离与原始的欧氏距离获得更有效的排序距离.与基于<i>k</i>近邻和<i>k</i>互邻的重排序方法不同, 本文引入了拓展近邻距离的概念, 通过聚合每一对图片的拓展近邻距离来进行重排序 (本文称之为拓展近邻重排序) .</p>
                </div>
                <div class="p1">
                    <p id="77">由此, 本文结合DCGAN与拓展近邻重排序进行行人重识别.通过DCGAN从原始训练集中生成类似的行人图片参与训练, 提高模型的泛化能力, 再对初始排序表进行拓展近邻重排序操作, 缓解行人匹配错误的现象, 提升行人重识别的性能.</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>2 基于DCGAN和拓展近邻重排序的行人重识别</b></h3>
                <div class="p1">
                    <p id="79">本文提出基于DCGAN和拓展近邻重排序的行人重识别方法, 整体框架如图2所示.首先使用训练好的卷积神经网络分别提取图库图片和查询图片的特征向量, 然后基于欧氏度量计算每一张图库图片和查询图片之间的距离生成初始排序表, 最后利用拓展近邻重排序对初始排序表进行重排序, 生成最终的排序表.本节将介绍卷积神经网络如何在训练期间引入DCGAN生成的行人图片, 以及拓展近邻重排序的定义与描述.</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文方法的整体框架" src="Detail/GetImg?filename=images/JFYZ201908006_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文方法的整体框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The whole framework of the method</p>

                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>2.1 卷积神经网络</b></h4>
                <div class="p1">
                    <p id="82">本文采用ResNet50<citation id="265" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>作为基础网络结构, 并且采用文献<citation id="266" type="reference">[<a class="sup">1</a>]</citation>中的训练策略.不同于文献<citation id="267" type="reference">[<a class="sup">1</a>]</citation>, 本文去掉了最后1 000维的分类层, 并且增加了2层全连接层:第1个全连接层的输出为1024维 (称为FC-1024) ;第2个全连接层的输出是T维 (称为FC-T, 其中T代表训练集中的类别数) .经实验证明, 添加这2层全连接层可以很有效地提升行人重识别的准确率, 并且不会影响模型的收敛速度.与文献<citation id="268" type="reference">[<a class="sup">1</a>]</citation>采取的策略不同, 本文并没有为生成图片赋予一个伪标签, 而是分配了一个对现有类别都统一的标签分布.在2.2节将对生成图片标签分布的分配进行讨论.结合生成图片的网络训练框架如图3所示.</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 结合生成图片的网络训练框架" src="Detail/GetImg?filename=images/JFYZ201908006_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 结合生成图片的网络训练框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Network training framework with generated image</p>

                </div>
                <div class="p1">
                    <p id="84">首先使用DCGAN从现有训练集中生成类似的行人图片, 然后将生成图片与真实图片混合在一起作为卷积神经网络的输入, 最后通过最小化交叉熵损失来对网络进行调参使模型更鲁棒.</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>2.2 标签平滑正则化</b></h4>
                <div class="p1">
                    <p id="86">标签平滑正则化<citation id="269" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation> (label smoothing regulari-zation, LSR) 的思想就是给非真实类别赋予一个较小的值而不是0.这种策略相当于加入一些噪音数据, 以防网络过于倾向真实类别.在每一个批次中都有一定数量的真实图片和生成图片, 所以本文的损失函数为</p>
                </div>
                <div class="p1">
                    <p id="87"><i>L</i><sub>Loss</sub>=<i>L</i><sub>R</sub>+<i>L</i><sub>G</sub>, (1) </p>
                </div>
                <div class="p1">
                    <p id="88">其中, <i>L</i><sub>R</sub>为真实图片的交叉熵损失;<i>L</i><sub>G</sub>为生成图片的交叉熵损失.交叉熵损失定义为</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>r</mtext><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mspace width="0.25em" /></mstyle><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mi>q</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></math></mathml>, (2) </p>
                </div>
                <div class="p1">
                    <p id="91">其中, <i>n</i>∈{1, 2, …, <i>N</i>}, <i>N</i>是原始训练集中预先定义好的类别总数;<i>p</i> (<i>n</i>) ∈[0, 1]是网络预测输入图片属于第<i>n</i>个类别的概率;本文使用softmax函数对<i>p</i> (<i>n</i>) 进行归一化处理, 因此<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>p</i> (<i>n</i>) =1;<i>q</i> (<i>n</i>) 指的是输入图片的真实标签分布.因为训练集中每张行人图片只有一个标签, 所以对应的标签分布中必有一项为1而其他项均为0, 所以<i>q</i> (<i>n</i>) 可写为</p>
                </div>
                <div class="area_img" id="93">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908006_09300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="95">其中, <i>y</i>代表图片的真实标签.</p>
                </div>
                <div class="p1">
                    <p id="96">在文献<citation id="270" type="reference">[<a class="sup">18</a>]</citation>中, 通过标签平滑正则化把非真实类别的分布考虑在内, 鼓励网络不要太倾向于真实类别.因此, 运用标签平滑正则化策略后, 图片的标签分布为</p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908006_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="99">其中, <i>ε</i>∈[0, 1].</p>
                </div>
                <div class="p1">
                    <p id="100">然而, 生成的行人图片不属于任意已知类别, 无法为之分配一个准确的标签.不同于以往文献处理无标签图片的策略, 本文提出为无标签图片分配一个虚拟的标签分布, 设置其标签分布在所有已知类别上都是统一的.因此, 对于生成图片, 本文改进后的标签分布被定义为</p>
                </div>
                <div class="p1">
                    <p id="101"><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>q</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>S</mtext><mtext>R</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac></mrow></math></mathml>, (5) </p>
                </div>
                <div class="p1">
                    <p id="103">此时, 生成图片的标签分布为均匀分布, 即默认任意一张生成图片属于任意已知类别的概率是相同的.所以, 生成图片的交叉熵损失为</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mtext>G</mtext></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mspace width="0.25em" /></mstyle><mrow><mi>log</mi></mrow><mtext> </mtext><mi>p</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></math></mathml>. (6) </p>
                </div>
                <div class="p1">
                    <p id="106">考虑到训练集中行人图片的内容和标签是正确匹配的, 本文并没有对训练集中的真实图片进行标签平滑正则化处理, 因此真实图片的交叉熵损失为</p>
                </div>
                <div class="p1">
                    <p id="107"><i>L</i><sub>R</sub>=-log (<i>p</i> (<i>y</i>) ) . (7) </p>
                </div>
                <div class="p1">
                    <p id="108">结合式 (6) 与式 (7) , 本文的损失函数为</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mo>-</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mfrac><mi>β</mi><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mspace width="0.25em" /></mstyle><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中, <i>β</i>的取值为0或者1, <i>β</i>=0时, 是真实图片的损失;当<i>β</i>=1时, 则是生成图片的损失.虽然本文使用的生成图片质量并不高, 无法为之分配一个与图片内容相符的标签, 但是本文使用改进后的标签平滑正则化方法可以直接处理在样本空间中位于真实图片附近的生成图片.通过这种方式可以引入更多的颜色、光照、背景、姿势变化等信息来正则化网络模型, 进而引导网络去寻找更具有判别力的特征.</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>2.3 拓展近邻重排序</b></h4>
                <div class="p1">
                    <p id="112">给定一个行人图库<i>G</i>={<i>g</i><sub><i>i</i></sub>|<i>i</i>=1, 2, …, <i>S</i>}以及一张查询图片, 那么查询图片与图库图片之间的距离可以用欧氏距离来计算:</p>
                </div>
                <div class="p1">
                    <p id="113"><i>d</i> (<i>p</i>, <i>g</i><sub><i>i</i></sub>) = (<b><i>x</i></b><sub><i>p</i></sub>-<b><i>x</i></b><sub><i>g</i><sub><i>i</i></sub></sub>) <sup>T</sup>× (<b><i>x</i></b><sub><i>p</i></sub>-<b><i>x</i></b><sub><i>g</i><sub><i>i</i></sub></sub>) , (9) </p>
                </div>
                <div class="p1">
                    <p id="114">其中, <i>p</i>指的是查询图片;<i>g</i><sub><i>i</i></sub>指的是图库中的第<i>i</i>张图片;<b><i>x</i></b><sub><i>p</i></sub>指的是查询图片<i>p</i>的特征向量;<b><i>x</i></b><sub><i>g</i><sub><i>i</i></sub></sub>指的是图库图片<i>g</i><sub><i>i</i></sub>的特征向量.</p>
                </div>
                <div class="p1">
                    <p id="115">重排序就是要重新计算查询图片与图库图片之间的距离, 进而生成新的排序表.给定一组图片 (包括一张查询图片与一张图库图片) .首先寻找2张图片对应的拓展近邻集合, 然后聚合这2个集合中每一对图片之间的距离, 并用聚合之后的距离 (本文称之为拓展近邻距离) 来代替该组图片原本的距离.查询图片的拓展近邻集合<i>R</i> (<i>p</i>, <i>K</i>) 为</p>
                </div>
                <div class="p1">
                    <p id="116"><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi>R</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>Μ</mi><mo stretchy="false">) </mo><mo>, </mo><mi>R</mi><mo stretchy="false"> (</mo><mi>Μ</mi><mo>, </mo><mi>Ν</mi><mo stretchy="false">) </mo></mrow><mo>}</mo></mrow><mo>→</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>Κ</mi><mo stretchy="false">) </mo></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="118">其中, <i>R</i> (<i>p</i>, <i>M</i>) 是与查询图片<i>p</i>最相似的<i>M</i>张近邻图片;<i>R</i> (<i>M</i>, <i>N</i>) 指的是与<i>R</i> (<i>p</i>, <i>M</i>) 中每个元素最相似的<i>N</i>张近邻图片.同理, 图库图片的拓展近邻集合计算方式也是如此.拓展近邻距离可以被定义为</p>
                </div>
                <div class="p1">
                    <p id="119"><i>d</i><sup>*</sup> (<i>p</i>, <i>g</i><sub><i>i</i></sub>) =<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mi>Κ</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>, <i>g</i><sub><i>i</i></sub>) +<i>d</i> (<i>g</i><sub><i>ij</i></sub>, <i>p</i>) ) , (11) </p>
                </div>
                <div class="p1">
                    <p id="121">其中, <i>p</i><sub><i>j</i></sub>指的是查询图片的拓展近邻集合<i>R</i> (<i>p</i>, <i>K</i>) 中的第<i>j</i>个近邻;<i>g</i><sub><i>ij</i></sub>指的是图库图片<i>g</i><sub><i>i</i></sub>的扩展近邻集合<i>R</i> (<i>g</i><sub><i>i</i></sub>, <i>K</i>) 中的第<i>j</i>个近邻;<i>d</i> (·) 是图片对之间的距离.</p>
                </div>
                <div class="p1">
                    <p id="122">本文采用基于排序列表的方式来计算<i>d</i> (·) , 即根据2个排序列表前<i>k</i>个近邻的位置来计算2个排序列表之间的距离.虽然该方法是由Jarvis等人<citation id="271" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出的并且已经成功地运用到人脸识别任务<citation id="272" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>中, 但是据我们所了解, 该方法是首次运用到行人重识别任务中.基于排序列表的距离计算:</p>
                </div>
                <div class="area_img" id="124">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201908006_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="125">其中, <i>S</i><sub><i>p</i></sub> (<i>n</i>) 表示图片<i>n</i>在排序列表<i>L</i><sub><i>p</i></sub>中的位置;<i>S</i><sub><i>g</i><sub><i>i</i></sub></sub> (<i>n</i>) 表示图片<i>n</i>在排序列表<i>L</i><sub><i>g</i><sub><i>i</i></sub></sub>中的位置;×表示矩阵乘法;<i>L</i><sub><i>p</i></sub>与<i>L</i><sub><i>g</i><sub><i>i</i></sub></sub>分别表示<i>p</i>与<i>g</i><sub><i>i</i></sub>所对应的排序列表.[·]<sub>+</sub>=max (·, 0) .因为只考虑排序列表中前<i>k</i>张行人图片的情况, 所以使用max函数将排序列表中前<i>k</i>张以外的图片排除在外.minmax (·) 是指对 (·) 进行归一化处理.</p>
                </div>
                <h3 id="126" name="126" class="anchor-tag"><b>3 实  验</b></h3>
                <div class="p1">
                    <p id="127">本文在Market-1501, DukeMTMC-reID和CUHK03数据集<citation id="273" type="reference"><link href="201" rel="bibliography" /><link href="203" rel="bibliography" /><link href="205" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>上进行实验, 并采用累积匹配特征曲线以及平均查准率 (mean average precision, mAP) 来评价实验性能.累积匹配特征曲线表示查询图片出现在排序后图库列表中的概率.因为在实际应用中往往只会考虑Rank-1, 即第一次就成功匹配的概率, 所以本文主要关注Rank-1.</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="129">Market-1501中的行人图片收集自6个摄像机, 其中包括来自于1 501个行人的32 668张标记好的行人图片.该数据集分为训练集, 图库集和查询集.训练集由来自751个行人的12 936张行人图片组成, 图库集则是由来自750个行人的19 732张行人图片组成.查询集合中拥有来自750个行人的3 368张查询图片.</p>
                </div>
                <div class="p1">
                    <p id="130">DukeMTMC-reID是最新发布的大规模行人重识别数据集, 其中有1 812个行人, 一共1 404个行人出现在至少2个摄像头下, 剩余408个行人仅出现在一个摄像头之下.它的训练集和测试集各包含702个人, 训练集包括16 522张图片, 图库集由17 661张图片组成, 查询集包括2 228张图片.</p>
                </div>
                <div class="p1">
                    <p id="131">CUHK03数据集由来自1 467个行人的14 097张行人图片组成.每个行人都会被2个不同的摄像机拍摄到并且在每个摄像机视图下平均每个行人有4.8张图片.该数据集有2种类型, 一种是由人工标注的, 另一种则是由检测器自动标注的.本文使用检测器标注而成的数据集, 在训练集中平均每个人有9.6张行人图片.</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132"><b>3.2 实验设置</b></h4>
                <div class="p1">
                    <p id="133">本文在Market-1501数据集上采用了Single Query和Multiple Query设置.Single Query指查询集中每个行人只有一张图片, Multiple Query指查询集中每个行人有多张图片.在CUHK03数据集上使用了single-shot和multi-shot设置.single-shot是指在图库中每个行人只有一张图片, multi-shot是指在图库中每个行人有多张图片.和本文方法进行比较的有DNS<citation id="274" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, Verif.-Identif.<citation id="275" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, SOMAnet<citation id="276" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, XQDA<citation id="277" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>等方法.DNS方法通过匹配训练数据判别零空间中的行人来克服行人重识别度量学习中的小样本问题.Verif.-Identif.方法基于验证模型和识别模型可以同时计算验证损失和识别损失, 进而可以得到一个更具有判别力的卷积神经网络和相似性度量.SOMAnet则是基于深度卷积神经网络的框架, 它试图通过对人体结构信息建模来缓解行人重识别的类内差异.XQDA方法首先提取特征, 并提出一种度量学习方法来使类内距离变小类间距离变大.</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134"><b>3.3 参数分析</b></h4>
                <div class="p1">
                    <p id="135">本文在Market1501数据集上采用控制变量法对重排序方法中的参数<i>M</i>与<i>N</i>进行测试, 以期望获得最优值.首先固定住一个参数, 然后通过调整另一个参数来查看Rank-1和mAP的变化.从图4中可以看出在Single Query设置下当<i>N</i>=8时, Rank-1和mAP达到最优值.从图5中可以看出在Single Query设置下当<i>M</i>=3时, Rank-1和mAP达到最优值.经实验测试, 在数据集CUHK03和DukeMTMC-reID上, <i>M</i>=3与<i>N</i>=8时Rank-1和mAP也可以达到最优.因此, 本文取<i>M</i>=3和<i>N</i>=8为实验参数.</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在Market1501数据集上参数N对行人重识别性能的影响" src="Detail/GetImg?filename=images/JFYZ201908006_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 在Market1501数据集上参数<i>N</i>对行人重识别性能的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Influence of parameter <i>N</i> on Re-ID performance on Market1501 dataset</p>

                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 在Market1501数据集上参数M对行人重识别性能的影响" src="Detail/GetImg?filename=images/JFYZ201908006_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 在Market1501数据集上参数<i>M</i>对行人重识别性能的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Influence of parameter <i>M</i> on Re-ID performance on Market1501 dataset</p>

                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 CUHK03数据集的生成图片" src="Detail/GetImg?filename=images/JFYZ201908006_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 CUHK03数据集的生成图片  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Generated images of CUHK03</p>

                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>3.4 生成图片的数量对Re-ID性能的影响</b></h4>
                <div class="p1">
                    <p id="140">图6～8分别展示了以CUHK03, Market-1501和DukeMTMC-reID为训练集的生成对抗网络生成的行人图片.随着生成图片数量的增加, 行人重识别的表现是否可以获得一个持续性的改进?表1展示了不同数量的生成图片参与Market-1501数据集训练的实验结果.在Market-1501数据集中训练集一共有12 936张图片.从表1中可以看出:太少的生成图片参与训练时, 标签平滑正则化的正则化能力并不充分;太多的生成图片参与训练时, 则会引入过多的干扰项.本文采取一个折中处理, 即设置参与训练的生成图片与真实图片的比例为1∶1.</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 Market-1501数据集的生成图片" src="Detail/GetImg?filename=images/JFYZ201908006_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 Market-1501数据集的生成图片  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 Generated images of Market-1501</p>

                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201908006_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 DukeMTMC-reID数据集的生成图片" src="Detail/GetImg?filename=images/JFYZ201908006_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 DukeMTMC-reID数据集的生成图片  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201908006_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Generated images of DukeMTMC-reID</p>

                </div>
                <div class="area_img" id="144">
                    <p class="img_tit"><b>表1 生成图片的数量对Re-ID性能的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Impact of the Number of Generated Images on Person Re-identification</b></p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td><br />Gan Image</td><td>Rank-1</td><td>mAP</td></tr><tr><td><br />0</td><td>0.799 3</td><td>0.587 7</td></tr><tr><td><br />6 000</td><td>0.800 5</td><td>0.589 1</td></tr><tr><td><br />12 000</td><td>0.811 5</td><td>0.593 8</td></tr><tr><td><br />18 000</td><td>0.810 9</td><td>0.588 3</td></tr><tr><td><br />24 000</td><td>0.794 8</td><td>0.568 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="145" name="145"><b>3.5 标签平滑正则化对Re-ID性能的改进</b></h4>
                <div class="p1">
                    <p id="146">表2～5展示了本文方法在Market-1501, CUHK03和DukeMTMC-reID数据集上的实验结果.在所有表中, LSR代表在训练期间加入了生成图片, rerank代表在测试期间使用了拓展近邻重排序操作.从中可以看出:当使用生成图片参与训练时, 实验效果明显超过了本文的基准方法Resnet50:在Market-1501数据上Single Query与Multiple Query情况下, Rank-1分别提升了1.22%和0.66%, mAP分别提升了0.61%和1.48% (如表2所示) ;在CUHK03数据集Single-shot与multi-shot情况下, Rank-1分别提升了2.97%和0.51%, mAP分别提升了2.32%和0.70% (如表3、表4所示) ;在DukeMTMC-reID数据上, Rank-1和mAP上分别提升了1.22%和0.64% (如表5所示) .这使我们有兴趣探索使用现实生活中真实的行人图片参与训练是否也有好的效果.为了验证这一点, 本文随机挑选DukeMTMC-reID数据集中12 000张真实的图片来代替Market-1501训练集中的生成图片.</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表2 在Market-1501数据集上的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Performance Comparison on the Market-1501 Dataset</b></p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="2"><br />Single Query</td><td colspan="2">Multiple Query</td></tr><tr><td><br />Rank-1</td><td>mAP</td><td>Rank-1</td><td>mAP</td></tr><tr><td><br />Bow<sup>[7]</sup></td><td>0.343 8</td><td>0.141 0</td><td>0.426 4</td><td>0.194 7</td></tr><tr><td><br />MR CNN<sup>[24]</sup></td><td>0.455 8</td><td>0.261 1</td><td>0.565 9</td><td>0.323 6</td></tr><tr><td><br />DNS<sup>[20]</sup></td><td>0.554 3</td><td>0.298 7</td><td>0.715 6</td><td>0.460 3</td></tr><tr><td><br />Gated-SCNN<sup>[25]</sup></td><td>0.658 8</td><td>0.395 5</td><td>0.760 4</td><td>0.484 5</td></tr><tr><td><br />SOMAnet<sup>[22]</sup></td><td>0.738 7</td><td>0.478 9</td><td>0.812 9</td><td>0.569 8</td></tr><tr><td><br />Verif.-Identif<sup>[21]</sup></td><td>0.795 1</td><td>0.598 7</td><td>0.858 4</td><td>0.703 3</td></tr><tr><td><br />ResNet50</td><td>0.799 3</td><td>0.587 7</td><td>0.870 2</td><td>0.684 4</td></tr><tr><td><br />DeepTransfer<sup>[26]</sup></td><td>0.837 0</td><td>0.655 0</td><td>0.896 0</td><td>0.738 0</td></tr><tr><td><br />Ours+LSR</td><td>0.811 5</td><td>0.593 8</td><td>0.876 8</td><td>0.699 2</td></tr><tr><td><br />Ours+LSR+rerank</td><td>0.850 1</td><td>0.753 3</td><td>0.897 0</td><td>0.828 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表3 single-shot情况下多种方法在CUHK03 数据集上的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Performance Comparison on the CUHK03 Dataset Under single-shot Setting</b></p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td><br />Methods</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td></tr><tr><td><br />KISSME<sup>[27]</sup></td><td>0.117 0</td><td>0.330 0</td><td>0.480 0</td><td></td></tr><tr><td><br />DeepReID<sup>[8]</sup></td><td>0.199 0</td><td>0.493 0</td><td>0.647 0</td><td></td></tr><tr><td><br />CaffeNet<sup>[28]</sup></td><td>0.358 0</td><td>0.653 0</td><td>0.779 6</td><td>0.426 0</td></tr><tr><td><br />VGG16<sup>[29]</sup></td><td>0.491 0</td><td>0.784 0</td><td>0.872 0</td><td>0.557 0</td></tr><tr><td><br />SOMAnet<sup>[22]</sup></td><td>0.724 0</td><td>0.921 0</td><td>0.958 0</td><td></td></tr><tr><td><br />ResNet50</td><td>0.705 3</td><td>0.915 7</td><td>0.959 4</td><td>0.752 9</td></tr><tr><td><br />Ours+LSR</td><td>0.735 0</td><td>0.922 4</td><td>0.960 4</td><td>0.776 1</td></tr><tr><td><br />Ours+LSR+rerank</td><td>0.789 5</td><td>0.917 6</td><td>0.943 0</td><td>0.817 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表4 multi-short情况下CUHK03数据集上的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Performance Comparison on the CUHK03 Dataset Under multi-shot Setting</b></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br />Methods</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td></tr><tr><td><br />CaffeNet<sup>[28]</sup></td><td>0.433 0</td><td>0.635 0</td><td>0.768 0</td><td>0.372 0</td></tr><tr><td><br />VGG16<sup>[29]</sup></td><td>0.588 0</td><td>0.802 0</td><td>0.873 0</td><td>0.510 0</td></tr><tr><td><br />Gated-SCNN<sup>[25]</sup></td><td>0.681 0</td><td>0.881 0</td><td>0.946 0</td><td>0.588 0</td></tr><tr><td><br />ResNet50</td><td>0.795 7</td><td>0.897 4</td><td>0.941 1</td><td>0.741 5</td></tr><tr><td><br />Ours+LSR</td><td>0.800 8</td><td>0.908 5</td><td>0.945 1</td><td>0.748 5</td></tr><tr><td><br />Ours+LSR+rerank</td><td>0.876 0</td><td>0.910 6</td><td>0.938 0</td><td>0.874 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表5 DukeMTMC-reID数据集上的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Performance Comparison on the DukeMTMC-reID Dataset</b></p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td><br />Methods</td><td>Rank-1</td><td>mAP</td></tr><tr><td><br />BOW+KISSME<sup>[7]</sup></td><td>0.251 3</td><td>0.121 7</td></tr><tr><td><br />LOMO+XQDA<sup>[23]</sup></td><td>0.307 5</td><td>0.170 4</td></tr><tr><td><br />OIM<sup>[30]</sup></td><td>0.681 0</td><td>0.474 0</td></tr><tr><td><br />TriNet<sup>[31]</sup></td><td>0.724 4</td><td>0.535 0</td></tr><tr><td><br />ResNet50</td><td>0.692 5</td><td>0.492 8</td></tr><tr><td><br />Ours+LSR</td><td>0.704 7</td><td>0.499 2</td></tr><tr><td><br />Ours+LSR+rerank</td><td>0.764 4</td><td>0.675 9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="152">从表6可以看出, 加入12 000张DukeMTMC-reID数据集中的真实图片参与训练也有助于模型的正则化, 改善了行人重识别的准确度, 但是加入DCGAN生成的图片参与训练取得的效果更好.</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表6 生成图片与真实图片对行人重识别性能的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 Impact of Generated Images and Real Images on Person Re-identification</b></p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td><br />Image</td><td>Rank-1</td><td>mAP</td></tr><tr><td><br />0</td><td>0.799 3</td><td>0.5877</td></tr><tr><td><br />DukeMTMC-reID-Real-12000</td><td>0.804 6</td><td>0.584 0</td></tr><tr><td><br />Market-1501-GAN-12000</td><td>0.811 5</td><td>0.593 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="154" name="154"><b>3.6 拓展近邻重排序对Re-ID性能的改进</b></h4>
                <div class="p1">
                    <p id="155">从表2～5中可以看出, 拓展近邻重排序可以有效地改善行人重识别的性能:在Market-1501数据集Single Query情况下, Rank-1和mAP分别提升了3.86%和16%, Multiple Query情况下, Rank-1和mAP分别提升了2.02%和12.94%;在CUHK03数据集single-short情况下Rank-1和mAP分别提升了5.45%和4.16%, multi-short情况下Rank-1和mAP分别提升了7.52%和12.61%;在DukeMTMC-reID数据集上Rank-1和mAP分别提升了5.97%和17.67%.</p>
                </div>
                <div class="p1">
                    <p id="156">本文还将拓展近邻重排序方法和目前2种流行的重排序方法相比较, 这2种重排序方法分别为稀疏上下文激活重排序<citation id="278" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation> (sparse contextual activa-tion, SCA) , <i>k</i>互邻重排序<citation id="279" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation> (k-reciprocal, k-r) .SCA通过考虑上下文空间中的原始成对距离, 设计一个稀疏上下文激活的特征向量来对图像进行编码.k-r则是通过<i>k</i>互近邻方法降低图像错误匹配情况, 最后将欧氏距离和杰卡德距离加权来对排序表进行重排序.表7展示了在Market-1501, CUHK03和DukeMTMC-reID这3个数据集上拓展近邻重排序方法和其他2种先进重排序方法的实验比较结果.从表7中可以看出, 本文提出的拓展近邻重排序方法无论是Rank-1指标还是mAP指标都要比上述2种重排序方法效果要好.</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表7 不同重排序方法的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 7 Comparison of Various Reranking Methods</b></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td rowspan="2"><br />Methods</td><td colspan="2"><br />Market-1501</td><td colspan="2">CUHK03</td><td colspan="2">DukeMTMC-reID</td></tr><tr><td><br />Rank-1</td><td>mAP</td><td>Rank-1</td><td>mAP</td><td>Rank-1</td><td>mAP</td></tr><tr><td><br />ResNet-50</td><td>0.799 3</td><td>0.587 7</td><td>0.705 3</td><td>0.752 9</td><td>0.692 5</td><td>0.492 8</td></tr><tr><td><br />Ours+LSR</td><td>0.811 5</td><td>0.593 8</td><td>0.735 0</td><td>0.776 1</td><td>0.704 7</td><td>0.499 2</td></tr><tr><td><br />Ours+LSR+SCA</td><td>0.831 1</td><td>0.740 1</td><td>0.750 0</td><td>0.801 9</td><td>0.738 7</td><td>0.652 7</td></tr><tr><td><br />Ours+LSR+k-r</td><td>0.838 1</td><td>0.745 4</td><td>0.760 9</td><td>0.804 4</td><td>0.745 1</td><td>0.661 8</td></tr><tr><td><br />Ours+LSR+rerank</td><td>0.850 1</td><td>0.753 3</td><td>0.789 5</td><td>0.817 7</td><td>0.764 4</td><td>0.675 9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="158" name="158" class="anchor-tag"><b>4 总  结</b></h3>
                <div class="p1">
                    <p id="159">基于深度学习算法的行人重识别方法往往需要大量标记好的训练数据, 然而标记大量的行人图片极其耗时.针对这个问题, 本文使用改进的生成对抗网络DCGAN从现有训练集中生成类似行人图片参与训练, 并采用标签平滑正则化方法来同时训练生成图片与真实图片.通过这种方式在训练过程中引入更多的颜色、光照、背景、姿势变化等信息, 提升模型的鲁棒性.大量实验表明该方法能有效提升行人重识别性能.在此基础上, 本文进一步提出了用于行人重识别的新的重排序方法——拓展近邻重排序方法.该方法根据2个排序列表前<i>k</i>个近邻的位置来计算2个排序列表之间的距离, 无需重新计算每一对图片的排序列表, 通过有说服力的实验, 证明该方法比其他重排序方法有更好的性能表现, 能更好地提升行人重识别性能.本文所报道的上述方法也在2018全球 (南京) 人工智能应用大赛多目标跨摄像头跟踪赛中得到了成功应用 (荣获第2名) .</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="189">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification:Past,present and future">

                                <b>[1]</b>Zheng Liang, Yang Yi, Hauptmann Alexander G.Person re-identification:Past, present and future[EB/OL].2016[2019-05-10].https://arxiv.org/pdf/1610.02984.pdf
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[2]</b>LeCun Y, Bengio Y, Hinton G.Deep learning[J].Nature, 2015, 521 (7553) :436- 444
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEGF3B664FA3FDBE4D126303ADEF12C24B3&amp;v=MDg0NTk2eGFnPU5pZk9hY1c3Yk5mS3EvazBaNTE3ZmdrOXV4Y1I3RHg5U3c2VzJXUTBlOEdXUWNpY0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3Mg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Wang Hongyuan, Ding Zongyuan, Zhang Ji, et al.Person reidentification by semisupervised dictionary rectification learning with retraining module[J].Journal of Electronic Imaging, 2018, 27 (4) :No.043043
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relative distance metric leaning based on clustering centralization and projection vectors learning for person re-identification">

                                <b>[4]</b>NI Tongguang, Ding Zongyuan, Chen Fuhua, et al.Relative distance metric leaning based on clustering centralization and projection vectors learning for person re-identification[J].IEEE Access, 2018, 6 (1) :11405- 11411
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201708015&amp;v=Mjk1OTJwNDlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1ZycklMeXZTZExHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Ding Zongyuan, Wang Hongyuan, Chen Fuhua, et al.Pedestrian weight recognition based on distance centralization and projection vector learning[J].Journal of Computer Research and Development, 2017, 54 (8) :1785- 1794 (in Chinese) (丁宗元, 王洪元, 陈付华, 等.基于距离中心化与投影向量学习的行人重识别[J].计算机研究与发展, 2017, 54 (8) :1785- 1794) 
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122500448808&amp;v=MTIxMzVQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUkxc1JieEk9TmlmT2ZiSzlIOVBPcW85RllPOEhCSHd4b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>Liong V E, Lu J, Ge Y.Regularized local metric learning for person re-identification[J].Pattern Recognition Letters, 2015, 68 (2015) :288- 296
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">

                                <b>[7]</b>Zheng Liang, Shen Liyue, Tian Lu, et al.Scalable person re-identification:a benchmark[C] //Proc of IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1116- 1124
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepReID:Deep filter pairing neural network for person re-identification">

                                <b>[8]</b>Li Wei, Zhao Rui, Xiao Tong, et al.Deepreid:Deep filter pairing neural network for person re-identification[C] //Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:152- 159
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance Measures and a Data Set for Multi-target">

                                <b>[9]</b>Ristani E, Solera F, Zou R, et al.Performance measures and a data set for multi-target, multi-camera tracking[C] //Proc of European Conf on Computer Vision.Berlin:Springer, 2016:17- 35
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJEG&amp;filename=SJEG2F42C2CB5ED91F63DEA377A0B00E13F5&amp;v=MjAxMDJ5YUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjZ4YWc9TmlmT2FiSE9HdE8vcmZ3M1laNTdCWDFQeVJWbm4wNStUM2lUckdBMWVjZVZScw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Ni Tongguang, Gu Xiaoqing, Wang Hongyuan, et al.Discirminative deep trasfer metric learning for cross-scenario person re-identification[J].Journal of Electronic Imaging, 2018, 27 (4) :No.043026
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA4893570C6A35DB3A1347252839AC421&amp;v=MjkzNjRGKzErRDNsTnZSVmk2eng1VDMzbnJobzJjTVBuUWJpZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjZ4YWc9TmlmT2ZjSzhGdGpQcW9oRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>Xin Xiaomeng, Wang Jinjun, Xie Ruji, et al.Semi-supervised person Re-Identification using multi-view clustering[J].Pattern Recognition, 2019, 88 (2019) :285- 297
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object retrieval and localization with spatially-constrained similarity measure and knn re-ranking">

                                <b>[12]</b>Shen Xiaohui, Lin Zhe, Brandt J, et al.Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:3013- 3020
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Coupled-view based ranking optimization for person re-identification&amp;quot;">

                                <b>[13]</b>Ye Mang, Chen Jun, Leng Qingming, et al.Coupled-view based ranking optimization for person re-identification[C] //Proc of the 21st Int Conf on Multimedia Modeling.Berlin:Springer, 2015:105- 117
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Re-ranking Person Re-identification with k-reciprocal Encoding">

                                <b>[14]</b>Zhong Zhun, Zheng Liang, Cao Donglin, et al.Re-ranking person re-identification with k-reciprocal encoding[C] //Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:1318- 1327
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse Contextual Activation for Efficient Visual Re-ranking.">

                                <b>[15]</b>Bai Song, Bai Xiang.Sparse contextual activation for efficient visual reranking[J].IEEE Transactions on Image Processing, 2016, 25 (3) :1056- 1069
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CLUSTERING USING A SIMILARITY MEASURE BASED ON SHARED NEAR NEIGHBORS">

                                <b>[16]</b>Jarvis R A, Patrick E A.Clustering using a similarity measure based on shared near neighbors[J].IEEE Transactions on Computers, 1973, 100 (11) :1025- 1034
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">

                                <b>[17]</b>Radford A, Metz L, Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].2016 [2019-05-10].https://arxiv.org/abs/1511.06434.pdf
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">

                                <b>[18]</b>Szegedy C, Vanhoucke V, Ioffe S, et al.Rethinking the inception architecture for computer vision[C] //Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2818- 2826
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pose,illumination and expression invariant pairwise face-similarity measure via doppelg?nger list comparison">

                                <b>[19]</b>Schroff F, Treibitz T, Kriegman D, et al.Pose, illumination and expression invariant pairwise face-similarity measure via doppelgänger list comparison[C] //Proc of the 29th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2011:2494- 2501
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Null Space for Person Re-identification">

                                <b>[20]</b>Zhang Li, Xiang Tao, Gong Shaogang.Learning a discriminative null space for person re-identification[C] //Proc of the 34th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:1239- 1248
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM68E09C1E98C844456DEFD79E7048C559&amp;v=MjIwMTE5eXhNVm5rb0xQSGpyMlJVMWZicm5RTCtXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjFoeDcyNnhhZz1OaWZJWTdXd2E5SEYzSTR3YmVOOEJIZw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b>Zheng Zhedong, Zheng Liang, Yang Yi.A discriminatively learned cnn embedding for person reidentification[J].ACM Transactions on Multimedia Computing, Communications, and Applications, 2018, 14 (1) :13:1- 13:20
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE3E2EA8B65341A4DDFB5F68EA6F7D4D9&amp;v=MzI2MTFnUWM2V0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaHg3MjZ4YWc9TmlmT2ZjYTdhOU81M29jM1l1NE1DSDFJeTJKbm5FMTRQbm5xMldNekQ3WA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>Barbosa I B, Cristani M, Caputo B, et al.Looking beyond appearances:Synthetic training data for deep cnns in re-identification[J].Computer Vision and Image Understanding, 2018, 167:50- 62
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Person re-identification by Local Maximal Occurrence representation and metric learning.&amp;quot;">

                                <b>[23]</b>Liao Shengcai, Hu Yang, Zhu Xiangyu, et al.Person re-identification by local maximal occurrence representation and metric learning[C] //Proc of the 33rd IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:2197- 2206
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-region bilinear convolutional neural networks for person re-identification">

                                <b>[24]</b>Ustinova E, Ganin Y, Lempitsky V.Multi-region bilinear convolutional neural networks for person re-identification[C] //Proc of the 14th IEEE Int Conf on Advanced Video and Signal Based Surveillance (AVSS) .Piscataway, NJ:IEEE, 2017:1- 6
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification">

                                <b>[25]</b>Varior R R, Haloi M, Wang G.Gated siamese convolutional neural network architecture for human re-identification[C] //Proc of European Conf on Computer Vision.Berlin:Springer, 2016:791- 808
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep transfer learning for person re-identification">

                                <b>[26]</b>Geng Mengyue, Wang Yaowei, Xiang Tao.Deep transfer learning for person re-identification[EB/OL].2016[2019-05-10].https://arxiv.org/pdf/1611.05244.pdf
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large scale metric learning from equivalence constraints">

                                <b>[27]</b>Koestinger M, Hirzer M, Wohlhart P, et al.Large scale metric learning from equivalence constraints[C] //Proc of the 30th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2012:2288- 2295
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[28]</b>Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C] //Proc of Advances in Neural Information Processing Systems.San Francisco, CA:Morgan Kaufmann, 2012:1097- 1105
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[29]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL].2015[2019-05-10].https://arxiv.org/pdf/1409.1556.pdf
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint detection and identification feature learning for person search">

                                <b>[30]</b>Xiao Tong, Li Shuang, Wang Baochao, et al.Joint detection and identification feature learning for person search[C] //Proc of the 35th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2017:3415- 3424
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of the triplet loss for person re-identification">

                                <b>[31]</b>Hermans A, Beyer L, Leibe B.In defense of the triplet loss for person re-identification[EB/OL].2017[2019-05-10].https://arxiv.org/pdf/1703.07737.pdf
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201908006" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201908006&amp;v=MjY4ODk5ak1wNDlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnl2Z1ZycklMeXZTZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVIR281VVIraUtKL2NPYXB1bz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

