<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243322783750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902002%26RESULT%3d1%26SIGN%3dCvwKQp6i3BzHhVwmm07RJxeUs5w%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902002&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902002&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902002&amp;v=MTY5NTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydklMeXZTZExHNEg5ak1yWTlGWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#434" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;神经网络概述&lt;/b&gt; "><b>1</b><b>神经网络概述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#443" data-title="&lt;b&gt;1.1&lt;/b&gt;&lt;b&gt;卷积层&lt;/b&gt;"><b>1.1</b><b>卷积层</b></a></li>
                                                <li><a href="#449" data-title="&lt;b&gt;1.2&lt;/b&gt;&lt;b&gt;池化层&lt;/b&gt;"><b>1.2</b><b>池化层</b></a></li>
                                                <li><a href="#451" data-title="&lt;b&gt;1.3&lt;/b&gt;&lt;b&gt;全连接层&lt;/b&gt;"><b>1.3</b><b>全连接层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#457" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;通用芯片对人工神经网络的支持&lt;/b&gt; "><b>2</b><b>通用芯片对人工神经网络的支持</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#459" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;CPU&lt;/b&gt;"><b>2.1</b><b>CPU</b></a></li>
                                                <li><a href="#463" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;GPU&lt;/b&gt;"><b>2.2</b><b>GPU</b></a></li>
                                                <li><a href="#469" data-title="&lt;b&gt;2.3&lt;/b&gt;&lt;b&gt;DSP&lt;/b&gt;"><b>2.3</b><b>DSP</b></a></li>
                                                <li><a href="#477" data-title="&lt;b&gt;2.4&lt;/b&gt;&lt;b&gt;FPGA&lt;/b&gt;"><b>2.4</b><b>FPGA</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#485" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;专用人工神经网络加速器&lt;/b&gt; "><b>3</b><b>专用人工神经网络加速器</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#487" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;体系结构设计&lt;/b&gt;"><b>3.1</b><b>体系结构设计</b></a></li>
                                                <li><a href="#505" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;数据流调度和优化&lt;/b&gt;"><b>3.2</b><b>数据流调度和优化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#527" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;加速器未来发展的挑战和机遇&lt;/b&gt; "><b>4</b><b>加速器未来发展的挑战和机遇</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#437" data-title="图1 神经网络的演变过程">图1 神经网络的演变过程</a></li>
                                                <li><a href="#438" data-title="图3 一个具有代表性的CNN结构——LeNet5">图3 一个具有代表性的CNN结构——LeNet5</a></li>
                                                <li><a href="#440" data-title="图2 卷积的过程">图2 卷积的过程</a></li>
                                                <li><a href="#461" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;操作数精度对识别准确率的影响&lt;/b&gt;"><b>表1</b><b>操作数精度对识别准确率的影响</b></a></li>
                                                <li><a href="#467" data-title="图4 Tensor Core的4&#215;4&#215;4矩阵乘法与累加">图4 Tensor Core的4×4×4矩阵乘法与累加</a></li>
                                                <li><a href="#468" data-title="图5 Tensor Core流程图">图5 Tensor Core流程图</a></li>
                                                <li><a href="#471" data-title="图6 EV6x结构图">图6 EV6x结构图</a></li>
                                                <li><a href="#492" data-title="图7 一个典型树状结构的NFU">图7 一个典型树状结构的NFU</a></li>
                                                <li><a href="#493" data-title="图8 TPU矩阵运算单元的脉动数据流">图8 TPU矩阵运算单元的脉动数据流</a></li>
                                                <li><a href="#510" data-title="图9 神经网络的数据流">图9 神经网络的数据流</a></li>
                                                <li><a href="#520" data-title="图10 处理2维卷积时的PE阵列中的行固定流示意图">图10 处理2维卷积时的PE阵列中的行固定流示意图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="615">


                                    <a id="bibliography_1" title="Dean J.Large-scale deep learning for building intelligent computer systems[C]Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016:1-1" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale deep learning for building intelligent computer systems">
                                        <b>[1]</b>
                                        Dean J.Large-scale deep learning for building intelligent computer systems[C]Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016:1-1
                                    </a>
                                </li>
                                <li id="617">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                    Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the 26th Annual Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105</a>
                                </li>
                                <li id="619">


                                    <a id="bibliography_3" title="Szegedy C, Liu Wei, Jia Yangqing, et al.Going deeper with convolutions[C]Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition.Los Alamitos:IEEEComputer Society, 2015:1-9" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">
                                        <b>[3]</b>
                                        Szegedy C, Liu Wei, Jia Yangqing, et al.Going deeper with convolutions[C]Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition.Los Alamitos:IEEEComputer Society, 2015:1-9
                                    </a>
                                </li>
                                <li id="621">


                                    <a id="bibliography_4" title="He Kaiming, Zhang Xianyu, Ren Shaoqing, et al.Identity mappings in deep residual networks[C]Proc of the 2016European Conf on Computer Vision.Berlin:Springer, 2016:630-645" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[4]</b>
                                        He Kaiming, Zhang Xianyu, Ren Shaoqing, et al.Identity mappings in deep residual networks[C]Proc of the 2016European Conf on Computer Vision.Berlin:Springer, 2016:630-645
                                    </a>
                                </li>
                                <li id="623">


                                    <a id="bibliography_5" title="Chen Yunji, Luo Tao, Liu Shaoli, et al.DaDianNao:Amachine-learning supercomputer[C]Proc of the 47th Annual IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2014:609-622" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dadiannao:Amachine-learning supercomputer">
                                        <b>[5]</b>
                                        Chen Yunji, Luo Tao, Liu Shaoli, et al.DaDianNao:Amachine-learning supercomputer[C]Proc of the 47th Annual IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2014:609-622
                                    </a>
                                </li>
                                <li id="625">


                                    <a id="bibliography_6" title="Chen Yunji, Chen Tianshi, Du Zidong, et al.DianNao:Asmall-footprint high-throughput accelerator for ubiquitous machine-learning[C]Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2014:269-284" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning">
                                        <b>[6]</b>
                                        Chen Yunji, Chen Tianshi, Du Zidong, et al.DianNao:Asmall-footprint high-throughput accelerator for ubiquitous machine-learning[C]Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2014:269-284
                                    </a>
                                </li>
                                <li id="627">


                                    <a id="bibliography_7" title="Liu Daofu, Chen Tianshi, Liu Shaoli, et al.PuDianNao:Apolyvalent machine learning accelerator[C]Proc of the20th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2015:369-381" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pu Dian Nao:A polyvalent machine learning accelerator">
                                        <b>[7]</b>
                                        Liu Daofu, Chen Tianshi, Liu Shaoli, et al.PuDianNao:Apolyvalent machine learning accelerator[C]Proc of the20th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2015:369-381
                                    </a>
                                </li>
                                <li id="629">


                                    <a id="bibliography_8" title="Du Zidong, Fasthuber R, Chen Tianshi, et al.ShiDianNao:Shifting vision processing closer to the sensor[C]Proc of the 42nd Annual Int Symp on Computer Architecture.New York:ACM, 2015:92-104" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ShiDianNao:Shifting vision processing closer to the sensor">
                                        <b>[8]</b>
                                        Du Zidong, Fasthuber R, Chen Tianshi, et al.ShiDianNao:Shifting vision processing closer to the sensor[C]Proc of the 42nd Annual Int Symp on Computer Architecture.New York:ACM, 2015:92-104
                                    </a>
                                </li>
                                <li id="631">


                                    <a id="bibliography_9" title="Jouppi N P, Young C, Patil N, et al.In-Datacenter performance analysis of a tensor processing unit[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1-12" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indatacenter performance analysis of a tensor processing unit">
                                        <b>[9]</b>
                                        Jouppi N P, Young C, Patil N, et al.In-Datacenter performance analysis of a tensor processing unit[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1-12
                                    </a>
                                </li>
                                <li id="633">


                                    <a id="bibliography_10" title="Google.Cloud TPUs:Google’s second-generation tensor processing unit is coming to cloud[EB/OL].[2017-10-30].https:ai.google/tools/cloud-tpus/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cloud TPUs:Google&amp;#39;&amp;#39;s second-generation tensor processing unit is coming to cloud">
                                        <b>[10]</b>
                                        Google.Cloud TPUs:Google’s second-generation tensor processing unit is coming to cloud[EB/OL].[2017-10-30].https:ai.google/tools/cloud-tpus/
                                    </a>
                                </li>
                                <li id="635">


                                    <a id="bibliography_11" title="Venkataramani S, Dubey P, Raghunathan A, et al.ScaleDeep:A scalable compute architecture for learning and evaluating deep networks[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:13-26" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ScaleDeep:A scalable compute architecture for learning and evaluating deep networks">
                                        <b>[11]</b>
                                        Venkataramani S, Dubey P, Raghunathan A, et al.ScaleDeep:A scalable compute architecture for learning and evaluating deep networks[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:13-26
                                    </a>
                                </li>
                                <li id="637">


                                    <a id="bibliography_12" title="Chen Yu-Hsin, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:367-379" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks">
                                        <b>[12]</b>
                                        Chen Yu-Hsin, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:367-379
                                    </a>
                                </li>
                                <li id="639">


                                    <a id="bibliography_13" title="Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:Aconvolutional neural network accelerator with in-situ analog arithmetic in crossbars[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:14-26" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ISAAC:a convolutional neural network accelerator with in-situ analog arithmetic in crossbars">
                                        <b>[13]</b>
                                        Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:Aconvolutional neural network accelerator with in-situ analog arithmetic in crossbars[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:14-26
                                    </a>
                                </li>
                                <li id="641">


                                    <a id="bibliography_14" title="Parashar A, Rhu M, Mukkara A, et al.SCNN:An accelerator for compressed-sparse convolutional neural networks[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:27-40" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SCNN:An accelerator for compressed-sparse convolutional neural networks">
                                        <b>[14]</b>
                                        Parashar A, Rhu M, Mukkara A, et al.SCNN:An accelerator for compressed-sparse convolutional neural networks[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:27-40
                                    </a>
                                </li>
                                <li id="643">


                                    <a id="bibliography_15" title="Akopyan F, Sawada J, Cassidy A, et al.TrueNorth:Design and tool flow of a 65mW 1 million neuron programmable neurosynaptic chip[J].IEEE Transactions on ComputerAided Design of Integrated Circuits and Systems, 2015, 34 (10) :1537-1557" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TrueNorth:Design and Tool Flow of a 65 mW 1Million Neuron Programmable Neurosynaptic Chip">
                                        <b>[15]</b>
                                        Akopyan F, Sawada J, Cassidy A, et al.TrueNorth:Design and tool flow of a 65mW 1 million neuron programmable neurosynaptic chip[J].IEEE Transactions on ComputerAided Design of Integrated Circuits and Systems, 2015, 34 (10) :1537-1557
                                    </a>
                                </li>
                                <li id="645">


                                    <a id="bibliography_16" title="Furber S B, Lester D R, Plana L A, et al.Overview of the spiNNaker system architecture[J].IEEE Transactions on Computers, 2013, 62 (12) :2454-2467" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overview of the Spi NNaker system architecture">
                                        <b>[16]</b>
                                        Furber S B, Lester D R, Plana L A, et al.Overview of the spiNNaker system architecture[J].IEEE Transactions on Computers, 2013, 62 (12) :2454-2467
                                    </a>
                                </li>
                                <li id="647">


                                    <a id="bibliography_17" title="Furber S B, Galluppi F, Temple S, et al.The SpiNNaker project[J].Proceedings of the IEEE, 2014, 102 (5) :652-665" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The SpiNNaker project">
                                        <b>[17]</b>
                                        Furber S B, Galluppi F, Temple S, et al.The SpiNNaker project[J].Proceedings of the IEEE, 2014, 102 (5) :652-665
                                    </a>
                                </li>
                                <li id="649">


                                    <a id="bibliography_18" title="Jin Xin, Lujan M, Plana L A, et al.Modeling spiking neural networks on SpiNNaker[J].Computing in Science&amp;amp;Engineering, 2010, 12 (5) :91-97" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MODELING SPIKING NEURAL NETWORKS ON SPINNAKER">
                                        <b>[18]</b>
                                        Jin Xin, Lujan M, Plana L A, et al.Modeling spiking neural networks on SpiNNaker[J].Computing in Science&amp;amp;Engineering, 2010, 12 (5) :91-97
                                    </a>
                                </li>
                                <li id="651">


                                    <a id="bibliography_19" title="McCulloch W S, Pitts W.A logical calculus of the ideas immanent in nervous activity[J].Bulletin of Mathematical Biophysics, 1943, 5 (4) :115-133" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001082082&amp;v=MDY3NzNTbmxWYjNCSkY0PU5qN0Jhck80SHRITnI0ZEhaT01OWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRG&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                        McCulloch W S, Pitts W.A logical calculus of the ideas immanent in nervous activity[J].Bulletin of Mathematical Biophysics, 1943, 5 (4) :115-133
                                    </a>
                                </li>
                                <li id="653">


                                    <a id="bibliography_20" title="Rosenblatt F.The perceptron:A probabilistic model for information storage and organization in the brain[J].Psychological Review, 1958, 65 (6) :386-408" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain">
                                        <b>[20]</b>
                                        Rosenblatt F.The perceptron:A probabilistic model for information storage and organization in the brain[J].Psychological Review, 1958, 65 (6) :386-408
                                    </a>
                                </li>
                                <li id="655">


                                    <a id="bibliography_21" title="Hubel D H, Wiesel T N.Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex[J].Journal of Physiology, 1962, 160 (1) :106-154" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Receptive fields, binocular interaction and functional architecture in the cat&amp;#39;s visual cortex">
                                        <b>[21]</b>
                                        Hubel D H, Wiesel T N.Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex[J].Journal of Physiology, 1962, 160 (1) :106-154
                                    </a>
                                </li>
                                <li id="657">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                    L&#233;cun Y, Bottou L, Bengio Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324</a>
                                </li>
                                <li id="659">


                                    <a id="bibliography_23" title="B9ttcher A, Silbermann B.Introduction to Large Truncated Toeplitz Matrices[M].Berlin:Springer, 1999" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction to Large Truncated Toeplitz Matrices">
                                        <b>[23]</b>
                                        B9ttcher A, Silbermann B.Introduction to Large Truncated Toeplitz Matrices[M].Berlin:Springer, 1999
                                    </a>
                                </li>
                                <li id="661">


                                    <a id="bibliography_24" title="Winograd S.Arithmetic Complexity of Computations[M].Philiadelphia PA:Society for Industrial&amp;amp;Applied Mathematics, 1980" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Arithmetic Complexity of Computations">
                                        <b>[24]</b>
                                        Winograd S.Arithmetic Complexity of Computations[M].Philiadelphia PA:Society for Industrial&amp;amp;Applied Mathematics, 1980
                                    </a>
                                </li>
                                <li id="663">


                                    <a id="bibliography_25" title="Vasilache N, Johnson J, Mathieu M, et al.Fast convolutional nets with fbfft:A GPU performance evaluation[OL].[2018-04-13].https:arxiv.org/abs/1412.7580" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast convolutional nets with fbfft:A GPU performance evaluation[OL]">
                                        <b>[25]</b>
                                        Vasilache N, Johnson J, Mathieu M, et al.Fast convolutional nets with fbfft:A GPU performance evaluation[OL].[2018-04-13].https:arxiv.org/abs/1412.7580
                                    </a>
                                </li>
                                <li id="665">


                                    <a id="bibliography_26" title="Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the 26th Annual Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press 2012:1097-1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolution neural networks">
                                        <b>[26]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the 26th Annual Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press 2012:1097-1105
                                    </a>
                                </li>
                                <li id="667">


                                    <a id="bibliography_27" title="Smith S W.The Scientist and Engineer’s Guide to Digital Signal Processing[M].Poway, CA:California Technical Publishing, 1997" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Scientist and Engineer&amp;#39;&amp;#39;s Guide to Digital Signal Processing">
                                        <b>[27]</b>
                                        Smith S W.The Scientist and Engineer’s Guide to Digital Signal Processing[M].Poway, CA:California Technical Publishing, 1997
                                    </a>
                                </li>
                                <li id="669">


                                    <a id="bibliography_28" title="He Kaiming, Zhang Xianyu, Ren Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence, 2014, 37 (9) :1904-1916" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">
                                        <b>[28]</b>
                                        He Kaiming, Zhang Xianyu, Ren Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence, 2014, 37 (9) :1904-1916
                                    </a>
                                </li>
                                <li id="671">


                                    <a id="bibliography_29" title="Dong L T, Mintram R.Genetic algorithm-neural network (GANN) :A study of neural network activation functions and depth of genetic algorithm search applied to feature selection[J].International Journal of Machine Learning&amp;amp;Cybernetics, 2010, 1 (1/2/3/4) :75-87" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Genetic Algorithm-Neural Network (GANN): A study of neural network activation functions and depth of genetic algorithm search applied to feature selection">
                                        <b>[29]</b>
                                        Dong L T, Mintram R.Genetic algorithm-neural network (GANN) :A study of neural network activation functions and depth of genetic algorithm search applied to feature selection[J].International Journal of Machine Learning&amp;amp;Cybernetics, 2010, 1 (1/2/3/4) :75-87
                                    </a>
                                </li>
                                <li id="673">


                                    <a id="bibliography_30" title="Jia Y, Shelhamer E, Donahue J, et al.Caffe:Convolutional architecture for fast feature embedding[C]Proc of the 22nd ACM Int Conf on Multimedia.New York:ACM, 2014:675-678" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">
                                        <b>[30]</b>
                                        Jia Y, Shelhamer E, Donahue J, et al.Caffe:Convolutional architecture for fast feature embedding[C]Proc of the 22nd ACM Int Conf on Multimedia.New York:ACM, 2014:675-678
                                    </a>
                                </li>
                                <li id="675">


                                    <a id="bibliography_31" title="Abadi M, Agarwal A, Barham P, et al.TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL].[2018-04-13].https:arxiv.org/abs/1603.04467" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL]">
                                        <b>[31]</b>
                                        Abadi M, Agarwal A, Barham P, et al.TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL].[2018-04-13].https:arxiv.org/abs/1603.04467
                                    </a>
                                </li>
                                <li id="677">


                                    <a id="bibliography_32" title="Patrick K.Intel Xeon Phi Knights Mill for machine learning[EB/OL]. (2017-08-21) [2017-10-18].https:www.servethehome.com/intel-knights-mill-for-machine-learning/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intel Xeon Phi Knights Mill for machine learning">
                                        <b>[32]</b>
                                        Patrick K.Intel Xeon Phi Knights Mill for machine learning[EB/OL]. (2017-08-21) [2017-10-18].https:www.servethehome.com/intel-knights-mill-for-machine-learning/
                                    </a>
                                </li>
                                <li id="679">


                                    <a id="bibliography_33" title="Anker G.Cuda-convnet2:A fast C++/Cuda implementation of convolutional (or more generally, feedforward) neural networks[EB/OL].[2017-10-20].https:code.google.com/p/cuda-convnet2/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cuda-convnet2:A fast C++/Cuda implementation of convolutional (or more generally,feedforward)neural networks">
                                        <b>[33]</b>
                                        Anker G.Cuda-convnet2:A fast C++/Cuda implementation of convolutional (or more generally, feedforward) neural networks[EB/OL].[2017-10-20].https:code.google.com/p/cuda-convnet2/
                                    </a>
                                </li>
                                <li id="681">


                                    <a id="bibliography_34" title="Collobert R, Kavukcuoglu K, Farabet C.Torch7:A matlablike environment for machine learning[C/OL]Proc of NIPSWorkshop.2011[2018-04-13].http:citeseerx.ist.psu.edu/viewdoc/summary/doi=10.1.1.231.4195" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Torch7:A matlablike environment for machine learning[C/OL]">
                                        <b>[34]</b>
                                        Collobert R, Kavukcuoglu K, Farabet C.Torch7:A matlablike environment for machine learning[C/OL]Proc of NIPSWorkshop.2011[2018-04-13].http:citeseerx.ist.psu.edu/viewdoc/summary/doi=10.1.1.231.4195
                                    </a>
                                </li>
                                <li id="683">


                                    <a id="bibliography_35" title="Bastien F, Lamblin P, Pascanu R, et al.Theano:New features and speed improvements[J/OL].Computer Science, 2012[2018-04-13].https:arxiv.org/abs/1211.5590" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Theano:New features and speed improvements">
                                        <b>[35]</b>
                                        Bastien F, Lamblin P, Pascanu R, et al.Theano:New features and speed improvements[J/OL].Computer Science, 2012[2018-04-13].https:arxiv.org/abs/1211.5590
                                    </a>
                                </li>
                                <li id="685">


                                    <a id="bibliography_36" title="NVIDIA.Cudnn:GPU-accelerated library of primitives for deep neural networks[EB/OL].[2017-10-30].https:developer.nvidia.com/cuDNN" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cudnn:GPU-accelerated library of primitives for deep neural networks">
                                        <b>[36]</b>
                                        NVIDIA.Cudnn:GPU-accelerated library of primitives for deep neural networks[EB/OL].[2017-10-30].https:developer.nvidia.com/cuDNN
                                    </a>
                                </li>
                                <li id="687">


                                    <a id="bibliography_37" title="Vasilache N, Johnson J, Mathieu M, et al.Fast convolutional nets with fbfft:A GPU performance evaluation[OL].[2018-04-13].https:arxiv.org/abs/1412.7580" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast convolutional nets with fbfft:A GPU performance evaluation[OL]">
                                        <b>[37]</b>
                                        Vasilache N, Johnson J, Mathieu M, et al.Fast convolutional nets with fbfft:A GPU performance evaluation[OL].[2018-04-13].https:arxiv.org/abs/1412.7580
                                    </a>
                                </li>
                                <li id="689">


                                    <a id="bibliography_38" title="Cooper G.Facial expression analysis with deep learning&amp;amp;computer vision[EB/OL].[2018-04-13].https:www.synopsys.com/designware-ip/technical-bulletin/ev-facial-expressiondwtb-q117.html" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression analysis with deep learning&amp;amp;computer vision">
                                        <b>[38]</b>
                                        Cooper G.Facial expression analysis with deep learning&amp;amp;computer vision[EB/OL].[2018-04-13].https:www.synopsys.com/designware-ip/technical-bulletin/ev-facial-expressiondwtb-q117.html
                                    </a>
                                </li>
                                <li id="691">


                                    <a id="bibliography_39" title="CEVA.CEVA-XM6:Fifth-generation computer vision and deep learning embedded platform[EB/OL].[2017-10-30]].https:www.ceva-dsp.com/product/ceva-xm6/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CEVA-XM6:Fifth-generation computer vision and deep learning embedded platform">
                                        <b>[39]</b>
                                        CEVA.CEVA-XM6:Fifth-generation computer vision and deep learning embedded platform[EB/OL].[2017-10-30]].https:www.ceva-dsp.com/product/ceva-xm6/
                                    </a>
                                </li>
                                <li id="693">


                                    <a id="bibliography_40" title="Miya K.VeriSilicon’s Vivante VIP8000 neural network processor IP delivers over 3 Tera MACs Per second[EB/OL].[2017-10-30].http:www.verisilicon.com/newsdetail_499_VivanteVIP8000.html" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=VeriSilicon&amp;#39;&amp;#39;s Vivante VIP8000 neural network processor IP delivers over 3 Tera MACs Per second">
                                        <b>[40]</b>
                                        Miya K.VeriSilicon’s Vivante VIP8000 neural network processor IP delivers over 3 Tera MACs Per second[EB/OL].[2017-10-30].http:www.verisilicon.com/newsdetail_499_VivanteVIP8000.html
                                    </a>
                                </li>
                                <li id="695">


                                    <a id="bibliography_41" title="Cadence.Tensilica Vision DSPs for imaging, computer vision, and neural networks[EB/OL].[2017-10-18].https:ip.cadence.com/vision" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensilica Vision DSPs for imaging,computer vision,and neural networks">
                                        <b>[41]</b>
                                        Cadence.Tensilica Vision DSPs for imaging, computer vision, and neural networks[EB/OL].[2017-10-18].https:ip.cadence.com/vision
                                    </a>
                                </li>
                                <li id="697">


                                    <a id="bibliography_42" >
                                        <b>[42]</b>
                                    Yun S B, Kim Y J, Dong S S, et al.Hardware implementation of neural network with expansible and reconfigurable architecture[C]Proc of the 9th Int Conf on Neural Information Processing.Piscataway, NJ:IEEE, 2002:970-975</a>
                                </li>
                                <li id="699">


                                    <a id="bibliography_43" >
                                        <b>[43]</b>
                                    Farabet C, Martini B, Corda B, et al.NeuFlow:A runtime reconfigurable dataflow processor for vision[C]Proc of the29th Computer Vision and Pattern Recognition Workshops.Piscataway, NJ:IEEE, 2011:109-116</a>
                                </li>
                                <li id="701">


                                    <a id="bibliography_44" title="Zhang Chi, Prasanna V.Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C]Proc of the 25th ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2017:35-44" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system">
                                        <b>[44]</b>
                                        Zhang Chi, Prasanna V.Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C]Proc of the 25th ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2017:35-44
                                    </a>
                                </li>
                                <li id="703">


                                    <a id="bibliography_45" title="Gao Mingyu, Pu Jing, Yang Xuan, et al.TETRIS:Scalable and efficient neural network acceleration with 3D memory[C]Proc of the 22nd Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2017:751-764" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TETRIS:Scalable and efficient neural network acceleration with 3D memory">
                                        <b>[45]</b>
                                        Gao Mingyu, Pu Jing, Yang Xuan, et al.TETRIS:Scalable and efficient neural network acceleration with 3D memory[C]Proc of the 22nd Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2017:751-764
                                    </a>
                                </li>
                                <li id="705">


                                    <a id="bibliography_46" title="Li Zhen, Wang Yuqing, Zhi Tian, et al.A survey of neural network accelerators[J].Frontiers of Computer Science, 2017, 11 (5) :746-761" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of neural network accelerators">
                                        <b>[46]</b>
                                        Li Zhen, Wang Yuqing, Zhi Tian, et al.A survey of neural network accelerators[J].Frontiers of Computer Science, 2017, 11 (5) :746-761
                                    </a>
                                </li>
                                <li id="707">


                                    <a id="bibliography_47" title="Kim D, Kung J, Chai S, et al.Neurocube:A programmable digital neuromorphic architecture with high-density 3Dmemory[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:380-392" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neurocube:A programmable digital neuromorphic architecture with high-density 3Dmemory">
                                        <b>[47]</b>
                                        Kim D, Kung J, Chai S, et al.Neurocube:A programmable digital neuromorphic architecture with high-density 3Dmemory[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:380-392
                                    </a>
                                </li>
                                <li id="709">


                                    <a id="bibliography_48" title="Chi Ping, Li Shuangchen, Xu Cong, et al.PRIME:A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2016:27-39" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Prime:a novel processing-in-memory architecture for neural net-work computation in reram-based main memory">
                                        <b>[48]</b>
                                        Chi Ping, Li Shuangchen, Xu Cong, et al.PRIME:A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2016:27-39
                                    </a>
                                </li>
                                <li id="711">


                                    <a id="bibliography_49" title="Ji Yu, Zhang Youhui, Li Shuangchen, et al.NEUTRAMS:Neural network transformation and co-design under neuromorphic hardware constraints[C]Proc of the 49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.21" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NEUTRAMS:Neural network transformation and co-design under neuromorphic hardware constraints">
                                        <b>[49]</b>
                                        Ji Yu, Zhang Youhui, Li Shuangchen, et al.NEUTRAMS:Neural network transformation and co-design under neuromorphic hardware constraints[C]Proc of the 49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.21
                                    </a>
                                </li>
                                <li id="713">


                                    <a id="bibliography_50" title="Li Chuxi, Fan Xiaoya, Zhao Changhe, et al.A memristorbased processing-in-memory architecture for deep convolutional neural networks approximate computation[J].Journal of Computer Research and Development, 2017, 54 (6) :1367-1380 (in Chinese) (李楚曦, 樊晓桠, 赵昌和, 等.基于忆阻器的PIM结构实现深度卷积神经网络近似计算[J].计算机研究与发展, 2017, 54 (6) :1367-1380) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201706020&amp;v=MzExMzlJTHl2U2RMRzRIOWJNcVk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XcnY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[50]</b>
                                        Li Chuxi, Fan Xiaoya, Zhao Changhe, et al.A memristorbased processing-in-memory architecture for deep convolutional neural networks approximate computation[J].Journal of Computer Research and Development, 2017, 54 (6) :1367-1380 (in Chinese) (李楚曦, 樊晓桠, 赵昌和, 等.基于忆阻器的PIM结构实现深度卷积神经网络近似计算[J].计算机研究与发展, 2017, 54 (6) :1367-1380) 
                                    </a>
                                </li>
                                <li id="715">


                                    <a id="bibliography_51" title="Liu Shaoli, Du Zidong, Tao Jinhua, et al.Cambricon:An instruction set architecture for neural networks[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:393-405" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cambricon:an instruction set architecture for neural networks">
                                        <b>[51]</b>
                                        Liu Shaoli, Du Zidong, Tao Jinhua, et al.Cambricon:An instruction set architecture for neural networks[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:393-405
                                    </a>
                                </li>
                                <li id="717">


                                    <a id="bibliography_52" title="Sankaradas M, Jakkula V, Cadambi S, et al.A massively parallel coprocessor for convolutional neural networks[C]Proc of the 20th IEEE Int Conf on Application-Specific Systems, Architectures and Processors.Piscataway, NJ:IEEE, 2009:53-60" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A massively parallel coprocessor for convolutional neural networks">
                                        <b>[52]</b>
                                        Sankaradas M, Jakkula V, Cadambi S, et al.A massively parallel coprocessor for convolutional neural networks[C]Proc of the 20th IEEE Int Conf on Application-Specific Systems, Architectures and Processors.Piscataway, NJ:IEEE, 2009:53-60
                                    </a>
                                </li>
                                <li id="719">


                                    <a id="bibliography_53" title="Sriram V, Cox D, Tsoi K H, et al.Towards an embedded biologically-inspired machine vision processor[C]Proc of the 9th Int Conf on Field-Programmable Technology.Piscataway, NJ:IEEE, 2011:273--278" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards an embedded biologically-inspired machine vision processor">
                                        <b>[53]</b>
                                        Sriram V, Cox D, Tsoi K H, et al.Towards an embedded biologically-inspired machine vision processor[C]Proc of the 9th Int Conf on Field-Programmable Technology.Piscataway, NJ:IEEE, 2011:273--278
                                    </a>
                                </li>
                                <li id="721">


                                    <a id="bibliography_54" title="Chakradhar S, Sankaradas M, Jakkula V, et al.Adynamically configurable coprocessor for convolutional neural networks[C]Proc of the 38th Int Symp on Computer Architecture.New York:ACM, 2010:247-257" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000014244&amp;v=MjI5MThmSVk3SzdIdGpOcjQ5RlpPb0xEbmc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSmx3ZGJoST1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[54]</b>
                                        Chakradhar S, Sankaradas M, Jakkula V, et al.Adynamically configurable coprocessor for convolutional neural networks[C]Proc of the 38th Int Symp on Computer Architecture.New York:ACM, 2010:247-257
                                    </a>
                                </li>
                                <li id="723">


                                    <a id="bibliography_55" title="Gupta S, Agrawal A, Gopalakrishnan K, et al.Deep learning with limited numerical precision[J/OL].Computer Science, 2015[2018-04-13].https:arxiv.org/abs/1502.02551" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning with limited numerical precision">
                                        <b>[55]</b>
                                        Gupta S, Agrawal A, Gopalakrishnan K, et al.Deep learning with limited numerical precision[J/OL].Computer Science, 2015[2018-04-13].https:arxiv.org/abs/1502.02551
                                    </a>
                                </li>
                                <li id="725">


                                    <a id="bibliography_56" title="Peemen M, Setio A A A, Mesman B, et al.Memory-centric accelerator design for convolutional neural networks[C]Proc of the 31st Int Conf on Computer Design.Piscataway, NJ:IEEE, 2013:13-19" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Memory-centric accelerator design for convolutional neural networks">
                                        <b>[56]</b>
                                        Peemen M, Setio A A A, Mesman B, et al.Memory-centric accelerator design for convolutional neural networks[C]Proc of the 31st Int Conf on Computer Design.Piscataway, NJ:IEEE, 2013:13-19
                                    </a>
                                </li>
                                <li id="727">


                                    <a id="bibliography_57" title="Zhang Chen, Li Peng, Sun Guangyu, et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2015:161-170" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing fpga-based accelerator design for deep convolu-tional neural networks">
                                        <b>[57]</b>
                                        Zhang Chen, Li Peng, Sun Guangyu, et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2015:161-170
                                    </a>
                                </li>
                                <li id="729">


                                    <a id="bibliography_58" title="Albericio J, Judd P, Hetherington T, et al.Cnvlutin:Ineffectual-neuron-free deep neural network computing[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:1-13" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cnvlutin:Ineffectual-neuron-free deep neural network computing">
                                        <b>[58]</b>
                                        Albericio J, Judd P, Hetherington T, et al.Cnvlutin:Ineffectual-neuron-free deep neural network computing[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:1-13
                                    </a>
                                </li>
                                <li id="731">


                                    <a id="bibliography_59" >
                                        <b>[59]</b>
                                    Zhang Shijin, Du Zidong, Zhang Lei, et al.Cambricon-X:An accelerator for sparse neural networks[C]Proc of the49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.20</a>
                                </li>
                                <li id="733">


                                    <a id="bibliography_60" title="Han Song, Liu Xingyu, Mao Huizi, et al.EIE:Efficient inference engine on compressed deep neural network[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:243-254" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EIE:Efficient inference engine on compressed deep neural network">
                                        <b>[60]</b>
                                        Han Song, Liu Xingyu, Mao Huizi, et al.EIE:Efficient inference engine on compressed deep neural network[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:243-254
                                    </a>
                                </li>
                                <li id="735">


                                    <a id="bibliography_61" title="Yu Jiecao, Lukefahr A, Palframan D, et al.Scalpel:Customizing DNN pruning to the underlying hardware parallelism[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:548-560" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalpel:Customizing DNN pruning to the underlying hardware parallelism">
                                        <b>[61]</b>
                                        Yu Jiecao, Lukefahr A, Palframan D, et al.Scalpel:Customizing DNN pruning to the underlying hardware parallelism[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:548-560
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-01-29 13:16</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),240-253 DOI:10.7544/issn1000-1239.2019.20170852            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>硬件加速神经网络综述</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%A1%82%E6%9E%97&amp;code=41253831&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈桂林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E8%83%9C&amp;code=23817039&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E9%98%B3&amp;code=20282380&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭阳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0269230&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国防科技大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>人工神经网络目前广泛应用于人工智能的应用当中, 如语音助手、图像识别和自然语言处理等.随着神经网络愈加复杂, 计算量也急剧上升, 传统的通用芯片在处理复杂神经网络时受到了带宽和能耗的限制, 人们开始改进通用芯片的结构以支持神经网络的有效处理.此外, 研发专用加速芯片也成为另一条加速神经网络处理的途径.与通用芯片相比, 它能耗更低, 性能更高.通过介绍目前通用芯片和专用芯片对神经网络所作的支持, 了解最新神经网络硬件加速平台设计的创新点和突破口.具体来说, 主要概述了神经网络的发展, 讨论各类通用芯片为支持神经网络所作的改进, 其中包括支持低精度运算和增加一个加速神经网络处理的计算模块.然后从运算结构和存储结构的角度出发, 归纳专用芯片在体系结构上所作的定制设计, 另外根据神经网络中各类数据的重用总结了各个神经网络加速器所采用的数据流.最后通过对已有加速芯片的优缺点分析, 给出了神经网络加速器未来的设计趋势和挑战.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%9A%E7%94%A8%E8%8A%AF%E7%89%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">通用芯片;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%93%E7%94%A8%E5%8A%A0%E9%80%9F%E8%8A%AF%E7%89%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">专用加速芯片;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">体系结构;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *马胜 (masheng@nudt.edu.cn) ;
                                </span>
                                <span>
                                    陈桂林 cglnudt@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-11-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61672526);</span>
                                <span>国防科技大学科研计划项目 (ZK17-03-06);</span>
                    </p>
            </div>
                    <h1><b>Survey on Accelerating Neural Network with Hardware</b></h1>
                    <h2>
                    <span>Chen Guilin</span>
                    <span>Ma Sheng</span>
                    <span>Guo Yang</span>
            </h2>
                    <h2>
                    <span>College of Computer , National University of Defense Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Artificial neural networks are widely used in artificial intelligence applications such as voice assistant, image recognition and natural language processing. With the rise of complexity of the application, the computational complexity has also increased dramatically. The traditional general-purpose processor is limited by the memory bandwidth and energy consumption when dealing with the complex neural network. People began to improve the architecture of the general-purpose processors to support the efficient processing of the neural network. In addition, the development of special-purpose accelerators becomes another way to accelerate processing of neural network. Compared with the general-purpose processor, it has lower energy consumption and higher performance. The article aims to introduce the designs from current general-purpose processors and special-purpose accelerators for supporting the neural network. It also summarizes the latest design innovation and breakthrough of the neural network acceleration platforms. In particular, the article provides an overview of the neural network and discusses the improvements made by various general-purpose chips to support neural networks, which include supporting low-precision operations and adding a calculation module to speed up neural network processing. Then from the viewpoint of the computational structure and storage structure, the article summarizes the customized designs of special-purpose accelerators, and describes the dataflow used by the neural network chips based on the reuse of various types of the data in the neural network. Through analyzing the advantages and disadvantages of these solutions, the article puts forward the future design trend and challenge of the neural network accelerator.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=general-purpose%20processor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">general-purpose processor;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=special-purpose%20accelerator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">special-purpose accelerator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=architecture&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">architecture;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Chen Guilin, born in 1994.MSc.Student member of CCF.His main research interests include architecture, machine learning and neural network accelerator.<image id="610" type="formula" href="images/JFYZ201902002_61000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Ma Sheng, born in 1986.PhD, assistant professor.Member of CCF.His main research interests include computer architecture, on-chip networks and arithmetic unit designs.<image id="612" type="formula" href="images/JFYZ201902002_61200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Guo Yang, born in 1971.PhD, professor, PhD supervisor.Senior member of CCF.His main research interests include low power VLSI circuits, microprocessor design and verification, and electronic design automation (EDA) techniques for VLSIcircuits. (guoyang@nudt.edu.cn) <image id="614" type="formula" href="images/JFYZ201902002_61400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-11-14</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61672526);</span>
                                <span>the Research Project of NUDT (ZK17-03-06);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="432">随着数据的爆炸式增长和计算性能的阶跃式提升, 机器学习的研究在经历20世纪由计算瓶颈引起的寒潮后重新变得火热起来.机器学习算法已广泛地运用到许多智能应用和云服务之中.常见应用有语音识别如苹果Siri和微软小娜, 面部识别如Apple iPhoto或Google Picasa, 同时智能机器人也大量使用了机器学习算法.目前, 机器学习领域最火热的是以神经网络为代表的深度学习.据统计, 深度神经网络运用在语音识别上比传统方法要减少了30%的单词识别错误率<sup><a class="sup">[1]</a></sup>, 将图像识别竞赛的错误率从2011年的26%降至3.5%<sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup>.此外它还被广泛地应用到数据挖掘中.但由于神经网络的应用范围越来越广, 精度要求越来越高, 导致神经网络的规模也越来越大.通常对大规模神经网络加速的方法是设计性能更强大的通用芯片, 并且增加专门的神经网络处理模块, 如英特尔的Knight Mill和英伟达最新的Volte架构都添加了对神经网络的加速模块.不过它们作为通用运算器件始终限制了它们完成特殊任务时的效率, 比如CPU必须包含高速缓存、分支预测、批处理、地址合并、多线程、上下文切换等多种通用功能, 这些功能并不会完全用于神经网络的加速, 但它们会占用芯片的设计面积.这就导致神经网络加速时硬件资源并没有完全利用.所以人们也开始设计专用芯片来实现对大型神经网络的加速.</p>
                </div>
                <div class="p1">
                    <p id="433">近年来各大科研机构纷纷提出了各自的加速器结构, 如中国科学院陈天石团队<sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup>的Diannao家族、Google的TPU<sup><a class="sup">[9]</a></sup> (tensor process unit) 和cloud TPU<sup><a class="sup">[10]</a></sup>、普度大学推出的Scaledeep<sup><a class="sup">[11]</a></sup>、MIT (Massa-chusetts Institute of Technology) 提出的Eyeriss<sup><a class="sup">[12]</a></sup>、HP实验室和犹他大学联合提出的基于忆阻器的ISAAC<sup><a class="sup">[13]</a></sup>, 以及Parashar等人<sup><a class="sup">[14]</a></sup>提出的压缩稀疏卷积神经网络加速器SCNN等.现有神经网络加速芯片的研究主要集中在4个方面:1) 从神经网络的计算结构出发, 研究了树状结构和阵列结构如何高效地完成神经网络的卷积运算;2) 从存储的瓶颈角度出发, 研究了3D存储技术如何应用到加速器的设计中;3) 从新材料器件的探索出发, 研究了忆阻器等新器件如何实现神经网络处理存储一体化;4) 从数据流的调度和优化出发, 研究了如何最大化利用网络中各类数据的局部重用以及稀疏网络的处理.</p>
                </div>
                <h3 id="434" name="434" class="anchor-tag"><b>1</b><b>神经网络概述</b></h3>
                <div class="p1">
                    <p id="435">神经网络分为生物启发的神经网络 (biologically inspired neural network) 和人工神经网络 (artificial neural network) 2类.前者由神经生物学家关注, 用来建立一种灵感来源于生物学的模型以帮助理解神经网络和大脑是如何运转的, 比较有代表性的是脉冲神经网络.IBM的TrueNorth<sup><a class="sup">[15]</a></sup>和Furber等人<sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup>提出的SpiNNaker就是这种结构.但由于脉冲神经网络在处理机器学习任务时精度低的缺点, 目前并没有得到广泛应用, 本文不对其进行过多的赘述.后者人工神经网络是本文讨论的重点.</p>
                </div>
                <div class="p1">
                    <p id="436">人工神经网络是一种典型的机器学习方法, 也是深度学习的一种重要形式.1943年McCulloch和Pitts<sup><a class="sup">[19]</a></sup>提出了第1个人工神经元模型 (M-P神经元如图1 (a) ) , 在这个模型中, 神经元接受来自<i>n</i>个其他神经元传递过来的信号, 这些输入信号通过带权重的连接进行传递, 神经元接受到的总输入值将与神经元的阈值进行比较, 然后通过“激活函数”产生神经元的输出.常用的激活函数有sigmoid函数、阶跃函数、ReLU等非线性函数.为了把人工神经网络研究从理论探讨付诸工程实践, 1958年Rosenblatt<sup><a class="sup">[20]</a></sup>设计制作了感知机, 如图1 (b) .感知机输入层并不是功能神经元, 只接受外界输入信号后传递给输出层, 输出层是一个M-P功能神经元.一个感知机可以实现逻辑与、或、非等简单的线性可分问题, 但要解决一个复杂的非线性可分问题时, 就要用到多层感知机 (神经网络) .如图1 (c) 的2层感知机可以解决一个异或问题, 与单层感知机相比, 它包含一个隐含层.当隐含层不断增加就有了深度神经网络 (deep neural network, DNN) 的概念, 如图1 (d) .这里所谓的深度并没有严格的定义, 在语音识别中4层网络就可以被认为是“较深的”, 而在图像识别中20层以上的网络比比皆是.不过随着隐含层数的增加, 深度神经网络有个明显的问题——参数数量的急剧膨胀.这就导致了计算量的急剧上升, 进而使得网络模型的计算时间增加、计算功耗升高.为了解决这个问题, 20世纪60年代, Hubel和Wiesel<sup><a class="sup">[21]</a></sup>提出了卷积神经网络 (convolutional neural network, CNN) 的概念.卷积神经网络局部感知和参数共享的特点使它能够有效地减少参数数量, 降低深度神经网络的复杂性.</p>
                </div>
                <div class="area_img" id="437">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_437.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 神经网络的演变过程" src="Detail/GetImg?filename=images/JFYZ201902002_437.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 神经网络的演变过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_437.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 The evolution of neural network</p>

                </div>
                <div class="area_img" id="438">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_438.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 一个具有代表性的CNN结构——LeNet5" src="Detail/GetImg?filename=images/JFYZ201902002_438.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 一个具有代表性的CNN结构——LeNet5  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_438.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 A representative CNN architecture—LeNet5</p>

                </div>
                <div class="p1">
                    <p id="439">1) 局部感知, 指通过对局部信息进行整合到达识别图像的目的.在传统的深度神经网络中, 第<i>i</i>层每个神经元都会与第<i>i</i>-1层所有神经元连接, 这样做不仅导致了计算量非常大, 而且网络缺少泛化能力, 容易造成过拟合的情况.采用局部感知的方法, 在机器进行图像识别时每个神经元不必对全局图像进行感知, 只需要对局部进行感知, 而后在更高层将局部的信息综合起来就得到了全局的信息.这样做不仅增强了网络的泛化能力, 还有效减少了计算量.对应到具体操作是指上一层的数据区有一个滑动的窗口, 只有窗口内的数据会与下一层神经元有关联, 如图2所示.</p>
                </div>
                <div class="area_img" id="440">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_440.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 卷积的过程" src="Detail/GetImg?filename=images/JFYZ201902002_440.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 卷积的过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_440.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 The process of convolution</p>

                </div>
                <div class="p1">
                    <p id="441">2) 参数共享, 即权值共享.对一个特征图来说, 卷积操作就是提取特征的过程, 这里简单介绍卷积核的概念, 卷积核就是一个有相关权重的方阵, 它的作用是用来提取图像的特定特征.提取特征和目标在图像中的位置无关, 这也意味着某一部分学习的特征也能用在另一部分上, 所以对于这个图像上的所有位置, 都能使用同样的学习特征.具体来说就是将卷积核的每一个元素作用到输入特征图的每一个位置 (边界因素先不考虑) .参数共享保证了训练时对输入特征图只需要学习一个参数集合, 而不是对每一个位置都需要学习一个单独的参数集合.</p>
                </div>
                <div class="p1">
                    <p id="442">因为局部感知和参数共享这2个特点, 卷积神经网络被广泛应用在图像识别领域.图3展示了一个经典卷积神经网络框架LeNet5<sup><a class="sup">[22]</a></sup>完成数字识别的过程.从图3中可以看出, 卷积神经网络主要由卷积层、池化层和全连接层组成.</p>
                </div>
                <h4 class="anchor-tag" id="443" name="443"><b>1.1</b><b>卷积层</b></h4>
                <div class="p1">
                    <p id="444">介绍卷积层前, 我们先介绍一下输入图像、输入特征图、输出特征图的概念.输入图像是指要进行识别的原始图像, 并且它也是第1层网络的输入特征图, 输出特征图是指神经网络某1层的输出, 同时它也是下一层网络的输入特征图.卷积层是为了提取输入特征图中的某些特征, 它的输出是由神经元组成的一幅新的2D特征图.计算过程是先卷积再通过一个激活函数得到结果, 卷积过程如图2所示.计算公式为</p>
                </div>
                <div class="p1">
                    <p id="445" class="code-formula">
                        <mathml id="445"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mn>0</mn></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><msub><mrow></mrow><mi>y</mi></msub><mo>=</mo><mn>0</mn></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></munderover><mi>w</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>k</mi><msub><mrow></mrow><mi>x</mi></msub><mo>, </mo><mi>k</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo><mo>×</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="446"><i>In</i> (<i>x</i>+<i>k</i><sub><i>x</i></sub>, <i>y</i>+<i>k</i><sub><i>y</i></sub>) <sup><i>f</i><sub><i>i</i></sub></sup>+<i>β</i><sup><i>f</i><sub><i>i</i></sub></sup>) ) , </p>
                </div>
                <div class="p1">
                    <p id="447">其中, <i>f</i> (·) 是非线性激活函数, <i>β</i><sup><i>f</i><sub><i>i</i></sub></sup>表示偏移值, <i>out</i> (<i>x</i>, <i>y</i>) 表示在输出特征图坐标 (<i>x</i>, <i>y</i>) 处的值, <i>w</i> (<i>k</i><sub><i>x</i></sub>, <i>k</i><sub><i>y</i></sub>) 表示卷积核坐标 (<i>k</i><sub><i>x</i></sub>, <i>k</i><sub><i>y</i></sub>) 上的权重值, <i>In</i> (<i>x</i>+<i>k</i><sub><i>x</i></sub>, <i>y</i>+<i>k</i><sub><i>y</i></sub>) 表示输入特征图坐标 (<i>x</i>+<i>k</i><sub><i>x</i></sub>, <i>y</i>+<i>k</i><sub><i>y</i></sub>) 上的输入值;<i>K</i><sub><i>x</i></sub>, <i>K</i><sub><i>y</i></sub>表示卷积核的大小, <i>f</i><sub><i>i</i></sub>表示第<i>i</i>幅输入特征图, <i>N</i><sub><i>f</i><sub><i>i</i></sub></sub>表示输入特征图的数目.</p>
                </div>
                <div class="p1">
                    <p id="448">卷积层计算中卷积的实现方法有4种, 直接卷积、展开式卷积 (Toeplitz矩阵方法<sup><a class="sup">[23]</a></sup>) 、Winograd卷积<sup><a class="sup">[24]</a></sup>和快速傅里叶变换 (fast Fourier transform, FFT) 卷积<sup><a class="sup">[25]</a></sup>.直接卷积方案在Alex编写的cuda-convnet框架<sup><a class="sup">[26]</a></sup>中有详细介绍.卷积过程如图2所示, 与数字信号中的卷积运算不同<sup><a class="sup">[27]</a></sup>, 直接卷积是指卷积核在输入特征图上滑动时将滑动窗口内的元素对应相乘与累加 (数字信号中的卷积会将卷积核翻转再与输入对应相乘累加) .展开式卷积就是将输入图像展开成列, 将卷积核展开成行, 将卷积操作转换成矩阵乘法操作, 最后通过高度优化数学库 (GPU的cuBLAS) 实现.Winograd卷积是通过计算步骤的转换将卷积操作中的乘法操作和加法操作的数目改变, 使乘法操作减少, 加法操作增加, 从而提升效率.FFT卷积就是将空间域中的离散卷积转化为傅里叶域的乘积.它的实现分3个步骤:1) 通过快速傅里叶变换将输入特征图和卷积核从空间域转换到频域;2) 在频域中这些变换后的矩阵相乘;3) 计算结果从频域反转到空间域.</p>
                </div>
                <h4 class="anchor-tag" id="449" name="449"><b>1.2</b><b>池化层</b></h4>
                <div class="p1">
                    <p id="450">池化层也叫下采样层, 通常是跟在卷积层的后面, 根据一定的采样规则 (通常是最大值或平均值采样) <sup><a class="sup">[28]</a></sup>做采样.池化层的作用是提取局部特征.这是由于图像具有一种“静态性”的属性, 在一个图像区域有用的特征可能在另一个区域同样适用.例如, 卷积层输出的特征图中2个相邻的点的特征通常会很相似, 假设<i>a</i>[0, 0], <i>a</i>[0, 1], <i>a</i>[1, 0], <i>a</i>[1, 1]都表示颜色特征是红色, 没有必要都保留作下一层的输入.池化层可以将这4个点做一个整合, 输出红色这个特征.这样可以降低输入特征图的维度, 减少过拟合现象, 同时缩短网络模型的执行时间.</p>
                </div>
                <h4 class="anchor-tag" id="451" name="451"><b>1.3</b><b>全连接层</b></h4>
                <div class="p1">
                    <p id="452">全连接层的作用是将有用的局部信息提取整合, 也就是将以前的局部特征通过权重矩阵重新组装成新的特征图.它的核心思想是非线性变换和拟合, 多个全连接层的非线性变换嵌套叠加会使网络有很强的拟合能力 (也可能造成过拟合) .具体来说, 在全连接层中, 输出神经元与上层的输入神经元全部以独立的权重值相连接.我们可以将全连接层的计算看作是一个矩阵乘向量, 再加上激活函数的非线性映射, 即:</p>
                </div>
                <div class="p1">
                    <p id="453"><mathml id="454"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>o</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>f</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>×</mo><mi>i</mi><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>, </p>
                </div>
                <div class="p1">
                    <p id="455">其中, <i>out</i>是输出值, <i>f</i> (·) 是激活函数, <i>n</i>表示输入特征图的输入数目, <i>β</i>代表偏移值.每个输入对应1个权重值.全连接层的运算实质也是一个乘累加运算.</p>
                </div>
                <div class="p1">
                    <p id="456">以上就是一个卷积神经网络的基本组成, 卷积神经网络的训练基本上和BP (back propagation) 神经网络的训练一样, 通过递归不断地更新权重减小误差.现在又新兴的一种权重训练方法即采用遗传算法训练权重<sup><a class="sup">[29]</a></sup>, 文中总结的加速器大多只完成推导阶段, 所以这里对训练不进行过多赘述.</p>
                </div>
                <h3 id="457" name="457" class="anchor-tag"><b>2</b><b>通用芯片对人工神经网络的支持</b></h3>
                <div class="p1">
                    <p id="458">常见的通用芯片包括CPU, GPU, DSP, FPGA, 随着深度学习的火热, 它们的最新设计都对神经网络进行了一些支持.下面分别来介绍它们为支持神经网络所作的改进.</p>
                </div>
                <h4 class="anchor-tag" id="459" name="459"><b>2.1</b><b>CPU</b></h4>
                <div class="p1">
                    <p id="460">CPU作为应用最广泛的通用处理器, 其对神经网络的运算优化主要集中在软件层面, 比如优化对神经网络框架 (如Caffe<sup><a class="sup">[30]</a></sup>和TensorFlow<sup><a class="sup">[31]</a></sup>) 的支持.硬件方面, 随着研究表明推导时操作数的精度对准确率的影响不大, 如表1所示:</p>
                </div>
                <div class="area_img" id="461">
                    <p class="img_tit"><b>表1</b><b>操作数精度对识别准确率的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1</b><b>Effect of Operand Accuracy on Recognition Accuracy</b></p>
                    <p class="img_note"></p>
                    <table id="461" border="1"><tr><td><br />Type</td><td>Error Rate</td></tr><tr><td><br />32 b Floating-Point</td><td>0.031 1</td></tr><tr><td><br />16 b Floating-Point</td><td>0.033 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="462">CPU也开始支持低精度运算.Intel就对最新的处理器Knights Mill<sup><a class="sup">[32]</a></sup>增加了他们称之为“可变精度”的支持.另外, 它对向量处理单元 (vector pro-cessing unit, VPU) 也做了改进.和上一代Knights Landing相比, Knights Mill在VPU上用1个较小的双精度端口和4个向量神经网络指令 (vector neural network instruction, VNNI) 端口, 取代了Knights Landing的VPU上2个大的双精度/单精度浮点 (64 b/32 b) 端口.VNNI端口支持单精度浮点和混合精度整数 (16 b输入/32 b输出) .由于操作数精度的降低, 以往指令取出的32 b操作数如今变成了2个16 b操作数, 这样运算相同数目的操作数将会采用更少的指令, 同时也降低了计算的复杂度, 进而提高了性能.</p>
                </div>
                <h4 class="anchor-tag" id="463" name="463"><b>2.2</b><b>GPU</b></h4>
                <div class="p1">
                    <p id="464">图形处理器GPU强大的并行计算能力适用于CNN计算过程中的大量并行浮点计算以及矩阵和向量运算, 这促使了GPU成为最常见的神经网络硬件加速平台.许多机器学习的框架都利用了含有CUDA编程接口的GPU, 如cuda-convnet<sup><a class="sup">[33]</a></sup>, Torch<sup><a class="sup">[34]</a></sup>, Theano<sup><a class="sup">[35]</a></sup>, Caffe<sup><a class="sup">[30]</a></sup>.与此同时, 许多GPU的深度学习库也在被探索用来加速神经网络, 如英伟达的cuDNN<sup><a class="sup">[36]</a></sup>和Facebook的fbfft<sup><a class="sup">[37]</a></sup>等.需要注意的是并没有哪一种框架或库对所有类型的神经网络都能有最好的加速效果.针对不同类型、不同结构神经网络上述框架和库都有不同的表现.</p>
                </div>
                <div class="p1">
                    <p id="465">除了支持深度学习库之外, GPU也在体系结构上支持半精度的运算, 如tesla P100采用的GP100核心、tesla V100采用的GV100核心以及AMD的Vega架构.英伟达最新的Volta架构更是设计了一个专门用来处理深度学习任务的Tensor Core.每个Tensor Core包含1个4×4×4的矩阵处理阵列来完成<b><i>D</i></b>=<b><i>A</i></b>×<b><i>B</i></b>+<b><i>C</i></b>的运算, 其中<b><i>A</i></b>, <b><i>B</i></b>, <b><i>C</i></b>, <b><i>D</i></b>是4×4的矩阵, 如图4所示.</p>
                </div>
                <div class="p1">
                    <p id="466">矩阵相乘的输入<b><i>A</i></b>和<b><i>B</i></b>是16 b半精度浮点数 (FP16) 矩阵, 矩阵<b><i>C</i></b>和<b><i>D</i></b>可能是FP16矩阵或FP32矩阵.该Tensor Core支持16 b和32 b的混合精度运算.FP16的乘法得到了一个全精度结果, 该结果和上一周期得到部分和进行累加, 如图5所示.</p>
                </div>
                <div class="area_img" id="467">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_467.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Tensor Core的4&#215;4&#215;4矩阵乘法与累加" src="Detail/GetImg?filename=images/JFYZ201902002_467.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Tensor Core的4×4×4矩阵乘法与累加  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_467.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 4×4×4 matrix multiplication and accumulation of the Tensor Core</p>

                </div>
                <div class="area_img" id="468">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_468.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Tensor Core流程图" src="Detail/GetImg?filename=images/JFYZ201902002_468.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Tensor Core流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_468.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 Flow chart of the Tensor Core</p>

                </div>
                <h4 class="anchor-tag" id="469" name="469"><b>2.3</b><b>DSP</b></h4>
                <div class="p1">
                    <p id="470">数字信号处理 (DSP) 芯片能够提供强大的数字信号处理能力, 它也是一种实现神经网络加速的有效平台.四大DSP IP产商也都发布了支持神经网络的DSP IP, 包括Synopsys的EV6x (embedded vision) 处理器<sup><a class="sup">[38]</a></sup>、CEVA的CEVA-XM6<sup><a class="sup">[39]</a></sup>、VeriSilicon的VIP8000<sup><a class="sup">[40]</a></sup>以及Cadence的Vision C5 DSP<sup><a class="sup">[41]</a></sup>.</p>
                </div>
                <div class="area_img" id="471">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_471.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 EV6x结构图" src="Detail/GetImg?filename=images/JFYZ201902002_471.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 EV6x结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_471.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The structure of EV6x</p>

                </div>
                <div class="p1">
                    <p id="472">EV6x在硬件结构上实现了对CNN的加速.为了增加对神经网络卷积运算的支持, 从图6可以看出在EV6x的设计中引入了1个CNN Engine.CNN Engine是一个可编程嵌入式深层神经网络引擎, 它支持当前所有主流神经网络模型 (包括AlexNet, GoogleNet, ResNet, VGG, Yolo, Faster R-CNN, SqueezeNet) .CNN引擎的可配置性使不同的网络模型可以灵活地映射到硬件资源.它能提供每周期800MAC (multiply-and-accumulate) (一般将1次MAC看作2个操作:1个乘法和1个加法) 性能, 与同类解决方案相比, 性能提高了6倍.此外, 神经网络还经常涉及到稀疏矩阵的处理, 有效数据并没有连续存储, 因此在EV6x的4个视觉CPU中提供了“分散-收集” (scatter-gather) 功能, scatter-gather是将不规则存放的数据一次取出, 再组合成一个向量以便它们能够被VPU并行处理.</p>
                </div>
                <div class="p1">
                    <p id="473">CEVE-XM6针对神经网络处理做了2点优化:1) 为了优化对神经网络中稀疏矩阵的处理, 针对不规则访存增加了并行Scatter-Gather内存装载机制;2) 采用了类似EV6x设计中DSP加硬件加速器的结构.为了利用图像卷积处理时卷积核滑过输入特征图时出现的数据重叠, 加速器中采用了滑动窗口2.0 (sliding windows 2.0) 机制, 这有助于在更广泛的神经网络中实现更高的利用率.</p>
                </div>
                <div class="p1">
                    <p id="474">VIP8000由高度多线程的并行处理单元、神经网络单元和通用存储缓存单元组成.它具有可灵活配置的特点, 可以导入由Caffe和TensorFlow等主流深度学习框架生成的神经网络.它的并行处理单元、神经网络单元和通用存储单元都具有可扩展性.为了实现最佳计算效率和准确率, 神经网络单元支持定点8 b精度和16 b浮点精度, 并支持混合模式应用.VIP8000最终能取得每秒3 TMAC的性能.</p>
                </div>
                <div class="p1">
                    <p id="475">最后, Cadence的Vision C5 DSP为了实现神经网络所有层的计算 (卷积层、池化层、全连接层) , 而不仅仅是卷积层的加速.它将图像处理运算单元和神经网络加速单元合二为一.通过移除神经网络加速器和主视觉/图像DSP之间的冗余数据传输, Vision C5 DSP的功耗远低于现有的神经网络加速器.为了支持低精度运算提高计算效率, 它大量增加了低精度乘累加 (MAC) 单元的数量, 有1 024个8 b MAC单元 (512个16 b MAC单元) .另外为了适应快速变换的神经网络领域, 可编程的设计也是必不可少的.</p>
                </div>
                <div class="p1">
                    <p id="476">通过比较以上4个DSP IP的设计, 不难发现设计DSP IP实现神经网络硬件加速的一些共同点:1) 都支持低精度的向量运算;2) 具有可编程可扩展的特点;3) 支持当前所有的主流神经网络框架.</p>
                </div>
                <h4 class="anchor-tag" id="477" name="477"><b>2.4</b><b>FPGA</b></h4>
                <div class="p1">
                    <p id="478">现场可编程门阵列 (field programmable gate array, FPGA) 也常被用作神经网络的加速平台, 这类加速器利用可编程门阵列的特性, 设计新的硬件结构, 更好地匹配了神经网络的计算特点.与CPU和GPU相比, 它节省了部分通用功能所占芯片面积, 更加高效地利用了硬件资源, 性能更高并且能效也更高;与ASIC (application specific integrated circuit) 相比, FPGA能够实现快速的硬件功能验证和评估, 从而加快设计的迭代速度.</p>
                </div>
                <div class="p1">
                    <p id="479">具体来说, FPGA针对神经网络的优化主要包括2个方面:1) 从计算结构的角度出发;2) 从算法的角度出发.计算结构上的优化尽量使神经网络计算并行化和流水化, 并且利用FPGA可编程的特性来设计满足加速器的灵活性和可扩展性;算法上的优化主要针对卷积层, 利用FPGA的可编程性硬件实现各类卷积方法.下面通过几个例子介绍这些优化方法是如何实现的.</p>
                </div>
                <div class="p1">
                    <p id="480">早在2002年Yun等人<sup><a class="sup">[42]</a></sup>就在FPGA上实现了多层感知机, 他们提出了一种硬件实现数字神经网络的新体系结构——ERNA.在传统SIMD (single instruction multiple data) 结构的基础上, 采用灵活的阶梯式总线和内部连接网络.所提出的架构实现了基于并行化和流水线化的快速处理, 同时不放弃传统方法的灵活性和可扩展性.此外, 用户还可以通过设置配置寄存器来改变网络拓扑.</p>
                </div>
                <div class="p1">
                    <p id="481">2011年Farabet等人<sup><a class="sup">[43]</a></sup>提出了基于FPGA的卷积神经网络处理器——NeuFlow.它是一种运行时可重构的数据流处理器, 包含多个计算瓦片.在计算瓦片中, 集成了多个1D卷积器 (乘累加单元MAC) , 形成1个2D卷积算子.输入输出块通过级联卷积器和其他具有可编程性的操作元件形成, 然后再经过路由多路复用器连接到全局总线.</p>
                </div>
                <div class="p1">
                    <p id="482">在FPGA2017会议上, 张弛等人<sup><a class="sup">[44]</a></sup>提出了一种在CPU-FPGA共享内存系统上对卷积神经网络进行频域加速的方法.首先, 利用快速傅里叶变换 (FFT) 和重叠加法 (overlap-and-add, OaA) 来减少卷积层的计算量.具体做法是将频域算法映射到FPGA上高度并行的基于OaA的2D卷积器上.然后在共享内存器中提出了一种新颖的数据布局, 用于CPU和FPGA之间高效的数据通信.为了减少内存访问延迟并保持FPGA的峰值性能, 该设计采用了双缓冲.为了减少层间数据重映射延迟, 该设计利用了CPU和FPGA进行并发处理.通过使用OaA, CNN浮点运算次数可以减少39.14%～54.10%.</p>
                </div>
                <div class="p1">
                    <p id="483">总的来说, 由于FPGA可编程性强, 作为加速器研发周期短, 在它上面实现神经网络加速的研究越来越多.但目前的深度神经网络 (DNN) 计算还是重度依赖密集的浮点矩阵乘法 (GEMM) , 抛开独特的数据类型 (利用稀疏压缩后的数据类型) 设计, 它更利于映射到GPU上 (常规并行性) .因此市场上依然是GPU被广泛地用于加速DNN.FPGA虽然提供了卓越的能耗效率, 但它并不能达到当今GPU的性能.但是考虑到FPGA的技术进展以及DNN的算法创新速度, 未来的高性能FPGA在处理DNN方面可能会优于GPU的性能.例如英特尔即将推出的14 nm的Stratix, 10个FPGA将会具有数千个DSP和片上RAM.还将具有高带宽存储器HBM (一种3D存储技术) .这种功能组合就提供了FPGA与GPU相差不多的浮点计算性能.再加上现在的DNN算法里开始利用稀疏 (剪枝等产生) 和紧凑的数据类型来提升算法的效率.这些自定义数据类型也引入了不规则的并行性, 这对于GPU来说很难处理, 但是利用FPGA的可定制性就能非常高效地解决.例如清华大学汪玉团队就在FPGA上优化了对神经网络稀疏性的处理.</p>
                </div>
                <div class="p1">
                    <p id="484">以上4种通用硬件加速平台由于在优化神经网络处理的同时还要考虑其对通用计算的支持, 因此并不能完全利用所有计算资源来完成神经网络的加速.此时, 体系结构研究者考虑设计一种专用芯片来完成这一加速工作.</p>
                </div>
                <h3 id="485" name="485" class="anchor-tag"><b>3</b><b>专用人工神经网络加速器</b></h3>
                <div class="p1">
                    <p id="486">当前存在许多关于神经网络加速器的研究, 本节从运算存储结构和数据流调度优化的角度对现有专用人工神经网络加速器设计进行分析梳理.</p>
                </div>
                <h4 class="anchor-tag" id="487" name="487"><b>3.1</b><b>体系结构设计</b></h4>
                <div class="p1">
                    <p id="488">现有加速器大都采用基于CMOS (complementary metal oxide semiconductor) 工艺的冯·诺依曼体系结构, 这类加速器设计注重2个的模块:运算单元和存储单元.运算单元的实现分为2种:树状结构和阵列结构.存储单元的设计用来存储神经网络每一层的输入值、权重和输出值等参数.如何平衡片上片外的数据分配, 最小化数据的搬移是它设计的重点.值得注意的是, 随着器件工艺的发展, 一些体系结构研究者开始采用忆阻器等新器件来设计处理存储一体化的加速器.这类加速器有效地解决了带宽的瓶颈, 具有功耗低速度快的特点.另外, 专用指令集也是体系结构设计的一大重点.下面分别介绍这几个方面.</p>
                </div>
                <h4 class="anchor-tag" id="489" name="489">3.1.1 CMOS工艺下的运算单元结构</h4>
                <div class="p1">
                    <p id="490">运算单元是加速器设计的重点, 根据神经网络的计算特点, 本文总结了2种运算单元的实现结构:1) 树状结构, 通过对神经元计算过程的抽象得到;2) PE阵列结构, 利用神经网络每一层有大量乘累加并行计算的特点, 将乘累加操作放入1个PE里, 这样可以通过PE阵列实现神经元的并行处理.具体设计方案如下.</p>
                </div>
                <div class="p1">
                    <p id="491">图7给出了树状结构图.方框内是一个简易的神经元计算单元NFU, 最右端的根节点是每个神经元的输出, 子节点包括乘法器、加法器和非线性激活函数, 叶子节点是神经元的输入.特别地, 图7中第2层的结果有的并没有经过NFU-3的函数激活, 而是直接输出 (图7中虚线部分) , 这是因为神经网络中有的层并没有激活操作, 典型的如池化层.采用这种树状结构的设计有DianNao<sup><a class="sup">[6]</a></sup>, DaDianNao<sup><a class="sup">[5]</a></sup>.两者都采用了NFU作为加速器的基本处理单元, 不同的是DaDianNao有更好的扩展性, 可以高性能地支持大尺寸网络模型.不过从NFU的结构可以看出DianNao和DaDianNao仅能很好地支持神经网络的计算 (NFU只有乘累加运算单元) .为了运行更多的机器学习算法, PuDianNao<sup><a class="sup">[7]</a></sup>设计了新的计算单元 (machine learning unit, MLU) , 它的子节点包括计数器、加法器、乘法器、加法树、累加器 (accumulator) 和杂项作业 (miscellaneous, Misc) .支持了更多的机器学习算法, 如<i>k</i>-均值、<i>k</i>-最近邻、朴素贝叶斯、支持向量机、线性回归、神经网络、决策树等.PuDianNao运行上述机器学习算法时的平均性能与主流通用图形处理器相当, 但面积和功耗仅为后者的百分之一量级.</p>
                </div>
                <div class="area_img" id="492">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_492.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 一个典型树状结构的NFU" src="Detail/GetImg?filename=images/JFYZ201902002_492.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 一个典型树状结构的NFU  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_492.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 A typical tree structure of NFU</p>

                </div>
                <div class="area_img" id="493">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_493.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 TPU矩阵运算单元的脉动数据流" src="Detail/GetImg?filename=images/JFYZ201902002_493.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 TPU矩阵运算单元的脉动数据流  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_493.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 8 Systolic data flow of the matrix multiply  unit in TPU</p>

                </div>
                <div class="p1">
                    <p id="494">阵列结构主要以Google TPU<sup><a class="sup">[9]</a></sup>的脉动阵列结构为代表.TETRIS<sup><a class="sup">[45]</a></sup>, Eyeriss<sup><a class="sup">[12]</a></sup>, ShiDianNao<sup><a class="sup">[8]</a></sup>, Scaledeep<sup><a class="sup">[11]</a></sup>所采用的处理结构都在脉动阵列上进行或多或少的改动.脉动阵列的设计核心是让数据在运算单元的阵列中进行流动, 增加数据的复用, 减少访存的次数.图8展示了TPU中矩阵乘法单元的脉动数据流.权重由上向下流动, 输入特征图的数据从左向右流动.在最下方有一些累加单元, 主要用于权重矩阵或者输入特征图超出矩阵运算单元范围时保存部分结果.控制单元负责数据的组织, 具体来说就是控制权重和输入特征图的数据如何传入脉动阵列以及如何在脉动阵列中进行处理和流动.</p>
                </div>
                <h4 class="anchor-tag" id="495" name="495">3.1.2 CMOS工艺下的存储结构</h4>
                <div class="p1">
                    <p id="496">因为数据存取的速度大大低于数据处理的速度, 因此存储单元的设计直接影响到加速器的性能.英伟达公司首席科学家Steve<sup><a class="sup">[46]</a></sup>曾指出, 在40 nm工艺下, 将64 b数据搬运20 mm的能耗是做64 b浮点乘法的数倍.因此, 要降低处理器功耗, 仅仅优化处理结构是不够的, 必须对片上存储单元的结构也进行优化.传统的存储单元设计是将不同数据放在同一个存储器里.在DianNao里提出了一种存储方式 (图7) , 对神经网络参数进行分块存储 (用于存放输入神经元的输入缓冲器NBin、存放输出神经元的输出缓冲器NBout、存放突触权重的缓冲器SB) , 将不同类型的数据块存放在片上不同的随机存储器中, 再优化神经网络运算过程中对不同类型数据的调度策略.与CPU/GPU上基于缓存层次的数据搬运方法相比, DianNao的设计方案可将数据搬运次数减少至前者的1/30～1/10.DaDianNao也延续了这种存储结构设计, 同时DaDianNao使用了eDRAM来存放所有的权重值并将其放在计算部件附近以减少访存延时.</p>
                </div>
                <div class="p1">
                    <p id="497">另外, 随着神经网络规模越来越大, 训练集越来越大, 处理能力也越来越强, 对带宽的要求就变得越来越高.传统2D存储结构的DDR技术不能提供如此高的带宽.人们开始将3D存储技术引入到神经网络加速器的设计中.现在3D存储方案有AMD和海力士研发的HBM以及Intel和美光研发的HMC.Google的Cloud TPU<sup><a class="sup">[9]</a></sup>采用了HBM作为存储结构.Neurocube<sup><a class="sup">[47]</a></sup>和TETRIS<sup><a class="sup">[45]</a></sup>采用了HMC作为加速器的存储结构.另外, 3D存储的出现使人们开始考虑将累加器设计到3D存储体里.这样做可以减少1次数据的读取, 从而降低延迟, 节省功耗, 提高性能.TETRIS给出了4种在HMC中设计累加器的方案, 分别是内存控制器级累加、DRAM芯片级累加、Bank级累加和子阵列级累加.第1种方案是将累加器做在HMC的逻辑层, 这样设计并不能减少数据的读写次数.后3种设计能带来性能提升, 但子阵列级累加实现困难, 所以主要还是采用DRAM级累加和Bank级累加.</p>
                </div>
                <h4 class="anchor-tag" id="498" name="498">3.1.3 新工艺下的一体化结构</h4>
                <div class="p1">
                    <p id="499">运算存储一体化结构的加速器打破了冯·诺依曼体系结构的束缚, 要想实现这类加速器依靠传统CMOS工艺较为困难, 新型器件的研究和非传统电路的实现是这类加速器的研究方向.最新研究表明忆阻器是实现一体化结构的一种较好选择, 它本身具有存储数据的功能, 另外利用基尔霍夫定律产生的位线电流就是卷积运算乘累加的结果, 节省了乘法和累加的计算时间.去年, 在忆阻器领域具有领先地位的HP公司与犹他大学合作研究发表了基于忆阻器的神经网络加速器ISAAC<sup><a class="sup">[13]</a></sup>.它的结构与DaDianNao<sup><a class="sup">[5]</a></sup>相似, 由多个Nodes或Tiles组成, 忆阻器交叉 (crossbar) 开关阵列组成了每个Tile的核心来完成存储权重和卷积计算的功能.与DaDianNao相比, 吞吐量是其14.8倍, 功效是其5.5倍, 计算密度是其7.5倍.</p>
                </div>
                <div class="p1">
                    <p id="500">目前做神经网络加速的忆阻器器件主要以ReRAM为主.加州大学圣塔芭芭拉分校谢源教授课题组也发布了一种基于ReRAM的神经网络加速器PRIME<sup><a class="sup">[48]</a></sup>.基于新型材料的ReRAM被认为是今后可以替代当前DRAM的下一代存储技术之一.作为忆阻器的一种, ReRAM除了是存储单元之外, 其独特的交叉网络结构 (crossbar) 和多比特存储 (multi-level cell) 性质, 能以很高的能量效率加速神经网络计算中的重要计算模块——乘累加.实现方法是通过在ReRAM存储体内修改一部分外围电路, 使其可以在“存储”状态和“神经网络加速器”状态之间灵活切换.</p>
                </div>
                <div class="p1">
                    <p id="501">现阶段使用ReRAM之类的忆阻器器件来实现CNN加速器还有许多挑战, 如精确度、内部数据调度以及模数转换等.清华大学的张悠慧教授等人<sup><a class="sup">[49]</a></sup>提出了神经网络模型转换方法, 将原神经网络稀疏化后划分成规模适应于ReRAM阵列的子网络, 并对数据进行量化来解决硬件精度受限问题, 这一定程度上从软件的角度解决了计算精度的问题.而李楚曦等人<sup><a class="sup">[50]</a></sup>提出的基于忆阻器的PIM结构实现深度卷积神经网络近似计算, 通过利用模拟忆阻器大大增加数据密度, 并将卷积过程分解到不同形式的忆阻器阵列中分别计算, 增加了数据并行性, 减少了数据转换次数并消除了中间存储, 从而实现了加速和节能.但是目前实现人工神经网络大多还是采用存储分离的结构, 不过随着忆阻器技术的不断成熟, 由于其低功耗和快速处理数据的特点, 新工艺下的一体化结构终将会是加速器发展的一大趋势.</p>
                </div>
                <h4 class="anchor-tag" id="502" name="502">3.1.4 指令集设计</h4>
                <div class="p1">
                    <p id="503">与基于RISC指令集的通用处理器不同, 专用芯片多采用基于CISC来设计自己指令集.这类指令集侧重于完成更复杂的任务, 适用于指令数量少和复杂程度高的神经网络.传统指令集实现一个神经元的操作需要上百条指令, 如果使用专有指令集一个神经元的操作可以只用一条指令执行, 减少功耗的同时提升了速度.TPU的指令集就是CISC类型, 平均每个指令的时钟周期数是10～20.指令总共只有十几条.其中比较重要的包括读权重值和读输入特征图数据、矩阵运算/卷积、激活和写回计算结果等一系列神经网络运算相关的指令.</p>
                </div>
                <div class="p1">
                    <p id="504">不过另一方面.越来越多研究者指出指令集的设计除了专用性外, 还需要足够的灵活来满足加速器处理不同的神经网络的效率.为了解决这个问题, 中科院陈天石团队<sup><a class="sup">[51]</a></sup>基于RISC设计了一种新的用于神经网络加速器领域的专用指令集架构, 称为Cambricon, 它是一种集标量、向量、逻辑、数据传输和控制指令于一体的存取架构.Cambricon在广泛的神经网络技术上表现出强大的描述能力, 并且比通用指令集架构 (X86, MIPS, GPGPU) 提供更高的代码密度.</p>
                </div>
                <h4 class="anchor-tag" id="505" name="505"><b>3.2</b><b>数据流调度和优化</b></h4>
                <div class="p1">
                    <p id="506">3.1节介绍了专用芯片在体系结构方面对神经网络所作的支持, 但是在神经网络处理过程中数据流的组织和调度的方式以及数据流的优化对硬件实现神经网络的性能影响同样很大.在神经网络中常用的数据流有权重固定流、输出固定流、无局部复用和行固定流, 常用的数据流优化处理手段有0值跳过、稀疏矩阵、参数压缩等.</p>
                </div>
                <h4 class="anchor-tag" id="507" name="507">3.2.1 数据流的调度</h4>
                <h4 class="anchor-tag" id="508" name="508">1) 权重固定流</h4>
                <div class="p1">
                    <p id="509">在卷积神经网络的卷积操作过程中, 同一个卷积核会卷积多个输入特征图.此时对这个卷积核里的权重是有复用的.如果把多次使用的权重存在每个PE的寄存器里, 那么卷积层的计算过程将会减少从全局缓冲取数据和向全局缓冲存数据的开销.由于部分和 (partial sum, Psum) 还会用到, 因此将它暂时存放在全局缓冲里, 如果缓冲区不大就必须限制同时产生Psum的数量, 这样也限制了一次性可以在片上加载的权重.此时若采用脉冲的方式将会有效减少Psum对全局缓冲大小的依赖, 图9 (a) 给出了这种数据流的处理过程.权重值存放在PE里, 输入激活值被广播到所有PE单元.部分和在每个PE累加过后传到下一个PE.</p>
                </div>
                <div class="area_img" id="510">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_510.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 神经网络的数据流" src="Detail/GetImg?filename=images/JFYZ201902002_510.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 神经网络的数据流  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_510.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 9 Dataflow of the neural network</p>

                </div>
                <div class="p1">
                    <p id="511">采用这种数据流的芯片设计有NneuFlow<sup><a class="sup">[43]</a></sup>, 它用了2D卷积引擎来处理卷积核.每个引擎处理过程中权重值保持不变.其他采用这种数据流的例子还有<sup>[<a class="sup">52</a>,<a class="sup">53</a>,<a class="sup">54</a>]</sup>.</p>
                </div>
                <h4 class="anchor-tag" id="512" name="512">2) 输出固定流</h4>
                <div class="p1">
                    <p id="513">指在寄存器中存放每个周期计算的部分和.卷积操作是一个乘累加操作, 每个周期计算的部分和需要与下一个周期的部分和相加得到新的部分和, 直到这个卷积核的计算完全结束.把部分和存在寄存器里同样也是减少了从全局缓冲取数据和向全局缓冲存数据的开销.同样这种数据流设计采用脉冲结构实现是最佳的.图9 (b) 给出了这种数据流的处理过程.部分和在PE内部累积最后流出到全局缓冲区, 权重值被广播到所有PE.输入激活值在PE进行乘法运算后传给下一个PE.</p>
                </div>
                <div class="p1">
                    <p id="514">采用这种数据流的芯片设计有ShiDianNao<sup><a class="sup">[8]</a></sup>.它的每个PE通过从相邻PE获取相应的输入激活来处理每个输出激活值.PE阵列通过实现专用网络来水平和垂直地传递数据.每个PE还具有数据延迟寄存器, 以便在所需的循环周期内保持数据周期.全局缓冲区流入输入激活值, 并将权重广播到PE阵列中.部分和积累在每个PE内部, 然后被流出回到全局缓冲区.其他采用这种数据流的例子还有<sup>[<a class="sup">55</a>,<a class="sup">56</a>]</sup>.</p>
                </div>
                <h4 class="anchor-tag" id="515" name="515">3) 无局部复用</h4>
                <div class="p1">
                    <p id="516">指不在PE的寄存器文件里存放权重和部分和, 每个周期需要计算的数据都从全局缓冲获取, 计算完后将结果传回全局缓冲, 最后通过PE阵列累积部分和.这样的好处是减少了PE所占的面积, 但是由于PE没有数据保持固定, 所以这样的设计也增加了全局缓冲区和空间PE阵列的流量.图9 (c) 给出了这类数据流的处理过程.</p>
                </div>
                <div class="p1">
                    <p id="517">采用这种数据流的芯片设计有UCLA<sup><a class="sup">[57]</a></sup>, Dian-Nao<sup><a class="sup">[6]</a></sup>, DaDianNao<sup><a class="sup">[5]</a></sup>, PuDianNao<sup><a class="sup">[7]</a></sup>.其中DianNao系列由于采用了专门的寄存器来保存部分和, 并不用从内存里取, 因此可以进一步降低访问部分和的功耗.</p>
                </div>
                <h4 class="anchor-tag" id="518" name="518">4) 行固定流</h4>
                <div class="p1">
                    <p id="519">指在高维卷积过程中通过将高维卷积分解成可以并行运行的1D卷积原语;每个基元 (原语操作) 表示1行卷积核权重和1行输入特征图像素的点积, 并生成1行部分和Psum.不同原语的Psum被进一步累积在一起以生成输出特征图.具体做法是将每个原语映射到1个PE进行处理, 每个行对的计算在PE中保持独立, 这就实现了卷积核和寄存器级别的输入特征图数据重用.图10给出了这类数据流的处理过程.</p>
                </div>
                <div class="area_img" id="520">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902002_520.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 处理2维卷积时的PE阵列中的行固定流示意图" src="Detail/GetImg?filename=images/JFYZ201902002_520.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 处理2维卷积时的PE阵列中的行固定流示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902002_520.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 10 The row stationary dataflow in PEs set to process a 2D convolution</p>

                </div>
                <div class="p1">
                    <p id="521">采用这种数据流的芯片设计有Eyeriss<sup><a class="sup">[12]</a></sup>和TETRIS<sup><a class="sup">[45]</a></sup>.</p>
                </div>
                <h4 class="anchor-tag" id="522" name="522">5) 其他数据流</h4>
                <div class="p1">
                    <p id="523">上述数据流都没有将权重值和输入激活值 (上一层输出经过激活函数得到的值) 完全复用.在SCNN<sup><a class="sup">[14]</a></sup>中提出了一种全新的数据流, 笛卡儿数据流.实现方法是将需要计算的权重和输入激活值做成2个向量, 然后2个向量做笛卡儿积, 这样向量中每个数能达到最大复用.</p>
                </div>
                <h4 class="anchor-tag" id="524" name="524">3.2.2 数据流的优化</h4>
                <div class="p1">
                    <p id="525">神经网络在数据的处理过程中还存在很多的优化技巧.首先, 神经网络在推导过程中的对精度要求不高, 这使几乎全部的神经网络专用加速器都支持低精度运算, 其中TPU<sup><a class="sup">[9]</a></sup>更是支持了8 b低精度运算.其次, 稀疏性.人们在神经网络训练和推导过程中发现会产生许多的0值, 一部分来源于网络训练阶段的剪枝, 另外一部分来源于推导阶段采用ReLU激活函数参数的0激活值.而0值在卷积操作中是可以跳过的, 因为卷积操作由多个乘累加操作组成, 乘数有一个为0则乘法和后面的加法就没必要计算.在Cnvlutin<sup><a class="sup">[58]</a></sup>就采用了跳过权重中0值的数据流, 在Cambricon-X<sup><a class="sup">[59]</a></sup>里则采用了跳过输入的0激活值, 而在SCNN里通过压缩稀疏性和将非0参数编码的方式同时做到了跳过0值权重和0激活值.这也引出了神经网络的另一种优化方法——压缩.在SCNN<sup><a class="sup">[14]</a></sup>和EIE<sup><a class="sup">[60]</a></sup>中都采用了压缩参数的方法.</p>
                </div>
                <div class="p1">
                    <p id="526">虽说压缩稀疏权重从计算量的角度来看能够直接提高性能, 但最近的研究表明在不损失精度的情况下直接通过权重剪枝却会造成性能的下降<sup><a class="sup">[56]</a></sup>, 这是由于解码压缩的权重需要额外的时间, 并且压缩的非0权重值需要额外的索引值来记录它们原来矩阵的位置, 这也增加了存储开销.最终的结果导致只有在大量剪枝的情况下, 剪枝后的新网络的性能才会优于剪枝前的网络, 而且会有精度的损失.在文献[61]中提出了2种解决办法分别针对并行性不同的2种硬件:1) 采用基于SIMD的权重剪枝用于低并行度的硬件, 实现方法是采用权重分组索引, 这样一条SIMD指令可以读取多个权重值;2) 使用节点剪枝用于高并行的硬件, 具体实现思想是利用正则化既不完全去掉参数又可以将它的影响降到最小.</p>
                </div>
                <h3 id="527" name="527" class="anchor-tag"><b>4</b><b>加速器未来发展的挑战和机遇</b></h3>
                <div class="p1">
                    <p id="528">第2节和3节论述了许多硬件加速神经网络的方案, 其中在通用芯片平台实现的包括支持低精度计算、支持更多的神经网络框架以及设计一个加速卷积运算的模块;在专用芯片平台实现的包括改进运算存储结构、优化数据流和设计专用指令集.虽然这些设计已经在神经网络加速方面取得了重大进展.但还是有许多问题和挑战要解决.下面列举的4个方面或许会是加速器未来研究的可行方向.</p>
                </div>
                <div class="p1">
                    <p id="529">1) 设计功耗更低的加速器.嵌入式应用是加速器应用的一种趋势, 加速器未来可能会应用到像可穿戴这样的领域, 这就需要把功耗进一步降低, 可突破的方向包括采用新器件, 进一步探索数据的组织形式以减少数据的移动等.</p>
                </div>
                <div class="p1">
                    <p id="530">2) 设计通用性更强的加速器.随着神经网络应用的增多, 神经网络的结构和框架也会越来越多.未来加速器设计需要考虑到支持各个框架的核心算法.</p>
                </div>
                <div class="p1">
                    <p id="531">3) 解决访存瓶颈.存取速度跟不上运算速度依旧是加速器设计的难题.目前3D存储是解决该问题的一个方向.不过随着新工艺的发展, 采用新器件的非冯·诺依曼体系结构或许能进一步改善这个问题.</p>
                </div>
                <div class="p1">
                    <p id="532">4) 应用其他领域的突破成果.技术革命通常伴随着不同领域的飞跃, 采用基于生物启发的脉冲神经网络、量子计算机以及使用忆阻器等新器件都可能是加速器未来设计的可行方案.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="615">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale deep learning for building intelligent computer systems">

                                <b>[1]</b>Dean J.Large-scale deep learning for building intelligent computer systems[C]Proc of the 9th ACM Int Conf on Web Search and Data Mining.New York:ACM, 2016:1-1
                            </a>
                        </p>
                        <p id="617">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the 26th Annual Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105
                            </a>
                        </p>
                        <p id="619">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">

                                <b>[3]</b>Szegedy C, Liu Wei, Jia Yangqing, et al.Going deeper with convolutions[C]Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition.Los Alamitos:IEEEComputer Society, 2015:1-9
                            </a>
                        </p>
                        <p id="621">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[4]</b>He Kaiming, Zhang Xianyu, Ren Shaoqing, et al.Identity mappings in deep residual networks[C]Proc of the 2016European Conf on Computer Vision.Berlin:Springer, 2016:630-645
                            </a>
                        </p>
                        <p id="623">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dadiannao:Amachine-learning supercomputer">

                                <b>[5]</b>Chen Yunji, Luo Tao, Liu Shaoli, et al.DaDianNao:Amachine-learning supercomputer[C]Proc of the 47th Annual IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2014:609-622
                            </a>
                        </p>
                        <p id="625">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning">

                                <b>[6]</b>Chen Yunji, Chen Tianshi, Du Zidong, et al.DianNao:Asmall-footprint high-throughput accelerator for ubiquitous machine-learning[C]Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2014:269-284
                            </a>
                        </p>
                        <p id="627">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pu Dian Nao:A polyvalent machine learning accelerator">

                                <b>[7]</b>Liu Daofu, Chen Tianshi, Liu Shaoli, et al.PuDianNao:Apolyvalent machine learning accelerator[C]Proc of the20th Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2015:369-381
                            </a>
                        </p>
                        <p id="629">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ShiDianNao:Shifting vision processing closer to the sensor">

                                <b>[8]</b>Du Zidong, Fasthuber R, Chen Tianshi, et al.ShiDianNao:Shifting vision processing closer to the sensor[C]Proc of the 42nd Annual Int Symp on Computer Architecture.New York:ACM, 2015:92-104
                            </a>
                        </p>
                        <p id="631">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indatacenter performance analysis of a tensor processing unit">

                                <b>[9]</b>Jouppi N P, Young C, Patil N, et al.In-Datacenter performance analysis of a tensor processing unit[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:1-12
                            </a>
                        </p>
                        <p id="633">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cloud TPUs:Google&amp;#39;&amp;#39;s second-generation tensor processing unit is coming to cloud">

                                <b>[10]</b>Google.Cloud TPUs:Google’s second-generation tensor processing unit is coming to cloud[EB/OL].[2017-10-30].https:ai.google/tools/cloud-tpus/
                            </a>
                        </p>
                        <p id="635">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ScaleDeep:A scalable compute architecture for learning and evaluating deep networks">

                                <b>[11]</b>Venkataramani S, Dubey P, Raghunathan A, et al.ScaleDeep:A scalable compute architecture for learning and evaluating deep networks[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:13-26
                            </a>
                        </p>
                        <p id="637">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks">

                                <b>[12]</b>Chen Yu-Hsin, Emer J, Sze V.Eyeriss:A spatial architecture for energy-efficient dataflow for convolutional neural networks[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:367-379
                            </a>
                        </p>
                        <p id="639">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ISAAC:a convolutional neural network accelerator with in-situ analog arithmetic in crossbars">

                                <b>[13]</b>Shafiee A, Nag A, Muralimanohar N, et al.ISAAC:Aconvolutional neural network accelerator with in-situ analog arithmetic in crossbars[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:14-26
                            </a>
                        </p>
                        <p id="641">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SCNN:An accelerator for compressed-sparse convolutional neural networks">

                                <b>[14]</b>Parashar A, Rhu M, Mukkara A, et al.SCNN:An accelerator for compressed-sparse convolutional neural networks[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:27-40
                            </a>
                        </p>
                        <p id="643">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TrueNorth:Design and Tool Flow of a 65 mW 1Million Neuron Programmable Neurosynaptic Chip">

                                <b>[15]</b>Akopyan F, Sawada J, Cassidy A, et al.TrueNorth:Design and tool flow of a 65mW 1 million neuron programmable neurosynaptic chip[J].IEEE Transactions on ComputerAided Design of Integrated Circuits and Systems, 2015, 34 (10) :1537-1557
                            </a>
                        </p>
                        <p id="645">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overview of the Spi NNaker system architecture">

                                <b>[16]</b>Furber S B, Lester D R, Plana L A, et al.Overview of the spiNNaker system architecture[J].IEEE Transactions on Computers, 2013, 62 (12) :2454-2467
                            </a>
                        </p>
                        <p id="647">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The SpiNNaker project">

                                <b>[17]</b>Furber S B, Galluppi F, Temple S, et al.The SpiNNaker project[J].Proceedings of the IEEE, 2014, 102 (5) :652-665
                            </a>
                        </p>
                        <p id="649">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MODELING SPIKING NEURAL NETWORKS ON SPINNAKER">

                                <b>[18]</b>Jin Xin, Lujan M, Plana L A, et al.Modeling spiking neural networks on SpiNNaker[J].Computing in Science&amp;Engineering, 2010, 12 (5) :91-97
                            </a>
                        </p>
                        <p id="651">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001082082&amp;v=MjUwOTU0SHRITnI0ZEhaT01OWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU25sVmIzQkpGND1OajdCYXJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b>McCulloch W S, Pitts W.A logical calculus of the ideas immanent in nervous activity[J].Bulletin of Mathematical Biophysics, 1943, 5 (4) :115-133
                            </a>
                        </p>
                        <p id="653">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain">

                                <b>[20]</b>Rosenblatt F.The perceptron:A probabilistic model for information storage and organization in the brain[J].Psychological Review, 1958, 65 (6) :386-408
                            </a>
                        </p>
                        <p id="655">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Receptive fields, binocular interaction and functional architecture in the cat&amp;#39;s visual cortex">

                                <b>[21]</b>Hubel D H, Wiesel T N.Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex[J].Journal of Physiology, 1962, 160 (1) :106-154
                            </a>
                        </p>
                        <p id="657">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                Lécun Y, Bottou L, Bengio Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324
                            </a>
                        </p>
                        <p id="659">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction to Large Truncated Toeplitz Matrices">

                                <b>[23]</b>B9ttcher A, Silbermann B.Introduction to Large Truncated Toeplitz Matrices[M].Berlin:Springer, 1999
                            </a>
                        </p>
                        <p id="661">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Arithmetic Complexity of Computations">

                                <b>[24]</b>Winograd S.Arithmetic Complexity of Computations[M].Philiadelphia PA:Society for Industrial&amp;Applied Mathematics, 1980
                            </a>
                        </p>
                        <p id="663">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast convolutional nets with fbfft:A GPU performance evaluation[OL]">

                                <b>[25]</b>Vasilache N, Johnson J, Mathieu M, et al.Fast convolutional nets with fbfft:A GPU performance evaluation[OL].[2018-04-13].https:arxiv.org/abs/1412.7580
                            </a>
                        </p>
                        <p id="665">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolution neural networks">

                                <b>[26]</b>Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]Proc of the 26th Annual Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press 2012:1097-1105
                            </a>
                        </p>
                        <p id="667">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Scientist and Engineer&amp;#39;&amp;#39;s Guide to Digital Signal Processing">

                                <b>[27]</b>Smith S W.The Scientist and Engineer’s Guide to Digital Signal Processing[M].Poway, CA:California Technical Publishing, 1997
                            </a>
                        </p>
                        <p id="669">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition">

                                <b>[28]</b>He Kaiming, Zhang Xianyu, Ren Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis&amp;Machine Intelligence, 2014, 37 (9) :1904-1916
                            </a>
                        </p>
                        <p id="671">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Genetic Algorithm-Neural Network (GANN): A study of neural network activation functions and depth of genetic algorithm search applied to feature selection">

                                <b>[29]</b>Dong L T, Mintram R.Genetic algorithm-neural network (GANN) :A study of neural network activation functions and depth of genetic algorithm search applied to feature selection[J].International Journal of Machine Learning&amp;Cybernetics, 2010, 1 (1/2/3/4) :75-87
                            </a>
                        </p>
                        <p id="673">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:Convolutional architecture for fast feature embedding">

                                <b>[30]</b>Jia Y, Shelhamer E, Donahue J, et al.Caffe:Convolutional architecture for fast feature embedding[C]Proc of the 22nd ACM Int Conf on Multimedia.New York:ACM, 2014:675-678
                            </a>
                        </p>
                        <p id="675">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL]">

                                <b>[31]</b>Abadi M, Agarwal A, Barham P, et al.TensorFlow:Largescale machine learning on heterogeneous distributed Systems[OL].[2018-04-13].https:arxiv.org/abs/1603.04467
                            </a>
                        </p>
                        <p id="677">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intel Xeon Phi Knights Mill for machine learning">

                                <b>[32]</b>Patrick K.Intel Xeon Phi Knights Mill for machine learning[EB/OL]. (2017-08-21) [2017-10-18].https:www.servethehome.com/intel-knights-mill-for-machine-learning/
                            </a>
                        </p>
                        <p id="679">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cuda-convnet2:A fast C++/Cuda implementation of convolutional (or more generally,feedforward)neural networks">

                                <b>[33]</b>Anker G.Cuda-convnet2:A fast C++/Cuda implementation of convolutional (or more generally, feedforward) neural networks[EB/OL].[2017-10-20].https:code.google.com/p/cuda-convnet2/
                            </a>
                        </p>
                        <p id="681">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Torch7:A matlablike environment for machine learning[C/OL]">

                                <b>[34]</b>Collobert R, Kavukcuoglu K, Farabet C.Torch7:A matlablike environment for machine learning[C/OL]Proc of NIPSWorkshop.2011[2018-04-13].http:citeseerx.ist.psu.edu/viewdoc/summary/doi=10.1.1.231.4195
                            </a>
                        </p>
                        <p id="683">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Theano:New features and speed improvements">

                                <b>[35]</b>Bastien F, Lamblin P, Pascanu R, et al.Theano:New features and speed improvements[J/OL].Computer Science, 2012[2018-04-13].https:arxiv.org/abs/1211.5590
                            </a>
                        </p>
                        <p id="685">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cudnn:GPU-accelerated library of primitives for deep neural networks">

                                <b>[36]</b>NVIDIA.Cudnn:GPU-accelerated library of primitives for deep neural networks[EB/OL].[2017-10-30].https:developer.nvidia.com/cuDNN
                            </a>
                        </p>
                        <p id="687">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast convolutional nets with fbfft:A GPU performance evaluation[OL]">

                                <b>[37]</b>Vasilache N, Johnson J, Mathieu M, et al.Fast convolutional nets with fbfft:A GPU performance evaluation[OL].[2018-04-13].https:arxiv.org/abs/1412.7580
                            </a>
                        </p>
                        <p id="689">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression analysis with deep learning&amp;amp;computer vision">

                                <b>[38]</b>Cooper G.Facial expression analysis with deep learning&amp;computer vision[EB/OL].[2018-04-13].https:www.synopsys.com/designware-ip/technical-bulletin/ev-facial-expressiondwtb-q117.html
                            </a>
                        </p>
                        <p id="691">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CEVA-XM6:Fifth-generation computer vision and deep learning embedded platform">

                                <b>[39]</b>CEVA.CEVA-XM6:Fifth-generation computer vision and deep learning embedded platform[EB/OL].[2017-10-30]].https:www.ceva-dsp.com/product/ceva-xm6/
                            </a>
                        </p>
                        <p id="693">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=VeriSilicon&amp;#39;&amp;#39;s Vivante VIP8000 neural network processor IP delivers over 3 Tera MACs Per second">

                                <b>[40]</b>Miya K.VeriSilicon’s Vivante VIP8000 neural network processor IP delivers over 3 Tera MACs Per second[EB/OL].[2017-10-30].http:www.verisilicon.com/newsdetail_499_VivanteVIP8000.html
                            </a>
                        </p>
                        <p id="695">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensilica Vision DSPs for imaging,computer vision,and neural networks">

                                <b>[41]</b>Cadence.Tensilica Vision DSPs for imaging, computer vision, and neural networks[EB/OL].[2017-10-18].https:ip.cadence.com/vision
                            </a>
                        </p>
                        <p id="697">
                            <a id="bibliography_42" >
                                    <b>[42]</b>
                                Yun S B, Kim Y J, Dong S S, et al.Hardware implementation of neural network with expansible and reconfigurable architecture[C]Proc of the 9th Int Conf on Neural Information Processing.Piscataway, NJ:IEEE, 2002:970-975
                            </a>
                        </p>
                        <p id="699">
                            <a id="bibliography_43" >
                                    <b>[43]</b>
                                Farabet C, Martini B, Corda B, et al.NeuFlow:A runtime reconfigurable dataflow processor for vision[C]Proc of the29th Computer Vision and Pattern Recognition Workshops.Piscataway, NJ:IEEE, 2011:109-116
                            </a>
                        </p>
                        <p id="701">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system">

                                <b>[44]</b>Zhang Chi, Prasanna V.Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C]Proc of the 25th ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2017:35-44
                            </a>
                        </p>
                        <p id="703">
                            <a id="bibliography_45" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TETRIS:Scalable and efficient neural network acceleration with 3D memory">

                                <b>[45]</b>Gao Mingyu, Pu Jing, Yang Xuan, et al.TETRIS:Scalable and efficient neural network acceleration with 3D memory[C]Proc of the 22nd Int Conf on Architectural Support for Programming Languages and Operating Systems.New York:ACM, 2017:751-764
                            </a>
                        </p>
                        <p id="705">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of neural network accelerators">

                                <b>[46]</b>Li Zhen, Wang Yuqing, Zhi Tian, et al.A survey of neural network accelerators[J].Frontiers of Computer Science, 2017, 11 (5) :746-761
                            </a>
                        </p>
                        <p id="707">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neurocube:A programmable digital neuromorphic architecture with high-density 3Dmemory">

                                <b>[47]</b>Kim D, Kung J, Chai S, et al.Neurocube:A programmable digital neuromorphic architecture with high-density 3Dmemory[C]Proc of the 43rd Annual Int Symp on Computer Architecture.New York:ACM, 2016:380-392
                            </a>
                        </p>
                        <p id="709">
                            <a id="bibliography_48" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Prime:a novel processing-in-memory architecture for neural net-work computation in reram-based main memory">

                                <b>[48]</b>Chi Ping, Li Shuangchen, Xu Cong, et al.PRIME:A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2016:27-39
                            </a>
                        </p>
                        <p id="711">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NEUTRAMS:Neural network transformation and co-design under neuromorphic hardware constraints">

                                <b>[49]</b>Ji Yu, Zhang Youhui, Li Shuangchen, et al.NEUTRAMS:Neural network transformation and co-design under neuromorphic hardware constraints[C]Proc of the 49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.21
                            </a>
                        </p>
                        <p id="713">
                            <a id="bibliography_50" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201706020&amp;v=MTE5NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZ5N25XcnZJTHl2U2RMRzRIOWJNcVk5SFpJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[50]</b>Li Chuxi, Fan Xiaoya, Zhao Changhe, et al.A memristorbased processing-in-memory architecture for deep convolutional neural networks approximate computation[J].Journal of Computer Research and Development, 2017, 54 (6) :1367-1380 (in Chinese) (李楚曦, 樊晓桠, 赵昌和, 等.基于忆阻器的PIM结构实现深度卷积神经网络近似计算[J].计算机研究与发展, 2017, 54 (6) :1367-1380) 
                            </a>
                        </p>
                        <p id="715">
                            <a id="bibliography_51" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cambricon:an instruction set architecture for neural networks">

                                <b>[51]</b>Liu Shaoli, Du Zidong, Tao Jinhua, et al.Cambricon:An instruction set architecture for neural networks[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:393-405
                            </a>
                        </p>
                        <p id="717">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A massively parallel coprocessor for convolutional neural networks">

                                <b>[52]</b>Sankaradas M, Jakkula V, Cadambi S, et al.A massively parallel coprocessor for convolutional neural networks[C]Proc of the 20th IEEE Int Conf on Application-Specific Systems, Architectures and Processors.Piscataway, NJ:IEEE, 2009:53-60
                            </a>
                        </p>
                        <p id="719">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards an embedded biologically-inspired machine vision processor">

                                <b>[53]</b>Sriram V, Cox D, Tsoi K H, et al.Towards an embedded biologically-inspired machine vision processor[C]Proc of the 9th Int Conf on Field-Programmable Technology.Piscataway, NJ:IEEE, 2011:273--278
                            </a>
                        </p>
                        <p id="721">
                            <a id="bibliography_54" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000014244&amp;v=MTQ4MjJJPU5pZklZN0s3SHRqTnI0OUZaT29MRG5nOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWI3SUpsd2RiaA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[54]</b>Chakradhar S, Sankaradas M, Jakkula V, et al.Adynamically configurable coprocessor for convolutional neural networks[C]Proc of the 38th Int Symp on Computer Architecture.New York:ACM, 2010:247-257
                            </a>
                        </p>
                        <p id="723">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning with limited numerical precision">

                                <b>[55]</b>Gupta S, Agrawal A, Gopalakrishnan K, et al.Deep learning with limited numerical precision[J/OL].Computer Science, 2015[2018-04-13].https:arxiv.org/abs/1502.02551
                            </a>
                        </p>
                        <p id="725">
                            <a id="bibliography_56" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Memory-centric accelerator design for convolutional neural networks">

                                <b>[56]</b>Peemen M, Setio A A A, Mesman B, et al.Memory-centric accelerator design for convolutional neural networks[C]Proc of the 31st Int Conf on Computer Design.Piscataway, NJ:IEEE, 2013:13-19
                            </a>
                        </p>
                        <p id="727">
                            <a id="bibliography_57" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing fpga-based accelerator design for deep convolu-tional neural networks">

                                <b>[57]</b>Zhang Chen, Li Peng, Sun Guangyu, et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays.New York:ACM, 2015:161-170
                            </a>
                        </p>
                        <p id="729">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cnvlutin:Ineffectual-neuron-free deep neural network computing">

                                <b>[58]</b>Albericio J, Judd P, Hetherington T, et al.Cnvlutin:Ineffectual-neuron-free deep neural network computing[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:1-13
                            </a>
                        </p>
                        <p id="731">
                            <a id="bibliography_59" >
                                    <b>[59]</b>
                                Zhang Shijin, Du Zidong, Zhang Lei, et al.Cambricon-X:An accelerator for sparse neural networks[C]Proc of the49th IEEE/ACM Int Symp on Microarchitecture.Piscataway, NJ:IEEE, 2016:No.20
                            </a>
                        </p>
                        <p id="733">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EIE:Efficient inference engine on compressed deep neural network">

                                <b>[60]</b>Han Song, Liu Xingyu, Mao Huizi, et al.EIE:Efficient inference engine on compressed deep neural network[C]Proc of the 43rd Int Symp on Computer Architecture.Piscataway, NJ:IEEE, 2016:243-254
                            </a>
                        </p>
                        <p id="735">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalpel:Customizing DNN pruning to the underlying hardware parallelism">

                                <b>[61]</b>Yu Jiecao, Lukefahr A, Palframan D, et al.Scalpel:Customizing DNN pruning to the underlying hardware parallelism[C]Proc of the 44th Annual Int Symp on Computer Architecture.New York:ACM, 2017:548-560
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902002" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902002&amp;v=MTY5NTA1NE8zenFxQnRHRnJDVVJMT2VaZVZ2Rnk3bldydklMeXZTZExHNEg5ak1yWTlGWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
