<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133243489502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201902008%26RESULT%3d1%26SIGN%3dFjb060FsqutTW2YJ0FSPqves9TU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201902008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902008&amp;v=Mjg5Mjc0SDlqTXJZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVzc3Qkx5dlNkTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#219" data-title="&lt;b&gt;1&lt;/b&gt;&lt;b&gt;相关工作&lt;/b&gt; "><b>1</b><b>相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#229" data-title="&lt;b&gt;2&lt;/b&gt;&lt;b&gt;多尺度Faster-RCNN检测算法&lt;/b&gt; "><b>2</b><b>多尺度Faster-RCNN检测算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#230" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;b&gt;多尺度检测&lt;/b&gt;"><b>2.1</b><b>多尺度检测</b></a></li>
                                                <li><a href="#236" data-title="&lt;b&gt;2.2&lt;/b&gt;&lt;b&gt;训练数据变换&lt;/b&gt;"><b>2.2</b><b>训练数据变换</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#242" data-title="&lt;b&gt;3&lt;/b&gt;&lt;b&gt;实&lt;/b&gt;&lt;b&gt;验&lt;/b&gt; "><b>3</b><b>实</b><b>验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#243" data-title="&lt;b&gt;3.1&lt;/b&gt;&lt;b&gt;实验设置&lt;/b&gt;"><b>3.1</b><b>实验设置</b></a></li>
                                                <li><a href="#247" data-title="&lt;b&gt;3.2&lt;/b&gt;&lt;b&gt;实验结果&lt;/b&gt;"><b>3.2</b><b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#272" data-title="&lt;b&gt;4&lt;/b&gt;&lt;b&gt;总结与展望&lt;/b&gt; "><b>4</b><b>总结与展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#209" data-title="图1 常规目标与小目标">图1 常规目标与小目标</a></li>
                                                <li><a href="#233" data-title="图2 重构图像">图2 重构图像</a></li>
                                                <li><a href="#234" data-title="图3 网络结构">图3 网络结构</a></li>
                                                <li><a href="#238" data-title="图4 训练数据与测试数据">图4 训练数据与测试数据</a></li>
                                                <li><a href="#239" data-title="图5 不同目标的分布">图5 不同目标的分布</a></li>
                                                <li><a href="#249" data-title="&lt;b&gt;表1&lt;/b&gt;&lt;b&gt;检测精度&lt;/b&gt;"><b>表1</b><b>检测精度</b></a></li>
                                                <li><a href="#255" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;b&gt;检测时间&lt;/b&gt;"><b>表2</b><b>检测时间</b></a></li>
                                                <li><a href="#266" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;b&gt;训练数据采样后的检测精度&lt;/b&gt;"><b>表3</b><b>训练数据采样后的检测精度</b></a></li>
                                                <li><a href="#267" data-title="图6 部分检测结果">图6 部分检测结果</a></li>
                                                <li><a href="#271" data-title="图7 不同目标采样后的分布">图7 不同目标采样后的分布</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="309">


                                    <a id="bibliography_1" title="Joshi K A, Thakore D G.A survey on moving object detection and tracking in video surveillance system[J].International Journal of Soft Computing and Engineering, 2012, 2 (3) :44-48" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey on moving object detection and tracking in video surveillance system">
                                        <b>[1]</b>
                                        Joshi K A, Thakore D G.A survey on moving object detection and tracking in video surveillance system[J].International Journal of Soft Computing and Engineering, 2012, 2 (3) :44-48
                                    </a>
                                </li>
                                <li id="311">


                                    <a id="bibliography_2" title="Cai Zhaowei, Fan Quanfu, Feris R S, et al.A unified multiscale deep convolutional neural network for fast object detection[C]Proc of the 14th European Conf on Computer Vision.Berlin:Springer, 2016:354-370" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A unified multi-scale deep convolutional neural network for fast object detetcion">
                                        <b>[2]</b>
                                        Cai Zhaowei, Fan Quanfu, Feris R S, et al.A unified multiscale deep convolutional neural network for fast object detection[C]Proc of the 14th European Conf on Computer Vision.Berlin:Springer, 2016:354-370
                                    </a>
                                </li>
                                <li id="313">


                                    <a id="bibliography_3" title="Ren Shaoqing, He Kaiming, Girshick R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[C]Proc of the 29th Advances in Neural Information Processing Systems.Cambridge, MA:MITPress, 2015:91-99" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards real-time object detection with region pro-posal networks">
                                        <b>[3]</b>
                                        Ren Shaoqing, He Kaiming, Girshick R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[C]Proc of the 29th Advances in Neural Information Processing Systems.Cambridge, MA:MITPress, 2015:91-99
                                    </a>
                                </li>
                                <li id="315">


                                    <a id="bibliography_4" title="Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 26th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[4]</b>
                                        Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 26th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_5" title="Everingham M, Van Gool L, Williams C K I, et al.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MTk4Njg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU25sVmIzQUlWYz1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRo&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Everingham M, Van Gool L, Williams C K I, et al.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_6" title="Felzenszwalb P, McAllester D, Ramanan D.A discriminatively trained, multiscale, deformable part model[C]Proc of the 21st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1-8" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained,multiscale,deformable part model">
                                        <b>[6]</b>
                                        Felzenszwalb P, McAllester D, Ramanan D.A discriminatively trained, multiscale, deformable part model[C]Proc of the 21st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1-8
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_7" title="Felzenszwalb P, Girshick R B, McAllester D, et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (9) :1627-1645" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">
                                        <b>[7]</b>
                                        Felzenszwalb P, Girshick R B, McAllester D, et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (9) :1627-1645
                                    </a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_8" title="Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[8]</b>
                                        Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_9" title="Girshick R.Fast R-CNN[C]Proc of the 14th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[9]</b>
                                        Girshick R.Fast R-CNN[C]Proc of the 14th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_10" title="Uijlings J R R, Van De Sande K E A, Gevers T, et al.Selective search for object recognition[J].International Journal of Computer Vision, 2013, 104 (2) :154-171" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MDEzMThNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYjdJSmx3Y2F4cz1OajdCYXJLN0h0bk1yWTlGWk9vTUNuODlvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        Uijlings J R R, Van De Sande K E A, Gevers T, et al.Selective search for object recognition[J].International Journal of Computer Vision, 2013, 104 (2) :154-171
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_11" title="He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:346-361" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[11]</b>
                                        He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:346-361
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_12" title="Sermanet P, Eigen D, Zhang Xiang, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[EB/OL]. (2014-02-24) [2017-08-30].https:arxiv.org/abs/1312.6229" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Over Feat:Integrated Recognition,Localization and Detection using Convolutional Networks">
                                        <b>[12]</b>
                                        Sermanet P, Eigen D, Zhang Xiang, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[EB/OL]. (2014-02-24) [2017-08-30].https:arxiv.org/abs/1312.6229
                                    </a>
                                </li>
                                <li id="333">


                                    <a id="bibliography_13" title="Redmon J, Divvala S, Girshick R, et al.You only look once:Unified, real-time object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:779-788" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:unified,real-time object detection">
                                        <b>[13]</b>
                                        Redmon J, Divvala S, Girshick R, et al.You only look once:Unified, real-time object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:779-788
                                    </a>
                                </li>
                                <li id="335">


                                    <a id="bibliography_14" title="Liu Wei, Anguelov D, Erhan D, et al.SSD:Single shot multibox detector[C]Proc of the 14th European Conf on Computer Vision.Berlin:Springer, 2016:21-37" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single shot multibox detector">
                                        <b>[14]</b>
                                        Liu Wei, Anguelov D, Erhan D, et al.SSD:Single shot multibox detector[C]Proc of the 14th European Conf on Computer Vision.Berlin:Springer, 2016:21-37
                                    </a>
                                </li>
                                <li id="337">


                                    <a id="bibliography_15" title="Erhan D, Szegedy C, Toshev A, et al.Scalable object detection using deep neural networks[C]Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:2147-2154" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">
                                        <b>[15]</b>
                                        Erhan D, Szegedy C, Toshev A, et al.Scalable object detection using deep neural networks[C]Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:2147-2154
                                    </a>
                                </li>
                                <li id="339">


                                    <a id="bibliography_16" title="Mathe S, Pirinen A, Sminchisescu C.Reinforcement learning for visual object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2894-2902" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning for visual object detection">
                                        <b>[16]</b>
                                        Mathe S, Pirinen A, Sminchisescu C.Reinforcement learning for visual object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2894-2902
                                    </a>
                                </li>
                                <li id="341">


                                    <a id="bibliography_17" title="Takeki A, Trinh T T, Yoshihashi R, et al.Combining deep features for object detection at various scales:Finding small birds in landscape images[J].IPSJ Transactions on Computer Vision and Applications, 2016, 8 (1) :5-13" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining deep features for object detection at various scales:finding small birds in landscape images">
                                        <b>[17]</b>
                                        Takeki A, Trinh T T, Yoshihashi R, et al.Combining deep features for object detection at various scales:Finding small birds in landscape images[J].IPSJ Transactions on Computer Vision and Applications, 2016, 8 (1) :5-13
                                    </a>
                                </li>
                                <li id="343">


                                    <a id="bibliography_18" title="Chen Chenyi, Liu Mingyu, Tuzel O, et al.R-CNN for small object detection[C]Proc of the 13th Asian Conf on Computer Vision.Berlin:Springer, 2016:214-230" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CNN for small object detection">
                                        <b>[18]</b>
                                        Chen Chenyi, Liu Mingyu, Tuzel O, et al.R-CNN for small object detection[C]Proc of the 13th Asian Conf on Computer Vision.Berlin:Springer, 2016:214-230
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_19" title="Eggert C, Zecha D, Brehm S, et al.Improving small object proposals for company logo detection[C]Proc of the 7th ACM Int Conf on Multimedia Retrieval.New York:ACM, 2017:167-174" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving small object proposals for company logo detection">
                                        <b>[19]</b>
                                        Eggert C, Zecha D, Brehm S, et al.Improving small object proposals for company logo detection[C]Proc of the 7th ACM Int Conf on Multimedia Retrieval.New York:ACM, 2017:167-174
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_20" title="Gatys L A, Ecker A S, Bethge M.Image style transfer using convolutional neural networks[C]Proc of the 29th IEEEConf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2414-2423" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Style Transfer Using Convolutional Neural Networks">
                                        <b>[20]</b>
                                        Gatys L A, Ecker A S, Bethge M.Image style transfer using convolutional neural networks[C]Proc of the 29th IEEEConf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2414-2423
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_21" title="Mahendran A, Vedaldi A.Understanding deep image representations by inverting them[C]Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:5188-5196" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding deep image representations by inverting them">
                                        <b>[21]</b>
                                        Mahendran A, Vedaldi A.Understanding deep image representations by inverting them[C]Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:5188-5196
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_22" title="Hosang J, Benenson R, Doll&#225;r P, et al.What makes for effective detection proposals[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (4) :814-830" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What makes for effective detection proposals?">
                                        <b>[22]</b>
                                        Hosang J, Benenson R, Doll&#225;r P, et al.What makes for effective detection proposals[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (4) :814-830
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                    Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[C]Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:818-833</a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_24" title="Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2017-08-30].https:arxiv.org/abs/1409.1556" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[24]</b>
                                        Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2017-08-30].https:arxiv.org/abs/1409.1556
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_25" title="Maaten L, Hinton G.Visualizing data using t-SNE[J].Journal of Machine Learning Research, 2008, 9 (9) :2579-2605" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing data using t-SNE">
                                        <b>[25]</b>
                                        Maaten L, Hinton G.Visualizing data using t-SNE[J].Journal of Machine Learning Research, 2008, 9 (9) :2579-2605
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(02),319-327 DOI:10.7544/issn1000-1239.2019.20170749            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向小目标的多尺度Faster-RCNN检测算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E7%BB%A7%E9%B9%8F&amp;code=41253833&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄继鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B2%E9%A2%96%E6%AC%A2&amp;code=23471192&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">史颖欢</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E9%98%B3&amp;code=08037024&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高阳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BD%AF%E4%BB%B6%E6%96%B0%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%AD%A6)&amp;code=0069758&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机软件新技术国家重点实验室(南京大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>小目标是指图像中覆盖区域较小的一类目标.与常规目标相比, 小目标信息量少, 训练数据难以标记, 这导致通用的目标检测方法对小目标的检测效果不好, 而专门为小目标设计的检测方法往往复杂度过高或不具有通用性.在分析现有目标检测方法的基础上, 提出了一种面向小目标的多尺度快速区域卷积神经网络 (faster-regions with convolutional neural network, Faster-RCNN) 检测算法.根据卷积神经网络的特性, 修改了Faster-RCNN的网络结构, 使网络可以同时使用低层和高层的特征进行多尺度目标检测, 提升了以低层特征为主要检测依据的小目标检测任务的精度.同时, 针对训练数据难以标记的问题, 使用从搜索引擎上获取的数据来训练模型.因为这些训练数据与任务测试数据分布不同, 又利用下采样和上采样的方法对目标高分辨率的训练图像进行转化, 使训练图像和测试图像的特征分布更类似.实验结果表明:所提出的方法在小目标检测任务上的平均精度均值 (mean average precision, mAP) 可以比原始的Faster-RCNN提高约5%.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Faster-RCNN%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Faster-RCNN算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%87%87%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">采样;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *高阳 (gaoy@nju.edu.cn) ;
                                </span>
                                <span>
                                    黄继鹏 huangjipengnju@gmail.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-09-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61432008, 61673203);</span>
                    </p>
            </div>
                    <h1><b>Multi-Scale Faster-RCNN Algorithm for Small Object Detection</b></h1>
                    <h2>
                    <span>Huang Jipeng</span>
                    <span>Shi Yinghuan</span>
                    <span>Gao Yang</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory for Novel Software Technology (Nanjing University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Normally, small object is the object which only covers a small part of a whole image. Compared with regular object, small object has less information and the training data of small object is difficult to be marked. This leads to the poor performance when directly employing the previous object detection methods for small object detection. Moreover, the detection methods designed for small object are often too complex or not generic. In this paper, we propose a small object detection algorithm named multi-scale Faster-RCNN. According to the characteristics of convolutional neural network, the structure of Faster-RCNN is modified, such that the network can integrate both the low-level and high-level features for multi-scale object detection. Through such a manner, the accuracy of small object detection is improved. Simultaneously, with the goal of solving the problem that training data is difficult to be marked, we use training data crawled from search engine to train the model. Because the distribution of crawled data is different from the real test data's, training images in which objects have high resolution are transformed by means of down sampling and up sampling. It makes the feature distribution of training images and test images more similar. The experiment results show that the mean average precision (mAP) of proposed approach can be up to 5% higher than the original Faster-RCNN's in the task of small object detection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=small%20object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">small object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Faster-RCNN%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Faster-RCNN algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sampling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Huang Jipeng, born in 1994.MSc candidate.His main research interests include computer vision and deep learning.<image id="304" type="formula" href="images/JFYZ201902008_30400.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Shi Yinghuan, born in 1984.PhD.His main research interests include artificial intelligence and machine learning. (syh@nju.edu.cn) <image id="306" type="formula" href="images/JFYZ201902008_30600.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Gao Yang, born in 1972.PhD, professor and PhD supervisor.His main research interests include artificial intelligence and machine learning.<image id="308" type="formula" href="images/JFYZ201902008_30800.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-09-30</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61432008, 61673203);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="207">目标检测是结合了目标定位和识别2个任务的一项基础性计算机视觉任务, 它的目的是在图像的复杂背景中找到若干目标, 给出一个精确的目标边框 (bounding box) , 并判断该边框中目标所属的类别<sup><a class="sup">[1]</a></sup>.因为目标检测的效果直接决定了图像语义理解、目标重识别等众多高层视觉任务的效果, 并且它在智能监控系统、医学图像分析等方面具有很好的应用前景, 所以对它的研究具有很强的理论和应用价值, 目标检测也一直是计算机视觉领域备受关注的若干研究方向之一.</p>
                </div>
                <div class="p1">
                    <p id="208">小目标<sup><a class="sup">[2]</a></sup>是指自身真实的物理尺寸过小或与拍摄设备距离较远导致其在整幅图像中占比小的一类目标, 通常只含有几十个或更少的像素.如图1所示, 第1行图像中边框标记出的挖掘机为小目标, 第2行图像是Faster-RCNN<sup><a class="sup">[3]</a></sup>的一些检测结果, 第2行中的各类目标为目标检测中经常研究的一些目标.装配在许多设备上的摄像头, 比如:无人机摄像头、通信基站摄像头以及其他一些架设高度较高的监控摄像头等, 它们采集的图像中存在很多小目标, 所以研究小目标检测对于分析和利用这些图像非常重要, 这在安防、交通、救援等方面有重要的应用价值.</p>
                </div>
                <div class="area_img" id="209">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_209.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 常规目标与小目标" src="Detail/GetImg?filename=images/JFYZ201902008_209.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 常规目标与小目标  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_209.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Regular object and small object</p>

                </div>
                <div class="p1">
                    <p id="210">自2012年Krizhevsky等人<sup><a class="sup">[4]</a></sup>提出的AlexNet在ImageNet图像分类任务的精度上取得显著提升以来, 以卷积神经网络 (convolutional neural network, CNN) 为代表的各类深度学习方法被广泛应用于许多视觉任务中, 这其中也包括目标检测.因为相较于传统的基于手工特征的方法, 基于深度学习的方法通常可以取得更好的效果, 所以现在深度学习的方法在目标检测这一方向上已经成为主流, 绝大多数的研究工作都是围绕CNN展开的.</p>
                </div>
                <div class="p1">
                    <p id="211">然而, 即使这些基于深度学习的方法在通用的目标检测数据集上取得了很好的效果, 它们仍然不能很好地解决小目标检测这一问题.例如:图1第1行图像中的挖掘机用通用的目标检测方法是检测不出来的, 而用本文的方法则可以检测出来.</p>
                </div>
                <div class="p1">
                    <p id="212">小目标检测问题主要有2个难点:</p>
                </div>
                <div class="p1">
                    <p id="213">1) 当目标在图像中占比非常小的情况下, 对应区域的像素所反映出的信息量是非常有限的.在极端情况下, 小目标检测任务甚至可能退化为像素分类任务.这导致一些通用的目标检测算法难以适用于小目标检测, 而一些专门为小目标检测设计的算法只能针对特定的应用背景, 缺乏通用性.</p>
                </div>
                <div class="p1">
                    <p id="214">2) 标记图像中的小目标作为训练数据时很容易出现误差, 在目标本身已经很小的情况下, 细微的误差容易对检测结果造成较大的影响, 而且标记数据的人工成本也很高<sup><a class="sup">[5]</a></sup>, 所以目前为止还没有一个较大的完整的用于小目标检测研究的数据集, 这阻碍了学术界对于小目标检测的研究.</p>
                </div>
                <div class="p1">
                    <p id="215">针对上述的问题, 本文提出了一种面向小目标的多尺度Faster-RCNN检测算法.该方法根据卷积神经网络的特性, 对网络结构做出修改, 使得网络可以同时利用低层和高层的特征进行多尺度检测.同时, 本文还用网上搜索引擎爬虫获得的高分辨率大目标图像进行训练以解决缺少小目标训练数据的问题.由于高分辨率的大目标训练图像与低分辨率的小目标测试图像的数据分布存在很大差异, 通过可视化分析问题后, 又使用下采样和上采样的方法尽可能消除训练图像与测试图像的差异.实验表明, 所提出的方法确实可以较好地解决小目标检测的问题.</p>
                </div>
                <div class="p1">
                    <p id="216">具体来说, 本文的主要贡献包括:</p>
                </div>
                <div class="p1">
                    <p id="217">1) 针对Faster-RCNN在小目标检测上的不足, 提出了通用的网络结构修改规则, 同时利用低层和高层的特征进行多尺度检测, 提高了小目标检测的精度;</p>
                </div>
                <div class="p1">
                    <p id="218">2) 通过对目标高分辨率图像进行下采样和上采样的方法, 使网上获取的数据与实际测试数据的分布尽可能接近, 解决缺少小目标训练数据的问题.</p>
                </div>
                <h3 id="219" name="219" class="anchor-tag"><b>1</b><b>相关工作</b></h3>
                <div class="p1">
                    <p id="220">在2014年以前, 目标检测领域效果最好的方法是Felzenszwalb等人<sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup>提出的可变形部件模型 (deformable part model, DPM) , 这一方法本质上利用的是手工特征.然而随着深度学习的发展, DPM的检测效果已经远远比不上深度学习的方法了, 因此现在目标检测领域的主流是深度学习, 绝大多数研究工作都是围绕CNN展开的.</p>
                </div>
                <div class="p1">
                    <p id="221">区域卷积神经网络 (regions with convolutional neural network, RCNN) 系列<sup>[<a class="sup">3</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup>的方法是用深度学习的方法进行目标检测的代表性工作.Girshick等人<sup><a class="sup">[8]</a></sup>提出的RCNN开创性地将候选区域生成和深度学习的分类方法结合起来.RCNN通过过分割<sup><a class="sup">[10]</a></sup>生成一些候选区域, 然后用CNN分别对每一块候选区域提取特征, 最后送入分类器判断类别并对边框进行回归.因为不同候选区域重复卷积的问题, 该方法的速度非常慢.</p>
                </div>
                <div class="p1">
                    <p id="222">在借鉴何恺明等人<sup><a class="sup">[11]</a></sup>提出的空间金字塔池化网络 (spatial pyramid pooling network, SPPNet) 和Sermanet等人<sup><a class="sup">[12]</a></sup>提出的定位思想后, Girshick等人<sup><a class="sup">[9]</a></sup>又提出了Fast-RCNN.该方法在RCNN的基础上引入了目标区域池化 (region of interest pooling, ROI pooling) , 这实际上是一个单层的金字塔池化层, 它使得网络可以对不同尺寸的输入图像都产生相同大小的特征, 保证了输入图像的尺寸不变性, 同时它通过特征映射的方法直接在整张图像的特征图 (feature map) 上提取候选区域的特征, 避免了重复卷积, 在精度和速度上都优于RCNN. Fast-RCNN之后, 制约该方法速度的主要原因变成了候选区域生成所使用的过分割.</p>
                </div>
                <div class="p1">
                    <p id="223">随后, 任少卿等人<sup><a class="sup">[3]</a></sup>在Fast-RCNN的基础上又提出了Faster-RCNN.该方法采用锚点 (anchor) 的方式来生成候选区域, 将候选区域生成也交由深度网络来做, 速度和精度进一步提升.至此, 使用深度学习做目标检测被RCNN统一到了一个深度框架中.</p>
                </div>
                <div class="p1">
                    <p id="224">继RCNN之后, Redmon等人<sup><a class="sup">[13]</a></sup>又提出了更快的目标检测方法YOLO (you only look once) .YOLO不同于RCNN, 它将目标检测作为回归问题处理, 直接在划分的网格上回归目标边界框和所属类别.因为免去了复杂费时的候选区域生成, YOLO的速度非常快, 但对于靠得很近或较小的目标, 其检测精度不高且泛化能力偏弱.</p>
                </div>
                <div class="p1">
                    <p id="225">结合RCNN的Anchor思想和YOLO的回归思想, 刘伟等人<sup><a class="sup">[14]</a></sup>提出了单次多框检测器 (single shot multi-box detector, SSD) .SSD有RCNN定位准确和YOLO速度快的优点, 因为引入多尺度检测<sup><a class="sup">[15]</a></sup>, 它对于不同尺寸的目标都有较好的检测效果.检测速度和精度进一步提升.</p>
                </div>
                <div class="p1">
                    <p id="226">这3类方法对于一般的目标检测问题有不错的精度, 然而对于小目标的检测精度却都不理想.事实上, 这些方法检测不出来的目标往往不是一些复杂的目标, 而是一些较小的目标, 比如PASCAL VOC数据集<sup><a class="sup">[5]</a></sup>中的瓶子.这说明不是深度网络缺乏学习和表示能力, 而是深度网络提取的小目标特征所能表示的信息实在是太少了<sup><a class="sup">[16]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="227">除此之外, 还有一些研究者专门针对小目标的检测进行了研究.Takeki等人<sup><a class="sup">[17]</a></sup>提出了一种结合图像语义分割的小目标检测方法, 该方法将全卷积网络 (fully convolutional network, FCN) 及其变体和CNN结合起来, 以支持向量机 (support vector machine, SVM) 整合三者的结果, 但是这个方法只适用于在纯净的天空背景下检测小鸟这一任务, 难以适用于复杂背景下的多类目标检测任务.Chen等人<sup><a class="sup">[18]</a></sup>基于RCNN进行改进, 使得RCNN可以生成更小的候选区域, 对RCNN在小目标检测这一任务上的提升非常大, 但算法复杂度较高.Eggert等人<sup><a class="sup">[19]</a></sup>同样也是基于RCNN进行改进, 他们对feature map分辨率与检测效果的关系进行了研究, 在公司商标检测的问题背景下, 用改进的anchor box生成方法提升了RCNN使用高分辨率feature map进行检测的效果.</p>
                </div>
                <div class="p1">
                    <p id="228">由此可见, 虽然已经有人针对小目标检测做出一些工作, 但是这些方法都只能在特定的问题背景下使用, 或者对于一般目标检测的效果不如前面3种方法, 缺乏一定的通用性.</p>
                </div>
                <h3 id="229" name="229" class="anchor-tag"><b>2</b><b>多尺度Faster-RCNN检测算法</b></h3>
                <h4 class="anchor-tag" id="230" name="230"><b>2.1</b><b>多尺度检测</b></h4>
                <div class="p1">
                    <p id="231">在一个多层卷积神经网络中, 低层的特征往往能很好地表示图像的纹理、边缘等的细节信息, 而越往高层走, 随着神经元感受野的扩大, 高层的特征往往能很好地表示图像的语义信息, 但是相应的就会忽略一些细节信息<sup><a class="sup">[20]</a></sup>.</p>
                </div>
                <div class="p1">
                    <p id="232">当一个目标非常小时, 从仅有的像素中所能反应出来的语义信息是非常有限的, 而太小的目标也完全不需要神经元具有很大的感受野, 所以我们一般要更多地依靠低层的细节信息才能识别出小目标.为了证明这个结论, 我们利用梯度上升法对图像特征进行重构<sup><a class="sup">[21]</a></sup>, 以此可视化地展示深度网络不同层提取出的小目标特征有何区别.如图2所示, 图2 (a) 中边框里的挖掘机为需要检测的小目标, 我们通过VGG16网络提取它的特征, 并利用conv1_2和conv5_3层提取出的特征对图像进行重构;图2 (b) 是用VGG16网络conv1_2层特征重构的结果, 可以清楚看出是一个挖掘机;而图2 (c) 是用VGG16网络conv5_3层特征重构的结果, 只能看清轮廓.所以对于小目标检测的问题, 卷积神经网络的低层特征往往比高层特征要更加有效.</p>
                </div>
                <div class="area_img" id="233">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_233.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 重构图像" src="Detail/GetImg?filename=images/JFYZ201902008_233.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 重构图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_233.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Reconstructed image</p>

                </div>
                <div class="area_img" id="234">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_234.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 网络结构" src="Detail/GetImg?filename=images/JFYZ201902008_234.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_234.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Network structure</p>

                </div>
                <div class="p1">
                    <p id="235">在原始Faster-RCNN的方法中, 候选区域由候选区域生成网络 (region proposal network, RPN) 生成, 候选区域的特征仅由最后一个卷积层经过目标区域池化得到, 利用这样的高层特征对小目标进行检测显然会存在比较大的问题<sup><a class="sup">[22]</a></sup>.因此我们参考了SSD方法的思想, 对Faster-RCNN引入了多尺度检测, 即不单单依靠最后一层的feature map进行检测, 而是对网络中的多个尺度的feature map都进行生成候选区域的操作.具体流程如图3所示, 输入图片经过一个卷积神经网络提取特征, 将不同层提取出的多个不同尺度的feature map送入各自的RPN生成候选区域, 不同尺度对应的RPN是有区别的, 因为低层神经元的感受野小, 对应的anchor box尺寸也要小, 所以越低层的特征得到的候选区域越小, 具体anchor设置将在实验环节详细说明.得到生成的候选区域后, 求取feature map映射, 再通过ROI pooling将特征变成统一大小, 最后送入分类器, 这样就可以充分利用低层特征对小目标进行检测.这样的结构适用于不同的特征网络, 我们在实验部分分别对利用ZF<sup><a class="sup">[23]</a></sup>和VGG16<sup><a class="sup">[24]</a></sup>2种特征网络的Faster-RCNN方法进行改进.对于ZF网络, 将conv1, conv2, conv5这3层的输出送入候选区域生成网络和ROI pooling进行多尺度检测;对于VGG16网络, 将conv1_2, conv2_2, conv3_3, conv4_3, conv5_3这5层的输出送入候选区域生成网络和ROI pooling进行多尺度检测, 其他具体参数设置会在实验部分说明.</p>
                </div>
                <h4 class="anchor-tag" id="236" name="236"><b>2.2</b><b>训练数据变换</b></h4>
                <div class="p1">
                    <p id="237">网络结构的改进解决了只利用高层网络特征难以检测小目标的问题, 针对小目标难以标记、缺乏训练样本, 我们利用了从网络上获取的图像作为训练数据, 共7 804张.然而通过搜索引擎关键字检索出来的图像往往是以目标为主体的, 相比测试图像中的小目标, 这些图像中的目标分辨率较大.比如:我们通过“挖掘机”检索出来的一般都是以挖掘机为主体的图像, 挖掘机这个目标在图像中所占的比例非常大, 而测试图像中目标所占比例却非常小, 两者的像素不一样多, 所反映的信息量也不一样, 因而数据的分布可能存在差异.如图4所示, 图4第1列为部分网上获取的高分辨率大目标训练图像, 图4第2列为采样处理后的训练图像, 图4第3列和第4列为小目标检测的部分测试图像.</p>
                </div>
                <div class="area_img" id="238">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_238.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 训练数据与测试数据" src="Detail/GetImg?filename=images/JFYZ201902008_238.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 训练数据与测试数据  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_238.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Training data and test data</p>

                </div>
                <div class="area_img" id="239">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_239.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同目标的分布" src="Detail/GetImg?filename=images/JFYZ201902008_239.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同目标的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_239.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The distribution of different object</p>

                </div>
                <div class="p1">
                    <p id="240">为了说明高分辨率 (high resolution, HR) 目标和低分辨率 (low resolution, LR) 目标的分布确实存在差异, 我们利用t-SNE<sup><a class="sup">[25]</a></sup>的方法对这2类图像中的目标特征进行降维.我们以原始Faster-RCNN结构进行验证, 特征网络为VGG16.用目标低分辨率图像训练网络, 再用目标低分辨率图像和目标高分辨率图像测试.对ROI pooling层后得到的大小相同的特征向量用T-SNE方法降维.可视化结果如图5所示, 紫色圆点代表低分辨率目标, 蓝色十字代表高分辨率目标, 可见两者确实存在很大差异.</p>
                </div>
                <div class="p1">
                    <p id="241">针对这样一个分布差异, 我们采用下采样和上采样的方式对训练数据进行预处理.使用的下采样方法包括最大池化和平均池化, 这使得高分辨率图像的信息量减少.而使用的上采样方法包括线性插值、区域插值和最近邻插值, 这将图像还原为原始大小, 并引入了一些噪声.从人的视觉上来看, 采样后的训练图像和测试图像更类似, 我们在实验部分对这6种采样方法的组合进行了实验, 经过效果最好的采样方法处理过后的训练图像如图4第2列所示.实验证明, 下采样和上采样可以有效提升使用高分辨率目标图像训练出的模型检测目标低分辨率图像的检测精度.不同的下采样和上采样方式对检测效果的影响会在实验部分给出说明.</p>
                </div>
                <h3 id="242" name="242" class="anchor-tag"><b>3</b><b>实</b><b>验</b></h3>
                <h4 class="anchor-tag" id="243" name="243"><b>3.1</b><b>实验设置</b></h4>
                <div class="p1">
                    <p id="244">实验采用的数据集由2部分组成, 检测的小目标为挖掘机.一部分来源于基站铁塔上的监控摄像头所拍摄的图像, 共有14 449张, 其中的挖掘机通常非常小;另一部分来源于通过搜索引擎搜索“挖掘机”关键字获取的高分辨率大目标图像, 共有7 804张, 其中的挖掘机通常比较大, 如图4所示.</p>
                </div>
                <div class="p1">
                    <p id="245">为了比较方法的精度, 分别采用了ZF和VGG16这2种网络作为特征网络进行实验.因为检测目标是挖掘机, 所以anchor box的比例参数都设置为0.7, 1, 1.4.每个尺度的feature map对应候选区域生成网络的anchor尺寸不同, 对于ZF网络:conv1层对应的尺度参数为2, 4, 8;conv2层对应的尺度参数为4, 8, 16;conv5层对应的尺度参数为8, 16, 32;对于VGG16网络:conv1_2层对应的尺度参数为2, 4;conv2_2层对应的尺度参数为4, 8;conv3_3层对应的尺度参数为4, 8;conv4_3层对应的尺度参数为8, 16;conv5_3层对应的尺度参数为8, 16;其余参数均与原始Faster-RCNN一致.</p>
                </div>
                <div class="p1">
                    <p id="246">为了比较方法的计算复杂度, 在比较精度的实验设置基础上作如下设置:对于ZF网络, 分别测试只采用conv1特征、conv2特征、conv5特征和同时采用三者的方法复杂度;对于VGG网络, 分别测试只采用conv1_2特征、conv2_2特征、conv5_3特征和同时采用三者的方法复杂度.以平均单张图像的检测时间为计算复杂度的评估指标, 单位为s.</p>
                </div>
                <h4 class="anchor-tag" id="247" name="247"><b>3.2</b><b>实验结果</b></h4>
                <div class="p1">
                    <p id="248">将目标低分辨率数据集划分成2部分, 7 225张用于评估模型性能, 7 224张用于单独或搭配目标高分辨率图像进行训练, 在不同特征网络下所得的检测结果如表1所示:</p>
                </div>
                <div class="area_img" id="249">
                    <p class="img_tit"><b>表1</b><b>检测精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1</b><b>The mAP of Detection</b></p>
                    <p class="img_note">%</p>
                    <table id="249" border="1"><tr><td><br />Network</td><td>HR</td><td>LR</td><td>LR+HR</td></tr><tr><td><br />ZF</td><td>12.1</td><td>51.2</td><td>47.7</td></tr><tr><td><br /><b>MS-ZF</b></td><td><b>20.4</b></td><td><b>56.2</b></td><td><b>48.0</b></td></tr><tr><td><br />VGG16</td><td>17.1</td><td>53.1</td><td>50.3</td></tr><tr><td><br /><b>MS-VGG16</b></td><td><b>30.5</b></td><td><b>58.7</b></td><td><b>50.4</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="250">第1列表示模型使用的网络结构, 带MS前缀的表示使用多尺度检测改进的模型.之后每一列第1行表示训练数据, HR表示从网上获取的目标高分辨率图像, LR表示用于训练的7 224张目标低分辨率图像, 其余数值分别表示在对应数据下训练出的模型的检测精度, 指标为平均精度均值 (mean average precision, mAP) , 在这里实际上是挖掘机的AP.</p>
                </div>
                <div class="p1">
                    <p id="251">从表1中可以得出2个结论:</p>
                </div>
                <div class="p1">
                    <p id="252">1) 不论采用高分辨率图像还是低分辨率图像作为训练数据, 使用多尺度检测的方法都可以有效提升小目标的检测精度, 这说明结合深度网络的低层和高层特征进行多尺度检测的方法确实可行;</p>
                </div>
                <div class="p1">
                    <p id="253">2) 仅使用高分辨率图像作为训练数据的模型检测效果不佳, 仅使用低分辨率图像作为训练数据的模型检测效果较好, 而两者结合时性能折中, 这说明直接使用网上获取的目标高分辨率图像训练是不行的, 想要用这部分数据必须解决训练集和测试集之间存在的差异, 即高分辨率目标和低分辨率目标的差异.</p>
                </div>
                <div class="p1">
                    <p id="254">将7 225张测试图像的平均检测时间作为评估计算复杂度的指标, 不同特征网络下所得的检测结果如表2所示:</p>
                </div>
                <div class="area_img" id="255">
                    <p class="img_tit"><b>表2</b><b>检测时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2</b><b>The Time of Detection</b></p>
                    <p class="img_note">s</p>
                    <table id="255" border="1"><tr><td><br />Network</td><td>conv1</td><td>conv2</td><td>conv5</td><td>All</td></tr><tr><td><br />ZF</td><td>0.357</td><td>0.224</td><td><b>0.072</b></td><td>0.613</td></tr><tr><td><br />VGG16</td><td>0.484</td><td>0.354</td><td><b>0.081</b></td><td>0.757</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="256">第1列表示模型使用的网络结构, 之后每一列第1行表示采用哪一层的特征进行检测, All表示采用全部3个特征.为便于表格说明, VGG的conv1_2, conv2_2, conv5_3分别简写为conv1, conv2, conv5.</p>
                </div>
                <div class="p1">
                    <p id="257">从表2中可以得出2个结论:</p>
                </div>
                <div class="p1">
                    <p id="258">1) 在只利用一个特征检测的前提下, 利用高层特征的平均检测时间少, 这说明大的低层特征虽然适用于小目标检测, 但会带来额外的计算开销;</p>
                </div>
                <div class="p1">
                    <p id="259">2) 同时利用多个特征增加的计算开销仍然在可以接受的范围内, 可以胜任实时性要求不高的任务.</p>
                </div>
                <div class="p1">
                    <p id="260">通过对目标高分辨率图像进行下采样和上采样得到新的训练数据, 用新数据训练出的模型检测精度如表3所示.表3中的字符含义与表1基本一致, 训练数据HR后的2个后缀分别表示不同下采样和上采样的操作组合, 第1个后缀M和A分别表示最大池化和平均池化2种下采样的方式, 池化操作的窗口为2×2, 滑动步长为2, 第2个后缀A, L, N分别表示区域插值、线性插值和最近邻插值3种上采样的方式.</p>
                </div>
                <div class="p1">
                    <p id="261">从表3中可以看出:</p>
                </div>
                <div class="p1">
                    <p id="262">1) 只需要通过简单的下采样就可以大幅提升目标高分辨率图像训练出的模型精度, 这说明下采样的方式可以从一定程度上消除目标高分辨率图像和目标低分辨率图像数据差异带来的影响;</p>
                </div>
                <div class="p1">
                    <p id="263">2) 最大池化在这样一个问题背景下一般比平均池化效果好;</p>
                </div>
                <div class="p1">
                    <p id="264">3) 在下采样的基础上使用线性插值上采样的方法可以略微提升模型的精度, 目前还不能从理论上解释原因, 但可能是因为这样增加了噪声, 从一定程度上防止了过拟合;</p>
                </div>
                <div class="p1">
                    <p id="265">4) 结合采样变换后的目标高分辨率图像和目标低分辨率图像训练出的模型检测精度较高, 不但没有出现表1精度降低的情况, 反而提升了精度, 这说明对目标高分辨率图像进行采样变换确实可以消除目标高分辨率图像和目标低分辨率图像数据差异带来的影响.在小目标检测数据难以标记、缺少训练数据的情况下, 可以通过这种方式简单、快速地增加训练数据量, 提升检测精度.部分检测结果如图6所示.</p>
                </div>
                <div class="area_img" id="266">
                    <p class="img_tit"><b>表3</b><b>训练数据采样后的检测精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3</b><b>The mAP of Detection with Sampled Training Data</b></p>
                    <p class="img_note">%</p>
                    <table id="266" border="1"><tr><td><br />Network</td><td>HR</td><td>LR</td><td>LR+HR-M-L</td><td>HR-M</td><td>HR-M-A</td><td>HR-M-L</td><td>HR-M-N</td><td>HR-A</td><td>HR-A-A</td><td>HR-A-L</td><td>HR-A-N</td></tr><tr><td><br /><b>MS-ZF</b></td><td>20.4</td><td>56.2</td><td><b>58.4</b></td><td>47.3</td><td>47.7</td><td><b>49.4</b></td><td>45.8</td><td>43.2</td><td>42.6</td><td>44.0</td><td>36.8</td></tr><tr><td><br /><b>MS-VGG16</b></td><td>30.5</td><td>58.7</td><td><b>61.5</b></td><td>52.3</td><td>51.2</td><td><b>55.2</b></td><td>49.6</td><td>43.3</td><td>43.2</td><td>46.2</td><td>37.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Note:First suffix—M: max pooling; A: average. Second suffix—L: linear interpolation; N: nearest neighbor interpolation; A: area interpolation</p>
                </div>
                <div class="area_img" id="267">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_267.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 部分检测结果" src="Detail/GetImg?filename=images/JFYZ201902008_267.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 部分检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_267.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 Partial detection results</p>

                </div>
                <div class="p1">
                    <p id="268">同样, 为了说明对目标高分辨率图像进行采样变换可以消除目标高分辨率图像和目标低分辨率图像数据差异带来的影响, 我们利用T-SNE<sup><a class="sup">[25]</a></sup>方法对采样操作前后目标的特征进行降维.</p>
                </div>
                <div class="p1">
                    <p id="269">我们以实验效果最好的MS-VGG16结构进行验证.用目标低分辨率图像训练网络, 再用目标低分辨率图像、目标高分辨率图像和采样操作后的目标高分辨率图像三者来测试.对ROI pooling层后得到的大小相同的特征向量用T-SNE方法降维.</p>
                </div>
                <div class="p1">
                    <p id="270">可视化的结果如图7所示, 紫色圆点代表低分辨率目标, 蓝色十字代表高分辨率目标, 红色叉代表采样后的高分辨率目标, 可以看出采样操作后的特征分布确实比采样操作前的特征分布更接近目标低分辨率图像的特征分布.</p>
                </div>
                <div class="area_img" id="271">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201902008_271.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同目标采样后的分布" src="Detail/GetImg?filename=images/JFYZ201902008_271.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同目标采样后的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201902008_271.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 7 The distribution of different objects after sampling</p>

                </div>
                <h3 id="272" name="272" class="anchor-tag"><b>4</b><b>总结与展望</b></h3>
                <div class="p1">
                    <p id="273">目标检测作为计算机视觉领域的一个基本任务一直受到许多科研人员的关注, 目标检测方法的性能也直接关系到许多高层领域的研究.当前通用的目标检测方法在小目标检测上效果不佳, 而专门为小目标检测设计的方法又不具有通用性, 因而本文针对小目标检测问题进行研究.首先, 我们根据卷积神经网络的特性, 对Faster-RCNN进行改进, 引入了多尺度检测, 改进后的方法比原始Faster-RCNN的检测精度提升了约5%;然后, 我们用下采样和上采样组合的方式变换网上获取的高分辨率图像, 使高分辨率目标的特征分布更接近低分辨率目标, 从而可以很方便地扩充训练数据集, 解决了小目标数据难以标记的问题.</p>
                </div>
                <div class="p1">
                    <p id="274">将来的工作可以从2个方面进行:现在的目标检测领域几乎都是深度学习的方法, 虽然深度学习方法在提取图像特征方面的表现确实非常优秀, 但是对于语义信息相对不是非常明显的小目标, 我们也可以尝试研究一些非深度学习的方法;其次, 当目标非常小时, 目标检测就退化成了像素分类的问题, 我们也可以尝试从像素分类的角度出发, 研究分割的一些方法能以什么样的形式应用在小目标检测的问题上.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="309">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey on moving object detection and tracking in video surveillance system">

                                <b>[1]</b>Joshi K A, Thakore D G.A survey on moving object detection and tracking in video surveillance system[J].International Journal of Soft Computing and Engineering, 2012, 2 (3) :44-48
                            </a>
                        </p>
                        <p id="311">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A unified multi-scale deep convolutional neural network for fast object detetcion">

                                <b>[2]</b>Cai Zhaowei, Fan Quanfu, Feris R S, et al.A unified multiscale deep convolutional neural network for fast object detection[C]Proc of the 14th European Conf on Computer Vision.Berlin:Springer, 2016:354-370
                            </a>
                        </p>
                        <p id="313">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards real-time object detection with region pro-posal networks">

                                <b>[3]</b>Ren Shaoqing, He Kaiming, Girshick R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[C]Proc of the 29th Advances in Neural Information Processing Systems.Cambridge, MA:MITPress, 2015:91-99
                            </a>
                        </p>
                        <p id="315">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[4]</b>Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]Proc of the 26th Advances in Neural Information Processing Systems.Cambridge, MA:MIT Press, 2012:1097-1105
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDc1NzdCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU25sVmIzQUlWYz1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Everingham M, Van Gool L, Williams C K I, et al.The pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision, 2010, 88 (2) :303-338
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained,multiscale,deformable part model">

                                <b>[6]</b>Felzenszwalb P, McAllester D, Ramanan D.A discriminatively trained, multiscale, deformable part model[C]Proc of the 21st IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2008:1-8
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">

                                <b>[7]</b>Felzenszwalb P, Girshick R B, McAllester D, et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (9) :1627-1645
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[8]</b>Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:580-587
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[9]</b>Girshick R.Fast R-CNN[C]Proc of the 14th IEEE Int Conf on Computer Vision.Piscataway, NJ:IEEE, 2015:1440-1448
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MjQ5NTlCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViN0lKbHdjYXhzPU5qNw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>Uijlings J R R, Van De Sande K E A, Gevers T, et al.Selective search for object recognition[J].International Journal of Computer Vision, 2013, 104 (2) :154-171
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[11]</b>He Kaiming, Zhang Xiangyu, Ren Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:346-361
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Over Feat:Integrated Recognition,Localization and Detection using Convolutional Networks">

                                <b>[12]</b>Sermanet P, Eigen D, Zhang Xiang, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[EB/OL]. (2014-02-24) [2017-08-30].https:arxiv.org/abs/1312.6229
                            </a>
                        </p>
                        <p id="333">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:unified,real-time object detection">

                                <b>[13]</b>Redmon J, Divvala S, Girshick R, et al.You only look once:Unified, real-time object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:779-788
                            </a>
                        </p>
                        <p id="335">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single shot multibox detector">

                                <b>[14]</b>Liu Wei, Anguelov D, Erhan D, et al.SSD:Single shot multibox detector[C]Proc of the 14th European Conf on Computer Vision.Berlin:Springer, 2016:21-37
                            </a>
                        </p>
                        <p id="337">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">

                                <b>[15]</b>Erhan D, Szegedy C, Toshev A, et al.Scalable object detection using deep neural networks[C]Proc of the 27th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2014:2147-2154
                            </a>
                        </p>
                        <p id="339">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning for visual object detection">

                                <b>[16]</b>Mathe S, Pirinen A, Sminchisescu C.Reinforcement learning for visual object detection[C]Proc of the 29th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2894-2902
                            </a>
                        </p>
                        <p id="341">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining deep features for object detection at various scales:finding small birds in landscape images">

                                <b>[17]</b>Takeki A, Trinh T T, Yoshihashi R, et al.Combining deep features for object detection at various scales:Finding small birds in landscape images[J].IPSJ Transactions on Computer Vision and Applications, 2016, 8 (1) :5-13
                            </a>
                        </p>
                        <p id="343">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CNN for small object detection">

                                <b>[18]</b>Chen Chenyi, Liu Mingyu, Tuzel O, et al.R-CNN for small object detection[C]Proc of the 13th Asian Conf on Computer Vision.Berlin:Springer, 2016:214-230
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving small object proposals for company logo detection">

                                <b>[19]</b>Eggert C, Zecha D, Brehm S, et al.Improving small object proposals for company logo detection[C]Proc of the 7th ACM Int Conf on Multimedia Retrieval.New York:ACM, 2017:167-174
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Style Transfer Using Convolutional Neural Networks">

                                <b>[20]</b>Gatys L A, Ecker A S, Bethge M.Image style transfer using convolutional neural networks[C]Proc of the 29th IEEEConf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2016:2414-2423
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding deep image representations by inverting them">

                                <b>[21]</b>Mahendran A, Vedaldi A.Understanding deep image representations by inverting them[C]Proc of the 28th IEEE Conf on Computer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:5188-5196
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What makes for effective detection proposals?">

                                <b>[22]</b>Hosang J, Benenson R, Dollár P, et al.What makes for effective detection proposals[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (4) :814-830
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                Zeiler M D, Fergus R.Visualizing and understanding convolutional networks[C]Proc of the 13th European Conf on Computer Vision.Berlin:Springer, 2014:818-833
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[24]</b>Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[EB/OL]. (2015-04-10) [2017-08-30].https:arxiv.org/abs/1409.1556
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing data using t-SNE">

                                <b>[25]</b>Maaten L, Hinton G.Visualizing data using t-SNE[J].Journal of Machine Learning Research, 2008, 9 (9) :2579-2605
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201902008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201902008&amp;v=Mjg5Mjc0SDlqTXJZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGeTduVzc3Qkx5dlNkTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sR01QWmh5WWp6SmI1Z2pYeEdIOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="3" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
