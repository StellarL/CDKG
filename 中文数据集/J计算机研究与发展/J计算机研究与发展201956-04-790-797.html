

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129043483868750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201904012%26RESULT%3d1%26SIGN%3djpz%252bU%252b6SbE8gxOHjwfK5tnPB1h8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201904012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201904012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201904012&amp;v=MDUzOTBScUZDbmhXNzdCTHl2U2RMRzRIOWpNcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#46" data-title="&lt;b&gt;1 重叠存储优化地球科学软件输出&lt;/b&gt; "><b>1 重叠存储优化地球科学软件输出</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;2 WRF的I/O优化&lt;/b&gt; "><b>2 WRF的I/O优化</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="&lt;b&gt;3 WRF算例测试结果&lt;/b&gt; "><b>3 WRF算例测试结果</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;4 对同类软件的优化&lt;/b&gt; "><b>4 对同类软件的优化</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;4.1 ROMS_AGRIF&lt;/b&gt;"><b>4.1 ROMS_AGRIF</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;4.2 GRAPES&lt;/b&gt;"><b>4.2 GRAPES</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;5 总 结&lt;/b&gt; "><b>5 总 结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 WRF的工作示意图">图1 WRF的工作示意图</a></li>
                                                <li><a href="#71" data-title="图2 输出优化后的WRF性能图">图2 输出优化后的WRF性能图</a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表1 I/O进程数和分布值的影响&lt;/b&gt;"><b>表1 I/O进程数和分布值的影响</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;表2 总进程数不变下不同I/O进程数的表现&lt;/b&gt;"><b>表2 总进程数不变下不同I/O进程数的表现</b></a></li>
                                                <li><a href="#86" data-title="图3 输出优化后的ROMS_AGRIF性能图">图3 输出优化后的ROMS_AGRIF性能图</a></li>
                                                <li><a href="#95" data-title="图4 输出优化后的GRAPES性能图">图4 输出优化后的GRAPES性能图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="119">


                                    <a id="bibliography_1" title="Kaushik K, Kamil S, Williams S, et al.Optimization and performance modeling of stencil computations on modern microprocessors[J].SIAM Review, 2009, 51 (1) :129-159" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST15012900404699&amp;v=Mjg4MzdySzlIdERPcG85RllPc0xDblV3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvY2F4cz1OaWZZZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        Kaushik K, Kamil S, Williams S, et al.Optimization and performance modeling of stencil computations on modern microprocessors[J].SIAM Review, 2009, 51 (1) :129-159
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_2" title="Gibson G A, Vitter J S, Wilkes J, et al.Strategic directions in storage I/O issues in large-scale computing[J].ACMComputing Surveys, 1996, 28 (4) :779-793" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017162&amp;v=MjQ3MTNUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvY2F4cz1OaWZJWTdLN0h0ak5yNDlGWk9vSURYbzdvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        Gibson G A, Vitter J S, Wilkes J, et al.Strategic directions in storage I/O issues in large-scale computing[J].ACMComputing Surveys, 1996, 28 (4) :779-793
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_3" title="Rosario J M D, Bordawekar R, Choudhary A.Improved parallel I/O via a two-phase run-time access strategy[J].ACM SIGARCH Computer Architecture News, 1993, 21 (5) :31-38" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000043802&amp;v=MjY1MThzPU5pZklZN0s3SHRqTnI0OUZaTzhNQkh3N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2TElWb2NheA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        Rosario J M D, Bordawekar R, Choudhary A.Improved parallel I/O via a two-phase run-time access strategy[J].ACM SIGARCH Computer Architecture News, 1993, 21 (5) :31-38
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_4" title="Kotz D.Disk-directed I/O for MIMD multiprocessors[J].ACM Transactions on Computer Systems, 1997, 15 (1) :41-74" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000095913&amp;v=MjkyMzZheHM9TmlmSVk3SzdIdGpOcjQ5RlpPSUtCWDA2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvYw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Kotz D.Disk-directed I/O for MIMD multiprocessors[J].ACM Transactions on Computer Systems, 1997, 15 (1) :41-74
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_5" title="Seamons K E, Chen Ying, Jones P, et al.Server-directed collective I/O in panda[C]//Proc of the 1995ACM/IEEEConf on Supercomputing.New York:ACM, 1995:No.57" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Server-directed collective I/O in Panda">
                                        <b>[5]</b>
                                        Seamons K E, Chen Ying, Jones P, et al.Server-directed collective I/O in panda[C]//Proc of the 1995ACM/IEEEConf on Supercomputing.New York:ACM, 1995:No.57
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_6" title="Lustre.About the Lustre file system[EB/OL]. (2018-04-03) [2018-04-26].http://lustre.org/about/" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=About the Lustre file system">
                                        <b>[6]</b>
                                        Lustre.About the Lustre file system[EB/OL]. (2018-04-03) [2018-04-26].http://lustre.org/about/
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_7" title="Thakur R, Gropp W, Lusk E.Data sieving and collective I/O in ROMIO[C]//Proc of the 7th Symp on the Frontiers of Massively Parallel Computation.Washington, DC:IEEEComputer Society, 1999:182-189" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data sieving and collective I/O in ROMIO">
                                        <b>[7]</b>
                                        Thakur R, Gropp W, Lusk E.Data sieving and collective I/O in ROMIO[C]//Proc of the 7th Symp on the Frontiers of Massively Parallel Computation.Washington, DC:IEEEComputer Society, 1999:182-189
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_8" title="Thakur R, Gropp W, Lusk E.Optimizing noncontiguous accesses in MPI-IO[J].Parallel Computing, 2002, 28 (1) :83-105" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300419975&amp;v=MTgyMjliSzdIdERPckk5RllPb0dCWHM4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvY2F4cz1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Thakur R, Gropp W, Lusk E.Optimizing noncontiguous accesses in MPI-IO[J].Parallel Computing, 2002, 28 (1) :83-105
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_9" title="Zhang Xuechen, Jiang Song, Davis K.Making resonance a common case:A high-performance implementation of collective I/O on parallel file systems[C/OL]//Proc of the2009IEEE Int Symp on Parallel&amp;amp;Distributed Processing.Washington, DC:IEEE Computer Society, 2009[2009-05-23].https://ieeexplore.ieee.org/document/5161070" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making Resonance a Common Case:A High-Performance Implementation of Collective I/O on Parallel File Systems">
                                        <b>[9]</b>
                                        Zhang Xuechen, Jiang Song, Davis K.Making resonance a common case:A high-performance implementation of collective I/O on parallel file systems[C/OL]//Proc of the2009IEEE Int Symp on Parallel&amp;amp;Distributed Processing.Washington, DC:IEEE Computer Society, 2009[2009-05-23].https://ieeexplore.ieee.org/document/5161070
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_10" title="He Shuibing, Wang Yang, Sun Xianhe, et al.Heterogeneity-aware collective I/O for parallel I/O systems with hybrid HDD/SSD servers[J].IEEE Transactions on Computers, 2017, 66 (6) :1091-1098" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Heterogeneity-aware collective I/O for parallel I/O systems with hybrid HDD/SSD servers">
                                        <b>[10]</b>
                                        He Shuibing, Wang Yang, Sun Xianhe, et al.Heterogeneity-aware collective I/O for parallel I/O systems with hybrid HDD/SSD servers[J].IEEE Transactions on Computers, 2017, 66 (6) :1091-1098
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_11" title="Tsujita Y, Mugurnuma H, Yoshinaga K, et al.Improving collective I/O performance using pipelined two-phase I/O[C]//Proc of the 2012Symp on High Performance Computing.San Diego:Society for Computer Simulation International, 2012:No.7" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving collective I/O performance using pipelined two-phase I/O">
                                        <b>[11]</b>
                                        Tsujita Y, Mugurnuma H, Yoshinaga K, et al.Improving collective I/O performance using pipelined two-phase I/O[C]//Proc of the 2012Symp on High Performance Computing.San Diego:Society for Computer Simulation International, 2012:No.7
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_12" title="Sehrish S, Son S W, Liao Weikeng, et al.Improving collective I/O performance by pipelining request aggregation and file access[C]//Proc of the 20th European MPI Users’Group Meeting.New York:ACM, 2013:37-42" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving collective I/O performance by pipelining request aggregation and file access">
                                        <b>[12]</b>
                                        Sehrish S, Son S W, Liao Weikeng, et al.Improving collective I/O performance by pipelining request aggregation and file access[C]//Proc of the 20th European MPI Users’Group Meeting.New York:ACM, 2013:37-42
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_13" title="Denis A, Aumage O, Namyst R.Improving reactivity and communication overlap in MPI using ageneric I/O manager[C]//Proc of European Conf on Recent Advances in Parallel Virtual Machine&amp;amp;Message Passing Interface.Berlin:Springer, 2007:170-177" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving reactivity and communication overlap in MPI using ageneric I/O manager">
                                        <b>[13]</b>
                                        Denis A, Aumage O, Namyst R.Improving reactivity and communication overlap in MPI using ageneric I/O manager[C]//Proc of European Conf on Recent Advances in Parallel Virtual Machine&amp;amp;Message Passing Interface.Berlin:Springer, 2007:170-177
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_14" title="Patrick C M, Son S W, Kandemir M.Comparative evaluation of overlap strategies with study of I/O overlap in MPI-IO[J].Operating System Review, 2008, 42 (6) :43-49" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000055327&amp;v=MTE1NDFlWnVIeWptVUx2TElWb2NheHM9TmlmSVk3SzdIdGpOcjQ5RlpPNEtEMzQrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        Patrick C M, Son S W, Kandemir M.Comparative evaluation of overlap strategies with study of I/O overlap in MPI-IO[J].Operating System Review, 2008, 42 (6) :43-49
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_15" title="Patrick C M, Son S W, Kandemir M.Enhancing the performance of MPI-IO applications by overlapping I/O, computation and communication[C]//Proc of the 13th ACMSIGPLAN Symp on Principles&amp;amp;Practice of Parallel Programming.New York:ACM, 2008:277-278" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancing the performance of MPI-IO applications by overlapping I/O computation and communication">
                                        <b>[15]</b>
                                        Patrick C M, Son S W, Kandemir M.Enhancing the performance of MPI-IO applications by overlapping I/O, computation and communication[C]//Proc of the 13th ACMSIGPLAN Symp on Principles&amp;amp;Practice of Parallel Programming.New York:ACM, 2008:277-278
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_16" title="Yang Lipeng, Che Yonggang.HDF5 based parallel I/Otechniques for multi-zone structured grids CFD applications[J].Journal of Computer Research and Development, 2015, 52 (4) :861-868 (in Chinese) (杨丽鹏, 车永刚.基于HDF5实现多区结构网格CFD程序的并行I/O[J].计算机研究与发展, 2015, 52 (4) :861-868) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201504012&amp;v=MTcwMjRSTE9lWmVScUZDbmhXNzdCTHl2U2RMRzRIOVRNcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        Yang Lipeng, Che Yonggang.HDF5 based parallel I/Otechniques for multi-zone structured grids CFD applications[J].Journal of Computer Research and Development, 2015, 52 (4) :861-868 (in Chinese) (杨丽鹏, 车永刚.基于HDF5实现多区结构网格CFD程序的并行I/O[J].计算机研究与发展, 2015, 52 (4) :861-868) 
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_17" title="Acharya A, Bennett R, Beynon M, et al.Tuning the performance of I/O intensive parallel applications[C]//Proc of Workshop on Input/Output in Parallel&amp;amp;Distributed System.New York:ACM, 1996:15-27" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tuning the performance of I/O intensive parallel applications">
                                        <b>[17]</b>
                                        Acharya A, Bennett R, Beynon M, et al.Tuning the performance of I/O intensive parallel applications[C]//Proc of Workshop on Input/Output in Parallel&amp;amp;Distributed System.New York:ACM, 1996:15-27
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_18" title="Ma Xiaosong, Jiao Xiangmin, Campbell M, et al.Flexible and efficient parallel I/O for large-scale multi-component simulations[C]//Proc of the 17th Int Parallel&amp;amp;Distributed Processing Symp.Washington, DC:IEEE Computer Society, 2003:255a" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Flexible and efficient parallel I/O for large-scale multi-component simulations">
                                        <b>[18]</b>
                                        Ma Xiaosong, Jiao Xiangmin, Campbell M, et al.Flexible and efficient parallel I/O for large-scale multi-component simulations[C]//Proc of the 17th Int Parallel&amp;amp;Distributed Processing Symp.Washington, DC:IEEE Computer Society, 2003:255a
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(04),790-797 DOI:10.7544/issn1000-1239.2019.20170906            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>地球科学大规模并行应用的重叠存储优化</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%92%9F%E9%94%9F&amp;code=27543150&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈璟锟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%9C%E4%BA%91%E9%A3%9E&amp;code=37857073&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杜云飞</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E5%AE%B6%E8%B6%85%E7%BA%A7%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%B7%9E%E4%B8%AD%E5%BF%83(%E4%B8%AD%E5%B1%B1%E5%A4%A7%E5%AD%A6)&amp;code=0140250&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国家超级计算广州中心(中山大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>存储是地球科学类软件的重要组成部分, 周期性输出中间态和检查点会带来大量的访存操作, 不恰当的访存设计会严重影响软件在大规模计算时的性能表现.针对地球科学类软件的存储问题, 从软件层面提出一个重叠存储优化方法, 通过设置额外的I/O进程隐藏输出过程.该重叠存储优化主要有3个优势:1) 将输出和计算操作重叠在一起, 实现了输出的重叠化和隐藏化;2) 抑制了收集通信的开销, 突破了收集操作的通信带宽瓶颈和内存限制;3) 能容易地使用各种高级并行输出库函数.利用重叠存储优化了天河二号上的WRF, ROMS＿AGRIF, GRAPES, 并完成了性能测试.结果表明:经过存储优化后, 程序的峰值性能都获得了显著的提升.还讨论了在固定总进程数下, 计算进程和I/O进程数的最佳比例是多少.优化后的程序与原版相比, 模式专家只需要在配置文件额外设置2个新变量即可使用, 十分易于学习.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%87%8D%E5%8F%A0%E5%AD%98%E5%82%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重叠存储;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9AI%2FO%E8%BF%9B%E7%A8%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多I/O进程;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%9B%E7%A8%8B%E5%88%86%E5%B8%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">进程分布;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%B0%E7%90%83%E7%A7%91%E5%AD%A6%E6%A8%A1%E6%8B%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">地球科学模拟;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E8%A1%8C%E5%BA%94%E7%94%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大规模并行应用;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *杜云飞 (yunfei.du@nscc-gz.cn) ;
                                </span>
                                <span>
                                    陈璟锟, jingkun.chen@nscc-gz.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-11-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2016YFB0201401);</span>
                    </p>
            </div>
                    <h1><b>An Overlap Store Optimization for Large-Scale Parallel Earth Science Application</b></h1>
                    <h2>
                    <span>Chen Jingkun</span>
                    <span>Du Yunfei</span>
            </h2>
                    <h2>
                    <span>National Supercomputer Center in Guangzhou (Sun Yat-sen University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Weather forecast, atmosphere or ocean simulations have much output data during the iterative computation for the intermediate status or check point. However, an unreasonable output design limits the performance of the earth science application in large-scale parallel computation. In this paper, we propose an overlap store optimization to solve this problem. The key issue of this overlap store optimization is setting some I/O processes to hide the I/O cost. This optimization has three main advantages: first, we hide the I/O operation through the overlap of output and computing; second, we limit the cost of gather operation, break though the bottleneck of gather communication bandwidth and memory size; third, the I/O process is flexible to use different high-performance parallel I/O API. We use this method to optimize WRF, ROMS＿AGRIF and GRAPES in Tianhe II super computer, and test their performance after the optimization. The result of the tests shows that we obtain about 30% to 900% improvement in the peak. We also discuss the best proportion of computer process and I/O process when the total number of processes is fixed. The optimized version is very easy to used, and the only cost is the scientists need to setup two more variables in the namelist.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=overlap%20store&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">overlap store;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multiply%20I%2FO%20processes&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multiply I/O processes;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=processes%20distribution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">processes distribution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=earth%20science%20simulation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">earth science simulation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=large-scale%20parallel%20application&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">large-scale parallel application;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Chen Jingkun, born in 1986.PhD and associate  professor  in  Sun  Yat-sen University.Member of CCF.His main research interests include parallel I/O and heterogeneous system.<image id="170" type="" href="images/JFYZ201904012_17000.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Du Yunfei, born in 1980.PhD and professor in Sun Yat-sen University.Senior member of CCF.His main research interests include the system of supercomputer, parallel computing and heterogeneous system.<image id="172" type="" href="images/JFYZ201904012_17200.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-11-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Key Research and Development Program of China (2016YFB0201401);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="41">对提升大规模并行计算规模与效率的研究和优化通常集中在算法的设计和优化上, 比如在地球科学类程序中, 通过特别的网格划分以更好地实现负载均衡和压缩通信的研究仍然长盛不衰<citation id="155" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.然而各种算法的加速比测试往往略去了I/O的占用时间, 比如设置了超过计算时间的输出间隔、关闭结尾输出等.但在实际应用中, 研究者通常需要获得一个可以描述连续变化的过程而非一个最终状态结果.这就要求模式在迭代计算过程中定期输出中间状态、输入边界数据进行同化以提高结果准确性和输出数个检查点以应对长时间运行可能碰到的硬件故障.所以, 地球科学类软件很早就被意识到是一种I/O敏感型的应用<citation id="156" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="42">地球科学研究者往往缺乏足够的时间和意愿去系统地学习与研究大规模并行程序的编写, 他们更愿意把主要精力放在对科学问题的研究上.因此地球科学模拟软件最常见的输出方式是简单地将信息收集到根进程, 由根进程整理并完成串行输出.然而这种输出方式会极大地限制程序在大规模计算时的性能表现.因为I/O时间不会随着进程数的增加而下降, 所以I/O时间占总时间的比例会越来越高, 最终严重地削弱了峰值性能.在一些算例中, 甚至会出现I/O时间远超计算时间的情况.这除了让科研工作者浪费时间和金钱外, 更大的负面影响是在精细化数值预报领域, 不但降低了数值预报的时效性, 还增加了对硬件资源的需求并且提高了维护工作的难度.</p>
                </div>
                <div class="p1">
                    <p id="43">在过去的数10年中, 计算机科学家为了克服地球物理科学模拟的存储墙问题进行多种研究, 最终形成的主流共识是:1) 由并行I/O取代串行I/O;2) 将I/O和计算重叠<citation id="157" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>;3) 优化必须是通用且轻量级的.因为计算进程的数据保存顺序和文件存储顺序不同, 所以需要建立一个缓存机制将每个进程的数据进行重新排列后再做并行输出, 由此诞生了收集式I/O的思路<citation id="158" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.收集式I/O的具体实现还能细分为采用本地硬盘作为缓存<citation id="159" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和设立I/O结点作为缓存2种<citation id="160" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.这些思路还继续发展并影响到新一代超级计算机的并行文件系统设计<citation id="161" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>.为了让收集式I/O发挥出这种并行文件系统的性能, 计算科学家又在并行读写 (MPI-IO) 的基础上根据I/O结点的具体硬件架构进一步开发了更高级的并行I/O库<citation id="163" type="reference"><link href="131" rel="bibliography" /><link href="133" rel="bibliography" /><link href="135" rel="bibliography" /><link href="137" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>, 比如轻便的收集式并行I/O库ROMIO<citation id="162" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.因为收集式I/O的全过程可再分为通信收集和并行读写2部分, 所以后续又有许多工作分别针对收集过程<citation id="164" type="reference"><link href="139" rel="bibliography" /><link href="141" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>和如何隐藏读写过程<citation id="165" type="reference"><link href="143" rel="bibliography" /><link href="145" rel="bibliography" /><link href="147" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="44">上述研究基本围绕着底层硬件和基础库, 这给程序的移植带来了方便.但在实际的应用当中, 地球科学软件往往会采用以常用科学数据格式存储数据, 比如NetCDF (network common data form) , HDF5 (hierarchical data format 5) 等.这些文件格式库的数据收集性能在大规模计算下是无法保证的, 而且缺乏异步输出的能力.这使得存储墙对现今的地球科学大规模计算的负面影响依然非常严重, 极端条件下甚至会出现超过80%的墙钟时间是在输出的情况.虽然有相当多工作在努力地提升并行I/O的读写性能<citation id="166" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 但单从底层对I/O进行优化依然不足以克服存储墙问题, 我们还需要对应用软件的访存行为做出优化.</p>
                </div>
                <div class="p1">
                    <p id="45">本文就是从应用层出发, 参考主-从架构<citation id="167" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 通过设立I/O进程<citation id="168" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、分离I/O过程和计算过程以实现对地球科学软件的访存优化.本文将按照3方面组织:1) 如何从应用层面设计一个重叠存储以适应在天河二号上进行大规模计算;2) 将详细阐述这种优化是如何提升WRF (weather research &amp; forecasting mode) 的I/O性能, 并证明其有效性;3) 将这种优化移植到ROMS_AGRIF (regional ocean modeling system_adaptative grid refinement in Fortran) 和GRAPES (global/regional assimilation and PrEdiction system) , 以证明它的普遍性.</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag"><b>1 重叠存储优化地球科学软件输出</b></h3>
                <div class="p1">
                    <p id="47">因为地球科学的性质, 其模拟过程往往是首先读入初始条件, 然后进入迭代循环, 每当抵达设定好的输出时间时将进行一次中间状态输出, 直到循环到达终止时间.这意味着输出的开销会远大于输入, 因此, 我们先将精力集中在输出优化上.</p>
                </div>
                <div class="p1">
                    <p id="48">现在我们面临的情况是输出所花费的时间是一步迭代的数倍甚至数10倍, 但一般也不会超过一个输出周期.因此, 我们的想法是将输出过程和迭代计算分离, 并确保一个输出间隔内完成输出, 就是让输出隐藏在计算中, 实现输出优化的目标.这样我们就不需要去追求并行输出的极限速度, 优化难度会减轻许多.</p>
                </div>
                <div class="p1">
                    <p id="49">为了达成这个目标, 我们进一步细分整个思路:</p>
                </div>
                <div class="p1">
                    <p id="50">1) 建立专用的I/O进程, 以实现输出和计算同时进行.</p>
                </div>
                <div class="p1">
                    <p id="51">2) I/O进程通过收集通信gather收集数据.</p>
                </div>
                <div class="p1">
                    <p id="52">3) 因为gather是堵塞型通信, 所以会暴露通信.</p>
                </div>
                <div class="p1">
                    <p id="53">4) 根据最小生成树算法 (minimum-spanning tree algorithm, MST) , 需要建立多个I/O进程以减少gather通信的消耗.</p>
                </div>
                <div class="p1">
                    <p id="54">5) 多个I/O进程之间使用并行I/O完成输出.</p>
                </div>
                <div class="p1">
                    <p id="55">6) 由思路4, 5可知, 多个I/O进程和计算进程如何分组要符合连续读写的原则.</p>
                </div>
                <div class="p1">
                    <p id="56">7) I/O进程应该分布在不同物理结点.</p>
                </div>
                <div class="p1">
                    <p id="57">8) 通过划分新通信域从而实现不同功能进程的混合分组.</p>
                </div>
                <div class="p1">
                    <p id="58">思路1是实现输出和计算重叠的基础, 虽然MPI-IO提供有非堵塞读写 (MPI_FILE_iread (iwrite) _at) 等接口, 但NetCDF和HDF5并没有使用.我们采用的gather通信方式会暴露通信时间, 但带来的好处是减轻结点内存的压力以及减少程序编写的复杂性.又因为gather通信是MST算法, 耗时正比于进程数的数量, 所以需要设置多个I/O进程, 每个I/O进程只负责收集部分计算进程的数据, 以将通信开销抑制到可以接受的程度.思路5是我们要让每次输出耗时一定要小于一个输出间隔.虽然从理论上无法完备地证明任何情况下并行输出就能满足输出耗时小于输出间隔, 但在实际的地球物理模拟中并不需要每一步迭代完毕后都要输出.在绝大部分情况下输出间隔的墙钟时间的量级都在10～100 s, 而相应的输出量在1～20 GB.所以我们在此选择并行输出, 既可以满足现实应用中的绝大部分情况而且还留有相当富余的性能空间以应对未来的性能需求.为了发挥并行输出的性能, 就产生了思路6的要求.思路7是为了突破单结点的物理内存限制和通信带宽限制.思路8是为了方便管理不同种类的进程和减少对核心算法的修改, 只需要将计算进程归入一个全新的通信域, 并用其代替核心算法中原来的全局通信域 (MPI_COMM_WORLD) 就可完成修改.</p>
                </div>
                <div class="p1">
                    <p id="59">重叠存储优化的具体操作步骤有7个:1) 读取配置文件里面关于I/O进程的个数和分布值;2) 从总进程数算得计算进程的数量并完成负载均衡的划分;3) 根据负载均衡的情况、结果文件的数据存储顺序要求、计算进程的数目和分布值, 确定计算进程和I/O进程在CPU核上的实际分布;4) 两类进程各自组成新的通信域1用于同类进程内的通信, 同时计算进程组成的通信域1还用于替换掉原核心算法中的全局通信域;5) 将计算进程和I/O进程划分到若干个通信域2内, 每个通信域2中都包含若干个计算进程和一个I/O进程, 划分原则为尽可能保证I/O进程收集并整理后的数据是连续的;6) I/O进程必须先完成不同文件的数据收集, 然后再安排输出, 并形成接收—输出循环直到结束;7) 并行输出方式可以使用多种接口, 例如NetCDF, PNetCDF (parallel NetCDF) , MPI-IO等方式, 视乎具体需求.</p>
                </div>
                <div class="p1">
                    <p id="60">从上述的修改流程中可以看到这种优化方法有4种好处:1) 相当于在原来的程序上外接了一个功能包, 不涉及修改核心算法的逻辑;2) 对核心算法的修改只有通信域的名字, 这使得我们可以在不明白模型和算法细节的情况下也能完成重叠存储优化, 这也是优化方法具有通用性的一个表现;3) 在通信域划分时已经考虑了后续并行输出的速度问题, 减少磁盘读写竞争问题;4) 更加充分地利用了超级计算机的硬件, 突破单结点带来的网络带宽和内存瓶颈.</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>2 WRF的I/O优化</b></h3>
                <div class="p1">
                    <p id="62">WRF是目前国内与国际上最流行的中尺度天气预报研究软件之一, 是典型的地球科学类软件.WRF从3.0版本就开始考虑大规模计算下的输出问题, 它同样通过提供设置额外I/O进程的方式实现优化.假设我们将I/O进程设为2, 并为作业提供总计12个进程, 而运行作业的超级计算机每个结点具有6个CPU核.那设置I/O进程前后的WRF进程分配如图1 (a) 和图1 (b) , 反映到具体的物理CPU核划分如图1 (c) .</p>
                </div>
                <div class="p1">
                    <p id="63">图1 (a) (b) (d) 中, 每一列代表一个进程, 圆形 (黑色) 箭头代表迭代计算过程, 菱形 (绿色) 箭头代表通信, 长方形 (红色) 代表输出过程, 虚线代表计算进程空闲;图1 (c) (e) 中, 深色 (蓝色) 方块代表一个计算进程, 浅色 (黄色) 阴影方块代表一个I/O进程, 褐色线条围成的区域代表通信域的划分.图1 (a) 没有I/O进程, 由根进程负责输出2个结果文件;图1 (b) 设置了2个I/O进程, 但只有第2个文件的输出与计算实现重叠;图1 (c) 不同类型进程在12个CPU核上的分布和数据收集通信域的划分;图1 (d) 优化后的输出流程;图1 (e) 优化后不同类型进程在12个CPU核上的分布和新的数据收集通信域划分.</p>
                </div>
                <div class="p1">
                    <p id="64">图1 (a) 展示了在没有I/O进程时, 根进程负责完成数据的收集并输出结果, 在整个收集-输出的过程, 其余进程除了发送数据的操作外都处于闲置状态.图1 (b) 表示设置了2个I/O进程后, 每当进入预设的输出时间点, 计算进程都会将数据发送给I/O进程, 随后由I/O进程完成数据整理和并行输出.每个I/O进程都只会接收到处于同一个通信域的计算进程的数据, 数据收集通信域的划分如图1 (c) .对比图1 (a) 和图1 (b) 我们就能发现原设计的第1个问题.虽然在设置了通信进程后, 最后1个文件的输出时间和计算时间发生了重叠, 对输出时间占比有所改善, 但除最后1个文件外, 其他文件的输出时间仍然暴露在外.WRF的文件输出顺序是第1层网格—第2层网格—第3层网格.通常研究者会将第3层网格设置得最为密集, 其结果文件也是最大, 输出耗时最长.这使得原版的设计在部分情况下已经取得了比较明显的效果.但该设计却不适合2种情况:情况1是不同层网格的格点数相差不大, 只是格点间代表的距离不同;情况2是研究者设置了辅助变量输出, 输出顺序变成先输出1层网格的数据, 再收集该层的辅助变量数据并另开1个新文件保存.情况2会使得只有最后1层的辅助变量文件成功被隐藏, 而一般辅助数据文件都远小于主要数据, 是否隐藏意义不大.原设计的另1个问题是所有的I/O进程都安排在最后, 如图1 (c) 所示, 这往往意味着I/O进程会放在同一个结点.虽然放在同一个结点对于收集式I/O是一种理想情况, 但却也有通信和内存这2个瓶颈.</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904012_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 WRF的工作示意图" src="Detail/GetImg?filename=images/JFYZ201904012_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 WRF的工作示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904012_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 Cartoon of WRF’s workflow</p>

                </div>
                <div class="p1">
                    <p id="66">针对这2个问题, 我们首先选择调整I/O进程的工作顺序.每进入1次新的收集-输出时间点, I/O进程先全部接收本次输出的所有数据, 然后才开始逐一整理并输出到硬盘.如此调整后, 除最后1次输出外, 其余时间点的输出将全部实现隐藏, 只剩余收集通信暴露在外, 如图1 (d) 所示.</p>
                </div>
                <div class="p1">
                    <p id="67">为了抑制收集通信gather的耗时, 根据MST算法的特点, 我们一是可以增加更多的I/O进程以减少每个I/O进程负责的计算进程数, 二是可以将I/O进程分布在各个不同的结点以变相提高通信带宽.为此, 我们引入1个可由使用者设定的分布值, 可主动控制I/O进程的分布程度.如果设置为1, 则如同原设计一般让所有I/O进程排列在最后;如果设置为2, 则意味着12个进程被分为2组, 每组最后的2/2=1个进程为I/O进程, 效果如图1 (e) , 成功利用了2个结点的通信带宽.</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag"><b>3 WRF算例测试结果</b></h3>
                <div class="p1">
                    <p id="69">我们使用超级计算机天河二号作为性能测试平台, 它具有17 920个计算结点, 每个结点的配置为2路24 CPU核, 单核频率2.2 GHz, 4路64 GB内存, 自主研发的高速互联具有点点带宽140 Gbps, 采用层次式Lustre文件系统, 在存储服务器 (OSS) 和计算结点间还有1层配备了固态硬盘 (SSD) 的I/O结点用于加速存储性能.</p>
                </div>
                <div class="p1">
                    <p id="70">我们选用的WRF测试算例为模拟全中国2015-07-27T12:00:00—2015-07-30T12:00:00共72小时的天气变化, 每90 s为1个模拟步.3层嵌套网格:220×180×45, 481×391×45, 1 216×895×45, 每层网格的相邻格点间距分别为45 000 m, 15 000 m, 5 000 m.每层网格的输出间隔均为1 h, 采用PNetCDF库并行输出NetCDF格式文件, 没有辅助变量输出, 条带数设置为4.我们使用的WRF版本为3.6.1, 但其关于输出的代码与3.7.1相同, 与3.8.1和3.9相似.我们的编译器为Intel-14, WRF模式选择em_real, 只允许根进程输出日志信息.</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904012_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 输出优化后的WRF性能图" src="Detail/GetImg?filename=images/JFYZ201904012_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 输出优化后的WRF性能图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904012_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 Performance of WRF after output optimization</p>

                </div>
                <div class="p1">
                    <p id="72">我们选择3种设定以对比优化效果, 分别是:原版无I/O进程、原版带2个I/O进程、优化版带2个I/O进程并且分布值为2.第1种设定的输出方式为NetCDF, 而后两者均为PNetCDF.我们的测试结果如图2所示.每一组柱状图从左起按顺序分别代表第1, 2, 3种设定方案.横坐标代表计算进程数, 所以每一组柱状图的第2和第3条使用的总进程数实为计算进程数加2.从图2可以看到随着进程数的增加, 输出占总时间的比会越来越高, 即使在超过了性能峰值后仍具有这种情况.原版程序在设置了I/O进程后, 在3072进程时的性能提升了约19.8%, 但还是能明显看到输出时间.在对输出做了优化后, 峰值性能再次获得约11.4%的提升, 相比无I/O进程时则是提升了约33.5%.这证明了我们的输出优化是有效且可行, 达到了预想的效果.</p>
                </div>
                <div class="p1">
                    <p id="73">我们还能从图2看到, 随着进程数的增加, 暴露的收集通信时间也在增长, 尤其是从进程3 072提升到6 144时最为明显.我们进一步测试在不同I/O进程数和不同分布值下收集通信时间的变化情况, 并将结果整理成表1.</p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表1 I/O进程数和分布值的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Effect of Output Tasks and Splits</b></p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td><br />Computation<br />Tasks</td><td>I/O<br />Tasks</td><td>Distribution<br />Value</td><td>Communication<br />Time/s</td></tr><tr><td><br />6 144</td><td>2</td><td>1</td><td>903</td></tr><tr><td><br />6 144</td><td>2</td><td>2</td><td>866 </td></tr><tr><td><br />6 144</td><td>10</td><td>1</td><td>64</td></tr><tr><td><br />6 144</td><td>10</td><td>2</td><td>39</td></tr><tr><td><br />6 144</td><td>10</td><td>5</td><td>41</td></tr><tr><td><br />6 144</td><td>10</td><td>10</td><td>39</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="75">从表1我们可以看到, 在相同分布值下10个I/O进程的通信开销远低于2个, 主要原因就是gather通信开销和进程数直接相关.增加I/O进程数意味着每个I/O进程负担的计算进程数变少, 自然gather通信开销也会相应地减小.在同I/O进程数不同分布值下, 通信开销也会有一定的下降, 这表明我们的修改确实更有效地利用了带宽.但因为本例子的通信量并不大, 所以还是进程数数量导致的开销占据主导.</p>
                </div>
                <div class="p1">
                    <p id="76">我们为完成图1和表1的测试, 选择了在计算进程保持一致的基础上设置额外I/O进程.这样可以保证计算的稳定性, 但也带来总进程数不一致的问题.由此, 我们还可以引出另外1个问题:总进程数一致的情况下, 如何分配I/O进程和计算进程是最优解?</p>
                </div>
                <div class="p1">
                    <p id="77">在讨论这个问题前, 我们有必要先解释为什么会选择固定计算进程数的做法.这是因为WRF的负载均衡策略是将计算进程数分解为2个尽可能相近的因数, 并利用它们完成<i>xy</i>平面的划分.但如果这2个因数相差较大时, 这个简单的负载均衡策略就会极容易引起程序崩溃.假设现在从6 144个进程数中划分出10个I/O进程, 那6 134只能被分解为2×3 067, 进而导致作业崩溃.所以当总进程数一定时, I/O进程数的设置还必须考虑负载均衡策略, 不能只考虑通信开销和计算时间之间的竞争.在此基础上, 我们做出表2.</p>
                </div>
                <div class="p1">
                    <p id="78">从表2可得, 计算与I/O进程的数量比在1 000∶1至500∶1间能取得最优效果.我们认为这是天河二号硬件性能参数下的特解, 不同系统应该会有不同的结果.</p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表2 总进程数不变下不同I/O进程数的表现</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Performance of Different I/O Processes Numbers Under Constant Total Processes Number</b></p>
                    <p class="img_note"></p>
                    <table id="79" border="1"><tr><td><br />Computation<br />Tasks</td><td>I/O<br />Tasks</td><td>Computation<br />Time/s</td><td>Communication<br />Time/s</td></tr><tr><td>6 141</td><td>3</td><td>12 067.2</td><td>346.46</td></tr><tr><td><br />6 138</td><td>6</td><td>12 038.4</td><td>90.57</td></tr><tr><td><br />6 132</td><td>12</td><td>12 161.1</td><td>40.37</td></tr><tr><td><br />6 120</td><td>24</td><td>12 240</td><td>27.28</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>4 对同类软件的优化</b></h3>
                <h4 class="anchor-tag" id="81" name="81"><b>4.1 ROMS_AGRIF</b></h4>
                <div class="p1">
                    <p id="82">ROMS是国际上知名的海洋模拟软件, 它具有多个分支.其中的1个分支ROMS_AGRIF由法国研究小组开发, 加入了多个生物化学模块, 侧重于模拟研究海洋与生物之间的关系.因为开发者认为应该尽可能地避开通信引起的消耗, 所以其输出过程被设计为每个进程按顺序逐一将其保有的数据串行输出到同一个NetCDF文件.</p>
                </div>
                <div class="p1">
                    <p id="83">这种设计虽然避开了输出导致的通信消耗, 但在大规模计算下多个进程逐一打开关闭同一个文件, 逐一输出不连续数据所耗费的时间也同样可观, 在现代超级计算机硬件性能下不再是1个合理的交换.按第1节所述的方式, 我们优化了ROMS_AGRIF的输出设计.收集过程分为3步:1) 通过gather将各计算进程需要传输的数据量大小传送给I/O进程;2) I/O进程通过接收到的信息计算出接收内存的大小;3) 用gather完成该变量的数据收集.等所有变量的数据都收集完毕后, I/O进程按顺序整理并采用并行库PNetCDF完成并行输出.</p>
                </div>
                <div class="p1">
                    <p id="84">我们选择的算例为南中国海单层网格, 网格大小为1 200×1 310×60, 模拟129 600步, 每720步输出3个文件, 每个文件的每次输出量约在1.5 GB, 条带数设置为4.</p>
                </div>
                <div class="p1">
                    <p id="85">测试结果如图3所示, 我们在前48, 96, 192个计算进程时仅比较了原版和存在2个I/O进程时的情况, 从384计算进程开始加入对比24个I/O进程的情况.在48和96计算进程时, 2个额外I/O进程不仅抑制了输出时间, 连带计算速度也出现了加快的情况.这是因为ROMS对内存很敏感, 在加入2个I/O进程后, 我们必须提交3个结点才能满足所需的CPU核数.依照天河二号的作业管理系统设定, 这种提交方式会导致50个进程被3个结点平分.这意味着减少了内存带宽的竞争, 因此出现计算速度加快的现象.这种对内存敏感的情况也体现在384进程前获得了超线性加速比.</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904012_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 输出优化后的ROMS_AGRIF性能图" src="Detail/GetImg?filename=images/JFYZ201904012_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 输出优化后的ROMS_AGRIF性能图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904012_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 3 Performance of ROMS_AGRIF after output  optimization</p>

                </div>
                <div class="p1">
                    <p id="87">图3还展示了与计算规模相匹配的I/O进程有利于控制收集通信的开销.在3 072进程时, 24个I/O进程可以有效保证输出所占时间不到总时间的5%, 充分释放出原作者对通信优化后的计算性能.</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>4.2 GRAPES</b></h4>
                <div class="p1">
                    <p id="89">GRAPES是我国自主开发的天气预报程序, 在天河二号上已经实现1 km模式的业务化运行, 是目前国内网络精度的最高水平.1 km模式需要每12 min启动1次.经过存储优化后的1 km模式, 墙钟时间从26 min缩短至9 min, 减少了对硬件资源的需求, 减轻了运维压力, 也提升了1 km预报业务的稳定性.</p>
                </div>
                <div class="p1">
                    <p id="90">鉴于GRAPES的版本众多, 我们以广州热带所提供的GRAPES业务版本为例开展GRAPES的I/O优化.原版GRAPES的I/O过程为:所有进程通过发送命令MPI_send将数据发送到根进程, 而根进程先使用多个接收命令MPI_recv完成通信后开始进行数据整理并串行输出Fortran非格式化文件.</p>
                </div>
                <div class="p1">
                    <p id="91">在实际测试中发现, GRAPES的输出所占时间比会随着进程数量的增加而急速上升.这是因为频繁的启动发送和接收命令消耗了大量的时间.我们对GRAPES的收集通信优化手段与ROMS_AGRIF类似, 避免了因为频繁启动通信命令而引起的巨额开销.</p>
                </div>
                <div class="p1">
                    <p id="92">GRAPES的输出文件采用Fortran非格式化文件格式, 与MPI-IO的格式并不完全一致.为了方便后处理程序, 我们编写的并行输出过程也仿照了该格式, 虽然会对读写速度带来负面影响, 但输出耗时依然远小于1个输出周期, 不会对总时间带来显著影响.</p>
                </div>
                <div class="p1">
                    <p id="93">从负载均衡的考虑出发, 几乎所有地球科学模拟软件的负载均衡策略都是按照进程数将<i>xy</i>平面划分为大小相近的区域, 而<i>z</i>方向不做划分, GRAPES也是如此.因为算法上的要求, GRAPES的变量数组存储按照的是<i>xzy</i>顺序, 这意味着当存在多个I/O进程时, I/O进程只能在<i>x</i>或<i>y</i>方向中选取1个保证该方向的数据连续性.因为GRAPES结果文件内的数据是按照<i>xyz</i>顺序排列, 所以在多个I/O进程并行输出同一个变量的同一个<i>xy</i>平面时必然会引起磁盘资源的争抢, 最终拖慢输出表现.考虑到在日常的业务化使用当中, GRAPES的每次输出量均不超过1 GB的情况, 我们暂时将所有的I/O进程放在同一个结点内, 这样做可以直接利用天河二号的层次式存储加速输出速度, 避开磁盘资源发生争抢的情况.我们会在将来采取更成熟的收集式I/O, 比如ROMIO, 以尝试解决这个问题.</p>
                </div>
                <div class="p1">
                    <p id="94">对GRAPES的测试结果如图4所示, 可见在设置了24个I/O进程时, 在3072进程时依然取得了相当好的加速效果.在实际的业务预报应用中规模已经达到了5300核, 而其I/O时间依然可以忽略不计.</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904012_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 输出优化后的GRAPES性能图" src="Detail/GetImg?filename=images/JFYZ201904012_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 输出优化后的GRAPES性能图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904012_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 Performance of GRAPES after output  optimization</p>

                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>5 总 结</b></h3>
                <div class="p1">
                    <p id="97">因为硬件上的原因, 存储的速度远落后于计算的速度, 这使得存储问题一直是大规模算法能否实用化的主要挑战之一.地球科学领域内常用的软件几乎都会碰到这个问题, 并且随着规模的扩大, 这个问题会越发严重.存储的解决方式和硬件架构有关, 因此我们针对天河二号的架构, 设计了一套应用级别的重叠存储优化, 希望解决大规模计算时的存储问题.</p>
                </div>
                <div class="p1">
                    <p id="98">这套重叠存储优化的核心思路是:</p>
                </div>
                <div class="p1">
                    <p id="99">1) 建立专用I/O进程;</p>
                </div>
                <div class="p1">
                    <p id="100">2) 设置多个I/O进程以抑制通信开销, 变相增加内存容量和通信带宽;</p>
                </div>
                <div class="p1">
                    <p id="101">3) I/O进程收集完所有文件的数据后再输出;</p>
                </div>
                <div class="p1">
                    <p id="102">4) I/O进程之间采用并行输出.</p>
                </div>
                <div class="p1">
                    <p id="103">虽然部分应用软件, 比如WRF已经有了这方面的探索, 但要么不完善, 要么不适应天河二号的存储架构, 所以都无法发挥出天河二号的存储性能.</p>
                </div>
                <div class="p1">
                    <p id="104">本文对WRF, ROMS_AGRIF, GRAPES做出适合天河二号架构的存储优化, 均获得了预期中的良好效果.测试表明:经过优化后的WRF峰值性能相比原版有了近11.4%的提升, ROMS_AGRIF的峰值性能提升936%, GRAPES的峰值性能提升140%.因为都不涉及物理模型, 优化后的计算结果在有效精度内与原版程序保持一致, 这验证了重叠存储优化的正确性和有效性, 也证明了该优化思路能普遍应用于天河二号上的地球科学类软件.我们对存储的优化也进一步提升了天河二号在华南地区高精度天气预报业务和南中国海预报业务中的作用.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="119">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST15012900404699&amp;v=MDMwODVCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2TElWb2NheHM9TmlmWWVySzlIdERPcG85RllPc0xDblV3bw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>Kaushik K, Kamil S, Williams S, et al.Optimization and performance modeling of stencil computations on modern microprocessors[J].SIAM Review, 2009, 51 (1) :129-159
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017162&amp;v=MTM2MDhIdGpOcjQ5RlpPb0lEWG83b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvY2F4cz1OaWZJWTdLNw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>Gibson G A, Vitter J S, Wilkes J, et al.Strategic directions in storage I/O issues in large-scale computing[J].ACMComputing Surveys, 1996, 28 (4) :779-793
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000043802&amp;v=MzI1MTJIdGpOcjQ5RlpPOE1CSHc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvY2F4cz1OaWZJWTdLNw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>Rosario J M D, Bordawekar R, Choudhary A.Improved parallel I/O via a two-phase run-time access strategy[J].ACM SIGARCH Computer Architecture News, 1993, 21 (5) :31-38
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000095913&amp;v=MDM3NjFYMDZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMdkxJVm9jYXhzPU5pZklZN0s3SHRqTnI0OUZaT0lLQg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Kotz D.Disk-directed I/O for MIMD multiprocessors[J].ACM Transactions on Computer Systems, 1997, 15 (1) :41-74
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Server-directed collective I/O in Panda">

                                <b>[5]</b>Seamons K E, Chen Ying, Jones P, et al.Server-directed collective I/O in panda[C]//Proc of the 1995ACM/IEEEConf on Supercomputing.New York:ACM, 1995:No.57
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=About the Lustre file system">

                                <b>[6]</b>Lustre.About the Lustre file system[EB/OL]. (2018-04-03) [2018-04-26].http://lustre.org/about/
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data sieving and collective I/O in ROMIO">

                                <b>[7]</b>Thakur R, Gropp W, Lusk E.Data sieving and collective I/O in ROMIO[C]//Proc of the 7th Symp on the Frontiers of Massively Parallel Computation.Washington, DC:IEEEComputer Society, 1999:182-189
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300419975&amp;v=MDg2NzJyUmRHZXJxUVRNbndaZVp1SHlqbVVMdkxJVm9jYXhzPU5pZk9mYks3SHRET3JJOUZZT29HQlhzOG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Thakur R, Gropp W, Lusk E.Optimizing noncontiguous accesses in MPI-IO[J].Parallel Computing, 2002, 28 (1) :83-105
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making Resonance a Common Case:A High-Performance Implementation of Collective I/O on Parallel File Systems">

                                <b>[9]</b>Zhang Xuechen, Jiang Song, Davis K.Making resonance a common case:A high-performance implementation of collective I/O on parallel file systems[C/OL]//Proc of the2009IEEE Int Symp on Parallel&amp;Distributed Processing.Washington, DC:IEEE Computer Society, 2009[2009-05-23].https://ieeexplore.ieee.org/document/5161070
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Heterogeneity-aware collective I/O for parallel I/O systems with hybrid HDD/SSD servers">

                                <b>[10]</b>He Shuibing, Wang Yang, Sun Xianhe, et al.Heterogeneity-aware collective I/O for parallel I/O systems with hybrid HDD/SSD servers[J].IEEE Transactions on Computers, 2017, 66 (6) :1091-1098
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving collective I/O performance using pipelined two-phase I/O">

                                <b>[11]</b>Tsujita Y, Mugurnuma H, Yoshinaga K, et al.Improving collective I/O performance using pipelined two-phase I/O[C]//Proc of the 2012Symp on High Performance Computing.San Diego:Society for Computer Simulation International, 2012:No.7
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving collective I/O performance by pipelining request aggregation and file access">

                                <b>[12]</b>Sehrish S, Son S W, Liao Weikeng, et al.Improving collective I/O performance by pipelining request aggregation and file access[C]//Proc of the 20th European MPI Users’Group Meeting.New York:ACM, 2013:37-42
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving reactivity and communication overlap in MPI using ageneric I/O manager">

                                <b>[13]</b>Denis A, Aumage O, Namyst R.Improving reactivity and communication overlap in MPI using ageneric I/O manager[C]//Proc of European Conf on Recent Advances in Parallel Virtual Machine&amp;Message Passing Interface.Berlin:Springer, 2007:170-177
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000055327&amp;v=MTczMzk9TmlmSVk3SzdIdGpOcjQ5RlpPNEtEMzQrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZMSVZvY2F4cw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>Patrick C M, Son S W, Kandemir M.Comparative evaluation of overlap strategies with study of I/O overlap in MPI-IO[J].Operating System Review, 2008, 42 (6) :43-49
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancing the performance of MPI-IO applications by overlapping I/O computation and communication">

                                <b>[15]</b>Patrick C M, Son S W, Kandemir M.Enhancing the performance of MPI-IO applications by overlapping I/O, computation and communication[C]//Proc of the 13th ACMSIGPLAN Symp on Principles&amp;Practice of Parallel Programming.New York:ACM, 2008:277-278
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201504012&amp;v=MDk1NzMzenFxQnRHRnJDVVJMT2VaZVJxRkNuaFc3N0JMeXZTZExHNEg5VE1xNDlFWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>Yang Lipeng, Che Yonggang.HDF5 based parallel I/Otechniques for multi-zone structured grids CFD applications[J].Journal of Computer Research and Development, 2015, 52 (4) :861-868 (in Chinese) (杨丽鹏, 车永刚.基于HDF5实现多区结构网格CFD程序的并行I/O[J].计算机研究与发展, 2015, 52 (4) :861-868) 
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tuning the performance of I/O intensive parallel applications">

                                <b>[17]</b>Acharya A, Bennett R, Beynon M, et al.Tuning the performance of I/O intensive parallel applications[C]//Proc of Workshop on Input/Output in Parallel&amp;Distributed System.New York:ACM, 1996:15-27
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Flexible and efficient parallel I/O for large-scale multi-component simulations">

                                <b>[18]</b>Ma Xiaosong, Jiao Xiangmin, Campbell M, et al.Flexible and efficient parallel I/O for large-scale multi-component simulations[C]//Proc of the 17th Int Parallel&amp;Distributed Processing Symp.Washington, DC:IEEE Computer Society, 2003:255a
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201904012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201904012&amp;v=MDUzOTBScUZDbmhXNzdCTHl2U2RMRzRIOWpNcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

