

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129045035587500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJFYZ201904018%26RESULT%3d1%26SIGN%3dwEo8Ve3E01HKzgSn7PS%252bpZgQXaY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201904018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JFYZ201904018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201904018&amp;v=MTk4MDk5ak1xNDlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqa1U3cktMeXZTZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#106" data-title="&lt;b&gt;1 背 景&lt;/b&gt; "><b>1 背 景</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="&lt;b&gt;1.1 带权词格&lt;/b&gt;"><b>1.1 带权词格</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;1.2 GRU循环神经网络模型&lt;/b&gt;"><b>1.2 GRU循环神经网络模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="&lt;b&gt;2 基于带权词格的GRU循环神经网络&lt;/b&gt; "><b>2 基于带权词格的GRU循环神经网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#140" data-title="&lt;b&gt;2.1 浅层带权词格&lt;/b&gt;"><b>2.1 浅层带权词格</b></a></li>
                                                <li><a href="#162" data-title="&lt;b&gt;2.2 深层带权词格&lt;/b&gt;"><b>2.2 深层带权词格</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;2.3 融合函数&lt;/b&gt;"><b>2.3 融合函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#212" data-title="&lt;b&gt;3 模型目标和训练&lt;/b&gt; "><b>3 模型目标和训练</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#223" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#225" data-title="&lt;b&gt;4.1 任务和数据集&lt;/b&gt;"><b>4.1 任务和数据集</b></a></li>
                                                <li><a href="#229" data-title="&lt;b&gt;4.2 实验设置&lt;/b&gt;"><b>4.2 实验设置</b></a></li>
                                                <li><a href="#247" data-title="&lt;b&gt;4.3 实验结果分析&lt;/b&gt;"><b>4.3 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#265" data-title="&lt;b&gt;5 相关工作&lt;/b&gt; "><b>5 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#268" data-title="&lt;b&gt;6 总 结&lt;/b&gt; "><b>6 总 结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="图1 一个句子的带权词格">图1 一个句子的带权词格</a></li>
                                                <li><a href="#122" data-title="图2 GRU单元">图2 GRU单元</a></li>
                                                <li><a href="#142" data-title="图3 浅层带权词格GRU单元">图3 浅层带权词格GRU单元</a></li>
                                                <li><a href="#164" data-title="图4 深层带权词格GRU单元">图4 深层带权词格GRU单元</a></li>
                                                <li><a href="#252" data-title="&lt;b&gt;表1 基线模型的情感分类和问句分类实验结果&lt;/b&gt; %"><b>表1 基线模型的情感分类和问句分类实验结果</b> %</a></li>
                                                <li><a href="#253" data-title="&lt;b&gt;表2 本文模型的情感分类和问句分类实验结果&lt;/b&gt;"><b>表2 本文模型的情感分类和问句分类实验结果</b></a></li>
                                                <li><a href="#260" data-title="图5 一个例句的语义建模">图5 一个例句的语义建模</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="332">


                                    <a id="bibliography_" title=".[1]Yu Kai, Jia Lei, Chen Yuqiang, et al.Deep learning:Yesterday, today, and tommorrow[J].Journal of Computer Research and Development, 2013, 50 (9) :1799-1804 (in Chinese) (余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804) [2]Chen Ke, Liang Bin, Ke Wende, et al.Chinese micro-blog sentiment analysis based on multi-channels convolutional neural networks[J].Journal of Computer Research and Development, 2018, 55 (5) :945-957 (in Chinese) (陈珂, 梁斌, 柯文德, 等.基于多通道卷积神经网络的中文微博情感分析[J].计算机研究与发展, 2018, 55 (5) :945-957) [3]Mikolov T, Karafi&#225;t M, Burget L, et al.Recurrent neural network based language model[C]//Proc of the 11th Annual Conf of the Int Speech Communication Association.Phoenix, Arizona:ISCA, 2010:1045-1048" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=基于多通道卷积神经网络的中文微博情感分析">
                                        .[1]Yu Kai, Jia Lei, Chen Yuqiang, et al.Deep learning:Yesterday, today, and tommorrow[J].Journal of Computer Research and Development, 2013, 50 (9) :1799-1804 (in Chinese) (余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804) [2]Chen Ke, Liang Bin, Ke Wende, et al.Chinese micro-blog sentiment analysis based on multi-channels convolutional neural networks[J].Journal of Computer Research and Development, 2018, 55 (5) :945-957 (in Chinese) (陈珂, 梁斌, 柯文德, 等.基于多通道卷积神经网络的中文微博情感分析[J].计算机研究与发展, 2018, 55 (5) :945-957) [3]Mikolov T, Karafi&#225;t M, Burget L, et al.Recurrent neural network based language model[C]//Proc of the 11th Annual Conf of the Int Speech Communication Association.Phoenix, Arizona:ISCA, 2010:1045-1048
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_4" title="Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MDY3MTBud1plWnVIeWptVUx2TElGOFVieEE9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_5" title="Cho K, Van Merri3nboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:1724-1734" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning phrase representations using RNN encoder-decoder for statistical machine translation">
                                        <b>[5]</b>
                                        Cho K, Van Merri3nboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:1724-1734
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_6" title="Dyer C, Muresan S, Resnik P.Generalizing word lattice translation[C]//Proc of the 46th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2008:1012-1020" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalizing word lattice translation">
                                        <b>[6]</b>
                                        Dyer C, Muresan S, Resnik P.Generalizing word lattice translation[C]//Proc of the 46th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2008:1012-1020
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_7" title="Ladhak F, Gandhe A, Dreyer M, et al.LatticeRNN:Recurrent neural networks over lattices[C]//Proc of the17th Annual Conf of the Int Speech Communication Association.Phoenix, Arizona:ISCA, 2016:695-699" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LatticeRNN:Recurrent neural networks over lattices">
                                        <b>[7]</b>
                                        Ladhak F, Gandhe A, Dreyer M, et al.LatticeRNN:Recurrent neural networks over lattices[C]//Proc of the17th Annual Conf of the Int Speech Communication Association.Phoenix, Arizona:ISCA, 2016:695-699
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_8" title="Charniak E, Johnson M.Coarse-to-fine n-best parsing and MaxEnt discriminative reranking[C]//Proc of the 43rd Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2005:173-180" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coarse-to-fine n-best parsing and MaxEnt discriminative reranking">
                                        <b>[8]</b>
                                        Charniak E, Johnson M.Coarse-to-fine n-best parsing and MaxEnt discriminative reranking[C]//Proc of the 43rd Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2005:173-180
                                    </a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_9" title="Huang Liang.Forest reranking:Discriminative parsing with non-local features[C]//Proc of the 46th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2008:586-594" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Forest reranking:Discriminative parsing with non-local features">
                                        <b>[9]</b>
                                        Huang Liang.Forest reranking:Discriminative parsing with non-local features[C]//Proc of the 46th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2008:586-594
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_10" title="Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult[J].IEEETransactions on Neural Networks, 1994, 5 (2) :157-166" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning long-term dependencies with gradient descent is difficult">
                                        <b>[10]</b>
                                        Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult[J].IEEETransactions on Neural Networks, 1994, 5 (2) :157-166
                                    </a>
                                </li>
                                <li id="348">


                                    <a id="bibliography_11" title="Bengio Y, Frasconi P, Simard P.The problem of learning long-term dependencies in recurrent networks[C]//Proc of IEEE Int Conf on Neural Networks.Piscataway, NJ:IEEE, 1993:1183-1188" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The problem of learning long-term dependencies in recurrent networks">
                                        <b>[11]</b>
                                        Bengio Y, Frasconi P, Simard P.The problem of learning long-term dependencies in recurrent networks[C]//Proc of IEEE Int Conf on Neural Networks.Piscataway, NJ:IEEE, 1993:1183-1188
                                    </a>
                                </li>
                                <li id="350">


                                    <a id="bibliography_12" title="Erhan D, Manzagol P A, Bengio Y, et al.The difficulty of training deep architectures and the effect of unsupervised pretraining[C]//Proc of the 12th Int Conf on Artificial Intelligence and Statistics.Cambridge, MA:MIT Press, 2009:153-160" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The difficulty of training deep architectures and the effect of unsupervised pre-training">
                                        <b>[12]</b>
                                        Erhan D, Manzagol P A, Bengio Y, et al.The difficulty of training deep architectures and the effect of unsupervised pretraining[C]//Proc of the 12th Int Conf on Artificial Intelligence and Statistics.Cambridge, MA:MIT Press, 2009:153-160
                                    </a>
                                </li>
                                <li id="352">


                                    <a id="bibliography_13" title="Jiang Wenbin, Mi Haitao, Liu Qun.Word lattice reranking for Chinese word segmentation and part-of-speech tagging[C]//Proc of the 22nd Int Conf on Computational Linguistics.Stroudsburg, PA:ACL, 2008:385-392" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Word Lattice Reranking for Chinese Word Segmentation and Part-ofSpeech Tagging">
                                        <b>[13]</b>
                                        Jiang Wenbin, Mi Haitao, Liu Qun.Word lattice reranking for Chinese word segmentation and part-of-speech tagging[C]//Proc of the 22nd Int Conf on Computational Linguistics.Stroudsburg, PA:ACL, 2008:385-392
                                    </a>
                                </li>
                                <li id="354">


                                    <a id="bibliography_14" title="Wang Zhiguo, Zong Chengqing, Xue Nianwen.A latticebased framework for joint Chinese word segmentation, pos tagging and parsing[C]//Proc of the 51st Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2013:623-627" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Lattice-based Framework for Joint Chinese Word Segmentation,POS Tagging and Parsing">
                                        <b>[14]</b>
                                        Wang Zhiguo, Zong Chengqing, Xue Nianwen.A latticebased framework for joint Chinese word segmentation, pos tagging and parsing[C]//Proc of the 51st Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2013:623-627
                                    </a>
                                </li>
                                <li id="356">


                                    <a id="bibliography_15" title="Su Jinsong, Tan Zhixing, Xiong Deyi, et al.Lattice-based recurrent neural network encoders for neural machine translation[C]//Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2017:3302-3308" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lattice-based recurrent neural network encoders for neural machine translation">
                                        <b>[15]</b>
                                        Su Jinsong, Tan Zhixing, Xiong Deyi, et al.Lattice-based recurrent neural network encoders for neural machine translation[C]//Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2017:3302-3308
                                    </a>
                                </li>
                                <li id="358">


                                    <a id="bibliography_16" title="Le P, Zuidema W.The forest convolutional network:Compositional distributional semantics with a neural chart and without binarization[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:1155-1164" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The forest convolutional network:Compositional distributional semantics with a neural chart and without binarization">
                                        <b>[16]</b>
                                        Le P, Zuidema W.The forest convolutional network:Compositional distributional semantics with a neural chart and without binarization[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:1155-1164
                                    </a>
                                </li>
                                <li id="360">


                                    <a id="bibliography_17" title="Zeiler M D.ADADELTA:An adaptive learning rate method[OL].2012[2017-10-01].https://www.arxiv.org/abs/1212.5701" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ADADELTA:An adaptive learning rate method[OL]">
                                        <b>[17]</b>
                                        Zeiler M D.ADADELTA:An adaptive learning rate method[OL].2012[2017-10-01].https://www.arxiv.org/abs/1212.5701
                                    </a>
                                </li>
                                <li id="362">


                                    <a id="bibliography_18" title="Srivastava N, Hinton G, Krizhevsky A, et al.Dropout:Asimple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:asimple way to prevent neural networks from overfitting">
                                        <b>[18]</b>
                                        Srivastava N, Hinton G, Krizhevsky A, et al.Dropout:Asimple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958
                                    </a>
                                </li>
                                <li id="364">


                                    <a id="bibliography_19" title="Srebro N, Shraibman A.Rank, trace-norm and max-norm[C]//Proc of the 18th Int Conf on Computational Learning Theory.Berlin:Springer, 2005:545-560" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rank,trace-norm and max-norm">
                                        <b>[19]</b>
                                        Srebro N, Shraibman A.Rank, trace-norm and max-norm[C]//Proc of the 18th Int Conf on Computational Learning Theory.Berlin:Springer, 2005:545-560
                                    </a>
                                </li>
                                <li id="366">


                                    <a id="bibliography_20" title="Kim Y.Convolutional neural networks for sentence classification[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:1746-1751" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">
                                        <b>[20]</b>
                                        Kim Y.Convolutional neural networks for sentence classification[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:1746-1751
                                    </a>
                                </li>
                                <li id="368">


                                    <a id="bibliography_21" title="Kalchbrenner N, Grefenstette E, Blunsom P.Aconvolutional neural network for modelling sentences[C]//Proc of the 52nd Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2014:655-665" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">
                                        <b>[21]</b>
                                        Kalchbrenner N, Grefenstette E, Blunsom P.Aconvolutional neural network for modelling sentences[C]//Proc of the 52nd Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2014:655-665
                                    </a>
                                </li>
                                <li id="370">


                                    <a id="bibliography_22" title="Socher R, Pennington J, Huang E H, et al.Semi-supervised recursive autoencoders for predicting sentiment distributions[C]//Proc of the 16th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2011:151-161" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semisupervised recursive autoencoders for predicting sentiment distributions">
                                        <b>[22]</b>
                                        Socher R, Pennington J, Huang E H, et al.Semi-supervised recursive autoencoders for predicting sentiment distributions[C]//Proc of the 16th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2011:151-161
                                    </a>
                                </li>
                                <li id="372">


                                    <a id="bibliography_23" title="Yin Wenpeng, Kann K, Yu Mo, et al.Comparative study of CNN and RNN for natural language processing[OL].2017[2017-10-01].https://www.arxiv.org/abs/1702.01923" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparative study of CNN and RNN for natural language processing[OL]">
                                        <b>[23]</b>
                                        Yin Wenpeng, Kann K, Yu Mo, et al.Comparative study of CNN and RNN for natural language processing[OL].2017[2017-10-01].https://www.arxiv.org/abs/1702.01923
                                    </a>
                                </li>
                                <li id="374">


                                    <a id="bibliography_24" title="Graves A, Jaitly N, Mohamed A.Hybrid speech recognition with deep bidirectional LSTM[C]//Proc of the 8th IEEEWorkshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2013:273-278" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid speech recognition with deep bidirectional LSTM">
                                        <b>[24]</b>
                                        Graves A, Jaitly N, Mohamed A.Hybrid speech recognition with deep bidirectional LSTM[C]//Proc of the 8th IEEEWorkshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2013:273-278
                                    </a>
                                </li>
                                <li id="376">


                                    <a id="bibliography_25" title="Graves A, Mohamed A, Hinton G.Speech recognition with deep recurrent neural networks[C]//Proc of Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:6645-6649" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech recognition with deep recurrent neural networks">
                                        <b>[25]</b>
                                        Graves A, Mohamed A, Hinton G.Speech recognition with deep recurrent neural networks[C]//Proc of Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:6645-6649
                                    </a>
                                </li>
                                <li id="378">


                                    <a id="bibliography_26" title="Vinyals O, Toshev A, Bengio S, et al.Show and tell:Aneural image caption generator[C]//Proc of the 28th IEEEComputer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3156-3164" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show and tell:A neural image caption generator">
                                        <b>[26]</b>
                                        Vinyals O, Toshev A, Bengio S, et al.Show and tell:Aneural image caption generator[C]//Proc of the 28th IEEEComputer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3156-3164
                                    </a>
                                </li>
                                <li id="380">


                                    <a id="bibliography_27" title="Tai Kaisheng, Socher R, Manning C D.Improved semantic representations from tree-structured long short-term memory networks[C]//Proc of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Int Joint Conf on Natural Language Processing.Stroudsburg, PA:ACL, 2015:1556-1566" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks">
                                        <b>[27]</b>
                                        Tai Kaisheng, Socher R, Manning C D.Improved semantic representations from tree-structured long short-term memory networks[C]//Proc of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Int Joint Conf on Natural Language Processing.Stroudsburg, PA:ACL, 2015:1556-1566
                                    </a>
                                </li>
                                <li id="382">


                                    <a id="bibliography_28" title="Le P, Zuidema W.Compositional distributional semantics with long short term memory[C]//Proc of the 4th Joint Conf on Lexical and Computational Semantics.Stroudsburg, PA:ACL, 2015:10-19" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compositional distributional semantics with long short term memory">
                                        <b>[28]</b>
                                        Le P, Zuidema W.Compositional distributional semantics with long short term memory[C]//Proc of the 4th Joint Conf on Lexical and Computational Semantics.Stroudsburg, PA:ACL, 2015:10-19
                                    </a>
                                </li>
                                <li id="384">


                                    <a id="bibliography_29" title="Zhu Xiaodan, Sobihani P, Guo Hongyu.Long short-term memory over recursive structures[C]//Proc of the 32nd Int Conf on Machine Learning.Lille, France:PMLR, 2015:1604-1612" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory over recursive structures">
                                        <b>[29]</b>
                                        Zhu Xiaodan, Sobihani P, Guo Hongyu.Long short-term memory over recursive structures[C]//Proc of the 32nd Int Conf on Machine Learning.Lille, France:PMLR, 2015:1604-1612
                                    </a>
                                </li>
                                <li id="386">


                                    <a id="bibliography_30" title="Liu Pengfei, Qiu Xipeng, Chen Xinchi, et al.Multitimescale long short-term memory neural network for modelling sentences and documents[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2326-2335" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multitimescale long short-term memory neural network for modelling sentences and documents">
                                        <b>[30]</b>
                                        Liu Pengfei, Qiu Xipeng, Chen Xinchi, et al.Multitimescale long short-term memory neural network for modelling sentences and documents[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2326-2335
                                    </a>
                                </li>
                                <li id="388">


                                    <a id="bibliography_31" title="Chung J, Gulcehre C, Cho K H, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[OL].2014[2017-10-01].https://www.arxiv.org/abs/1412.3555" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of gated recurrent neural networks on sequence modeling[OL]">
                                        <b>[31]</b>
                                        Chung J, Gulcehre C, Cho K H, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[OL].2014[2017-10-01].https://www.arxiv.org/abs/1412.3555
                                    </a>
                                </li>
                                <li id="390">


                                    <a id="bibliography_32" title="Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks[C]//Proc of the 28th Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2014:3104-3112" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequence to sequence learning with neural networks">
                                        <b>[32]</b>
                                        Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks[C]//Proc of the 28th Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2014:3104-3112
                                    </a>
                                </li>
                                <li id="392">


                                    <a id="bibliography_33" title="Chen Xinchi, Qiu Xipeng, Zhu Chenxi, et al.Sentence modeling with gated recursive neural network[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:793-798" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentence modeling with gated recursive neural network">
                                        <b>[33]</b>
                                        Chen Xinchi, Qiu Xipeng, Zhu Chenxi, et al.Sentence modeling with gated recursive neural network[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:793-798
                                    </a>
                                </li>
                                <li id="394">


                                    <a id="bibliography_34" title="Socher R, Lin C C, Ng A Y, et al.Parsing natural scenes and natural language with recursive neural networks[C]//Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:129-136" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parsing Natural Scenes and Natural Language with Recursive Neural Networks">
                                        <b>[34]</b>
                                        Socher R, Lin C C, Ng A Y, et al.Parsing natural scenes and natural language with recursive neural networks[C]//Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:129-136
                                    </a>
                                </li>
                                <li id="396">


                                    <a id="bibliography_35" title="Socher R, Huval B, Manning C D, et al.Semantic compositionality through recursive matrix-vector spaces[C]//Proc of the 17th Joint Conf on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.Stroudsburg, PA:ACL, 2012:1201-1211" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic compositionality through recursive matrix-vector spaces">
                                        <b>[35]</b>
                                        Socher R, Huval B, Manning C D, et al.Semantic compositionality through recursive matrix-vector spaces[C]//Proc of the 17th Joint Conf on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.Stroudsburg, PA:ACL, 2012:1201-1211
                                    </a>
                                </li>
                                <li id="398">


                                    <a id="bibliography_36" title="Socher R, Perelygin A, Wu J, et al.Recursive deep models for semantic compositionality over a sentiment treebank[C]//Proc of the 18th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2013:1631-1642" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank">
                                        <b>[36]</b>
                                        Socher R, Perelygin A, Wu J, et al.Recursive deep models for semantic compositionality over a sentiment treebank[C]//Proc of the 18th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2013:1631-1642
                                    </a>
                                </li>
                                <li id="400">


                                    <a id="bibliography_37" title="Hermann K M, Blunsom P.The role of syntax in vector space models of compositional semantics[C]//Proc of the51st Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2013:894-904" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Role of Syntax in Vector Space Models of Compositional Semantics">
                                        <b>[37]</b>
                                        Hermann K M, Blunsom P.The role of syntax in vector space models of compositional semantics[C]//Proc of the51st Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2013:894-904
                                    </a>
                                </li>
                                <li id="402">


                                    <a id="bibliography_38" title="Iyyer M, Boyd-Graber J, Claudino L, et al.A neural network for factoid question answering over paragraphs[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:633-644" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Neural Network for Factoid Question Answering over Paragraphs">
                                        <b>[38]</b>
                                        Iyyer M, Boyd-Graber J, Claudino L, et al.A neural network for factoid question answering over paragraphs[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:633-644
                                    </a>
                                </li>
                                <li id="404">


                                    <a id="bibliography_39" title="Mou Lili, Peng Hao, Li Ge, et al.Discriminative neural sentence modeling by tree-based convolution[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2315-2325" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative Neural Sentence Modeling by Tree-Based Convolution">
                                        <b>[39]</b>
                                        Mou Lili, Peng Hao, Li Ge, et al.Discriminative neural sentence modeling by tree-based convolution[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2315-2325
                                    </a>
                                </li>
                                <li id="406">


                                    <a id="bibliography_40" title="Kalchbrenner N, Danihelka I, Graves A.Grid long shortterm memory[OL].2015[2017-10-01].https://www.arxiv.org/abs/1507.01526" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grid long shortterm memory[OL]">
                                        <b>[40]</b>
                                        Kalchbrenner N, Danihelka I, Graves A.Grid long shortterm memory[OL].2015[2017-10-01].https://www.arxiv.org/abs/1507.01526
                                    </a>
                                </li>
                                <li id="408">


                                    <a id="bibliography_41" title="Graves A, Fern&#225;ndez S, Schmidhuber J.Multi-dimensional recurrent neural networks[C]//Proc of the 17th Int Conf on Artificial Neural Networks.Berlin:Springer, 2007:549-558" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-dimensio nal recurrent neural networks">
                                        <b>[41]</b>
                                        Graves A, Fern&#225;ndez S, Schmidhuber J.Multi-dimensional recurrent neural networks[C]//Proc of the 17th Int Conf on Artificial Neural Networks.Berlin:Springer, 2007:549-558
                                    </a>
                                </li>
                                <li id="410">


                                    <a id="bibliography_42" title="Soltani R, Jiang Hui.Higher order recurrent neural networks[OL].2016[2017-10-01].https://www.arxiv.org/abs/1605.00064" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Higher order recurrent neural networks[OL]">
                                        <b>[42]</b>
                                        Soltani R, Jiang Hui.Higher order recurrent neural networks[OL].2016[2017-10-01].https://www.arxiv.org/abs/1605.00064
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JFYZ" target="_blank">计算机研究与发展</a>
                2019,56(04),854-865 DOI:10.7544/issn1000-1239.2019.20170917            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于带权词格的循环神经网络句子语义表示建模</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%A5%A5%E6%96%87&amp;code=37801803&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张祥文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%86%E7%B4%AB%E8%80%80&amp;code=41491415&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陆紫耀</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E9%9D%99&amp;code=10232060&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨静</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E5%80%A9&amp;code=09227676&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林倩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%A2%E5%AE%87&amp;code=39811379&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卢宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%B8%BF%E5%90%89&amp;code=41491416&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王鸿吉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8B%8F%E5%8A%B2%E6%9D%BE&amp;code=10911517&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏劲松</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8E%A6%E9%97%A8%E5%A4%A7%E5%AD%A6&amp;code=0125037&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">厦门大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E8%8B%8F%E7%9C%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4(%E8%8B%8F%E5%B7%9E%E5%A4%A7%E5%AD%A6)&amp;code=0240077&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江苏省计算机信息处理技术重点实验室(苏州大学)</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目前, 循环神经网络 (recurrent neural network, RNN) 已经被广泛应用于自然语言处理的文本序列语义表示建模.对于没有词语分隔符的语言, 例如中文, 该网络以经过分词预处理的词序列作为标准输入.然而, 非最优的分词粒度和分词错误会对句子语义表示建模产生负面作用, 影响后续自然语言处理任务的进行.针对这些问题, 提出基于带权词格的循环神经网络模型.该模型以带权词格作为输入, 在每个时刻融合多个输入向量和对应的隐状态, 融合生成新的隐状态.带权词格是一种包含指数级别分词结果的压缩数据结构, 词格中的边权重在一定程度上体现了不同分词结果的一致性.特别地, 利用词格权重作为融合函数中权重建模的监督信息, 进一步提升了模型句子语义表示的学习效果.相比于传统循环神经网络, 该模型不仅能够缓解分词错误对句子语义建模产生的负面影响, 同时使得语义建模具有更强的灵活性.在情感分类和问句分类2个任务上的实验结果证明了该模型的有效性.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%A6%E6%9D%83%E8%AF%8D%E6%A0%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">带权词格;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">循环神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%A5%E5%AD%90%E8%AF%AD%E4%B9%89%E5%BB%BA%E6%A8%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">句子语义建模;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%97%AE%E5%8F%A5%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">问句分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *苏劲松 (jssu@xmu.edu.cn) ;
                                </span>
                                <span>
                                    张祥文, xwzhang@stu.xmu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61672440);</span>
                                <span>北京语言大学语言资源高精尖创新中心资助;</span>
                                <span>国家语言文字工作委员会一般项目 (YB135-49);</span>
                                <span>中央高校基本科研业务费专项资金项目 (ZK1024);</span>
                                <span>苏州大学江苏省计算机信息处理技术重点实验室开放课题 (KJS1520);</span>
                    </p>
            </div>
                    <h1><b>Weighted Lattice Based Recurrent Neural Networks for Sentence Semantic Representation Modeling</b></h1>
                    <h2>
                    <span>Zhang Xiangwen</span>
                    <span>Lu Ziyao</span>
                    <span>Yang Jing</span>
                    <span>Lin Qian</span>
                    <span>Lu Yu</span>
                    <span>Wang Hongji</span>
                    <span>Su Jinsong</span>
            </h2>
                    <h2>
                    <span>Xiamen University</span>
                    <span>Jiangsu Provincial Key Laboratory for Computer Information Processing Technology (Soochow University)</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Currently, recurrent neural networks (RNNs) have been widely used in semantic representation modeling of text sequences in natural language processing. For those languages without natural word delimiters (e.g., Chinese) , RNNs generally take the segmented word sequence as input. However, sub-optimal segmentation granularity and segmentation errors may affect sentence semantic modeling negatively, as well as subsequent natural language processing tasks. To address these issues, the proposed weighted word lattice based RNNs take the weighted word lattice as input and produce current state at each time step by integrating arbitrarily many input vectors and the corresponding previous hidden states. Weighted word lattice expresses a compressed data structure that contains exponential word segmentation results. To a certain extent, the weighted word lattice reflects the consistency of different word segmentation results. Specifically, lattice weights are further exploited as a supervised regularizer to refine weights modeling of the semantic composition operation in this model, leading to better sentence semantic representation learning. Compared with traditional RNNs, the proposed model not only alleviates the negative impact of segmentation errors but also is more expressive and flexible to sentence representation learning. Experimental results on sentiment classification and question classification tasks demonstrate the superiority of the proposed model.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weighted%20word%20lattice&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weighted word lattice;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recurrent%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recurrent neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sentence%20semantics%20modeling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sentence semantics modeling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sentiment%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sentiment classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=question%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">question classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    Zhang Xiangwen, born in 1994. Master candidate. His main research interests include natural language processing and neural machine translation. <image id="463" type="" href="images/JFYZ201904018_46300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Lu Ziyao, born in 1996.Master candidate. His main research interests include natural language processing and neural machine translation. <image id="465" type="" href="images/JFYZ201904018_46500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Yang  Jing, born  in  1994. Master candidate. Her main research interests include natural language processing and neural machine translation. <image id="467" type="" href="images/JFYZ201904018_46700.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Lin Qian, born in 1995.Master candidate. Her main research interests include natural language processing and neural machine translation.<image id="469" type="" href="images/JFYZ201904018_46900.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Lu Yu, born in 1996.Graduate student. Her main research interests include natural language processing and neural machine translation. <image id="471" type="" href="images/JFYZ201904018_47100.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Wang Hongji, born in 1968. PhD. Associate professor in Xiamen University. His  main  research  interests  include automata  theory, cryptography  and information security. <image id="473" type="" href="images/JFYZ201904018_47300.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                                <span>
                                    Su Jinsong, born in 1982.Received his PhD degree from Chinese Academy of Sciences.Associate professor in Xiamen University.His main research interests include natural language processing and machine translation.<image id="475" type="" href="images/JFYZ201904018_47500.jpg" display="inline" placement="inline"><alt></alt></image>;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-12-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>supported by the National Natural Science Foundation of China (61672440);</span>
                                <span>the Project of Beijing Advanced Innovation Center for Language Resources;</span>
                                <span>the Scientific Research Project of National Language Committee of China (YB135-49);</span>
                                <span>the Fundamental Research Funds for the Central Universities (ZK1024);</span>
                                <span>the Open Project of Jiangsu Provincial Key Laboratory for Computer Information Processing Technology (Soochow University) (KJS1520);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="98">如何生成高质量的句子语义表示一直是自然语言处理的核心问题之一.由于现实中自然语言句子的数量是无限的, 因此, 我们训练好的模型往往需要处理从未在训练语料中出现过的句子.对此, 传统方法通常以高频词或多元词串为基础来表示句子, 然后在此基础上进行各种运算, 以获得表示句子语义的向量.然而, 这些方法往往需要人工事先定义特征, 所以建模效率较为低下.近年来, 随着深度学习研究及其应用的快速发展<sup><a class="sup">[1]</a></sup>, 学术界和产业界将目光转向了神经网络, 通过构建深度神经网络来学习句子的语义表示<sup><a class="sup">[2]</a></sup>, 以应用到后续的自然语言处理任务中.</p>
                </div>
                <div class="p1">
                    <p id="99">在基于深度学习的句子语义表示建模方面, 循环神经网络 (recurrent neural networks, RNNs) <sup><a class="sup">[3]</a></sup>得到了广泛应用.相比于传统的非神经网络模型, RNN能够保存序列的历史信息, 因此对长序列文本具有更好的建模能力.特别地, RNN的一些变种, 例如LSTM (long short term memory) <citation id="412" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和GRU (gated recurrent unit) <citation id="413" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 进一步引入门机制 (gating mechanism) 来控制信息流动, 提高捕获序列内部长距离依赖的能力.针对中文等没有天然词语分隔符的语言, 神经网络模型有2种实现句子语义建模的方案:第1种方案直接建模字序列.该方法忽略词语的边界信息, 不需要分词, 而这一信息对于建模字、词之间的组合关系至关重要;第2种方案则先进行分词, 然后以词为单位来建模.该方法同样存在缺陷:一方面, 分词工具产生的错误分词对句子的结构造成破坏, 并通过错误传播的形式对后续的表示建模产生负面影响;另一方面, 使用单一的词序列来表示句子, 使得文本表示建模缺乏灵活性.因此, 对于中文等语言, 如何利用RNN来提高句子语义表示建模的质量是一个有待深入研究的重要问题.</p>
                </div>
                <div class="p1">
                    <p id="100">针对上述问题, 本文提出基于带权词格的循环神经网络模型.词格是一个能够容纳多种分词结果的压缩数据结构, 与单一分词结果相比, 它具有丰富的表示能力.目前, 词格已经广泛地应用于许多自然语言处理任务当中, 并取得了很好的效果, 例如机器翻译<citation id="414" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和语音识别<citation id="415" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.通过基于带权词格进行句子语义表示建模, 我们期望提出的模型可以减轻分词错误造成的错误传播, 同时也能使句子语义表示建模具备更强的灵活性.在本文工作中, 我们提出了2种基于带权词格的GRU神经网络模型:1) 基于带权词格的浅层融合循环神经网络模型 (shallow weighted word lattice RNN, SWWL-RNN) , 该模型直接对多个分词输入和相应的前隐状态进行融合, 再输入到标准的RNN单元生成当前隐状态;2) 基于带权词格的深层融合循环神经网络模型 (deep weighted word lattice RNN, DWWL-RNN) .不同于SWWL-RNN, 该模型先根据每个分词输入和相应的前隐状态分别产生各自的当前隐状态, 然后再对这些隐状态进行融合, 生成最终的当前隐状态.显然, 2种模型都以融合函数为核心.因此, 针对隐状态的融合函数, 本文尝试了4种不同的融合策略:</p>
                </div>
                <div class="p1">
                    <p id="101">1) 池化 (pooling) 融合函数;</p>
                </div>
                <div class="p1">
                    <p id="102">2) 门机制融合函数;</p>
                </div>
                <div class="p1">
                    <p id="103">3) 基于词格边权重的融合函数;</p>
                </div>
                <div class="p1">
                    <p id="104">4) 融入词格边权重的门机制融合函数.</p>
                </div>
                <div class="p1">
                    <p id="105">最后, 我们在情感分类和问句分类实验上, 分析对比了2种模型、4种融合策略的效果.实验结果表明, 基于带权词格的RNN模型的性能明显超过传统的RNN变体模型和现有的其他模型.</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag"><b>1 背 景</b></h3>
                <div class="p1">
                    <p id="107">本节介绍本文工作的基础:带权词格<citation id="416" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和GRU<citation id="417" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>循环神经网络.</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>1.1 带权词格</b></h4>
                <div class="p1">
                    <p id="109">带权词格<citation id="418" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>是一种包含指数级别分词结果的压缩数据结构.图1所示为1个句子根据3种不同的分词标准进行分词的结果及相对应的词格结构.3种分词标准, 分别来自北京大学 (Peking University, PKU) 、中文树库 (Chinese treebank, CTB) 和微软研究院 (Microsoft Research, MSR) 公开的分词语料训练的分词模型.如图1 (d) 所示, 给定由<i>N</i>个字组成的1个序列<i>c</i><sub>1:<i>N</i></sub>=<i>c</i><sub>1</sub><i>c</i><sub>2</sub>…<i>c</i><sub><i>N</i></sub>, 带权词格在形式上表现为1个带权重的有向图<i>G</i>=&lt;<i>V</i>, <i>E</i>&gt;.这里, <i>V</i>表示结点的集合, 其中结点<i>v</i><sub><i>i</i></sub>∈<i>V</i> (<i>i</i>=1, 2, …, <i>N</i>-1) 表示<i>c</i><sub><i>i</i></sub>和<i>c</i><sub><i>i</i>+1</sub>之间的位置.此外, 词格还包含2个特殊的结点:1) <i>v</i><sub>0</sub>, 该结点在<i>c</i><sub>1</sub>之前, 表示字序列的开始位置;2) <i>v</i><sub><i>N</i></sub>, 该结点在<i>c</i><sub><i>N</i></sub>之后, 表示字序列的结束位置.<i>E</i>表示边的集合, 以边<i>e</i><sub><i>i</i>:<i>j</i></sub>为例, 它以<i>v</i><sub><i>i</i></sub>为起点, 并指向<i>v</i><sub><i>j</i></sub>, 同时覆盖了字序列<i>c</i><sub><i>i</i>:<i>j</i></sub>, <i>c</i><sub><i>i</i>:<i>j</i></sub>对应潜在的一个候选分词.而<i>e</i><sub><i>i</i>:<i>j</i></sub>对应的权重<i>weight</i><sub><i>e</i><sub><i>i</i>:<i>j</i></sub></sub>, 则代表<i>c</i><sub><i>i</i>:<i>j</i></sub>被作为候选分词的可能性.</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 一个句子的带权词格" src="Detail/GetImg?filename=images/JFYZ201904018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 一个句子的带权词格  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904018_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 1 A weighted word lattice</p>

                </div>
                <div class="p1">
                    <p id="111">词格中的边权重可以使用前后向算法<citation id="419" type="reference"><link href="342" rel="bibliography" /><link href="344" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>来计算.具体而言, 对于结点<i>v</i><sub><i>i</i></sub>, 我们首先递归遍历它左边的前序结点, 以迭代方式累加计算出从<i>v</i><sub>0</sub>到<i>v</i><sub><i>i</i></sub>的路径数目<i>α</i><sub><i>v</i><sub><i>i</i></sub></sub>, 即:</p>
                </div>
                <div class="p1">
                    <p id="112"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></munder><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></msub></mrow></math></mathml>, (1) </p>
                </div>
                <div class="p1">
                    <p id="114">其中, <i>v</i><sub><i>i</i><sub><i>k</i></sub></sub>是结点<i>v</i><sub><i>i</i></sub>的第<i>k</i>个前序结点.然后, 对于结点<i>v</i><sub><i>j</i></sub>, 我们递归地遍历它右边的后序结点, 同样以迭代累加的方式计算出从<i>v</i><sub><i>N</i></sub>到<i>v</i><sub><i>j</i></sub>的路径数目<i>β</i><sub><i>v</i><sub><i>j</i></sub></sub>, 即:</p>
                </div>
                <div class="p1">
                    <p id="115"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><msub><mrow></mrow><mrow><mi>j</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></munder><mi>β</mi></mstyle><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mrow><mi>j</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></msub></mrow></math></mathml>, (2) </p>
                </div>
                <div class="p1">
                    <p id="117">其中<i>v</i><sub><i>j</i><sub><i>k</i></sub></sub>是结点<i>v</i><sub><i>j</i></sub>的第<i>k</i>个后序结点.最后, <i>weight</i><sub><i>e</i><sub><i>i</i>:<i>j</i></sub></sub>可定义为</p>
                </div>
                <div class="p1">
                    <p id="118"><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><msub><mrow></mrow><mrow><mi>e</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>:</mo><mi>j</mi></mrow></msub></mrow></msub><mo>=</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>×</mo><mi>β</mi><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub></mrow><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>v</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></msub></mrow></mfrac></mrow></math></mathml>. (3) </p>
                </div>
                <div class="p1">
                    <p id="120">如图1 (d) 所示, 从<i>v</i><sub>0</sub>指向<i>v</i><sub>3</sub>的边<i>e</i><sub>0:3</sub>, 覆盖了<i>c</i><sub>1</sub>到<i>c</i><sub>3</sub>的字序列, 表示一个候选词“下雨天”, 其权重为0.33.边权重在一定程度上体现了不同分词标准的一致性.权重越大, 边覆盖的字序列被切分为词的可能性就越高.同时, 边权重也增强了词格的容错性, 使词格结构的信息表示更加丰富, 从而得以有效应用于各种自然语言处理任务中.</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>1.2 GRU循环神经网络模型</b></h4>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904018_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 GRU单元" src="Detail/GetImg?filename=images/JFYZ201904018_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 GRU单元  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904018_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 2 A GRU unit</p>

                </div>
                <div class="p1">
                    <p id="123">RNN<sup><a class="sup">[1]</a></sup>虽然具有较好的文本序列建模能力, 但仍然面临着模型参数梯度消失和爆炸的难题<citation id="422" type="reference"><link href="346" rel="bibliography" /><link href="348" rel="bibliography" /><link href="350" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>.对此, 研究者引入了带有门机制的LSTM<citation id="420" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和GRU<citation id="421" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>来控制网络信息流动, 以提高RNN在长序列文本上的建模能力.由于GRU与LSTM性能相同, 同时所需参数更少.因此, 本文选择GRU作为循环神经网络单元进行文本建模.需要说明的是, 本文方法同样适用于LSTM等其他RNN的变种模型.</p>
                </div>
                <div class="p1">
                    <p id="124">如图2所示, 与RNN相同, GRU在每个输入单元循环地应用1个转移函数, 以生成当前时刻的隐状态表示.</p>
                </div>
                <div class="p1">
                    <p id="125">具体来说, 时刻<i>t</i>的隐状态向量<b><i>h</i></b><sub><i>t</i></sub>∈R<sup><i>d</i></sup>, 由当前输入向量<b><i>x</i></b><sub><i>t</i></sub>∈R<sup><i>d</i></sup>和前一时刻的隐状态向量<b><i>h</i></b><sub><i>t</i>-1</sub>生成:</p>
                </div>
                <div class="p1">
                    <p id="126"><b><i>h</i></b><sub><i>t</i></sub>=<i>f</i> (<b><i>x</i></b><sub><i>t</i></sub>, <b><i>h</i></b><sub><i>t</i>-1</sub>) , (4) </p>
                </div>
                <div class="p1">
                    <p id="127">其中, <i>f</i> (*) 通常定义为一个仿射变换及双曲正切函数tanh.对于文本序列而言, <b><i>x</i></b><sub><i>t</i></sub>是句子中第<i>t</i>个词的向量表示, <b><i>h</i></b><sub><i>t</i></sub>则代表到时刻<i>t</i>为止的词序列向量.</p>
                </div>
                <div class="p1">
                    <p id="128">正如本节第1段所述, GRU在RNN的基础上, 进一步引入了重置门和更新门来控制信息流动.图2所示为一个时刻<i>t</i>的GRU单元, 其转移函数定义为</p>
                </div>
                <div class="p1">
                    <p id="129"><b><i>r</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sup> (r) </sup><b><i>x</i></b><sub><i>t</i></sub>+<b><i>U</i></b><sup> (r) </sup><b><i>h</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sup> (r) </sup>) , (5) </p>
                </div>
                <div class="p1">
                    <p id="130"><b><i>z</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sup> (z) </sup><b><i>x</i></b><sub><i>t</i></sub>+<b><i>U</i></b><sup> (z) </sup><b><i>h</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sup> (z) </sup>) , (6) </p>
                </div>
                <div class="p1">
                    <p id="131"><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>, (7) </p>
                </div>
                <div class="p1">
                    <p id="133"><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>, (8) </p>
                </div>
                <div class="p1">
                    <p id="135">其中, <b><i>r</i></b><sub><i>t</i></sub>是重置门, 用于控制前状态的信息流动:当它的值接近0时, 将使得GRU单元忽略前状态信息并使用当前输入进行重置, 这使得GRU单元丢弃被认为在未来无用的信息, 从而得到一个更加紧凑的隐状态表示;<b><i>z</i></b><sub><i>t</i></sub>是更新门, 用于控制在当前状态有多少前状态信息被保留;<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>是利用重置门对历史信息进行过滤后的候选隐状态向量;<i>σ</i>是一个逻辑斯蒂函数, ⊙表示逐元素乘法.式 (5) ～ (7) 中<b><i>W</i></b>和<b><i>U</i></b>是参数矩阵, 用于对输入和隐状态向量进行线性变换;<b><i>b</i></b>是一个偏置项向量;<b><i>W</i></b>, <b><i>U</i></b>, <b><i>b</i></b>的上标r, z, c分别表示该参数对应的是重置门、更新门、候选隐状态.</p>
                </div>
                <h3 id="137" name="137" class="anchor-tag"><b>2 基于带权词格的GRU循环神经网络</b></h3>
                <div class="p1">
                    <p id="138">受现有工作<citation id="424" type="reference"><link href="338" rel="bibliography" /><link href="352" rel="bibliography" /><link href="354" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>的启发, 本节对基于词格的循环神经网络<citation id="423" type="reference"><link href="356" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行扩展, 提出了基于带权词格的GRU循环神经网络, 以学习句子语义表示, 用于后续的自然语言处理任务.显然, 与词序列相比, 带权词格具有更为丰富的信息和更为复杂的网络拓扑结构.以它为基础来进行神经网络建模将面临着2个难题:1) 在带权词格中, 一个句子通常会存在许多分词结果, 这意味着当前单元可能会同时存在多个输入和多个前隐状态, 传统循环神经网络<citation id="425" type="reference"><link href="334" rel="bibliography" /><link href="336" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>无法建模这样的结构;2) 带权词格的边权重能够较好地区别不同分词结果的可能性.如何在本文所提出的模型中体现出不同分词结果在句子建模过程中作用的差异, 是本文研究工作的一个关键问题.</p>
                </div>
                <div class="p1">
                    <p id="139">在建模过程中, 我们的模型以句子的字序列为输入, 逐字地读取句子.在时刻<i>t</i>, 对于当前结点<i>v</i><sub><i>t</i></sub>, 我们首先确定以字<i>c</i><sub><i>t</i></sub>为结尾的一个入度边集合, 即{<i>e</i><sub><i>t</i><sub><i>k</i></sub>:<i>t</i></sub>= (<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>) <citation id="426">|0</citation>≤<i>t</i><sub><i>k</i></sub>&lt;<i>t</i>, 0≤<i>k</i>&lt;<i>K</i>}, 这里<i>K</i>表示入度边数, 即以字<i>c</i><sub><i>t</i></sub>为结尾的不同候选分词的数量, <b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>和<b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>分别为第<i>k</i>个候选分词的词向量表示和相应的前隐状态.本文提出的2种基于带权词格的循环神经网络模型, 分别是:基于带权词格的浅层融合GRU模型和基于带权词格的深层融合GRU模型.这2个模型均以融合函数为核心, 分别通过浅层、深层融合产生时刻<i>t</i>的隐状态.针对融合函数, 我们将在2.3节中详细介绍4种不同的融合策略.</p>
                </div>
                <h4 class="anchor-tag" id="140" name="140"><b>2.1 浅层带权词格</b></h4>
                <div class="p1">
                    <p id="141">浅层带权词格GRU模型的单元结构如图3所示:</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904018_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 浅层带权词格GRU单元" src="Detail/GetImg?filename=images/JFYZ201904018_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 浅层带权词格GRU单元  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904018_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 A SWWL-GRU unit</p>

                </div>
                <div class="p1">
                    <p id="143">针对GRU<sup><a class="sup">[3]</a></sup>, 我们使用词向量和相应的前隐状态构成的集合{ (<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>) }来表示词格中结点<i>v</i><sub><i>t</i></sub>的入度边集合.接着, 分别融合词向量{<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>}和前隐状态{<b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>}的2个集合, 生成<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>和<mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover></math></mathml><sub><i>t</i>-1</sub>, 并作为当前时刻唯一的输入词向量和前隐状态, 传递给循环单元, 生成时刻<i>t</i>的隐状态.特别地, 对于LSTM<sup><a class="sup">[2]</a></sup>等包含额外记忆单元的RNN变种, 入度边集合则表示为{ (<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>m</i></b><sub><i>t</i><sub><i>k</i></sub></sub>) }, 其中<b><i>m</i></b><sub><i>t</i><sub><i>k</i></sub></sub>表示对应的前一个记忆单元.因此, 通过合并入度边, 新的当前隐状态得以容纳多种潜在的候选分词.不难看出, 这一模型主要关注如何对循环单元的输入, 也就是{<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>}和{<b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>}分别进行融合, 而并不需要修改循环单元内部结构, 因此适用于任何基于RNN的变种模型<sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup>.</p>
                </div>
                <div class="p1">
                    <p id="146">形式上, 该单元的建模函数定义为</p>
                </div>
                <div class="p1">
                    <p id="147"><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (9) </p>
                </div>
                <div class="p1">
                    <p id="149"><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (10) </p>
                </div>
                <div class="p1">
                    <p id="151"><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>r</mtext><mo stretchy="false">) </mo></mrow></msup><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>r</mtext><mo stretchy="false">) </mo></mrow></msup><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>r</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>, (11) </p>
                </div>
                <div class="p1">
                    <p id="153"><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>z</mtext><mo stretchy="false">) </mo></mrow></msup><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>z</mtext><mo stretchy="false">) </mo></mrow></msup><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>z</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>, (12) </p>
                </div>
                <div class="p1">
                    <p id="155"><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>, (13) </p>
                </div>
                <div class="p1">
                    <p id="157"><mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>, (14) </p>
                </div>
                <div class="p1">
                    <p id="159">其中, <mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>和<mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover></math></mathml><sub><i>t</i>-1</sub>是经过语义融合操作后得到的输入词向量和前隐状态.式 (9) (10) 分别用于融合<i>K</i>个输入词向量和前隐状态, 而式 (11) ～ (13) 则与标准GRU完全一致.<i>g</i> (*) 是融合函数, 其具体定义在2.3节详细介绍.</p>
                </div>
                <h4 class="anchor-tag" id="162" name="162"><b>2.2 深层带权词格</b></h4>
                <div class="p1">
                    <p id="163">深层带权词格GRU循环神经网络的单元结构如图4所示:</p>
                </div>
                <div class="area_img" id="164">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904018_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 深层带权词格GRU单元" src="Detail/GetImg?filename=images/JFYZ201904018_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 深层带权词格GRU单元  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904018_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 4 A DWWL-GRU unit</p>

                </div>
                <div class="p1">
                    <p id="165">以入度边集合{ (<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>) }为基础, 我们将每条边 (<b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>) 独立地输入到循环单元中, 生成隐状态集合{<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover></math></mathml><sub><i>t</i><sub><i>k</i></sub></sub>}.<mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover></math></mathml><sub><i>t</i><sub><i>k</i></sub></sub>表示第<i>k</i>种潜在候选分词对应的隐状态, 接着融合所有的候选隐状态, 得到新的当前隐状态.与浅层带权词格GRU相似, 这一模型并不对循环单元结构造成影响, 因此仍然适用于任何RNN的变种模型<sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup>.</p>
                </div>
                <div class="p1">
                    <p id="168">与浅层带权词格GRU单元相比, 深层模型在更细粒度的语义表示层次上, 对多种分词结果进行分词状态的融合生成.浅层模型选择融合循环单元的输入, 而深层模型采取对循环单元的输出进行融合的方式.简单来说, 两者的具体区别在于选取融合操作的时机不同.从时间复杂度来考虑, 深层模型的时间复杂度为<i>O</i> (<i>KN</i>) , 即关于句子的字数和边的最大个数成正比;而浅层模型的时间复杂度为<i>O</i> (<i>N</i>) , 与基于字的普通RNN模型相等.这2个模型涉及到融合函数的部分, 时间代价可以忽略不计.</p>
                </div>
                <div class="p1">
                    <p id="169">形式上, 该单元的建模函数定义为</p>
                </div>
                <div class="p1">
                    <p id="170"><b><i>r</i></b><sub><i>t</i><sub><i>k</i></sub></sub>=<i>σ</i> (<b><i>W</i></b><sup> (r) </sup><b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>+<b><i>U</i></b><sup> (r) </sup><b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>+<b><i>b</i></b><sup> (r) </sup>) , (15) </p>
                </div>
                <div class="p1">
                    <p id="171"><b><i>z</i></b><sub><i>t</i><sub><i>k</i></sub></sub>=<i>σ</i> (<b><i>W</i></b><sup> (z) </sup><b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>+<b><i>U</i></b><sup> (z) </sup><b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>+<b><i>b</i></b><sup> (z) </sup>) , (16) </p>
                </div>
                <div class="p1">
                    <p id="172"><mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>⊙</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>c</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></math></mathml>, (17) </p>
                </div>
                <div class="p1">
                    <p id="174"><mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>⊙</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mo stretchy="false">) </mo><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></math></mathml>, (18) </p>
                </div>
                <div class="p1">
                    <p id="176"><mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>g</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (19) </p>
                </div>
                <div class="p1">
                    <p id="178">其中, <b><i>x</i></b><sub><i>t</i><sub><i>k</i></sub></sub>, <b><i>h</i></b><sub><i>t</i><sub><i>k</i></sub></sub>与2.1节公式符号的含义相同.式 (15) ～ (18) 用于生成第<i>k</i>个分词对应的隐状态, 式 (19) 采用语义融合函数<i>g</i> (*) 生成<b><i>h</i></b><sub><i>t</i></sub>.</p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>2.3 融合函数</b></h4>
                <div class="p1">
                    <p id="180">在常见的基于字或词的模型中, 句子可以被视为一个特殊的有向无环图, 其中每个结点的入度和出度均为1.然而, 对于词格, 每个结点的入度和出度则至少为1, 因此基于RNN的序列建模模型<citation id="427" type="reference"><link href="334" rel="bibliography" /><link href="336" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>无法处理词格结构的输入数据<citation id="428" type="reference"><link href="338" rel="bibliography" /><link href="340" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="181">在浅层、深层带权词格模型的基础上, 我们进一步提出了使用融合函数来融合循环单元的输入或输出, 生成单一的压缩表示, 以转换成标准循环单元能够接受的输入形式.这里, 本文在文献<citation id="429" type="reference">[<a class="sup">15</a>]</citation>中2种融合函数的基础上, 进一步提出2种基于词格边权重的融合函数.为了不失一般性, 本文以深层带权词格中的<b><i>h</i></b><sub><i>t</i></sub>为例, 描述在带权词格GRU单元中如何使用这些融合函数.需要注意的是, 这些定义同样适用于生成其他向量, 例如<b><i>x</i></b><sub><i>t</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="182">首先介绍文献<citation id="430" type="reference">[<a class="sup">15</a>]</citation>中2种基础的融合函数:池化融合函数与门机制融合函数;接着, 介绍本文提出的以门机制为基础的2种新融合函数.</p>
                </div>
                <h4 class="anchor-tag" id="183" name="183">1) 池化融合函数</h4>
                <div class="p1">
                    <p id="184">与文献<citation id="431" type="reference">[<a class="sup">15</a>,<a class="sup">16</a>]</citation>中的做法类似, 我们使用一个最大池化 (max pooling) 运算来融合{<mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover></math></mathml><sub><i>t</i><sub><i>k</i></sub></sub>}.池化运算能够自动捕捉最重要的分词状态信息用于句子建模.形式上, 基于池化运算的融合函数定义为</p>
                </div>
                <div class="p1">
                    <p id="186"><mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>Κ</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, (20) </p>
                </div>
                <div class="p1">
                    <p id="188">其中, max (*) 是一个逐元素最大值函数.</p>
                </div>
                <div class="p1">
                    <p id="189">池化融合函数忽略了词格的边权重信息, 直接通过聚集入度边集合对应的隐状态来获取最重要的特征.</p>
                </div>
                <h4 class="anchor-tag" id="190" name="190">2) 门机制融合函数</h4>
                <div class="p1">
                    <p id="191">目前, 门机制已经大量应用于神经网络中, 用以自动学习不同输入信息的权重.与文献<citation id="432" type="reference">[<a class="sup">16</a>]</citation>相似, 该融合函数在形式上定义为</p>
                </div>
                <div class="p1">
                    <p id="192"><mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mfrac><mrow><mi>σ</mi><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub><mi mathvariant="bold-italic">u</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>g</mtext><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>g</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>σ</mi></mstyle><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><msup><mi>k</mi><mo>′</mo></msup></msub></mrow></msub><mi mathvariant="bold-italic">u</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>g</mtext><mo stretchy="false">) </mo></mrow></msup><mo>+</mo><mi>b</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>g</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></math></mathml>, (21) </p>
                </div>
                <div class="p1">
                    <p id="194">其中, <b><i>u</i></b><sup> (g) </sup>和<i>b</i><sup> (g) </sup>分别是门机制融合函数的参数向量和偏置项标量, 上标g表示门.</p>
                </div>
                <div class="p1">
                    <p id="195">门机制融合函数则计算每个隐状态的归一化分数, 作为边的权重, 对隐状态进行加权平均.这个分数可以视为动态生成的边权重, 表示模型将该边作为候选分词的置信度.</p>
                </div>
                <h4 class="anchor-tag" id="196" name="196">3) 基于词格边权重的融合函数</h4>
                <div class="p1">
                    <p id="197">带权词格的一大特点是边权重可以有效区分不同分词结果的可能性.基于词格边权重, 我们将<b><i>h</i></b><sub><i>t</i></sub>定义为不同分词结果的隐状态的加权和, 即:</p>
                </div>
                <div class="p1">
                    <p id="198"><mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>w</mi></mstyle><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><msub><mrow></mrow><mrow><mi>e</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub><mo>:</mo><mi>t</mi></mrow></msub></mrow></msub><mover accent="true"><mi mathvariant="bold-italic">Η</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msub></mrow></math></mathml>, (22) </p>
                </div>
                <div class="p1">
                    <p id="200">其中, <i>weight</i><sub><i>e</i><sub><i>t</i><sub><i>k</i></sub>:<i>t</i></sub></sub>是根据式 (3) 计算出的边权重.显然, 在这种融合方式中, 融合权重主要取决于词格本身, 而独立于网络模型.</p>
                </div>
                <div class="p1">
                    <p id="201">与门机制融合函数不同, 基于词格边权重的融合函数使用1.1节所述算法计算的词格边权重, 对隐状态进行加权平均.同门机制生成的动态权重相比, 词格边权重是静态的, 可以直接表示边上的词作为候选分词的可能性.</p>
                </div>
                <h4 class="anchor-tag" id="202" name="202">4) 融入词格边权重的门机制融合函数</h4>
                <div class="p1">
                    <p id="203">该融合函数与2) 基于门机制的融合函数相类似.不同的地方在于, 门机制融合函数是无监督的, 直接受模型训练目标影响, 而相比之下, 基于词格边权重的门机制融合函数则利用词格边权重作为外部监督信息来改进门机制学习到的融合权重.具体而言, 我们要求门机制学习到的融合权重与词格边权分布尽量接近.为此, 本文进一步引入门机制权重与词格边权重的欧式距离来作为惩罚项:</p>
                </div>
                <div class="p1">
                    <p id="204"><mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>D</mi><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>D</mi></mrow></munder><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">g</mi><msubsup><mrow></mrow><mi>t</mi><mi>s</mi></msubsup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">W</mi><mo>˜</mo></mover><msubsup><mrow></mrow><mi>t</mi><mi>s</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>, (23) </p>
                </div>
                <div class="p1">
                    <p id="206">其中, <i>D</i>表示训练语料, <i>N</i><sub><i>s</i></sub>是句子<i>s</i>的字个数, <b><i>g</i></b><mathml id="207"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>s</mi></msubsup></mrow></math></mathml>是门机制融合函数中的权重分布向量.特别地, 为了保证2个分布具有可比性, 本文将词格的边权重集合{<i>weight</i><sub><i>e</i><sub><i>t</i><sub><i>k</i></sub>:<i>t</i></sub></sub><citation id="433">|0</citation>≤<i>k</i>&lt;<i>K</i>}进行归一化, 生成向量<mathml id="208"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">W</mi><mo>˜</mo></mover></math></mathml><mathml id="209"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>s</mi></msubsup></mrow></math></mathml>.</p>
                </div>
                <div class="p1">
                    <p id="210">融入词格边权重的门机制融合函数进一步使用静态边权重作为正则化项, 指导动态边权重的生成, 这一方法可以视为门机制融合函数与基于词格边权重的融合函数的结合.</p>
                </div>
                <div class="p1">
                    <p id="211">上述4种融合函数, 各自以递进的方式, 从静态和动态到动静态结合地利用词格边权重, 从而充分发挥模型的运算能力和利用词格结构提供的监督信息.</p>
                </div>
                <h3 id="212" name="212" class="anchor-tag"><b>3 模型目标和训练</b></h3>
                <div class="p1">
                    <p id="213">基于带权词格的GRU模型的训练过程与标准RNN相同.模型目标函数与后续所应用任务紧密相关.对于分类任务, 本文模型首先建模学习句子语义表示, 然后通过一个softmax层来预测句子的标签分布:</p>
                </div>
                <div class="p1">
                    <p id="214"><mathml id="215"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">p</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo stretchy="false"> (</mo><mi>s</mi></mrow></math></mathml>;<i>θ</i>) =<i>softmax</i> (<b><i>W</i></b><sup> (y) </sup><b><i>h</i></b><sub><i>N</i><sub><i>s</i></sub></sub>+<b><i>b</i></b><sup> (y) </sup>) , (24) </p>
                </div>
                <div class="p1">
                    <p id="216">其中, <i>θ</i>代表模型参数;<b><i>h</i></b><sub><i>N</i><sub><i>s</i></sub></sub>∈R<sup><i>d</i></sup>是句子<i>s</i>在时刻<i>t</i>的隐状态, 作为句子的向量表示;<b><i>W</i></b><sup> (y) </sup>和<b><i>b</i></b><sup> (y) </sup>分别是softmax层的参数矩阵和偏置项向量, 上标y表示该层的输出用于预测标签.设数据中共有<i>L</i>个候选标签, <mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">p</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo stretchy="false"> (</mo><mi>s</mi></mrow></math></mathml>;<i>θ</i>) ∈R<sup><i>L</i></sup>为模型建模的概率分布, 并且满足<mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></mstyle><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false"> (</mo><mi>s</mi></mrow></math></mathml>;<i>θ</i>) =1.给定训练数据<i>D</i>, 模型的目标函数最终定义为</p>
                </div>
                <div class="area_img" id="219">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JFYZ201904018_21900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="221">其中, <i>p</i><sup><i>l</i></sup> (<i>s</i>) 是句子真实标签的one-hot向量的第<i>l</i>个分量, <i>R</i><sub>gate</sub>是根据式 (23) 定义的惩罚项.当本文模型使用前3种融合函数时, <i>λ</i>=0;反之, 当使用第4种融合函数时, <i>λ</i>为一个大于0的常数.</p>
                </div>
                <div class="p1">
                    <p id="222">本文采用基于Adadelta<citation id="434" type="reference"><link href="360" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>的随机梯度下降算法来优化模型.此外, 本文在训练过程中使用dropout<citation id="435" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和最大范数正则化<citation id="436" type="reference"><link href="364" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>来防止模型训练过拟合.</p>
                </div>
                <h3 id="223" name="223" class="anchor-tag"><b>4 实验与分析</b></h3>
                <div class="p1">
                    <p id="224">为了验证本文模型的有效性, 我们将2种基于带权词格的GRU循环神经网络和4种融合策略, 分别应用于情感分类和问句分类任务, 与传统GRU及现有的其他模型进行比较.</p>
                </div>
                <h4 class="anchor-tag" id="225" name="225"><b>4.1 任务和数据集</b></h4>
                <div class="p1">
                    <p id="226">本文将在情感分类和问句分类2个数据集上测试我们提出的方法.下面从数据集大小和数据特点等方面分别介绍这2个数据集.</p>
                </div>
                <h4 class="anchor-tag" id="227" name="227">1) 情感分类</h4>
                <div class="p1">
                    <p id="228">数据集来自于新浪微博, 为了保证数据信息的充分性, 我们删除长度不足6个字的句子, 然后安排2名标注人员对句子按照不同的情感 (消极、中性和积极) 倾向进行独立标注, 最后保留标注结果完全一致的数据作为实验数据.按照上述方式, 本任务实验数据集共包含消极情感句子4 454条、中性情感句子5 100条和积极情感句子5 594条.然后, 本文采取分层抽样的方式, 按照7∶1∶2的比例从每个类别随机抽取样本, 将数据划分为训练集 (10 603条实例) 、验证集 (1 514条实例) 和测试集 (3 031条实例) .句子的平均长度17.19个词或25.69个字.</p>
                </div>
                <h4 class="anchor-tag" id="476" name="476">2) 问句分类</h4>
                <div class="p1">
                    <p id="477">数据来自FudanQuestionBank<sup> (1) </sup>提供的中文问句分类数据集.为了降低数据类别不均衡问题的影响, 本文只选取数据量最大的5个分类.该数据包含1 517, 4 987, 1 101, 3 185, 2 174条文本, 对应的类别分别为枚举、事实、评价、推荐和需求.同样, 本文对该数据集按照7∶1∶2的比例划分为训练集 (9 075条实例) 、验证集 (1 297条实例) 和测试集 (2 592条实例) .平均长度为9.33个词或14.60个字.</p>
                </div>
                <h4 class="anchor-tag" id="478" name="478">3) 带权词格生成</h4>
                <div class="p1">
                    <p id="479">本文使用北京大学 (PKU) 、宾州大学中文树库 (CTB) 以及微软研究院 (MSR) 的分词语料分别训练3个分词模型, 然后按照1.1节中所述方法生成每个句子的带权词格.</p>
                </div>
                <h4 class="anchor-tag" id="229" name="229"><b>4.2 实验设置</b></h4>
                <div class="p1">
                    <p id="230">本文所考察的对比模型包括:</p>
                </div>
                <h4 class="anchor-tag" id="231" name="231">1) GRU</h4>
                <div class="p1">
                    <p id="232">GRU<citation id="437" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的最后一个序列状态作为句子语义表示用于预测句子标签.另外, 除了最简单的单层单向GRU模型之外, 本文还同时比较了3个GRU的简单变种模型:双层单向 (2 layer GRU, 2L-GRU) 、单层双向 (bidirectional GRU, BiGRU) 和双层双向 (2 layer bidirectional GRU, 2L-BiGRU) 模型.</p>
                </div>
                <h4 class="anchor-tag" id="233" name="233">2) LSTM</h4>
                <div class="p1">
                    <p id="234">LSTM<citation id="438" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的实验设置与GRU模型相同.这一对比实验的目的是验证GRU与LSTM的性能, 证明2个RNN的变种模型在本文2个任务上的效果相近.</p>
                </div>
                <h4 class="anchor-tag" id="235" name="235">3) CNN</h4>
                <div class="p1">
                    <p id="236">卷积神经网络 (convolutional neural network, CNN) <citation id="439" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>使用不同大小的窗口处理输入序列, 能够获得句子在不同粒度, 包括字、词语甚至短语级别的语义信息.本文参考Kim<citation id="440" type="reference"><link href="366" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的实验设置, 使用单层卷积神经网络模型.</p>
                </div>
                <h4 class="anchor-tag" id="237" name="237">4) DCNN</h4>
                <div class="p1">
                    <p id="238">动态卷积神经网络 (dynamic convolutional neural network, DCNN) <citation id="441" type="reference"><link href="368" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>通过利用动态<i>k</i>最大池化操作, 具有与RNN相似的处理变长序列, 以及捕捉句子内部长短距离依赖关系的能力.DCNN使用2个卷积层, <i>k</i>最大池化操作的<i>k</i>=4.</p>
                </div>
                <h4 class="anchor-tag" id="239" name="239">5) RAE</h4>
                <div class="p1">
                    <p id="240">RAE (recursive autoencoder) <citation id="442" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>通过贪婪方式构造文本序列的树结构, 并将树的根结点作为该句子的向量表示.RAE能够建模序列中词与词之间的组合顺序关系, 学习句子内部成分的结构特征.模型参数参考Socher等人<citation id="443" type="reference"><link href="370" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>的实验设置.</p>
                </div>
                <h4 class="anchor-tag" id="241" name="241">6) MulSrc</h4>
                <div class="p1">
                    <p id="242">MulSrc (multiple source) 独立地建模句子的字序列及词序列, 最终通过2.3节所述的融合函数将句子表示进行一次融合, 生成句子的语义表示.MulSrc可以同时基于字和词建模, 是本文模型的简化版本.与词格不同, 由于不存在句子级别的权重, 我们简单地使用平均分布作为加权系数 (Avg) , 模拟2.3节中基于边权重的融合函数, 与本文所提出的带权词格GRU模型进行对比.</p>
                </div>
                <h4 class="anchor-tag" id="243" name="243">7) SWWL-GRU和DWWL-GRU</h4>
                <div class="p1">
                    <p id="244">本文提出的基于带权词格的GRU循环神经网络模型在4种融合函数上进行了实验, 相应的模型分别记为SWWL (Pool) , SWWL (Gate) , SWWL (Weight) , SWWL (wGate) , DWWL (Pool) , DWWL (Gate) , DWWL (Weight) , DWWL (wGate) .</p>
                </div>
                <div class="p1">
                    <p id="245">此外, 字序列与词序列相比, 是更简单的一种句子表示形式.为了研究这种表示是否有助于文本语义建模, 本文同样引入字序列到SWWL-GRU和DWWL-GRU的词格中, 并与之进行对比实验与分析.</p>
                </div>
                <div class="p1">
                    <p id="246">在实验参数方面, 本文统一使用dropout<citation id="444" type="reference"><link href="362" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>防止模型训练过拟合, 并根据验证集结果对dropout值进行选择.我们根据验证集挑选式 (22) 中调节惩罚项的<i>λ</i>值, 将其设为1.0.词表由语料中出现次数在2次及以上的高频词构成.词向量和隐状态的维度分别为50和300维.所有模型均使用基于随机梯度下降的Adadelta算法<citation id="445" type="reference"><link href="360" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>实现优化, 批梯度更新的大小为1.每个模型分别训练5次, 根据开发集的效果选择最优模型, 并取测试集上的平均准确率作为最终结果.</p>
                </div>
                <h4 class="anchor-tag" id="247" name="247"><b>4.3 实验结果分析</b></h4>
                <div class="p1">
                    <p id="248">表1和表2分别给出了基线模型与本文模型在情感分类和问句分类任务上的实验结果.从表1和表2中数据可以看出, 本文模型的分类效果要显著高于单一字序列或词序列的模型.</p>
                </div>
                <div class="p1">
                    <p id="249">从表1和表2可得出5条结论:</p>
                </div>
                <h4 class="anchor-tag" id="250" name="250">1) GRU和LSTM的性能相近</h4>
                <div class="p1">
                    <p id="251">GRU<citation id="446" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>与LSTM<citation id="447" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>模型, 是针对梯度消失和梯度爆炸问题<citation id="448" type="reference"><link href="346" rel="bibliography" /><link href="348" rel="bibliography" /><link href="350" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>所提出的2个RNN<sup><a class="sup">[3]</a></sup>变种模型.在本文实验中, GRU和LSTM在2个数据集上的性能没有表现出显著差异, 然而GRU模型具有更少的参数, 因此在某种程度上降低了过拟合的风险.</p>
                </div>
                <div class="area_img" id="252">
                    <p class="img_tit"><b>表1 基线模型的情感分类和问句分类实验结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Results of Baseline Models on Sentiment Classification and Question Classification</b></p>
                    <p class="img_note"></p>
                    <table id="252" border="1"><tr><td rowspan="2"><br />Model</td><td colspan="4"><br />Sentiment</td><td colspan="4">Question</td></tr><tr><td><br />Char</td><td>CTB</td><td>MSR</td><td>PKU</td><td>Char</td><td>CTB</td><td>MSR</td><td>PKU</td></tr><tr><td><br />GRU<sup>[5]</sup></td><td>70.8</td><td>73.3</td><td><b>73.7</b></td><td>72.8</td><td>86.4</td><td>86.2</td><td>86.1</td><td>84.8</td></tr><tr><td><br />LSTM<sup>[4]</sup></td><td>69.2</td><td>73.2</td><td>73.3</td><td>72.6</td><td><b>86.6</b></td><td><b>86.3</b></td><td>86.0</td><td>84.8</td></tr><tr><td><br />2L-GRU</td><td>69.5</td><td><b>73.7</b></td><td>73.1</td><td>72.6</td><td>86.5</td><td>86.2</td><td><b>86.4</b></td><td>84.5</td></tr><tr><td><br />BiGRU</td><td><b>71.4</b></td><td><b>73.7</b></td><td>73.5</td><td><b>73.1</b></td><td><b>86.6</b></td><td>86.0</td><td>85.9</td><td><b>86.2</b></td></tr><tr><td><br />2L-BiGRU</td><td>70.7</td><td>72.6</td><td>72.6</td><td>72.4</td><td>86.2</td><td>86.2</td><td>86.0</td><td>85.0</td></tr><tr><td><br />CNN<sup>[20]</sup></td><td>65.1</td><td>69.0</td><td>68.0</td><td>68.1</td><td>82.4</td><td>81.5</td><td>80.1</td><td>79.9</td></tr><tr><td><br />DCNN<sup>[21]</sup></td><td>66.4</td><td>64.8</td><td>62.2</td><td>65.2</td><td>84.2</td><td>80.6</td><td>80.5</td><td>71.9</td></tr><tr><td><br />RAE<sup>[22]</sup></td><td>59.9</td><td>68.6</td><td>68.8</td><td>68.4</td><td>72.1</td><td>79.1</td><td>79.2</td><td>78.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Notes:The values in boldface indicate the best accuracy in that experimental group.</p>
                </div>
                <div class="area_img" id="253">
                    <p class="img_tit"><b>表2 本文模型的情感分类和问句分类实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Results of Our Work on Sentiment Classification and Question Classification</b></p>
                    <p class="img_note"></p>
                    <table id="253" border="1"><tr><td rowspan="2"><br />Model</td><td rowspan="2">Gating</td><td colspan="2"><br />Sentiment</td><td colspan="2">Question</td></tr><tr><td><br />Words</td><td>Char+Words</td><td>Words</td><td>Char+Words</td></tr><tr><td><br /></td><td>Pool</td><td>72.1</td><td>71.7</td><td>85.5</td><td>85.9</td></tr><tr><td><br />MulSrc</td><td>Avg</td><td>71.7</td><td>71.9</td><td><b>85.7</b></td><td>86.1</td></tr><tr><td><br /></td><td>Gate</td><td><b>72.5</b></td><td><b>72.6</b></td><td>85.5</td><td><b>86.2</b></td></tr><tr><td rowspan="8"><br />Ours</td><td><br />SWWL (Pool) </td><td>73.7</td><td>72.8</td><td>86.3</td><td>87.2</td></tr><tr><td><br />SWWL (Gate) </td><td>73.8</td><td>73.1</td><td>86.2</td><td>87.3</td></tr><tr><td><br />SWWL (Weight) </td><td>74.0</td><td>74.0</td><td>86.5</td><td>87.1</td></tr><tr><td><br />SWWL (wGate) </td><td>73.7</td><td>73.3</td><td>86.3</td><td>87.4</td></tr><tr><td><br />DWWL (Pool) </td><td>74.3</td><td>73.9</td><td><b>87.0</b></td><td>87.1</td></tr><tr><td><br />DWWL (Gate) </td><td>72.3</td><td>71.6</td><td>84.8</td><td>85.5</td></tr><tr><td><br />DWWL (Weight) </td><td>74.1</td><td><b>74.2</b></td><td>86.8</td><td>87.2</td></tr><tr><td><br />DWWL (wGate) </td><td><b>74.6</b></td><td>73.6</td><td>86.8</td><td><b>87.7</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note">Notes:The values in boldface indicate the best accuracy in that experimental group.</p>
                </div>
                <h4 class="anchor-tag" id="254" name="254">2) 基于词序列的GRU及其变种模型一致地优于基于字序列的模型</h4>
                <div class="p1">
                    <p id="255">分词在中文等没有词语分隔符的自然语言处理任务中具有非常重要的作用, 这是因为分词可以在一定程度上消除纯字序列存在的语义歧义现象.相比之下, 基于字序列的模型忽略了句子中的词语边界信息, 从而无法消除句子中存在的语义歧义, 导致模型学习到的句子语义表示不能很好地服务于分类任务.然而, 模型在问题分类任务上的结果并没有明显地反映出这一趋势.据统计结果显示, GRU具备强大的捕捉长短期依赖的能力, 但对于短序列而言, 短期依赖则占据了主要地位.这使得循环神经网络无法发挥其建模长期依赖关系的优势, 从而对以短句为主的问句分类任务, 弱化了基于词序列与基于字序列的模型在结果上的差异.</p>
                </div>
                <h4 class="anchor-tag" id="256" name="256">3) 基于CNN的模型效果弱于基于RNN的模型</h4>
                <div class="p1">
                    <p id="257">正如文献<citation id="449" type="reference">[<a class="sup">23</a>]</citation>所示, 相比于CNN, RNN模型对长序列文本的建模优势较为明显.尽管CNN在速度上具有明显优势, 但在性能表现上却难以取代RNN.就表1中实验结果来说, CNN由于同时使用多个卷积核, 使其在某种程度上能够捕捉所有的多元词串, 从而与本文所提出的模型一样具备建模不同分词结果的能力.然而, 并非所有多元词串都能表示一般意义上的有效词语, 因此CNN也同时引入了更多的错误分词, 导致基于CNN的模型在2个任务上的表现均明显不如RNN模型</p>
                </div>
                <h4 class="anchor-tag" id="258" name="258">4) 基于带权词格的模型优于基于字、词序列以及MulSrc的模型</h4>
                <div class="p1">
                    <p id="259">相比于对比模型, 本文提出的2个模型在情感分类和问句分类任务上均一致取得了更高的准确率.首先, 只基于字和词建模的模型, 缺乏表达分词多样性的能力;其次, 同时基于字和词建模的MulSrc模型, 由于仅在句级别融合句子语义表示, 使得句子的最小单元无法在字、词序列间进行交互.此外, 在大部分情况下, DWWL-GRU性能超过SWWL-GRU, 取得了2个任务上的最好结果, 这证明深层次的语义融合比浅层次的语义融合效果更好.此外, 本文提出的2个模型在使用融入词格边权重的门机制融合函数上均取得最好结果, 其次分别是门机制融合函数, 以及基于词格边权重的融合函数.这一实验结果与我们的直觉相符.首先, 门机制是无监督的权重学习, 而基于词格边权重的融合函数则直接根据词格边权重来进行加权融合.相比之下, 融入词格边权重的门机制融合函数, 有效结合了上述2种融合机制的特点, 进一步提高了所生成权重的质量, 从而得到了更好的融合文本语义表示.</p>
                </div>
                <div class="area_img" id="260">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JFYZ201904018_260.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 一个例句的语义建模" src="Detail/GetImg?filename=images/JFYZ201904018_260.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 一个例句的语义建模  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JFYZ201904018_260.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 5 The semantic modeling of an example sentence</p>

                </div>
                <h4 class="anchor-tag" id="261" name="261">5) 融合函数的建模能力影响模型性能</h4>
                <div class="p1">
                    <p id="262">引入字信息后, 基于带权词格的模型在问句分类任务上的效果得到进一步提升, 但情感分类任务的效果却降低.直观上看, 字信息的引入能够有效扩充词格的信息量.但实际而言, 情感分类的词表大小为42 685, 问句分类则只有11 634, 因此情感分类任务的词表更大, 更难学习到有效的句子表示.在情感分类上引入字后, 词格模型所要建模的分词组合数量进一步增加.我们的融合函数无法充分建模所有相应的分词情况, 从而加剧了数据稀疏问题的影响.问句分类任务则恰恰相反, 单纯就词表大小而言, 即使引入字, 词格模型中可能的分词组合数量也远远低于情感分类.因此与Words相比, 我们的模型对于问句分类任务可以在Char+Words上更有效地建模, 充分利用引入字后的词格信息增益, 进而提升模型效果.实际上, 本文提出的4种融合函数中最复杂的wGate融合函数, 依然只包含一个与隐状态同等维度的参数向量, 所以建模能力有限.因此, 一个更复杂的融合函数应当能够在情感分类的Char+Words上进一步改进模型的性能.但为了证明基于带权词格的循环神经网络模型相对于传统基于词序列模型的有效性, 我们在尽量不引入额外参数的前提下, 保证融合函数足够简单.本文的讨论范围限于验证基于带权词格模型的有效性, 因此我们将对具有更强学习能力的融合函数的研究放到未来工作中深入探讨.</p>
                </div>
                <div class="p1">
                    <p id="263">为了探究所提出模型的工作机制, 以性能最好的DWWL (wGate) 为例, 我们在图5中展示了一个句子的文本建模结果.在所示词格中, 每条边标注有一个分数, 该分数为模型动态生成的权重, 表示该边所对应的词, 在特定上下文中被作为一个候选分词的可能性, 该权重直接影响模型的文本语义表示建模质量.</p>
                </div>
                <div class="p1">
                    <p id="264">图5中所示为句子:“不然肯定是纳税人白花冤枉钱.”的建模结果.句中存在歧义的部分集中在<i>v</i><sub>8</sub>～<i>v</i><sub>13</sub>部分, 即“白花冤枉钱”这一片段, 根据上下文, 我们判断其正确的分词结果应当为“白花/冤枉钱”或“白花/冤枉/钱”.图5中粗边表示错误的候选分词, 实边表示正确的候选分词.可以观察到, 词格中存在来自不同分词模型产生的错误分词, 如“白花冤”和“枉钱”.结点<i>v</i><sub>13</sub>有3条入度边, 分别对应:“钱”、“枉钱”、“冤枉钱”3个候选分词.其中, 正确的分词“钱”和“冤枉钱”被作为候选词的置信度<i>p</i>为0.36和0.35;而错误分词“枉钱”的置信度<i>p</i>只有0.29.尽管“白花冤”在结点<i>v</i><sub>11</sub>的置信度为1.00, 但由于错误分词“枉钱”存在于“白花冤”的分词路径中, 因此该路径依然得到了更低的分数.我们可以将模型建模的边置信度视为概率, 通过路径的概率来更好地理解这一示例.图5中包含错误分词“白花冤/枉钱”的路径, 其概率<i>p</i> (false) =1.00×0.29=0.29.而包含正确分词“白花/冤枉/钱”和“白花/冤枉钱”的路径, 通过将其概率相加, 可知正确路径的总概率为<i>p</i> (true) =1.00×1.00×0.36+1.00×0.35=0.71.因而在示例中正确路径的置信度是远高于错误路径的.不难看出, 词格模型具有容错的能力, 当错误的候选分词被赋予低权重后, 错误路径的权重被降低, 而正确路径所产生的影响通过高权重放大, 从而减轻纯词序列中分词错误传播的问题.另一方面, 单纯基于字和词序列的建模方法, 则易受到错误分词的影响, 而基于带权词格的模型则能够利用其容错能力来保证即使存在错误分词, 模型仍然能够学习到高质量的句子语义表示.</p>
                </div>
                <h3 id="265" name="265" class="anchor-tag"><b>5 相关工作</b></h3>
                <div class="p1">
                    <p id="266">目前, 基于深度神经网络的文本语义表示学习已经成为自然语言处理的热门研究方向.其中, 神经词袋 (bag-of-words) 模型是最为简单的一个模型, 它对句子中所有词的词向量取平均直接得到句子的语义表示向量.显然, 这种建模方式忽略了对文本语义表示极为重要的词序信息.因而, 许多研究者转向研究考虑词序信息的模型, 包括序列神经网络模型和拓扑神经网络模型等.典型的序列神经网络模型包括RNN<sup><a class="sup">[3]</a></sup>, LSTM<citation id="450" type="reference"><link href="334" rel="bibliography" /><link href="374" rel="bibliography" /><link href="376" rel="bibliography" /><link href="378" rel="bibliography" /><link href="380" rel="bibliography" /><link href="382" rel="bibliography" /><link href="384" rel="bibliography" /><link href="386" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">24</a>,<a class="sup">25</a>,<a class="sup">26</a>,<a class="sup">27</a>,<a class="sup">28</a>,<a class="sup">29</a>,<a class="sup">30</a>]</sup></citation>, 以及带门机制的其他变形<citation id="451" type="reference"><link href="388" rel="bibliography" /><link href="390" rel="bibliography" /><link href="392" rel="bibliography" /><sup>[<a class="sup">31</a>,<a class="sup">32</a>,<a class="sup">33</a>]</sup></citation>.而与序列神经网络模型不同, 拓扑神经网络模型依赖给定的词间拓扑结构来建模生成文本语义表示<citation id="452" type="reference"><link href="370" rel="bibliography" /><link href="394" rel="bibliography" /><link href="396" rel="bibliography" /><link href="398" rel="bibliography" /><sup>[<a class="sup">22</a>,<a class="sup">34</a>,<a class="sup">35</a>,<a class="sup">36</a>]</sup></citation>.例如句子的依存和组合范畴语法可被作为骨架用于学习句子语义表示<citation id="453" type="reference"><link href="382" rel="bibliography" /><link href="400" rel="bibliography" /><link href="402" rel="bibliography" /><link href="404" rel="bibliography" /><sup>[<a class="sup">28</a>,<a class="sup">37</a>,<a class="sup">38</a>,<a class="sup">39</a>]</sup></citation>.进一步, 一些研究者提出多维度的神经网络模型, 该类模型将文本组织成一个多维网格而非序列作为输入<citation id="454" type="reference"><link href="406" rel="bibliography" /><link href="408" rel="bibliography" /><sup>[<a class="sup">40</a>,<a class="sup">41</a>]</sup></citation>.此外, 除了上述模型, 卷积神经网络也被用于句子建模<citation id="455" type="reference"><link href="366" rel="bibliography" /><link href="368" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>.该类网络也是以词向量序列作为输入, 建模过程中通过多层的卷积和池化操作来得到句子语义表示.</p>
                </div>
                <div class="p1">
                    <p id="267">在上述工作中, 与本文较为相关的工作主要有文献<citation id="460" type="reference">[<a class="sup">15</a>,<a class="sup">27</a>,<a class="sup">28</a>]</citation>中所提出的模型.文献<citation id="461" type="reference">[<a class="sup">27</a>,<a class="sup">28</a>]</citation>在本质上属于拓扑神经网络模型, 分别将序列LSTM扩展到树结构和森林结构的网络.文献<citation id="456" type="reference">[<a class="sup">40</a>]</citation>提出了基于网格的LSTM, 把LSTM单元按照多维网格的方式排列, 以应用到一维、二维甚至更多维度的序列数据的语义建模学习.此外, 文献<citation id="457" type="reference">[<a class="sup">42</a>]</citation>提出在生成当前隐状态时, 对RNN中多个前隐状态使用与本文门机制相似的方式分别计算权重, 然后将多个前隐状态加权输入到RNN单元.文献<citation id="458" type="reference">[<a class="sup">15</a>]</citation>提出基于词格的循环神经网络, 通过Pooling运算和门机制来融合生成词格单元的输入.不同于这些网络, 本文工作在文献<citation id="459" type="reference">[<a class="sup">15</a>]</citation>的基础上进行扩展, 引入了带权词格来提高句子建模的能力, 更重要的是本文模型引入词格权重来指导融合函数的建模学习, 进一步提高词格循环神经网络语义表示的学习效果.</p>
                </div>
                <h3 id="268" name="268" class="anchor-tag"><b>6 总 结</b></h3>
                <div class="p1">
                    <p id="269">文本提出了2种基于带权词格的GRU循环神经网络模型, 用于句子的语义表示建模.2种模型均以带权词格为基础, 利用任意数量的输入词和前隐状态信息来融合生成当前隐状态, 最终得到句子语义表示.在以句子语义表示为基础的情感分类和问句分类2个任务上的实验结果证明了本文模型的有效性.</p>
                </div>
                <div class="p1">
                    <p id="270">未来, 我们将在下面3个研究方向展开工作:</p>
                </div>
                <div class="p1">
                    <p id="271">1) 研究如何把带权词格集成到其他神经网络中, 例如卷积神经网络等;</p>
                </div>
                <div class="p1">
                    <p id="272">2) 融入词格边权重的门机制融合函数虽然取得最好效果, 但与其他融合函数相比优势有限, 如何设计其他更加有效融合函数也是下一步工作的重点之一;</p>
                </div>
                <div class="p1">
                    <p id="273">3) 本文所使用构造词格的方法较为简单, 因此, 我们将尝试使用其他的语言学信息构造词格, 以进一步提升模型性能.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="332">
                            <a id="bibliography_" target="_blank" href="http://scholar.cnki.net/result.aspx?q=基于多通道卷积神经网络的中文微博情感分析">

                                <b>[]</b>.[1]Yu Kai, Jia Lei, Chen Yuqiang, et al.Deep learning:Yesterday, today, and tommorrow[J].Journal of Computer Research and Development, 2013, 50 (9) :1799-1804 (in Chinese) (余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804) [2]Chen Ke, Liang Bin, Ke Wende, et al.Chinese micro-blog sentiment analysis based on multi-channels convolutional neural networks[J].Journal of Computer Research and Development, 2018, 55 (5) :945-957 (in Chinese) (陈珂, 梁斌, 柯文德, 等.基于多通道卷积神经网络的中文微博情感分析[J].计算机研究与发展, 2018, 55 (5) :945-957) [3]Mikolov T, Karafiát M, Burget L, et al.Recurrent neural network based language model[C]//Proc of the 11th Annual Conf of the Int Speech Communication Association.Phoenix, Arizona:ISCA, 2010:1045-1048
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTI1MzdCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2TElGOFVieEE9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4bw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning phrase representations using RNN encoder-decoder for statistical machine translation">

                                <b>[5]</b>Cho K, Van Merri3nboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:1724-1734
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalizing word lattice translation">

                                <b>[6]</b>Dyer C, Muresan S, Resnik P.Generalizing word lattice translation[C]//Proc of the 46th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2008:1012-1020
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LatticeRNN:Recurrent neural networks over lattices">

                                <b>[7]</b>Ladhak F, Gandhe A, Dreyer M, et al.LatticeRNN:Recurrent neural networks over lattices[C]//Proc of the17th Annual Conf of the Int Speech Communication Association.Phoenix, Arizona:ISCA, 2016:695-699
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coarse-to-fine n-best parsing and MaxEnt discriminative reranking">

                                <b>[8]</b>Charniak E, Johnson M.Coarse-to-fine n-best parsing and MaxEnt discriminative reranking[C]//Proc of the 43rd Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2005:173-180
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Forest reranking:Discriminative parsing with non-local features">

                                <b>[9]</b>Huang Liang.Forest reranking:Discriminative parsing with non-local features[C]//Proc of the 46th Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2008:586-594
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning long-term dependencies with gradient descent is difficult">

                                <b>[10]</b>Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult[J].IEEETransactions on Neural Networks, 1994, 5 (2) :157-166
                            </a>
                        </p>
                        <p id="348">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The problem of learning long-term dependencies in recurrent networks">

                                <b>[11]</b>Bengio Y, Frasconi P, Simard P.The problem of learning long-term dependencies in recurrent networks[C]//Proc of IEEE Int Conf on Neural Networks.Piscataway, NJ:IEEE, 1993:1183-1188
                            </a>
                        </p>
                        <p id="350">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The difficulty of training deep architectures and the effect of unsupervised pre-training">

                                <b>[12]</b>Erhan D, Manzagol P A, Bengio Y, et al.The difficulty of training deep architectures and the effect of unsupervised pretraining[C]//Proc of the 12th Int Conf on Artificial Intelligence and Statistics.Cambridge, MA:MIT Press, 2009:153-160
                            </a>
                        </p>
                        <p id="352">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Word Lattice Reranking for Chinese Word Segmentation and Part-ofSpeech Tagging">

                                <b>[13]</b>Jiang Wenbin, Mi Haitao, Liu Qun.Word lattice reranking for Chinese word segmentation and part-of-speech tagging[C]//Proc of the 22nd Int Conf on Computational Linguistics.Stroudsburg, PA:ACL, 2008:385-392
                            </a>
                        </p>
                        <p id="354">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Lattice-based Framework for Joint Chinese Word Segmentation,POS Tagging and Parsing">

                                <b>[14]</b>Wang Zhiguo, Zong Chengqing, Xue Nianwen.A latticebased framework for joint Chinese word segmentation, pos tagging and parsing[C]//Proc of the 51st Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2013:623-627
                            </a>
                        </p>
                        <p id="356">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lattice-based recurrent neural network encoders for neural machine translation">

                                <b>[15]</b>Su Jinsong, Tan Zhixing, Xiong Deyi, et al.Lattice-based recurrent neural network encoders for neural machine translation[C]//Proc of the 31st AAAI Conf on Artificial Intelligence.Menlo Park, CA:AAAI, 2017:3302-3308
                            </a>
                        </p>
                        <p id="358">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The forest convolutional network:Compositional distributional semantics with a neural chart and without binarization">

                                <b>[16]</b>Le P, Zuidema W.The forest convolutional network:Compositional distributional semantics with a neural chart and without binarization[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:1155-1164
                            </a>
                        </p>
                        <p id="360">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ADADELTA:An adaptive learning rate method[OL]">

                                <b>[17]</b>Zeiler M D.ADADELTA:An adaptive learning rate method[OL].2012[2017-10-01].https://www.arxiv.org/abs/1212.5701
                            </a>
                        </p>
                        <p id="362">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:asimple way to prevent neural networks from overfitting">

                                <b>[18]</b>Srivastava N, Hinton G, Krizhevsky A, et al.Dropout:Asimple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958
                            </a>
                        </p>
                        <p id="364">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rank,trace-norm and max-norm">

                                <b>[19]</b>Srebro N, Shraibman A.Rank, trace-norm and max-norm[C]//Proc of the 18th Int Conf on Computational Learning Theory.Berlin:Springer, 2005:545-560
                            </a>
                        </p>
                        <p id="366">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for sentence classification">

                                <b>[20]</b>Kim Y.Convolutional neural networks for sentence classification[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:1746-1751
                            </a>
                        </p>
                        <p id="368">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">

                                <b>[21]</b>Kalchbrenner N, Grefenstette E, Blunsom P.Aconvolutional neural network for modelling sentences[C]//Proc of the 52nd Annual Meeting of the Association for Computational Linguistics.Stroudsburg, PA:ACL, 2014:655-665
                            </a>
                        </p>
                        <p id="370">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semisupervised recursive autoencoders for predicting sentiment distributions">

                                <b>[22]</b>Socher R, Pennington J, Huang E H, et al.Semi-supervised recursive autoencoders for predicting sentiment distributions[C]//Proc of the 16th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2011:151-161
                            </a>
                        </p>
                        <p id="372">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparative study of CNN and RNN for natural language processing[OL]">

                                <b>[23]</b>Yin Wenpeng, Kann K, Yu Mo, et al.Comparative study of CNN and RNN for natural language processing[OL].2017[2017-10-01].https://www.arxiv.org/abs/1702.01923
                            </a>
                        </p>
                        <p id="374">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid speech recognition with deep bidirectional LSTM">

                                <b>[24]</b>Graves A, Jaitly N, Mohamed A.Hybrid speech recognition with deep bidirectional LSTM[C]//Proc of the 8th IEEEWorkshop on Automatic Speech Recognition and Understanding.Piscataway, NJ:IEEE, 2013:273-278
                            </a>
                        </p>
                        <p id="376">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech recognition with deep recurrent neural networks">

                                <b>[25]</b>Graves A, Mohamed A, Hinton G.Speech recognition with deep recurrent neural networks[C]//Proc of Int Conf on Acoustics, Speech and Signal Processing.Piscataway, NJ:IEEE, 2013:6645-6649
                            </a>
                        </p>
                        <p id="378">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show and tell:A neural image caption generator">

                                <b>[26]</b>Vinyals O, Toshev A, Bengio S, et al.Show and tell:Aneural image caption generator[C]//Proc of the 28th IEEEComputer Vision and Pattern Recognition.Piscataway, NJ:IEEE, 2015:3156-3164
                            </a>
                        </p>
                        <p id="380">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks">

                                <b>[27]</b>Tai Kaisheng, Socher R, Manning C D.Improved semantic representations from tree-structured long short-term memory networks[C]//Proc of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Int Joint Conf on Natural Language Processing.Stroudsburg, PA:ACL, 2015:1556-1566
                            </a>
                        </p>
                        <p id="382">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compositional distributional semantics with long short term memory">

                                <b>[28]</b>Le P, Zuidema W.Compositional distributional semantics with long short term memory[C]//Proc of the 4th Joint Conf on Lexical and Computational Semantics.Stroudsburg, PA:ACL, 2015:10-19
                            </a>
                        </p>
                        <p id="384">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory over recursive structures">

                                <b>[29]</b>Zhu Xiaodan, Sobihani P, Guo Hongyu.Long short-term memory over recursive structures[C]//Proc of the 32nd Int Conf on Machine Learning.Lille, France:PMLR, 2015:1604-1612
                            </a>
                        </p>
                        <p id="386">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multitimescale long short-term memory neural network for modelling sentences and documents">

                                <b>[30]</b>Liu Pengfei, Qiu Xipeng, Chen Xinchi, et al.Multitimescale long short-term memory neural network for modelling sentences and documents[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2326-2335
                            </a>
                        </p>
                        <p id="388">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Empirical evaluation of gated recurrent neural networks on sequence modeling[OL]">

                                <b>[31]</b>Chung J, Gulcehre C, Cho K H, et al.Empirical evaluation of gated recurrent neural networks on sequence modeling[OL].2014[2017-10-01].https://www.arxiv.org/abs/1412.3555
                            </a>
                        </p>
                        <p id="390">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequence to sequence learning with neural networks">

                                <b>[32]</b>Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks[C]//Proc of the 28th Conf on Neural Information Processing Systems.Cambridge, MA:MIT Press, 2014:3104-3112
                            </a>
                        </p>
                        <p id="392">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentence modeling with gated recursive neural network">

                                <b>[33]</b>Chen Xinchi, Qiu Xipeng, Zhu Chenxi, et al.Sentence modeling with gated recursive neural network[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:793-798
                            </a>
                        </p>
                        <p id="394">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parsing Natural Scenes and Natural Language with Recursive Neural Networks">

                                <b>[34]</b>Socher R, Lin C C, Ng A Y, et al.Parsing natural scenes and natural language with recursive neural networks[C]//Proc of the 28th Int Conf on Machine Learning.New York:ACM, 2011:129-136
                            </a>
                        </p>
                        <p id="396">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic compositionality through recursive matrix-vector spaces">

                                <b>[35]</b>Socher R, Huval B, Manning C D, et al.Semantic compositionality through recursive matrix-vector spaces[C]//Proc of the 17th Joint Conf on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.Stroudsburg, PA:ACL, 2012:1201-1211
                            </a>
                        </p>
                        <p id="398">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank">

                                <b>[36]</b>Socher R, Perelygin A, Wu J, et al.Recursive deep models for semantic compositionality over a sentiment treebank[C]//Proc of the 18th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2013:1631-1642
                            </a>
                        </p>
                        <p id="400">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Role of Syntax in Vector Space Models of Compositional Semantics">

                                <b>[37]</b>Hermann K M, Blunsom P.The role of syntax in vector space models of compositional semantics[C]//Proc of the51st Annual Meeting on Association for Computational Linguistics.Stroudsburg, PA:ACL, 2013:894-904
                            </a>
                        </p>
                        <p id="402">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Neural Network for Factoid Question Answering over Paragraphs">

                                <b>[38]</b>Iyyer M, Boyd-Graber J, Claudino L, et al.A neural network for factoid question answering over paragraphs[C]//Proc of the 19th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2014:633-644
                            </a>
                        </p>
                        <p id="404">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative Neural Sentence Modeling by Tree-Based Convolution">

                                <b>[39]</b>Mou Lili, Peng Hao, Li Ge, et al.Discriminative neural sentence modeling by tree-based convolution[C]//Proc of the 20th Conf on Empirical Methods in Natural Language Processing.Stroudsburg, PA:ACL, 2015:2315-2325
                            </a>
                        </p>
                        <p id="406">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grid long shortterm memory[OL]">

                                <b>[40]</b>Kalchbrenner N, Danihelka I, Graves A.Grid long shortterm memory[OL].2015[2017-10-01].https://www.arxiv.org/abs/1507.01526
                            </a>
                        </p>
                        <p id="408">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-dimensio nal recurrent neural networks">

                                <b>[41]</b>Graves A, Fernández S, Schmidhuber J.Multi-dimensional recurrent neural networks[C]//Proc of the 17th Int Conf on Artificial Neural Networks.Berlin:Springer, 2007:549-558
                            </a>
                        </p>
                        <p id="410">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Higher order recurrent neural networks[OL]">

                                <b>[42]</b>Soltani R, Jiang Hui.Higher order recurrent neural networks[OL].2016[2017-10-01].https://www.arxiv.org/abs/1605.00064
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="3" href="javascript:void(0)">
                            <b>1</b> https://code.google.com/archive/p/fudannlp/
                        </span>
                    </p>
                    <p>
                        <span id="5" href="javascript:void(0)">
                            <b>2</b> 问句分类
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JFYZ201904018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201904018&amp;v=MTk4MDk5ak1xNDlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqa1U3cktMeXZTZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQzZ2tpSTBoVC9vc1JrQ2loSzM0UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

