

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129893036681250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dGUAN201911037%26RESULT%3d1%26SIGN%3dhFjY6ctoRIZLwE4v3Ti7xy3knWU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GUAN201911037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=GUAN201911037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GUAN201911037&amp;v=MTQ2ODBGckNVUkxPZVplUnBGQ25uVjc3S0lqaktZTEc0SDlqTnJvOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="引 言 ">引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#30" data-title="1 实验部分 ">1 实验部分</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#31" data-title="&lt;b&gt;1.1 基于CNN的高光谱图像分类&lt;/b&gt;"><b>1.1 基于CNN的高光谱图像分类</b></a></li>
                                                <li><a href="#42" data-title="&lt;b&gt;1.2 残差网络&lt;/b&gt;"><b>1.2 残差网络</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;1.3 基于残差网络分层融合的Large-Margin Softmax高光谱图像分类&lt;/b&gt;"><b>1.3 基于残差网络分层融合的Large-Margin Softmax高光谱图像分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="2 结果与讨论 ">2 结果与讨论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="&lt;b&gt;2.1 实验数据&lt;/b&gt;"><b>2.1 实验数据</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.2 结果与分析&lt;/b&gt;"><b>2.2 结果与分析</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;2.3 网络参数分析&lt;/b&gt;"><b>2.3 网络参数分析</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;2.4 方法比较&lt;/b&gt;"><b>2.4 方法比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#98" data-title="3 结 论 ">3 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;图1 基于CNN的高光谱图像分类流程图&lt;/b&gt;"><b>图1 基于CNN的高光谱图像分类流程图</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;图2 残差块的基本结构&lt;/b&gt;"><b>图2 残差块的基本结构</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;图3 基于残差网络多层融合分类算法流程图&lt;/b&gt;"><b>图3 基于残差网络多层融合分类算法流程图</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;图4 多层特征融合过程&lt;/b&gt;"><b>图4 多层特征融合过程</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;图5 三类样本伪彩色图像&lt;/b&gt;"><b>图5 三类样本伪彩色图像</b></a></li>
                                                <li><a href="#112" data-title="图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图">图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图</a></li>
                                                <li><a href="#112" data-title="图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图">图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图</a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图7 样本块大小对分类精度的影响曲线&lt;/b&gt;"><b>图7 样本块大小对分类精度的影响曲线</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;图8 网络深度对分类精度的影响曲线&lt;/b&gt;"><b>图8 网络深度对分类精度的影响曲线</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;表1 SVM, CNN, DRN softmax和large-margin softmax的 平均分类精度&lt;/b&gt;"><b>表1 SVM, CNN, DRN softmax和large-margin softmax的 平均分类精度</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;图9 不同网络结构在3个数据集上训练与测试时间的对比&lt;/b&gt;"><b>图9 不同网络结构在3个数据集上训练与测试时间的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Fauvel M,Tarabalka Y,Benediktsson J A,et al.Proceedings of the IEEE,2013,101(3):652." target="_blank"
                                       href="">
                                        <b>[1]</b>
                                         Fauvel M,Tarabalka Y,Benediktsson J A,et al.Proceedings of the IEEE,2013,101(3):652.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Chen Y,Nasrabadi N M,Tran T D.IEEE Transactions on Geoscience and Remote Sensing,2011,49(10):3973." target="_blank"
                                       href="">
                                        <b>[2]</b>
                                         Chen Y,Nasrabadi N M,Tran T D.IEEE Transactions on Geoscience and Remote Sensing,2011,49(10):3973.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Zhang H,Li J,Huang Y,et al.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2014,7(6):2056." target="_blank"
                                       href="">
                                        <b>[3]</b>
                                         Zhang H,Li J,Huang Y,et al.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2014,7(6):2056.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     Krizhevsky A,Sutskever I,Hinton G.ImageNet Classification with Deep Convolutional Neural Networks.NIPS 2012:Neuram Information Processing Systems,2012.</a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Chen Yushi,Jiang Hanlu,Li Chunyang,et al.IEEE Trans Geoscience and Remote Sensing,2016,54(10):6232." target="_blank"
                                       href="">
                                        <b>[5]</b>
                                         Chen Yushi,Jiang Hanlu,Li Chunyang,et al.IEEE Trans Geoscience and Remote Sensing,2016,54(10):6232.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Hu W,Huang Y,Wei L,et al.Journal of Sensors,2015(2):1." target="_blank"
                                       href="">
                                        <b>[6]</b>
                                         Hu W,Huang Y,Wei L,et al.Journal of Sensors,2015(2):1.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Zhao W Z and Du S H.IEEE Transactions on Geoscience and Remote Sensing,2016,54 (8):4544." target="_blank"
                                       href="">
                                        <b>[7]</b>
                                         Zhao W Z and Du S H.IEEE Transactions on Geoscience and Remote Sensing,2016,54 (8):4544.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Zhong Z,Li J,Luo Z,et al.IEEE Transactions on Geoscience and Remote Sensing,2018,56(2):847." target="_blank"
                                       href="">
                                        <b>[8]</b>
                                         Zhong Z,Li J,Luo Z,et al.IEEE Transactions on Geoscience and Remote Sensing,2018,56(2):847.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Liu W,Wen Y,Yu Z,et al.Large-Margin Softmax Loss for Convolutional Neural Networks.Proceedings of 33rd International Conference on Machine Learning,2016:507.</a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Chai S,Liu H,Gu Y,et al.IEEE Transactions on Geoscience and Remote Sensing,2017,55(8):4775." target="_blank"
                                       href="">
                                        <b>[10]</b>
                                         Chai S,Liu H,Gu Y,et al.IEEE Transactions on Geoscience and Remote Sensing,2017,55(8):4775.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Zhang X,Zhang H,Zhang Y,et al.IEEE Transactions on Image Processing,2016,25(3):1033." target="_blank"
                                       href="">
                                        <b>[11]</b>
                                         Zhang X,Zhang H,Zhang Y,et al.IEEE Transactions on Image Processing,2016,25(3):1033.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Song Weiwei,Li Shutao,Fang Leyuan,et al.IEEE Transactions on Geoscience and Remote Sensing,2018,56(6):3173." target="_blank"
                                       href="">
                                        <b>[12]</b>
                                         Song Weiwei,Li Shutao,Fang Leyuan,et al.IEEE Transactions on Geoscience and Remote Sensing,2018,56(6):3173.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=GUAN" target="_blank">光谱学与光谱分析</a>
                2019,39(11),3501-3507 DOI:10.3964/j.issn.1000-0593(2019)11-3501-07            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>残差网络分层融合的高光谱地物分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%80%A1%E5%8D%93&amp;code=06592579&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张怡卓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E8%8B%97%E8%8B%97&amp;code=43170164&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐苗苗</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%B0%8F%E8%99%8E&amp;code=40651786&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王小虎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%85%8B%E5%A5%87&amp;code=06585534&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王克奇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8C%97%E6%9E%97%E4%B8%9A%E5%A4%A7%E5%AD%A6%E6%9C%BA%E7%94%B5%E5%B7%A5%E7%A8%8B%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0003034&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东北林业大学机电工程工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>高光谱图像具有较高的空间分辨率,蕴含着丰富的空间光谱信息,近年来被广泛用于城市地物分类中。在高光谱图像分类过程中,空间光谱特征的提取直接影响着分类精度;传统的高光谱图像特征提取方法只利用了4或8邻域的像素进行简单卷积处理,因而丢失了大量的复杂、有效信息;卷积神经网络(CNN)虽然可以自动提取空间光谱特征,在保留图像空间信息的同时,简化网络模型,但是,随着网络深度增加,网络分类产生退化现象,而且网络间缺乏相关信息的互补性,从而影响分类精度。该工作引入CNN自动提取空间光谱特征,并且针对CNN深度增加所导致的退化问题,设计了面向地物分类的高光谱特征融合残差网络。首先,为了降低高光谱图像的光谱冗余度,利用PCA提取主要光谱波段;然后,为了逐级提取光谱图像的空间光谱特征,定义了卷积核为16, 32, 64的低、中、高3层残差网络模块,并利用64个1×1的卷积核对3层特征输出进行卷积,完成维度匹配与特征图融合;接着,对融合后的特征图进行全局平均池化(GAP)生成用于分类的特征向量;最后,引入具有可调节机制的Large-Margin Softmax损失函数,监督模型完成训练过程,实现高光谱图像分类。实验采用Indian Pines, University of Pavia和Salinas地区的高光谱图像来验证方法有效性,设置批次训练的样本集为100,网络训练的初始学习率为0.1,当损失函数稳定后学习率降低为0.001,动量为0.9,权重延迟为0.000 1,最大训练迭代次数为2×10<sup>4</sup>,当3个数据集的样本块像素分别设置为25×25, 23×23, 27×27,网络深度分别为28, 32和28时, 3个数据集的分类准确率最高,其平均总体准确率(OA)为98.75%、平均准确率(AA)的评价值为98.1%,平均Kappa系数为0.98。实验结果表明,基于残差网络的分类方法能够自动学习更丰富的空间光谱特征,残差网络层数的增加和不同网络层融合可以提高高光谱分类精度; Large-Margin Softmax实现了类内紧凑和类间分离,可以进一步提高高光谱图像分类精度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%85%89%E8%B0%B1%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高光谱图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Large-Margin%20Softmax&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Large-Margin Softmax;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张怡卓,1978年生,东北林业大学机电工程工程学院教授e-mail:nefuzyz@163.com;
                                </span>
                                <span>
                                    *王克奇,e-mail:zdhwkq@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>林业公益性行业科研专项(21504307)资助;</span>
                    </p>
            </div>
                    <h1><b>Hyperspectral Image Classification Based on Hierarchical Fusion of Residual Networks</b></h1>
                    <h2>
                    <span>ZHANG Yi-zhuo</span>
                    <span>XU Miao-miao</span>
                    <span>WANG Xiao-hu</span>
                    <span>WANG Ke-qi</span>
            </h2>
                    <h2>
                    <span>College of Mechanical and Electrical Engineering, Northeast Forestry University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Hyperspectral images contain a wealth of feature information, and they have been widely used in urban feature classification in recent years. In the process of hyperspectral image classification, the extraction of spatial spectral features directly affects the classification accuracy. Traditional hyperspectral image feature extraction methods only use 4 or 8 neighborhood pixels for simple convolution processing, thus losing a lot of complex and effective information. Convolution neural network(CNN) can automatically extract spatial spectral features and retain the same spatial information of the image, and the network model is simplified. However, with the increase of network depth, the network classification will degenerate, and the network lacks complementarity of relevant information, which will affect the classification accuracy. In this paper, a hyperspectral residual network for feature classification is designed for the degradation problem. Firstly, define the residual network module of the low, medium and high three-layer structure with convolution kernels of 16, 32, and 64. Then, convolve the 3-layer output features with 64 1×1 convolution kernels to complete the dimension matching and feature map. Next, the global average pooling(GAP) of the feature map is generated to generate the feature vector for classification. Finally, the Large-Margin Softmax objective function is introduced to achieve hyperspectral image classification. The experiments were performed using hyperspectral images from the Indian Pines, University of Pavia, and Salinas regions. The primary bands of the hyperspectral image were extracted by PCA. With the sample set of batch training being 100, the initial learning rate being 0.1, the momentum being 0.9, the weight delay being 0.000 1, and the maximum number of training iterations being 2×10<sup>4</sup>, when the sample sizes of the three data sets are set to be 25×25, 23×23 and 27×27, the network depth is 28, 32 and 28, the classification accuracy of the three data sets is the highest, and the average overall accuracy OA is 98.75%, the average accuracy AA is 98.1% and the average Kappa coefficient is 0.98. The experimental results show that the classification method based on residual network can get more affective features. It can improve the classification accuracy with the increase of the number of residual network layers and the fusion of complementary information of different network layer outputs; Large-Margin Softmax achieves intra-class compactness. Separation between classes further improves classification accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hyperspectral%20image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hyperspectral image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Residual%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Residual network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Large-Margin%20Softmax&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Large-Margin Softmax;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag">引 言</h3>
                <div class="p1">
                    <p id="28">高光谱图像含有丰富的光谱信息, 这些信息是由可见光谱到红外光谱的数百个光谱带组成。 高光谱图像的每个像素点通过高维向量来表示, 对应于特定波长的光谱反射率。 近年来, 国内外学者提出了多种高光谱图像分类算法。 Jia等<citation id="102" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>将SVM用于高光谱图像分类, 但是, SVM只利用了高光谱图像的光谱信息, 没有考虑图像的空间信息。 随着稀疏理论的发展, 稀疏表达在高光谱图像分类中得到广泛研究。 2011年, Chen等<citation id="103" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出联合稀疏表示模型来进行高光谱图像分类, 该模型通过空间信息确定待测像元的类别,  有效地融合了高光谱图像的空间光谱特征, 但是, 当地物种类丰富时,  容易出现错分现象。 2014年, Zhang等<citation id="104" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了非局部加权联合稀疏分类模型, 针对邻域像元对中心像元贡献值存在的差异进行了研究。 传统机器学习方法在高光谱图像分类中得到了很大的进展, 但是需要复杂的特征提取过程, 因而分类精度有待于进一步提高。</p>
                </div>
                <div class="p1">
                    <p id="29">深度学习通过分层网络获取分层次的特征信息, 简化了复杂的人工设计特征过程, 成为高光谱图像分类的研究热点。 卷积神经网络(CNN)是深度学习的一个重要分支, 在高光谱图像分类中的应用越来越广泛<citation id="109" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。 2015年,  Hu等<citation id="105" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>利用CNN网络模型对高光谱图像进行分类, 利用CNN的局部连接、 权值共享等特性, 减少了模型参数, 降低了训练难度, 提高了分类性能。 2016年, Zhao等<citation id="106" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过CNN和平衡局部判别嵌入算法提取高光谱图像的空间和光谱特征来进行高光谱图像分类; 但是, 基于CNN模型的分类算法仅包含单一卷积层, 降低了其特征学习的能力。 通过增加网络深度可以提高网络的性能, 尤其处理具有非常复杂的空间光谱特性的高光谱图像。 但是, 网络层数过多导致训练误差增大, 即退化现象, 使准确率达到一定值后迅速下降。 2018年, Zhong等<citation id="107" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了基于残差网络的高光谱图像分类方法, 网络中的残差块通过恒等映射连接其他卷积层, 并且在每个卷积层上进行批量归一化, 解决了网络层数增加带来的负担, 在高光谱图像分类中具有很好的效果。 残差网络利用残差块学习有判别力的特征, 但是没有充分利用浅层和深层的互补和相关信息; 而且, 传统的softmax分类器没有明确地鼓励类内紧凑性和类间可分离性<citation id="108" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 在进行复杂地物的分类时精确度不高。 基于深度学习的融合策略在图像分类中得到广泛研究<citation id="110" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。 本工作引入残差网络进行特征提取, 并设计了残差网络中不同层次特征融合策略, 通过Large-Margin Softmax分类器实现高光谱图像的有效分类。</p>
                </div>
                <h3 id="30" name="30" class="anchor-tag">1 实验部分</h3>
                <h4 class="anchor-tag" id="31" name="31"><b>1.1 基于CNN的高光谱图像分类</b></h4>
                <div class="p1">
                    <p id="32">图1为高光谱图像的CNN特征提取和分类流程图, 为充分利用空间信息, 提取以待分类像素为中心的样本作为输入, 经过卷积、 池化和全连接一系列的操作, 输入到softmax分类器得到分类结果。 定义样本块经过卷积和池化的输出特征向量为<i>x</i>,  softmax输出该样本块在各个类别的概率<i>p</i><sub><i>j</i></sub>如式(1)所示</p>
                </div>
                <div class="p1">
                    <p id="33" class="code-formula">
                        <mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></msup></mrow></mfrac><mo>,</mo><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mo>⋯</mo><mo>,</mo><mspace width="0.25em" /><mi>C</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="34">其中, <i>C</i>为类别总数; <i>p</i><sub><i>j</i></sub>的最大值为该样本块的所属类别; <i>f</i><sub><i>j</i></sub>表示全连接层输出向量的第<i>j</i>个元素, 其表达式如式(2)所示</p>
                </div>
                <div class="p1">
                    <p id="35" class="code-formula">
                        <mathml id="35"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mi>x</mi><mo>=</mo><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="36">其中, <i>W</i>表示网络权值。</p>
                </div>
                <div class="p1">
                    <p id="37">网络的损失函数表示为</p>
                </div>
                <div class="p1">
                    <p id="38" class="code-formula">
                        <mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mo>-</mo><mi>log</mi><mrow><mo>(</mo><mrow><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mo>,</mo><mspace width="0.25em" /><mi>c</mi><mo>=</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mo>⋯</mo><mo>,</mo><mspace width="0.25em" /><mi>C</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="39">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_039.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于CNN的高光谱图像分类流程图" src="Detail/GetImg?filename=images/GUAN201911037_039.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于CNN的高光谱图像分类流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_039.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.1 The flow chart of hyperspectral image</b> classification based on CNN</p>

                </div>
                <div class="p1">
                    <p id="41">虽然CNN在图像分类中有着广泛的应用, 但是, 仅利用几个简单的卷积和池化层, 不能充分提取高光谱图像深层的特征以进行准确分类。 虽然可以通过增加网络深度提高CNN的性能, 但是过度增加网络深度将会使网络产生退化现象, 会导致CNN出现过拟合、 梯度消失和梯度爆炸等问题。</p>
                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>1.2 残差网络</b></h4>
                <div class="p1">
                    <p id="43">残差网络为解决深层CNN出现的退化问题将深层网络后面的卷积层变为恒等函数。 让一些层去拟合一个恒等函数很困难, 所以把网络设计为</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>x</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">其中, <i>x</i>和<i>H</i>(<i>x</i>)分别表示一个残差块的输入和输出, <i>F</i>(<i>x</i>)是残差块学习的函数。</p>
                </div>
                <div class="p1">
                    <p id="46">学习一个恒等映射函数转换为学习一个残差函数</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Η</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>-</mo><mi>x</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="48">当<i>F</i>(<i>x</i>)=0时, 就构成了恒等映射<i>H</i>(<i>x</i>)=<i>x</i>。 由于恒等映射没有引入额外的参数, 不会给网络增加额外的参数和计算量, 同时能够加快模型训练速度、 提高训练效果。</p>
                </div>
                <div class="p1">
                    <p id="49">残差网络使用shortcut回路实现两个卷积层的优化<citation id="111" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 即在两个卷积层的基础上叠加<i>y</i>=<i>x</i>的恒等映射, 将输入和输出相加。 加入shortcut后, 训练过程中误差可以通过恒等映射向上一层传播, 减弱层数过多造成的梯度消失现象, 残差块的基本结构如图2所示。 但是, 该网络忽视了不同网络层之间信息的互补性与相关性, 影响分类精度。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 残差块的基本结构" src="Detail/GetImg?filename=images/GUAN201911037_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 残差块的基本结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.2 The structure of the residual block</b></p>

                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>1.3 基于残差网络分层融合的Large-Margin Softmax高光谱图像分类</b></h4>
                <div class="p1">
                    <p id="52">CNN中不同卷积层可以学习不同的特征, 浅层具有图像细节信息, 深层含有更多抽象信息。 为了充分利用不同层次特征信息, 将浅层特征与深层特征进行融合, 可以增强模型对复杂图像的表现能力, 图3为残差融合网络设计结构。</p>
                </div>
                <div class="p1">
                    <p id="53">根据卷积核的数量将残差网络分为3个模块, 其中O<sub>L</sub>, O<sub>M</sub>和O<sub>H</sub>分别表示3个模块的输出; 在3个模块中分别具有16, 32, 64个卷积核, 输出不同维度的特征图, 实现特征信息的逐层提取。 为确保特征融合前具有相同的维度, 经过数量为64, 大小为1×1的卷积核进行卷积完成维度匹配。 最后, 将3个模块的输出相加融合。 激活之后的融合特征图进行全局平均池化(GAP), 生成最终的特征图, 融合过程如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="54">最后, 将融合的特征图经过全连接网络转换成输出的特征向量, 输入到分类器进行分类。 传统的softmax分类器不能很好地学习到使类内紧凑、 类间分散的特征。 所以, 引入Large-Margin Softmax分类器。</p>
                </div>
                <div class="p1">
                    <p id="55">以二分类问题为例, 假设一个样本块属于类别1, 由式(1)和式(2)可知, softmax是通过让<i>p</i><sub>1</sub>&gt;<i>p</i><sub>2</sub>, 即</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mn>1</mn><mtext>Τ</mtext></msubsup><mi>x</mi><mo>&gt;</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup><mi>x</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">来进行分类, 可以写为</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>&gt;</mo><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于残差网络多层融合分类算法流程图" src="Detail/GetImg?filename=images/GUAN201911037_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 基于残差网络多层融合分类算法流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.3 Flow chart of multi-layer fusion classification algorithm based on residual network</b></p>

                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多层特征融合过程" src="Detail/GetImg?filename=images/GUAN201911037_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 多层特征融合过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.4 Multi-layer feature fusion process</b></p>

                </div>
                <div class="p1">
                    <p id="61">而Large-Margin softmax是将其变为</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>≥</mo><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>m</mi><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>&gt;</mo></mtd></mtr><mtr><mtd><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中, <i>m</i>是正整数,<mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo>≤</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>≤</mo><mfrac><mtext>π</mtext><mi>m</mi></mfrac></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="64">通过增加一个正整数变量<i>m</i>, 产生一个决策余量, 能够更加严格地约束式(7)。</p>
                </div>
                <div class="p1">
                    <p id="65">Large-Margin Softmax损失函数表示为</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mo>-</mo><mi>log</mi><mrow><mo>(</mo><mrow><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>Ψ</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>Ψ</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>≠</mo><mi>j</mi></mrow></munder><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中<i>Ψ</i>(<i>θ</i>)为</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ψ</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>cos</mi><mo stretchy="false">(</mo><mi>m</mi><mi>θ</mi><mo stretchy="false">)</mo><mo>,</mo></mtd><mtd columnalign="left"><mn>0</mn><mo>≤</mo><mi>θ</mi><mo>≤</mo><mfrac><mtext>π</mtext><mi>m</mi></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mi>cos</mi><mrow><mo>(</mo><mrow><mfrac><mtext>π</mtext><mi>m</mi></mfrac></mrow><mo>)</mo></mrow><mo>,</mo></mtd><mtd columnalign="left"><mfrac><mtext>π</mtext><mi>m</mi></mfrac><mo>&lt;</mo><mi>θ</mi><mo>≤</mo><mi>π</mi></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">通过调节间隔<i>m</i>的值来调节分类边界, 式(8)对学习<i>W</i><sub>1</sub>和<i>W</i><sub>2</sub>提出了更高的要求, 使模型学习到类内距离更小、 类间距离更大的可区分性的特征。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag">2 结果与讨论</h3>
                <h4 class="anchor-tag" id="71" name="71"><b>2.1 实验数据</b></h4>
                <div class="p1">
                    <p id="72">实验选用的高光谱数据为Indian Pines, University of Pavia和Salinas 3个典型的数据源。 其中, Indian Pines是美国印第安纳州西北部农区的高光谱图像, 该数据有10 249个地物像素, 包括苜蓿(Alfalfa)、 玉米(Corn)、 燕麦(Oats)、 小麦(Wheat)、 木材(Woods)等16个地物类别; University of Pavia是意大利帕维亚城的一部分高光谱数据, 该数据有42 776个用于分类的地物像素, 包含树、 沥青道路(asphalt)、 牧场(meadows)、 砾石(gravel)、 裸土(bare soil)等9类地物; Salinas是美国加利福尼亚州的Salinas山谷的高光谱图像, 该图像有16类54 129个像素, 包括休耕(fallow)、 残殊(stubble)、 葡萄(grapes)、 芹菜(celery)、 生菜(lettuce)等。</p>
                </div>
                <div class="p1">
                    <p id="73">图5是3个地区分别抽取3个波段叠加所成的伪彩色图像。 实验所用计算机配置为Intel(R) Core<sup>TM</sup> i5-4460处理器, NVIDIA GeForce GTX 1070 Ti显卡; 在Linux操作系统下基于caffe框架运行实验。 在进行数据降维时, 将3个高光谱图像经过PCA处理, 提取主要的光谱波段。 将3个数据集中被标记的像素分成训练集和测试集, 分别选择10%, 2%和0.5%的样本进行训练, 其余作为测试样本。 设置批次训练的样本集为100, 网络训练的初始学习率为0.1, 当损失函数稳定后学习率降低为0.001, 动量为0.9, 权重延迟为0.000 1,最大训练迭代次数为2×10<sup>4</sup>。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_074.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 三类样本伪彩色图像" src="Detail/GetImg?filename=images/GUAN201911037_074.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 三类样本伪彩色图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_074.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.5 Pseudocolored images</b></p>
                                <p class="img_note">(a）：印第安农场；（b）：帕维亚大学；（c）：萨利纳斯</p>
                                <p class="img_note">(a):Indian Pines;(b):University of Pavia;(c):Salinas</p>

                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.2 结果与分析</b></h4>
                <div class="p1">
                    <p id="77">图6为3个数据集基于残差网络的特征融合算法训练过程中, 样本块大小分别在25×25, 23×23和27×27, 网络深度分别为28, 32和28时, 随迭代次数增加损失函数值和准确率随迭代次数变化的曲线图。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图" src="Detail/GetImg?filename=images/GUAN201911037_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Loss function(a,c,e)and accuracy curve(b,d,f)</p>
                                <p class="img_note">(a),(b):Indian Pines样本大小为25×25，深度为28;(c),(d):Paviau在样本大小为23×23，深度为32;(e),(f):Salinas在样本大小为27×27，深度为28</p>
                                <p class="img_note">(a),(b):Indian Pines with a sample size of 25×25and a depth of 28;(c),(d):Paviau with a sample size of 23×23and a depth of 32;(e),(f):Salinas with a sample size of 27×27and a depth of 28</p>

                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图" src="Detail/GetImg?filename=images/GUAN201911037_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 损失函数（a,c,e）和准确率（b,d,f）变化曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Loss function(a,c,e)and accuracy curve(b,d,f)</p>
                                <p class="img_note">(a),(b):Indian Pines样本大小为25×25，深度为28;(c),(d):Paviau在样本大小为23×23，深度为32;(e),(f):Salinas在样本大小为27×27，深度为28</p>
                                <p class="img_note">(a),(b):Indian Pines with a sample size of 25×25and a depth of 28;(c),(d):Paviau with a sample size of 23×23and a depth of 32;(e),(f):Salinas with a sample size of 27×27and a depth of 28</p>

                </div>
                <div class="p1">
                    <p id="84">由图6(a), (c)和(e)可以看出, 当迭代次数达到2 500时, 损失函数开始趋于稳定, 比较图6(b), (d)和(f), 准确率呈现不同程度的下降, 说明随着网络层数的增加, 出现了退化问题。 当迭代次数超过10 000时, 由于残差块的学习, 将输入直接传到输出, 保护了信息的完整性, 解决了退化问题, 准确率达到最大并趋于稳定。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>2.3 网络参数分析</b></h4>
                <div class="p1">
                    <p id="86">样本块大小和网络层数是重要的网络参数。 样本块大小会影响分类精度, 样本块过大, 会忽视一些细节信息; 样本块过小, 也会放大冗余信息, 降低分类精度。 增加网络深度在某种程度上能够提高分类精度; 但是, 层数过多会使网络产生过拟合、 梯度消失和梯度爆炸等问题。 在此, 通过交替迭代实验对这两个参数的确定进行分析。 实验过程中, 样本块大小设置范围为17×17到31×31, 网络深度设置在8～36之间。 为定量评估不同参数的表现, 使用总体准确率(OA)、 平均准确率(AA)和Kappa系数三个评价指标对分类精度进行评价, 其中, OA是对分类结果质量的总体评价, AA是所有类别分类精度的平均值, Kappa系数反映了分类结果与参考数据之间的吻合程度。</p>
                </div>
                <div class="p1">
                    <p id="87">图7为3个数据集的网络深度分别设为28, 32和28时, 样本块大小对分类精度的影响曲线。 由图可以看出, 随着样本块的增大, Indian Pines分类的OA值变化较为平缓, Salinas逐渐增大, 而University of Pavia先增大后减小。 对于不同的数据集, 类内和类间的光滑度差异使得样本块大小对分类精度呈现不同的变化趋势, University of Pavia的类别区域更加复杂。 3个数据集的样本块分别设为25×25, 23×23和27×27时分类精度最高。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 样本块大小对分类精度的影响曲线" src="Detail/GetImg?filename=images/GUAN201911037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 样本块大小对分类精度的影响曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.7 The effect of sample block size</b> on classification rate</p>

                </div>
                <div class="p1">
                    <p id="90">图8所示为3个数据集的样本块大小分别设为25×25, 23×23, 27×27时, 网络深度对分类精度的影响曲线图。 由图8可以看出, 在网络层数8～36之间时, 分类精度都达到96%以上, 证明本工作提出的网络能有效平衡网络深度和分类的表现。 3个数据集的网络深度分别设为28, 32和28时, 分类精度达到最高。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 网络深度对分类精度的影响曲线" src="Detail/GetImg?filename=images/GUAN201911037_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 网络深度对分类精度的影响曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.8 The effect of network d</b>epth on classification accuracy</p>

                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>2.4 方法比较</b></h4>
                <div class="p1">
                    <p id="94">为验证基于残差网络分层融合分类算法的性能, 将该算法与常用的SVM算法、 卷积神经网络CNN和深度残差网络DRN高光谱分类方法进行比较。 其中, SVM算法利用了高光谱的像素作为输入, 使用具有高斯核的支持向量机库来实现分类; CNN是经过浅层的卷积层后输入到分类器进行分类; 而DRN采用了残差网络。 比较结果如表1所示, 相应的网络训练和测试时间如图9所示。</p>
                </div>
                <div class="p1">
                    <p id="95">通过比较表1中的OA可知, 与传统的机器学习算法SVM相比, CNN通过卷积层获取分层次的空间光谱特征, 减少了人工提取特征的经验性误差, OA值分别提高了4.9%, 7.8%和2.5%, 具有更好的分类效果; 但是, 卷积神经网络得到的特征数目多, 处理时间长, 大大增加了训练时间。 残差网络的引入, 解决了网络层数增加带来的负担, 同时提取到深层次的空间光谱特征, 将分类精度提高到95%左右; 但是网络深度的增加, 使训练时间明显增大。 通过比较DRN和本算法可以看出, 分层融合机制融合了不同卷积层输出的互补与相关信息, 得到更有利于分类的特征, 使分类精度达到98%以上, 证明了特征融合的有效性。 实验结果表明, 使用Large-Margin Softmax, 能够提取更有区分性的特征, 使分类精度得到进一步提高。 从图9可以看出, 网络结构的改变, 对训练时间影响较大, 但是对测试时间影响较小。</p>
                </div>
                <div class="area_img" id="96">
                    <p class="img_tit"><b>表1 SVM, CNN, DRN softmax和large-margin softmax的 平均分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 The average classification rates of SVM, CNN, DRN, softmax and large-margin softmax</b></p>
                    <p class="img_note"></p>
                    <table id="96" border="1"><tr><td><br />精度</td><td>OA/%</td><td>AA/%</td><td>Kappa</td></tr><tr><td><br />SVM</td><td>79.82</td><td>76.89</td><td>0.79</td></tr><tr><td><br />CNN</td><td>91.53</td><td>91.02</td><td>0.88</td></tr><tr><td><br />DRN</td><td>94.07</td><td>94.76</td><td>0.93</td></tr><tr><td><br />Softmax</td><td>98.34</td><td>98.00</td><td>0.98</td></tr><tr><td><br />Large-margin Softmax</td><td>98.74</td><td>98.10</td><td>0.98</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/GUAN201911037_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同网络结构在3个数据集上训练与测试时间的对比" src="Detail/GetImg?filename=images/GUAN201911037_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 不同网络结构在3个数据集上训练与测试时间的对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/GUAN201911037_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><b>Fig.9 Comparison of training and test time on 3 data sets with different network structures</b></p>

                </div>
                <h3 id="98" name="98" class="anchor-tag">3 结 论</h3>
                <div class="p1">
                    <p id="99">高光谱图像含有丰富的空间光谱信息, 但是在分类过程中复杂特征通常不能被充分利用。 在高光谱图像分类的方法中, SVM、 多元逻辑回归等传统的机器学习算法特征提取过程较为复杂, 而简单的CNN不能充分提取高光谱图像的深层光谱与空间特征, 并且没有充分利用多个网络层的互补与相关信息。 为解决高光谱数据特征丰富难以提取和特征信息利用不充分的问题, 设计了一种基于残差网络融合的Large-Margin Softmax进行高光谱图像分类的新方法。 通过对各种分类方法比较分析发现, 3个数据集的平均总体准确率(OA)为98.75%、 平均准确率(AA)为98.1%, 平均Kappa系数为0.98, 实验结果证明基于残差网络融合分类算法可以获得较好的效果。</p>
                </div>
                <div class="p1">
                    <p id="100">基于残差网络分层融合的Large-Margin Softmax分类方法可以提取高光谱图像的有效空间光谱特征, 解决了网络层数增加带来的负担, 为高光谱图像分类提供了新思路, 但是该方法仍然存在耗时较长的问题。 所以, 在高光谱图像分类的应用中, 残差网络和分类器的设计存在巨大的潜力, 如何提高分类的速度还有待进一步地研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="">

                                <b>[1]</b> Fauvel M,Tarabalka Y,Benediktsson J A,et al.Proceedings of the IEEE,2013,101(3):652.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="">

                                <b>[2]</b> Chen Y,Nasrabadi N M,Tran T D.IEEE Transactions on Geoscience and Remote Sensing,2011,49(10):3973.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="">

                                <b>[3]</b> Zhang H,Li J,Huang Y,et al.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,2014,7(6):2056.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 Krizhevsky A,Sutskever I,Hinton G.ImageNet Classification with Deep Convolutional Neural Networks.NIPS 2012:Neuram Information Processing Systems,2012.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="">

                                <b>[5]</b> Chen Yushi,Jiang Hanlu,Li Chunyang,et al.IEEE Trans Geoscience and Remote Sensing,2016,54(10):6232.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="">

                                <b>[6]</b> Hu W,Huang Y,Wei L,et al.Journal of Sensors,2015(2):1.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="">

                                <b>[7]</b> Zhao W Z and Du S H.IEEE Transactions on Geoscience and Remote Sensing,2016,54 (8):4544.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="">

                                <b>[8]</b> Zhong Z,Li J,Luo Z,et al.IEEE Transactions on Geoscience and Remote Sensing,2018,56(2):847.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Liu W,Wen Y,Yu Z,et al.Large-Margin Softmax Loss for Convolutional Neural Networks.Proceedings of 33rd International Conference on Machine Learning,2016:507.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="">

                                <b>[10]</b> Chai S,Liu H,Gu Y,et al.IEEE Transactions on Geoscience and Remote Sensing,2017,55(8):4775.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="">

                                <b>[11]</b> Zhang X,Zhang H,Zhang Y,et al.IEEE Transactions on Image Processing,2016,25(3):1033.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="">

                                <b>[12]</b> Song Weiwei,Li Shutao,Fang Leyuan,et al.IEEE Transactions on Geoscience and Remote Sensing,2018,56(6):3173.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="GUAN201911037" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GUAN201911037&amp;v=MTQ2ODBGckNVUkxPZVplUnBGQ25uVjc3S0lqaktZTEc0SDlqTnJvOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU44R0piVnl1a2k2YnhIV3M3endYND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

