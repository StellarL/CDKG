

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139229758388750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201909017%26RESULT%3d1%26SIGN%3dPJnnXWTdfoGXvGB582X%252bjrnmVGg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201909017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201909017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201909017&amp;v=MzI3NTZWTC9NSVRmQVpiRzRIOWpNcG85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;1&lt;/b&gt; 基线系统 "><b>1</b> 基线系统</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;2&lt;/b&gt; 听感量化编码模型 "><b>2</b> 听感量化编码模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="&lt;b&gt;2.1&lt;/b&gt; 模型概述"><b>2.1</b> 模型概述</a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.2&lt;/b&gt; 模型实现"><b>2.2</b> 模型实现</a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;2.3&lt;/b&gt; 模型训练和生成"><b>2.3</b> 模型训练和生成</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="&lt;b&gt;3&lt;/b&gt; 实验结果 "><b>3</b> 实验结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;3.1&lt;/b&gt; 不同数据量实验"><b>3.1</b> 不同数据量实验</a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;3.2&lt;/b&gt; 跨语种实验"><b>3.2</b> 跨语种实验</a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;3.3&lt;/b&gt; 情感控制实验"><b>3.3</b> 情感控制实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;4&lt;/b&gt; 结论和展望 "><b>4</b> 结论和展望</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 基线LSTM-RNN模型结构">图1 基线LSTM-RNN模型结构</a></li>
                                                <li><a href="#44" data-title="图2 听感量化编码模型训练框图">图2 听感量化编码模型训练框图</a></li>
                                                <li><a href="#53" data-title="图3 听感量化编码模型的概率图">图3 听感量化编码模型的概率图</a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;中文发音人不同数据量&lt;/b&gt;MOS&lt;b&gt;测听结果&lt;/b&gt;"><b>表</b>1 <b>中文发音人不同数据量</b>MOS<b>测听结果</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;英文发音人跨语种迁移学习结果&lt;/b&gt;"><b>表</b>2 <b>英文发音人跨语种迁移学习结果</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;合成语音情感听辨正确率结果&lt;/b&gt;"><b>表</b>3 <b>合成语音情感听辨正确率结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" Yoshimura T,Tokuda K,Masuko T,et al.Simultaneous modeling of spectrum,pitch and duration in HMM-based speech synthesis[C].Budapest:Sixth European Conference on Speech Communication and Technology,EUROSPEECH,1999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous Modeling of Spectrum,Pitch and Duration in HMM-Based Speech Synthesis">
                                        <b>[1]</b>
                                         Yoshimura T,Tokuda K,Masuko T,et al.Simultaneous modeling of spectrum,pitch and duration in HMM-based speech synthesis[C].Budapest:Sixth European Conference on Speech Communication and Technology,EUROSPEECH,1999.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" Tokuda K,Masuko T,Miyazaki N,et al.Hidden Markov models based on multi-space probability distribution for pitch pattern modeling[C].Phoenix:International Conference on Acoustics,Speech and Signal Processing (ICASSP),1999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HiddenMarkov models based on multi-space probabilitydistribution for pitch pattern modeling">
                                        <b>[2]</b>
                                         Tokuda K,Masuko T,Miyazaki N,et al.Hidden Markov models based on multi-space probability distribution for pitch pattern modeling[C].Phoenix:International Conference on Acoustics,Speech and Signal Processing (ICASSP),1999.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" Tokuda K,Yoshimura T,Masuko T,et al.Speech parameter generation algorithms for HMM-based speech synthesis[C].Istanbul:International Conference on Acoustics,Speech and Signal Processing (ICASSP),2000." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speec h parameter generation algorithms for HMM-based speech synthesis">
                                        <b>[3]</b>
                                         Tokuda K,Yoshimura T,Masuko T,et al.Speech parameter generation algorithms for HMM-based speech synthesis[C].Istanbul:International Conference on Acoustics,Speech and Signal Processing (ICASSP),2000.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" Ling Z H,Deng L,Yu D.Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis[J].IEEE Transactions on Audio,Speech,and Language Processing,2013,21(10):2129-2139." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling spectral envelopes using restricted BoltzmannMachines and Deep Belief Networks for statistical parameter speech synthesis">
                                        <b>[4]</b>
                                         Ling Z H,Deng L,Yu D.Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis[J].IEEE Transactions on Audio,Speech,and Language Processing,2013,21(10):2129-2139.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" Zen H.Deep learning in speech synthesis[C].Guangzhou:Keynote Speech Given at Isca Speech Synthesis Workshop (SSW8),2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning in speech synthesis">
                                        <b>[5]</b>
                                         Zen H.Deep learning in speech synthesis[C].Guangzhou:Keynote Speech Given at Isca Speech Synthesis Workshop (SSW8),2013.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" Fan Y,Qian Y,Xie F L,et al.TTS synthesis with bidirectional LSTM based recurrent neural networks[C].Minneapolis:Fifteenth Annual Conference of the International Speech Communication Association(ISCA),2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks">
                                        <b>[6]</b>
                                         Fan Y,Qian Y,Xie F L,et al.TTS synthesis with bidirectional LSTM based recurrent neural networks[C].Minneapolis:Fifteenth Annual Conference of the International Speech Communication Association(ISCA),2014.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" Zen H,Sak H.Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis[C].South Brisbane:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis">
                                        <b>[7]</b>
                                         Zen H,Sak H.Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis[C].South Brisbane:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2015.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" Ling Z H,Kang S Y,Zen H,et al.Deep learning for acoustic modeling in parametric speech generation:A systematic review of existing techniques and future trends[J].IEEE Signal Processing Magazine,2015,32(3):35-52." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends">
                                        <b>[8]</b>
                                         Ling Z H,Kang S Y,Zen H,et al.Deep learning for acoustic modeling in parametric speech generation:A systematic review of existing techniques and future trends[J].IEEE Signal Processing Magazine,2015,32(3):35-52.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" Takaki S,Yamagishi J.A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis[C].Shanghai:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis">
                                        <b>[9]</b>
                                         Takaki S,Yamagishi J.A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis[C].Shanghai:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2016.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" Chen L H,Raitio T,Valentini-Botinhao C,et al.DNN-based stochastic postfilter for HMM-based speech synthesis[C].Singapore:15&lt;sup&gt;th&lt;/sup&gt; Annual Conference of the International Speech Communication Association,INTERSPEECH,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DNN-based stochastic postfilter for HMM-based speech synthesis">
                                        <b>[10]</b>
                                         Chen L H,Raitio T,Valentini-Botinhao C,et al.DNN-based stochastic postfilter for HMM-based speech synthesis[C].Singapore:15&lt;sup&gt;th&lt;/sup&gt; Annual Conference of the International Speech Communication Association,INTERSPEECH,2014.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" Kaneko T,Kameoka H,Hojo N,et al.Generative adversarial network-based postfilter for statistical parametric speech synthesis[C].New Orleans:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial network-based postfilter for statistical parametric speech synthesis">
                                        <b>[11]</b>
                                         Kaneko T,Kameoka H,Hojo N,et al.Generative adversarial network-based postfilter for statistical parametric speech synthesis[C].New Orleans:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2017.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     刘庆峰.基于听感量化理论的语音合成系统研究[D].合肥:中国科学技术大学,2003.Liu Qingfeng.Research on perception quantification-based speech synthesis system[D].Hefei:University of Science and Technology of China,2003.</a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" Hu Y J,Ling Z H.DBN-based spectral feature representation for statistical parametric speech synthesis [J].IEEE Signal Processing Letters,2016,23(3):321-325." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DBN-based spectral feature representation for statistical parametric speech synthesis">
                                        <b>[13]</b>
                                         Hu Y J,Ling Z H.DBN-based spectral feature representation for statistical parametric speech synthesis [J].IEEE Signal Processing Letters,2016,23(3):321-325.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" Liu L J,Ding C,Jiang Y,et al.The IFLYTEK system for blizzard challenge[C].Stockholm:The Blizzard ChallengeWorkshop,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The IFLYTEK system for blizzard challenge">
                                        <b>[14]</b>
                                         Liu L J,Ding C,Jiang Y,et al.The IFLYTEK system for blizzard challenge[C].Stockholm:The Blizzard ChallengeWorkshop,2017.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" An S,Ling Z,Dai L.Emotional statistical parametric speech synthesis using LSTM-RNNs[C].Kuala Lumpur :Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),IEEE,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Emotional statistical parametric speech synthesis using LSTM-RNNs">
                                        <b>[15]</b>
                                         An S,Ling Z,Dai L.Emotional statistical parametric speech synthesis using LSTM-RNNs[C].Kuala Lumpur :Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),IEEE,2017.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" Hu Y J,Ling Z H.Extracting spectral features using deep autoencoders with binary distributed hidden units for statistical parametric speech synthesis[J].IEEE/ACM Transactions on Audio,Speech and Language Processing (TASLP),2018,26(4):713-724." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMF314F7712E21E5B17A2E7978227FD9E2&amp;v=Mjc1MTM4VzdIOVc2cUloRVpwNE5EUWs4dlJjVW16MElUM2JscEJBM2ZzVGdUTStkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dDlod0x5NHdLOD1OaWZJWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Hu Y J,Ling Z H.Extracting spectral features using deep autoencoders with binary distributed hidden units for statistical parametric speech synthesis[J].IEEE/ACM Transactions on Audio,Speech and Language Processing (TASLP),2018,26(4):713-724.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(09),76-79 DOI:10.16180/j.cnki.issn1007-7820.2019.09.016            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于听感量化编码的神经网络语音合成方法研究</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%BA%86%E5%B3%B0&amp;code=09575724&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘庆峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E6%BA%90&amp;code=26283234&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江源</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E4%BA%9A%E5%86%9B&amp;code=39688543&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡亚军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%88%A9%E5%A8%9F&amp;code=30278299&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘利娟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%AF%AD%E9%9F%B3%E5%8F%8A%E8%AF%AD%E8%A8%80%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E5%9B%BD%E5%AE%B6%E5%B7%A5%E7%A8%8B%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0002522&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音及语言信息处理国家工程实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对当前神经网络声学建模中数据混用困难的问题,文中提出了一种基于听感量化编码的神经网络语音合成方法。通过设计听感量化编码模型学习海量语音在音色、语种、情感上的不同差异表征,构建统一的多人数据混合训练的神经网络声学模型。在统一的听感量化编码声学模型内通过数据共享和迁移学习,可以显著降低合成系统搭建的数据量要求,并实现对合成语音的音色、语种、情感等属性的有效控制。提升了神经网络语音合成的质量和灵活性,一小时数据构建语音合成系统自然度可达到4.0MOS分,达到并超过普通说话人水平。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音合成;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%90%AC%E6%84%9F%E9%87%8F%E5%8C%96%E7%BC%96%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">听感量化编码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%91%E6%95%B0%E6%8D%AE%E9%87%8F%E5%90%88%E6%88%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">少数据量合成;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%A8%E8%AF%AD%E7%A7%8D%E5%90%88%E6%88%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跨语种合成;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E6%8E%A7%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感控制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘庆峰(1973-),男,博士,教授,博士生导师。研究方向:信号处理,语音及语言信息处理。;
                                </span>
                                <span>
                                    江源(1983-),男,博士研究生。研究方向:语音信号处理,语音合成。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-06-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61871358);</span>
                    </p>
            </div>
                    <h1><b>Research on Perception Quantification-based Neural Speech Synthesis Methods</b></h1>
                    <h2>
                    <span>LIU Qingfeng</span>
                    <span>JIANG Yuan</span>
                    <span>HU Yajun</span>
                    <span>LIU Lijuan</span>
            </h2>
                    <h2>
                    <span>National Engineering Laboratory for Speech and Language Information Processing</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Current neural network based speech synthesis framework is designed for single speaker, requiring at least a few hours training, and cannot make use of speech data from different speakers, languages, styles. To address this problem, a perception quantification-based neural network speech synthesis method was proposed. In the proposed method, a perception quantification-based model was designed to learn the representations for different attributes of speech. A unified acoustic model was built using the learnt perception quantification representations for different speakers, languages and styles. An adaptation method was introduced to transfer the knowledge from the unified acoustic model to new speakers with limited speech data. The proposed method could effectively control the speaker, language, and style of synthetic speech, achieve cross-language, cross-style speech synthesis, and the adaptation method could reduce the demand for training data to a few minutes. The proposed methods significantly improved the quality and flexibility of speech synthesis systems, and the naturalness of synthesized speech is similar to or better than an average mandarin speaker.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20synthesis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech synthesis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=perception%20quantification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">perception quantification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=limited%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">limited data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cross-language&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cross-language;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=style%20control&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">style control;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-06-24</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Natural Science Foundation of China(61871358);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="34">随着新一代人工智能浪潮的不断发展,人们对信息获取的手段和方式提出了越来越高的要求。语音语言是人类最自然便捷的交流方式,将引领万物互联时代的人机交互革命;语音语言也是文化的基础和民族的象征,对国家安全和文化传播具有重要的战略意义。其中,人工智能要实现语音和语言关键技术的突破,语音合成效果是否能够达到真人水平是一项关键难题。</p>
                </div>
                <div class="p1">
                    <p id="35">近年来,随着神经网络学习算法被引入到语音合成领域,其更好的建模精度和强大的数据利用能力,迅速替代了传统的隐马尔科夫模型(Hidden Markov Model,HMM)<citation id="84" type="reference"><link href="2" rel="bibliography" /><link href="4" rel="bibliography" /><link href="6" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>在统计建模语音合成方法中的地位,成为语音合成的主流方法。</p>
                </div>
                <div class="p1">
                    <p id="36">例如,Ling<citation id="85" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了使用受限玻尔兹曼机(Restricted Boltzmann Machines,RBMs)和深度信念网络(Deep Belief Network,DBN)替换掉HMM模型中状态分布中的高斯分布,以提升模型的表征能力;考虑到深度全连接网络(Deep Neural Networks,DNN)强大的建模能力,Zen<citation id="86" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出完全抛HMM模型,使用DNN直接对文本特征到声学参数的映射进行建模;为了考虑声学参数相邻帧之间的关系,Fan<citation id="87" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, Zen<citation id="88" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>进一步提出使用递归神经网络(Recurrent Neural Network,RNN)和长短时记忆单元(Long Short Term Memory,LSTM)对文本特征到声学参数的映射进行建模。基于深度神经网络的声学建模方法<citation id="89" type="reference"><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><link href="20" rel="bibliography" /><link href="22" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>成为了目前的主流方法。</p>
                </div>
                <div class="p1">
                    <p id="37">但当前语音合成技术在自然度、表现力和控制力上还有很多待提升的空间。分析其原因,同样一句文本,不同发音人在不同环境下会有多种多样的发音方式,而传统模型中并没有考虑他们的不同。本文提出一种神经网络框架下基于听感量化编码的多人混合语音合成建模方法。“听感量化”<citation id="90" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的基本思想就是把听感中不同属性特征相互剥离,通过多组发音人、风格、情感、语种等控制编码来表征,将自然语流中形形色色的复杂变体使用统一模型来建模。方法上本文首先根据语音数据的特点和属性设计多组听感量化编码,如发音人、语种、情感风格,再结合传统LSTM-RNN构建文本输入到语音输出的模型。实现海量多人数据的混合训练,可以高质量稳定的实现文本到语音的预测;在语音合成阶段可通过听感量化编码的预测控制,实现多音色、多语种、多风格的控制能力;并通过迁移学习,实现少数据量训练样本合成能力,以及跨语种跨风格的能力迁移。通过听感量化编码技术的加入,可稳定提升合成系统的自然度效果,并实现单人数据的跨语种、跨风格的语音合成。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag"><b>1</b> 基线系统</h3>
                <div class="p1">
                    <p id="39">基于神经网络的声学建模方法是在给定文本<i>c</i>的条件下,直接对声学参数<i>x</i>进行预测。本文的基线系统<citation id="91" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>采用全连接前馈(Feed-Forward,FF)网络和长短时记忆网络(LSTM-RNN)实现,模型框图如图所示,输入<i>c</i>为经过帧级拓展的音素上下文信息,输出<i>x</i>为帧级声学参数。模型结构(如图1所示)采用1层FF加3层LSTMP网络构成。</p>
                </div>
                <div class="p1">
                    <p id="40">在使用单人1小时左右标准录音棚数据情况下,该LSTM-RNN模型可以获得自然度约3.7MOS左右的基线系统。进一步提升单人的数据量至10小时左右,可提升自然度接近4.0MOS分。可见神经网络对数据有很强的利用能力,加大数据量对提升模型效果有显著帮助。但高质量的录音数据采集是较为困难的,录制单人10小时以上数据的周期和工作量很大。要进一步提升数据量,混用多人数据来训练模型成为必要选择。如果直接使用多人的海量数据采用同样模型训练合成系统,由于发音人音色、风格存在巨大差别,其模型结果混乱导致无法有效合成。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909017_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基线LSTM-RNN模型结构" src="Detail/GetImg?filename=images/DZKK201909017_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基线LSTM-RNN模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909017_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Structure of LSTM-RNN baseline system.</p>

                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>2</b> 听感量化编码模型</h3>
                <div class="p1">
                    <p id="43">针对多人、多语种、多风格混合语音的训练问题,本文提出了基于听感量化编码模型,首先对语音中的不同属性信息进行听感量化编码,然后对语音的声学参数进行预测,显示地对多人混合语音的说话人、语种、情感风格等属性建模。模型训练的流程框图如图2所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909017_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 听感量化编码模型训练框图" src="Detail/GetImg?filename=images/DZKK201909017_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 听感量化编码模型训练框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909017_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Flowchart of perception quantification-based
 model training method</p>

                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>2.1</b> 模型概述</h4>
                <div class="p1">
                    <p id="46">假设多人混合数据集中有<i>n</i>句训练语料,其声学参数为{<i>x</i>:<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,<i>x</i><sub>3</sub>,…,<i>x</i><sub><i>n</i></sub>},与其对应的文本为{<i>c</i>:<i>c</i><sub>1</sub>,<i>c</i><sub>2</sub>,<i>c</i><sub>3</sub>,…,<i>c</i><sub><i>n</i></sub>}。为了实现对说话人、语种、说话风格等的控制,对语音中的信息进行分解,定义了4种听感量化编码:说话人编码<i>λ</i><sup><i>s</i></sup>、语种编码<i>λ</i><sup><i>l</i></sup>、情感风格编码<i>λ</i><sup><i>e</i></sup>、残差编码<i>λ</i><sup><i>r</i></sup>。传统语音合成声学建模方法是用神经网络直接对<i>p</i>(<i>x</i>|<i>c</i>)建模,等价于隐式地对<i>p</i>(<i>x</i>,<i>λ</i><sup><i>s</i></sup>,<i>λ</i><sup><i>l</i></sup>,<i>λ</i><sup><i>e</i></sup>,<i>λ</i><sup><i>r</i></sup>)的边缘概率进行建模,因而无法对合成语音进行控制,如式(1)所示。</p>
                </div>
                <div class="p1">
                    <p id="47"><i>p</i>(<i>x</i>|<i>c</i>)=∫<sub><i>λ</i></sub><i>p</i>(<i>x</i>,<i>λ</i>|<i>c</i>)d<i>λ</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="48">其中,<i>λ</i>=[<i>λ</i><sup><i>s</i></sup>,<i>λ</i><sup><i>l</i></sup>,<i>λ</i><sup><i>e</i></sup>,<i>λ</i><sup><i>r</i></sup>]。</p>
                </div>
                <div class="p1">
                    <p id="49">为解决这一问题,本文通过显示地对说话人、语种、情感风格进行听感量化编码,直接对这些听感量化编码和声学参数的联合分布进行建模,如式(2)所示。</p>
                </div>
                <div class="p1">
                    <p id="50"><i>p</i>(<i>x</i>,<i>λ</i><sup><i>s</i></sup>,<i>λ</i><sup><i>l</i></sup>,<i>λ</i><sup><i>e</i></sup>,<i>λ</i><sup><i>r</i></sup>|<i>c</i>)=<i>p</i>(<i>λ</i><sup><i>s</i></sup> )<i>p</i>(<i>λ</i><sup><i>l</i></sup> )<i>p</i>(<i>λ</i><sup><i>r</i></sup> )<i>p</i>(<i>λ</i><sup><i>e</i></sup>|<i>c</i>,<i>λ</i><sup><i>l</i></sup>,<i>λ</i><sup><i>s</i></sup> )</p>
                </div>
                <div class="p1">
                    <p id="51"><i>p</i>(<i>x</i>│<i>λ</i><sup><i>s</i></sup>,<i>λ</i><sup><i>l</i></sup>,<i>λ</i><sup><i>e</i></sup>,<i>λ</i><sup><i>r</i></sup>,<i>c</i>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="52">其中说话人、语种和文本是不相关的;情感风格与依赖于说话人、语种以及文本;残差编码是语音中去除文本、说话人、语种、情感风格后剩余的信息,与其他编码都不相关。整个模型概率图如图3所示。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909017_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 听感量化编码模型的概率图" src="Detail/GetImg?filename=images/DZKK201909017_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 听感量化编码模型的概率图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909017_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Probabilistic graph of perception quantification model</p>

                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.2</b> 模型实现</h4>
                <div class="p1">
                    <p id="55">整个模型使用神经网络进行实现,主要包含两部分:</p>
                </div>
                <div class="p1">
                    <p id="56">(1)主网络是在给定文本和听感量化编码时预测声学参数,即p(x|λ<sup>s</sup>,λ<sup>l</sup>,λ<sup>e</sup>,λ<sup>r</sup>,c),为了更好地对语音的时序相关性建模,本文使用长短时记忆网络进行实现</p>
                </div>
                <div class="p1">
                    <p id="57">x=NN([λ<sup>s</sup>,λ<sup>l</sup>,λ<sup>e</sup>,λ<sup>r</sup>,c])      (3)</p>
                </div>
                <div class="p1">
                    <p id="58">通过不同说话人共享一个主网络,使模型能够学习到不同说话人、语种、风格编码对合成语音的影响。</p>
                </div>
                <div class="p1">
                    <p id="59">(2)旁支网络是在给定文本、语种、说话人时,对情感编码进行预测,即p(λ<sup>e</sup>|c,λ<sup>l</sup>,λ<sup>s</sup> ),同样使用神经网络进行实现,如下</p>
                </div>
                <div class="p1">
                    <p id="60">λ<sup>e</sup>=NN([λ<sup>s</sup>,λ<sup>l</sup>,c] )      (4)</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>2.3</b> 模型训练和生成</h4>
                <div class="p1">
                    <p id="62">在模型训练时,说话人、语种给定,情感量化编码通过人工标记进行定义。残差编码描述的是发音人在录制语音数据时由于情绪、环境、时间差异等不同状态下发音上的变化信息,无法人工标记,为此将每个句子使用一个单独残差编码λ<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>r</mi></msubsup></mrow></math></mathml>表示,残差编码随机初始化,通过模型训练进行更新。整个模型使用最小均方误差准则进行训练,使用随机梯度下降算法进行梯度更新。</p>
                </div>
                <div class="p1">
                    <p id="64">针对语料较少的新发音人,本文提出使用迁移学习的方法,提高少数据量下的语音合成效果。首先将训好的多人混合模型的权重作为新发音人模型的初始化,然后使用新发音人的少量数据进行微调,从而达到少数据量、新发音人的快速建模。</p>
                </div>
                <div class="p1">
                    <p id="65">在语音合成阶段,由于残差编码吸收的是说话人、语种、文本、情感风格之外的信息,无法进行预测,在合成时将其置零,以实现鲁棒的语音合成。通过指定说话人编码、语种编码、情感风格编码,我们能够实现跨说话人、跨语种、跨情感风格的语音合成。对于单语种的说话人,能够实现对其他语种的语音合成;对于单情感风格的说话人,能够实现多种情感风格的语音合成。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag"><b>3</b> 实验结果</h3>
                <div class="p1">
                    <p id="67">为了验证本文所提方法的有效性,采用了一个涵盖100个发音人共计1 000 <i>h</i>高质量语音数据库来进行实验。这个数据库在录音室环境下录制得到,覆盖了不同发音人(其中男生46人,女生54人)、不同语种(中文和英文)以及不同情感(中立、高兴、悲伤和生气)的语音。</p>
                </div>
                <div class="p1">
                    <p id="68">模型的输入包含898维上下文特征和11维时长、位置信息,这些特征中包含当前音素、前一个音素、后一个音素、声调、韵律特征等上下文信息。输出为50维声学特征,包含41维梅尔倒谱特征、基频及其一阶二阶差分特征、1维清浊标记和5维非周期谐波成份特征。听感量化部分对于说话人(100维)、语种(2维)、情感(4维)进行量化编码特征(<i>one</i>-<i>hot</i>形式),它们的编码向量(<i>embedding vector</i>)维度为512维。神经网络模型中,<i>FF</i>和<i>LSTM</i>隐层节点数均为512。合成系统的输出特征经过语音声码器恢复语音波形。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>3.1</b> 不同数据量实验</h4>
                <div class="p1">
                    <p id="70">为了对比基线系统和听感量化编码模型对数据量的要求,使用1位中文女声发音人,在10 h、1 h和5 min3种不同级别数据量上构建基线系统和听感量化编码模型系统。为了测试不同数据量下两组系统效果,实验安排10位测试人员对两个系统各50句中文文本合成语音进行自然度MOS打分测听。由于5 min下基线系统不能得到正常的合成语音,故未参加测试,测听结果如表1所示。</p>
                </div>
                <div class="area_img" id="71">
                    <p class="img_tit"><b>表</b>1 <b>中文发音人不同数据量</b>MOS<b>测听结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Mean Opinion Scores (MOS) on naturalness with different mandarin dataset</p>
                    <p class="img_note"></p>
                    <table id="71" border="1"><tr><td><br />中文自然度</td><td>基线系统</td><td>听感量化系统</td></tr><tr><td><br />中文10 h数据</td><td>4.02</td><td>4.22</td></tr><tr><td><br />中文1 h数据</td><td>3.70</td><td>4.05</td></tr><tr><td><br />中文5 min</td><td>-</td><td>3.46</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="72">由表1结果可以看出,相对于基线听感量化系统可有效提升同数据量下语音合成自然度约0.3MOS分,可在一小时数据下稳定达到普通人说话水平(4.0MOS分);并可显著降低合成系统构建的数据量要求,在5 min数据条件下也可获得自然度约3.5MOS分的实用合成系统。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>3.2</b> 跨语种实验</h4>
                <div class="p1">
                    <p id="74">为验证听感量化编码系统的跨语种合成能力,准备纯英文男声发音人1h数据,用于听感量化编码系统的迁移学习训练。同样安排10位测试人员对50句中文文本合成进行自然度MOS打分测听。由于纯英文数据的基线系统不能合成出正常的中文合成语音,故只测听感量化系统,结果如表2所示。</p>
                </div>
                <div class="area_img" id="75">
                    <p class="img_tit"><b>表</b>2 <b>英文发音人跨语种迁移学习结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2. Mean Opinion Scores (MOS) on naturalness of mandarin tests with English dataset</p>
                    <p class="img_note"></p>
                    <table id="75" border="1"><tr><td><br />中文自然度</td><td>基线系统</td><td>听感量化系统</td></tr><tr><td><br />英文1 h数据</td><td>-</td><td>3.74</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="76">从以上结果可以看出,听感量化编码系统可以有效的共享数据,纯英文发音人可通过听感量化编码进行迁移学习,获得中文文本合成能力,且发音人中英文音色完全相同,中文合成自然度达到3.7MOS分以上,足以实用。降低了对发音人多语言能力的要求,提升了跨语种合成系统能力。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>3.3</b> 情感控制实验</h4>
                <div class="p1">
                    <p id="78">为验证听感量化编码系统对情感风格的控制能力,准备一组情感童声发音人,拥有中立、高兴、悲伤和生气4种情感各1 h数据。分别构建4个单一情感风格的基线合成系统,以及4种情感加入混合训练的听感量化编码的合成系统。对15句中立文本在指定情感编码的情况分别合成,由10位测试人员对合成语音做情感判断选择。观察情感风格控制能力的正确率度,结果如表3所示。</p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表</b>3 <b>合成语音情感听辨正确率结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3. Percentages of perceived emotions of baseline and proposed methods</p>
                    <p class="img_note"></p>
                    <table id="79" border="1"><tr><td><br />情感判断<br />正确率</td><td>基线系统</td><td>听感量化系统</td><td>相对提升</td></tr><tr><td><br />中立</td><td>81.6%</td><td>92.7%</td><td>60.3%</td></tr><tr><td><br />开心</td><td>91.6%</td><td>100%</td><td>100%</td></tr><tr><td><br />生气</td><td>95.6%</td><td>98.3%</td><td>61.4%</td></tr><tr><td><br />悲伤</td><td>100%</td><td>100%</td><td>-</td></tr><tr><td><br />4项平均</td><td>92.2%</td><td>97.75%</td><td>71.2%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="80">从结果可见,4种情感混合训练的听感量化编码模型在控制情感风格的能力上相对单一风格的基线模型能力更强,在情感判断正确率上可以获得71.2%的相对提升。</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>4</b> 结论和展望</h3>
                <div class="p1">
                    <p id="82">本文提出了一种基于听感量化编码的神经网络语音合成方法,针对混合数据训练问题,本文设计了4种听感量化编码,分别来学习语音中说话人、语种、情感风格和残差信息,显示地对语音的不同属性进行分解建模,实现多人多语种多风格混合声学模型训练。在语音合成时,能够指定合成语音的发音人、语种和情感风格,显著提高了对合成语音的控制力;同时,能够在发音人缺少多语种、多情感风格数据时,实现跨语种、跨情感风格的语音合成;针对少量数据的新发音人,通过使用迁移学习的方法,将多人混合模型迁移到新的发音人上,显著降低了语音合成系统的数据要求,1 h数据构建的语音合成系统自然度即可达到4.0MOS分。</p>
                </div>
                <div class="p1">
                    <p id="83">语音中除说话人、语种、情感风格外,还有其他有关语音自然度的信息。从听感量化编码的角度来看,残差编码学到的正是这种信息,而现有方法对其没有深入探讨。为了提升合成语音自然度,未来将进一步对残差编码进行分析建模,包括尝试不同尺度的残差编码以及对残差编码的聚类分析等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous Modeling of Spectrum,Pitch and Duration in HMM-Based Speech Synthesis">

                                <b>[1]</b> Yoshimura T,Tokuda K,Masuko T,et al.Simultaneous modeling of spectrum,pitch and duration in HMM-based speech synthesis[C].Budapest:Sixth European Conference on Speech Communication and Technology,EUROSPEECH,1999.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HiddenMarkov models based on multi-space probabilitydistribution for pitch pattern modeling">

                                <b>[2]</b> Tokuda K,Masuko T,Miyazaki N,et al.Hidden Markov models based on multi-space probability distribution for pitch pattern modeling[C].Phoenix:International Conference on Acoustics,Speech and Signal Processing (ICASSP),1999.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speec h parameter generation algorithms for HMM-based speech synthesis">

                                <b>[3]</b> Tokuda K,Yoshimura T,Masuko T,et al.Speech parameter generation algorithms for HMM-based speech synthesis[C].Istanbul:International Conference on Acoustics,Speech and Signal Processing (ICASSP),2000.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling spectral envelopes using restricted BoltzmannMachines and Deep Belief Networks for statistical parameter speech synthesis">

                                <b>[4]</b> Ling Z H,Deng L,Yu D.Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis[J].IEEE Transactions on Audio,Speech,and Language Processing,2013,21(10):2129-2139.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning in speech synthesis">

                                <b>[5]</b> Zen H.Deep learning in speech synthesis[C].Guangzhou:Keynote Speech Given at Isca Speech Synthesis Workshop (SSW8),2013.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks">

                                <b>[6]</b> Fan Y,Qian Y,Xie F L,et al.TTS synthesis with bidirectional LSTM based recurrent neural networks[C].Minneapolis:Fifteenth Annual Conference of the International Speech Communication Association(ISCA),2014.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis">

                                <b>[7]</b> Zen H,Sak H.Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis[C].South Brisbane:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2015.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends">

                                <b>[8]</b> Ling Z H,Kang S Y,Zen H,et al.Deep learning for acoustic modeling in parametric speech generation:A systematic review of existing techniques and future trends[J].IEEE Signal Processing Magazine,2015,32(3):35-52.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis">

                                <b>[9]</b> Takaki S,Yamagishi J.A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis[C].Shanghai:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2016.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DNN-based stochastic postfilter for HMM-based speech synthesis">

                                <b>[10]</b> Chen L H,Raitio T,Valentini-Botinhao C,et al.DNN-based stochastic postfilter for HMM-based speech synthesis[C].Singapore:15<sup>th</sup> Annual Conference of the International Speech Communication Association,INTERSPEECH,2014.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial network-based postfilter for statistical parametric speech synthesis">

                                <b>[11]</b> Kaneko T,Kameoka H,Hojo N,et al.Generative adversarial network-based postfilter for statistical parametric speech synthesis[C].New Orleans:International Conference on Acoustics,Speech and Signal Processing (ICASSP),IEEE,2017.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 刘庆峰.基于听感量化理论的语音合成系统研究[D].合肥:中国科学技术大学,2003.Liu Qingfeng.Research on perception quantification-based speech synthesis system[D].Hefei:University of Science and Technology of China,2003.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DBN-based spectral feature representation for statistical parametric speech synthesis">

                                <b>[13]</b> Hu Y J,Ling Z H.DBN-based spectral feature representation for statistical parametric speech synthesis [J].IEEE Signal Processing Letters,2016,23(3):321-325.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The IFLYTEK system for blizzard challenge">

                                <b>[14]</b> Liu L J,Ding C,Jiang Y,et al.The IFLYTEK system for blizzard challenge[C].Stockholm:The Blizzard ChallengeWorkshop,2017.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Emotional statistical parametric speech synthesis using LSTM-RNNs">

                                <b>[15]</b> An S,Ling Z,Dai L.Emotional statistical parametric speech synthesis using LSTM-RNNs[C].Kuala Lumpur :Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),IEEE,2017.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMF314F7712E21E5B17A2E7978227FD9E2&amp;v=MjM4NzFnVE0rZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXQ5aHdMeTR3Szg9TmlmSVk4VzdIOVc2cUloRVpwNE5EUWs4dlJjVW16MElUM2JscEJBM2ZzVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Hu Y J,Ling Z H.Extracting spectral features using deep autoencoders with binary distributed hidden units for statistical parametric speech synthesis[J].IEEE/ACM Transactions on Audio,Speech and Language Processing (TASLP),2018,26(4):713-724.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201909017" />
        <input id="dpi" type="hidden" value="299" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201909017&amp;v=MzI3NTZWTC9NSVRmQVpiRzRIOWpNcG85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

