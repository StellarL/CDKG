

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139156330732500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201912008%26RESULT%3d1%26SIGN%3dQusPMImjFSK3ca9rlICwiM0SeT4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201912008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201912008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201912008&amp;v=MDU3MzZxQnRHRnJDVVI3cWZadVpwRnlyaFVidktJVGZBWmJHNEg5ak5yWTlGYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;1&lt;/b&gt; 基于Attention-CTC解码的识别算法 "><b>1</b> 基于Attention-CTC解码的识别算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="&lt;b&gt;1.1&lt;/b&gt; 多尺度特征提取"><b>1.1</b> 多尺度特征提取</a></li>
                                                <li><a href="#49" data-title="&lt;b&gt;1.2&lt;/b&gt; LSTM特征编码"><b>1.2</b> LSTM特征编码</a></li>
                                                <li><a href="#53" data-title="&lt;b&gt;1.3&lt;/b&gt; Attention-CTC解码"><b>1.3</b> Attention-CTC解码</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;2&lt;/b&gt; 实验分析 "><b>2</b> 实验分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;2.1&lt;/b&gt; 数据集与评价标准"><b>2.1</b> 数据集与评价标准</a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;2.2&lt;/b&gt; 训练细节"><b>2.2</b> 训练细节</a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;2.3&lt;/b&gt; 实验结果比较"><b>2.3</b> 实验结果比较</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="&lt;b&gt;3&lt;/b&gt; 结束语 "><b>3</b> 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="图1 网络结构">图1 网络结构</a></li>
                                                <li><a href="#51" data-title="图2 双向长短时期记忆网络">图2 双向长短时期记忆网络</a></li>
                                                <li><a href="#61" data-title="图3 Attention-CTC解码架构">图3 Attention-CTC解码架构</a></li>
                                                <li><a href="#78" data-title="图4 不同&lt;i&gt;λ&lt;/i&gt;的收敛曲线">图4 不同<i>λ</i>的收敛曲线</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同方法的识别率&lt;/b&gt;"><b>表</b>1 <b>不同方法的识别率</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" Yao C,Bai X,Shi B,et al.Strokelets:a learned multi-scale representation for scene text recognition[C].Columbus:Computer Vision and Pattern Recognition,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Strokelets:A learned multi-scale representation for scene text recognition">
                                        <b>[1]</b>
                                         Yao C,Bai X,Shi B,et al.Strokelets:a learned multi-scale representation for scene text recognition[C].Columbus:Computer Vision and Pattern Recognition,2014.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" Jaderberg M,Vedaldi A,Zisserman A.Deep features for text spotting[C].Cham:Computer Vision,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep features for text spot-ting">
                                        <b>[2]</b>
                                         Jaderberg M,Vedaldi A,Zisserman A.Deep features for text spotting[C].Cham:Computer Vision,2014.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" 薛皓天,杨晶东,谈凯德.一种改进的BP神经网络在手写体识别上的应用[J].电子科技,2015,28(5):20-23.Xue Haotian,Yang Jingdong,Tan Kaide.Application of an improved BP neural network in handwriting recognition[J].Electronic Science and Technology,2015,28(5):20-23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201505006&amp;v=MTkwNTVxcUJ0R0ZyQ1VSN3FmWnVacEZ5cmhVYnZLSVRmQVpiRzRIOVRNcW85RllvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         薛皓天,杨晶东,谈凯德.一种改进的BP神经网络在手写体识别上的应用[J].电子科技,2015,28(5):20-23.Xue Haotian,Yang Jingdong,Tan Kaide.Application of an improved BP neural network in handwriting recognition[J].Electronic Science and Technology,2015,28(5):20-23.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" 熊海朋,陈洋洋,陈春玮.基于卷积神经网络的场景图像文本定位研究[J].电子科技,2018,31(1):50-53.Xiong Haipeng,Chen Xiangxiang,Chen Chunwei.Text location in image based on convolution neural network[J].Electronic Science and Technology,2018,31(1):50-53." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201801014&amp;v=MzA2MTVFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyaFVidktJVGZBWmJHNEg5bk1ybzk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         熊海朋,陈洋洋,陈春玮.基于卷积神经网络的场景图像文本定位研究[J].电子科技,2018,31(1):50-53.Xiong Haipeng,Chen Xiangxiang,Chen Chunwei.Text location in image based on convolution neural network[J].Electronic Science and Technology,2018,31(1):50-53.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" Bahdanau D,Chorowski J,Serdyuk D,et al.End-to-end attention-based large vocabulary speech recognition[C].Shanghai:The 41&lt;sup&gt;st&lt;/sup&gt; IEEE International Conference on Acoustics,Speech and Signal Processing,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end attention-based large vocabulary speech recognition">
                                        <b>[5]</b>
                                         Bahdanau D,Chorowski J,Serdyuk D,et al.End-to-end attention-based large vocabulary speech recognition[C].Shanghai:The 41&lt;sup&gt;st&lt;/sup&gt; IEEE International Conference on Acoustics,Speech and Signal Processing,2016.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" Luong M T,Pham H,Manning C D.Effective approaches to attention-based neural machine translation[C].Lisbon:Empirical Methods in Natural Language Processing,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective Approaches to Attention-based Neural Machine Translation">
                                        <b>[6]</b>
                                         Luong M T,Pham H,Manning C D.Effective approaches to attention-based neural machine translation[C].Lisbon:Empirical Methods in Natural Language Processing,2015.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" Qu S,Xi Y,Ding S.Visual attention based on long-short term memory model for image caption generation[C].Melbourne:Control &amp;amp; Decision Conference.2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual attention based on long-short term memory model for image caption generation">
                                        <b>[7]</b>
                                         Qu S,Xi Y,Ding S.Visual attention based on long-short term memory model for image caption generation[C].Melbourne:Control &amp;amp; Decision Conference.2017.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[C].San Diego:International Conference on Learning Representations,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">
                                        <b>[8]</b>
                                         Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[C].San Diego:International Conference on Learning Representations,2015.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" Graves A,Gomez F.Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C].Hong Kong:International Conference on Machine Learning,2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Connectionist temporal classification:Labelling unsegmented sequence data with recurrent neural networks">
                                        <b>[9]</b>
                                         Graves A,Gomez F.Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C].Hong Kong:International Conference on Machine Learning,2006.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" 尹征,唐春晖,张轩雄.基于改进型稀疏自动编码器的图像识别[J].电子科技,2016,29(1):124-127.Yin Zheng,Tang Chunhui,Zhang Xuanxiong.Image recognition based on improved sparse auto-encoder[J].Electronic Science and Technology,2016,29(1):124-127." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201601035&amp;v=MTUxOTFNcm85R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5cmhVYnZLSVRmQVpiRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         尹征,唐春晖,张轩雄.基于改进型稀疏自动编码器的图像识别[J].电子科技,2016,29(1):124-127.Yin Zheng,Tang Chunhui,Zhang Xuanxiong.Image recognition based on improved sparse auto-encoder[J].Electronic Science and Technology,2016,29(1):124-127.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjI5NDFCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUlsb1diaEE9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4bw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" Lucas S M,Panaretos A,Sosa L,et al.ICDAR 2003 robust reading competitions[J].Proceeding of the Icdar,2003,7(2-3):105-122." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ICDAR 2003 Robust Read-ing Competitions">
                                        <b>[12]</b>
                                         Lucas S M,Panaretos A,Sosa L,et al.ICDAR 2003 robust reading competitions[J].Proceeding of the Icdar,2003,7(2-3):105-122.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" Ioffe S,Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C].Lille Grand Palais:International Conference on Machine Learning,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[13]</b>
                                         Ioffe S,Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C].Lille Grand Palais:International Conference on Machine Learning,2015.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" Szegedy C,Vanhoucke V,Ioffe S,et al.Rethinking the inception architecture for computer vision[C].Las Vegas:Computer Vision and Pattern Recognition,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the Inception Architecture for Computer Vision">
                                        <b>[14]</b>
                                         Szegedy C,Vanhoucke V,Ioffe S,et al.Rethinking the inception architecture for computer vision[C].Las Vegas:Computer Vision and Pattern Recognition,2016.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" Szegedy C,Ioffe S,Vanhoucke V,et al.Inception-v4,inception-resnet and the impact of residual connections on learning[C].San Francisco:The Thirty-First AAAI Conference on Artificial Intelligence,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the impact of residual connections on learning">
                                        <b>[15]</b>
                                         Szegedy C,Ioffe S,Vanhoucke V,et al.Inception-v4,inception-resnet and the impact of residual connections on learning[C].San Francisco:The Thirty-First AAAI Conference on Artificial Intelligence,2017.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" Kim S,Hori T,Watanabe S.Joint CTC-attention based end-to-end speech recognition using multi-task learning[C].New Orleans:The 42&lt;sup&gt;nd&lt;/sup&gt; IEEE International Conference on Acoustics,Speech and Signal Processing,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning">
                                        <b>[16]</b>
                                         Kim S,Hori T,Watanabe S.Joint CTC-attention based end-to-end speech recognition using multi-task learning[C].New Orleans:The 42&lt;sup&gt;nd&lt;/sup&gt; IEEE International Conference on Acoustics,Speech and Signal Processing,2017.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" Hori T,Watanabe S,Zhang Y,et al.Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM[C].USA:IEEE International Conference,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM">
                                        <b>[17]</b>
                                         Hori T,Watanabe S,Zhang Y,et al.Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM[C].USA:IEEE International Conference,2017.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" Xu K,Li D,Cassimatis N,et al.LCANet:end-to-end lipreading with cascaded attention-CTC[C].Xi’an:China Automatic Face &amp;amp; Gesture Recognition,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LCANet:end-to-end lipreading with cascaded attention-CTC">
                                        <b>[18]</b>
                                         Xu K,Li D,Cassimatis N,et al.LCANet:end-to-end lipreading with cascaded attention-CTC[C].Xi’an:China Automatic Face &amp;amp; Gesture Recognition,2018.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2019-03-15 10:03</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(12),32-36 DOI:10.16180/j.cnki.issn1007-7820.2019.12.007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于Attention-CTC的自然场景文本识别算法</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%92%8C%E6%96%87%E6%9D%B0&amp;code=33367410&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">和文杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%95%AC%E5%BD%AA&amp;code=07048030&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘敬彪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BD%98%E5%8B%89&amp;code=32363910&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潘勉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E5%B8%85%E5%B8%85&amp;code=37919180&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕帅帅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%9D%AD%E5%B7%9E%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0073968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杭州电子科技大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对自然场景下文本识别所存在的字符分割困难、识别精度依赖字典等问题,文中提出了一种基于注意力机制与连接时间分类损失相结合的文本识别算法。利用卷积神经网络与双向长短时期记忆网络实现对图像的特征编码,再使用Attention-CTC结构实现对特征序列的解码,有效解决Attention解码无约束的问题。该算法避免了对标签进行额外对齐预处理和后续语法处理,在加快训练收敛速度的同时显著提高了文本识别率。实验结果表明,该算法对字体模糊、背景复杂的文本图像都具有很好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%9E%E6%8E%A5%E6%97%B6%E9%97%B4%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">连接时间分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">循环神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    和文杰(1994-),男,硕士研究生。研究方向:机器视觉。;
                                </span>
                                <span>
                                    刘敬彪(1964-),男,博士,教授。研究方向:电子系统集成技术。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61871164,61501155);</span>
                    </p>
            </div>
                    <h1><b>Natural Scene Text Recognition Algorithm Based on Attention-CTC</b></h1>
                    <h2>
                    <span>HE Wenjie</span>
                    <span>LIU Jingbiao</span>
                    <span>PAN Mian</span>
                    <span>LÜ Shuaishuai</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Information,Hangzhou Dianzi University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problems of the difficulty of character segmentation in text recognition and the recognition accuracy dependent on dictionary in natural scene, a text recognition algorithm with attention mechanism and connection time classification loss was proposed. The convolutional neural network and the bidirectional long-term memory network were used to extra the feature of the image. The Attention-CTC structure was used to decode the feature sequence, which effectively solved the problem of Attention decoding unconstrained. The algorithm avoided additional alignment preprocessing and subsequent syntax processing on the tag, which sped up the training convergence rate and significantly improved the text recognition rate. Experimental results showed that the algorithm was robust to texts with complex fonts and complex backgrounds.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=connection%20time%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">connection time classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recurrent%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recurrent neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-07</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Natural Science Foundation of China(61871164,61501155);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="38">随着智能化和移动终端的大力普及,自然场景图像的语义信息在自动驾驶、智能交通和视觉辅助等领域发挥越来越重要的作用。传统的光学字符识别(Optical Character Recognition,OCR)主要识别背景简单、文本分布固定和字体颜色大小一致的高质量文档图像。而自然场景中的文本存在背景复杂、分率低、字体颜色多样且大小不统一等问题,这些复杂因素使得传统的光学识别算法无法直接应用自然场景图像上。</p>
                </div>
                <div class="p1">
                    <p id="39">为解决自然场景图像中文本难以识别的问题,Strokelets<citation id="88" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>通过对图像块聚类来获取文本中的笔画特征,利用方向梯度直方图(Histogram of Oriented Gradient,HOG)特征检测字符,结合随机森林分类器对字符进行分类。Jaderberg M<citation id="89" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等提出的算法结合文本/非文本分类器、字符分类器和二元语言模型分类器,对图像进行密集滑动窗口扫描,并利用固定词典对图片中的单词进行分析,从而达到对文本语义分割和识别的目的。</p>
                </div>
                <div class="p1">
                    <p id="40">目前文本识别算法普遍存在以下问题:(1)文字特征依赖人工定义。人工定义的特征难以捕获图片的深层语义,制作耗时且通用性不高;(2)基于单字符的识别<citation id="90" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。单字符识别会脱离上下文环境,易导致歧义;(3)需要对字符进行分割定位<citation id="91" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。文本结构复杂、语义多变,强行分割会破坏字符结构;(4)过度依赖分类字典。分类字典的选取直接影响识别结果,导致识别模型泛化能力差。</p>
                </div>
                <div class="p1">
                    <p id="41">本文针对文本识别分割困难和识别精度依赖字典等问题,提出一种新的基于注意力机制(Attention Mechanism,AM)<citation id="96" type="reference"><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>和连接时间分类损失(Connectionist Temporal Classification,CTC) <citation id="92" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的自然场景文本识别算法。该算法由多尺度特征提取层<citation id="93" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,长短时期记忆网络(Long Short-Term Memory,LSTM)<citation id="94" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>编码层和Attention-CTC解码层组成。输入为自然场景文本图像,输出为解码后的文本,实现了从图像到文本的端到端识别机制,并在ICDAR2003<citation id="95" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和SVT数据集上对该算法的先进性进行了客观验证。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>1</b> 基于Attention-CTC解码的识别算法</h3>
                <div class="p1">
                    <p id="43">本文提出的基于Attention-CTC的自然场景文本识别算法,充分考虑到各功能模块的局部最优效果和最终的整体识别效果。首先对原始数据集进行预处理,输入到网络后利用多尺度卷积神经网络模型进行特征提取;再将特征图输入到LSTM网络进行编码;最后利用Attention-CTC对编码后的特征序列解码。网络整体设计如图1所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201912008_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络结构" src="Detail/GetImg?filename=images/DZKK201912008_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201912008_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Network structure</p>

                </div>
                <div class="p1">
                    <p id="45">该算法具备以下特点:(1)利用基于Inception V3<citation id="97" type="reference"><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>卷积神经网络提取文本图像的多尺度特征;(2)使用LSTM对特征序列编码并隐式的学习字符级的语言特征,从而避免了使用<i>n</i>元语法模型对字符进行后处理;(3)本文首次利用Attention与CTC相结合的结构对特征序列解码,其中Attention用来对特征序列解码,CTC实现对Attention的约束。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46"><b>1.1</b> 多尺度特征提取</h4>
                <div class="p1">
                    <p id="47">卷积神经网络模型的特征提取层一般使用单一尺度卷积核,该设计存在以下问题:(1)单一尺寸的卷积核表达能力有限,导致基础特征提取能力较弱;(2)随着网络的加深参数增多,在数据有限的情况下容易过拟合;(3)网络层数增加导致梯度消失,使得训练难以进行。</p>
                </div>
                <div class="p1">
                    <p id="48">针对以上问题,本文利用基于Inception V3卷积神经网络提取文本图像的多尺度特征。Inception模块是googLet中的基本单位,使用不同的卷积核提取不同尺寸的特征,增强了基本特征的提取能力。本文基于Inception V3进行改进,使用1×1、1×3和3×1大小的基本卷积核,通过多个卷积交替结构实现3×3和5×5尺寸的卷积核。交替卷积比单一卷积层的结构能更好的提取出深层特征,并且将参数量减少了28%。</p>
                </div>
                <h4 class="anchor-tag" id="49" name="49"><b>1.2</b> LSTM特征编码</h4>
                <div class="p1">
                    <p id="50">循环神经网络(Recurrent Neural Network,RNN)对处理文本序列问题有独特的优势,但RNN随着时间间隔增加会出现梯度消失现象,导致其无法处理长距离依赖问题。针对此问题,LSTM 可以通过特殊的门结构让信息有选择性地影响RNN每个时刻的状态,从而避免梯度消失。LSTM是定向的,它只使用过去的上下文。然而,在基于图像的序列中,两个方向的上下文是相互有用且互补的。因此本文采用双向长短时期记忆网络对图片特征进行双向编码,如图2所示。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201912008_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双向长短时期记忆网络" src="Detail/GetImg?filename=images/DZKK201912008_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 双向长短时期记忆网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201912008_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Bidirectional long short-term memory network</p>

                </div>
                <div class="p1">
                    <p id="52">其中,<i><b>x</b></i>为输入,即图片经过卷积后得到的特征图序列;<i><b>h</b></i>为前向LSTM的隐含层; <i><b>h</b></i>′为反向LSTM隐含层;<i><b>y</b></i>为预测输出;SOS为起始符;EOS为终止符。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53"><b>1.3</b> Attention-CTC解码</h4>
                <div class="p1">
                    <p id="54">解码是将图像特征序列转换为字符序列的过程。给定输入特征序列<i><b>X</b></i>={<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>w</i></sub>},隐变量<i><b>Z</b></i>={<i>z</i><sub><i>t</i></sub>∈<i><b>D</b></i>∪blank | <i>t</i>=1,…,<i>W</i>},输出长度为<i>L</i>的序列<i><b>Y</b></i>={<i>y</i><sub><i>l</i></sub>∈<i><b>D</b></i><image href="images/DZKK201912008_084.jpg" type="" display="inline" placement="inline"><alt></alt></image><i>l</i>=1,…,<i>T</i>}。其中,<i>W</i>为编码序列个数;<i>T</i>为字符个数;<i><b>D</b></i>为包含所有字符的字典。CTC(Connectionist Temporal Classification)假设标签之间是独立的,利用贝叶斯定理计算预测序列的后验概率分布</p>
                </div>
                <div class="p1">
                    <p id="55"><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>t</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>≈</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>z</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∏</mo><mi>t</mi></munder><mi>p</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>z</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="56">式中,<i>p</i>(<i><b>Y</b></i>)为字符级别的语言模型;<i>p</i>(<i>z</i><sub><i>t</i></sub>|<i><b>X</b></i>)为已知输入特征得到的隐变量概率;<i>p</i>(<i>z</i><sub><i>t</i></sub>|<i>z</i><sub><i>t</i></sub><sub>-1</sub>,<i><b>Y</b></i>)为已知上一时刻输出的隐变量预测下一时刻的隐变量的条件概率。CTC算法假设标签内部之间是条件独立的,并且每次输出都是单个字符的概率,这导致CTC只是针对局部信息进行预测,忽略了整体信息,因此无法有效预测长文本序列。</p>
                </div>
                <div class="p1">
                    <p id="57">相对于CTC的局部预测,注意力机制直接预测文本序列而不需要计算隐变量和做出标签内部相互独立的假设,直接计算联合预测序列的概率</p>
                </div>
                <div class="p1">
                    <p id="58"><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∏</mo><mi>l</mi></munder><mi>p</mi></mstyle><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>∶</mo><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="59">式中,<i>p</i>(<i>y</i><sub><i>l</i></sub>|<i>y</i><sub>1∶</sub><sub><i>l</i></sub><sub>-1</sub>,<i><b>X</b></i>)表示已知输入特征<i><b>X</b></i>和前<i>l</i>个输出得到l时刻的预测概率。</p>
                </div>
                <div class="p1">
                    <p id="60">单纯的注意力机制未引入任何引导对齐的约束条件而造成对噪声敏感、解码时会产生错位的问题。因此,本文设计了一种基于Attention和CTC联合训练的多任务学习解码器<citation id="98" type="reference"><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>,使用 Attention对字符级别的语义进行解码,并利用CTC实现对 Attention解码的约束。基于Attention-CTC的解码算法不但有效解决了纯粹的数据驱动方法对长序列输入难以训练的问题,还可充分对长字符提取信息。Attention-CTC解码框架如图3所示。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201912008_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Attention-CTC解码架构" src="Detail/GetImg?filename=images/DZKK201912008_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Attention-CTC解码架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201912008_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Decoding structure base Attention-CTC</p>

                </div>
                <div class="p1">
                    <p id="62">其中CTC和Attention模型共享编码网络;<i><b>q</b></i>为输入的编码网络的特征;<i><b>h</b></i>为编码时每个输入对应的隐含状态;<i><b>a</b></i>为注意力权重向量;<i><b>c</b></i>为解码的语义向量;<i><b>s</b></i>表示解码网络的隐含层状态;<i><b>y</b></i>为预测输出。CTC与Attention预测的最大化联合概率可表示为</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">Y</mi><mo>^</mo></mover><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>Y</mi><mo>∈</mo><mi>D</mi></mrow></munder><mo stretchy="false">{</mo><mi>λ</mi><mrow><mi>log</mi></mrow><mspace width="0.25em" /><mi>p</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>t</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>λ</mi><mo stretchy="false">)</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">将最大化联合概率转换成最小化多任务损失函数,可直接通过梯度下降进行训练求解</p>
                </div>
                <div class="p1">
                    <p id="65"><i>L</i>=<i>λL</i><sub>ctc</sub>+(1-<i>λ</i>)<i>L</i><sub>attn</sub>      (6)</p>
                </div>
                <div class="p1">
                    <p id="66">其中,<i>L</i><sub>ctc</sub>为CTC的损失函数,<i>L</i><sub>attn</sub>是注意力模型的损失函数;可变参数<i>λ</i>的取值范围为0≤<i>λ</i>≤1。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag"><b>2</b> 实验分析</h3>
                <h4 class="anchor-tag" id="68" name="68"><b>2.1</b> 数据集与评价标准</h4>
                <div class="p1">
                    <p id="69">数据集:为了验证本文算法的有效性和适用性,本文选取ICDAR 2003、Street View Text和IIIT5K 3个标准的文本识别数据集样本。其中ICDAR 2003选取了最小尺寸为100×100像素的561幅图像,对于单词识别任务,只考虑长度大于或等于3个字符的单词。Street View Text数据集中图像文字大多数来自商业标牌,因此该数据集常被用于户外文本识别研究。IIIT5K数据集包含由各类场景图像和生成图像裁剪得到的5 000单词图像,包括广告牌、招牌、门牌号码、房屋牌名和电影海报等丰富的类别。</p>
                </div>
                <div class="p1">
                    <p id="70">评价标准:本文使用词错误率(Word Error Rate,WER)作为字符识别的评价标准。为了使识别出来的词序列和标准的词序列之间保持一致,需要进行替换,删除,或者插入某些词,这些变换次数除以标准的词序列中词的个数的百分比,即为WER,其计算公式如下所示</p>
                </div>
                <div class="p1">
                    <p id="71"><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>W</mtext><mtext>E</mtext><mtext>R</mtext><mo>=</mo><mn>1</mn><mn>0</mn><mn>0</mn><mo>×</mo><mfrac><mrow><mi>S</mi><mo>+</mo><mi>D</mi><mo>+</mo><mi>Ι</mi></mrow><mi>Ν</mi></mfrac><mi>%</mi></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="72">式中,<i>S</i>为替换的字符个数;<i>D</i>为删除的字符个数;<i>I</i>为插入的字符个数;<i>N</i>为标签的字符个数。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>2.2</b> 训练细节</h4>
                <div class="p1">
                    <p id="74">本文算法自动从图片中裁剪出文本区域,并将高固定缩放到32 px,宽等比例缩放。训练过程中使用标准的梯度下降法,学习率初始为0.001,每批次32张图片,训练1 000个epoch后停止。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>2.3</b> 实验结果比较</h4>
                <div class="p1">
                    <p id="76">传统方法大都依赖于字典,与本文提出的方法不具有对比性,因此本文选取CRNN和Attention-OCR两种深度学习方法与本文提出的识别算法进行对比。其中CRNN使用CTC进行解码,Attention-OCR利用Attention解码。并分别在ICDAR2003、III5K-word和SVT共3个数据集上对各方法的性能作比较分析。</p>
                </div>
                <div class="p1">
                    <p id="77">本文引入了一个超参数,用来平衡注意力模型和CTC的权重。<i>λ</i>=1时只使用CTC解码;<i>λ</i>=0时只使用Attention进行解码。<i>λ</i>在0～1之间,CTC对Attention进行不同权重的约束。本文设计了0.0,0.2,0.5,0.8,1.0共5个<i>λ</i>值来进行实验对比分析。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201912008_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同λ的收敛曲线" src="Detail/GetImg?filename=images/DZKK201912008_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同<i>λ</i>的收敛曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201912008_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Convergence curves of different models</p>

                </div>
                <div class="p1">
                    <p id="79">图4所示为<i>λ</i>值与迭代次数的关系曲线。从图中可以看出,<i>λ</i>越大收敛速度越快,两者之间成正相关。其原因是CTC反向传播时直接对参数进行更新;而Attention需要计算解码特征和所有编码特征之间的注意力权重,再对权重进行更新,特征之间没有约束,导致参数解空间过大,不易收敛。当添加CTC作为Attention约束惩罚时,模型收敛更快,<i>λ</i>越大,CTC对Attention约束越强,对解空间范围限制越强,收敛速度越快。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表</b>1 <b>不同方法的识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Recognition rate of different methods</p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />模型</td><td>ICDAR03</td><td>IIIT5K</td><td>SVT</td></tr><tr><td><br />CRNN</td><td>0.76</td><td>0.68</td><td>0.70</td></tr><tr><td><br />Attention</td><td>0.80</td><td><b>0.74</b></td><td>0.77</td></tr><tr><td><br />Proposed (0.2)</td><td><b>0.86</b></td><td>0.76</td><td><b>0.78</b></td></tr><tr><td><br />Proposed (0.5)</td><td>0.81</td><td>0.73</td><td>0.76</td></tr><tr><td><br />Proposed (0.8)</td><td>0.78</td><td>0.70</td><td>0.71</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">表1为不同方法在不同数据集上测试的准确率。从图中可以看出,本文提出的算法在不同数据集上具有较高的识别率。<i>λ</i>=0.2时,在ICDAR03和SVT上均超过了CRNN和Attention-OCR算法。这是因为CRNN预测时是基于条件独立的,丢弃了先前时刻的信息,造成解码时只关注局部特征。单纯的Attention解码没有空间约束,当文本图像中有相似字符时解码的特征权重偏移,导致解码字符偏移或误检。Attention-CTC使用CTC对Attention进行空间约束,使得解码时更关注当前的特征,减缓了注意力偏移的问题,提高了识别率。</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag"><b>3</b> 结束语</h3>
                <div class="p1">
                    <p id="83">自然场景下文本的识别对计算机理解图像具有重要的意义,但是传统的方法需要进行复杂的预处理和精心设计的特征提取方法,导致识别精度低,泛化能力差。本文分析了CTC解码和Attention解码的优缺点,并提出了基于Attention与CTC联合训练的方法,在不同数据集上均获得了较高的识别率。但本文仍存在需要改进的地方:(1)改进卷积神经网络架构获得更强的特征提取能力;(2)增加过拟合机制提高网络的泛化能力;(3)增加多语言数据,如汉语、蒙语等,扩展模型的应用范围。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Strokelets:A learned multi-scale representation for scene text recognition">

                                <b>[1]</b> Yao C,Bai X,Shi B,et al.Strokelets:a learned multi-scale representation for scene text recognition[C].Columbus:Computer Vision and Pattern Recognition,2014.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep features for text spot-ting">

                                <b>[2]</b> Jaderberg M,Vedaldi A,Zisserman A.Deep features for text spotting[C].Cham:Computer Vision,2014.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201505006&amp;v=MDgzNjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyaFVidktJVGZBWmJHNEg5VE1xbzlGWW9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 薛皓天,杨晶东,谈凯德.一种改进的BP神经网络在手写体识别上的应用[J].电子科技,2015,28(5):20-23.Xue Haotian,Yang Jingdong,Tan Kaide.Application of an improved BP neural network in handwriting recognition[J].Electronic Science and Technology,2015,28(5):20-23.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201801014&amp;v=MDI3MDBJVGZBWmJHNEg5bk1ybzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyaFVidks=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 熊海朋,陈洋洋,陈春玮.基于卷积神经网络的场景图像文本定位研究[J].电子科技,2018,31(1):50-53.Xiong Haipeng,Chen Xiangxiang,Chen Chunwei.Text location in image based on convolution neural network[J].Electronic Science and Technology,2018,31(1):50-53.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end attention-based large vocabulary speech recognition">

                                <b>[5]</b> Bahdanau D,Chorowski J,Serdyuk D,et al.End-to-end attention-based large vocabulary speech recognition[C].Shanghai:The 41<sup>st</sup> IEEE International Conference on Acoustics,Speech and Signal Processing,2016.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective Approaches to Attention-based Neural Machine Translation">

                                <b>[6]</b> Luong M T,Pham H,Manning C D.Effective approaches to attention-based neural machine translation[C].Lisbon:Empirical Methods in Natural Language Processing,2015.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual attention based on long-short term memory model for image caption generation">

                                <b>[7]</b> Qu S,Xi Y,Ding S.Visual attention based on long-short term memory model for image caption generation[C].Melbourne:Control &amp; Decision Conference.2017.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">

                                <b>[8]</b> Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[C].San Diego:International Conference on Learning Representations,2015.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Connectionist temporal classification:Labelling unsegmented sequence data with recurrent neural networks">

                                <b>[9]</b> Graves A,Gomez F.Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C].Hong Kong:International Conference on Machine Learning,2006.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201601035&amp;v=MjUwMTZVYnZLSVRmQVpiRzRIOWZNcm85R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5cmg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 尹征,唐春晖,张轩雄.基于改进型稀疏自动编码器的图像识别[J].电子科技,2016,29(1):124-127.Yin Zheng,Tang Chunhui,Zhang Xuanxiong.Image recognition based on improved sparse auto-encoder[J].Electronic Science and Technology,2016,29(1):124-127.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjY2MjIvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUlsb1diaEE9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ICDAR 2003 Robust Read-ing Competitions">

                                <b>[12]</b> Lucas S M,Panaretos A,Sosa L,et al.ICDAR 2003 robust reading competitions[J].Proceeding of the Icdar,2003,7(2-3):105-122.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[13]</b> Ioffe S,Szegedy C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C].Lille Grand Palais:International Conference on Machine Learning,2015.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the Inception Architecture for Computer Vision">

                                <b>[14]</b> Szegedy C,Vanhoucke V,Ioffe S,et al.Rethinking the inception architecture for computer vision[C].Las Vegas:Computer Vision and Pattern Recognition,2016.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the impact of residual connections on learning">

                                <b>[15]</b> Szegedy C,Ioffe S,Vanhoucke V,et al.Inception-v4,inception-resnet and the impact of residual connections on learning[C].San Francisco:The Thirty-First AAAI Conference on Artificial Intelligence,2017.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning">

                                <b>[16]</b> Kim S,Hori T,Watanabe S.Joint CTC-attention based end-to-end speech recognition using multi-task learning[C].New Orleans:The 42<sup>nd</sup> IEEE International Conference on Acoustics,Speech and Signal Processing,2017.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM">

                                <b>[17]</b> Hori T,Watanabe S,Zhang Y,et al.Advances in joint CTC-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM[C].USA:IEEE International Conference,2017.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LCANet:end-to-end lipreading with cascaded attention-CTC">

                                <b>[18]</b> Xu K,Li D,Cassimatis N,et al.LCANet:end-to-end lipreading with cascaded attention-CTC[C].Xi’an:China Automatic Face &amp; Gesture Recognition,2018.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201912008" />
        <input id="dpi" type="hidden" value="299" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201912008&amp;v=MDU3MzZxQnRHRnJDVVI3cWZadVpwRnlyaFVidktJVGZBWmJHNEg5ak5yWTlGYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

