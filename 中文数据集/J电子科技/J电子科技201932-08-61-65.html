

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139235033857500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201908014%26RESULT%3d1%26SIGN%3d0b%252b1IetLxn0W6vQxnd501oArzIE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201908014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201908014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201908014&amp;v=MTMzNzQva1Y3N0tJVGZBWmJHNEg5ak1wNDlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;1 本文算法&lt;/b&gt; "><b>1 本文算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;1.1 生成器网络&lt;/b&gt;"><b>1.1 生成器网络</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;1.2 判别器网络&lt;/b&gt;"><b>1.2 判别器网络</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;1.3 损失函数&lt;/b&gt;"><b>1.3 损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="&lt;b&gt;2 实验结果与分析&lt;/b&gt; "><b>2 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;2.1 实验环境与数据集&lt;/b&gt;"><b>2.1 实验环境与数据集</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;2.2 网络性能与实验结果&lt;/b&gt;"><b>2.2 网络性能与实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;3 结束语&lt;/b&gt; "><b>3 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="图1 生成器网络内部结构">图1 生成器网络内部结构</a></li>
                                                <li><a href="#46" data-title="图2 判别器网络内部结构">图2 判别器网络内部结构</a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同网络损失&lt;/b&gt;"><b>表</b>1 <b>不同网络损失</b></a></li>
                                                <li><a href="#77" data-title="图3 测试区域">图3 测试区域</a></li>
                                                <li><a href="#82" data-title="图4 序列15">图4 序列15</a></li>
                                                <li><a href="#83" data-title="图5 序列30">图5 序列30</a></li>
                                                <li><a href="#84" data-title="图6 序列81">图6 序列81</a></li>
                                                <li><a href="#85" data-title="图7 序列130">图7 序列130</a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;视频图像大小&lt;/b&gt;"><b>表</b>2 <b>视频图像大小</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;性能指标&lt;/b&gt;"><b>表</b>3 <b>性能指标</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同方法用时比较&lt;/b&gt;"><b>表</b>4 <b>不同方法用时比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="107">


                                    <a id="bibliography_1" title=" 管超.基于稀疏表示理论的图像超分辨率重构算法研究[D].上海:上海交通大学, 2013.Guan Chao.Research of image super-resolution reconstruction based on sparse representation theory[D].Shanghai:Shanghai Jiao Tong University, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1013022069.nh&amp;v=MTU0NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeS9rVjc3S1ZGMjZIYk82SE5IS3BwRWJQSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         管超.基于稀疏表示理论的图像超分辨率重构算法研究[D].上海:上海交通大学, 2013.Guan Chao.Research of image super-resolution reconstruction based on sparse representation theory[D].Shanghai:Shanghai Jiao Tong University, 2013.
                                    </a>
                                </li>
                                <li id="109">


                                    <a id="bibliography_2" title=" Han J W, Suryanto, Kim J H, et al.New edge-adaptive image interpolation using anisotropic Gaussian filters[J].Digital Signal Processing, 2013, 23 (1) :110-117." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600435696&amp;v=MjU4ODJxUVRNbndaZVp0RmlubFVyaklKMThRYXhBPU5pZk9mYks4SHRETXFZOUZZT2dLQ25VL29CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Han J W, Suryanto, Kim J H, et al.New edge-adaptive image interpolation using anisotropic Gaussian filters[J].Digital Signal Processing, 2013, 23 (1) :110-117.
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_3" title=" Zhang Q, Wu J.Image super-resolution using windowed ordinary Kriging interpolation[J].Optics Communications, 2015, 33 (6) :140-145." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122200136656&amp;v=MTQ0ODROaWZPZmJLOUg5UE9yWTlGWmVnSkNuay9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKMThRYXhBPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Zhang Q, Wu J.Image super-resolution using windowed ordinary Kriging interpolation[J].Optics Communications, 2015, 33 (6) :140-145.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_4" title=" Freeman W T, Jones T R, Pasztor E C.Example-based super-resolution[J].Computer Graphics &amp;amp; Applications IEEE, 2002, 22 (2) :56-65." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Example-based super-resolution">
                                        <b>[4]</b>
                                         Freeman W T, Jones T R, Pasztor E C.Example-based super-resolution[J].Computer Graphics &amp;amp; Applications IEEE, 2002, 22 (2) :56-65.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_5" title=" Zhang K, Gao X, Tao D, et al.Single image super-resolution with non-local means and steering kernel regression[J].IEEE Transactions on Image Processing, 2012, 21 (11) :4544-56." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single Image Super-Resolution With Non-Local Means and Steering Kernel Regression">
                                        <b>[5]</b>
                                         Zhang K, Gao X, Tao D, et al.Single image super-resolution with non-local means and steering kernel regression[J].IEEE Transactions on Image Processing, 2012, 21 (11) :4544-56.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_6" title=" Bevilacqua M, Roumy A, Guillemot C, et al.Neighbor embedding based single-image super-resolution using Semi-Nonnegative Matrix Factorization[C].Kyoto:IEEE International Conference on Acoustics, Speech and Signal Processing, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neighbor embedding based single image super resolution using seminonnegative matrix factorization">
                                        <b>[6]</b>
                                         Bevilacqua M, Roumy A, Guillemot C, et al.Neighbor embedding based single-image super-resolution using Semi-Nonnegative Matrix Factorization[C].Kyoto:IEEE International Conference on Acoustics, Speech and Signal Processing, 2012.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_7" title=" Kim C, Choi K, Ra J B.Example-based super-resolution via structure analysis of patches[J].IEEE Signal Processing Letters, 2013, 20 (4) :407-410." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Example-Based Super-Resolution via Structure Analysis of Patches">
                                        <b>[7]</b>
                                         Kim C, Choi K, Ra J B.Example-based super-resolution via structure analysis of patches[J].IEEE Signal Processing Letters, 2013, 20 (4) :407-410.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_8" title=" Ledig C, Theis L, Huszar F, et al.Photo-realistic single image super-resolution using a generative adversarial network[C].Honolulu:30&lt;sup&gt;th&lt;/sup&gt; IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network">
                                        <b>[8]</b>
                                         Ledig C, Theis L, Huszar F, et al.Photo-realistic single image super-resolution using a generative adversarial network[C].Honolulu:30&lt;sup&gt;th&lt;/sup&gt; IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_9" title=" 冈萨雷斯.数字图像处理[M]北京:电子工业出版社, 2005。Gonzalez.Digital image processing[M].Beijing:Publishing House of Electronics Industry, 2005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=0007121014564999&amp;v=MjUxMDZJR0JSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnluaVU3akpKRjhXVlYyN0diSzZIOUhOcTRwRFlP&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         冈萨雷斯.数字图像处理[M]北京:电子工业出版社, 2005。Gonzalez.Digital image processing[M].Beijing:Publishing House of Electronics Industry, 2005.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_10" title=" Levin A, Lischinski D, Weiss Y.Colorization using optimization[J].Image and Vision Computing, 2003, 46 (3) :197-202." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorization using optimization">
                                        <b>[10]</b>
                                         Levin A, Lischinski D, Weiss Y.Colorization using optimization[J].Image and Vision Computing, 2003, 46 (3) :197-202.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_11" title=" Yatziv L, Bartesaghi A, Sapiro G.O (N) implementation of the fast marching algorithm[J].Journal of Computational Physics, 2006, 212 (2) :393-399." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601262225&amp;v=MDQ2NzlsVXJqSUoxOFFheEE9TmlmT2ZiSzdIdEROcVk5RVp1ME5EbjQ4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Yatziv L, Bartesaghi A, Sapiro G.O (N) implementation of the fast marching algorithm[J].Journal of Computational Physics, 2006, 212 (2) :393-399.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_12" title=" Welsh T, Ashikhmin M, Mueller K.Transferring color to greyscale images[J].ACM Transactions on Graphics, 2002, 21 (3) :277-280." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097914&amp;v=MjkyNjIwOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUoxOFFheEE9TmlmSVk3SzdIdGpOcjQ5RlpPSUlCWA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Welsh T, Ashikhmin M, Mueller K.Transferring color to greyscale images[J].ACM Transactions on Graphics, 2002, 21 (3) :277-280.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_13" title=" Isola P, Zhu J Y, Zhou T, et al.Image-to-image translation with conditional adversarial networks[C].Honolulu:30&lt;sup&gt;th&lt;/sup&gt; IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">
                                        <b>[13]</b>
                                         Isola P, Zhu J Y, Zhou T, et al.Image-to-image translation with conditional adversarial networks[C].Honolulu:30&lt;sup&gt;th&lt;/sup&gt; IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_14" title=" Goodfellow I J, Pouget-Abadie J, Mirza M, et al.Generative adversarial networks[J].Advances in Neural Information Processing Systems, 2014 (3) :2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Networks">
                                        <b>[14]</b>
                                         Goodfellow I J, Pouget-Abadie J, Mirza M, et al.Generative adversarial networks[J].Advances in Neural Information Processing Systems, 2014 (3) :2672-2680.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_15" title=" Richardson I E G.H.264 and MPEG-4 video compression:video coding for next-generation multimedia[M].Hoboken:Wiley Press, 2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=H.264 and MPEG-4 video compression:video coding for next-generation multimedia">
                                        <b>[15]</b>
                                         Richardson I E G.H.264 and MPEG-4 video compression:video coding for next-generation multimedia[M].Hoboken:Wiley Press, 2004.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_16" title=" 沈秋, 李小凡, 孔繁锵, 等.基于仿射模型的无人机视频实时压缩算法[J].电子与信息学报, 2014, 36 (12) :2855-2860.Shen Qiu, Li Xiaofan, Kong Fanqiang, et al.A real-time video compression for UAV based on affine model[J].Journal of Electronics &amp;amp; Information Technology, 2014, 36 (12) :2855-2860." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201412010&amp;v=MjM2MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5L2tWNzdLSVRmU2RyRzRIOVhOclk5RVpJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         沈秋, 李小凡, 孔繁锵, 等.基于仿射模型的无人机视频实时压缩算法[J].电子与信息学报, 2014, 36 (12) :2855-2860.Shen Qiu, Li Xiaofan, Kong Fanqiang, et al.A real-time video compression for UAV based on affine model[J].Journal of Electronics &amp;amp; Information Technology, 2014, 36 (12) :2855-2860.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-20 07:08</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(08),61-65 DOI:10.16180/j.cnki.issn1007-7820.2019.08.013            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于</b>GAN<b>的无人机航拍图像重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E7%90%A8&amp;code=38083006&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹琨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E9%A3%9E&amp;code=26926938&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E5%B0%8F%E7%91%9E&amp;code=37447774&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱小瑞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E7%85%A7%E5%9D%A4&amp;code=41843542&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨照坤</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0202052&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海工程技术大学电子电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统无人机采集传输过程中所传输的数据量常常造成无人机电池的高消耗。针对此类问题, 文中提出一种融合超分辨重建和灰度图像彩色化的CsRGAN模型。通过生成网络对低分辨的灰度图像进行重建:先将图片进行分辨率放大, 再进行色彩填充, 然后通过判别器进行图片修正, 最终将图片重建为彩色高清图像。实验结果表明, 在固定区域下, 所提出的模型能够在保证成像质量的同时减少无人机航拍的传输数据量, 提高无人机的电池利用率, 且模型具有较强的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E4%BA%BA%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无人机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%89%B2%E5%BD%A9%E5%A1%AB%E5%85%85&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">色彩填充;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E5%BC%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗式网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BA%E5%AE%9A%E5%8C%BA%E5%9F%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">固定区域;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    曹琨 (1994-) , 女, 硕士研究生。研究方向:深度学习、图像处理。;
                                </span>
                                <span>
                                    吴飞 (1968-) , 男, 博士, 教授。研究方向:计算机组织与系统结构、分布式多媒体技术。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61272097);</span>
                                <span>上海市科技学术委员会资助项目 (13510501400);</span>
                                <span>上海市科委重点项目 (18511101600);</span>
                    </p>
            </div>
                    <h1><b>Aerial Image Reconstruction of Drone Based on GAN</b></h1>
                    <h2>
                    <span>CAO Kun</span>
                    <span>WU Fei</span>
                    <span>QIAN Xiaorui</span>
                    <span>YANG Zhaokun</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Electrical Engineering, Shanghai University of Engineering Science</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The amount of data transmitted during the traditional UAV acquisition and transmission process often resulted in high consumption of the UAV battery. Aiming at solving the problem, a CsRGAN model that combined super-resolution reconstruction and gray-scale image colorization was proposed. The low resolution grayscale image was reconstructed by generating a network: the image was first subjected to resolution amplification, color filling was performed, and then the image was corrected by the discriminator, and finally the image was reconstructed into a color high-definition image. The experimental results showed that under the fixed area, the model could reduce the transmission data of the drone aerial photography and improve the battery utilization of the drone under the condition of ensuring the imaging quality. These results proved the model had strong robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=UVA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">UVA;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20colorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image colorization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=GAN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">GAN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fix%20area&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fix area;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-13</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Natural Science Foundation of China (61272097);</span>
                                <span>Shanghai Municipal Committee of Science and Technology Project (13510501400);</span>
                                <span>Shanghai Municipal Committee of Science and Technology Project (18511101600);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="35">传统无人机航拍中视频采集及传输的数据量较大, 且受限于无线网络的带宽, 需要对视频进行处理方可实时将其传输到地面端。然而视频分辨率越高, 压缩过程越复杂, 将造成高功率消耗。因此在无人机功率受限的条件下, 如何提高无人机电池利用率从而延长无人机的飞行时间, 是一个急需解决的问题。</p>
                </div>
                <div class="p1">
                    <p id="36">传统的图像超分辨重建是基于插值方法<citation id="143" type="reference"><link href="107" rel="bibliography" /><link href="109" rel="bibliography" /><link href="111" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>, 如Han等人<citation id="139" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出了一种基于各向异性高斯滤波器的边缘自适应插值重建方法, 该方法在移除噪声和边缘附近的阶梯瑕疵时较有优势;Zhang和Wu<citation id="140" type="reference"><link href="111" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了一种窗口化的普通Kriging插值重建方法, 该方法能较好地保护图像的边缘。但基于插值的方法不考虑图像的内容, 只是简单地进行像素点之间的计算, 容易丢失大量细节信息, 从而导致模糊。另一种基于重构的超分辨率方法<citation id="141" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>结合了图像的降质模型, 解决了基于插值方法无法引入先验信息的问题。该方法虽然对复杂度低的图像效果较好, 但对纹理结构丰富的图像则效果一般。随后研究人员提出了基于学习的方法, 如Chao Dong等人<citation id="142" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的SRCNN。该方法结构简单, 结果相较于其他方法更精确, 但是会损失许多图像的细节信息, 同时数据来源并不多, 并不适用于本文中无人机的应用背景。</p>
                </div>
                <div class="p1">
                    <p id="37">灰色图像彩色化是将RGB的值重新添加到灰度图像的过程<citation id="144" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。传统的色彩填充基本都是通过数字图像传感器进行对颜色进行插值来完成的。例如, Levin等<citation id="145" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出能量方程最优化的颜色扩散方法, 其效率比较低。Yatziv等<citation id="146" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了类似的快速图像和视频的着色方法, 但对输入要求较严苛。之后, Welsh 等<citation id="147" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了基于参考彩色图像的颜色传递方法, 该方法不仅耗时且不能取得满意的结果。</p>
                </div>
                <div class="p1">
                    <p id="38">对于固定区域的无人机航拍区域, 针对这种特性, 文中提出一种融合超分辨率重建和灰色图像彩色化CsRGAN (Color super Resolution Generative Adversarial Network) 模型。本文首先在保留图像所有细节信息的基础上, 将输入的灰色低分辨率图像在颜色超分辨率生成器中先进行放大;然后, 在颜色超分辨率生成器部分, 输入的彩色帧与灰度帧分别经过多个编码器后融合, 再通过多个解码器解码成高清彩色帧并利用判断网络进行比对;根据损失函数对生产网络经过调整, 不断地迭代上述过程;最终, 在地面段将其还原重构为彩色超清图像。该方法在不影响最终成像的质量的情况下, 从减少像素点数量与通道的角度提高了无人机的电池利用率。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>1 本文算法</b></h3>
                <h4 class="anchor-tag" id="40" name="40"><b>1.1 生成器网络</b></h4>
                <div class="p1">
                    <p id="41">本文模型中的生成器网络先对灰度图像进行超分辨率放大, 然后再通过多个编解码器进行颜色填充。具体步骤如下: (1) 灰度图像首先通过3个编码器不断地减少图像的大小, 目的是为了用一个更小的高维的矩阵来表示这个图片; (2) 经过6次残差网络来提取特征, 使用残差网络即能使整个网络增加深度, 又可防止因梯度弥散而出现不收敛的情况;再通过解码器, 用反卷积的方式, 将其还原成256×256的矩阵, 在保持其维度的情况下放大矩阵。最终用亚像素卷积的方式再次放大, 这种方法在保留图像所有细节信息的基础对图像进行放大, 从而使图像不会发生失真或者形变的情况。然后超分辨重建后的灰度图片转换为三通道相同灰度值图片, 再通过一系列的编码器不断地减少图像的大小, 提高图像的通道数。这样做的目的是为了用一个更小的高维矩阵来表示这个图片, 最终将其压缩为一个512维的像素点来表示整个图像。最后, 通过解码器, 用反卷积的方式, 将这个高维的像素点还原成一副随机的彩色图像, 在这个过程中, 整个网络并不会对图像的整体结构发生太多变化。生成器的内部结构如图1所示。</p>
                </div>
                <div class="area_img" id="42">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 生成器网络内部结构" src="Detail/GetImg?filename=images/DZKK201908014_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 生成器网络内部结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Internal structure for generator network</p>

                </div>
                <div class="p1">
                    <p id="43">但是, 单一网络进行卷积反卷积很容易导致网络处于过拟合的状态, 特别是当网络有了6层以上卷积时, 图像特征包含了大量的高维特征的同时忽略了过多的低层特征信息。因此, 本文借鉴U-net的思想, 使用跳层连接, 通过在每一层的输入和输出之间共享大量的低级信息来保证在解码器中反卷积时不会缺少低层的特征信息。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>1.2 判别器网络</b></h4>
                <div class="p1">
                    <p id="45">本文模型中的判别器网络分别通过卷积将生成图片和真实图片同时形成一个维度为128、256×256大小的矩阵, 再将两者链接起来。该方式可避免因将过大尺寸图片送入解码模块, 使得最后一层维度过高而出现过拟合的情况。判别器网络内部结构如图2所示。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 判别器网络内部结构" src="Detail/GetImg?filename=images/DZKK201908014_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 判别器网络内部结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Internal structure for discriminator network</p>

                </div>
                <div class="p1">
                    <p id="47">本文的判别网络借鉴了自编码器的思想, 网络的输入层先将真实数据库和生成器生成的图片拼接成一张256×256×6的图像。输入拼接的六通道的图像矩阵, 经过3个编码块压缩为32×32大小的 256通道的特征矩阵, 编码模块由一个卷积核为3×3, 步长为1的卷积层、一个批量正则化层 (Batch Normalization) 和一个修正线性单元激活层组成。最终输出一个大小为30×30, 通道为512的特征矩阵, 其中每个像素值 (0～1) 表示未知图像对应部分的置信程度。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>1.3 损失函数</b></h4>
                <div class="p1">
                    <p id="49">本文使用生成对抗网络来完成图像的重建, 所以沿用了Goodfellow<citation id="148" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>所提出的生成对抗网络原型的损失函数, 如式 (1) 所示。</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>-</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mspace width="0.25em" /><mspace width="0.25em" /><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">本文的生成网络中包括超分辨率重建和灰色图像彩色化两部分, 所以本文引用超分辨重建部分损失<i>G</i><sub>1</sub>和灰图像彩色化损失<i>G</i><sub>2</sub>。</p>
                </div>
                <div class="p1">
                    <p id="52"><i>G</i><sub><i>g</i></sub>=<i>G</i><sub>1</sub>+<i>G</i><sub>2</sub>       (2) </p>
                </div>
                <div class="p1">
                    <p id="53">超分辨重建部分整体损失函数如式 (3) 和 (4) 所示。针对于灰度图像的放大部分, 考虑到图像的结构相似性, 本文采用SSIM来代替MSE, 同时仍然使用预先训练完成的VGG19作为网络损失函数。</p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><mrow><mi>min</mi></mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>l</mi></mstyle><msub><mrow></mrow><mrow><mi>C</mi><mi>R</mi></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo>, </mo><mi>Τ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="56"><i>l</i><sub><i>CR</i></sub>=<i>l</i><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext></mrow><mrow><mi>C</mi><mi>R</mi></mrow></msubsup></mrow></math></mathml>+<i>l</i><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>V</mtext><mtext>G</mtext><mtext>G</mtext><mn>1</mn><mn>9</mn></mrow><mrow><mi>C</mi><mi>R</mi></mrow></msubsup></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="59">灰色图像彩色化的整体损失如式 (5) 所示, 其中<i>L</i><sub>1</sub>范数的损失函数为式 (6) , 同时增加HSV的值作为第二部分的损失函数 (7) 。</p>
                </div>
                <div class="p1">
                    <p id="60"><i>G</i><sub>2</sub>=<i>l</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>C</mi><mi>R</mi></mrow></msubsup></mrow></math></mathml>+<i>l</i><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Η</mtext><mtext>S</mtext><mtext>V</mtext></mrow><mrow><mi>C</mi><mi>R</mi></mrow></msubsup></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="63"><i>l</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>C</mi><mi>R</mi></mrow></msubsup></mrow></math></mathml>=avg‖<i>T</i>-<i>G</i>‖      (6) </p>
                </div>
                <div class="area_img" id="65">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201908014_06500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="67">本文的总损失函数需要添加<i>L</i><sub>1</sub>范数和<i>L</i><sub>HSV</sub>, 目的是增加真实图片和合成输出的相匹配度, 在本文的数据集中实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表</b>1 <b>不同网络损失</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Loss of different networks</p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td><br />Loss</td><td>Per-pixel acc.</td><td>Per-class acc.</td><td>Class IOU</td></tr><tr><td><br /><i>L</i><sub>1</sub>+GAN</td><td>0.68</td><td>0.19</td><td>0.14</td></tr><tr><td><br /><i>L</i><sub>1</sub>+GAN+<i>L</i><sub>HSV</sub></td><td>0.70</td><td>0.22</td><td>0.15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">最终本文的损失函如式 (8) 所示。</p>
                </div>
                <div class="p1">
                    <p id="70"><i>G</i>=<i>G</i><sup>*</sup>+<i>λ</i>[<i>l</i><sub><i>L</i><sub>1</sub></sub> (<i>G</i>) +<i>l</i><sub>HSV</sub> (<i>G</i>) ]      (8) </p>
                </div>
                <div class="p1">
                    <p id="71">其中, </p>
                </div>
                <div class="p1">
                    <p id="72"><i>l</i><sub><i>L</i><sub>1</sub></sub> (<i>G</i>) =<i>E</i><sub><i>T</i>, <i>Z</i>～<i>p</i><sub>data</sub></sub>⎣‖<i>T</i>-<i>G</i> (<i>Z</i>) ‖<sub>1</sub>」      (9) </p>
                </div>
                <div class="p1">
                    <p id="73"><i>l</i><sub>HSV</sub> (<i>G</i>) =<i>E</i><sub><i>T</i>, <i>Z</i>～<i>p</i><sub>data</sub></sub>⎣‖<i>T</i><sub>HSV</sub>-<i>G</i><sub>HSV</sub>‖<sub>MSE</sub>」      (10) </p>
                </div>
                <h3 id="74" name="74" class="anchor-tag"><b>2 实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="75" name="75"><b>2.1 实验环境与数据集</b></h4>
                <div class="p1">
                    <p id="76">本文实验环境:Ubuntu16.04平台上由配有 Tensorflow1.20框架的Python 编程实现, 处理器为 Intel Core i7-6300HQ, 2.9 GHz16核CPU, 内存为 64 GB, 显卡为GTX1080Ti, 显存为8 GB, 训练共使用了30 000张图片, 训练时长144小时。如图3所示, 在校园的图书馆到行政楼, 无人机按照逆时针方向按照 <i>A B C D A</i>的顺序进行拍摄。实验数据集是无人机从所有的实验图像, 低分辨率和高分辨率之间的比例因子是4<i>x</i>, 这相当于图像像素减少了16<i>x</i>。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 测试区域" src="Detail/GetImg?filename=images/DZKK201908014_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 测试区域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Test area</p>

                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>2.2 网络性能与实验结果</b></h4>
                <div class="p1">
                    <p id="79">在本实验中, 只考虑<i>YC</i><sub><i>r</i></sub><i>C</i><sub><i>b</i></sub>颜色空间的亮度通道, 因此本文中第一层/最后一层中<i>c</i>=1。两个色光通道是为了显示, 而不是为了进行训练和测试。本文的方法可以通过设置<i>c</i>=3来扩展到对颜色图像的直接训练。但本文使用<i>c</i>=1是为了与以前的方法进行比较。</p>
                </div>
                <div class="p1">
                    <p id="80">为了避免在训练过程中产生边界效应, 所有的卷积层都没有填充, 而网络产生的输出会更小。在处理测试图像中, 卷积神经网络可以应用在任意大小的图像上。在测试期间, 所有的卷积层都得到了足够的零填充, 因此输出图像的大小与输入的大小相同。为了解决边界效应, 在每个卷积层中, 每个像素的输出 (在ReLU之前) 都是通过有效输入像素的数量来规格化的, 这可以预先计算出来。</p>
                </div>
                <div class="p1">
                    <p id="81">每一层的滤波器权重由随机抽取的高斯分布随机抽取, 零均值和标准差0.001。学习速率是前两层的10<sup>-4</sup>, 最后一层是10<sup>-5</sup>。从实验经验上发现, 在最后一层中, 一个较小的学习速率对于网络的收敛是很重要的。实验效果图具体如图4～图7所示。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 序列15" src="Detail/GetImg?filename=images/DZKK201908014_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 序列15  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Frame 15</p>
                                <p class="img_note"> (a) 输入图片 (b) 输出图片 (c) 目标原图</p>
                                <p class="img_note"> (a) Input picture (b) Output picture (c) Target picture</p>

                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 序列30" src="Detail/GetImg?filename=images/DZKK201908014_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 序列30  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5. Frame 30</p>
                                <p class="img_note"> (a) 输入图片 (b) 输出图片 (c) 目标原图</p>
                                <p class="img_note"> (a) Input picture (b) Output picture (c) Target picture</p>

                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 序列81" src="Detail/GetImg?filename=images/DZKK201908014_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 序列81  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6. Frame 81</p>
                                <p class="img_note"> (a) 输入图片 (b) 输出图片 (c) 目标原图</p>
                                <p class="img_note"> (a) Input picture (b) Output picture (c) Target picture</p>

                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908014_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 序列130" src="Detail/GetImg?filename=images/DZKK201908014_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 序列130  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908014_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7. Frame 130</p>
                                <p class="img_note"> (a) 输入图片 (b) 输出图片 (c) 目标原图</p>
                                <p class="img_note"> (a) Input picture (b) Output picture (c) Target picture</p>

                </div>
                <div class="p1">
                    <p id="86">图中4个序列的图像为重建效果, 基本符合实验预期。接下来对从本模型中输出的图片与原始目标图片进行比较和分析, 具体如表2所示。由表可知, 彩色与黑白图像之间的差距较大, 最大为35.2倍, 平均为33.9倍。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表</b>2 <b>视频图像大小</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2. Video size</p>
                    <p class="img_note"></p>
                    <table id="87" border="1"><tr><td><br />序列</td><td>输入图像/kB</td><td>输出图像/kB</td><td>差值倍数</td></tr><tr><td><br />15</td><td>27</td><td>875</td><td>32.4</td></tr><tr><td><br />30</td><td>28</td><td>935</td><td>33.4</td></tr><tr><td><br />81</td><td>39</td><td>1 356</td><td>34.8</td></tr><tr><td><br />130</td><td>30</td><td>1 057</td><td>35.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="88">上述视频序列中的客观评价指标见表3。</p>
                </div>
                <div class="area_img" id="89">
                    <p class="img_tit"><b>表</b>3 <b>性能指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3. Performance index</p>
                    <p class="img_note"></p>
                    <table id="89" border="1"><tr><td><br />序列</td><td>保真性PSNR</td><td>鲁棒性NC</td><td>结构相似性SSIM</td></tr><tr><td><br />15</td><td>28.617 190</td><td>0.997 791</td><td>0.981 915</td></tr><tr><td><br />30</td><td>28.815 120</td><td>0.997 691</td><td>0.981 965</td></tr><tr><td><br />81</td><td>27.958 641</td><td>0.997 781</td><td>0.982 935</td></tr><tr><td><br />130</td><td>21.758 541</td><td>0.997 715</td><td>0.981 915</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="90">使用本文方法与普通无人机航拍视频传输方法<citation id="149" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>及文献<citation id="150" type="reference">[<a class="sup">16</a>]</citation>中基于仿射模型的无人机视频压缩方法, 进行传输300帧视频所需时间的方法进行比较, 结果如表4所示。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>4 <b>不同方法用时比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4. Times of different networks</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td><br />方法</td><td>数据量<br />/kB·s<sup>-1</sup></td><td>传输视频<br />分辨率</td><td>播放视频<br />分辨率</td><td>300帧视频<br />传输时间/s</td></tr><tr><td><br />文献[15]</td><td>10 640.228</td><td>640×480</td><td>640×480</td><td>20.66</td></tr><tr><td><br />文献[16]</td><td>12 702.890</td><td>640×480</td><td>640×480</td><td>17.31</td></tr><tr><td><br />本文方法</td><td>10 640.228</td><td>256×256</td><td>1024×1024</td><td>2.25</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">由表2～表4得出, 相比于其他方法, 本文提出的传输低分辨灰度图像在接收端进行超分辨色彩还原的方法模型, 其传输的图像在视觉上效果与原始效果相差不大, 且传输的数据量更小, 传输时间更短, 模型鲁棒性较强。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>3 结束语</b></h3>
                <div class="p1">
                    <p id="94">本文通过融合超分辨重建和色彩重建所建立的CsRGAN模型, 该模型能够重建灰色低分辨率图像, 将其还原为高相似度的彩色高清图像。实验结果表明, 本文提出的模型结构简单且具有较高的重建精度和较强的鲁棒性。未来将在模型中加入语义分析网络, 以便进一步改善重构结果。对于无人机航拍来说, 该模型能够在视频质量约束条件下, 大幅减少视频压缩及传输部分的能耗, 显著延长无人机的续航时间。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="107">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1013022069.nh&amp;v=MTE0NDU3cWZadVpwRnkva1Y3N0tWRjI2SGJPNkhOSEtwcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 管超.基于稀疏表示理论的图像超分辨率重构算法研究[D].上海:上海交通大学, 2013.Guan Chao.Research of image super-resolution reconstruction based on sparse representation theory[D].Shanghai:Shanghai Jiao Tong University, 2013.
                            </a>
                        </p>
                        <p id="109">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600435696&amp;v=MTk0NTd0RE1xWTlGWU9nS0NuVS9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKMThRYXhBPU5pZk9mYks4SA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Han J W, Suryanto, Kim J H, et al.New edge-adaptive image interpolation using anisotropic Gaussian filters[J].Digital Signal Processing, 2013, 23 (1) :110-117.
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122200136656&amp;v=MTg4MDRyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKMThRYXhBPU5pZk9mYks5SDlQT3JZOUZaZWdKQ25rL29CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Zhang Q, Wu J.Image super-resolution using windowed ordinary Kriging interpolation[J].Optics Communications, 2015, 33 (6) :140-145.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Example-based super-resolution">

                                <b>[4]</b> Freeman W T, Jones T R, Pasztor E C.Example-based super-resolution[J].Computer Graphics &amp; Applications IEEE, 2002, 22 (2) :56-65.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single Image Super-Resolution With Non-Local Means and Steering Kernel Regression">

                                <b>[5]</b> Zhang K, Gao X, Tao D, et al.Single image super-resolution with non-local means and steering kernel regression[J].IEEE Transactions on Image Processing, 2012, 21 (11) :4544-56.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neighbor embedding based single image super resolution using seminonnegative matrix factorization">

                                <b>[6]</b> Bevilacqua M, Roumy A, Guillemot C, et al.Neighbor embedding based single-image super-resolution using Semi-Nonnegative Matrix Factorization[C].Kyoto:IEEE International Conference on Acoustics, Speech and Signal Processing, 2012.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Example-Based Super-Resolution via Structure Analysis of Patches">

                                <b>[7]</b> Kim C, Choi K, Ra J B.Example-based super-resolution via structure analysis of patches[J].IEEE Signal Processing Letters, 2013, 20 (4) :407-410.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network">

                                <b>[8]</b> Ledig C, Theis L, Huszar F, et al.Photo-realistic single image super-resolution using a generative adversarial network[C].Honolulu:30<sup>th</sup> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=0007121014564999&amp;v=MjIwMTZWVjI3R2JLNkg5SE5xNHBEWU9JR0JSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnluaVU3akpKRjhX&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 冈萨雷斯.数字图像处理[M]北京:电子工业出版社, 2005。Gonzalez.Digital image processing[M].Beijing:Publishing House of Electronics Industry, 2005.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorization using optimization">

                                <b>[10]</b> Levin A, Lischinski D, Weiss Y.Colorization using optimization[J].Image and Vision Computing, 2003, 46 (3) :197-202.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601262225&amp;v=MDczODlNbndaZVp0RmlubFVyaklKMThRYXhBPU5pZk9mYks3SHRETnFZOUVadTBORG40OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Yatziv L, Bartesaghi A, Sapiro G.O (N) implementation of the fast marching algorithm[J].Journal of Computational Physics, 2006, 212 (2) :393-399.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000097914&amp;v=MjA3ODllWnRGaW5sVXJqSUoxOFFheEE9TmlmSVk3SzdIdGpOcjQ5RlpPSUlCWDA5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Welsh T, Ashikhmin M, Mueller K.Transferring color to greyscale images[J].ACM Transactions on Graphics, 2002, 21 (3) :277-280.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">

                                <b>[13]</b> Isola P, Zhu J Y, Zhou T, et al.Image-to-image translation with conditional adversarial networks[C].Honolulu:30<sup>th</sup> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Adversarial Networks">

                                <b>[14]</b> Goodfellow I J, Pouget-Abadie J, Mirza M, et al.Generative adversarial networks[J].Advances in Neural Information Processing Systems, 2014 (3) :2672-2680.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=H.264 and MPEG-4 video compression:video coding for next-generation multimedia">

                                <b>[15]</b> Richardson I E G.H.264 and MPEG-4 video compression:video coding for next-generation multimedia[M].Hoboken:Wiley Press, 2004.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201412010&amp;v=MjgxODJDVVI3cWZadVpwRnkva1Y3N0tJVGZTZHJHNEg5WE5yWTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 沈秋, 李小凡, 孔繁锵, 等.基于仿射模型的无人机视频实时压缩算法[J].电子与信息学报, 2014, 36 (12) :2855-2860.Shen Qiu, Li Xiaofan, Kong Fanqiang, et al.A real-time video compression for UAV based on affine model[J].Journal of Electronics &amp; Information Technology, 2014, 36 (12) :2855-2860.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201908014" />
        <input id="dpi" type="hidden" value="299" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201908014&amp;v=MTMzNzQva1Y3N0tJVGZBWmJHNEg5ak1wNDlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

