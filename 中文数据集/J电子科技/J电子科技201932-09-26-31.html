

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139227452920000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201909007%26RESULT%3d1%26SIGN%3dxlUuGl7jQA5UD0Poz1tNW37XWOg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201909007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201909007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201909007&amp;v=MDAwOTZxQnRHRnJDVVI3cWZadVpwRnl6aFVMek1JVGZBWmJHNEg5ak1wbzlGWTRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1&lt;/b&gt; 关键技术 "><b>1</b> 关键技术</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="&lt;b&gt;1.1&lt;/b&gt; 深度学习及框架"><b>1.1</b> 深度学习及框架</a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;1.2&lt;/b&gt; 深度学习模型结构"><b>1.2</b> 深度学习模型结构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;2&lt;/b&gt; 实验流程设计 "><b>2</b> 实验流程设计</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="&lt;b&gt;3&lt;/b&gt; 实验内容及结果分析 "><b>3</b> 实验内容及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;3.1&lt;/b&gt; 传统方法与深度学习准确率比较"><b>3.1</b> 传统方法与深度学习准确率比较</a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;3.2&lt;/b&gt; 运行时间比较"><b>3.2</b> 运行时间比较</a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;3.3&lt;/b&gt; 实验结果分析"><b>3.3</b> 实验结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="&lt;b&gt;4&lt;/b&gt; 结束语 "><b>4</b> 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;深度学习人脸识别模型参数&lt;/b&gt;"><b>表</b>1 <b>深度学习人脸识别模型参数</b></a></li>
                                                <li><a href="#63" data-title="图1 Inception-V3结构(1)">图1 Inception-V3结构(1)</a></li>
                                                <li><a href="#64" data-title="图2 Inception-V3结构(2)">图2 Inception-V3结构(2)</a></li>
                                                <li><a href="#65" data-title="图3 Inception-V3结构(3)">图3 Inception-V3结构(3)</a></li>
                                                <li><a href="#72" data-title="图4 判识方法流程图">图4 判识方法流程图</a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同条件下身份识别准确率比较&lt;/b&gt;"><b>表</b>2 <b>不同条件下身份识别准确率比较</b></a></li>
                                                <li><a href="#79" data-title="图5 两种身份识别的方法100次平均识别率比较">图5 两种身份识别的方法100次平均识别率比较</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同条件下安全帽识别准确率&lt;/b&gt;"><b>表</b>3 <b>不同条件下安全帽识别准确率</b></a></li>
                                                <li><a href="#86" data-title="图6 安全帽识别的两种方法100次平均识别率比较">图6 安全帽识别的两种方法100次平均识别率比较</a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;两种方法平均运行时间&lt;/b&gt;"><b>表</b>4 <b>两种方法平均运行时间</b></a></li>
                                                <li><a href="#93" data-title="图7 实验可视化结果">图7 实验可视化结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" Liu P,Han S,Meng Z,et al.Facial expression recognition via a boosted deep belif network[C].Washington D C:Proceedlings of the IEEE Conference on Computer Vision and Pattern Recognition,IEEE,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition via a boosted deep belief network[C/OL]">
                                        <b>[1]</b>
                                         Liu P,Han S,Meng Z,et al.Facial expression recognition via a boosted deep belif network[C].Washington D C:Proceedlings of the IEEE Conference on Computer Vision and Pattern Recognition,IEEE,2014.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" 熊海朋,陈洋洋,陈春玮.基于卷积神经网络的场景图像文本定位研究[J].电子科技,2018,31(1):50-53.Xiong Haipeng,Chen Yangyang,Chen Chunwei.Text location in image based on convolution neual network[J].Electronic Science and Technology,2018,31(1):50-53." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201801014&amp;v=MTAzOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXpoVUx6TUlUZkFaYkc0SDluTXJvOUVZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         熊海朋,陈洋洋,陈春玮.基于卷积神经网络的场景图像文本定位研究[J].电子科技,2018,31(1):50-53.Xiong Haipeng,Chen Yangyang,Chen Chunwei.Text location in image based on convolution neual network[J].Electronic Science and Technology,2018,31(1):50-53.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" Rafael C.Gonzalez digital image processing[M].Beijing:Publishing House of Electronics Industry,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gonzalez digital image processing">
                                        <b>[3]</b>
                                         Rafael C.Gonzalez digital image processing[M].Beijing:Publishing House of Electronics Industry,2017.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" 完颜勇.基于对比度增强和背景估计的文档图像二值化[J].电子科技,2018,31(4):20-25.Wanyan Yong.Doucument image binarization based on contrast enhancement and background estimation[J].Electronic Science and Technology,2018,31(4):20-25." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201804007&amp;v=MzAxNDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnl6aFVMek1JVGZBWmJHNEg5bk1xNDlGWTRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         完颜勇.基于对比度增强和背景估计的文档图像二值化[J].电子科技,2018,31(4):20-25.Wanyan Yong.Doucument image binarization based on contrast enhancement and background estimation[J].Electronic Science and Technology,2018,31(4):20-25.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" Zhang Z,Luo P,Loy C C,et al.Facial landmark detection bydeep multi-task learning[C].Berlin:Proceedings of the European Conference on Computer Vision,Springer,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial landmark detection by deep multi-task learning">
                                        <b>[5]</b>
                                         Zhang Z,Luo P,Loy C C,et al.Facial landmark detection bydeep multi-task learning[C].Berlin:Proceedings of the European Conference on Computer Vision,Springer,2014.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" 朱煜,赵江坤,王逸宁,等.基于深度学习的人体行为识别算法综述[J].自动化学报,2016,42(6):848-875.Zhu Yu,Zhao Jiangkun,Wang Yining,et al.A review of human action recognition based on deep learning[J].Acta Automatica Sinica,2016,42(6):848-875." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606005&amp;v=MTIyMDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXpoVUx6TUtDTGZZYkc0SDlmTXFZOUZZWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         朱煜,赵江坤,王逸宁,等.基于深度学习的人体行为识别算法综述[J].自动化学报,2016,42(6):848-875.Zhu Yu,Zhao Jiangkun,Wang Yining,et al.A review of human action recognition based on deep learning[J].Acta Automatica Sinica,2016,42(6):848-875.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" Fischer A,Igel C.Training restricted boltzmann machines:an introduction[J].Pattern Recognition,2014,47(1):25-39." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161945&amp;v=MDEzMTlUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKRm9YYVJZPU5pZk9mYks5SDlQT3JJOUZaZTBPQlhnOG9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Fischer A,Igel C.Training restricted boltzmann machines:an introduction[J].Pattern Recognition,2014,47(1):25-39.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" Christian Szegedy.Vincent vanhoucke rethinking the inception architecture for computer vision[C].Boston:IEEE Conference on Computer Vision and Pattern Recognition,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the Inception Architecture for Computer Vision">
                                        <b>[8]</b>
                                         Christian Szegedy.Vincent vanhoucke rethinking the inception architecture for computer vision[C].Boston:IEEE Conference on Computer Vision and Pattern Recognition,2016.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" Ding Changxing,Tao Dacheng.Trunk-branch ensemble convolutional neural networks for video-based face recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):89-95." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Trunk-branch ensemble convolutional neural networks for video-based face recognition">
                                        <b>[9]</b>
                                         Ding Changxing,Tao Dacheng.Trunk-branch ensemble convolutional neural networks for video-based face recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):89-95.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" Schmidhuber J.Deep learning in neural networks:an overview[J].Neural Networks,2015,61(7):85-117." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MDEzODZNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpJSkZvWGFSWT1OaWZPZmJLOEg5RE1xSTlGWnVJSkRIMDdvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Schmidhuber J.Deep learning in neural networks:an overview[J].Neural Networks,2015,61(7):85-117.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" 龚爱平.基于嵌入式机器视觉的信息采集与处理技术研究[D].杭州:浙江大学,2013.Gong Aiping.Study of the information acquisition and processing technology based on embaedded computer vision[D].Hangzhou:Zhejiang University,2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1013153329.nh&amp;v=MDkwMjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emhVTHpNVkYyNkhiSzlIZExPcHBFYlBJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         龚爱平.基于嵌入式机器视觉的信息采集与处理技术研究[D].杭州:浙江大学,2013.Gong Aiping.Study of the information acquisition and processing technology based on embaedded computer vision[D].Hangzhou:Zhejiang University,2013.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" Joe Minichino.Learning OpenCV3 computer vision with python[M].Paris:Packet Publishing,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning OpenCV3 computer vision with python">
                                        <b>[12]</b>
                                         Joe Minichino.Learning OpenCV3 computer vision with python[M].Paris:Packet Publishing,2015.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" 毕林,谢伟,崔君.基于卷积神经网络的矿工安全帽佩戴识别研究[J].黄金科学技术,2017,25(4):73-80.Bi Lin,Xie Wei,Cui Jun.Identification research on the miner’s safety helmet wear based on convolutional neural network[J].Gold Science and Technology,2017,25(4):73-80." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HJKJ201704013&amp;v=MTk0MTJPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emhVTHpNTFNmQVpMRzRIOWJNcTQ5RVo0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         毕林,谢伟,崔君.基于卷积神经网络的矿工安全帽佩戴识别研究[J].黄金科学技术,2017,25(4):73-80.Bi Lin,Xie Wei,Cui Jun.Identification research on the miner’s safety helmet wear based on convolutional neural network[J].Gold Science and Technology,2017,25(4):73-80.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" 黄文坚,唐源.Tensorflow实战[M].北京:电子工业出版社,2017.Huang Wenjian,Tang Yuan.Practice of tensorflow[M].Beijing:Publishing House of Electronics Industry,2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121309120000&amp;v=MjkxOTRMTXBvNUhaT3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5pVTd2TUkxMFFYRnF6R2JLNkg5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         黄文坚,唐源.Tensorflow实战[M].北京:电子工业出版社,2017.Huang Wenjian,Tang Yuan.Practice of tensorflow[M].Beijing:Publishing House of Electronics Industry,2017.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" Jose M Chaquet,Enrique J Carmona.A survey of video datasets for human actions and activity recognition[J].Computer Vision and Image Understanding,2013(9):633-659." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900015781&amp;v=Mjc4OThvS0MzUTRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKRm9YYVJZPU5pZk9mYks3SHRUTXBvOUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Jose M Chaquet,Enrique J Carmona.A survey of video datasets for human actions and activity recognition[J].Computer Vision and Image Understanding,2013(9):633-659.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" Alejandro Newell,Yang Kaiyu,Deng Jia.Stacked hourglass networks for human pose estimation[C].Guangzhou:ECCV,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Stacked hourglass networks for human pose estimation,&amp;quot;">
                                        <b>[16]</b>
                                         Alejandro Newell,Yang Kaiyu,Deng Jia.Stacked hourglass networks for human pose estimation[C].Guangzhou:ECCV,2016.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" Chen X,Yuille A.Articulated pose estimation by agraphical model with image dependent pairwise relations[C].Shanghai:NIPS,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations">
                                        <b>[17]</b>
                                         Chen X,Yuille A.Articulated pose estimation by agraphical model with image dependent pairwise relations[C].Shanghai:NIPS,2014.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-20 09:12</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(09),26-31 DOI:10.16180/j.cnki.issn1007-7820.2019.09.006            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于安全帽佩戴检测的矿山人员违规行为研究</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%9D%E6%B3%BD%E5%8F%8B&amp;code=42588309&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">仝泽友</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E4%BB%95%E6%B0%91&amp;code=39901755&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯仕民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BE%AF%E6%99%93%E6%99%B4&amp;code=42588310&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">侯晓晴</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%81%E6%81%A9%E6%9D%B0&amp;code=10362516&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">丁恩杰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%BF%E4%B8%9A%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91(%E6%84%9F%E7%9F%A5%E7%9F%BF%E5%B1%B1)%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0041682&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国矿业大学物联网(感知矿山)研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%9F%BF%E5%B1%B1%E4%BA%92%E8%81%94%E7%BD%91%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E5%9C%B0%E6%96%B9%E8%81%94%E5%90%88%E5%B7%A5%E7%A8%8B%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">矿山互联网应用技术国家地方联合工程实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%BF%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%8E%A7%E5%88%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国矿业大学信息与控制工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对矿山人员安全帽佩戴检测问题,文中提出了一种基于人脸的身份识别及安全帽佩戴检测的违规行为识别方法。首先在视频图像中检测人脸以识别身份,然后运用卷积神经网络方法检测人员是否佩戴安全帽,实验阶段将此方法与传统的图像处理方法进行测试对比。实验结果显示,基于深度学习的安全帽检测方法的鲁棒性强于传统方法,在不同条件下识别率和运行效率均优于传统方法,深度学习方法的平均识别率高达97%,所需平均运行时间少于传统方法的1/7。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%89%E5%85%A8%E5%B8%BD%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安全帽检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BA%AB%E4%BB%BD%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">身份识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%9D%E8%A7%84%E8%A1%8C%E4%B8%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">违规行为;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%87%86%E7%A1%AE%E7%8E%87%E4%B8%8E%E9%80%9F%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">准确率与速度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B3%E5%9D%87%E6%97%B6%E9%97%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平均时间;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    仝泽友(1994-),男,硕士研究生。研究方向:人体行为识别,计算机视觉等。;
                                </span>
                                <span>
                                    冯仕民(1983-),男,博士,讲师。研究方向:多传感器智能信息处理、融合与人机交互。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>2017年国家重点研发计划项目(2017YFC0804401);</span>
                    </p>
            </div>
                    <h1><b>Recognition of Underground Miners’ Rule-Violated Behavior Based on Safety Helmet Detection</b></h1>
                    <h2>
                    <span>TONG Zeyou</span>
                    <span>FENG Shimin</span>
                    <span>HOU Xiaoqing</span>
                    <span>DING Enjie</span>
            </h2>
                    <h2>
                    <span>Internet of Things (Perception Mine) Research Center,China University of Mining and Technology</span>
                    <span>The National and Local Joint Engineering Laboratory of Internet Application Technology on Mine</span>
                    <span>Institute of Information and Control Engineering,China University of Mining and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem of the miner's helmet-wearing recognition, this paper proposed a method of detecting the safety helmet based rule-violated activity. It was based on the face recognition and helmet detection. Firstly, detect the human faced in the picture and identify them, and then detect whether the person wears a safety helmet or not with the Convolutional Neural Network approach. This paper compared the depth learning method with the traditional method in different conditions in the experiments.The experimental results showed that the robustness of the depth learning based safety helmet detection method was stronger than the traditional method. The recognition rate of the depth learning method was higher than that of the traditional approach under different conditions. The deep learning method achieved an average recognition rate of 97% and the average running time was less than 1/7 of that of the traditional approach.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=safety%20helmet%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">safety helmet detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=identity%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">identity recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=rule-violated%20activity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">rule-violated activity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=accuracy%20and%20efficiency&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">accuracy and efficiency;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=average%20time&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">average time;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-06</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>2017 National Key R&amp;D Projects(2017YFC0804401);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="36">在矿山生产中,安全帽是对矿工生命的保障,能够有效地减少或防止外来危险对矿工头部的伤害。安全帽的佩戴,可降低矿工工作时的作业风险。本文主要使用深度学习研究方法,针对矿工身份识别并对矿工不安全行为做出违规判识,这种不安全行为主要是安全帽的佩戴问题。深度学习方法是近年来的研究热点,目前国内外众多高校与科研机构都对基于深度学习的目标识别进行了广泛而深入的研究<citation id="99" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。刘云波等人使用检测目标像素点的颜色方法进行安全帽佩戴识别研究;冯国臣等人使用机器视觉方法进行安全自动识别研究,主要以统计颜色特征方法进行安全帽检测。现有方法在特定场景中可实现针对颜色特征的高精度识别,但易受背景颜色等因素干扰。论文采用两种方法进行对比:深度学习方法主要使用卷积神经网络训练大量已标记的数据集<citation id="100" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,提取安全帽的抽象特征,通过学习抽象特征进行识别目标,具有适应能力强、准确率高且受背景颜色干扰较小的优点;传统识别方法则使用Haar特征级联分类器检测人脸用于身份识别,并用归一化相关系数模板匹配法检测是否佩戴安全帽。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>1</b> 关键技术</h3>
                <div class="p1">
                    <p id="38">针对矿井这种特殊的作业环境,论文主要针对矿井下监控系统采集的视频图像的研究。为了提高实验方法的运行速度与准确率,需要对采集的图像进行几何变换及去噪处理<citation id="101" type="reference"><link href="6" rel="bibliography" /><link href="8" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。由于电子电路以及低照明或高温会给传感器带来高斯噪声,故采用高斯低通滤波器平滑图像。具体公式如下</p>
                </div>
                <div class="p1">
                    <p id="39"><i>H</i>(<i>u</i>,<i>v</i>)=<i>e</i><sup>-<i>D</i><sup>2</sup>(<i>u</i>,<i>v</i>)/2<i>σ</i><sup>2</sup></sup>      (1)</p>
                </div>
                <div class="p1">
                    <p id="40">式中,<i>D</i>(<i>u</i>,<i>v</i>)是距频率矩形中心的距离,<i>σ</i>是关于中心的扩展度的度量,其中高斯核大小为5×5。</p>
                </div>
                <h4 class="anchor-tag" id="41" name="41"><b>1.1</b> 深度学习及框架</h4>
                <div class="p1">
                    <p id="42">深度学习由Hinton等人在2006年提出<citation id="102" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,目前已经在计算机视觉、自然语言处理等方面取得了优异的成绩。深度学习的神经网络由输入层、隐藏层、输出层构成,其中输入层与输出层只有一层神经网络且每层神经网络均由单元组成。输入层是为了将数据的特征向量传入神经网络,网络之间经过连接点的权重传入下一层,每一层的输出是下一层的输入。隐藏层可以包含任意多层网络结构<citation id="103" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。计算机视觉中隐藏层是为了提取图像特征,通过与输出层之间的全连接权重计算出待识别对象的识别结果信息。</p>
                </div>
                <div class="p1">
                    <p id="43">训练模型均采用监督学习方式,训练神经网络模型又称作学习过程,神经网络学习的本质就是调整各神经元之间的权重以达到预期的目标识别结果。在学习过程中,初始化时使用随机函数产生0～1之间方差为0.01的初始权重,使用反向传播算法不断根据损失调整各神经元之间权重,经过不断学习得到最终需要的模型<citation id="104" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。使用BP反向传播算法来以最小化误差来更新每个连接的权重。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44">(1)对于输出层,公式如下</h4>
                <div class="p1">
                    <p id="45"><i>E</i><sub><i>rrj</i></sub>=<i>O</i><sub><i>j</i></sub>(1-<i>O</i><sub><i>j</i></sub>)(<i>T</i><sub><i>j</i></sub>-<i>O</i><sub><i>j</i></sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="46">其中,<i>T</i><sub><i>j</i></sub>为真实值;<i>O</i><sub><i>j</i></sub>为预测值。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">(2)对于隐藏层,公式如下</h4>
                <div class="p1">
                    <p id="48"><mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>r</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>Ο</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>Ο</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>E</mi></mstyle><msub><mrow></mrow><mrow><mi>r</mi><mi>r</mi><mi>k</mi></mrow></msub><mi>W</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="50">其中,<i>E</i><sub><i>rrk</i></sub>为连接到下一个点的误差。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">(3)权重更新,公式为</h4>
                <div class="p1">
                    <p id="52">Δ<i>W</i><sub><i>ij</i></sub>=<i>lE</i><sub><i>rrj</i></sub><i>O</i><sub><i>i</i></sub>      (4)</p>
                </div>
                <div class="p1">
                    <p id="53">其中,<i>l</i>为学习率下。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">(4)偏向更新,公式为</h4>
                <div class="p1">
                    <p id="55">Δ<i>θ</i><sub><i>j</i></sub>=<i>lE</i><sub><i>rrj</i></sub>      (5)</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>1.2</b> 深度学习模型结构</h4>
                <h4 class="anchor-tag" id="57" name="57">(1)身份识别。</h4>
                <div class="p1">
                    <p id="58">身份识别模型主要参考卷积神经网络与ImageNet大赛冠军模型编写,具有3个卷基层与3个池化层和一个全连接层<citation id="105" type="reference"><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。模型网络参数如表1所示。</p>
                </div>
                <div class="area_img" id="59">
                    <p class="img_tit"><b>表</b>1 <b>深度学习人脸识别模型参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Parameters of deep learning based face recognition model</p>
                    <p class="img_note"></p>
                    <table id="59" border="1"><tr><td><br />类型</td><td>kernel尺寸/步长</td><td>输入尺寸</td></tr><tr><td><br />卷积</td><td>3×3/2</td><td>416×416×3</td></tr><tr><td><br />池化</td><td>3×3/1</td><td>207×207×32</td></tr><tr><td><br />卷积</td><td>3×3/1</td><td>103×103×64</td></tr><tr><td><br />池化</td><td>3×3/1</td><td>101×101×64</td></tr><tr><td><br />卷积</td><td>3×3/2</td><td>99×99×64</td></tr><tr><td><br />池化</td><td>3×3/1</td><td>45×45×64</td></tr><tr><td><br />线性logits</td><td>1×1×64</td><td></td></tr><tr><td><br />softmax</td><td>分类输出</td><td>1×1×10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">(2)安全帽佩戴检测。</h4>
                <div class="p1">
                    <p id="61">安全帽识别模型由迁移学习重新训练Inception-V3最后全连接层得到。所谓迁移学习,就是将在一个数据集上训练好的神经网络通过调整使其适应于一个新的问题。在新的训练数据集上直接利用这个训练好的神经网络的隐含层对图像进行特征提取,然后再将提取到的特征向量作为全连接层的输入来训练一个新的神经网络,用来处理新的问题<citation id="106" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。这样可以大幅减少训练集样本数量,但是准确率相比重新训练一个模型稍低。虽然准确率有所下降,但是综合考虑重新训练新模型的工作量较大,且使用迁移学习训练得到的模型可以达到超过93%的综合准确率,最后选择使用迁移学习。</p>
                </div>
                <div class="p1">
                    <p id="62">Inception-V3模型总共有46层网络,由11个Inception模块组成,共含有96个卷积层,不同的卷积层通过并联方式结合在一起,其中kernel均为3×3,最后由全连接层组成,其结构如图1所示。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Inception-V3结构(1)" src="Detail/GetImg?filename=images/DZKK201909007_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Inception-V3结构(1)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Structure of Inception-V3 (1)</p>

                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Inception-V3结构(2)" src="Detail/GetImg?filename=images/DZKK201909007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Inception-V3结构(2)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Structure of Inception-V3 (2)</p>

                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Inception-V3结构(3)" src="Detail/GetImg?filename=images/DZKK201909007_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Inception-V3结构(3)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Structure of Inception-V3 (3)</p>

                </div>
                <div class="p1">
                    <p id="66">将<i>n</i>×<i>n</i>卷积拆成<i>n</i>×1卷积和1×<i>n</i>卷积,既可以节约大量参数、加速学习、减轻过拟合,同时增加了一层非线性扩展模型表达能力。原论文<citation id="107" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>指出,对这种非对称的卷积结构拆分,其结果比对称的拆分为几个相同的小卷积核效果更好,可以处理更多、更丰富的空间特征,增加特征提取多样性。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag"><b>2</b> 实验流程设计</h3>
                <div class="p1">
                    <p id="68">基于深度学习框架TensorFlow,在训练深度学习模型时使用GeForce GTX GPU运行,运算效率高,训练所有数据只需要3小时。实验中各子功能运行流程及实现方式如下。</p>
                </div>
                <div class="p1">
                    <p id="69">(1)采集需要识别的矿工脸部信息作为训练人脸识别模型的数据集,每人需10 000张样本数据,其中随机选取5%的数据作为测试集,其余数据作为训练集并以名字作为标签。数据集数量选取应当适中,数据集数量不足会导致学习不充分,若过多则导致过拟合且训练时间增加<citation id="108" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。数据采集方面使用dlib检测人脸并保存脸部信息,并给其对应的标签,同时从网上获取一部分不同人群脸部信息用来增强模型鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="70">(2)采集大量安全帽数据训练安全帽识别模型,训练安全帽在Inception-v3基础上使用迁移学习,根据迁移学习特性,数据集则只需数百张数据即可。实验共采集900张安全帽样本数据,其中随机选取5%的数据作为测试集,其余样本数据用于训练模型。训练时需要多个类别数据,若只训练安全帽一类数据集则会导致模型鲁棒性下降。当模型准确率达到设定阈值且训练次数达到指定要求时,保存模型并停止训练。</p>
                </div>
                <div class="p1">
                    <p id="71">实验过程中,首先在实时视频或静态图像中检测人脸用于身份识别。若未检测到人脸表明此刻未出现矿工则不判断安全帽佩戴情况;若检测到人脸则对身份加以识别;若身份识别为未知人员则发出警报并判断是否佩戴安全帽;若识别出身份则检测是否佩戴安全帽并对违规行为作出判识<citation id="109" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。实验方法整体流程如图4所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 判识方法流程图" src="Detail/GetImg?filename=images/DZKK201909007_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 判识方法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Flowchart of the recognition method</p>

                </div>
                <h3 id="73" name="73" class="anchor-tag"><b>3</b> 实验内容及结果分析</h3>
                <div class="p1">
                    <p id="74">为避免实验偶然性,进行多次实验,取平均时间与平均准确率,同时为了比较传统方法与深度学习方法测试时间与准确率,均在X86 Windows电脑上使用Python语言进行测试且在每次测试时均采用相同验证集<citation id="110" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。为全面测试两种方法性能,笔者选取10名在校大学生,平均年龄24岁,其中6名男性与4名女性,每次随机选取1人作为实验测试对象。论文实验环境为实验室模拟真实场景,以矿业公司使用的安全帽作为识别对象。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.1</b> 传统方法与深度学习准确率比较</h4>
                <div class="p1">
                    <p id="76">对于传统方法,为识别身份的结果设置指定阈值,当超过指定阈值时即认为检测结果正确<citation id="111" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。深度学习方法输出的检测结果则是按照已训练模型计算的结果输出。下面对光线、距离等几种可能影响实验结果的因素进行实验,光线因素中3 000 lux相当于室外强光下环境,200 lux相当于阴天室内环境。传统方法与深度学习方法对于身份识别准确率如表2所示。</p>
                </div>
                <div class="area_img" id="77">
                    <p class="img_tit"><b>表</b>2 <b>不同条件下身份识别准确率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2. Comparison of the average face recognition accuracy</p>
                    <p class="img_note"></p>
                    <table id="77" border="1"><tr><td rowspan="2"><br />实验次数</td><td colspan="2"><br />传统方法</td><td colspan="2">深度学习方法</td></tr><tr><td><br />50</td><td>100</td><td>50</td><td>100</td></tr><tr><td><br />安全帽正面</td><td>94%</td><td>95%</td><td>98%</td><td>97%</td></tr><tr><td><br />安全帽侧面</td><td>12%</td><td>10%</td><td>96.6%</td><td>98%</td></tr><tr><td><br />光线弱(200 lux)</td><td>92%</td><td>90%</td><td>98%</td><td>97%</td></tr><tr><td><br />光线强(3 000 lux)</td><td>89%</td><td>98%</td><td>98%</td><td></td></tr><tr><td><br />距离2 m</td><td>96%</td><td>94%</td><td>96%</td><td>97%</td></tr><tr><td><br />距离4 m</td><td>26%</td><td>24%</td><td>94%</td><td>96%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="78">传统方法与深度学习方法对于身份识别准确率如图5所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 两种身份识别的方法100次平均识别率比较" src="Detail/GetImg?filename=images/DZKK201909007_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 两种身份识别的方法100次平均识别率比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5. Comparison of the average face recognition rates
 for two identification methods</p>

                </div>
                <div class="p1">
                    <p id="80">由实验数据发现,距离2 m正脸检测准确率最高可以达到90%。研究中两种方法在身份识别方面均对光线变化比较敏感,在光线条件影响下识别准确率均下降至50%以下,距离因素对识别结果也会有影响。深度学习识别准确率不及传统方法,主要由深度学习的卷积层的深度决定,若增加卷积层可提高识别准确率<citation id="112" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="81">针对多人实验发现,Haar级联分类器可同时检测且识别多人,且准确率相对单人识别准确率几乎无差别。但深度学习研究方法不具备同时检测多人能力,若同时出现多人则准确率急剧下降。</p>
                </div>
                <div class="p1">
                    <p id="82">实验将传统方法阈值设置为40%,只有检测结果置信度大于指定阈值时才认为身份识别正确。随着阈值不断提高准确率随之下降,对于传统方法设置相对低的阈值可获得较高的准确率,深度学习方法则不受阈值影响较为适用。</p>
                </div>
                <div class="p1">
                    <p id="83">基于安全帽佩戴检测的违规行为识别研究,以身份识别为基础。识别身份后需判断是否佩戴安全帽,未佩戴安全帽则判识为违规行为<citation id="113" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。下面根据深度学习与传统方法对安全帽识别做出比较,两种方法实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表</b>3 <b>不同条件下安全帽识别准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3. Comparison of the average safety helmet recognition rate</p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td rowspan="2"><br />实验次数</td><td colspan="2"><br />传统方法</td><td colspan="2">深度学习方法</td></tr><tr><td><br />50</td><td>100</td><td>50</td><td>100</td></tr><tr><td><br />正常光照条件</td><td>92%</td><td>90%</td><td>86%</td><td>85%</td></tr><tr><td><br />光线弱(200 lux)</td><td>42%</td><td>39%</td><td>25%</td><td>30%</td></tr><tr><td><br />光线强(3 000 lux)</td><td>56%</td><td>60%</td><td>24%</td><td>20%</td></tr><tr><td><br />距离2 m</td><td>94%</td><td>91%</td><td>84%</td><td>86%</td></tr><tr><td><br />距离4 m</td><td>20%</td><td>23%</td><td>62%</td><td>65%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85">两种实验方法平均识别准确率如图6所示。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 安全帽识别的两种方法100次平均识别率比较" src="Detail/GetImg?filename=images/DZKK201909007_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 安全帽识别的两种方法100次平均识别率比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6. Comparison of the average safety helmet 
recognition rates for two methods</p>

                </div>
                <div class="p1">
                    <p id="87">由实验数据发现,传统方法选取的归一化相关系数模板匹配方法克服了光线变化敏感问题,最高可以达到95%的准确率。但受模板选取影响,对安全帽侧面识别率较低且对距离变化敏感,准确率下降至约20%。深度学习在识别安全帽方面拥有较好的结果,能够克服光线、距离等因素的干扰且准确率达到95%以上。若同时检测多个佩戴安全帽人员,传统方法只能判识某一位矿工是否佩戴了安全帽,改进方法是使用多对象的模板匹配;深度学习方法同样不能正确地同时判识出多个人佩戴安全帽情况。</p>
                </div>
                <div class="p1">
                    <p id="88">针对实验数据发现在安全帽识别率方面,深度学习方法优于传统方法,两种方法均针对单人判识结果准确率较高。下面将对两种方法运行时间进行比较。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>3.2</b> 运行时间比较</h4>
                <div class="p1">
                    <p id="90">测试传统方法与深度学习方法时均未计算模型加载时间。因为模型只需在实验开始时加载即可,若计算模型加载时间则会导致第一次运行时间明显大于后面若干次运行时间。深度学习方法与传统方法实验运行时间如表4所示。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>4 <b>两种方法平均运行时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 4. Average running time of two methods</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td><br />     次数<br />方法     </td><td>50</td><td>100</td></tr><tr><td><br />传统方法</td><td>12 338 ms</td><td>12 260 ms</td></tr><tr><td><br />深度学习方法</td><td>1 703 ms</td><td>1 681 ms</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">针对实验数据发现,深度学习方法运行时间不到传统方法的1/7,可以较好的满足实际应用。部分实验可视化结果如图7所示。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909007_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 实验可视化结果" src="Detail/GetImg?filename=images/DZKK201909007_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 实验可视化结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909007_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7. Visualization results of experiment</p>

                </div>
                <div class="p1">
                    <p id="94">实验可视化结果中,若识别出人员身份则显示人员姓名首字母缩写,否则用UP (Unknow identity Person)表示未知人员身份。HY(Helmet Yes)表示佩戴安全帽,HN(Helmet No)表示未佩戴安全帽,<i>T</i>(time)表示时间,单位为ms。实验过程中发现,运用基于深度学习模型的单人检测,安全帽识别率高且能准确迅速判识出结果,适用于实际场景,为基于深度学习的矿山目标识别研究提供了参考。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>3.3</b> 实验结果分析</h4>
                <div class="p1">
                    <p id="96">身份识别实验结果显示,光照过弱或过强都会导致两种实验方法的识别准确率下降,下降幅度达50%～60%,深度学习方法对距离因素的鲁棒性强于传统方法,传统方法在人脸识别准确率受难以设置的阈值影响大,手动调整阈值可使传统方法的识别准确率在不同条件下高于基于深度学习方法的识别准确率,但阈值选取不当会导致准确率下降至低于深度学习方法的准确率。实验结果显示,深度学习方法在不同条件下识别率均高于传统方法,平均识别率高达97%,而传统方法受安全帽检测角度和距离影响时识别率仅10%和24%。总之,深度学习方法对于身份识别正确率约为85%,安全帽佩戴检查正确率超过95%,且深度学习方法实验运行效率远高于传统方法,所需平均运行时间不到传统方法的1/7,可以满足实际应用需求。</p>
                </div>
                <h3 id="97" name="97" class="anchor-tag"><b>4</b> 结束语</h3>
                <div class="p1">
                    <p id="98">本文提出了一种基于矿工人脸识别和安全帽佩戴检测的违规行为识别方法,运用卷积神经网络的方法对安全帽进行检测,识别矿工不戴安全帽的违规行为。实验阶段,在测试对象角度不同、光照强弱和距离远近等条件下,分别对比了基于深度学习的方法与传统方法在人脸识别和安全帽佩戴检测两个方面的准确率和运行效率,实验结果显示,基于深度学习的方法优于传统方法。论文提出的基于深度学习的矿工违规行为识别方法,在安全帽佩戴检测方面。准确率高、运行速度快,适合在实际环境不同条件下运行,将在示范工程中应用。后续研究将围绕视频图像中多人检测及多防护用具识别展开进一步研究,从而可以识别更多的违规行为。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition via a boosted deep belief network[C/OL]">

                                <b>[1]</b> Liu P,Han S,Meng Z,et al.Facial expression recognition via a boosted deep belif network[C].Washington D C:Proceedlings of the IEEE Conference on Computer Vision and Pattern Recognition,IEEE,2014.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201801014&amp;v=MDI4NTE0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXpoVUx6TUlUZkFaYkc0SDluTXJvOUVZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 熊海朋,陈洋洋,陈春玮.基于卷积神经网络的场景图像文本定位研究[J].电子科技,2018,31(1):50-53.Xiong Haipeng,Chen Yangyang,Chen Chunwei.Text location in image based on convolution neual network[J].Electronic Science and Technology,2018,31(1):50-53.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gonzalez digital image processing">

                                <b>[3]</b> Rafael C.Gonzalez digital image processing[M].Beijing:Publishing House of Electronics Industry,2017.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201804007&amp;v=MTU1MzFwRnl6aFVMek1JVGZBWmJHNEg5bk1xNDlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 完颜勇.基于对比度增强和背景估计的文档图像二值化[J].电子科技,2018,31(4):20-25.Wanyan Yong.Doucument image binarization based on contrast enhancement and background estimation[J].Electronic Science and Technology,2018,31(4):20-25.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial landmark detection by deep multi-task learning">

                                <b>[5]</b> Zhang Z,Luo P,Loy C C,et al.Facial landmark detection bydeep multi-task learning[C].Berlin:Proceedings of the European Conference on Computer Vision,Springer,2014.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606005&amp;v=MDY0NTdCdEdGckNVUjdxZlp1WnBGeXpoVUx6TUtDTGZZYkc0SDlmTXFZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 朱煜,赵江坤,王逸宁,等.基于深度学习的人体行为识别算法综述[J].自动化学报,2016,42(6):848-875.Zhu Yu,Zhao Jiangkun,Wang Yining,et al.A review of human action recognition based on deep learning[J].Acta Automatica Sinica,2016,42(6):848-875.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161945&amp;v=MTI0MzJVcmpJSkZvWGFSWT1OaWZPZmJLOUg5UE9ySTlGWmUwT0JYZzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Fischer A,Igel C.Training restricted boltzmann machines:an introduction[J].Pattern Recognition,2014,47(1):25-39.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the Inception Architecture for Computer Vision">

                                <b>[8]</b> Christian Szegedy.Vincent vanhoucke rethinking the inception architecture for computer vision[C].Boston:IEEE Conference on Computer Vision and Pattern Recognition,2016.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Trunk-branch ensemble convolutional neural networks for video-based face recognition">

                                <b>[9]</b> Ding Changxing,Tao Dacheng.Trunk-branch ensemble convolutional neural networks for video-based face recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):89-95.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MDg3NjJqSUpGb1hhUlk9TmlmT2ZiSzhIOURNcUk5Rlp1SUpESDA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Schmidhuber J.Deep learning in neural networks:an overview[J].Neural Networks,2015,61(7):85-117.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1013153329.nh&amp;v=MTA3NzlHRnJDVVI3cWZadVpwRnl6aFVMek1WRjI2SGJLOUhkTE9wcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 龚爱平.基于嵌入式机器视觉的信息采集与处理技术研究[D].杭州:浙江大学,2013.Gong Aiping.Study of the information acquisition and processing technology based on embaedded computer vision[D].Hangzhou:Zhejiang University,2013.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning OpenCV3 computer vision with python">

                                <b>[12]</b> Joe Minichino.Learning OpenCV3 computer vision with python[M].Paris:Packet Publishing,2015.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HJKJ201704013&amp;v=MTIyOTlNTFNmQVpMRzRIOWJNcTQ5RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emhVTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 毕林,谢伟,崔君.基于卷积神经网络的矿工安全帽佩戴识别研究[J].黄金科学技术,2017,25(4):73-80.Bi Lin,Xie Wei,Cui Jun.Identification research on the miner’s safety helmet wear based on convolutional neural network[J].Gold Science and Technology,2017,25(4):73-80.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121309120000&amp;v=MjY4NjhGeW5pVTd2TUkxMFFYRnF6R2JLNkg5TE1wbzVIWk9zUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 黄文坚,唐源.Tensorflow实战[M].北京:电子工业出版社,2017.Huang Wenjian,Tang Yuan.Practice of tensorflow[M].Beijing:Publishing House of Electronics Industry,2017.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900015781&amp;v=MTMyNDRGaW5sVXJqSUpGb1hhUlk9TmlmT2ZiSzdIdFRNcG85RlpPb0tDM1E0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Jose M Chaquet,Enrique J Carmona.A survey of video datasets for human actions and activity recognition[J].Computer Vision and Image Understanding,2013(9):633-659.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Stacked hourglass networks for human pose estimation,&amp;quot;">

                                <b>[16]</b> Alejandro Newell,Yang Kaiyu,Deng Jia.Stacked hourglass networks for human pose estimation[C].Guangzhou:ECCV,2016.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations">

                                <b>[17]</b> Chen X,Yuille A.Articulated pose estimation by agraphical model with image dependent pairwise relations[C].Shanghai:NIPS,2014.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201909007" />
        <input id="dpi" type="hidden" value="299" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201909007&amp;v=MDAwOTZxQnRHRnJDVVI3cWZadVpwRnl6aFVMek1JVGZBWmJHNEg5ak1wbzlGWTRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

