

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139162618857500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201911005%26RESULT%3d1%26SIGN%3dfJ0XmeRshlf4BBFtbsZ5O6EpEE4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201911005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201911005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201911005&amp;v=MDQwODBJVGZBWmJHNEg5ak5ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkza1Zyekk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;1 分割模型&lt;/b&gt; "><b>1 分割模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="&lt;b&gt;1.1 空洞残差模块&lt;/b&gt;"><b>1.1 空洞残差模块</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;1.2  空洞金字塔池模块&lt;/b&gt;"><b>1.2  空洞金字塔池模块</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;1.3 注意力模块&lt;/b&gt;"><b>1.3 注意力模块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="&lt;b&gt;2 实验结果与分析&lt;/b&gt; "><b>2 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="&lt;b&gt;2.1 数据和预处理&lt;/b&gt;"><b>2.1 数据和预处理</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;2.2 训练细节&lt;/b&gt;"><b>2.2 训练细节</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;2.3 性能评估和分析&lt;/b&gt;"><b>2.3 性能评估和分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="&lt;b&gt;3 结束语&lt;/b&gt; "><b>3 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 网络框架">图1 网络框架</a></li>
                                                <li><a href="#64" data-title="图2 模块说明 
">图2 模块说明 
</a></li>
                                                <li><a href="#80" data-title="图3 注意力模块">图3 注意力模块</a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;模块性能对比&lt;/b&gt;"><b>表</b>1 <b>模块性能对比</b></a></li>
                                                <li><a href="#101" data-title="图4 分割模型结果对比">图4 分割模型结果对比</a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表&lt;/b&gt;2 Warwick-Qu&lt;b&gt;数据集验证结果&lt;/b&gt;"><b>表</b>2 Warwick-Qu<b>数据集验证结果</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表&lt;/b&gt;3 UCSB breast&lt;b&gt;数据集验证结果&lt;/b&gt;"><b>表</b>3 UCSB breast<b>数据集验证结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Winawer S J,Fletcher R H,Miller L,et al.Colorectal cancer screening:clinical guidelines and rationale[J].Gastroenterology,1997,113(4):594-642." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601546817&amp;v=MTAyNzd0RE5xWTlFWWU4SkJIMCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKVjhSYVJFPU5pZk9mYks3SA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Winawer S J,Fletcher R H,Miller L,et al.Colorectal cancer screening:clinical guidelines and rationale[J].Gastroenterology,1997,113(4):594-642.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 陈爱斌,江霞.细胞分割算法研究方法综述[J].电子世界,2011(15):76-79.Chen Aibin,Jiang Xia.Review of research methods for cell segmentation algorithms[J].Electronic World,2011(15):76-79." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ELEW201115041&amp;v=MTIyMDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeTNrVnJ6TElDSE9lYkc0SDlETnFvOUJaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         陈爱斌,江霞.细胞分割算法研究方法综述[J].电子世界,2011(15):76-79.Chen Aibin,Jiang Xia.Review of research methods for cell segmentation algorithms[J].Electronic World,2011(15):76-79.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Elston C W,Ellis I O.Pathological prognostic factors in breast cancer.I.The value of histological grade in breast cancer:experience from a large study with long-term follow-up[J].Histopathology,2010,19(5):403-410." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pathological prognostic factors in breast cancer. I. The value of histological grade in breast cancer: experience from a large study with long-term follow-up">
                                        <b>[3]</b>
                                         Elston C W,Ellis I O.Pathological prognostic factors in breast cancer.I.The value of histological grade in breast cancer:experience from a large study with long-term follow-up[J].Histopathology,2010,19(5):403-410.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Gleason D F.Histologic grading of prostate cancer:A perspective[J].Human Pathology,1992,23(3):273-279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histologic grading of prostate cancer: a perspective">
                                        <b>[4]</b>
                                         Gleason D F.Histologic grading of prostate cancer:A perspective[J].Human Pathology,1992,23(3):273-279.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Fleming M,Ravula S,Tatishchev S F,et al.Colorectal carcinoma:Pathologic aspects[J].Journal of Gastrointestinal Oncology,2012,3(3):153-153." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Colorectal carcinoma: Pathologic aspects">
                                        <b>[5]</b>
                                         Fleming M,Ravula S,Tatishchev S F,et al.Colorectal carcinoma:Pathologic aspects[J].Journal of Gastrointestinal Oncology,2012,3(3):153-153.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 章毓晋.图像处理和分析基础[M].北京:高等教育出版社,2002.Zhang Yujin.The basis of image processing and analysis [M].Beijing:Higher Education Press,2002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787040111262001&amp;v=MDM4NTVEWnVzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnluaVU3ckpKVjBYWEZxekdiTzhIdEROcm8x&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         章毓晋.图像处理和分析基础[M].北京:高等教育出版社,2002.Zhang Yujin.The basis of image processing and analysis [M].Beijing:Higher Education Press,2002.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Gurcan M N,Boucheron L E,Can A,et al.Histopathological image analysis:a review[J].IEEE Reviews in Biomedical Engineering,2009(2):147-147." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histopathological Image Analysis: A Review">
                                        <b>[7]</b>
                                         Gurcan M N,Boucheron L E,Can A,et al.Histopathological image analysis:a review[J].IEEE Reviews in Biomedical Engineering,2009(2):147-147.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Altunbay D,Cigir C,Sokmensuer C,et al.Color graphs for automated cancer diagnosis and grading[J].IEEE Transactions on Biomedical Engineering,2010,57(3):665-665." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Color graphs for automated cancer diagnosis and grading">
                                        <b>[8]</b>
                                         Altunbay D,Cigir C,Sokmensuer C,et al.Color graphs for automated cancer diagnosis and grading[J].IEEE Transactions on Biomedical Engineering,2010,57(3):665-665.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Gunduz Demir C,Kandemir M,Tosun A B,et al.Automatic segmentation of colon glands using object-graphs[J].Medical Image Analysis,2010,14(1):1-12." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300711399&amp;v=MDcwNzk3SHRETnJJOUZZK29PRDNVd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUpWOFJhUkU9TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Gunduz Demir C,Kandemir M,Tosun A B,et al.Automatic segmentation of colon glands using object-graphs[J].Medical Image Analysis,2010,14(1):1-12.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Fu H,Qiu G,Shu J,et al.A novel polar space random field model for the detection of glandular structures[J].IEEE Transactions on Medical Imaging,2014,33(3):764-764." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel polar space random field model for the detection of glandular structures">
                                        <b>[10]</b>
                                         Fu H,Qiu G,Shu J,et al.A novel polar space random field model for the detection of glandular structures[J].IEEE Transactions on Medical Imaging,2014,33(3):764-764.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Khasawneh S,Al Wahadni A,Lloyd C H.A stochastic polygons model for glandular structures in colon histology images[J].IEEE Transactions on Medical Imaging,2015,34(11):2366-2378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Stochastic Polygons Model for Glandular Structures in Colon Histology Images">
                                        <b>[11]</b>
                                         Khasawneh S,Al Wahadni A,Lloyd C H.A stochastic polygons model for glandular structures in colon histology images[J].IEEE Transactions on Medical Imaging,2015,34(11):2366-2378.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Ronneberger O,Fischer P,Brox T.U-Net:convolutional networks for biomedical image segmentation[M].Berlin:Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015,Springer International Publishing,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[12]</b>
                                         Ronneberger O,Fischer P,Brox T.U-Net:convolutional networks for biomedical image segmentation[M].Berlin:Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015,Springer International Publishing,2015.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Chen H,Qi X,Yu L,et al.DCAN:Deep contour-aware networks for object instance segmentation from histology images[J].Medical Image Analysis,2017,36(9):135-146." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFA277EA38DE2BE696CCDA87165D73997&amp;v=MDM5ODlYZmxyUlF3RGJXWFRMT1lDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0OWh3Ym02dzZzPU5pZk9mY1hKSE5iTDJ2NUdiSjk2RGc1TXlSOFZtVXdKTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Chen H,Qi X,Yu L,et al.DCAN:Deep contour-aware networks for object instance segmentation from histology images[J].Medical Image Analysis,2017,36(9):135-146.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Xu Y,Li Y,Liu M,et al.Gland instance segmentation by deep multichannel side supervision[C].Athens:International Conference on Medical Image Computing and Computer-Assisted Intervention,Springer,Cham,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gland instance segmentation by deep multichannel side supervision">
                                        <b>[14]</b>
                                         Xu Y,Li Y,Liu M,et al.Gland instance segmentation by deep multichannel side supervision[C].Athens:International Conference on Medical Image Computing and Computer-Assisted Intervention,Springer,Cham,2016.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Xu Y,Li Y,Wang Y,et al.Gland instance segmentation using deep multichannel neural networks[J].IEEE Transactions on Biomedical Engineering,2017,64(12):2901-2912." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gland instance segmentation using deep multichannel neural networks">
                                        <b>[15]</b>
                                         Xu Y,Li Y,Wang Y,et al.Gland instance segmentation using deep multichannel neural networks[J].IEEE Transactions on Biomedical Engineering,2017,64(12):2901-2912.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Raza S E A,Cheung L,Epstein D,et al.MIMONet:gland segmentation using multi-input-multi-output convolutional neural network[C].Edinburgh:Annual Conference on Medical Image Understanding and Analysis,Springer,Cham,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MIMONet:gland segmentation using multi-input-multi-output convolutional neural network">
                                        <b>[16]</b>
                                         Raza S E A,Cheung L,Epstein D,et al.MIMONet:gland segmentation using multi-input-multi-output convolutional neural network[C].Edinburgh:Annual Conference on Medical Image Understanding and Analysis,Springer,Cham,2017.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Chen L C,Papandreou G,Kokkinos I,et al.Deeplab:Semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected crfs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[17]</b>
                                         Chen L C,Papandreou G,Kokkinos I,et al.Deeplab:Semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected crfs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):834-848.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Drelie Gelasca E,Obara B,Fedorov D,et al.A biosegmentation benchmark for evaluation of bioimage analysis methods[J].BMC Bioinformatics,2009,10(1):368-368." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A biosegmentation benchmark for evaluation of bioimage analysis methods">
                                        <b>[18]</b>
                                         Drelie Gelasca E,Obara B,Fedorov D,et al.A biosegmentation benchmark for evaluation of bioimage analysis methods[J].BMC Bioinformatics,2009,10(1):368-368.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Sirinukunwattana K,Pluim J P,Chen H,et al.Gland segmentation in colon histology images:The glas challenge contest[J].Medical Image Analysis,2017,35(8):489-502." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gland segmentation in colon histology images:The glas challenge contest">
                                        <b>[19]</b>
                                         Sirinukunwattana K,Pluim J P,Chen H,et al.Gland segmentation in colon histology images:The glas challenge contest[J].Medical Image Analysis,2017,35(8):489-502.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-436." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Learning,&amp;quot;">
                                        <b>[20]</b>
                                         Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-436.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Yu F,Koltun V,Funkhouser T A.Dilated residual networks[C].Hawaii:Conference on Computer Vision and Pattern Recognition,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dilated residual networks">
                                        <b>[21]</b>
                                         Yu F,Koltun V,Funkhouser T A.Dilated residual networks[C].Hawaii:Conference on Computer Vision and Pattern Recognition,2017.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Wang P,Chen P,Yuan Y,et al.Understanding convolution for semantic segmentation[C].California:IEEE Winter Conference on Applications of Computer Vision (WACV),2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding convolution for semantic segmentation">
                                        <b>[22]</b>
                                         Wang P,Chen P,Yuan Y,et al.Understanding convolution for semantic segmentation[C].California:IEEE Winter Conference on Applications of Computer Vision (WACV),2018.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Hu J,Shen L,Sun G.Squeeze-and-excitation networks[J].IEEE Transaction on Network,2017,28(4):523-539." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Squeeze-and-excitation networks">
                                        <b>[23]</b>
                                         Hu J,Shen L,Sun G.Squeeze-and-excitation networks[J].IEEE Transaction on Network,2017,28(4):523-539.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" Liu Z,Li X,Luo P,et al.Semantic image segmentation via deep parsing network[C].Santiago:Proceedings of the IEEE International Conference on Computer Vision,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic image segmentation via deep parsing network">
                                        <b>[24]</b>
                                         Liu Z,Li X,Luo P,et al.Semantic image segmentation via deep parsing network[C].Santiago:Proceedings of the IEEE International Conference on Computer Vision,2015.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" Oktay O,Schlemper J,Folgoc L L,et al.Attention U-Net:learning where to look for the pancreas[J].IEEE Transaction on Network,2018,29(2):117-129." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention U-Net:learning where to look for the pancreas">
                                        <b>[25]</b>
                                         Oktay O,Schlemper J,Folgoc L L,et al.Attention U-Net:learning where to look for the pancreas[J].IEEE Transaction on Network,2018,29(2):117-129.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" Abadi,Mart&#237;n,Barham P,et al.TensorFlow:A system for large-scale machine learning[J].IEEE Transaction on Network,2016,27(9):1024-1037." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A system for large-scale machine learning">
                                        <b>[26]</b>
                                         Abadi,Mart&#237;n,Barham P,et al.TensorFlow:A system for large-scale machine learning[J].IEEE Transaction on Network,2016,27(9):1024-1037.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" 何菁,陈胜.一种全新的两步自动化医学图像分割方案[J].电子科技,2016,29(7):85-87.He Jing,Chen Sheng.A new two-step automated medical image segmentation scheme[J].Electronic Science and Technology,2016,29(7):85-87." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201607025&amp;v=MjI3NDViRzRIOWZNcUk5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5M2tWcnpMSVRmQVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         何菁,陈胜.一种全新的两步自动化医学图像分割方案[J].电子科技,2016,29(7):85-87.He Jing,Chen Sheng.A new two-step automated medical image segmentation scheme[J].Electronic Science and Technology,2016,29(7):85-87.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-29 14:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(11),18-22 DOI:10.16180/j.cnki.issn1007-7820.2019.11.004            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进</b>U-Net<b>网络的腺体细胞图像分割算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%9D%E7%90%9B%E5%9C%86&amp;code=38736417&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贝琛圆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E6%B5%B7%E6%BB%A8&amp;code=11245823&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于海滨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BD%98%E5%8B%89&amp;code=32363910&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潘勉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E6%B4%81&amp;code=32363913&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋洁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%95%E7%82%B3%E8%B5%9F&amp;code=31779256&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吕炳赟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%9D%AD%E5%B7%9E%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0073968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杭州电子科技大学电子信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%8D%8E%E6%8A%80%E6%9C%AF%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0487683&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江大华技术股份有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对腺体图像在自动分割过程中由于多尺度目标和信息丢失影响导致准确率降低的问题,文中采用了一种引入注意力模块的全卷积神经网络模型。该模型遵循编码器-解码器结构,在编码网络中用空洞残差卷积层代替原有的普通卷积层,并添加空洞金字塔池;再在解码网络中加入注意力模块,使模型输出高分辨率特征图,提高对多尺度目标的分割精度。实验结果表明,提出的网络模型参数少分割精度高,对腺体图像的平均分割精度高达89.7%,具有较好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">编码器-解码器结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E6%B4%9E%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空洞金字塔池;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力模块;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E7%89%B9%E5%BE%81%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高分辨率特征图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%89%B2%E7%B2%BE%E5%BA%A6%E9%AB%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分割精度高;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    贝琛圆(1994-),女,硕士研究生。研究方向:图像处理。;
                                </span>
                                <span>
                                    于海滨(1979-),男,博士,副教授。研究方向:图像处理与计算机视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>浙江省自然科学基金(LY18F010014);</span>
                    </p>
            </div>
                    <h1><b>Gland Cell Image Segmentation Algorithm Based on Improved U-Net Network</b></h1>
                    <h2>
                    <span>BEI Chenyuan</span>
                    <span>YU Haibin</span>
                    <span>PAN Mian</span>
                    <span>JIANG Jie</span>
                    <span>LÜ Bingyun</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Information,Hangzhou Dianzi University</span>
                    <span>Zhejiang Dahua Technology Co. Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>This paper proposed a full convolutional neural network model with attention module to solve the problem that multi-scale targets and information loss affect the segmentation accuracy of gland images in the automatic segmentation process. This model followed the encoder-decoder structure. Firstly, the atrous spatial pyramid pooling was added to the encoder path, and the original residual convolution layer was replaced by the atrous residual convolution layer in the encoder path. Secondly, the attention module was added to the decoder path to make the model output high-resolution feature map and improve the segmentation accuracy of the multi-scale object. The experimental results showed that the proposed network model had fewer parameters, high segmentation precision and good robustness, besides, the average segmentation accuracy of gland images was as high as 89.7%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=full%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">full convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=encoder-decoder%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">encoder-decoder structure;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=atrous%20spatial%20pyramid%20pooling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">atrous spatial pyramid pooling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20module&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention module;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high-resolution%20feature%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high-resolution feature map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high%20segmentation%20precision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high segmentation precision;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-01</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Natural Science Foundation of Zhejiang Province(LY18F010014);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="57">结肠直肠癌是男性中第三常见的癌症,也是妇女中第二常见的癌症,其发病率逐年升高,其中大约95%的结肠直肠癌是腺癌<citation id="124" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。通常情况下,一个典型的腺体是由形成内部管状结构的腔管区域和细胞质周围的上皮细胞核组成的<citation id="125" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。由腺上皮产生的恶性肿瘤,也被称为腺癌,是最常见的癌症形式。在组织病理学检查中,腺体形态被广泛用于评估几种腺癌,包括乳腺<citation id="126" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,前列腺<citation id="127" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和结肠<citation id="128" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。精确的腺体分割是获得可靠的形态学统计信息的一个关键先决条件,可帮助分析肿瘤的侵袭性。在以前,腺体分割是由评估活检样本中腺体结构的病理学专家进行的<citation id="129" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,但手工注释存在再现性有限、工作量大、费时等问题。随着计算病理学的出现,数字化组织学幻灯片正在被大量使用,需要分析大规模的组织病理学资料。因此,临床实践中为了提高分割效率和可靠性及减少病理学家的工作量,对自动分割方法提出很高的要求。</p>
                </div>
                <div class="p1">
                    <p id="58">现有技术通常利用各种手工特征或先验知识来分析组织病理学图像中的腺体结构<citation id="130" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。如基于图的方法<citation id="138" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>、极坐标空间随机场模型<citation id="131" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、随机多边形模型<citation id="132" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。近年来,深度学习因其强大的特征表达能力,在计算机视觉的图像识别相关任务中取得了巨大的成功,也推动了医学图像分析的发展。U-Net<citation id="133" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在腺体分割任务上取得了优异性能。虽然U-Net是一个较为有效且简单的模型,但其模型深度不够,导致特征表达能力有限。为了进一步提高腺体实例分割性能,Chen等人通过在训练过程中制定明确的轮廓损失函数,提出了一个深度轮廓感知网络<citation id="134" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,并在2015年MICCAI腺体分割(Gland Segmentation,GlaS)现场挑战中取得最佳性能。Yan 等人<citation id="135" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了一个将复杂的多通道区域和边界模式与侧面监督相结合的框架,以此实现腺体实例分割。这项工作在后期又进行了扩展<citation id="136" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,通过加入附加的边界框信息来提高性能。最近,Shan等人<citation id="137" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了MIMO-Net多输入多输出网络,并实现了较好的性能。</p>
                </div>
                <div class="p1">
                    <p id="59">虽然上述模型对腺体细胞的分割都表现出较好的性能,但仍然存在以下两个问题:(1)由于神经网络中的下采样导致目标边缘的细节信息丢失,使得上采样后的特征图分辨率不高,分割精度降低;(2)待分割的腺体具有不同的大小和形状,特别是随着癌症等级的增加,腺体出现结构分化现象,增加了分割难度。</p>
                </div>
                <div class="p1">
                    <p id="60">因此,为了提高分割精度,本文提出的分割模型在基础U-Net模型框架中引入注意力模块(Attention Gate,AG)和空洞空间金字塔池(Atrous Spatial Pyramid Pooling,ASPP)<citation id="139" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>来解决上述问题。并选用UCSB breast cancer cell数据集<citation id="140" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和GlaS比赛提供的Warwick-Qu数据集<citation id="141" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>作为实验数据来验证新模型的有效性。实验结果表明,AG和ASPP的引入确实能提高模型对腺体细胞的分割精度,且具有较好的鲁棒性。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>1 分割模型</b></h3>
                <div class="p1">
                    <p id="62">本文提出的模型框架结构图如图1所示。这是一个端到端的深度网络,输入图像是3通道的RGB图像,输出图像是二值分割图,分割图中白色部分代表腺体目标。整个网络没有全连接层,只是通过卷积层实现特征提取,和基础的U-Net网络相似,是一个对称模型。尽管深层神经网络可以生成高层次的上下文信息,但为了精确描绘腺体边界,融合低层次的信息也是非常重要的。融合过程中每次进行2倍上采样,并将低级特征连接到每个上采样块的开始处。如图1中横向虚线部分所示,在级联之前,使用1×1卷积来增加低级特征的深度,确保级联时两个组件有相同的贡献。在网络最后,使用Softmax层将神经网络的结果转换成二分类问题。下文将对模型中引入的空洞残差模块、ASPP模块和AG模块分别进行详细的描述。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911005_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络框架" src="Detail/GetImg?filename=images/DZKK201911005_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911005_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Network structure</p>

                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911005_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 模块说明" src="Detail/GetImg?filename=images/DZKK201911005_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 模块说明 
  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911005_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Module description 
</p>
                                <p class="img_note">(a)残差模块 (b)空洞残差模块 (c)模型网络层</p>
                                <p class="img_note">(a) Residual unit (b) Dilated residual unit (c) Model layers</p>

                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>1.1 空洞残差模块</b></h4>
                <div class="p1">
                    <p id="66">腺体分割是一项复杂的任务,需要非常深的网络才能进行有意义的特征提取。因此,在网络框架中使用残差单元能实现有效的梯度传播。如图1中(a)所示,传统的残差单元可以定义为</p>
                </div>
                <div class="p1">
                    <p id="67"><i>y</i>=<i>f</i>(<i>x</i>,<i>W</i><sub><i>i</i></sub>)+<i>x</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="68"><i>f</i>(<i>x</i>)=<i>W</i><sub>2</sub>(<i>σ</i>(<i>W</i><sub>1</sub><i>x</i>))      (2)</p>
                </div>
                <div class="p1">
                    <p id="69">其中,<i>x</i>和<i>y</i>分别定义为输入和输出,<i>W</i><sub><i>i</i></sub>定义为权重,<i>f</i>函数中的<i>σ</i>定义为ReLU函数。输入<i>x</i>和<i>f</i>通过相加操作结合在一起。</p>
                </div>
                <div class="p1">
                    <p id="70">传统的卷积神经网络(Convolutional Neural Network,CNN)采用最大池化层和卷积层结合的方式来增加感受野的大小<citation id="142" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,而最大池化层会导致低级信息的丢失,从而对精确分割造成严重影响。因此,为了减弱低级信息丢失的影响,除使用传统的残差单元外,在特征提取过程中还添加了一种改进版残差单元:空洞残差单元。空洞残差和传统残差之间的区别在于空洞残差使用的是空洞卷积<citation id="143" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。C.Liang-Chieh等人提出空间卷积,是为了解决分割任务中,既需要高分辨率的特征图,又需要较大感受野之间的矛盾。空洞卷积核是通过在传统卷积核中插入不同比例的零得到的。相比传统卷积操作,空洞卷积能在不增加参数的情况下得到较大的感受野,并得到和输入大小相同的特征图。在本模型中只需将每个3×3的普通卷积替换为3×3空洞卷积就可以将空洞卷积合并到残差单元中。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>1.2  空洞金字塔池模块</b></h4>
                <div class="p1">
                    <p id="72">尽管空洞卷积解决了特征图分辨率和感受野之间的矛盾,但这种方法仍然有局限。因为空洞卷积特征图中的所有神经元共享着同一种感受野尺寸,这就意味着分割掩码产生过程中只使用了一种尺度的特征。而许多文献<citation id="144" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>中显示,多尺度信息可以帮助解决类别模棱两可的问题。因此,C.Liang-Chieh等人提出了ASPP,ASPP可以结合不同尺度的空间卷积产生的特征图,使输出特征图的神经元包含了多种尺度的感受野,实现多尺度信息的融合并提高分割精度。在本文的框架中,ASPP的目标是检测处于不同癌症级别的腺体,因为随着癌症级别的变化,腺体的形状和大小会发生较大改变,而ASPP中的多尺度空洞卷积恰好可以提取不同尺度的目标特征。但要注意的是,不能将空洞尺寸扩张太大,当扩大的内核比输入的特征图更大时,该卷积操作就退化成1×1的卷积。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>1.3 注意力模块</b></h4>
                <div class="p1">
                    <p id="74">为解决多尺度目标问题,模型引入了ASPP模块,但空洞卷积是一种稀疏计算,这可能会导致产生网格伪像<citation id="145" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。因此尝试SENet<citation id="146" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和Parsenet<citation id="147" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>的方法,使用注意力模块从CNN的高层次特征中提取出准确的像素级注意力特征来解决该问题。最近Oktay O等人<citation id="148" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation> 提出AG模块,该模块能够抑制无关区域的激活,这样就可以减弱非目标区域的定位。AG模块被定义为</p>
                </div>
                <div class="p1">
                    <p id="75"><i>q</i><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext></mrow><mi>l</mi></msubsup></mrow></math></mathml>=<i>ψ</i><sup>T</sup>(<i>σ</i><sub>1</sub>(<i>W</i><sup>T</sup><sub><i>x</i></sub><i>x</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>+<i>W</i><sup>T</sup><sub><i>g</i></sub><i>g</i><sub><i>i</i></sub>+<i>b</i><sub><i>g</i></sub>))+<i>b</i><sub><i>ψ</i></sub>      (3)</p>
                </div>
                <div class="p1">
                    <p id="76"><i>α</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>=<i>σ</i><sub>2</sub>(<i>q</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>a</mtext><mtext>t</mtext><mtext>t</mtext></mrow><mi>l</mi></msubsup></mrow></math></mathml>(<i>x</i><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>,<i>g</i><sub><i>i</i></sub>;<i>Θ</i><sub>att</sub>))      (4)</p>
                </div>
                <div class="p1">
                    <p id="77"><i>y</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>c</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>=<i>x</i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>c</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>·<i>α</i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="79">式中,<i>x</i><sub><i>i</i></sub><sup><i>l</i></sup>和<i>y</i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>c</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>分别代表输入和输出;<i>g</i><sub><i>i</i></sub>代表高层次上下文信息提供的门信号;式(6)代表sigmoid激活函数。AG的参数<i>Θ</i><sub>att</sub>包括线性转换参数<i>W</i><sub><i>x</i></sub>、<i>W</i><sub><i>g</i></sub>、<i>ψ</i>和偏置<i>b</i><sub><i>ψ</i></sub>、<i>b</i><sub><i>g</i></sub>,具体模块如图3所示。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 注意力模块" src="Detail/GetImg?filename=images/DZKK201911005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 注意力模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Attention module</p>

                </div>
                <div class="p1">
                    <p id="81">具体来说,就是分别对高层次特征<i>g</i>和低层次特征<i>x</i><sup><i>l</i></sup>执行1×1的卷积操作,以减少CNN特征图的通道数。再将两种特征图合并,并依次经过ReLU函数、1×1卷积、批量归一化、sigmoid函数和上采样,得到加权值<i>α</i>。最后,高层次特征与加权后的低层次特征<i>y</i><sup><i>l</i></sup>相加并进行逐步的上采样过程。在加入AG模块后,能使模型进一步减少对非腺体区域的定位,从而更加专注于腺体结构的学习。</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag"><b>2 实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="83" name="83"><b>2.1 数据和预处理</b></h4>
                <div class="p1">
                    <p id="84">在实验中使用了两个数据集:(1)Warwick-Qu数据集,是由MICCAI 2015的腺体分割(GlaS)挑战赛提供的;(2)UCSB breast数据集,是由耶鲁大学的David Rimm实验室提供的公共数据集。在(1)中共有165张775×522像素的图像,是从16个苏木精-伊红(Hematoxylin  Eosin,H&amp;E)染色的组织切片中获得的,其中包括85张训练图像(37张良性和48张恶性)和80张测试图像(37张良性和43张恶性),测试图像被分为测试集A和测试集B,并且所有图像都有对应的精确分割标签。在(2)中共有58张896×768像素的图像,是David Rimm实验室从10个H&amp;E染色的乳腺癌活组织切片中获得的,包括 26个癌细胞图像和32个正常病例图像,所有图像都以24位的RGB格式存储。由于这两个都是小型数据集,分别只有165张和58张图像,为了使模型学习到平移可变性和较好的鲁棒性,采用随机旋转、平移和随机翻转等变换进行数据增广,最终获得了16 000张训练图像。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>2.2 训练细节</b></h4>
                <div class="p1">
                    <p id="86">模型框架实现是基于开源软件库TensorFlow1.4.0版本<citation id="149" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>,并在NVIDIA GeForce Titan X GPU上采用端到端的训练方式进行训练。网络从原始图像中随机裁剪一个464×464的区域作为输入,最后输出腺体的轮廓预测掩码。训练一共分为75个阶段,每批次20张图像,初始学习率为0.001(在最后的分类层学习率为0.01),每迭代1 000次学习率乘以0.1。采用0.9的动量和0.000 5的权重衰减。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.3 性能评估和分析</b></h4>
                <div class="p1">
                    <p id="88">对于腺体分割算法的评估标准包括<i>F</i><sub>1</sub>评分、对象级Dice系数和Hausdorff距离,分别用于评估单个腺体检测的准确性、腺体和分割掩码的基于体积的准确性,以及腺体和分割掩码的基于边界的相似性。</p>
                </div>
                <div class="p1">
                    <p id="89"><i>F</i><sub>1</sub>评分的定义为</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub><mtext>s</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mo>=</mo><mfrac><mrow><mn>2</mn><mo>⋅</mo><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo>⋅</mo><mtext>R</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo>+</mo><mtext>R</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></mfrac></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="91"><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Ρ</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo>=</mo><mfrac><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext><mo>+</mo><mtext>F</mtext><mtext>Ρ</mtext></mrow></mfrac><mo>,</mo><mtext>R</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mo>=</mo><mfrac><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow><mrow><mtext>Τ</mtext><mtext>Ρ</mtext><mo>+</mo><mtext>F</mtext><mtext>Ν</mtext></mrow></mfrac></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="92">其中,TP代表本来就是腺体并检测为腺体;FP代表本来不是腺体但检测为腺体;FN代表本来是腺体但检测为非腺体;Precision代表准确率;Recall代表召回率,又称“查全率”。</p>
                </div>
                <div class="p1">
                    <p id="93">对象级Dice系数是一种集合相似度度量函数,通常用于计算<i>X</i>,<i>Y</i>样本的相似度</p>
                </div>
                <div class="area_img" id="151">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201911005_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="95">Hausdorff距离是用于度量空间中<i>X</i>,<i>Y</i>子集之间的距离</p>
                </div>
                <div class="p1">
                    <p id="96"><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>Η</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>max</mi></mrow><mrow><mo>{</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>sup</mi></mrow></mstyle><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mspace width="0.25em" /><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>inf</mi></mrow></mstyle><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mo>,</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>sup</mi></mrow></mstyle><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mspace width="0.25em" /><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>inf</mi></mrow></mstyle><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mspace width="0.25em" /><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>}</mo></mrow></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="97">为了验证各个模块对整体模型的影响,本文采用Warwick-Qu数据集的训练集训练模型,并在该数据集的测试集上进行测试,最后得到各模块对模型性能的影响,如表1所示。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表</b>1 <b>模块性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Comparison of model performance</p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td><br />方法</td><td><i>F</i><sub>1</sub>score</td></tr><tr><td><br />U-Net</td><td>0.788</td></tr><tr><td><br />U-Net ＼dilated CNN</td><td>0.836</td></tr><tr><td><br />U-Net ＼attention</td><td>0.832</td></tr><tr><td><br />U-Net ＼attention ＼dilated CNN</td><td>0.871</td></tr><tr><td><br />U-Net ＼ASPP</td><td>0.847</td></tr><tr><td><br />U-Net ＼ASPP ＼dilated CNN</td><td>0.885</td></tr><tr><td><br />U-Net ＼ASPP ＼attention ＼dilated CNN</td><td>0.897</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="99">由表1可以看出,通过用空洞卷积替换原U-net模型中的普通卷积,使得模型分割性能有所提升,相比原模型提高4.8%。当增加注意力模块时,性能相比原模型也有提升。加入ASPP模块后,模型的性能达到了最高89.7%,相比之前有较大进步。</p>
                </div>
                <div class="p1">
                    <p id="100">同时,将改进后的模型与几个优秀的分割模型进行对比分析,包括SegNet,FCN-8和Deeplab-v3等,并给出了评估的结果,如表2和图4所示。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911005_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 分割模型结果对比" src="Detail/GetImg?filename=images/DZKK201911005_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 分割模型结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911005_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Comparison of model results</p>

                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表</b>2 Warwick-Qu<b>数据集验证结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2. Warwick-Qu dataset testing results</p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td rowspan="2"><br /></td><td colspan="4"><br /><i>F</i><sub>1</sub>score</td><td colspan="4">Object Dice</td><td colspan="4">Object Hausdorff</td><td rowspan="2">Sum</td></tr><tr><td><br />A</td><td>Rank</td><td>B</td><td>Rank</td><td>A</td><td>Rank</td><td>B</td><td>Rank</td><td>A</td><td>Rank</td><td>B</td><td>Rank</td></tr><tr><td>Proposed<br />DeepLab-v3<br />FCN-8<br />SegNet<br />U-Net<br />Freidburg1 <br />CUMedVision1 <br />Freidburg2 <br />CVML<br />ExB3<br />ExB2<br />ExB1<br />LIB<br />vision4GlaS</td><td>0.897<br />0.858<br />0.783<br />0.862<br />0.788<br />0.834<br />0.868<br />0.870<br />0.652<br />0.896<br />0.892<br />0.891<br />0.777<br />0.635</td><td>1<br />8<br />11<br />7<br />10<br />9<br />6<br />5<br />13<br />2<br />3<br />4<br />12<br />14</td><td>0.832<br />0.753<br />0.692<br />0.764<br />0.697<br />0.605<br />0.769<br />0.695<br />0.541<br />0.719<br />0.686<br />0.703<br />0.306<br />0.527</td><td>1<br />4<br />9<br />3<br />7<br />11<br />2<br />8<br />12<br />5<br />10<br />6<br />14<br />13</td><td>0.885<br />0.864<br />0.795<br />0.859<br />0.781<br />0.875<br />0.867<br />0.876<br />0.644<br />0.886<br />0.884<br />0.882<br />0.781<br />0.737</td><td>2<br />8<br />10<br />9<br />11<br />6<br />7<br />5<br />14<br />1<br />3<br />4<br />12<br />13</td><td>0.825<br />0.807<br />0.767<br />0.804<br />0.781<br />0.783<br />0.800<br />0.786<br />0.654<br />0.765<br />0.754<br />0.786<br />0.617<br />0.610</td><td>1<br />2<br />9<br />3<br />8<br />7<br />4<br />5<br />12<br />10<br />11<br />5<br />13<br />14</td><td>54.20<br />62.62<br />105.04<br />65.72<br />102.47<br />57.19<br />74.60<br />57.09<br />155.43<br />57.36<br />54.79<br />57.41<br />112.71<br />107.49</td><td>1<br />7<br />11<br />8<br />10<br />4<br />9<br />3<br />14<br />5<br />2<br />6<br />13<br />12</td><td>119.93<br />118.51<br />147.28<br />124.97<br />143.75<br />146.61<br />153.65<br />148.47<br />176.24<br />159.87<br />187.44<br />145.58<br />190.45<br />210.10</td><td>2<br />1<br />7<br />3<br />4<br />6<br />9<br />8<br />11<br />10<br />12<br />5<br />13<br />14</td><td>8<br />30<br />57<br />33<br />50<br />43<br />37<br />34<br />76<br />33<br />41<br />30<br />77<br />80</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="103">从表2可以看出,仅从<i>F</i><sub>1</sub>score来看,FCN-8模型和原U-Net模型的性能相对较差,原因在于这两个模型的层数较少,对特征的提取不够好;SegNet模型和Deeplab-v3模型的层数较深,性能也相对较好,但在Warwick-Qu数据集上,仍然不如改进后的模型。</p>
                </div>
                <div class="p1">
                    <p id="104">从图4中可看出,FCN-8模型的分割结果黏连较多,且部分腺体分割不完整;传统U-Net模型的分割结果出现误分割现象,将原本不是腺体的地方误分成腺体;SegNet模型和Deeplab-v3模型的分割结果也出现少许黏连,且细节部分处理较差;而本模型的腺体分割结果具有更加准确的边界并且精度更高。</p>
                </div>
                <div class="p1">
                    <p id="105">最后,为了体现本模型的泛化能力,在UCSB breast数据集上进行了验证,并表现出比其他模型更优秀的性能,结果对比如表3所示。</p>
                </div>
                <div class="area_img" id="106">
                                            <p class="img_tit">
                                                <b>表</b>3 UCSB breast<b>数据集验证结果</b>
                                                    <br />
                                                Table 3. UCSB breast dataset testing results
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911005_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/DZKK201911005_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911005_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 UCSB breast数据集验证结果" src="Detail/GetImg?filename=images/DZKK201911005_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="107" name="107" class="anchor-tag"><b>3 结束语</b></h3>
                <div class="p1">
                    <p id="108">本文提出了一种用于分割腺体细胞的改进U-Net模型,该模型在腺体细胞分割中有较好表现。本模型的成功得益于引入了空洞空间金字塔池和注意力模块,较好的解决了分割任务中腺体大小不一致和下采样导致的腺体分辨率下降的问题。并且本模型较为通用,通过训练和微调很容易运用到其他医学图像的分割任务。</p>
                </div>
                <div class="p1">
                    <p id="109">但本文所提方法对病变严重的腺体细胞分割不理想,因此接下来可做出如下改进:(1)改善算法。改进图像进入模型前的预处理,并使用多模型融合,是的算法更加可靠和精确;(2)可以着手拓展算法的应用范围<citation id="150" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>,除了应用于细胞图像,还可以用于X光医学图像、CT图像等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601546817&amp;v=MzA1NTQwK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUpWOFJhUkU9TmlmT2ZiSzdIdEROcVk5RVllOEpCSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Winawer S J,Fletcher R H,Miller L,et al.Colorectal cancer screening:clinical guidelines and rationale[J].Gastroenterology,1997,113(4):594-642.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ELEW201115041&amp;v=MTI0NDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkza1ZyekxJQ0hPZWJHNEg5RE5xbzlCWllRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 陈爱斌,江霞.细胞分割算法研究方法综述[J].电子世界,2011(15):76-79.Chen Aibin,Jiang Xia.Review of research methods for cell segmentation algorithms[J].Electronic World,2011(15):76-79.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pathological prognostic factors in breast cancer. I. The value of histological grade in breast cancer: experience from a large study with long-term follow-up">

                                <b>[3]</b> Elston C W,Ellis I O.Pathological prognostic factors in breast cancer.I.The value of histological grade in breast cancer:experience from a large study with long-term follow-up[J].Histopathology,2010,19(5):403-410.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histologic grading of prostate cancer: a perspective">

                                <b>[4]</b> Gleason D F.Histologic grading of prostate cancer:A perspective[J].Human Pathology,1992,23(3):273-279.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Colorectal carcinoma: Pathologic aspects">

                                <b>[5]</b> Fleming M,Ravula S,Tatishchev S F,et al.Colorectal carcinoma:Pathologic aspects[J].Journal of Gastrointestinal Oncology,2012,3(3):153-153.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787040111262001&amp;v=MzE1OTE5ZmJ2bktyaWZaZVp2RnluaVU3ckpKVjBYWEZxekdiTzhIdEROcm8xRFp1c1BEUk04enhVU21EZDlTSDduM3hF&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 章毓晋.图像处理和分析基础[M].北京:高等教育出版社,2002.Zhang Yujin.The basis of image processing and analysis [M].Beijing:Higher Education Press,2002.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histopathological Image Analysis: A Review">

                                <b>[7]</b> Gurcan M N,Boucheron L E,Can A,et al.Histopathological image analysis:a review[J].IEEE Reviews in Biomedical Engineering,2009(2):147-147.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Color graphs for automated cancer diagnosis and grading">

                                <b>[8]</b> Altunbay D,Cigir C,Sokmensuer C,et al.Color graphs for automated cancer diagnosis and grading[J].IEEE Transactions on Biomedical Engineering,2010,57(3):665-665.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300711399&amp;v=MjI2NTBLN0h0RE5ySTlGWStvT0QzVXdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKVjhSYVJFPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Gunduz Demir C,Kandemir M,Tosun A B,et al.Automatic segmentation of colon glands using object-graphs[J].Medical Image Analysis,2010,14(1):1-12.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel polar space random field model for the detection of glandular structures">

                                <b>[10]</b> Fu H,Qiu G,Shu J,et al.A novel polar space random field model for the detection of glandular structures[J].IEEE Transactions on Medical Imaging,2014,33(3):764-764.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Stochastic Polygons Model for Glandular Structures in Colon Histology Images">

                                <b>[11]</b> Khasawneh S,Al Wahadni A,Lloyd C H.A stochastic polygons model for glandular structures in colon histology images[J].IEEE Transactions on Medical Imaging,2015,34(11):2366-2378.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[12]</b> Ronneberger O,Fischer P,Brox T.U-Net:convolutional networks for biomedical image segmentation[M].Berlin:Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015,Springer International Publishing,2015.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFA277EA38DE2BE696CCDA87165D73997&amp;v=MjU5NjM2RGc1TXlSOFZtVXdKT1hmbHJSUXdEYldYVExPWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXQ5aHdibTZ3NnM9TmlmT2ZjWEpITmJMMnY1R2JKOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Chen H,Qi X,Yu L,et al.DCAN:Deep contour-aware networks for object instance segmentation from histology images[J].Medical Image Analysis,2017,36(9):135-146.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gland instance segmentation by deep multichannel side supervision">

                                <b>[14]</b> Xu Y,Li Y,Liu M,et al.Gland instance segmentation by deep multichannel side supervision[C].Athens:International Conference on Medical Image Computing and Computer-Assisted Intervention,Springer,Cham,2016.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gland instance segmentation using deep multichannel neural networks">

                                <b>[15]</b> Xu Y,Li Y,Wang Y,et al.Gland instance segmentation using deep multichannel neural networks[J].IEEE Transactions on Biomedical Engineering,2017,64(12):2901-2912.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MIMONet:gland segmentation using multi-input-multi-output convolutional neural network">

                                <b>[16]</b> Raza S E A,Cheung L,Epstein D,et al.MIMONet:gland segmentation using multi-input-multi-output convolutional neural network[C].Edinburgh:Annual Conference on Medical Image Understanding and Analysis,Springer,Cham,2017.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[17]</b> Chen L C,Papandreou G,Kokkinos I,et al.Deeplab:Semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected crfs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):834-848.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A biosegmentation benchmark for evaluation of bioimage analysis methods">

                                <b>[18]</b> Drelie Gelasca E,Obara B,Fedorov D,et al.A biosegmentation benchmark for evaluation of bioimage analysis methods[J].BMC Bioinformatics,2009,10(1):368-368.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gland segmentation in colon histology images:The glas challenge contest">

                                <b>[19]</b> Sirinukunwattana K,Pluim J P,Chen H,et al.Gland segmentation in colon histology images:The glas challenge contest[J].Medical Image Analysis,2017,35(8):489-502.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Learning,&amp;quot;">

                                <b>[20]</b> Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-436.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dilated residual networks">

                                <b>[21]</b> Yu F,Koltun V,Funkhouser T A.Dilated residual networks[C].Hawaii:Conference on Computer Vision and Pattern Recognition,2017.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding convolution for semantic segmentation">

                                <b>[22]</b> Wang P,Chen P,Yuan Y,et al.Understanding convolution for semantic segmentation[C].California:IEEE Winter Conference on Applications of Computer Vision (WACV),2018.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Squeeze-and-excitation networks">

                                <b>[23]</b> Hu J,Shen L,Sun G.Squeeze-and-excitation networks[J].IEEE Transaction on Network,2017,28(4):523-539.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic image segmentation via deep parsing network">

                                <b>[24]</b> Liu Z,Li X,Luo P,et al.Semantic image segmentation via deep parsing network[C].Santiago:Proceedings of the IEEE International Conference on Computer Vision,2015.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention U-Net:learning where to look for the pancreas">

                                <b>[25]</b> Oktay O,Schlemper J,Folgoc L L,et al.Attention U-Net:learning where to look for the pancreas[J].IEEE Transaction on Network,2018,29(2):117-129.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A system for large-scale machine learning">

                                <b>[26]</b> Abadi,Martín,Barham P,et al.TensorFlow:A system for large-scale machine learning[J].IEEE Transaction on Network,2016,27(9):1024-1037.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201607025&amp;v=MzE5NDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkza1ZyekxJVGZBWmJHNEg5Zk1xSTlIWVlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 何菁,陈胜.一种全新的两步自动化医学图像分割方案[J].电子科技,2016,29(7):85-87.He Jing,Chen Sheng.A new two-step automated medical image segmentation scheme[J].Electronic Science and Technology,2016,29(7):85-87.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201911005" />
        <input id="dpi" type="hidden" value="299" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201911005&amp;v=MDQwODBJVGZBWmJHNEg5ak5ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkza1Zyekk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

