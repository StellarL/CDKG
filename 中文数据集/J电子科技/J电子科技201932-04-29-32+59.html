

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139873027763750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201904008%26RESULT%3d1%26SIGN%3dtW%252fSDecD%252baB0iiM%252f8z4tDHDY5ik%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201904008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201904008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201904008&amp;v=MjIzNDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0ZpRGdXcnpOSVRmQVpiRzRIOWpNcTQ5RmJJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;1 相关研究工作&lt;/b&gt; "><b>1 相关研究工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;2&lt;/b&gt; Bi-LSTM-CRF&lt;b&gt;模型&lt;/b&gt; "><b>2</b> Bi-LSTM-CRF<b>模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="&lt;b&gt;2.1&lt;/b&gt; Bi-LSTM&lt;b&gt;神经网络&lt;/b&gt;"><b>2.1</b> Bi-LSTM<b>神经网络</b></a></li>
                                                <li><a href="#47" data-title="&lt;b&gt;2.2&lt;/b&gt; CRF&lt;b&gt;模型&lt;/b&gt;"><b>2.2</b> CRF<b>模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="&lt;b&gt;3 异构数据联合训练方法&lt;/b&gt; "><b>3 异构数据联合训练方法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;4 模型训练&lt;/b&gt; "><b>4 模型训练</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="&lt;b&gt;4.1 文本向量化&lt;/b&gt;"><b>4.1 文本向量化</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;4.2&lt;/b&gt; Dropout&lt;b&gt;方法&lt;/b&gt;"><b>4.2</b> Dropout<b>方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="&lt;b&gt;5 实验&lt;/b&gt; "><b>5 实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="&lt;b&gt;5.1 数据预处理&lt;/b&gt;"><b>5.1 数据预处理</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;5.2 实验结果与分析&lt;/b&gt;"><b>5.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;6 结论与展望&lt;/b&gt; "><b>6 结论与展望</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="图1 Bi-LSTM-CRF模型结构">图1 Bi-LSTM-CRF模型结构</a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;对不同语料库中添加标记&lt;/b&gt;"><b>表</b>1 <b>对不同语料库中添加标记</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同模型对比&lt;/b&gt;"><b>表</b>2 <b>不同模型对比</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;单一语料与多语料训练的模型对比&lt;/b&gt;"><b>表</b>3 <b>单一语料与多语料训练的模型对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 郑捷.NLP汉语自然语言处理原理与实践[M].北京:电子工业出版社, 2017. Zheng Jie.Principles and practice of NLP Chinese natural language processing[M].Beijing: Publishing House of Electronics Industry, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121307652000&amp;v=MDI0NTd4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlplWnZGeW5qVXJmTktWMFJYRnF6R2JLNkg5TE1xSWxBWnVzUERCTTh6&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         郑捷.NLP汉语自然语言处理原理与实践[M].北京:电子工业出版社, 2017. Zheng Jie.Principles and practice of NLP Chinese natural language processing[M].Beijing: Publishing House of Electronics Industry, 2017.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     Hinton G E, Salakhutdinov R R.Reducing the dimensionality of data with neural networks[J]. Science, 2006, 313 (5786) :504-507.</a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Collobert R, Wesotn J, Bottou L.Natural language processing (almost) from scratch[J].The Journal of Machine Learning Research, 2011 (12) :2493-2537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Natural language processing (almost) from scratch">
                                        <b>[3]</b>
                                         Collobert R, Wesotn J, Bottou L.Natural language processing (almost) from scratch[J].The Journal of Machine Learning Research, 2011 (12) :2493-2537.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Zheng X, Chen H, Xu T.Deep learning for Chinese word segmentation and POS tagging[C].Seattle:Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for Chinese word segmentation and POS tagging">
                                        <b>[4]</b>
                                         Zheng X, Chen H, Xu T.Deep learning for Chinese word segmentation and POS tagging[C].Seattle:Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Chen X, Qiu X, Zhu C.Gated recursive neural network for Chinese word segmentation[C]. Beijing:Proceedings of the 53&lt;sup&gt;rd&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics and the 7&lt;sup&gt;th&lt;/sup&gt; International Joint Conference on Natural Language Processing, Association for Computational Linguistics, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated recursive neural network for Chinese word segmentation">
                                        <b>[5]</b>
                                         Chen X, Qiu X, Zhu C.Gated recursive neural network for Chinese word segmentation[C]. Beijing:Proceedings of the 53&lt;sup&gt;rd&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics and the 7&lt;sup&gt;th&lt;/sup&gt; International Joint Conference on Natural Language Processing, Association for Computational Linguistics, 2015.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Chen X, Qiu X, Zhu C.Long short-term memory neural networks for Chinese word segmentation[C].Lisbon:Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long Short-term Memory Neural Networks for Chinese Word Segmentation">
                                        <b>[6]</b>
                                         Chen X, Qiu X, Zhu C.Long short-term memory neural networks for Chinese word segmentation[C].Lisbon:Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2015.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Yao Yushi, Huang Zheng. Bi-directional LSTM recurrent neural netword for Chinese word segmentation[C].Kyoto:International Conference on Neural Information Processing, Springer, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation">
                                        <b>[7]</b>
                                         Yao Yushi, Huang Zheng. Bi-directional LSTM recurrent neural netword for Chinese word segmentation[C].Kyoto:International Conference on Neural Information Processing, Springer, 2016.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Jiang Wenbin, Huang Liang, Liu Qun.Automatic adaptation of annotation standards:Chinese word segmentation and POS tagging[C].Morristown: In the Joint Conference of the 47&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the ACL and the 4&lt;sup&gt;th&lt;/sup&gt; International Joint Conference, Association for Computational Linguistics, 2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic adaptation of annotation standards:Chinese word segmentation and POS tagging">
                                        <b>[8]</b>
                                         Jiang Wenbin, Huang Liang, Liu Qun.Automatic adaptation of annotation standards:Chinese word segmentation and POS tagging[C].Morristown: In the Joint Conference of the 47&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the ACL and the 4&lt;sup&gt;th&lt;/sup&gt; International Joint Conference, Association for Computational Linguistics, 2009.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Sun Weiwei, Wan Xiaojun.Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations[C].Jeju Island:Proceedings of the 50&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reducing Approximation and Estimation Errors for ChineseLexical Processing with Heterogeneous Annotations">
                                        <b>[9]</b>
                                         Sun Weiwei, Wan Xiaojun.Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations[C].Jeju Island:Proceedings of the 50&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2012.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Qiu Xipeng, Zhao Jiayi, Huang Xuanjing.Joint Chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning[C].Seattle:Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Chinese Word Segmentation and POS Tagging onHeterogeneous Annotated Corpora with Multiple Task Learning">
                                        <b>[10]</b>
                                         Qiu Xipeng, Zhao Jiayi, Huang Xuanjing.Joint Chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning[C].Seattle:Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Li Zhenghua, Chao Jiayuan, Zhang Min, et al. Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study[C]. Beijing:In Proceedings of the 53&lt;sup&gt;rd&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics and the 7&lt;sup&gt;th&lt;/sup&gt; International Joint Conference on Natural Language Processing. Association for Computational Linguistics, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study">
                                        <b>[11]</b>
                                         Li Zhenghua, Chao Jiayuan, Zhang Min, et al. Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study[C]. Beijing:In Proceedings of the 53&lt;sup&gt;rd&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics and the 7&lt;sup&gt;th&lt;/sup&gt; International Joint Conference on Natural Language Processing. Association for Computational Linguistics, 2015.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Chen Xinchi, Shi Zhan, Qiu Xipeng, et al. Adversarial multi-criteria learning for Chinese word segmentation[C].Vancouver:Proceedings of the 55&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 201l." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial multi-criteria learning for Chinese word segmentation">
                                        <b>[12]</b>
                                         Chen Xinchi, Shi Zhan, Qiu Xipeng, et al. Adversarial multi-criteria learning for Chinese word segmentation[C].Vancouver:Proceedings of the 55&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 201l.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Melvin Johnson, Mike Schuster, Quoc V Le. Google’s multilingual neural machine translation system-enabling zero-shot translation[J]. Computational Linguistics, 2017 (5) :339-351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Google&amp;#39;&amp;#39;s multilingual neural machine translation system-enabling zero-shot translation">
                                        <b>[13]</b>
                                         Melvin Johnson, Mike Schuster, Quoc V Le. Google’s multilingual neural machine translation system-enabling zero-shot translation[J]. Computational Linguistics, 2017 (5) :339-351.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 王贵新, 郑孝宗.基于Word2vec的短信向量化算法[J].电子科技2016, 29 (4) :49-52. Wang Guixin, Zheng Xiaozong.An algorithm for vectoring SMS based on Word2vec[J]. Electronic Science and Technology, 2016, 29 (4) :49-52." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201604013&amp;v=MjgzOTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0ZpRGdXcnpOSVRmQVpiRzRIOWZNcTQ5RVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         王贵新, 郑孝宗.基于Word2vec的短信向量化算法[J].电子科技2016, 29 (4) :49-52. Wang Guixin, Zheng Xiaozong.An algorithm for vectoring SMS based on Word2vec[J]. Electronic Science and Technology, 2016, 29 (4) :49-52.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Zheng Xiaoqing, Chen Hanyang, Xu Tianyu.Deep learning for Chinese word segmentation and POS tagging[C].Seattle:Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for Chinese word segmentation and POS tagging">
                                        <b>[15]</b>
                                         Zheng Xiaoqing, Chen Hanyang, Xu Tianyu.Deep learning for Chinese word segmentation and POS tagging[C].Seattle:Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Cai Deng, Zhao Hai.Neural word segmentation learning for Chinese[C].Berlin:Proceedings of the 54&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural word segmentation learning for Chinese">
                                        <b>[16]</b>
                                         Cai Deng, Zhao Hai.Neural word segmentation learning for Chinese[C].Berlin:Proceedings of the 54&lt;sup&gt;th&lt;/sup&gt; Annual Meeting of the Association for Computational Linguistics, 2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(04),29-32+59 DOI:10.16180/j.cnki.issn1007-7820.2019.04.007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于异构数据联合训练的中文分词法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E7%8C%9B&amp;code=41466849&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜猛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AD%90%E7%89%9B&amp;code=06938742&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王子牛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E5%BB%BA%E7%93%B4&amp;code=06943681&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高建瓴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0159277&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州大学大数据与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%BD%91%E7%BB%9C%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%8C%96%E7%AE%A1%E7%90%86%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州大学网络与信息化管理中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>中文分词技术作为中文信息处理中的关键基础技术之一, 基于深度学习模型的中文分词法受到广泛关注。然而, 深度学习模型需要大规模数据训练才能获得良好的性能, 而当前中文分词语料数据相对缺乏且标准不一。文中提出了一种简单有效的异构数据处理方法, 对不同语料数据加上两个人工设定的标识符, 使用处理过的数据应用于双向长短期记忆网络结合条件随机场 (Bi-LSTM-CRF) 的中文分词模型的联合训练。实验结果表明, 基于异构数据联合训练的Bi-LSTM-CRF模型比单一数据训练的模型具有更好的分词性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中文分词;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bi-LSTM-CRF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bi-LSTM-CRF;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异构数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%94%E5%90%88%E8%AE%AD%E7%BB%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">联合训练;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E6%96%99%E5%BA%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语料库;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    姜猛 (1994-) , 男, 硕士研究生。研究方向:机器学习、数据挖掘。;
                                </span>
                                <span>
                                    王子牛 (1961-) , 男, 副教授。研究方向:信息与信号处理、数据挖掘、计算机仿真技术。;
                                </span>
                                <span>
                                    高建瓴 (1969-) , 女, 副教授。研究方向:云计算、数据库应用。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>贵州省科学技术基金 (黔科合J字[2015]2045);</span>
                                <span>贵州大学研究生创新基金 (研理工2017016);</span>
                    </p>
            </div>
                    <h1><b>Chinese Word Segmentation Based on Joint Training of Heterogeneous Data</b></h1>
                    <h2>
                    <span>JIANG Meng</span>
                    <span>WANG Ziniu</span>
                    <span>GAO Jianling</span>
            </h2>
                    <h2>
                    <span>School of Big Data & Information Engineering, Guizhou University</span>
                    <span>Network and Information Management Center, Guizhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Chinese word segmentation technology is one of the key basic technologies in Chinese information processing. The Chinese word segmentation method based on deep learning model is widely concerned. However, the deep learning model requires large-scale data training to obtain good performance, but the current Chinese sub-word data is relatively lacking and the standards are not the same. This paper proposes a simple and effective method of heterogeneous data processing. Firstly, two artificially-set identifiers are added to different corpus data, and then the processed data is applied to the joint training of Bi-LSTM-CRF Chinese word segmentation model. Experimental results show that the Bi-LSTM-CRF model based on heterogeneous data joint training has better segmentation performance than the single data training model.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Chinese%20word%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Chinese word segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bi-LSTM-CRF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bi-LSTM-CRF;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=heterogeneous%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">heterogeneous data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=joint%20training&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">joint training;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=corpus&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">corpus;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-18</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Guizhou Science and Technology Fund (Guizhou Science and Technology Agency J [2015]2045);</span>
                                <span>Guizhou University Graduate Innovation Fund (Graduate Science and Technology 2017016);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="35">汉语词汇是语言中能够独立运用的最小语言单位, 是语言中的原子结构<citation id="83" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。因此, 对中文文本进行中文分词 (Chinese Word Segmentation, CWS) 就显得至关重要。中文分词是指将组成句子的汉字序列用分隔符分成单独的词语序列的过程。中文分词作为中文信息处理的基础任务, 已经被广泛应用于中文信息处理中的多个领域, 例如, 信息抽取、自动摘要、文本校对、语义理解、汉字简繁体转换、机器翻译等领域。</p>
                </div>
                <div class="p1">
                    <p id="36">近年来, 深层神经网络模型在中文分词技术领域表现出了良好的效果。深层神经网络模型需要从大量的训练数据中提出抽象特征, 而对于中文分词领域来说, 对应的训练数据就是语料库。语料库的建立是一项繁重而枯燥的工作, 不同的研究机构采用的分词标准参差不齐, 而且受到人为主观因素的影响, 人工分词的标准并不统一。因此, 利用不同标准的语料库训练出来的模型分词效果也不一样, 例如, 通过PKU语料训练出来的模型对“李铁映”的分词结果为“李 铁映”, 切分为“姓+名”, 而通过MSR语料训练出来的模型则将“李铁映”看作一个整体, 不做切分。</p>
                </div>
                <div class="p1">
                    <p id="37">为了能有效利用现有的多种不同标准的语料库来联合训练同一个分词模型, 实现分词模型的性能最大化。本文试图对不同语料库数据加上特定的标识符, 解决多种不同标准的语料数据对同一个模型进行联合训练的问题。经过多组实验比对, 通过该方法训练的分词模型比单一语料数据训练的模型分词性能更有优势。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag"><b>1 相关研究工作</b></h3>
                <div class="p1">
                    <p id="39">自2006年Hinton等人<citation id="84" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出深度学习以来, 基于深度神经网络的模型被广泛的应用于自然语言处理领域, 如词性标注、命名实体识别以及情感分类等任务。Collobert等人<citation id="85" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>设计了SENNA (Semantic Extraction using a Neural Network Architecture) 系统, 利用神经网络解决了英文序列标注问题;Zheng等人<citation id="86" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>利用 SENNA系统实现了中文分词和词性标注, 并提出一个感知机算法, 加速了整个训练过程;Chen等人<citation id="88" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>将GRNN (Gated Recursive Neural Network) 运用于中文分词任务, 提出基于GRNN模型改进的具有上下文记忆单元的LSTM (Long Short-Term Memory模型来进行中文分词, 取得了与传统统计模型分词方法相当的成绩。但LSTM模型只能记住过去的上下文信息, 无法对未来的上下文信息进行处理, 因此Yao等人<citation id="87" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了双向LSTM模型来进行中文分词, 进一步的提升分词的准确率。</p>
                </div>
                <div class="p1">
                    <p id="40">在深层神经网络模型训练中, 由于训练任务和异构标注数据之间存在分歧, 多任务训练相对困难。近年来, 有学者开始研究关于中文分词和词性标注的联合学习的方法。例如, Jiang等人<citation id="89" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>将两个分类器叠加在一起, 后者使用前者的预测作为附加特征。Sun等人<citation id="90" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了一种基于结构的堆积模型, 其中一个标记器被设计为改进另一个标记器的预测。这些早期模型中缺乏统一的损失函数, 并且容易受到错误传播的影响。Qiu等人<citation id="91" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了学习异构语料库之间的映射函数。Li等人<citation id="92" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出并利用耦合序列标注模型, 可以同时直观地解析和预测两个异质标注。这些研究主要集中在利用不同标记集之间的关系, 但不是共享特征。Chen等人<citation id="93" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>设计了一个复杂的框架, 利用生成对抗网络 (GAN) 共享层来提取标准不变特征, 以及与数据相关的专用层来检测标准相关特征。但这项研究工作与先前的单一标准学习的模型相比, 并没有表现出很大的优势。</p>
                </div>
                <div class="p1">
                    <p id="41">本文的解决方案受到多语种机器翻译系统的启发, Johnson等人<citation id="94" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一个非常简单的解决方案, 不需要添加任何复杂的体系结构或专用层, 只是添加了一个对应于平行语料库的人工标记, 并共同训练它们。根据Johnson等人的这种思路, 将这种方法运用到中文分词中的模型训练上, 实现了不同语料数据间的联合训练。实验表明, 通过多种语料联合训练的分词模型性能有了明显的提升。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>2</b> Bi-LSTM-CRF<b>模型</b></h3>
                <div class="p1">
                    <p id="43">基于深层神经网络的中文分词模型的一般结构由3个部分组成:字符嵌入层、特征层和标签推理层。特征层的作用是提取出特征, 可以是卷积神经网络 (Convolutional Neural Network, CNN) 或循环神经网络 (Recurrent Neural Network, RNN) 。在本文中, 使用的是基于双向长短期记忆网络结合条件随机场的神经网络模型, 特征层采用基于循环神经网络改进的双向长短期记忆网络, 标签推理层采用条件随机场, 其模型结构如图1所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201904008_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Bi-LSTM-CRF模型结构" src="Detail/GetImg?filename=images/DZKK201904008_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Bi-LSTM-CRF模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201904008_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure1. Bi-LSTM-CRF model structure</p>

                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>2.1</b> Bi-LSTM<b>神经网络</b></h4>
                <div class="p1">
                    <p id="46">深层神经网络模型通过Bi-LSTM层自动提取句子特征。Bi-LSTM神经网络的思路来自于双向RNN模型。它拥有两个不同方向的并行层, 前向层与后向层的运行方式和常规神经网络的运行方式相同。这两个层分别从句子的前端和末端开始运行, 并且在各个位置输出的隐藏状态进行按位置拼接, 得到完整的隐藏状态序列, 因此能存储来自两个方向的句子信息。这样Bi-LSTM既能保存前面的上下文信息, 也能同时考虑到未来的上下文信息, 从而使其在中文分词中拥有更好的表现。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47"><b>2.2</b> CRF<b>模型</b></h4>
                <div class="p1">
                    <p id="48">通过使用条件随机场模型可以对文本序列进行句子层面的序列标注, 标注方法可以使得每个字标注的状态不仅受到时间顺序上的从前向后的影响, 同时也会被未来的状态所影响。CRF层的参数是一个状态转移矩阵<b><i>A</i></b>。<i>A</i><sub><i>i</i>, <i>j</i></sub>表示时序上从第<i>i</i>个状态转移到第<i>j</i>个状态的概率, <i>P</i><sub><i>i</i>, <i>j</i></sub>表示输入观察序列中第<i>i</i>个词为第<i>j</i>个标注的概率。则观测序列<i>x</i>= (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>n</i></sub>) 对应的标注序列<i>y</i>= (<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>) 的预测输出为</p>
                </div>
                <div class="p1">
                    <p id="49">score<mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>A</mi></mstyle><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="51">由式 (1) 可以看出, 整个序列的打分等于各个位置的打分之和, 而每个位置的打分由两部分得到, 一部分是由LSTM输出的<i>P</i><sub><i>i</i></sub>决定, 另一部分则由CRF的转移矩阵<b><i>A</i></b>决定。利用softmax函数得到归一化后的概率</p>
                </div>
                <div class="p1">
                    <p id="52"><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><msup><mi>y</mi><mo>′</mo></msup></msub><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="54">模型在预测过程中使用动态规划中的Viterbi算法来求解最优标注序列</p>
                </div>
                <div class="p1">
                    <p id="55"><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><msup><mi>y</mi><mo>′</mo></msup></munder><mtext>s</mtext><mtext>c</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <h3 id="57" name="57" class="anchor-tag"><b>3 异构数据联合训练方法</b></h3>
                <div class="p1">
                    <p id="58">不同标注的语料库与机器翻译中的多国语言十分类似:表达类似的意思, 却采用了不同的方式。以前的多语种互译系统需要针对每个语种都需要设计“编码-解码”对。对于<i>n</i>种语言来讲, 就需要<i>n</i>× (<i>n</i>-1) 对“编码-解码”。类似地, 针对每个分词语料库设计网络层的话, 对<i>n</i>种分词标准, 就需要<i>n</i>个私有层。但是这种方法带来的结果就是造成系统臃肿不堪, 模型结构过度复杂。</p>
                </div>
                <div class="p1">
                    <p id="59">对于多语言翻译系统等密切相关的多任务学习, Johnson等人提出了一个简单实用的解决方案。该方案不需要修改网络架构, 只需要在输入句子的开头添加一个人工标记来指定所需的目标语言, 就可以把所有语种的平行语料混合在一起训练。</p>
                </div>
                <div class="p1">
                    <p id="60">受到这种方式的启发, 在中文分词技术中应用这种处理方法, 实现不同标准的语料库能够一起训练分词模型。根据Johnson等人的这种思路, 在需要分词的句子的开始和结尾添加两个人工标记来指定所需的目标标准。具体的做法是用一对闭合的标识符将每个句子包裹起来。例如, SIGHAN Bakeoff 2005中的句子处理过后如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="61">通过两个标识符可以指明句子是来自哪个数据集, 在模型训练过程中, 将两个标记看成是普通的字符, 不需要做其他处理。这对人工标记会提示神经网络模型这个句子属于哪种分词标准, 使其为每个字符生成的上下文信息都受到该分词标准的影响。在它们的长依赖关系中包含分词标准相关的信息, 从而影响每个角色的上下文表示, 最终生成与目标标准相匹配的分词决策。在进行测试时, 这些标记也用于指定所需的分词标准, 并且在计算模型性能时将它们排除在外。</p>
                </div>
                <div class="area_img" id="62">
                    <p class="img_tit"><b>表</b>1 <b>对不同语料库中添加标记</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Add tags to different corpora</p>
                    <p class="img_note"></p>
                    <table id="62" border="1"><tr><td><br />语料库</td><td>标识符</td></tr><tr><td><br />PKU</td><td>&lt;pku&gt;江 泽民 发表 新年 贺词&lt;/pku&gt;</td></tr><tr><td><br />MSR</td><td>&lt;msr&gt;穆守家 两间 低矮 的 小屋 里&lt;/msr&gt;</td></tr><tr><td><br />AS</td><td>&lt;as&gt;民族所 所长 庄英章 先生&lt;/as&gt;</td></tr><tr><td><br />CityU</td><td>&lt;cityu&gt;选举 出现 了 戏剧性 的 结果&lt;/cityu&gt;</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="63" name="63" class="anchor-tag"><b>4 模型训练</b></h3>
                <div class="p1">
                    <p id="64">为了将中文分词问题转化成序列标注问题, 就需要将分词中的每一个字进行标注。在利用深度学习分词研究工作中, 一个常用的标记集是四词位标注 (<i>B</i>, <i>M</i>, <i>E</i>, <i>S</i>) , <i>B</i>表示一个词的开始, <i>M</i>表示词的中间, <i>E</i>表示词的结尾, <i>S</i>表示单个字符形成一个词。本文实验也采用四词位的标注集对语料库文本的进行预处理。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>4.1 文本向量化</b></h4>
                <div class="p1">
                    <p id="66">输入分词模型的文本序列首先需要进行向量化处理。通过Word2vec方法<citation id="95" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 可以将训练语料中每一个字的转化成一个长度为<i>d</i>的空间向量。对于每一个输入分词模型的字, 其对应的向量由该字和该字上下文的字向量拼接而成。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>4.2</b> Dropout<b>方法</b></h4>
                <div class="p1">
                    <p id="68">在模型训练时采用dropout方法来防止神经网络过拟合。Dropout方法直接作用在神经网络结构上, 随机选择部分单元连同它们的输入输出连接, 都暂时从网络中删除它们。其中单元被选中暂时删除的概率可以在训练的时候人为设定。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag"><b>5 实验</b></h3>
                <div class="p1">
                    <p id="70">本文所提方法主要是为了解决多分词标准语料库联合训练的问题, 针对这一问题来设计实验。本文的深层神经网络模型实验基于Dynet开发实现。Dynet是一种用于深度学习的动态神经网络框架。此外, 实验还用Python实现了CRF层, 为了正确评价本文分词模型的性能, 实验采用 SIGHAN规定的综合指标值 (F) 来评价。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>5.1 数据预处理</b></h4>
                <div class="p1">
                    <p id="72">为了验证本文所提方法的有效性, 本实验对SIGHAN bakeoff 2005中的PKU、MSR、AS和CityU 4个流行的中文分词数据集进行了实验, 这些数据集是开源而且在分词研究中也是常用的。通过用一系列的英文字符或数字替换语料库中的唯一标记来预处理所有数据集。对于训练集和开发集的处理, 实验时根据文本中的标点符号将句子划分为更短的句子或分句, 以便得到更快的处理效果。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>5.2 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="75">实验设置了一个基线模型是分别在每个数据集上训练的Bi-LSTM-CRF, 然后用本文所提的异构数据联合训练方法来改进它, 并将实验数据与前人所做研究进行了对比。表2列出了最终的综合评价指标F值。</p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表</b>2 <b>不同模型对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2. Comparison of different models</p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td>模型</td><td>PKU</td><td>MSR</td><td>AS</td><td>CityU</td></tr><tr><td><br />Bakeoff-best</td><td>0.950</td><td>0.964</td><td>-</td><td>-</td></tr><tr><td><br /> (Zheng, 2013) </td><td>0.961</td><td>0.974</td><td>-</td><td>-</td></tr><tr><td><br /> (Chen, 2017) </td><td>0.943</td><td>0.960</td><td>-</td><td>0.948</td></tr><tr><td><br /> (Cai, 2017) </td><td>0.958</td><td>0.971</td><td>0.953</td><td>0.956</td></tr><tr><td><br />Bi-LSTM-CRF+多语料</td><td>0.960</td><td>0.974</td><td>0.954</td><td>0.962</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="77">由表2中可知, Bakeoff-best是SIGHAN bakeoff 2005测评中表现最好的结果, Zheng等人<citation id="96" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的基于深度学习的分词方法在MSR数据集上面的有良好性能表现, 而本文所提方法在MSR语料库上的表现并没有明显提升, Cai等人<citation id="97" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>使用神经网络模型, 并消除上下文窗口的方法, 使得中文分词的有性能比前人所做研究有了进一步的提升。实验结果表明, 本文方法在PKU和CityU语料库上的分词性能提升较大, 整体来说, 利用Bi-LSTM-CRF结合多语料训练的模型分词效果都有不俗表现。</p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表</b>3 <b>单一语料与多语料训练的模型对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 3. Comparison of single corpus and multi-corpus training models</p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td>模型</td><td>PKU</td><td>MSR</td><td>AS</td><td>CityU</td></tr><tr><td><br />Bi-LSTM-CRF</td><td>0.952</td><td>0.973</td><td>0.949</td><td>0.951</td></tr><tr><td><br />+多语料</td><td>0.953</td><td>0.973</td><td>0.950</td><td>0.953</td></tr><tr><td><br />+多语料+标签</td><td>0.960</td><td>0.974</td><td>0.954</td><td>0.962</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="79">从表3中可以看到基于异构数据联合训练的方法在模型训练中表现出了良好的性能优势, 对比单一数据集性能都有了至少1%的提升。其中, “Bi-LSTM-CRF”表示单一语料库训练的模型分词性能, “+多语料”表示模型中使用不加标签处理的多个语料库联合训练的分词性能, “+多语料+标签”表示模型多种语料加上标签并联合训练的分词性能。</p>
                </div>
                <div class="p1">
                    <p id="80">通过使用多个不同标准的语料库对模型训练, 模型可以充分学习各个数据集间的知识, 而且不会增加模型的复杂度。该方法是在各个数据集之间实现知识共享, 从而提高模型整体的分词性能。</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>6 结论与展望</b></h3>
                <div class="p1">
                    <p id="82">本文提出了一种多个语料联合训练中文分词模型的实用方法, 可以在不增加模型复杂度的情况下联合多个语料库训练单个模型。该方法可以在不同标注信息的语料之间传递知识, 全面提高模型在不同语料数据中的性能。同时, 在面对数据集较小的语料库时, 模型性能提升明显, 但面对数据集特别大的语料库时, 所带来性能提升很小甚至无法提升。实验结果表明, 该方法能够简单的解决异构数据联合训练的问题, 并且能够有效的提高分词模型的性能。该方法可以推广到其他序列标签任务, 例如词性标注和命名实体识别等方面。在下一步的研究工作中进行更多的实验, 进一步验证该方法在其他序列标注将领域的有效性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121307652000&amp;v=MTg1MDFiSzZIOUxNcUlsQVp1c1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5bmpVcmZOS1YwUlhGcXpH&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 郑捷.NLP汉语自然语言处理原理与实践[M].北京:电子工业出版社, 2017. Zheng Jie.Principles and practice of NLP Chinese natural language processing[M].Beijing: Publishing House of Electronics Industry, 2017.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 Hinton G E, Salakhutdinov R R.Reducing the dimensionality of data with neural networks[J]. Science, 2006, 313 (5786) :504-507.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Natural language processing (almost) from scratch">

                                <b>[3]</b> Collobert R, Wesotn J, Bottou L.Natural language processing (almost) from scratch[J].The Journal of Machine Learning Research, 2011 (12) :2493-2537.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for Chinese word segmentation and POS tagging">

                                <b>[4]</b> Zheng X, Chen H, Xu T.Deep learning for Chinese word segmentation and POS tagging[C].Seattle:Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated recursive neural network for Chinese word segmentation">

                                <b>[5]</b> Chen X, Qiu X, Zhu C.Gated recursive neural network for Chinese word segmentation[C]. Beijing:Proceedings of the 53<sup>rd</sup> Annual Meeting of the Association for Computational Linguistics and the 7<sup>th</sup> International Joint Conference on Natural Language Processing, Association for Computational Linguistics, 2015.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long Short-term Memory Neural Networks for Chinese Word Segmentation">

                                <b>[6]</b> Chen X, Qiu X, Zhu C.Long short-term memory neural networks for Chinese word segmentation[C].Lisbon:Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2015.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation">

                                <b>[7]</b> Yao Yushi, Huang Zheng. Bi-directional LSTM recurrent neural netword for Chinese word segmentation[C].Kyoto:International Conference on Neural Information Processing, Springer, 2016.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic adaptation of annotation standards:Chinese word segmentation and POS tagging">

                                <b>[8]</b> Jiang Wenbin, Huang Liang, Liu Qun.Automatic adaptation of annotation standards:Chinese word segmentation and POS tagging[C].Morristown: In the Joint Conference of the 47<sup>th</sup> Annual Meeting of the ACL and the 4<sup>th</sup> International Joint Conference, Association for Computational Linguistics, 2009.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reducing Approximation and Estimation Errors for ChineseLexical Processing with Heterogeneous Annotations">

                                <b>[9]</b> Sun Weiwei, Wan Xiaojun.Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations[C].Jeju Island:Proceedings of the 50<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2012.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Chinese Word Segmentation and POS Tagging onHeterogeneous Annotated Corpora with Multiple Task Learning">

                                <b>[10]</b> Qiu Xipeng, Zhao Jiayi, Huang Xuanjing.Joint Chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning[C].Seattle:Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study">

                                <b>[11]</b> Li Zhenghua, Chao Jiayuan, Zhang Min, et al. Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study[C]. Beijing:In Proceedings of the 53<sup>rd</sup> Annual Meeting of the Association for Computational Linguistics and the 7<sup>th</sup> International Joint Conference on Natural Language Processing. Association for Computational Linguistics, 2015.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial multi-criteria learning for Chinese word segmentation">

                                <b>[12]</b> Chen Xinchi, Shi Zhan, Qiu Xipeng, et al. Adversarial multi-criteria learning for Chinese word segmentation[C].Vancouver:Proceedings of the 55<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 201l.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Google&amp;#39;&amp;#39;s multilingual neural machine translation system-enabling zero-shot translation">

                                <b>[13]</b> Melvin Johnson, Mike Schuster, Quoc V Le. Google’s multilingual neural machine translation system-enabling zero-shot translation[J]. Computational Linguistics, 2017 (5) :339-351.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201604013&amp;v=MjYwMzh6TklUZkFaYkc0SDlmTXE0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GaURnV3I=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 王贵新, 郑孝宗.基于Word2vec的短信向量化算法[J].电子科技2016, 29 (4) :49-52. Wang Guixin, Zheng Xiaozong.An algorithm for vectoring SMS based on Word2vec[J]. Electronic Science and Technology, 2016, 29 (4) :49-52.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for Chinese word segmentation and POS tagging">

                                <b>[15]</b> Zheng Xiaoqing, Chen Hanyang, Xu Tianyu.Deep learning for Chinese word segmentation and POS tagging[C].Seattle:Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2013.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural word segmentation learning for Chinese">

                                <b>[16]</b> Cai Deng, Zhao Hai.Neural word segmentation learning for Chinese[C].Berlin:Proceedings of the 54<sup>th</sup> Annual Meeting of the Association for Computational Linguistics, 2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201904008" />
        <input id="dpi" type="hidden" value="399" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201904008&amp;v=MjIzNDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0ZpRGdXcnpOSVRmQVpiRzRIOWpNcTQ5RmJJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

