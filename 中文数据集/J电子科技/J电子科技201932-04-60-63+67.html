

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139874375576250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201904014%26RESULT%3d1%26SIGN%3dF3WXZIBjgM6pnLYynmxazCpKuHw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201904014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201904014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201904014&amp;v=MTc3NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0ZpRGhVcjNCSVRmQVpiRzRIOWpNcTQ5RVk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;1 基于&lt;/b&gt;Spark&lt;b&gt;的改进随机森林算法&lt;/b&gt; "><b>1 基于</b>Spark<b>的改进随机森林算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="&lt;b&gt;1.1&lt;/b&gt; Spark&lt;b&gt;介绍&lt;/b&gt;"><b>1.1</b> Spark<b>介绍</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;1.2 改进的随机森林算法&lt;/b&gt;"><b>1.2 改进的随机森林算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="&lt;b&gt;2 实验结果&lt;/b&gt; "><b>2 实验结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="&lt;b&gt;2.1 实验配置和数据集&lt;/b&gt;"><b>2.1 实验配置和数据集</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;2.2&lt;/b&gt; IRFA&lt;b&gt;算法的分类性能实验&lt;/b&gt;"><b>2.2</b> IRFA<b>算法的分类性能实验</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;2.3&lt;/b&gt; IRFA&lt;b&gt;算法加速比实验&lt;/b&gt;"><b>2.3</b> IRFA<b>算法加速比实验</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;2.4&lt;/b&gt; IRFA&lt;b&gt;算法效率实验&lt;/b&gt;"><b>2.4</b> IRFA<b>算法效率实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="&lt;b&gt;3 结束语&lt;/b&gt; "><b>3 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;算法的平均精度和平均&lt;/b&gt;CPU&lt;b&gt;耗时&lt;/b&gt;"><b>表</b>1 <b>算法的平均精度和平均</b>CPU<b>耗时</b></a></li>
                                                <li><a href="#87" data-title="图1 IRFA算法加速比曲线">图1 IRFA算法加速比曲线</a></li>
                                                <li><a href="#94" data-title="图2 IRFA算法的效率曲线">图2 IRFA算法的效率曲线</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Kulkarni V Y, Sinha P K.Efficient learning of random forest classifier using disjoint partitioning approach[J]. Proceedings of the World Congress on Engineering, 2013, 2 (5) :1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient learning of random forest classifier using disjoint partitioning approach">
                                        <b>[1]</b>
                                         Kulkarni V Y, Sinha P K.Efficient learning of random forest classifier using disjoint partitioning approach[J]. Proceedings of the World Congress on Engineering, 2013, 2 (5) :1-5.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Mi Y.Imbalanced classification based on active learning SMOTE[J].Research Journal of Applied Sciences Engineering &amp;amp; Technology, 2013, 5 (3) :944-949." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imbalanced Classification Based on Active Learning SMOTE">
                                        <b>[2]</b>
                                         Mi Y.Imbalanced classification based on active learning SMOTE[J].Research Journal of Applied Sciences Engineering &amp;amp; Technology, 2013, 5 (3) :944-949.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Amaratunga D, Cabrera J, Lee Y S.Enriched random forests.[J]. Bioinformatics, 2008, 24 (18) :2010-2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enriched random forests">
                                        <b>[3]</b>
                                         Amaratunga D, Cabrera J, Lee Y S.Enriched random forests.[J]. Bioinformatics, 2008, 24 (18) :2010-2014.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Xu B X, Huang J Z, Williams G, et al.Classifying very high-dimensional data with random forests built from small subspaces[J].International Journal of Data Warehousing and Mining, 2011, 8 (2) :44-63." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classifying very high-dimensional data with random forests built from small subspaces">
                                        <b>[4]</b>
                                         Xu B X, Huang J Z, Williams G, et al.Classifying very high-dimensional data with random forests built from small subspaces[J].International Journal of Data Warehousing and Mining, 2011, 8 (2) :44-63.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Ye Y M, Wu Q Y, Huang J Z, et al.Stratified sampling for feature subspace selection in random forests for high dimensional data[J].Pattern Recognition, 2013, 46 (3) : 769-787." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600107412&amp;v=MjM0MDY5Rlplc0lDSDA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5KS0ZvVmFCbz1OaWZPZmJLOEh0RE1xWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Ye Y M, Wu Q Y, Huang J Z, et al.Stratified sampling for feature subspace selection in random forests for high dimensional data[J].Pattern Recognition, 2013, 46 (3) : 769-787.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Sun K, Miao W, Zhang X, et al.An improvement to feature selection of random forests on Spark[C].Chengdu:2014 IEEE 17&lt;sup&gt;th&lt;/sup&gt; International Conference on Computational Science and Engineering, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An improvement to feature selection of random forests on Spark">
                                        <b>[6]</b>
                                         Sun K, Miao W, Zhang X, et al.An improvement to feature selection of random forests on Spark[C].Chengdu:2014 IEEE 17&lt;sup&gt;th&lt;/sup&gt; International Conference on Computational Science and Engineering, 2014.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Wu X, Zhu X, Wu G Q.Data mining with big data[J]. IEEE Transactions on Knowledge Data Engineering, 2014, 26 (1) :97-107." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data mining with big data">
                                        <b>[7]</b>
                                         Wu X, Zhu X, Wu G Q.Data mining with big data[J]. IEEE Transactions on Knowledge Data Engineering, 2014, 26 (1) :97-107.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Kuang L, Hao F, Yang L T, et.al.A tensor-based approach for big data representation and dimensionality reduction[J].IEEE Transactions on Emerge Topics Computer, 2014, 2 (3) :280-291." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A tensor-based approach for big data representation and dimensionality reduction">
                                        <b>[8]</b>
                                         Kuang L, Hao F, Yang L T, et.al.A tensor-based approach for big data representation and dimensionality reduction[J].IEEE Transactions on Emerge Topics Computer, 2014, 2 (3) :280-291.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Zhang C, Yuan D.Fast fine-grained air quality index level prediction using random forest algorithm on cluster computing of Spark[C].Beijing: IEEE, UIC-ATC-ScalCom-CBDCom-Iop, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Fast Fine-Grained Air Quality Index Level Prediction Using Random Forest Algorithm on Cluster Computing of Spark,&amp;quot;">
                                        <b>[9]</b>
                                         Zhang C, Yuan D.Fast fine-grained air quality index level prediction using random forest algorithm on cluster computing of Spark[C].Beijing: IEEE, UIC-ATC-ScalCom-CBDCom-Iop, 2015.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Dean J, Ghemawat S.MapReduce: simplified data processing on large clusters[J].Communications of the ACM, 2008, 51 (1) : 107-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000030554&amp;v=MjE3NTFKS0ZvVmFCbz1OaWZJWTdLN0h0ak5yNDlGWk9nUENYazlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Dean J, Ghemawat S.MapReduce: simplified data processing on large clusters[J].Communications of the ACM, 2008, 51 (1) : 107-113.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 夏卫雷, 王立松.基于MapReduce的并行蚁群算法研究与实现[J].电子科技, 2013, 26 (2) :146-149. Xia Weilei, Wang Lisong.Research on and implementation of parallel ant colony algorithm based on MapReduce[J].Electronic Science and Technology, 2013, 26 (2) :146-149." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201302047&amp;v=MjkxOTMzenFxQnRHRnJDVVI3cWZadVpvRmlEaFVyM0FJVGZBWmJHNEg5TE1yWTlCWTRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         夏卫雷, 王立松.基于MapReduce的并行蚁群算法研究与实现[J].电子科技, 2013, 26 (2) :146-149. Xia Weilei, Wang Lisong.Research on and implementation of parallel ant colony algorithm based on MapReduce[J].Electronic Science and Technology, 2013, 26 (2) :146-149.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Dimple B, Sudarshan T.IBM text analytics on Apache Spark[M].San Francisco:Saprk Summit, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=IBM text analytics on Apache Spark">
                                        <b>[12]</b>
                                         Dimple B, Sudarshan T.IBM text analytics on Apache Spark[M].San Francisco:Saprk Summit, 2014.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Li W, Cheng H L, Peng Y.Visualized data mining platform based on the Spark[C].Hangzhou:Proceedings of the 16&lt;sup&gt;th&lt;/sup&gt; System Simulation Technology and Application, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Visualized data mining platform based on the Spark,&amp;quot;">
                                        <b>[13]</b>
                                         Li W, Cheng H L, Peng Y.Visualized data mining platform based on the Spark[C].Hangzhou:Proceedings of the 16&lt;sup&gt;th&lt;/sup&gt; System Simulation Technology and Application, 2014.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Bian H Q, Chen Y G, Du X Y.Equal-join optimization on Spark[J].Journal of East China Normal University :Natural Science, 2014, 5 (1) :263-270." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HDSZ201405025&amp;v=MDIzNjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GaURoVXIzQUxTbllkTEc0SDlYTXFvOUhZWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Bian H Q, Chen Y G, Du X Y.Equal-join optimization on Spark[J].Journal of East China Normal University :Natural Science, 2014, 5 (1) :263-270.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Zhang J, Li T, Da R.A parallel method for computing rough set approximations[J].Information Sciences, 2012, 194 (5) : 209-223." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601416330&amp;v=MDU2MzlWYUJvPU5pZk9mYks3SHRETnFZOUVZT29KRDM4NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJuSktGbw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Zhang J, Li T, Da R.A parallel method for computing rough set approximations[J].Information Sciences, 2012, 194 (5) : 209-223.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Zhu Weisheng, Wang Peng.Large-scale image retrieval solution based on hadoop cloud computing platform[J]. Journal of Computer Aplliction, 2014, 34 (3) :695-699." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201403020&amp;v=MjQ0MTFvRmlEaFVyM0FMejdCZDdHNEg5WE1ySTlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Zhu Weisheng, Wang Peng.Large-scale image retrieval solution based on hadoop cloud computing platform[J]. Journal of Computer Aplliction, 2014, 34 (3) :695-699.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(04),60-63+67 DOI:10.16180/j.cnki.issn1007-7820.2019.04.013            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于</b>Spark<b>的改进随机森林算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E6%82%A6&amp;code=25044566&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙悦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E5%81%A5&amp;code=08584676&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁健</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E5%85%89%E7%94%B5%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0256814&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海理工大学光电信息与计算机工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对基于单机的经典随机森林算法无法满足海量数据处理需求的问题, 文中采用Spark分布式存储计算技术设计并实现了改进的随机森林算法。首先计算特征的重要程度, 将特征分为公共特征、独有特征和非重要特征;然后按顺序和比例分别在各个特征子空间中随机选择特征;最后通过Spark集群进行实验, 分析改进的随机森林算法分类性能、加速比和效率。结果证实改进的算法提高了随机森林构建效率, 可以用来解决海量数据挖掘问题, 具有良好的可扩展性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机森林;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Spark&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Spark;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征空间;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ReliefF%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ReliefF算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E7%BB%B4%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高维数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    孙悦 (1993-) , 男, 硕士研究生。研究方向:数据挖掘。;
                                </span>
                                <span>
                                    袁健 (1971-) , 女, 博士, 副教授。研究方向:云计算安全与大数据关联和智能交通。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61775139);</span>
                    </p>
            </div>
                    <h1><b>Improved Random Forest Algorithm Based on Spark</b></h1>
                    <h2>
                    <span>SUN Yue</span>
                    <span>YUAN Jian</span>
            </h2>
                    <h2>
                    <span>School of Optical Electrical and Computer Engineering, University of Shanghai for Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>For the classical random forest algorithm based on single machine couldn't meet the demand of dealing with massive data, an improved random forest algorithm based on Spark was designed and implemented by using Spark distributed memory computing technology. Firstly, after calculating the importance of features the features were divided into public features, unique features, and non-important features;. Then, randomly features were selected in each feature subspace in order and proportion. Finally, the experiment was performed through Spark clusters to analyze the improved classification performance, speedup ratio and efficiency of the random forest algorithm. The result demonstrated that the improved algorithm could improve the efficiency of random forest construction and could be used to solve the massive data mining problem with good scalability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=random%20forest&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">random forest;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spark&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spark;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20space&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature space;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ReliefF%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ReliefF algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high%20dimensional%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high dimensional data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-18</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Natural Science Foundation of China (61775139);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="35">互联网的高速发展衍生出大量高维复杂数据, 如何有效对这些数据进行分类以挖掘出有价值的信息是具有重大意义的课题。随机森林算法是一种重要的分类算法, 其分类性能优于其他分类器。随机森林算法在数据不平衡的数据库上执行效率高, 能够有效的处理缺失的数据。因此, 深入研究随机森林算法将有助于加快机器学习领域对数据分类的研究<citation id="98" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="36">目前许多研究者对随机森林算法进行研究并做出了相应的改进, 主要针对于拥有噪声的和冗余特征的高维度数据。部分研究者<citation id="99" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>将特征权值方法引入到特征随机选择过程中, 使特征子空间中的特征与类别相关性更强, 增加了树与树之间的强度。然而其缺点在于更加倾向选择权值大的特征, 降低了特征子空间的多样性, 使得决策树之间的相关性增加, 导致出现过拟合问题。另一些研究者<citation id="100" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>采用分层采样的技术将特征进行细化, 生成特征子空间, 然后在每个层内进行随机采样, 该方法有助于保留与类别相关性强的特征。但在分层采样过程中使用该方法, 可能要重复计算特征分组, 大大增加了计算成本。</p>
                </div>
                <div class="p1">
                    <p id="37">近年来, 随着各种新的信息传播方式的不断涌现, 以及云计算 (Cloud Computing) 和物联网 (Internet of Things, IoT) 技术的兴起, 数据的增长速度也随之加快。据统计, 全球数据的规模以每两年增加一倍的速度不断增加<citation id="101" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 数据在各个领域中的用价值变得越来越重要。</p>
                </div>
                <div class="p1">
                    <p id="38">然而, 数据的高速增长也带来了严重的问题和挑战。由于业务需求和竞争压力, 几乎每个企业都对数据处理的实时性和有效性要求很高<citation id="102" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。因此, 如何高效准确的从海量数据中挖掘有价值的信息是当前要解决的问题之一。大量的数据通常拥有数百或数千个属性, 其中每一个属性都可能包含一些信息;同时, 大数据也有其自身特点, 比如特征维度高、数据复杂和拥有噪声等。那么在高维度数据集中, 如何选择适当的技术以提升分类性能, 是目前需要解决的一个问题。</p>
                </div>
                <div class="p1">
                    <p id="39">对于特征维度较高且样本较大的数据集, 目前标准的随机森林算法分类准确率还很低。针对高维大样本数据分类的不足, 本文改进了标准随机森林算法。然而, 随着大数据时代的到来, 基于单机的随机森林算法无法满足处理海量数据的需求。尽管MapReduce方法可以处理大规模的数据集, 但在长时间磁盘读写和资源应用程序的过程中, 该算法的效率难以保证<citation id="103" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。因此, 有必要设计海量数据的快速随机森林算法来实现处理。本文利用Spark分布式存储计算技术设计并实现了改进的随机森林算法 (Improved Random Forest Algorithm, IRFA) , 并通过实验对算法的性能进行比较和分析。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>1 基于</b>Spark<b>的改进随机森林算法</b></h3>
                <h4 class="anchor-tag" id="41" name="41"><b>1.1</b> Spark<b>介绍</b></h4>
                <div class="p1">
                    <p id="42">Spark使用MapReduce思想实现分布式计算, 具有MapReduce的相关优势<citation id="104" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。但与同样基于MapReduce的Hadoop操作模型不同, Spark提出了一种内存聚类模型, 将任务的中间输出和结果保存到内存中, 从而节省了大量磁盘访问开销, 提高了数据读写效率, 可以更好的应用于如随机森林, 蚁群等诸多迭代算法<citation id="105" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="43">Spark能够进行高速计算, 其基础的核心是实现和采用基于弹性分布式数据集 (Resilient Distributed Dataset, RDD) 的共享内存。该数据集已分区且不可变, 并且支持并行处理。RDD支持两种操作:转换和操作。所有的Spark动作都是延迟的, 只有当真正需要返回结果给RDD时, 这些操作才会发生, 这也提高了计算效率。与此同时, Spark将在RDD上记录一系列转换操作来构建RDD依赖项LINEAGE, 以确保RDD数据的健壮性。此外, Spark为用户提供了更丰富, 更简单的操作, 包括创建操作, 转换操作, 控制操作和移动操作。Spark程序开发人员可以充分利用这些运算符对RDD进行免费操作。</p>
                </div>
                <div class="p1">
                    <p id="44">Spark是近年来开源社区最活跃的项目之一, 2014年成为Apache项目的顶级项目。2016年7月26日发布了最新的Spark 2.0.0, 主要更新了APIs, 支持SQL2003, 支持RUDF。300名开发者修复了2500个补丁。许多企业和个人都加入了Spark应用研究领域, 并取得了预期的成果。IBM研究了使用Spark来提高大规模文本分析的性能<citation id="106" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。中国科学技术大学建立了基于Spark的大型数据挖掘平台<citation id="107" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 利用内存计算来提高迭代速度。此外, 它支持各种分布式计算和存储场景, 具有很强的可扩展性。在数据分析领域, 连接操作是基本操作之一, 文献<citation id="108" type="reference">[<a class="sup">14</a>]</citation>讨论了Spark上的快速等价连接算法。</p>
                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>1.2 改进的随机森林算法</b></h4>
                <h4 class="anchor-tag" id="46" name="46">1.2.1 随机森林算法</h4>
                <div class="p1">
                    <p id="47">随机森林是L. Breiman提出的机器学习算法之一, 该算法由一组决策树组合而成, 其中每棵决策树的学习方法如下:从样本集中用Bootstrap采样选出<i>n</i>个样本, 对于训练集中的<i>T</i>个属性, 随机选择<i>t</i> (<i>t</i>&lt;&lt;<i>T</i>) 个属性, 在每个节点处使用最佳的属性进行分割, 便建立了一颗CART决策树。</p>
                </div>
                <div class="p1">
                    <p id="48">与经典决策树分类器相比, 随机森林有很多优点, 其在泛化能力, 分类效果等方面表现出色, 能有效克服过拟合问题, 还能够衡量特征的权值等。尽管随机森林的优点比较多, 但是在特征维度较多, 且样本数量较多的数据中还存在一些不足之处:</p>
                </div>
                <div class="p1">
                    <p id="49"> (1) 对高维数据而言, 为了保持基学习器的学习强度, 那么基学习器的个数会随之增加, 导致程序的时间和空间复杂度大幅增加;</p>
                </div>
                <div class="p1">
                    <p id="50"> (2) 随着特征维度的增加, 在随机选择特征的过程中不确定性也会增加, 增加了使用最佳属性进行分割的难度。同时, 决策树之间的相关度随着维度的增加而变大, 并且会有部分冗余属性参与分割, 降低了决策树的学习强度, 增加了随机森林的泛化误差。</p>
                </div>
                <div class="p1">
                    <p id="51">对于标准的随机森林算法而言, 为了降低树与树之间的相关程度, 一般从<i>T</i>个特征中随机选择<i>t</i> (<i>t</i>&lt;&lt;<i>T</i>) 个特征, 每个特征以相同的概率被选中。但是, 对于不同的应用, 其特征的重要性是不同的, 对节点进行分裂时的影响也不同。文献<citation id="109" type="reference">[<a class="sup">3</a>]</citation>中提出在进行特征选择时, 需要根据特征之间的相关性。但是该算法倾向于选择权值大的特征, 从而降低了特征子空间的多样性, 使得决策树之间的相关性增加, 会出现过拟合的问题, 反而使泛化误差变大。</p>
                </div>
                <h4 class="anchor-tag" id="52" name="52">1.2.2 改进的随机森林算法 (IRFA算法) </h4>
                <div class="p1">
                    <p id="53">鉴于以上分析, 本文给出特征选择改进思路: 首先计算特征的重要程度, 将特征分为公共特征、独有特征和非重要特征;然后按顺序和比例分别在各个特征子空间中随机选择特征。</p>
                </div>
                <div class="p1">
                    <p id="54">具体实现步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="55"> (1) 特征重要性评价。本文采用ReliefF算法计算特征的重要性权值:首先从训练样本中随机选取一个样本<i>x</i>, 然后从同一类的样本中找出<i>k</i>个最近邻样本, 并从不同类别的样本中随机找出<i>k</i>个非相似最近邻样本。通过比较类内距离和类间距离来调整特征加权矢量以给出特征的权重。对每个特征重复上述步骤, 最后得到每个特征的权重值。ReliefF算法更新特征的权值计算式为</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>W</mi><msubsup><mrow></mrow><mi>f</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mi>f</mi><mi>i</mi></msubsup><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>≠</mo><mtext>c</mtext><mtext>l</mtext><mtext>a</mtext><mtext>s</mtext><mtext>s</mtext></mrow></munder><mrow><mfrac><mrow><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mrow><mn>1</mn><mo>-</mo><mi>p</mi><mo stretchy="false"> (</mo><mtext>c</mtext><mtext>l</mtext><mtext>a</mtext><mtext>s</mtext><mtext>s</mtext><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow></munder><mtext>d</mtext></mstyle><mtext>i</mtext><mtext>f</mtext><mtext>f</mtext><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow></mfrac></mrow></mstyle><mo>-</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>d</mtext></mstyle><mtext>i</mtext><mtext>f</mtext><mtext>f</mtext><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>Η</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mi>m</mi><mo>×</mo><mi>k</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中, diff<sub><i>f</i></sub> (·) 是特征<i>f</i>上两个样本的距离;<i>H</i><sub><i>j</i></sub> (<i>X</i>) 是样本<i>x</i>所在类别中的近邻样本;<i>M</i><sub><i>j</i></sub> (<i>X</i>) 是不同类别的近邻样本;<i>P</i> (<i>X</i>) 是类别的概率。</p>
                </div>
                <div class="p1">
                    <p id="58">在研究中, 使用欧氏距离来计算样本的类内距离和类间距离。欧氏距离反映两个样本之间的相似程度, 该值越小, 两个样本之间的差异越小。欧氏距离公式为</p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="61"> (2) 分区域随机选择特征。根据ReliefF算法计算各个子类特征集合<i>F</i><sub>1</sub>, <i>F</i><sub>2</sub>, …, <i>F</i><sub><i>r</i></sub>, 然后运用集合运算计算出公共特征<i>F</i><sub>share</sub>, 各个子类独有的特征<i>F</i><sub>1</sub>, …, <i>F</i><sub><i>r</i></sub>和非重要特征<i>F</i><sub>left</sub></p>
                </div>
                <div class="p1">
                    <p id="62"><i>F</i><sub>share</sub>=<i>F</i><sub>1</sub>∩<i>F</i><sub>2</sub>∩…∩<i>F</i><sub><i>r</i></sub></p>
                </div>
                <div class="p1">
                    <p id="63"><i>F</i><sub>1</sub>=<i>F</i><sub>1</sub>-<i>F</i><sub>share</sub></p>
                </div>
                <div class="p1">
                    <p id="64"><i>F</i><sub><i>r</i></sub>=<i>F</i><sub><i>r</i></sub>-<i>F</i><sub>share</sub></p>
                </div>
                <div class="p1">
                    <p id="65">︙</p>
                </div>
                <div class="p1">
                    <p id="66"><i>F</i><sub>left</sub>=<i>F</i><sub>all</sub>-<i>F</i><sub>1</sub>-<i>F</i><sub>2</sub>-…-<i>F</i><sub><i>r</i></sub></p>
                </div>
                <div class="p1">
                    <p id="67">根据比例, 依次从{<i>F</i><sub>share</sub>, <i>F</i><sub>1</sub>, <i>F</i><sub>left</sub>}, {<i>F</i><sub>share</sub>, <i>F</i><sub>2</sub>, <i>F</i><sub>left</sub>}, …, {<i>F</i><sub>share</sub>, <i>F</i><sub><i>r</i></sub>, <i>F</i><sub>left</sub>}中随机选择特征, 构造特征子空间。</p>
                </div>
                <div class="p1">
                    <p id="68">改进的随机森林原始训练集样本数为<i>n</i>, 特征数为<i>f</i>, 结果输出<i>k</i><sub>sub</sub>个CART树, 基本步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="69"><b>步骤1</b> 采用Bootstrap策略, 有放回的随机抽取<i>k</i><sub>sub</sub>个自助样本集;</p>
                </div>
                <div class="p1">
                    <p id="70"><b>步骤2</b> 选取一个自助样本集作为根节点, 以完全分裂的方式进行训练;</p>
                </div>
                <div class="p1">
                    <p id="71"><b>步骤3</b> 根据ReliefF算法, 计算各个子类特征集合<i>F</i><sub>1</sub>, <i>F</i><sub>2</sub>, …, <i>F</i><sub><i>r</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="72"><b>步骤4</b> 按顺序和比例从<i>F</i><sub>share</sub>, <i>F</i><sub>1</sub>, <i>F</i><sub>left</sub>3个特征子集随机选取<i>f</i><sub>tree</sub>维特征子空间 (<i>f</i><sub>tree</sub>&lt;&lt;<i>f</i>) , 以节点不纯度 (Gini不纯度、熵不纯度) 最小为原则, 选取最优特征对节点进行分裂生长;</p>
                </div>
                <div class="p1">
                    <p id="73"><b>步骤5</b> 最大限度地使节点进行分裂生长, 若节点具有最小的不纯度, 则将其标记为叶子节点;</p>
                </div>
                <div class="p1">
                    <p id="74"><b>步骤6</b> 重复步骤2～步骤5, 直到所有节点都被训练过或被标记为叶子节点;</p>
                </div>
                <div class="p1">
                    <p id="75"><b>步骤7</b> 重复步骤2～步骤6, 直到所有的CART都被训练过。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag"><b>2 实验结果</b></h3>
                <h4 class="anchor-tag" id="77" name="77"><b>2.1 实验配置和数据集</b></h4>
                <div class="p1">
                    <p id="78">通过实验比较IRFA算法的性能, 然后分析基于Spark的IRFA算法的有效性。实验中使用的Spark集群由5个节点组成, 其中包括1个主节点和4个工作节点。 每个节点的具体配置如下:Intel Xeon CPU E5-2609-V3, 1 GB RAM, 100 GB硬盘, 操作系统CentOS6.5, Hadoop版本2.4.1, Spark版本2.0.0, 节点间100 Mbit·s<sup>-1</sup>网络连接。数据集来自信用数据集, 其中包含大约200 MB, 16个属性, 6个连续属性和2个类别。所使用的数据集存储在HDFS上, 并且对随机森林在改进之前和之后具有相同的参数设置。在实验中, 分别复制原始数据集1、2、4、8次, 以此获得大数据集<i>D</i><sub>1</sub>、<i>D</i><sub>2</sub>、<i>D</i><sub>3</sub>和<i>D</i><sub>4</sub>。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.2</b> IRFA<b>算法的分类性能实验</b></h4>
                <div class="p1">
                    <p id="80">比较IRFA与经典随机森林算法的性能, 且实验在单节点下运行, 为了排除偶然因素对结果的影响, 实验使用10倍交叉验证来确定IRFA的平均分类准确率和平均CPU时间, 以此作为实验记录。分类准确率定义为测试集中正确识别的样本数量与样本总数的比率, 结果如表1所示。</p>
                </div>
                <div class="area_img" id="81">
                    <p class="img_tit"><b>表</b>1 <b>算法的平均精度和平均</b>CPU<b>耗时</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Average accuracy of the algorithm and the average cpu time-consuming</p>
                    <p class="img_note"></p>
                    <table id="81" border="1"><tr><td><br /></td><td>平均精度/%</td><td>平均CPU耗时/ms</td></tr><tr><td><br />RF</td><td>86.885</td><td>42.159</td></tr><tr><td><br />IRFA</td><td>86.282</td><td>27.897</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="82">从平均准确率来看, 可以得出这样的结论:随机森林算法固有的随机性导致分类结果略有差异, 应该将其视为在误差范围内。且随机森林算法改进前后的准确率没有显着影响, 符合以前的定性分析。从运行时间可以得出, CPU的平均耗时比以前有了一定的缩短, 在一定程度上得到了改善。总之, IRFA可以提高随机森林构建时的效率, 对平均准确率几乎没有显着影响。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>2.3</b> IRFA<b>算法加速比实验</b></h4>
                <div class="p1">
                    <p id="84">该实验在Spark集群环境中进行, 用于验证IRFA是否具有良好的加速性能。加速比是衡量并行计算性能的重要手段<citation id="110" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 用于描述并行算法运行时间减少时的性能。其值为单节点处理器系统和多节点并行处理器系统中单个任务消耗的时间的比率。加速比定义为</p>
                </div>
                <div class="p1">
                    <p id="85"><i>S</i>=<i>T</i><sub>1</sub>/<i>T</i><sub><i>n</i></sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="86">其中, <i>T</i><sub>1</sub>代表单个节点的运行时间, <i>T</i><sub><i>n</i></sub>代表<i>n</i>个节点的运行时间。理想情况下, 在按顺序添加节点时, <i>S</i>将与<i>y</i>=<i>x</i>直线重合, IRFA算法的加速比曲线如图1所示。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201904014_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 IRFA算法加速比曲线" src="Detail/GetImg?filename=images/DZKK201904014_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 IRFA算法加速比曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201904014_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. IRFA algorithm speedup curve</p>

                </div>
                <div class="p1">
                    <p id="88">实验结果表明, 在相同的Spark集群节点下, 大数据集<i>D</i><sub>1</sub>、<i>D</i><sub>2</sub>、<i>D</i><sub>3</sub>和<i>D</i><sub>4</sub>分别复制原始数据集 (1、2、4、8次) 的加速比大小是<i>D</i><sub>4</sub>&gt;<i>D</i><sub>3</sub>&gt;<i>D</i><sub>2</sub>&gt;<i>D</i><sub>1</sub>, 说明IRAF算法的加速比随着数据量的增加而增加, 加速曲线呈线性增加。理论上, 加速性能将随节点数量线性增加, 效率将保持不变。 但是, 在实际情况下, 会有额外的耗时任务, 如节点间的管理和通信会导致加速比不能线性增加, 加速比的增加速度减慢。从图1可以看出, 算法在具有相同节点数的较大数据集上更为有效, 推测出当数据量增大时, 加速比的性能将得到进一步提高。实验结果表明IRFA算法具有很好的加速性能, 可以用来解决海量数据挖掘问题。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>2.4</b> IRFA<b>算法效率实验</b></h4>
                <div class="p1">
                    <p id="90">本实验在Spark集群环境中进行, 用于验证IRFA算法是否具有良好的可扩展性。为了评估和反映集群在并行计算中的使用情况, 引入了算法效率的概念。并行算法的效率是算法的加速比与节点数的比率, 算法的效率定义为</p>
                </div>
                <div class="p1">
                    <p id="91"><i>E</i>=<i>S</i>/<i>N</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="92">其中, <i>S</i>是加速比, <i>N</i>是节点数, 效率值一般在0～1之间。根据文献<citation id="111" type="reference">[<a class="sup">16</a>]</citation>, 当算法效率大于0.5时, 认为该算法可以取得良好的性能。IRFA算法的效率曲线如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="93">从图2可以看出, 在相同的Spark集群节点下, 大数据集<i>D</i><sub>1</sub>、<i>D</i><sub>2</sub>、<i>D</i><sub>3</sub>和<i>D</i><sub>4</sub> (分别复制原始数据集1、2、4、8次) 的效率大小是<i>D</i><sub>4</sub>&gt;<i>D</i><sub>3</sub>&gt;<i>D</i><sub>2</sub>&gt;<i>D</i><sub>1</sub>, 或者对于同一个数据集而言, 其效率随着节点数的增加而减少, 说明通过改变数据的大小和计算节点的数量, 随着簇的扩展, 算法的总体效率呈现相对平坦的下降趋势。 这说明IRFA算法的效率不会随着数据量的增加而增加, 节点性能也显著降低, 说明该算法具有良好的可扩展性, 即节点运算利用率较高。从下降趋势可以看出, IRFA算法对于海量数据具有良好的可扩展性。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201904014_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 IRFA算法的效率曲线" src="Detail/GetImg?filename=images/DZKK201904014_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 IRFA算法的效率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201904014_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. IRFA algorithm efficiency curve</p>

                </div>
                <h3 id="95" name="95" class="anchor-tag"><b>3 结束语</b></h3>
                <div class="p1">
                    <p id="96">针对高维大样本数据分类的不足, 本文首先根据ReliefF算法计算特征的权值, 通过集合运算计算出公共特征、独有特征和非重要特征;然后按顺序和比例分别在各个特征子空间中随机选择特征;最后利用Spark分布式存储计算技术设计并实现了改进的随机森林算法。通过分类性能实验、加速比实验和效率实验对算法的性能进行比较和分析, 证明了该算法可以提高随机森林构建效率, 可以用来解决海量数据挖掘问题, 具有良好的可扩展性。</p>
                </div>
                <div class="p1">
                    <p id="97">在高维大数据环境下, 本文目前对随机森林分类问题的改进, 只考虑了高维大数据的特征对随机森林的特征随机选择过程的影响, 之后的工作可以考虑数据其他方面因素的特点, 比如大数据不平衡对应算法的影响。该实验具有一定的复杂性但是由于实验条件有限, 因此本文基于Spark的改进算法运行时间略长, 那么如何在保证准确率和泛化误差的同时, 又能进一步减少算法的运行时间, 需要在之后的研究中继续研究。此外, 提高实验规模和分类精度, 优化Spark计算模型仍是进一步研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient learning of random forest classifier using disjoint partitioning approach">

                                <b>[1]</b> Kulkarni V Y, Sinha P K.Efficient learning of random forest classifier using disjoint partitioning approach[J]. Proceedings of the World Congress on Engineering, 2013, 2 (5) :1-5.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imbalanced Classification Based on Active Learning SMOTE">

                                <b>[2]</b> Mi Y.Imbalanced classification based on active learning SMOTE[J].Research Journal of Applied Sciences Engineering &amp; Technology, 2013, 5 (3) :944-949.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enriched random forests">

                                <b>[3]</b> Amaratunga D, Cabrera J, Lee Y S.Enriched random forests.[J]. Bioinformatics, 2008, 24 (18) :2010-2014.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classifying very high-dimensional data with random forests built from small subspaces">

                                <b>[4]</b> Xu B X, Huang J Z, Williams G, et al.Classifying very high-dimensional data with random forests built from small subspaces[J].International Journal of Data Warehousing and Mining, 2011, 8 (2) :44-63.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600107412&amp;v=MTYyNzU3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5KS0ZvVmFCbz1OaWZPZmJLOEh0RE1xWTlGWmVzSUNIMA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Ye Y M, Wu Q Y, Huang J Z, et al.Stratified sampling for feature subspace selection in random forests for high dimensional data[J].Pattern Recognition, 2013, 46 (3) : 769-787.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An improvement to feature selection of random forests on Spark">

                                <b>[6]</b> Sun K, Miao W, Zhang X, et al.An improvement to feature selection of random forests on Spark[C].Chengdu:2014 IEEE 17<sup>th</sup> International Conference on Computational Science and Engineering, 2014.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data mining with big data">

                                <b>[7]</b> Wu X, Zhu X, Wu G Q.Data mining with big data[J]. IEEE Transactions on Knowledge Data Engineering, 2014, 26 (1) :97-107.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A tensor-based approach for big data representation and dimensionality reduction">

                                <b>[8]</b> Kuang L, Hao F, Yang L T, et.al.A tensor-based approach for big data representation and dimensionality reduction[J].IEEE Transactions on Emerge Topics Computer, 2014, 2 (3) :280-291.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Fast Fine-Grained Air Quality Index Level Prediction Using Random Forest Algorithm on Cluster Computing of Spark,&amp;quot;">

                                <b>[9]</b> Zhang C, Yuan D.Fast fine-grained air quality index level prediction using random forest algorithm on cluster computing of Spark[C].Beijing: IEEE, UIC-ATC-ScalCom-CBDCom-Iop, 2015.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000030554&amp;v=MDIyMzhvVmFCbz1OaWZJWTdLN0h0ak5yNDlGWk9nUENYazlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybkpLRg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Dean J, Ghemawat S.MapReduce: simplified data processing on large clusters[J].Communications of the ACM, 2008, 51 (1) : 107-113.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201302047&amp;v=MjE2MjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GaURoVXIzQUlUZkFaYkc0SDlMTXJZOUJZNFFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 夏卫雷, 王立松.基于MapReduce的并行蚁群算法研究与实现[J].电子科技, 2013, 26 (2) :146-149. Xia Weilei, Wang Lisong.Research on and implementation of parallel ant colony algorithm based on MapReduce[J].Electronic Science and Technology, 2013, 26 (2) :146-149.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=IBM text analytics on Apache Spark">

                                <b>[12]</b> Dimple B, Sudarshan T.IBM text analytics on Apache Spark[M].San Francisco:Saprk Summit, 2014.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Visualized data mining platform based on the Spark,&amp;quot;">

                                <b>[13]</b> Li W, Cheng H L, Peng Y.Visualized data mining platform based on the Spark[C].Hangzhou:Proceedings of the 16<sup>th</sup> System Simulation Technology and Application, 2014.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HDSZ201405025&amp;v=MDM0NjBGckNVUjdxZlp1Wm9GaURoVXIzQUxTbllkTEc0SDlYTXFvOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Bian H Q, Chen Y G, Du X Y.Equal-join optimization on Spark[J].Journal of East China Normal University :Natural Science, 2014, 5 (1) :263-270.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601416330&amp;v=MDYxMTBuSktGb1ZhQm89TmlmT2ZiSzdIdEROcVk5RVlPb0pEMzg1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Zhang J, Li T, Da R.A parallel method for computing rough set approximations[J].Information Sciences, 2012, 194 (5) : 209-223.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201403020&amp;v=MzExODdmWnVab0ZpRGhVcjNBTHo3QmQ3RzRIOVhNckk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Zhu Weisheng, Wang Peng.Large-scale image retrieval solution based on hadoop cloud computing platform[J]. Journal of Computer Aplliction, 2014, 34 (3) :695-699.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201904014" />
        <input id="dpi" type="hidden" value="399" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201904014&amp;v=MTc3NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0ZpRGhVcjNCSVRmQVpiRzRIOWpNcTQ5RVk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZNc25zRDRuRVU4L0NLNVRPODU4TT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

