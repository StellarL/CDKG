

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139233116357500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201908004%26RESULT%3d1%26SIGN%3dqS4MxFHPHVaytFz2Q69BYRjUuiQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201908004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201908004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201908004&amp;v=MDcxODNVUjdxZlp1WnBGeS9rVTd2SUlUZkFaYkc0SDlqTXA0OUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;1 自适应位置融合的相关滤波目标跟踪&lt;/b&gt; "><b>1 自适应位置融合的相关滤波目标跟踪</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;1.1 单层卷积特征&lt;/b&gt;"><b>1.1 单层卷积特征</b></a></li>
                                                <li><a href="#42" data-title="&lt;b&gt;1.2  多相关滤波器&lt;/b&gt;"><b>1.2  多相关滤波器</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;1.3 自适应位置融合&lt;/b&gt;"><b>1.3 自适应位置融合</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;1.4 稀疏模型更新&lt;/b&gt;"><b>1.4 稀疏模型更新</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;2 实验结果分析&lt;/b&gt; "><b>2 实验结果分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;3 结束语&lt;/b&gt; "><b>3 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;实验参数设置&lt;/b&gt;"><b>表</b>1 <b>实验参数设置</b></a></li>
                                                <li><a href="#82" data-title="图1 不同算法平均DP和速度对比实验">图1 不同算法平均DP和速度对比实验</a></li>
                                                <li><a href="#85" data-title="图2 4种算法OPE精确度和成功率对比曲线">图2 4种算法OPE精确度和成功率对比曲线</a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;视频序列及其描述&lt;/b&gt;"><b>表</b>2 <b>视频序列及其描述</b></a></li>
                                                <li><a href="#92" data-title="图3 4种算法的跟踪效果对比">图3 4种算法的跟踪效果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="117">


                                    <a id="bibliography_1" title=" 高文, 朱明, 贺柏根, 等.目标跟踪技术综述[J].中国光学, 2014, 3 (7) :365-375.Gao Wen, Zhu Ming, He Baigen, et al.Overview of target tracking technology[J].Chinese Optics, 2014, 3 (7) :365-375." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGGA201403003&amp;v=MDM2MjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkva1U3dklQeXJNYjdHNEg5WE1ySTlGWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         高文, 朱明, 贺柏根, 等.目标跟踪技术综述[J].中国光学, 2014, 3 (7) :365-375.Gao Wen, Zhu Ming, He Baigen, et al.Overview of target tracking technology[J].Chinese Optics, 2014, 3 (7) :365-375.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_2" title=" Paschalakis S, Bober M.Real-time face detection and tracking for mobile video conferencing[J].Real-Time Imaging, 2004, 10 (2) :81-94." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501085058&amp;v=MjIyMjFpbmxVcmpJSjE4VWJoST1OaWZPZmJLN0h0RE5xbzlFWk9NS0RIa3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Paschalakis S, Bober M.Real-time face detection and tracking for mobile video conferencing[J].Real-Time Imaging, 2004, 10 (2) :81-94.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_3" title=" Coifman B, Beymer D, Mclanuchlan P, et al.A real-time computer vision system for vehicle tracking and traffic surveillance[J].Transportation Research Part C Emerging Technologies, 1998, 6 (4) :271-288." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501418870&amp;v=MjU5MjBxbzlFWU9vSEJIczVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKMThVYmhJPU5pZk9mYks3SHRETg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Coifman B, Beymer D, Mclanuchlan P, et al.A real-time computer vision system for vehicle tracking and traffic surveillance[J].Transportation Research Part C Emerging Technologies, 1998, 6 (4) :271-288.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_4" title=" Henriques J F, Rui C, Martins P.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37 (3) :583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Highspeed tracking with kernelized correlation filters">
                                        <b>[4]</b>
                                         Henriques J F, Rui C, Martins P.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37 (3) :583-596.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_5" title=" 席志红, 李永佳, 段炼.基于局部特征和Mean Shift的目标跟踪算法研究[J].电子科技, 2015, 28 (6) :13-17.Xi Zhihong, Li Yongjia, Duan Lian.Research on object tracking method based on local feature and mean shift[J].Electronic Science and Technology, 2015, 28 (6) :13-17." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201506005&amp;v=Mjg5Nzc3dklJVGZBWmJHNEg5VE1xWTlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkva1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         席志红, 李永佳, 段炼.基于局部特征和Mean Shift的目标跟踪算法研究[J].电子科技, 2015, 28 (6) :13-17.Xi Zhihong, Li Yongjia, Duan Lian.Research on object tracking method based on local feature and mean shift[J].Electronic Science and Technology, 2015, 28 (6) :13-17.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_6" title=" 李弼程, 邵美珍, 黄杰.模式识别原理与应用[M].西安:西安电子科技大学出版社, 2008.Li Bicheng, Shao Meizhen, Huang Jie.Principle and application of pattern recognition[M].Xi’an:Xidian University, 2008." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787560619859001&amp;v=MDU2NzVuS3JpZlplWnZGeW5pVTdqSklGb1VYRnF6R2JhK0h0Zk5wb2RBYmVzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         李弼程, 邵美珍, 黄杰.模式识别原理与应用[M].西安:西安电子科技大学出版社, 2008.Li Bicheng, Shao Meizhen, Huang Jie.Principle and application of pattern recognition[M].Xi’an:Xidian University, 2008.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_7" title=" Belongie S, Malik J, Puzicha J.Shape matching and object recognition using shape contexts[J].IEEE Transaction on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape Matching and Object Recognition Using Shape Contexts">
                                        <b>[7]</b>
                                         Belongie S, Malik J, Puzicha J.Shape matching and object recognition using shape contexts[J].IEEE Transaction on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_8" title=" Bertinetto L, Valmadre J, Goldetz S.Staple:complementary learners for real-time tracking[C].Washing DC:IEEE International Conference on Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[8]</b>
                                         Bertinetto L, Valmadre J, Goldetz S.Staple:complementary learners for real-time tracking[C].Washing DC:IEEE International Conference on Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Danellijan M, Hager G, Khan F.Accurate scaleestimation for robust visual tracking[C].Nottingham:British Machine Vision Conference, 2014.</a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_10" title=" Wang L, Ouyang W, Wang X, et al.Visual tracking with fully convolutional networks[C].Santiago:IEEE International Conference on Computer Vision, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual Tracking with Fully Convolutional Networks">
                                        <b>[10]</b>
                                         Wang L, Ouyang W, Wang X, et al.Visual tracking with fully convolutional networks[C].Santiago:IEEE International Conference on Computer Vision, 2015.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_11" title=" Nam H, Baek M, Han B.Modeling and propagating CNNs in a tree structure for visual tracking[EB/OL]. (2017-05-21) [2018-06-18]http:// arxiv.org/abs/1608.07242, 2017.5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling and propagating CNNs in a tree structure for visual tracking">
                                        <b>[11]</b>
                                         Nam H, Baek M, Han B.Modeling and propagating CNNs in a tree structure for visual tracking[EB/OL]. (2017-05-21) [2018-06-18]http:// arxiv.org/abs/1608.07242, 2017.5.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_12" title=" Ma C, Huang J, Yanf X, et al.Hierarchical convolutional features for visual tracking[C].Washing D C:Computer Vision and Pattern Recognition, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">
                                        <b>[12]</b>
                                         Ma C, Huang J, Yanf X, et al.Hierarchical convolutional features for visual tracking[C].Washing D C:Computer Vision and Pattern Recognition, 2015.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_13" title=" Wang X, Li H, Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washing D C:IEEE International Conference on Multimedia and Expo, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and real-time deep tracking via multi-scale domain adaptation">
                                        <b>[13]</b>
                                         Wang X, Li H, Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washing D C:IEEE International Conference on Multimedia and Expo, 2017.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Simonyan K, Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science, 2014 (11) :332-345.</a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Danelljan M, Bhat G, Khan F, et al.ECO:Efficient Convolution Operators for tracking[C].Hawaii:IEEE Conference on Computer Vision and Pattern Recognition, 2017.</a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_16" title=" Wu Y, Lim J, Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2015, 37 (9) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[16]</b>
                                         Wu Y, Lim J, Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2015, 37 (9) :1834-1848.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-20 07:08</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(08),12-15+32 DOI:10.16180/j.cnki.issn1007-7820.2019.08.003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>自适应位置融合的目标跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B6%A6%E7%8E%B2&amp;code=39828539&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王润玲</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0226398&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方工业大学理学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提高分层卷积特征目标跟踪算法的实时性和鲁棒性, 文中提出了一种基于多个相关滤波器预测位置自适应融合的实时目标跟踪算法。该算法首先提取VGG-19网络的Pool4层卷积特征, 通过特征均值比对多通道的特征图进行裁剪, 提高算法速度。然后利用不同高斯样本分布训练多个相关滤波分类器, 并对所有分类器预测的目标位置进行自适应融合, 提高算法对目标姿态变化的鲁棒性;最后采用稀疏模型更新策略, 进一步提高算法速度。在OTB100标准数据集上测试本文算法, 实验结果表明, 该算法的平均距离精度为86.3%, 比原分层卷积特征跟踪算法提高了2.6个百分点, 在目标发生遮挡、形变、相似背景干扰等情况时具有很好的鲁棒性;平均跟踪速度为45.2帧/s, 是原算法的4倍, 实时性能良好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8D%E7%BD%AE%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">位置融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型更新;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9E%E6%97%B6%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">实时性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王润玲 (1991-) , 女, 硕士研究生。研究方向:深度学习, 图像处理, 视频分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFC0821102);</span>
                    </p>
            </div>
                    <h1><b>Adaptive Positions Fusion for Visual Tracking</b></h1>
                    <h2>
                    <span>WANG Runling</span>
            </h2>
                    <h2>
                    <span>School of Sciences, North China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To improve the real-time and robust performances of hierarchical convolutional features for visual tracking method, the adaptive positions fusion based on multiple correlation filters for visual tracking was proposed. Firstly, features were extracted from Pool4 layer of VGG-19 network. Besides, multi-channel feature maps were pruned by average feature energy ratio to speed up the algorithm. Moreover, it trained several correlation filters with different Gaussian distributions of training samples, and fused all the predicted positions adaptively. Finally, the sparse model update strategy was used to further speed up. The proposed algorithm was evaluated on OTB100 benchmark dataset. The results showed that the average precision was 86.3%, which was 2.6 percentage points higher than the hierarchical convolutional features for visual tracking method. It was robust when there were occlusion、deformation and similarity interference. The average speed was 45.2 frames per second, four times than the original method and had favorable real-time performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=position%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">position fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20update&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model update;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=real-time%20performance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">real-time performance;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-20</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Key R&amp;D Program of China (2017YFC0821102);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="35">目标跟踪指在视频或图像序列中确定目标物体所处的位置<citation id="149" type="reference"><link href="117" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。目标跟踪技术是融合了图像处理、模式识别、人工智能等多领域的跨学科技术, 在智能交通、人机交互以及气象分析等方面有着广阔的应用前景<citation id="150" type="reference"><link href="119" rel="bibliography" /><link href="121" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="36">相关滤波的跟踪算法因实时性能良好而受到广大研究者的青睐。文献<citation id="151" type="reference">[<a class="sup">4</a>]</citation>提出了核相关滤波算法 (Kernelized Correlation Filters, KCF) , 将单一的灰度特征扩展到多通道的方向梯度直方图特征 (Histogram of Oriented Gradient, HOG) , 提高了算法精度, 但对多尺度、快速运动等效果不理想。 文献<citation id="152" type="reference">[<a class="sup">5</a>]</citation>将形状上下文特征<citation id="153" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>与Mean Shift算法<citation id="154" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>进行结合, 提高了对旋转以及噪声干扰的鲁棒性。文献<citation id="155" type="reference">[<a class="sup">8</a>]</citation>结合局部HOG特征和RGB颜色直方图特征, 利用两类特征的优势, 提高了形变上的鲁棒性, 但对光照变化和相似物干扰的适应性变差。 文献<citation id="156" type="reference">[<a class="sup">9</a>]</citation>设计了一种尺度相关滤波器, 有效缓解了目标尺度变化对跟踪的影响。</p>
                </div>
                <div class="p1">
                    <p id="37">2013年以来, 深度学习方法在目标跟踪领域崭露头角, 并逐渐在性能上超越传统手工方法。 文献<citation id="157" type="reference">[<a class="sup">10</a>]</citation>将卷积神经网络的特征图用于目标跟踪, 且利用高低层卷积特征在语义和空间信息上的互补性, 设计了一个特征筛选网络来选取不同层的特征进行目标跟踪, 取得了很好的效果, 但跟踪速度较慢。为解决目标在遭遇严重遮挡时的模型污染问题, 文献<citation id="158" type="reference">[<a class="sup">11</a>]</citation>设计了一种树结构的卷积神经网络模型, 但速度仅1.5帧/s。文献<citation id="159" type="reference">[<a class="sup">12</a>]</citation>选用3个不同卷积分层构造3个相关滤波目标跟踪器并进行优化组合, 跟踪效果良好但速度仅10帧/s, 且不具备尺度变化的能力。为提高其速度和对目标尺度的适应能力, 文献<citation id="160" type="reference">[<a class="sup">13</a>]</citation>提出多尺度域适应的跟踪算法, 将卷积特征通道数降为1/8, 使算法实时。</p>
                </div>
                <div class="p1">
                    <p id="38">基于手工特征的相关滤波算法实时性能很好, 但在跟踪过程中遇到严重遮挡、剧烈形变、背景复杂或相似物体干扰等复杂情况时, 易出现跟踪错误或者跟丢。深度卷积特征跟踪算法效果虽好, 速度却慢。于是, 须综合考虑跟踪效果与运行速度, 即改善算法效果的同时提高跟踪速度。因此, 本文在分层卷积特征相关滤波目标跟踪算法<citation id="161" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的框架下, 提出以下3点改进: (1) 减少卷积特征提取层数, 选取Pool4层卷积特征表征目标表观, 提高跟踪算法的速度; (2) 选取不同分布样本训练多个相关滤波分类器, 并对所有分类器的预测位置进行自适应融合, 提高跟踪的精确度; (3) 采用稀疏模型更新策略对滤波器进行更新, 对算法进一步提速, 达到实时跟踪的效果。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>1 自适应位置融合的相关滤波目标跟踪</b></h3>
                <h4 class="anchor-tag" id="40" name="40"><b>1.1 单层卷积特征</b></h4>
                <div class="p1">
                    <p id="41">VGG-19网络<citation id="162" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>不同层所提取的卷积特征侧重点不同, 低层特征保留了更多的纹理信息, 对目标的精确定位更有效;高层特征含有较为丰富的语义信息, 对目标的表观变化适应性较强。 因此, 分层卷积相关滤波目标跟踪算法<citation id="163" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>融合3个不同深度的卷积分层提取图像特征进行目标跟踪。然而, 在响应图融合时浅层特征的权值很小, 对跟踪任务的贡献几乎可以忽略不计, 却增加了计算的复杂度, 严重影响算法的速度。为提高算法的速度和精确度, 该算法仅选取Pool4作为特征提取层。Pool4层特征由Conv4-4层池化 (Pooling) 后得到, 保留了有效的特征, 同时具有高、低层特征的性质, 既可用于目标定位, 又可充分表示目标外观变化。</p>
                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>1.2  多相关滤波器</b></h4>
                <div class="p1">
                    <p id="43">对某一视频序列的第<i>t</i>帧图像, 目标的多维特征输入为<b><i>x</i></b>∈<i>R</i><sup><i>M</i>×<i>N</i>×<i>D</i></sup>, 其单个样本记为<b><i>x</i></b><sub><i>m</i>, <i>n</i></sub>, 通过最小化输出误差可得最优相关滤波器</p>
                </div>
                <div class="p1">
                    <p id="44"><mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>w</mi></munder><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false">∥</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>d</mi></msubsup><mo>-</mo><mi>g</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="46">式中, <i>λ</i>≥0为正则化参数, <i>g</i> (<i>m</i>, <i>n</i>) 为训练样本的类标函数, 决定像素的分类状况, 即该像素分类为目标或背景。 通常情况下采用单一的样本分布训练相关滤波器, 从而忽略了不同序列图像间的差异性。 于是, 本文提出用不同分布的样本训练多个相关滤波分类器对目标进行定位, 即采用多个高斯样本分布类标函数</p>
                </div>
                <div class="p1">
                    <p id="47"><mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false">[</mo><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>Μ</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>Ν</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mi>b</mi><mn>2</mn></msubsup></mrow></mfrac><mo stretchy="false">]</mo></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="49">训练多个相关滤波分类器并对所有分类器的预测结果进行自适应融合。其中, <i>g</i><sub><i>b</i></sub>和<i>σ</i><sub><i>b</i></sub>分别表示第<i>b</i>个高斯分布样本的类标函数和带宽因子。</p>
                </div>
                <div class="p1">
                    <p id="50">于是, 对于多通道卷积特征图<b><i>x</i></b>∈<sup><i>RM</i>×<i>N</i>×<i>D</i></sup>, 构造<i>l</i>层通道<i>d</i>∈{1, 2, …, <i>D</i>}上融合多层卷积特征相关滤波分类器</p>
                </div>
                <div class="p1">
                    <p id="51"><mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>D</mi><mi>d</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msubsup><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>D</mi></mrow><mi>d</mi></msubsup><mo>=</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mi>D</mi></msub><mo>⊙</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mi mathvariant="bold-italic">Q</mi></mstyle><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="53">其中, </p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Q</mi><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup><mo>=</mo><mfrac><mrow><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msubsup><mrow></mrow><mi>l</mi><mi>i</mi></msubsup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>l</mi><mi>i</mi></msubsup><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="56"><b><i>X</i>, <i>G</i></b>和<b><i>W</i></b>分别表示<b><i>x</i></b>、<i>g</i>和<b><i>w</i></b>的离散傅里叶变换 (Discrete Fourier Transform, <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>D</mtext><mtext>F</mtext><mtext>Τ</mtext><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover></mrow></math></mathml>为<b><i>X</i></b>的共轭复数;“⊙”表示Hardmard积。</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58"><b>1.3 自适应位置融合</b></h4>
                <div class="p1">
                    <p id="59">对于给定目标候选区域的傅里叶变换后的l层d通道特征<b><i>Z</i></b><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">l</mi><mi mathvariant="bold-italic">d</mi></msubsup></mrow></math></mathml>, 计算第<i>b</i>个相关滤波分类器的响应图<i>f</i><sub><i>b</i></sub></p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>b</mi></msub><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mi>b</mi></msub><mo>⊙</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mi mathvariant="bold-italic">Q</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup><mo>⊙</mo><mi mathvariant="bold-italic">Ζ</mi><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="63">相关滤波响应值表示输入图像与滤波模板的相似程度。因此通过计算响应<i>f</i><sub><i>b</i></sub>的最大值, 即可得第<i>b</i>个分类器预测的目标位置<i>p</i><sub><i>b</i></sub>。最后, 对不同高斯分布下所得到的所有目标位置进行自适应加权融合</p>
                </div>
                <div class="p1">
                    <p id="64"><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo>=</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mi>b</mi></msub><mrow><mfrac><mrow><mi>max</mi><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>b</mi></msub><mtext>m</mtext></mstyle><mtext>a</mtext><mtext>x</mtext><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></mstyle><mo>⋅</mo><mi>p</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></math></mathml>。      (6) </p>
                </div>
                <div class="p1">
                    <p id="66">目标在运动过程中尺度会发生改变, 因此采用文献<citation id="164" type="reference">[<a class="sup">9</a>]</citation>中的尺度金字塔策略对目标进行尺度估计。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>1.4 稀疏模型更新</b></h4>
                <div class="p1">
                    <p id="68">在目标运动的过程中, 自身表观会不断发生变化, 还会受到遮挡、相似物体的干扰, 因此要对模型进行阶段性更新。 若对视频每帧更新易造成模型漂移; 若不更新或者更新间隔太大, 会使模型跟不上目标变化的速度, 造成跟踪失败。</p>
                </div>
                <div class="p1">
                    <p id="69">受文献<citation id="165" type="reference">[<a class="sup">15</a>]</citation>模型更新阶段的启发, 经过大量实验, 本文选用间隔3帧更新模型, 在提高精确度的同时, 大幅提高了算法运行速度。令<b><i>A</i></b><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>和<b><i>B</i></b><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>分别表示无类标函数滤波器因式<b><i>Q</i></b><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>的分子和分母, 则每隔3帧对<b><i>Q</i></b><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>进行更新</p>
                </div>
                <div class="area_img" id="74">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201908004_07400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="76">由于目标物体尺度变化较快, 因此, 尺度滤波器对每帧图像都进行更新。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>2 实验结果分析</b></h3>
                <div class="p1">
                    <p id="78">本文算法在Ubuntu16.04系统下的MATLAB R2015b, 所有的实验均在配置为Intel Core i7-7800XCPU, GTX1080Ti GPU, 内存为16 GB的台式电脑上完成。 算法的具体参数设置如表1所示。 尺度相关滤波器相关参数与DSST<citation id="166" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>设置相同。</p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表</b>1 <b>实验参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Parameters setting for experiments</p>
                    <p class="img_note"></p>
                    <table id="79" border="1"><tr><td><br />参数名称</td><td><i>λ</i></td><td><i>η</i></td><td><i>σ</i></td></tr><tr><td><br />参数值</td><td>1e-4</td><td>1e-2</td><td>[0.1, 0.15]</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="80">本文共设计了3组实验: 第1组实验即为本文算法与HCFT<citation id="167" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、MSDAT<citation id="168" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>以及Staple<citation id="169" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>算法在OTB100标准数据集<citation id="170" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>上的平均距离精度 (Distance Precision, DP) 和跟踪速度对比实验;第2组实验为本文算法与其他3种跟踪算法的鲁棒性评估实验;第3组实验为4种算法的跟踪效果对比实验。 其中, Staple代表手工特征的相关滤波跟踪算法, HCFT代表分层卷积特征的相关滤波跟踪算法, MSDAT和本文算法代表HCFT的改进算法。</p>
                </div>
                <div class="p1">
                    <p id="81"><b>实验</b>1 平均距离精度和速度对比实验。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908004_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同算法平均DP和速度对比实验" src="Detail/GetImg?filename=images/DZKK201908004_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不同算法平均DP和速度对比实验  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908004_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Comparisons of the average DP and speed for 4 different algorithms</p>

                </div>
                <div class="p1">
                    <p id="83">图1给出了本文算法与其他3种算法在平均距离精度和速度上的对比结果。显然, 相比原HCFT算法, 本文在平均距离精度和平均速度方面均有很大的提升, 且跟踪速度最快, 充分显示了本文算法良好的实时性能。</p>
                </div>
                <div class="p1">
                    <p id="84"><b>实验</b>2 鲁棒性评估实验。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908004_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 4种算法OPE精确度和成功率对比曲线" src="Detail/GetImg?filename=images/DZKK201908004_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 4种算法OPE精确度和成功率对比曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908004_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Comparisons of the precision and success rate for 4 algorithms of OPE </p>
                                <p class="img_note"> (a) OPE精确度图 (b) OPE成功率图</p>
                                <p class="img_note"> (a) Precision plots of OPE (b) Success plots of OPE</p>

                </div>
                <div class="p1">
                    <p id="86">为了评估算法的鲁棒性, 在标准测试数据集OTB100上对本文算法鲁棒性进行评估, 其中包含尺度变化、旋转、背景混乱、遮挡等11种干扰因素。 通过对视频的第1帧图像建模, 统计后续图像的跟踪结果来对算法的性能进行评估。</p>
                </div>
                <div class="p1">
                    <p id="87">图2显示了4种算法在不同中心位置误差下, 一次通过评估 (One Pass Evaluation, OPE) 的精确度曲线和不同区域重叠率下的成功率曲线。 由图可知, 本文算法的精确度为86.3%, 成功率为62.8%, 均为最优, 相比HCFT分别提高了2.6和6.6个百分点。 表明本文算法具有稳定的跟踪性能。</p>
                </div>
                <div class="p1">
                    <p id="88"><b>实验</b>3 不同算法跟踪效果对比实验。</p>
                </div>
                <div class="p1">
                    <p id="89">本实验还选取了4个包含多个挑战因素的代表性视频序列MotorRolling、BlurOwl和CarScale进行分析, 每个序列的特点如表2所示。图3给出了Staple、HCFT、MSDAT和本文4种算法在4组视频序列上的部分跟踪结果。由图可知, 当目标物体发生尺度变化、旋转以及遭遇遮挡等各种复杂情况时, 本文算法均能够准确地定位目标, 显示了良好的鲁棒性。</p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>2 <b>视频序列及其描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 2. Video sequences and their descriptions</p>
                    <p class="img_note"></p>
                    <table id="91" border="1"><tr><td><br />序列名称</td><td>序列特点描述</td></tr><tr><td><br />MotorRolling</td><td>快速移动、运动模糊、旋转</td></tr><tr><td><br />BlurOwl</td><td>尺度变化、运动模糊、快速运动、旋转</td></tr><tr><td><br />CarScale</td><td>尺度变化、遮挡、旋转、快速运动</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201908004_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 4种算法的跟踪效果对比" src="Detail/GetImg?filename=images/DZKK201908004_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 4种算法的跟踪效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201908004_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Comparisons of tracking performances for 4 algorithms</p>

                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>3 结束语</b></h3>
                <div class="p1">
                    <p id="94">本文在分层卷积目标跟踪算法的框架下, 选取VGG-19的单层的Pool4层卷积特征来表征目标表观, 有效提升了算法的跟踪速度。通过多高斯分布的样本训练分类器, 并对所有分类器的跟踪结果进行自适应加权融合, 提高了跟踪器的泛化能力。最后, 对模板进行稀疏更新, 防止过拟合现象的同时进一步提高算法的跟踪速度。在标准数据集OTB100上测试本文算法, 取得了良好的实时性和实效性。但是, 本文并未对所提取的多维特征进行筛选, 存在冗余特征。因此, 下一步在自适应特征选择上进行研究, 以期取得更好的跟踪效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="117">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGGA201403003&amp;v=MTY1ODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5L2tVN3ZJUHlyTWI3RzRIOVhNckk5Rlo0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 高文, 朱明, 贺柏根, 等.目标跟踪技术综述[J].中国光学, 2014, 3 (7) :365-375.Gao Wen, Zhu Ming, He Baigen, et al.Overview of target tracking technology[J].Chinese Optics, 2014, 3 (7) :365-375.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501085058&amp;v=MjI2NTE3SHRETnFvOUVaT01LREhreG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUoxOFViaEk9TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Paschalakis S, Bober M.Real-time face detection and tracking for mobile video conferencing[J].Real-Time Imaging, 2004, 10 (2) :81-94.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501418870&amp;v=MjQxMDlSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUoxOFViaEk9TmlmT2ZiSzdIdEROcW85RVlPb0hCSHM1b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Coifman B, Beymer D, Mclanuchlan P, et al.A real-time computer vision system for vehicle tracking and traffic surveillance[J].Transportation Research Part C Emerging Technologies, 1998, 6 (4) :271-288.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Highspeed tracking with kernelized correlation filters">

                                <b>[4]</b> Henriques J F, Rui C, Martins P.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2014, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201506005&amp;v=Mjc3NjZxZlp1WnBGeS9rVTd2SUlUZkFaYkc0SDlUTXFZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 席志红, 李永佳, 段炼.基于局部特征和Mean Shift的目标跟踪算法研究[J].电子科技, 2015, 28 (6) :13-17.Xi Zhihong, Li Yongjia, Duan Lian.Research on object tracking method based on local feature and mean shift[J].Electronic Science and Technology, 2015, 28 (6) :13-17.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787560619859001&amp;v=MDQ3MThpZlplWnZGeW5pVTdqSklGb1VYRnF6R2JhK0h0Zk5wb2RBYmVzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bkty&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 李弼程, 邵美珍, 黄杰.模式识别原理与应用[M].西安:西安电子科技大学出版社, 2008.Li Bicheng, Shao Meizhen, Huang Jie.Principle and application of pattern recognition[M].Xi’an:Xidian University, 2008.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape Matching and Object Recognition Using Shape Contexts">

                                <b>[7]</b> Belongie S, Malik J, Puzicha J.Shape matching and object recognition using shape contexts[J].IEEE Transaction on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[8]</b> Bertinetto L, Valmadre J, Goldetz S.Staple:complementary learners for real-time tracking[C].Washing DC:IEEE International Conference on Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Danellijan M, Hager G, Khan F.Accurate scaleestimation for robust visual tracking[C].Nottingham:British Machine Vision Conference, 2014.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual Tracking with Fully Convolutional Networks">

                                <b>[10]</b> Wang L, Ouyang W, Wang X, et al.Visual tracking with fully convolutional networks[C].Santiago:IEEE International Conference on Computer Vision, 2015.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling and propagating CNNs in a tree structure for visual tracking">

                                <b>[11]</b> Nam H, Baek M, Han B.Modeling and propagating CNNs in a tree structure for visual tracking[EB/OL]. (2017-05-21) [2018-06-18]http:// arxiv.org/abs/1608.07242, 2017.5.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">

                                <b>[12]</b> Ma C, Huang J, Yanf X, et al.Hierarchical convolutional features for visual tracking[C].Washing D C:Computer Vision and Pattern Recognition, 2015.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and real-time deep tracking via multi-scale domain adaptation">

                                <b>[13]</b> Wang X, Li H, Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washing D C:IEEE International Conference on Multimedia and Expo, 2017.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Simonyan K, Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science, 2014 (11) :332-345.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Danelljan M, Bhat G, Khan F, et al.ECO:Efficient Convolution Operators for tracking[C].Hawaii:IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[16]</b> Wu Y, Lim J, Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201908004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201908004&amp;v=MDcxODNVUjdxZlp1WnBGeS9rVTd2SUlUZkFaYkc0SDlqTXA0OUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

