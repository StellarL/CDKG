

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139226852920000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201909004%26RESULT%3d1%26SIGN%3d%252b2wJlrQS8N0GbiQmv6K2YesRATU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201909004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201909004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201909004&amp;v=MzI0NTZVN3pNSVRmQVpiRzRIOWpNcG85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;1&lt;/b&gt; 相关滤波器 "><b>1</b> 相关滤波器</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="&lt;b&gt;2&lt;/b&gt; 自适应位置切换相关滤波器 "><b>2</b> 自适应位置切换相关滤波器</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;2.1&lt;/b&gt; 自适应卷积特征"><b>2.1</b> 自适应卷积特征</a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;2.2&lt;/b&gt; 自适应切换滤波器"><b>2.2</b> 自适应切换滤波器</a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;2.3&lt;/b&gt; 模型更新"><b>2.3</b> 模型更新</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="&lt;b&gt;3&lt;/b&gt; 实验结果 "><b>3</b> 实验结果</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;3.1&lt;/b&gt; 实验平台及评估指标"><b>3.1</b> 实验平台及评估指标</a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;3.2&lt;/b&gt; 跟踪性能对比"><b>3.2</b> 跟踪性能对比</a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;3.3&lt;/b&gt; 鲁棒性评估"><b>3.3</b> 鲁棒性评估</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="&lt;b&gt;4&lt;/b&gt; 结束语 "><b>4</b> 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="图1 MotorRolling视频图像搜索区域与特征图">图1 MotorRolling视频图像搜索区域与特征图</a></li>
                                                <li><a href="#76" data-title="图2 不同高斯样本分布对不同视频图像特征响应图">图2 不同高斯样本分布对不同视频图像特征响应图</a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表&lt;/b&gt;1 10&lt;b&gt;种算法平均&lt;/b&gt;DP、OP、CLE&lt;b&gt;及速度对比实验&lt;/b&gt;*"><b>表</b>1 10<b>种算法平均</b>DP、OP、CLE<b>及速度对比实验</b>*</a></li>
                                                <li><a href="#112" data-title="图3 4种算法精确度(左)和成功率(右)对比曲线">图3 4种算法精确度(左)和成功率(右)对比曲线</a></li>
                                                <li><a href="#114" data-title="图4 4种跟踪算法的部分跟踪结果对比">图4 4种跟踪算法的部分跟踪结果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" Henriques J F,Rui C,Martins P.Exploiting the circulant structure of tracking-by-detection with kernels[C].Heidelberg:European Conference on Computer Vision,Springer,2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[1]</b>
                                         Henriques J F,Rui C,Martins P.Exploiting the circulant structure of tracking-by-detection with kernels[C].Heidelberg:European Conference on Computer Vision,Springer,2012.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" Henriques J F,Rui C,Martins P.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,37(3):583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[2]</b>
                                         Henriques J F,Rui C,Martins P.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,37(3):583-596.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" 熊昌镇,赵璐璐,郭芬红.自适应特征融合的核相关滤波跟踪算法[J].计算机辅助设计与图形学学报,2017,29(6):1068-1074.Xiong Changzhen,Zhao Lulu,Guo Fenhong.Kernelized correlation filters tracking based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2017,29(6):1068-1074." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201706012&amp;v=MzA4ODJDVVI3cWZadVpwRnl6aFU3ek1MejdCYUxHNEg5Yk1xWTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         熊昌镇,赵璐璐,郭芬红.自适应特征融合的核相关滤波跟踪算法[J].计算机辅助设计与图形学学报,2017,29(6):1068-1074.Xiong Changzhen,Zhao Lulu,Guo Fenhong.Kernelized correlation filters tracking based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2017,29(6):1068-1074.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" Bertinetto L,Valmadre J,Goldetz S.Staple:complementary learners for real-time tracking[C].Washington D C:IEEE International Conference on Computer Vision and Pattern Recognition,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[4]</b>
                                         Bertinetto L,Valmadre J,Goldetz S.Staple:complementary learners for real-time tracking[C].Washington D C:IEEE International Conference on Computer Vision and Pattern Recognition,2016.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" Danellijan M,Hager G,Khan F.Accurate scale estimation for robust visual tracking[C].Nottingham:British Machine Vision Conference,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[5]</b>
                                         Danellijan M,Hager G,Khan F.Accurate scale estimation for robust visual tracking[C].Nottingham:British Machine Vision Conference,2014.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" 贾建兵,廖嘉伟,周羽.基于EK算法改进的多目标跟踪技术[J].电子科技,2018,31(4):91-96.Jia Jianbing,Liao Jiawei,Zhou Yu.Multitarget tracking technique based on Extended Kalman algorithm[J].Electronic Science and Technology,2018,31(4):91-96." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201804025&amp;v=MDY0NjdmWnVacEZ5emhVN3pNSVRmQVpiRzRIOW5NcTQ5SFlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         贾建兵,廖嘉伟,周羽.基于EK算法改进的多目标跟踪技术[J].电子科技,2018,31(4):91-96.Jia Jianbing,Liao Jiawei,Zhou Yu.Multitarget tracking technique based on Extended Kalman algorithm[J].Electronic Science and Technology,2018,31(4):91-96.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" Danellijan M,Gustav H,Fahad S.Learning spatially regularized correlation filters for visual tracking[C].Washington D C:IEEE International Conference on Computer Vision,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[7]</b>
                                         Danellijan M,Gustav H,Fahad S.Learning spatially regularized correlation filters for visual tracking[C].Washington D C:IEEE International Conference on Computer Vision,2015.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" Danellijan M,Hager G,Khan F S,et al.Convolutional features for correlation filter based visual tracking[C].Washington D C:IEEE International Conference on Computer Vision Workshop,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Features for Correlation Filter Based Visual Tracking">
                                        <b>[8]</b>
                                         Danellijan M,Hager G,Khan F S,et al.Convolutional features for correlation filter based visual tracking[C].Washington D C:IEEE International Conference on Computer Vision Workshop,2016.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" Danellijan M,Robinson A,Khan F S.Beyond correlation filters:learning continuous convolution operators for visual tracking[C].Cham:European Conference on Computer Vision,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond correlation filters:Learning continuous convolution operators for visual tracking">
                                        <b>[9]</b>
                                         Danellijan M,Robinson A,Khan F S.Beyond correlation filters:learning continuous convolution operators for visual tracking[C].Cham:European Conference on Computer Vision,2016.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" Danellijan M,Bhat G,Khan F,et al.ECO:Efficient Convolution Operators for tracking[C].Washington D C:Computer Vision and Pattern Recognition,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ECO:Efficient Convolution Operators for Tracking">
                                        <b>[10]</b>
                                         Danellijan M,Bhat G,Khan F,et al.ECO:Efficient Convolution Operators for tracking[C].Washington D C:Computer Vision and Pattern Recognition,2017.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" Ma C,Huang J,Yanf X,et al.Hierarchical convolutional features for visual tracking[C].Washington D C:Computer Vision and Pattern Recognition,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical convolutional features for visual tracking">
                                        <b>[11]</b>
                                         Ma C,Huang J,Yanf X,et al.Hierarchical convolutional features for visual tracking[C].Washington D C:Computer Vision and Pattern Recognition,2015.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" Wang X,Li H,Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washington D C:IEEE International Conference on Multimedia and Expo,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and real-time deep tracking via multi-scale domain adaptation">
                                        <b>[12]</b>
                                         Wang X,Li H,Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washington D C:IEEE International Conference on Multimedia and Expo,2017.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" Simonyan K,Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science,2014(7):633-638." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[13]</b>
                                         Simonyan K,Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science,2014(7):633-638.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Wu Y,Lim J,Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,37(9):1834-1848.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Qi Y,Zhang S,Qin L,et al.Hedged deep tracking[C].Las Vegas:IEEE Conference on Computer Vision and Pattern Recognition,2016.</a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" Li Y,Zhu J.A scale adaptive kernel correlation filter tracker with feature integration[J].Springer,2014,8926:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">
                                        <b>[16]</b>
                                         Li Y,Zhu J.A scale adaptive kernel correlation filter tracker with feature integration[J].Springer,2014,8926:254-265.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-20 07:08</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(09),10-14+31 DOI:10.16180/j.cnki.issn1007-7820.2019.09.003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">自适应位置切换相关滤波目标跟踪</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B6%A6%E7%8E%B2&amp;code=39828539&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王润玲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BB%95%E7%A1%95&amp;code=40933475&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">滕硕</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0226398&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方工业大学理学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为解决卷积特征目标跟踪算法精确度和速度相互制约的问题,文中提出了一种基于峰值旁瓣比的自适应位置切换的相关滤波目标跟踪算法。将Pool4和Conv5-3层作为特征提取层,通过特征能量均值比获取有效的卷积特征,提高算法的速度;然后利用不同样本分布训练多个相关滤波器,并根据峰值旁瓣比筛选出最适分类器进行位置预测,提高了跟踪器的泛化能力;最后利用稀疏模型更新策略更新滤波器模板,减小过拟合现象的同时进一步提高算法的速度。在OTB100标准数据集上测试该算法,实验结果表明,文中所提算法的精确度为88.8%,较原分层卷积跟踪算法提高了6.1%;跟踪速度为47.5帧/s,是原算法的5倍,显示了良好的实时性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B3%B0%E5%80%BC%E6%97%81%E7%93%A3%E6%AF%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">峰值旁瓣比;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型更新;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高斯分布;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王润玲(1991-),女,硕士研究生。研究方向:深度学习,图像处理和视频分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划(2017YFC0821102);</span>
                    </p>
            </div>
                    <h1><b>Adaptive Position Switching Based on Correlation Filters for Visual Tracking</b></h1>
                    <h2>
                    <span>WANG Runling</span>
                    <span>TENG Shuo</span>
            </h2>
                    <h2>
                    <span>School of Sciences,North China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To solve the problem of constrains between the accuracy and speed for convolutional features for visual tracking methods, an algorithm namely adaptive position switching based on correlation filters was proposed. Pool4 and Conv5-3 layers were selected for features extraction. At the same time, effective features were obtained by the average feature energy ratio, which improved the tracking speed. Then it trained correlation filters with different Gaussian distributions of samples. Therefore, the best classifier was selected to predict the position according to the peak-side-lobe ratio, with a promotion in the generalization ability of the tracker. Finally, the sparse model update strategy was adopted to reduce the over-fitting and further speed up the algorithm. This algorithm was tested on OTB100 benchmark dataset. Tracking results demonstrated that the accuracy was 88.8%, 6.1% higher than the hierarchical convolutional features for visual tracking method. The tracking speed was 47.5 frames per second, which was 5 times than the original method, and showed favorable real-time performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=peak-side-lobe%20ratio&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">peak-side-lobe ratio;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20update&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model update;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gaussian%20distribution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gaussian distribution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-22</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Key R&amp;D Program of China(2017YFC0821102);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="34">视觉跟踪技术通过计算机获取视频,对视频中的信息进行分析处理,找到视频后续帧中的运动目标并标记出来。</p>
                </div>
                <div class="p1">
                    <p id="35">基于相关滤波的跟踪方法由于其在速度上的优势受到研究者的青睐。文献<citation id="117" type="reference">[<a class="sup">1</a>]</citation>设计了单通道的核相关滤波器,速度高达362帧/s(fps),但灰度特征易导致跟踪失败。于是,将灰度特征扩展为多通道的方向梯度直方图(Histogram of Oriented Gradient,HOG)特征,提高算法鲁棒性<citation id="118" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。文献<citation id="119" type="reference">[<a class="sup">3</a>]</citation>提出一种自适应加权融合颜色属性特征和HOG特征的核相关滤波跟踪算法,解决了由于遮挡和光照变化引起的跟踪失败问题。文献<citation id="120" type="reference">[<a class="sup">4</a>]</citation>融合局部特征HOG和全局颜色直方图特征的互补优势,并引入尺度金字塔策略<citation id="121" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,跟踪速度达到约80 fps。文献<citation id="122" type="reference">[<a class="sup">6</a>]</citation>将线性高斯系统的目标跟踪扩展到非线性系统,提高了匀速转弯模型目标的跟踪性能。文献<citation id="123" type="reference">[<a class="sup">7</a>]</citation>提出空间正则化的相关滤波算法,解决了循环矩阵导致的边缘效应问题。</p>
                </div>
                <div class="p1">
                    <p id="36">深度特征可以更好地适应目标的表观变化,被广泛地应用到视觉跟踪领域。文献<citation id="124" type="reference">[<a class="sup">8</a>]</citation>在空间正则化的基础上加入卷积特征,提高了原算法的跟踪效果。文献<citation id="125" type="reference">[<a class="sup">9</a>]</citation>提出连续卷积操作的相关滤波跟踪算法,效果良好,但速度缓慢,还易引起过拟合。于是作者提出因式分解方法对特征进行降维,并利用稀疏模型更新策略,有效提高了算法速度<citation id="126" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="37">为提高深度学习算法的跟踪速度,研究者们将深度学习和相关滤波进行结合。文献<citation id="127" type="reference">[<a class="sup">11</a>]</citation>充分利用分层卷积特征在语义表达和空间定位的互补性结合相关滤波器来定位目标,性能稳定,但未对目标尺度变化进行评估。文献<citation id="128" type="reference">[<a class="sup">12</a>]</citation>针对分层卷积特征的目标跟踪算法<citation id="129" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>速度慢的问题,提出多尺度域跟踪算法,取得实时效果,显示出深度特征与相关滤波相结合的巨大优势。</p>
                </div>
                <div class="p1">
                    <p id="38">为解决分层卷积特征相关滤波跟踪算法实时性和跟踪效果互相制约的问题,本文主要做了以下两点工作: (1)为了提高跟踪算法的实时性,采用多通道的深度卷积特征,通过特征能量均值比对特征通道进行裁剪,同时对模型进行稀疏更新,使算法满足实时跟踪的要求;(2)为了避免速度提升时精确度下降,本文融合两层深度卷积特征,利用不同样本分布训练多个分类器,然后通过峰值旁瓣比筛选出最优分类器的预测结果,提高了对目标姿态变化的鲁棒性。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>1</b> 相关滤波器</h3>
                <div class="p1">
                    <p id="40">相关滤波的跟踪算法利用快速傅里叶变换进行滤波器训练和响应图计算,极大地提高了跟踪速度,具有很好的跟踪实时性。对第t帧图像,记目标特征为<b><i>x</i></b>∈<b><i>R</i></b><sup><i>M</i>×<i>N</i>×<i>D</i></sup>,将其沿水平和垂直方向上的所有循环移位作为训练样本,对于单个样本<b><i>x</i></b><sub><i>m</i>,<i>n</i></sub>,其类标函数为<i>g</i>(<i>m</i>,<i>n</i>),则最优相关滤波器</p>
                </div>
                <div class="area_img" id="41">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201909004_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="42">式中,<i>λ</i>为正则化参数,<i>g</i>(<i>m</i>,<i>n</i>)为峰值在<b><i>x</i></b><sub><i>m</i>,<i>n</i></sub>中心位置的二维高斯核函数。令</p>
                </div>
                <div class="p1">
                    <p id="43"><mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ζ</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow></munder><mo stretchy="false">∥</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">h</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>,</mo><mi>n</mi></mrow><mi>d</mi></msubsup><mo>-</mo><mi>g</mi><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">h</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="45">其频域表示</p>
                </div>
                <div class="p1">
                    <p id="46"><mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ζ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Μ</mi><mi>Ν</mi></mrow></mfrac><mo stretchy="false">(</mo><mo stretchy="false">∥</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>d</mi></msup><mo>-</mo><mi mathvariant="bold-italic">G</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>d</mi></msup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="48">式中,<b><i>X</i></b>,<b><i>G</i></b>和<b><i>W</i></b>分别表示<b><i>x</i></b>,<i>g</i>和<b><i>w</i></b>的离散傅里叶变换,<mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover></math></mathml>为<b><i>X</i></b>的共轭复数;⊙表示对应元素相乘。于是,通道<i>d</i>上的最优滤波器为</p>
                </div>
                <div class="p1">
                    <p id="50"><mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>d</mi></msup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">G</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>d</mi></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msup><mrow></mrow><mi>i</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>i</mi></msup><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="52">因此,给定目标候选区域的多维特征图<i>z</i>,其傅里叶变换为<b><i>Z</i></b>,可以得到相关响应图<i>f</i></p>
                </div>
                <div class="p1">
                    <p id="53"><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>=</mo><mtext>F</mtext><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Ζ</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>d</mi></msup><mo stretchy="false">)</mo></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="55">其中,<mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Ζ</mi><mo>¯</mo></mover></math></mathml>为<b><i>Z</i></b>的共轭复数,F<sup>-1</sup>表示傅里叶逆变换。在相关响应图<i>f</i>中寻找最大响应值即可以得到目标的预测位置。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag"><b>2</b> 自适应位置切换相关滤波器</h3>
                <div class="p1">
                    <p id="58">对输入的第1帧图像,利用VGG-19卷积神经网络<citation id="130" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提取目标搜索区域的Pool4和Conv5-3层的卷积特征,根据特征能量均值比获取有效的卷积特征,同时用不同带宽因子的高斯分布样本训练多个相关滤波分类器。然后利用峰值旁瓣比筛选出最适合的相关滤波器对后续输入的视频帧进行位置预测。最后对相关滤波器模板进行更新。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>2.1</b> 自适应卷积特征</h4>
                <div class="p1">
                    <p id="60">VGG-19网络低层卷积特征保存了更多细粒度的纹理信息,对目标定位较为有利;而高层可以适应目标的各种表观变化。因此,本文选取Pool4与Conv5-3层作为特征提取层来完成跟踪任务。Conv5-3卷积层接近全连接层,对目标的形变、遮挡等更加鲁棒。Pool4属于中间层,同时具有低层特征的空间信息与高层特征的语义信息。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909004_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 MotorRolling视频图像搜索区域与特征图" src="Detail/GetImg?filename=images/DZKK201909004_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 MotorRolling视频图像搜索区域与特征图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909004_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Search region and feature maps for MotorRolling</p>

                </div>
                <div class="p1">
                    <p id="62">相关滤波算法根据上一帧目标位置确定搜索区域,通过获取搜索区域特征的相关滤波最大响应值来定位目标。如图1所示,卷积神经网络的许多特征图是多维的,有些通道的响应图可以很好地表征目标,而有些通道的响应图含有部分背景信息,有的甚至是可以对跟踪造成干扰的冗余特征,同时也有一部分通道的响应图中不包含目标特征,属于无效特征,影响算法的速度。</p>
                </div>
                <div class="p1">
                    <p id="63">于是,针对卷积特征图中搜索区域与目标区域的关系,提出了目标区域特征能量均值<b><i>F</i></b><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup></mrow></math></mathml>(<i>O</i>)与搜索区域特征均值<b><i>F</i></b><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup></mrow></math></mathml>(<i>S</i>)相比,反映特征图中目标区域特征的交叠比例</p>
                </div>
                <div class="area_img" id="68">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201909004_06800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="70">R值越大表示所获取的特征越有效;R值越小,背景干扰特征越多。因此,选取满足卷积特征均值比大于给定阈值T的通道特征用于目标定位,提高跟踪精确度的同时提高了算法的速度。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>2.2</b> 自适应切换滤波器</h4>
                <div class="p1">
                    <p id="75">训练相关滤波器时,采用二维高斯分布的类标函数,其分布的分散程度由高斯函数的带宽因子<i>σ</i>决定。而现有的跟踪算法通常选用固定样本分布来训练分类器。图2显示了不同高斯样本分布对不同视频图像特征可视化。图中<i>σ</i><sub>1</sub>=0.06,<i>σ</i><sub>2</sub>=0.1,<i>σ</i><sub>3</sub>=0.12。图2(a)为Singer2视频图像输入(背景混乱、光照变化),带宽因子为<i>σ</i><sub>1</sub>的类标函数得到的分类器可以较好地预测目标位置;图2(b)为Lemming视频图像输入(遮挡),带宽因子为<i>σ</i><sub>3</sub>的类标函数得到的分类器可以较好地预测目标位置;图2(c)为Basketball视频图像输入(形变、相似背景干扰),带宽因子为<i>σ</i><sub>2</sub>的类标函数得到的分类器可以较好地预测目标位置。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909004_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同高斯样本分布对不同视频图像特征响应图" src="Detail/GetImg?filename=images/DZKK201909004_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同高斯样本分布对不同视频图像特征响应图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909004_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Response maps for different video images with 
different Gaussian distributions</p>

                </div>
                <div class="p1">
                    <p id="77">为更好地定位目标,引入峰值旁瓣比</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Ρ</mtext><mtext>S</mtext><mtext>R</mtext><mo>=</mo><mfrac><mrow><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></mfrac></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="80">对目标位置的预测结果的置信度进行判断。其中,f<sub>1</sub>为视频序列第一帧图像响应值,μ<sub>1</sub>为响应均值。<i>PSR</i>值越大,表明在某种程度上预测结果的置信度越高。因此,选取<i>PSR</i>大于给定阈值的相关滤波器进行跟踪。 因此本文采用多个高斯样本分布类标函数</p>
                </div>
                <div class="p1">
                    <p id="81"><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false">[</mo><mo>-</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mi>Μ</mi><mo>/</mo><mn>2</mn><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mi>Ν</mi><mo>/</mo><mn>2</mn><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mi>b</mi><mn>2</mn></msubsup></mrow></mfrac><mo stretchy="false">]</mo></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="83">训练多个分类器并对根据峰值旁瓣比对所有分类器的预测结果进行筛选。其中,g<sub>b</sub>,σ<sub>b</sub>分别表示第b个高斯分布样本的类标函数和带宽因子。于是,对于多通道卷积特征图<b><i>x</i></b>∈<i>R</i><sup><i>M</i>×<i>N</i>×<i>D</i></sup>,构造融合多层卷积特征相关滤波分类器<b><i>W</i></b><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>b</mi></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="85"><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>b</mi><mi>d</mi></msubsup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mi>b</mi></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>d</mi></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msup><mrow></mrow><mi>i</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>i</mi></msup><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="87">对于给定目标候选区域的傅立叶变换后的<i>l</i>层<i>d</i>通道特征<b><i>Z</i></b><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup></mrow></math></mathml>,计算第<i>b</i>个相关滤波分类器的响应图<i>f</i><sub><i>b</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>b</mi></msub><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msubsup><mrow></mrow><mi>b</mi><mi>d</mi></msubsup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Ζ</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>l</mi><mi>d</mi></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="91">在相关响应图<i>f</i><sub><i>b</i></sub>中寻找最大响应值可得第<i>b</i>个分类器预测的目标位置<i>p</i><sub><i>b</i></sub>,即为目标预测位置。</p>
                </div>
                <div class="p1">
                    <p id="92">由于目标在运动过程中会发生尺度变化,因此采用文献<citation id="131" type="reference">[<a class="sup">6</a>]</citation>中的尺度金字塔策略对目标进行尺度估计。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>2.3</b> 模型更新</h4>
                <div class="p1">
                    <p id="94">通常情况下,当目标发生严重遮挡或者相似背景干扰时,频繁更新滤波器模板易造成模型漂移;若不更新或者更新间隔太大以致于模型跟不上目标变化的速度,会导致跟踪失败。经过大量实验,本文选用间隔4帧的稀疏模型更新策略。令<b><i>A</i></b><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>和<b><i>B</i></b><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>分别表示无类标函数滤波器因式<b><i>Q</i></b><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>的分子和分母,则每隔4帧对<b><i>Q</i></b><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow></math></mathml>进行更新</p>
                </div>
                <div class="area_img" id="99">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201909004_09900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="101">由于尺度变化较快,尺度滤波器在每帧视频序列都进行更新。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag"><b>3</b> 实验结果</h3>
                <h4 class="anchor-tag" id="103" name="103"><b>3.1</b> 实验平台及评估指标</h4>
                <div class="p1">
                    <p id="104">为充分验证本文跟踪算法的鲁棒性和尺度估计的准确性,在标准测试数据集OTB100<citation id="132" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>上进行实验评估,涉及尺度、姿态变化、遮挡、光照等11种干扰因素。</p>
                </div>
                <div class="p1">
                    <p id="105">本文算法的实验平台为Ubuntu16.04系统下的MATLAB R2015b,所有的实验均在配置为Intel Core i7-7800XCPU,GTX1080Ti GPU,内存为16 GB的台式电脑上完成。具体参数设置为: 正则化参数<i>λ</i>=1e-4,学习率<i>η</i>=1e-2,样本分布类标函数带宽因子为<i>σ</i><sub>1</sub>=0.1、<i>σ</i><sub>2</sub>=0.15和<i>σ</i><sub>3</sub>=0.2,尺度相关滤波器相关参数与DSST算法<citation id="133" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的设置相同。</p>
                </div>
                <div class="p1">
                    <p id="106">采用中心位置误差(Average Center Location Error,CLE)、距离精度(Distance Precision,DP)和重叠精度(Overlap Precision,OP)作为评价指标对跟踪的效果进行评估。CLE为检测到的跟踪目标中心位置与标准目标中心位置之间的欧式距离,单位为像素;DP为中心位置误差小于某一阈值(20个像素)的帧数占视频总帧数的百分比;OP为边界框重叠部分面积超过某一阈值(0.5)的帧数占视频总帧数的百分比。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>3.2</b> 跟踪性能对比</h4>
                <div class="p1">
                    <p id="108">选取HCFT<citation id="134" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、MSDAT<citation id="135" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、HDT<citation id="136" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、DeepSRDCF<citation id="137" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、KCF<citation id="138" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、SRDCF<citation id="139" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、SAMF<citation id="140" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、DSST<citation id="141" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Staple<citation id="142" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>这9种表现优异的目标跟踪算法进行跟踪性能对比。其中 HCFT、MSDAT、HDT、DeepSRDCF为基于深度特征的跟踪算法,KCF、SRDCF、SAMF、DSST、Staple为基于相关滤波的跟踪算法。实验测试了OTB100的数据集上的平均CLE,平均DP,平均OP和平均速度,结果如表1所示。本文算法的平均DP、OP、CLE均为最优,平均距离精度较HCFT目标跟踪算法在OTB100标准数据集上提高了5.1个百分点;平均CLE减小了10.9个百分点。就速度而言,较HCFT增加了37.1帧/s,提高了约4倍。</p>
                </div>
                <div class="area_img" id="109">
                                            <p class="img_tit">
                                                <b>表</b>1 10<b>种算法平均</b>DP、OP、CLE<b>及速度对比实验</b>*
                                                    <br />
                                                Table 1. Comparisons of average DP、OP、CLE and speed for 10 algorithms
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909004_10900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/DZKK201909004_10900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909004_10900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 10种算法平均DP、OP、CLE及速度对比实验*" src="Detail/GetImg?filename=images/DZKK201909004_10900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>
                                <p class="img_note">*测试的9种算法速度均为原论文中给出的。</p>

                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>3.3</b> 鲁棒性评估</h4>
                <div class="p1">
                    <p id="111">为了评估算法的鲁棒性,在OTB100上测试表1中排名前4的算法在不同中心位置误差的精度曲线和不同重叠率下的成功率曲线,实验结果如图3所示。由图可知,本文算法在OTB100上的一次通过评估(One Pass Evaluation,OPE)精确度和成功率分别为88.8% 和64.1%,均为最优,相比HCFT算法分别提高了6.1%和14.1%。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909004_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 4种算法精确度(左)和成功率(右)对比曲线" src="Detail/GetImg?filename=images/DZKK201909004_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 4种算法精确度(左)和成功率(右)对比曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909004_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Comparisons of precision (left) and success
 rate (right) for 4 algorithms</p>

                </div>
                <div class="p1">
                    <p id="113">图4给出了DSST、HCFT、KCF和本文4种算法在4组包含多个挑战的代表性视频序列上的部分跟踪结果。图4(a)为Couple视频部分跟踪结果。随着目标尺度和运动的变化,至DSST、KCF相继丢失目标。而后目标剧烈运动且发生遮挡,HCFT定位偏差,虽完成跟踪,但较之本文算法不够准确。图4(b)为Jumping视频部分跟踪结果。该视频为灰度图像序列,且目标快速运动造成运动模糊、图像分辨率低等特点。DSST和KCF算法为传统手工特征,因此跟踪失败。图4(c)为CarScale视频部分跟踪结果,随着目标不断运动,尺度变化较为明显。本文算法可以更好地定位目标并适应目标的快速尺度变化。图4(d)为Diving视频部分跟踪结果。目标发生非刚性形变和旋转,DSST、KCF跟踪器出现漂移,在160帧丢失目标。随着目标尺度的不断变化,HCFT跟踪器定位稍有偏差,但仍与本文算法一起完成了跟踪。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201909004_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 4种跟踪算法的部分跟踪结果对比" src="Detail/GetImg?filename=images/DZKK201909004_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 4种跟踪算法的部分跟踪结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201909004_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Comparisons of tracking results for 4 algorithms</p>

                </div>
                <h3 id="115" name="115" class="anchor-tag"><b>4</b> 结束语</h3>
                <div class="p1">
                    <p id="116">本文选取VGG-19网络中的Pool4和Conv5-3两层卷积特征,采用特征能量均值比对冗余的通道进行自适应裁剪,提取出对跟踪任务有效的卷积特征,降低计算的复杂度,搭配稀疏模型更新策略来提高算法的跟踪速度。利用多个不同高斯分布的类标签函数训练多个相关滤波分类器,并通过峰值旁瓣比对每个滤波器跟踪结果的置信度进行判断,选择最适滤波器进行跟踪,提高对目标姿态变化的鲁棒性。在公开的标准数据集OTB100上与其它9种算法进行多组对比实验,验证本算法取得了良好的跟踪效果和实时性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[1]</b> Henriques J F,Rui C,Martins P.Exploiting the circulant structure of tracking-by-detection with kernels[C].Heidelberg:European Conference on Computer Vision,Springer,2012.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[2]</b> Henriques J F,Rui C,Martins P.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2014,37(3):583-596.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201706012&amp;v=MTA5NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXpoVTd6TUx6N0JhTEc0SDliTXFZOUVab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 熊昌镇,赵璐璐,郭芬红.自适应特征融合的核相关滤波跟踪算法[J].计算机辅助设计与图形学学报,2017,29(6):1068-1074.Xiong Changzhen,Zhao Lulu,Guo Fenhong.Kernelized correlation filters tracking based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp; Computer Graphics,2017,29(6):1068-1074.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[4]</b> Bertinetto L,Valmadre J,Goldetz S.Staple:complementary learners for real-time tracking[C].Washington D C:IEEE International Conference on Computer Vision and Pattern Recognition,2016.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[5]</b> Danellijan M,Hager G,Khan F.Accurate scale estimation for robust visual tracking[C].Nottingham:British Machine Vision Conference,2014.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201804025&amp;v=MDA1ODJDVVI3cWZadVpwRnl6aFU3ek1JVGZBWmJHNEg5bk1xNDlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 贾建兵,廖嘉伟,周羽.基于EK算法改进的多目标跟踪技术[J].电子科技,2018,31(4):91-96.Jia Jianbing,Liao Jiawei,Zhou Yu.Multitarget tracking technique based on Extended Kalman algorithm[J].Electronic Science and Technology,2018,31(4):91-96.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[7]</b> Danellijan M,Gustav H,Fahad S.Learning spatially regularized correlation filters for visual tracking[C].Washington D C:IEEE International Conference on Computer Vision,2015.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Features for Correlation Filter Based Visual Tracking">

                                <b>[8]</b> Danellijan M,Hager G,Khan F S,et al.Convolutional features for correlation filter based visual tracking[C].Washington D C:IEEE International Conference on Computer Vision Workshop,2016.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond correlation filters:Learning continuous convolution operators for visual tracking">

                                <b>[9]</b> Danellijan M,Robinson A,Khan F S.Beyond correlation filters:learning continuous convolution operators for visual tracking[C].Cham:European Conference on Computer Vision,2016.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ECO:Efficient Convolution Operators for Tracking">

                                <b>[10]</b> Danellijan M,Bhat G,Khan F,et al.ECO:Efficient Convolution Operators for tracking[C].Washington D C:Computer Vision and Pattern Recognition,2017.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical convolutional features for visual tracking">

                                <b>[11]</b> Ma C,Huang J,Yanf X,et al.Hierarchical convolutional features for visual tracking[C].Washington D C:Computer Vision and Pattern Recognition,2015.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and real-time deep tracking via multi-scale domain adaptation">

                                <b>[12]</b> Wang X,Li H,Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washington D C:IEEE International Conference on Multimedia and Expo,2017.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[13]</b> Simonyan K,Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science,2014(7):633-638.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Wu Y,Lim J,Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2015,37(9):1834-1848.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Qi Y,Zhang S,Qin L,et al.Hedged deep tracking[C].Las Vegas:IEEE Conference on Computer Vision and Pattern Recognition,2016.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">

                                <b>[16]</b> Li Y,Zhu J.A scale adaptive kernel correlation filter tracker with feature integration[J].Springer,2014,8926:254-265.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201909004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201909004&amp;v=MzI0NTZVN3pNSVRmQVpiRzRIOWpNcG85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5emg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

