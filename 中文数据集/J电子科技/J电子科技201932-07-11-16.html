

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139237664326250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201907004%26RESULT%3d1%26SIGN%3dJr%252bfnfr1EKOufAK7JSQUElV%252b2sQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201907004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201907004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201907004&amp;v=MjAxNTJGeS9rVzd6UElUZkFaYkc0SDlqTXFJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;2 相关滤波器&lt;/b&gt; "><b>2 相关滤波器</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="&lt;b&gt;3 基于单层卷积特征的实时鲁棒目标跟踪&lt;/b&gt; "><b>3 基于单层卷积特征的实时鲁棒目标跟踪</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="&lt;b&gt;3.1 单层卷积特征与带宽调整&lt;/b&gt;"><b>3.1 单层卷积特征与带宽调整</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;3.2 自适应模型更新&lt;/b&gt;"><b>3.2 自适应模型更新</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="&lt;b&gt;4 实验结果分析&lt;/b&gt; "><b>4 实验结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="&lt;b&gt;4.1 实验环境和参数设置&lt;/b&gt;"><b>4.1 实验环境和参数设置</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;4.2 定量分析&lt;/b&gt;"><b>4.2 定量分析</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;4.3 定性分析&lt;/b&gt;"><b>4.3 定性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#100" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="图1 本文算法流程图">图1 本文算法流程图</a></li>
                                                <li><a href="#73" data-title="图2 不同带宽因子对卷积特征层的影响">图2 不同带宽因子对卷积特征层的影响</a></li>
                                                <li><a href="#77" data-title="图3 Girl2序列图像与响应图">图3 Girl2序列图像与响应图</a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同干扰因素下&lt;/b&gt;OPE&lt;b&gt;精确度和重叠率对比&lt;/b&gt;"><b>表</b>1 <b>不同干扰因素下</b>OPE<b>精确度和重叠率对比</b></a></li>
                                                <li><a href="#94" data-title="图4 OPE精确度和成功率对比">图4 OPE精确度和成功率对比</a></li>
                                                <li><a href="#99" data-title="图5 6种算法的跟踪效果对比">图5 6种算法的跟踪效果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Rui C, Martins P, Batista J.Exploiting the circulant structure of tracking-by-detection with kernels[C].Heidelberg:European Conference on Computer Vision, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the Circulant Structure of Tracking-by-Detection with Kernels">
                                        <b>[1]</b>
                                         Rui C, Martins P, Batista J.Exploiting the circulant structure of tracking-by-detection with kernels[C].Heidelberg:European Conference on Computer Vision, 2012.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Henriques J F, Caseiro R, Martins P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37 (3) :583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[2]</b>
                                         Henriques J F, Caseiro R, Martins P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37 (3) :583-596.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Danelljan M, H&#228;ger G, Khan F S, et al.Accurate scale estimation for robust visual tracking[C].Nottingham:British Machine Vision Conference, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">
                                        <b>[3]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, et al.Accurate scale estimation for robust visual tracking[C].Nottingham:British Machine Vision Conference, 2014.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Bertinetto L, Valmadre J, Goldetz S.Staple:Complementary learners for real-time tracking[C].Washing DC:IEEE International Conference on Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:complementary learners for real-time tracking">
                                        <b>[4]</b>
                                         Bertinetto L, Valmadre J, Goldetz S.Staple:Complementary learners for real-time tracking[C].Washing DC:IEEE International Conference on Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C].Las Vegas:IEEE International Conference on Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">
                                        <b>[5]</b>
                                         Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C].Las Vegas:IEEE International Conference on Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Danelljan M, H&#228;ger G, Khan F S, et al.Convolutional features for correlation filter based visual tracking[C].Washing DC:IEEE International Conference on Computer Vision Workshop, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Features for Correlation Filter Based Visual Tracking">
                                        <b>[6]</b>
                                         Danelljan M, H&#228;ger G, Khan F S, et al.Convolutional features for correlation filter based visual tracking[C].Washing DC:IEEE International Conference on Computer Vision Workshop, 2016.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Fan H, Ling H.SANet:Structure-aware network for visual tracking[C].Washing DC:CVPR Deep Vision Workshop, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SANet:Structure-aware network for visual tracking">
                                        <b>[7]</b>
                                         Fan H, Ling H.SANet:Structure-aware network for visual tracking[C].Washing DC:CVPR Deep Vision Workshop, 2016.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Ma C, Huang J, Yang X, et al.Hierarchical convolutional features for visual tracking[C].Washing DC:Computer Vision and Pattern Recognition, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">
                                        <b>[8]</b>
                                         Ma C, Huang J, Yang X, et al.Hierarchical convolutional features for visual tracking[C].Washing DC:Computer Vision and Pattern Recognition, 2015.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Wang X, Li H, Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washing DC:IEEE International Conference on Multimedia and Expo, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust and real-time deep tracking via multi-scale domain adaptation">
                                        <b>[9]</b>
                                         Wang X, Li H, Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washing DC:IEEE International Conference on Multimedia and Expo, 2017.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 蔡玉柱, 杨德东, 毛宁, 等.基于自适应卷积特征的目标跟踪算法[J].光学学报, 2017 (3) :262-273.Cai Yuzhu, Yang Dedong, Mao Ning, et al.Visual tracking algorithm based on adaptive convolutional features[J].Acta Optica Sinica, 2017 (3) :262-273." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703032&amp;v=Mjg2MTh6UElqWFRiTEc0SDliTXJJOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeS9rVzc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         蔡玉柱, 杨德东, 毛宁, 等.基于自适应卷积特征的目标跟踪算法[J].光学学报, 2017 (3) :262-273.Cai Yuzhu, Yang Dedong, Mao Ning, et al.Visual tracking algorithm based on adaptive convolutional features[J].Acta Optica Sinica, 2017 (3) :262-273.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Danelljan M, Gustav H, Fahad S.Learning spatially regularized correlation filters for visual tracking[C].Santiago:IEEE International Conference on Computer Vision, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[11]</b>
                                         Danelljan M, Gustav H, Fahad S.Learning spatially regularized correlation filters for visual tracking[C].Santiago:IEEE International Conference on Computer Vision, 2015.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Simonyan K, Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science, 2014 (7) :78-86." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[12]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science, 2014 (7) :78-86.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Wang M, Liu Y, Huang Z.Large margin object tracking with circulant feature maps[C].Hawaii:IEEE International Conference on Computer Vision and Pattern Recognition, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[13]</b>
                                         Wang M, Liu Y, Huang Z.Large margin object tracking with circulant feature maps[C].Hawaii:IEEE International Conference on Computer Vision and Pattern Recognition, 2017.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Wu L, Lim J, Yang M.Online object tracking:a benchmark[C].Portland:Computer Vision and Pattern Recognition, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online Object Tracking:A Benchmark">
                                        <b>[14]</b>
                                         Wu L, Lim J, Yang M.Online object tracking:a benchmark[C].Portland:Computer Vision and Pattern Recognition, 2013.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Wu Y, Lim J, Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2015, 37 (9) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[15]</b>
                                         Wu Y, Lim J, Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2015, 37 (9) :1834-1848.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Qi Y, Zhang S, Qin L.Hedged deep tracking[C].Las Vegas:IEEE Conference on Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hedged Deep Tracking">
                                        <b>[16]</b>
                                         Qi Y, Zhang S, Qin L.Hedged deep tracking[C].Las Vegas:IEEE Conference on Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Zhang J, Ma S, Schlaroff S.MEEM:Robust tracking via multiple experts using entropy minimization[C].Cham:European Conference on Computer Vision, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MEEM:robust tracking via multiple experts using entropy minimization">
                                        <b>[17]</b>
                                         Zhang J, Ma S, Schlaroff S.MEEM:Robust tracking via multiple experts using entropy minimization[C].Cham:European Conference on Computer Vision, 2014.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Ning J, Yang J, Jiang S.Object tracking via dual linear structured SVM and explicit feature map[C].Las Vegas:Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object Tracking via Dual Linear Structured SVM and Explicit Feature Map">
                                        <b>[18]</b>
                                         Ning J, Yang J, Jiang S.Object tracking via dual linear structured SVM and explicit feature map[C].Las Vegas:Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Li Y, Zhu J.A scale adaptive kernel correlation filter tracker with feature integration[J].Springer, 2014, 8926:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">
                                        <b>[19]</b>
                                         Li Y, Zhu J.A scale adaptive kernel correlation filter tracker with feature integration[J].Springer, 2014, 8926:254-265.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Hong S, You T, Kwak S.Online tracking by learning discriminative saliency map with convolutional neural network[J].Computer Science, 2015 (6) :597-606." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network">
                                        <b>[20]</b>
                                         Hong S, You T, Kwak S.Online tracking by learning discriminative saliency map with convolutional neural network[J].Computer Science, 2015 (6) :597-606.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-20 15:17</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(07),11-16 DOI:10.16180/j.cnki.issn1007-7820.2019.07.003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于自适应模型更新的实时跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%BB%95%E7%A1%95&amp;code=40933475&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">滕硕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%B6%A6%E7%8E%B2&amp;code=39828539&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王润玲</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E6%96%B9%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0226398&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北方工业大学理学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提高分层卷积特征目标跟踪算法的速度和精度, 文中提出了一种基于自适应模型更新的单层卷积特征目标跟踪算法。首先提取Pool4层的多通道的卷积特征对训练样本的类标函数进行调整, 在确保跟踪精确度的同时提高了算法的速度。该算法引入了平均峰值能量比, 通过比值变化情况反馈目标跟踪的结果, 与稀疏模型更新策略相结合, 对跟踪器进行自适应更新, 提高了算法对遮挡和相似物干扰的鲁棒性。对于目标快速尺度变化问题, 文中采用尺度金字塔对尺度进行评估, 提高了跟踪器的泛化能力。在OTB2013和OTB2015上测试新算法, 实验结果表明, 该算法的平均距离精度分别为91.0%和86.8%, 平均速度约43帧/s, 局域良好的鲁棒性和实时性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型更新;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B3%E5%9D%87%E5%B3%B0%E5%80%BC%E8%83%BD%E9%87%8F%E6%AF%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平均峰值能量比;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B1%BB%E6%A0%87%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">类标函数;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    滕硕 (1994-) , 女, 硕士研究生。研究方向:图像处理。;
                                </span>
                                <span>
                                    王润玲 (1991-) , 女, 硕士研究生。研究方向:深度学习, 图像处理, 视频分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFC0821102);</span>
                    </p>
            </div>
                    <h1><b>Real-time Tracking Algorithm Based on Adaptive Model Update</b></h1>
                    <h2>
                    <span>TENG Shuo</span>
                    <span>WANG Runling</span>
            </h2>
                    <h2>
                    <span>School of Sciences, North China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To improve the speed and accuracy of hierarchical convolutional features for visual tracking method, a real-time and robust object tracking algorithm based on adaptive model update and single-layer convolutional features was proposed. This method firstly extracted the multi-channel convolutional features of the Pool4 layer to adjust the label function of the training samples, which improved the speed of the algorithm while ensuring the tracking accuracy.Meanwhile, the average peak-to-correlation energy was introduced in the proposed algorithm, which feedbacked the tracking results. Combined with the sparse model update strategy, the tracker was adaptively updated to improve the robustness of the algorithm to occlusion and similar object interference. For the problem of rapid scale variation, the scale pyramid was adopted to evaluate the scale to further improve the generalization ability of our tracker. Finally, the algorithm was verified on OTB2013 and OTB2015 benchmark datasets. The experimental results showed that the average distance precision was 91.0% and 86.8%, and the average speed was 43 frames per second, showing outperforming robustness and real-time performances.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20update&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model update;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=average%20peak-to-correlation%20energy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">average peak-to-correlation energy;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label function;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-29</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>National Key R&amp;D Program of China (2017YFC0821102);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="43">视觉跟踪通过对第一帧视频信息进行分析处理, 找到后续帧中的运动目标并标记出来。 随着研究者对视觉跟踪算法的深入研究, 其科学理论体系日趋完善, 极大地促进了视觉导航、智能交通等方面的发展, 也在多领域得到了广泛的应用。</p>
                </div>
                <div class="p1">
                    <p id="44">相关滤波目标跟踪算法利用两个信号之间的响应来定位目标中心。其在傅里叶域进行求解变换, 耗时较少, 因此备受研究者的青睐。Henriques等<citation id="102" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>利用循环矩阵对相邻帧进行相关性检测, 发现其实时效果良好, 但简单的灰度特征表征目标表观易造成跟踪失败。于是, 将单一特征扩展到多通道的方向梯度直方图特征 (Histogram of Oriented Gradient, HOG) , 提高了算法的鲁棒性<citation id="103" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。Danelljan等<citation id="104" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>针对跟踪过程中目标快速尺度变化的问题, 提出尺度金字塔策略, 提高了跟踪器的泛化能力。Bertinetto等人<citation id="105" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>利用颜色和HOG特征在对物体形变和颜色变化跟踪效果上的互补性来进行目标定位, 进一步提高了算法的鲁棒性, 且实时效果良好。2013年以来, 深度学习的方法被引入到目标跟踪领域, 并逐渐在跟踪性能上取得了巨大的突破, 涌现出如MDNet<citation id="106" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、DeepSRDCF<citation id="107" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、SANet<citation id="108" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等卷积特征目标跟踪算法。这些新算法虽然精确度高, 但其速度非常差, 无法满足应用需求。</p>
                </div>
                <div class="p1">
                    <p id="45">手工特征相关滤波跟踪算法实时性能良好, 但是表征目标表观具有局限性;深度卷积特征虽然可以很好地适应目标的各种变化, 但是速度较慢。于是, 研究者们充分利用相关滤波在跟踪速度上的优势以及卷积特征对目标表观变化强大的适应能力来改善跟踪算法的实时性和实效性。Ma等<citation id="109" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出分层卷积特征目标跟踪算法, 将含有较多纹理信息的低层卷积特征和含有丰富的语义信息的高层卷积特征输入相关滤波器来定位目标, 取得了很好的效果。但是, 该算法速度较慢且对目标的尺度变化问题效果较差。Wang等<citation id="110" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>对该方法进行了改进, 训练了多尺度域的相关滤波器, 并对原卷积特征降维, 大幅度提高了跟踪的速度。Cai等<citation id="111" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>针对空间正则化相关滤波跟踪算法<citation id="112" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对旋转、严重遮挡情况不鲁棒的问题, 提出了一种基于自适应卷积特征的目标跟踪算法, 并训练在线SVM分类器对目标跟踪失败的情况进行重新检测, 提高了算法的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="46">本文在分层卷积特征相关滤波目标跟踪算法<citation id="113" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的框架下, 做了以下改进: (1) 为了提高跟踪算法的实时性、减少卷积特征提取层数, 仅采用多通道的单层深度卷积特征, 并调整训练样本分布的类标函数以确保跟踪精确度的同时提高算法速度, 使其达到实时跟踪的要求; (2) 向在线更新阶段引入高置信度模型更新与稀疏模型更新策略对模型进行自适应更新, 提高跟踪模型对遮挡和相似物体干扰的鲁棒性。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>2 相关滤波器</b></h3>
                <div class="p1">
                    <p id="48">相关滤波的跟踪算法利用快速傅里叶变换进行滤波器训练和响应图计算, 具有很好的跟踪实时性。所以, 本文将相关滤波与分层卷积特征进行结合来提高算法的精度和速度。将输入特征<b><i>x</i></b>∈<i>R</i><sup><i>M</i>×<i>N</i>×<i>D</i></sup>进行循环移位, 记其单训练样本为<b><i>x</i></b><sub><i>m</i>, <i>n</i></sub>, 通过最小化输出误差得到最优相关滤波器</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">W</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>min</mi></mrow></mstyle><mi>w</mi></munder><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false">∥</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>d</mi></msubsup><mo>-</mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">式中, <i>λ</i>≥0为正则化参数, <i>g</i> (<i>m</i>, <i>n</i>) 为峰值在<b><i>x</i></b><sub><i>m</i>, <i>n</i></sub>中心位置的二维高斯核函数。令</p>
                </div>
                <div class="p1">
                    <p id="51"><mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ζ</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mo stretchy="false">∥</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">h</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow><mi>d</mi></msubsup><mo>-</mo><mi>g</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">h</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="53">则其频域表示</p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ζ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Μ</mi><mi>Ν</mi></mrow></mfrac><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>d</mi></msup><mo>-</mo><mi mathvariant="bold-italic">G</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>d</mi></msup><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="56">其中, <b><i>X</i></b>、<b><i>G</i></b>和<b><i>W</i></b>分别表示<b><i>x</i></b>、<i>g</i>和<b><i>w</i></b>的离散傅里叶变换 (Discrete Fourier Transform, <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>D</mtext><mtext>F</mtext><mtext>Τ</mtext><mo stretchy="false">) </mo><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover></mrow></math></mathml>为<b><i>X</i></b>的共轭复数;⊙即对应元素相乘。于是, 通道<i>d</i>∈{1, 2, …, <i>D</i>}上的最优滤波器为</p>
                </div>
                <div class="p1">
                    <p id="58"><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>d</mi></msup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">G</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>d</mi></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msup><mrow></mrow><mi>i</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msup><mrow></mrow><mi>i</mi></msup><mo>+</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="60">因此, 给定目标候选区域的多维特征图<i>z</i>, 其DFT变换为<b><i>Z</i></b>, 可以得到相关响应图<b><i>f</i></b></p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><mo>=</mo><mi mathvariant="bold-italic">F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msup><mrow></mrow><mi>d</mi></msup><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">Ζ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="63"><b><i>F</i></b><sup>-1</sup>表示DFT的逆变换。在相关响应图<b><i>f</i></b>中寻找最大响应值即可以得到目标的预测位置。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag"><b>3 基于单层卷积特征的实时鲁棒目标跟踪</b></h3>
                <div class="p1">
                    <p id="65">本文算法的流程图如图1所示。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201907004_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法流程图" src="Detail/GetImg?filename=images/DZKK201907004_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201907004_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Framework of the proposed algorithm</p>

                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>3.1 单层卷积特征与带宽调整</b></h4>
                <div class="p1">
                    <p id="68">在训练分类器时, 样本的类标函数符合二维高斯分布</p>
                </div>
                <div class="p1">
                    <p id="69"><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>-</mo><mi>Μ</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>Ν</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">) </mo></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="71">其中, σ为带宽因子, 决定样本的分布状况, 通常设为固定值0.1。带宽因子值越大, 样本分布越分散, 样本间的差异性越大, 此时分类器对输入图像中的相似物体与目标物体的判别性越强。</p>
                </div>
                <div class="p1">
                    <p id="72">因此, 考虑不同视频序列样本间的差异性, 本文对不同高斯样本分布与<i>VGG</i>-19网络<citation id="114" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>不同层卷积特征提取的联系进行了研究。如图2所示, 图2 (<i>a</i>) 为<i>Basketball</i>视频序列的图像;图2 (<i>b</i>) 为输入图像的可视化图;图2 (<i>c</i>) 为带宽因子分别取值0.1、0.15和0.2时的样本分布对不同层卷积特征响应图的影响的对比结果。显然, σ=0.15且卷积特征提取层为<i>Pool</i>4时效果最好。因此, 选取<i>Pool</i>4层作为特征提取层, 带宽因子为0.15的样本分布类标函数用于目标跟踪。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201907004_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同带宽因子对卷积特征层的影响" src="Detail/GetImg?filename=images/DZKK201907004_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同带宽因子对卷积特征层的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201907004_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Influence of different bandwidth factors on convolutional layers</p>
                                <p class="img_note"> (a) 原始图像 (b) 输入图像 (c) 由左至右分别为C4-3、C4-4、Pool4、C5-1、C5-2</p>
                                <p class="img_note"> (a) Original image (b) Input image (c) From left to right, C4-3, C4-4, Pool4, C5-1, C5-2</p>

                </div>
                <div class="p1">
                    <p id="74">在保证跟踪精确度的同时, 减少特征提取的层数以提高算法的速度。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.2 自适应模型更新</b></h4>
                <div class="p1">
                    <p id="76">在模型跟踪过程中, 目标表观和尺度都会发生变化。如图3所示, 图3 (a) 为Girl2视频序列的某帧图像, 白色和黑色包围框中分别为目标和背景相似物。当目标物体受到背景中的相似物体干扰时, 响应图出现多峰情况, 干扰物的响应峰值可能会比目标的响应峰值还大 (图3 (b) ) , 而理想的响应输出应为单峰且周围平滑 (图3 (c) ) 。此时若更新跟踪器, 则可能会使模型产生漂移甚至丢失目标。为了适应跟踪模型的变化, 文中引入最大响应值和平均峰值能量比 (Average Peak-to-Correlation Energy, APCE) <citation id="115" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>来反映响应图的震荡程度和目标预测位置的置信度, 如式 (7) 所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201907004_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Girl2序列图像与响应图" src="Detail/GetImg?filename=images/DZKK201907004_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Girl2序列图像与响应图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201907004_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Sequence image and response map of Girl2</p>
                                <p class="img_note"> (a) 视频图像 (b) 响应图 (c) 理想响应输出</p>
                                <p class="img_note"> (a) Video image (b) Response map (c) Expected response output</p>

                </div>
                <div class="p1">
                    <p id="78">apce<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>w</mi><mo>, </mo><mi>h</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>F</mi><msub><mrow></mrow><mrow><mi>w</mi><mo>, </mo><mi>h</mi></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>-</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="80">其中, <i>F</i><sub>max</sub>、<i>F</i><sub>min</sub>和<i>F</i><sub><i>w</i>, <i>h</i></sub>分别表示最大响应、最小响应以及当前帧响应值。</p>
                </div>
                <div class="p1">
                    <p id="81">对于目标明显或受干扰小的图像, 响应图呈现单峰或者噪声响应较小, apce的值将会变大;而当目标发生遮挡、相似背景干扰甚至丢失时, 当前帧与历史帧apce值将会显著减小, 最大响应值也会变化较大。于是, 只有当前帧与历史所有帧的最大响应值和apce值分别小于给定阈值时, 对跟踪模型进行更新, 以提高对相似背景和遮挡的鲁棒性</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mo>+</mo><mi>η</mi><mi mathvariant="bold-italic">Y</mi><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">B</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">B</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mo>+</mo><mi>η</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup></mrow><mrow><mi mathvariant="bold-italic">B</mi><msubsup><mrow></mrow><mi>t</mi><mi>d</mi></msubsup><mo>+</mo><mi>λ</mi></mrow></mfrac></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中, <i>η</i>为跟踪器的学习率。</p>
                </div>
                <div class="p1">
                    <p id="84">与此同时, 为防止过拟合现象以及进一步保证跟踪模型能够及时更新, 文中采用每隔几帧更新的稀疏模型更新策略与自适应更新同时进行, 有效防止了模型的漂移。由于目标尺度变化较快, 尺度滤波器在每帧视频序列都药进行更新。</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag"><b>4 实验结果分析</b></h3>
                <div class="p1">
                    <p id="86">为验证本文算法的有效性, 利用OTB2013<citation id="116" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和OTB2015<citation id="117" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>标准数据集进行算法评估, 并与HDT<citation id="118" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、MEEM<citation id="119" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、DLSSVM<citation id="120" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、KCF<citation id="121" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、 SRDCF<citation id="122" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、SAMF<citation id="123" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、Staple<citation id="124" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、DSST<citation id="125" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、CF2<citation id="126" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、 MSDAT<citation id="127" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和CNN-SVM<citation id="128" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>等11种优秀算法进行对比实验。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>4.1 实验环境和参数设置</b></h4>
                <div class="p1">
                    <p id="88">本文算法的实验平台为Ubuntu16.04系统下的MATLAB R2015b, 所有的实验均在配置为Intel Core i7-7800XCPU, GTX1080Ti GPU, 内存为16 GB的台式电脑上完成。 算法的具体参数设置为:正则化参数<i>λ</i>=1e<sup>-4</sup>, 学习率<i>η</i>=1e<sup>-2</sup>, 高斯分布样本带宽因子<i>σ</i>=0.15, 自适应模型更新阶段最大响应值与apce的阈值分别为0.3和0.6, 稀疏模型更新间隔为3, 尺度相关滤波器相关参数与DSST算法<citation id="129" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的设置相同。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>4.2 定量分析</b></h4>
                <div class="p1">
                    <p id="90">在OTB2015上测试本文算法并与其他11种算法进行对比。</p>
                </div>
                <div class="p1">
                    <p id="91">表1给出了11种不同干扰因素下一次通过评估 (One Pass Evaluation, OPE) 精确度和重叠率对比结果, 效果最好的用粗体表示, 排名第2的用下划线标出。 从表1可以看出本文算法平均精确度和重叠率均排名第一, 较之CF2分别提高2.0个百分点和2.1个百分点, 在遮挡 (Occlusion, OCC) 、非刚性形变 (Deformation, DEF) 、尺度变化 (Scale Variation, SV) 、旋转 (In-Plane/Out-of-Plane Rotation, IPR/OPR) 、背景混乱 (Background Clutters, BC) 、光照变化 (Illumination Variation, IV) 、运动模糊 (Motion Blur, MB) 和超出视野 (Out-of-View, OV) 等方面均具有优势。对其他因素如快速运动 (Fast Motion, FM) 和低分辨率 (Low Resolution, LR) 等亦表现优良, 与CF2和MSDAT相比均有所提高, 有效提高了算法的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="92">图4显示了表1中精确度排名前4的目标跟踪算法在OTB2013和OTB2015上的OPE精确度分别为91.0%和86.8%, 优于其它11种算法;OPE成功率分别为65.9%和63.1%, 较之CF2算法分别提高了8.9%和12.3%, 表现出良好的跟踪性能。</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表</b>1 <b>不同干扰因素下</b>OPE<b>精确度和重叠率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Comparisons of precision and success rate on different attributes</p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td rowspan="2"><br />属性</td><td colspan="6"><br />精确度/%</td><td colspan="6">重叠率/%</td></tr><tr><td>Ours</td><td>HDT</td><td>CF2</td><td>MSDAT</td><td>CNN-SVM</td><td>SRDCF</td><td>Ours</td><td>HDT</td><td>CF2</td><td>MSDAT</td><td>CNN-SVM</td><td>SRDCF</td></tr><tr><td>IV</td><td>86.7</td><td>82.0</td><td>81.7</td><td><u>82.5</u></td><td>79.5</td><td>79.2</td><td>78.2</td><td>60.8</td><td>61.6</td><td>63.5</td><td>61.5</td><td><u>74.7</u></td></tr><tr><td><br />OPR</td><td>84.7</td><td>80.5</td><td><u>80.7</u></td><td>79.7</td><td>79.8</td><td>74.2</td><td>74.2</td><td>62.7</td><td>62.9</td><td>63.6</td><td>64.9</td><td><u>66.4</u></td></tr><tr><td><br />SV</td><td>82.8</td><td><u>80.8</u></td><td>79.9</td><td>77.1</td><td>78.7</td><td>74.5</td><td>71.1</td><td>51.4</td><td>51.9</td><td>50.8</td><td>52.9</td><td><u>66.7</u></td></tr><tr><td><br />OCC</td><td>79.4</td><td><u>77.4</u></td><td>76.7</td><td>74.0</td><td>73.0</td><td>73.5</td><td>70.2</td><td>61.1</td><td>60.6</td><td>59.7</td><td>60.6</td><td><u>68.4</u></td></tr><tr><td><br />DEF</td><td>83.1</td><td><u>82.1</u></td><td>79.1</td><td>79.2</td><td>79.3</td><td>73.4</td><td>71.7</td><td>61.8</td><td>60.3</td><td>60.4</td><td>63.4</td><td><u>66.7</u></td></tr><tr><td><br />MB</td><td>81.5</td><td>78.9</td><td><u>80.4</u></td><td>76.1</td><td>75.1</td><td>76.7</td><td>77.7</td><td>68.9</td><td>69.8</td><td>65.9</td><td>71.5</td><td><u>72.9</u></td></tr><tr><td><br />FM</td><td>79.9</td><td>81.7</td><td><u>81.5</u></td><td>74.4</td><td>74.7</td><td>76.9</td><td>73.9</td><td>66.4</td><td>66.8</td><td>63.4</td><td>64.9</td><td><u>71.7</u></td></tr><tr><td><br />IPR</td><td>87.8</td><td>84.4</td><td><u>85.4</u></td><td>85.4</td><td>81.3</td><td>74.5</td><td>76.0</td><td>65.7</td><td>66.2</td><td><u>67.6</u></td><td>65.7</td><td>66.2</td></tr><tr><td><br />OV</td><td>71.5</td><td>66.3</td><td><u>67.7</u></td><td>62.7</td><td>65.0</td><td>59.7</td><td>61.6</td><td>54.7</td><td>54.0</td><td>56.0</td><td><u>59.1</u></td><td>55.8</td></tr><tr><td><br />BC</td><td>86.6</td><td><u>84.4</u></td><td>84.3</td><td>83.3</td><td>77.6</td><td>77.5</td><td>77.8</td><td>71.3</td><td>72.1</td><td><u>72.5</u></td><td>68.1</td><td>70.1</td></tr><tr><td><br />LR</td><td>83.3</td><td><u>88.7</u></td><td>84.7</td><td>85.0</td><td>92.5</td><td>76.5</td><td><u>52.9</u></td><td>35.4</td><td>32.7</td><td>35.9</td><td>29.3</td><td>66.8</td></tr><tr><td><br />Overall</td><td>86.8</td><td><u>84.8</u></td><td>83.7</td><td>82.1</td><td>81.4</td><td>78.9</td><td>77.6</td><td>65.7</td><td>65.5</td><td>65.5</td><td>65.1</td><td><u>72.8</u></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201907004_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 OPE精确度和成功率对比" src="Detail/GetImg?filename=images/DZKK201907004_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 OPE精确度和成功率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201907004_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Comparisons of precision and success rate of OPE</p>
                                <p class="img_note"> (a) OTB2013 (b) OTB2015</p>
                                <p class="img_note"> (a) OTB2013 (b) OTB2015</p>

                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>4.3 定性分析</b></h4>
                <div class="p1">
                    <p id="96">本文还选取了4组包含多个挑战的代表性视频序列Girl2、Lemming、BlurCar2、MotorRolling进行分析。图6给出了KCF、DSST、Staple、CF2、MSDAT以及本文6种算法在4组视频序列上的部分跟踪结果, 其中KCF、DSST、Staple代表手工特征相关滤波算法;本文和MSDAT为CF2的改进算法, 是基于深度特征的相关滤波算法。</p>
                </div>
                <div class="p1">
                    <p id="97">图5 (a) 为Girl2部分跟踪结果:目标受到相似背景的干扰, KCF、DSST、Staple、MSDAT以及CF2算法均学习到错误的表观特征, 丢失目标。</p>
                </div>
                <div class="p1">
                    <p id="98">图5 (b) 为Lemming部分跟踪结果:目标发生严重遮挡, 其他跟踪器均定位错误, 导致目标丢失;而后目标回到原遮挡区域, DSST、MSDAT和KCF重新学习目标, 最终虽完成跟踪任务, 但定位结果均出现偏差。图5 (c) 为Freeman4部分跟踪结果:该视频为灰度图像序列, 且背景混乱较为明显。 使用本文和CF2算法可以很好地定位目标。 图5 (d) 为MotorRolling部分跟踪结果:目标物体在快速运动过程中发生旋转且受到光照变化的干扰, DSST、Staple和KCF逐渐丢失目标, CF2和MSDAT定位稍有偏差。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201907004_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 6种算法的跟踪效果对比" src="Detail/GetImg?filename=images/DZKK201907004_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 6种算法的跟踪效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201907004_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5. Comparisons of tracking results among six algorithms</p>
                                <p class="img_note"> (a) Girl2 (b) Lemming (c) Freeman4 (d) MotorRolling</p>
                                <p class="img_note"> (a) Girl2 (b) Lemming (c) Freeman4 (d) MotorRolling</p>

                </div>
                <h3 id="100" name="100" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="101">本文在分层卷积相关滤波目标跟踪算法的框架下, 选取VGG-19网络中的Pool4单层卷积特征用于跟踪, 有效地提升了目标跟踪算法的实时性, 并通过增大类标函数高斯带宽值来增加样本间的差异, 提高了分类器的学习效果。文中同时引入高置信度模型更新与稀疏模型更新策略对跟踪器进行自适应更新, 有效地提升了算法对遮挡和相似物干扰的鲁棒性。在公开的标准数据集OTB2013和OTB2015上进行验证, 证明本文算法在平均距离精度和成功率等指标上均优于其它算法, 且速度为43帧/s, 是原算法速度的4倍, 充分体现了本文算法的鲁棒性和实时性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the Circulant Structure of Tracking-by-Detection with Kernels">

                                <b>[1]</b> Rui C, Martins P, Batista J.Exploiting the circulant structure of tracking-by-detection with kernels[C].Heidelberg:European Conference on Computer Vision, 2012.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[2]</b> Henriques J F, Caseiro R, Martins P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2014, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate scale estimation for robust visual tracking">

                                <b>[3]</b> Danelljan M, Häger G, Khan F S, et al.Accurate scale estimation for robust visual tracking[C].Nottingham:British Machine Vision Conference, 2014.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:complementary learners for real-time tracking">

                                <b>[4]</b> Bertinetto L, Valmadre J, Goldetz S.Staple:Complementary learners for real-time tracking[C].Washing DC:IEEE International Conference on Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">

                                <b>[5]</b> Nam H, Han B.Learning multi-domain convolutional neural networks for visual tracking[C].Las Vegas:IEEE International Conference on Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Features for Correlation Filter Based Visual Tracking">

                                <b>[6]</b> Danelljan M, Häger G, Khan F S, et al.Convolutional features for correlation filter based visual tracking[C].Washing DC:IEEE International Conference on Computer Vision Workshop, 2016.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SANet:Structure-aware network for visual tracking">

                                <b>[7]</b> Fan H, Ling H.SANet:Structure-aware network for visual tracking[C].Washing DC:CVPR Deep Vision Workshop, 2016.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Convolutional Features for Visual Tracking">

                                <b>[8]</b> Ma C, Huang J, Yang X, et al.Hierarchical convolutional features for visual tracking[C].Washing DC:Computer Vision and Pattern Recognition, 2015.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust and real-time deep tracking via multi-scale domain adaptation">

                                <b>[9]</b> Wang X, Li H, Li Y.Robust and real-time deep tracking via multi-scale domain adaptation[C].Washing DC:IEEE International Conference on Multimedia and Expo, 2017.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201703032&amp;v=Mjk2NDk5Yk1ySTlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkva1c3elBJalhUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 蔡玉柱, 杨德东, 毛宁, 等.基于自适应卷积特征的目标跟踪算法[J].光学学报, 2017 (3) :262-273.Cai Yuzhu, Yang Dedong, Mao Ning, et al.Visual tracking algorithm based on adaptive convolutional features[J].Acta Optica Sinica, 2017 (3) :262-273.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[11]</b> Danelljan M, Gustav H, Fahad S.Learning spatially regularized correlation filters for visual tracking[C].Santiago:IEEE International Conference on Computer Vision, 2015.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[12]</b> Simonyan K, Zisserman A.Very deep convolutional net works for large-scale image recognition[J].Computer Science, 2014 (7) :78-86.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[13]</b> Wang M, Liu Y, Huang Z.Large margin object tracking with circulant feature maps[C].Hawaii:IEEE International Conference on Computer Vision and Pattern Recognition, 2017.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online Object Tracking:A Benchmark">

                                <b>[14]</b> Wu L, Lim J, Yang M.Online object tracking:a benchmark[C].Portland:Computer Vision and Pattern Recognition, 2013.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[15]</b> Wu Y, Lim J, Yang M.Object tracking benchmark[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2015, 37 (9) :1834-1848.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hedged Deep Tracking">

                                <b>[16]</b> Qi Y, Zhang S, Qin L.Hedged deep tracking[C].Las Vegas:IEEE Conference on Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MEEM:robust tracking via multiple experts using entropy minimization">

                                <b>[17]</b> Zhang J, Ma S, Schlaroff S.MEEM:Robust tracking via multiple experts using entropy minimization[C].Cham:European Conference on Computer Vision, 2014.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object Tracking via Dual Linear Structured SVM and Explicit Feature Map">

                                <b>[18]</b> Ning J, Yang J, Jiang S.Object tracking via dual linear structured SVM and explicit feature map[C].Las Vegas:Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A scale adaptive kernel correlation filter tracker with feature integration">

                                <b>[19]</b> Li Y, Zhu J.A scale adaptive kernel correlation filter tracker with feature integration[J].Springer, 2014, 8926:254-265.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network">

                                <b>[20]</b> Hong S, You T, Kwak S.Online tracking by learning discriminative saliency map with convolutional neural network[J].Computer Science, 2015 (6) :597-606.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201907004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201907004&amp;v=MjAxNTJGeS9rVzd6UElUZkFaYkc0SDlqTXFJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEkxbmYxYzdzR0dyZ1pjdDhmMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

