

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139162375732500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZKK201911004%26RESULT%3d1%26SIGN%3dK7ccyX0h0qG1C3MhDdjF5EZWCus%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201911004&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZKK201911004&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201911004&amp;v=MDE4NTlPSVRmQVpiRzRIOWpOcm85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5M2tVYnY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;1 算法描述&lt;/b&gt; "><b>1 算法描述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;1.1 初始匹配代价计算&lt;/b&gt;"><b>1.1 初始匹配代价计算</b></a></li>
                                                <li><a href="#52" data-title="&lt;b&gt;1.2 基于视差和灰度的双层支持窗口&lt;/b&gt;"><b>1.2 基于视差和灰度的双层支持窗口</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;1.3 代价聚合&lt;/b&gt;"><b>1.3 代价聚合</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;1.4 视差精调&lt;/b&gt;"><b>1.4 视差精调</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="&lt;b&gt;2 实验与分析&lt;/b&gt; "><b>2 实验与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="&lt;b&gt;3 结束语&lt;/b&gt; "><b>3 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 双层支持窗,黑色为灰度窗边界,白色为视差窗边界">图1 双层支持窗,黑色为灰度窗边界,白色为视差窗边界</a></li>
                                                <li><a href="#69" data-title="图2 代价聚合路径图">图2 代价聚合路径图</a></li>
                                                <li><a href="#100" data-title="图3 本文算法在Middlebury图像对上的实验结果
">图3 本文算法在Middlebury图像对上的实验结果
</a></li>
                                                <li><a href="#104" data-title="图4 5种算法得到的视差图
">图4 5种算法得到的视差图
</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同算法误匹配率比&lt;/b&gt;"><b>表</b>1 <b>不同算法误匹配率比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 张力,黄影平.实时双目立体视觉系统的实现[J].电子科技,2016,29(3):68-70.Zhang Li,Huang Yingping.Implementation of real-time binocular stereo vision system[J].Electronic Science and Technology,2016,29(3):68-70." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201603017&amp;v=MjAyODViRzRIOWZNckk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5M2tVYnZPSVRmQVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         张力,黄影平.实时双目立体视觉系统的实现[J].电子科技,2016,29(3):68-70.Zhang Li,Huang Yingping.Implementation of real-time binocular stereo vision system[J].Electronic Science and Technology,2016,29(3):68-70.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Hosni A,Rhemann C,Bleyer M,et al.Fast cost-volume filtering for visual correspondence and beyond[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(2):504-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Cost-Volume Filtering for Visual Correspondence and Beyond">
                                        <b>[2]</b>
                                         Hosni A,Rhemann C,Bleyer M,et al.Fast cost-volume filtering for visual correspondence and beyond[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(2):504-11.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 林川.基于新扫描策略的快速立体匹配算法[J].电子科技,2011,24(8):12-15.Lin Chuan.Fast stereo matching algorithm based on new scanning strategy[J].Electronic Science and Technology,2011,24(8):12-15." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201108005&amp;v=MDE5MzZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeTNrVWJ2T0lUZkFaYkc0SDlETXA0OUY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         林川.基于新扫描策略的快速立体匹配算法[J].电子科技,2011,24(8):12-15.Lin Chuan.Fast stereo matching algorithm based on new scanning strategy[J].Electronic Science and Technology,2011,24(8):12-15.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Kanade T,Okutomi M.A stereo matching algorithm with an adaptive window:Theory and experiment[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1994,16(9):920-932." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A stereo matching algorithm with an adaptive window: Theory and experiment">
                                        <b>[4]</b>
                                         Kanade T,Okutomi M.A stereo matching algorithm with an adaptive window:Theory and experiment[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1994,16(9):920-932.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Yoon K J,Kweon I S.Adaptive support-weight approach for correspondence search[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(4):650-656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive support-weight approach for correspondence search">
                                        <b>[5]</b>
                                         Yoon K J,Kweon I S.Adaptive support-weight approach for correspondence search[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(4):650-656.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Hirschmuller H.Stereo processing by semiglobal matching and mutual information[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2008,30(2):328-341." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo Processing by Semiglobal Matching and Mutual Information">
                                        <b>[6]</b>
                                         Hirschmuller H.Stereo processing by semiglobal matching and mutual information[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2008,30(2):328-341.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Cheng F,Zhang H,Sun M,et al.Cross-trees,edge and superpixel priors-based cost aggregation for Stereo matching.[J].Pattern Recognition,2015,48(7):2269-2278." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300162493&amp;v=MTcwOTNCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUpWOFdiaFE9TmlmT2ZiSzlIOVBPckk5RlplME5DSFU2bw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Cheng F,Zhang H,Sun M,et al.Cross-trees,edge and superpixel priors-based cost aggregation for Stereo matching.[J].Pattern Recognition,2015,48(7):2269-2278.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 刘杰,张建勋,代煜.基于区域增长的稠密立体匹配[J].机器人,2017,39(2):182-188.Liu Jie,Zhang Jianxun,Dai Yu.Dense stereo matching based on region growing[J].Robot,2017,39(2):182-188." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201702007&amp;v=MjYwNTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeTNrVWJ2T0x6elpmTEc0SDliTXJZOUZZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         刘杰,张建勋,代煜.基于区域增长的稠密立体匹配[J].机器人,2017,39(2):182-188.Liu Jie,Zhang Jianxun,Dai Yu.Dense stereo matching based on region growing[J].Robot,2017,39(2):182-188.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Min D,Lu J,Do M N.Joint histogram-based cost aggregation for stereo matching[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(10):2539-2545." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint histogram-based cost aggregation for stereo matching">
                                        <b>[9]</b>
                                         Min D,Lu J,Do M N.Joint histogram-based cost aggregation for stereo matching[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(10):2539-2545.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Wang L Q,Liu Z,Zhang Z H.Feature based stereo matching using two-step expansion[J].Mathematical Problems in Engineering,2014(14):452803-452809." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD14123000000280&amp;v=MTUxNjROaWZEYXJLOEg5UFByNDlGWk9zUERuUTVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKVjhXYmhRPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Wang L Q,Liu Z,Zhang Z H.Feature based stereo matching using two-step expansion[J].Mathematical Problems in Engineering,2014(14):452803-452809.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 祝世平,闫利那,李政.基于改进 Census 变换和动态规划的立体匹配算法[J].光学学报,2016,36(4):0415001-415018.Zhu Shiping,Yan Lina,Li Zheng.Stereo Matching algorithm based on Improved census transform and dynamic programming[J].Acta Optics Sin,2016,36(4):0415001-0415018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604028&amp;v=MDI1NzZVYnZPSWpYVGJMRzRIOWZNcTQ5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5M2s=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         祝世平,闫利那,李政.基于改进 Census 变换和动态规划的立体匹配算法[J].光学学报,2016,36(4):0415001-415018.Zhu Shiping,Yan Lina,Li Zheng.Stereo Matching algorithm based on Improved census transform and dynamic programming[J].Acta Optics Sin,2016,36(4):0415001-0415018.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision,2002,47(1-3):7-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MjA0OTJwNHhGWStrTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmkvbFZyN0tKRmc9Tmo3QmFyTzRIdEhP&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision,2002,47(1-3):7-42.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Taniai T,Matsushita Y,Naemura T.Graph cut based continuous stereo matching using locally shared labels[C].Columbus:IEEE Conference on Computer Vision and Pattern Recognition,IEEE Computer Society,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph cut based continuous stereo matching using locally shared labels">
                                        <b>[13]</b>
                                         Taniai T,Matsushita Y,Naemura T.Graph cut based continuous stereo matching using locally shared labels[C].Columbus:IEEE Conference on Computer Vision and Pattern Recognition,IEEE Computer Society,2014.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Yang Q.A non-local cost aggregation method for stereo matching[C].Providence:Computer Vision and Pattern Recognition,IEEE,2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matching">
                                        <b>[14]</b>
                                         Yang Q.A non-local cost aggregation method for stereo matching[C].Providence:Computer Vision and Pattern Recognition,IEEE,2012.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Chang X,Zhou Z,Wang L,et al.Real-time accurate stereo matching using modified two-pass aggregation and winner-take-all guided dynamic programming[C].Hangzhou:International Conference on 3D Imaging,Modeling,Processing,Visualization and Transmission,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time Accurate Stereo Matching Using Modified Two-pass Aggregation and Winner-take-all Guided Dynamic Programming">
                                        <b>[15]</b>
                                         Chang X,Zhou Z,Wang L,et al.Real-time accurate stereo matching using modified two-pass aggregation and winner-take-all guided dynamic programming[C].Hangzhou:International Conference on 3D Imaging,Modeling,Processing,Visualization and Transmission,2011.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Rhemann C,Hosni A,Bleyer M,et al.Fast cost-volume filtering for visual correspondence and beyond[C].Colorado Springs:IEEE Conference on Computer Vision and Pattern Recognition,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast cost-volume filtering for visual correspondence and beyond">
                                        <b>[16]</b>
                                         Rhemann C,Hosni A,Bleyer M,et al.Fast cost-volume filtering for visual correspondence and beyond[C].Colorado Springs:IEEE Conference on Computer Vision and Pattern Recognition,2011.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Psota E T,Kowalczuk J,Carlson J,et al.A local iterative refinement method for adaptive support-weight stereo matching[C].Las Vegas:Proceedings of the International Conference on Image Processing,Computer Vision,and Pattern Recognition (IPCV),2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A local iterative refinement method for adaptive support-weight stereo matching">
                                        <b>[17]</b>
                                         Psota E T,Kowalczuk J,Carlson J,et al.A local iterative refinement method for adaptive support-weight stereo matching[C].Las Vegas:Proceedings of the International Conference on Image Processing,Computer Vision,and Pattern Recognition (IPCV),2011.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Wang L,Yang R.Global stereo matching leveraged by sparse ground control points[C].Colorado Springs:IEEE Conference on Computer Vision and Pattern Recognition,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global stereo matching leveraged by sparse ground control points">
                                        <b>[18]</b>
                                         Wang L,Yang R.Global stereo matching leveraged by sparse ground control points[C].Colorado Springs:IEEE Conference on Computer Vision and Pattern Recognition,2011.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-12-29 14:49</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZKK" target="_blank">电子科技</a>
                2019,32(11),12-17+27 DOI:10.16180/j.cnki.issn1007-7820.2019.11.003            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于视差与灰度双层支持窗的立体匹配算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%B0%8F%E6%9E%97&amp;code=13971530&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李小林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%96%87%E5%9B%BD&amp;code=07899040&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李文国</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%B5%A9&amp;code=27859229&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李浩</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E6%9C%BA%E7%94%B5%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0242668&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学机电工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统局部匹配支持窗难以利用空间、灰度距离远的像素信息的问题,提出一种基于视差和灰度的双层支持窗立体匹配算法。该算法根据视差图获得第一层视差支持窗,在视差支持窗内配合参考图RGB颜色灰度值获得灰度相似子支持窗;随后以视差窗、灰度窗、中心像素为优化路径,通过类动态规划算法优化聚合匹配代价;最后采用WTA策略选取最佳视差,更新所有视差,并不断迭代优化视差图直到视差收敛。经过Middlebury平台的测评,新算法的平均误匹配率为5.15%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E5%B7%AE%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视差图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%81%B0%E5%BA%A6%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">灰度图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E5%B1%82%E6%94%AF%E6%8C%81%E7%AA%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双层支持窗;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动态规划;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%AD%E4%BB%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迭代;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李小林(1993-),男,硕士研究生。研究方向:机器视觉。;
                                </span>
                                <span>
                                    李文国(1973-),男,博士,副教授。研究方向:光学精密测量技术、机器视觉与图像处理技术、微型计算机测控技术、机器人视觉测量与控制技术。;
                                </span>
                                <span>
                                    李浩(1992-),男,硕士研究生。研究方向:光学精密测量技术、机器视觉。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>昆明理工大学自然科学研究基金(KKSY201301070);</span>
                    </p>
            </div>
                    <h1><b>A Stereo Matching Using Double Layer Support Windows Based on RGB Map and Disparity Map</b></h1>
                    <h2>
                    <span>LI Xiaolin</span>
                    <span>LI Wenguo</span>
                    <span>LI Hao</span>
            </h2>
                    <h2>
                    <span>Faculty of Mechanical and Electrical Engineering,Kunming University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem that traditional local matching support window is difficult to use pixel information with long distance and gray distance, a stereoscopic matching algorithm using double layer support windows based on disparity map and RGB map was proposed in this study. The algorithm obtained the first layer disparity support window according to the disparity map, and obtained the gray scale similar sub-support window by matching the RGB color gray value of the reference picture in the disparity support window. The aggregation matching cost was optimized by class dynamic programming algorithm with parallax window, gray window and central pixel as optimized paths. Finally, the WTA strategy was used to select the best disparity, update all disparities and iteratively optimize the disparity map until convergence of the disparity map was achieved. After evaluation by the Middlebury platform, the average mismatch rate of the new algorithm was 5.15%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=the%20disparity%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">the disparity map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=the%20grey%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">the grey map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=double%20layer%20support%20windows&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">double layer support windows;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dynamic%20programming&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dynamic programming;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=iterative&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">iterative;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-12</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>Natural Science Research Foundation of Kunming University of Science and Technology(KKSY201301070);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="39">在机器视觉领域,双目立体视觉一直是热点议题,其核心问题是求出立体图像对中左右图像像素点之间的对应关系,以获得视差图,即双目立体匹配问题<citation id="122" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。根据Scharstein等<citation id="119" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>对双目匹配过程的划分,双目立体匹配可分为4步:(1)初始匹配代价计算,从参考图像像素到目标图像对应像素点的初始匹配代价计算;(2)代价聚合,聚合支持窗内像素初始匹配代价;(3)视差计算,选择优化策略来确定视差;(4)视差细化,对视差图进行精调细化以获得最终视差图。其中根据匹配代价计算方式的差异,双目立体匹配算法分为全局、半全局或局部两种立体匹配算法。全局算法将视差分配问题视为最小化全局能量函数的问题<citation id="120" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,使用全局优化策略获得全局能量最小的视差图。此类算法对局部信息不敏感且计算量大、运行慢。局部匹配算法利用局部约束信息计算单个像素最佳匹配<citation id="123" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>,具有计算速度快,编程实现简单等优点,但支持窗口产生与聚合代价的计算是个难题<citation id="121" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="40">传统支持窗口取自以待匹配像素为中心的固定大小的矩形框,通过简单累加窗口内像素的初始匹配代价得到聚合代价。传统支持窗在窗口尺寸较小时可以获得平滑、连续的视差图,但由于窗口包含的信息较少,因此在弱纹理、重复纹理区域匹配效果不佳,且易受噪声影响;在窗口尺寸较大时则会造成视差过平滑,前景膨胀严重等问题。为此,文献<citation id="124" type="reference">[<a class="sup">4</a>]</citation>提出了一种自适应窗口大小的方法,根据中心像素周围局灰度变化自适应地确定支持窗口尺寸,解决了前景膨胀问题,但是矩形窗口不能完全拟合局部灰度变化,且时间复杂度高。基于支持窗像素灰度值,Yoon等人<citation id="125" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了自适应权值立体匹配算法,根据与中心像素的颜色距离与空间距离确定支持窗内像素的聚合权值,弱化空间距离远和颜色差异较大的像素的影响,极大程度改善了局部匹配精度。文献<citation id="126" type="reference">[<a class="sup">6</a>]</citation>对初始匹配代价计算和代价聚合做了进一步的探索,在匹配代价中加入了左右图像的互信息,对中心像素多方向路径进行线性优化,使用类动态规划算法(Dynamic Programming,DP)累积路径上匹配代价,极大地提高了局部立体匹配算法的精度。随后一种改进的十字交叉形状的支持窗口构造方法<citation id="128" type="reference"><link href="15" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">15</a>]</sup></citation>被提出,向中心像素的水平方向和垂直方向根据颜色变化先后拓展,构成自适应不规则的支持窗口,一定程度上代替了计算量大的图像分割,取得很好的效果。针对支持窗口尺寸、形状难以完全适应灰度变化的问题,文献<citation id="127" type="reference">[<a class="sup">8</a>]</citation>提出了一种基于区域增长的支持窗口的方法,通过颜色相似性和连通性约束动态地获得完全自适应的支持窗口, 很大程度上保证了窗口内像素的深度相似。</p>
                </div>
                <div class="p1">
                    <p id="41">本文提出一种基于视差图视差标签信息和参考图像灰度信息的双层支持窗口获取的方法。根据视差图视差标签信息获得连通的视差支持窗,再基于参考图像灰度信息对视差支持窗进行再分割获得灰度支持窗。以相邻视差窗到邻域灰度窗最后到中心像素作为路径,使用类动态规划算法优化聚合窗口匹配代价(窗口匹配代价取窗口内像素匹配代价平均值)。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>1 算法描述</b></h3>
                <h4 class="anchor-tag" id="43" name="43"><b>1.1 初始匹配代价计算</b></h4>
                <div class="p1">
                    <p id="44">匹配代价是参考图像<i><b>I</b></i><sub><i>r</i></sub>中像素<i>p</i>与在目标图像图<i><b>I</b></i><sub><i>t</i></sub>相同位置位移<i>d</i>的像素<i>p</i>之间的相似性度量。常用的相似性度量函数有图像亮度或灰度相关度量函数和梯度相关测度函数。灰度相似性度量函数能直接反映像素之间的相似性,但鲁棒性较差,易受噪声影响。梯度相似性度量函数能反映像素周围梯度变化相关性,且对增亮/光照失真不敏感,对图像噪声也有很好的效果,应用广泛。本算法匹配代价采用灰度与梯度结合的方式,定义匹配代价为</p>
                </div>
                <div class="p1">
                    <p id="45"><i>e</i>(<i>p</i>,<i>d</i>)=<i>α</i>×<i>ρ</i><sub><i>c</i></sub>(<i>p</i>,<i>d</i>)+(1-<i>α</i>)×<i>ρ</i><sub><i>g</i></sub>(<i>p</i>,<i>d</i>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="46">式(1)中的<i>ρ</i><sub><i>c</i></sub>(<i>p</i>,<i>d</i>)和<i>ρ</i><sub><i>g</i></sub>(<i>p</i>,<i>d</i>)分别为像素<i>p</i>在视差为<i>d</i>时的灰度相关匹配代价和梯度相关匹配代价。权重参数<i>α</i>取值[0,1],调节两类代价函数的比重。</p>
                </div>
                <div class="p1">
                    <p id="47">计算RGB颜色(灰度)相似性一般使用欧式距离和曼哈顿距离,两者都能很好的体现像素灰度差。曼哈顿距离相比欧式距离对单通道差异更为敏感,且计算量更小。因此定义</p>
                </div>
                <div class="p1">
                    <p id="48"><i>ρ</i><sub><i>c</i></sub>(<i>p</i>,<i>d</i>)=‖<i><b>I</b></i><sub><i>r</i></sub>(<i>p</i>)-<i><b>I</b></i><sub><i>t</i></sub>(<i>p</i>+<i>d</i>)‖<sub>2</sub>      (2)</p>
                </div>
                <div class="p1">
                    <p id="49">式(2)中,<i><b>I</b></i><sub><i>r</i></sub>(<i>p</i>)和<i><b>I</b></i><sub><i>t</i></sub>(<i>p</i>+<i>d</i>)分别代表参考图像像素<i>p</i>的三通道RGB灰度值和目标图像<i>p</i>位移<i>d</i>的像素(<i>p</i>+<i>d</i>)的灰度值。</p>
                </div>
                <div class="p1">
                    <p id="50">为方便计算像素灰度梯度,将梯度定义为沿图像<i>X</i>和<i>Y</i>方向的一阶偏导数<i>g</i>=(<i>g</i><sub><i>x</i></sub>,<i>g</i><sub><i>y</i></sub>)。进而得到参考图像和目标图像的梯度图<i><b>G</b></i><sub><i>r</i></sub>=(<i><b>G</b></i><sub><i>rx</i></sub>,<i><b>G</b></i><sub><i>ry</i></sub>)、<i><b>G</b></i><sub><i>t</i></sub>=(<i><b>G</b></i><sub><i>tx</i></sub>,<i><b>G</b></i><sub><i>ty</i></sub>)。在实际应用中,可通过<i>X</i>和<i>Y</i>方向的模板算子计算梯度向量。使用曼哈顿距离计算梯度相似性,可以定义</p>
                </div>
                <div class="p1">
                    <p id="51"><i>ρ</i><sub><i>g</i></sub>(<i>p</i>,<i>d</i>)=‖<i><b>G</b></i><sub><i>rx</i></sub>-<i><b>G</b></i><sub><i>tx</i></sub>‖<sub>2</sub>+‖<i><b>G</b></i><sub><i>ry</i></sub>-<i><b>G</b></i><sub><i>ty</i></sub>‖<sub>2</sub>      (3)</p>
                </div>
                <h4 class="anchor-tag" id="52" name="52"><b>1.2 基于视差和灰度的双层支持窗口</b></h4>
                <div class="p1">
                    <p id="53">基于连通且灰度相似的像素有相同视差值的假设,本算法提出一种构建视差和灰度的双层支持窗的方法。根据视差图视差标签生成像素视差相同且连通视差窗,在视差窗内生成像素灰度相似且连通的灰度窗。在视差图中连通且有相同视差像素块,同在一个视差平面和有平滑的视差值的概率较非连通、相同视差的像素高。单一的视差窗无法获得物体边缘信息,容易造成前景膨胀,因此在视差窗内的灰度窗能有效获取物体边缘信息,从而提高深度不相续区域的匹配精度。</p>
                </div>
                <div class="p1">
                    <p id="54">本算法采用区域生长算法获得视差灰度双层支持窗。初始化区域生长算法生长点为随机点,视差图为随机图,像素灰度图为参考图像<i><b>I</b></i><sub><i>r</i></sub>。支持窗的连通域采用4连通域。</p>
                </div>
                <div class="p1">
                    <p id="55">视差窗生长点,向4领域生长,遇到视差相同点则生长该点,否则生长为新的视差窗种子点。每个点都有所属的视差窗,将视差窗从0开始编号,可得视差窗编号图<i><b>W</b></i><sub><i>d</i></sub>,在生长之前其被初始化为-1。可以定义视差窗编号传递过程为</p>
                </div>
                <div class="area_img" id="56">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZKK201911004_05600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="58"><i><b>W</b></i><sub><i>d</i></sub>(<i>q</i>)和<i><b>W</b></i><sub><i>d</i></sub>(<i>p</i>)为<i>p</i>点和<i>q</i>点的视差窗编号;<i>N</i><sub><i>p</i></sub>为<i>p</i>的4领域点集合;<i><b>D</b></i>为参考图<i><b>I</b></i><sub><i>r</i></sub>的视差图,<i><b>D</b></i>(<i>p</i>)和<i><b>D</b></i>(<i>q</i>)分别为<i>p</i>和<i>q</i>的视差值。</p>
                </div>
                <div class="p1">
                    <p id="59">如果生长点<i>p</i>生长<i>q</i>为同视差窗点,则对<i>q</i>进行灰度窗生长。同样对每个灰度窗从0开始编号,可得灰度窗编号图<i><b>W</b></i><sub><i>c</i></sub>,<i><b>W</b></i><sub><i>c</i></sub>被初始化为-1。<i>p</i>和<i>q</i>属于同视差窗,两点之间的灰度距离在一定阈值内,判定<i>q</i>与<i>p</i>有同样的灰度窗编号,可以定义灰度窗编号传递过程为</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mi>q</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>p</mi></msub><mo>&amp;</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>&amp;</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0</mn><mo>&amp;</mo><mi>φ</mi><mo stretchy="false">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi><mglyph src="1A.jpg" height="100%" width="100%" /></mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>l</mtext><mtext>o</mtext><mtext>r</mtext></mrow></msub></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn><mo>,</mo><mspace width="0.25em" /><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>s</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">式(5)中的<i><b>W</b></i><sub><i>c</i></sub>(<i>q</i>)和<i><b>W</b></i><sub><i>c</i></sub>(<i>p</i>)为<i>p</i>点和<i>q</i>点的灰度窗编号;<image id="136" type="formula" href="images/DZKK201911004_13600.jpg" display="inline" placement="inline"><alt></alt></image>为常量,其值越小,灰度窗像素灰度越接近;<i>φ</i>(<i>p</i>,<i>q</i>)为<i>p</i>所在灰度窗的平均灰度<i>C</i><sub><i>p</i></sub>与<i>q</i>点对应参考图灰度<i><b>I</b></i><sub><i>r</i></sub>(<i>q</i>)的曼哈顿距离</p>
                </div>
                <div class="p1">
                    <p id="62"><i>φ</i>(<i>p</i>,<i>q</i>)=‖<i>C</i><sub><i>p</i></sub>-<i><b>I</b></i><sub><i>r</i></sub>(<i>q</i>)‖<sub>2</sub>      (6)</p>
                </div>
                <div class="p1">
                    <p id="63"><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mi>ε</mi><mi>U</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></munder><mi mathvariant="bold-italic">Ι</mi></mstyle><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="64">式(7)中<i>U</i><sub><i>p</i></sub>为<i>p</i>所在灰度窗<i><b>W</b></i><sub><i>c</i></sub>(<i>p</i>)内所有点集合;<i>n</i><sub><i>p</i></sub>为灰度窗<i><b>W</b></i><sub><i>c</i></sub>(<i>q</i>)内像素点个数。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911004_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 双层支持窗,黑色为灰度窗边界,白色为视差窗边界" src="Detail/GetImg?filename=images/DZKK201911004_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 双层支持窗,黑色为灰度窗边界,白色为视差窗边界  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911004_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1. Double layer support windows, the gray window boundary 
is black and the disparity window boundary is white</p>

                </div>
                <div class="p1">
                    <p id="66">图1左图为原图,右图为支持窗边界图。从图中可以看出,白色边界的视差窗内像素在视差图<i>D</i>中有相同视差,其接近真实视差分布;黑色边界的灰度窗内像素灰度值相近,在复杂纹理处密集,从图1可以看出黑色边界围出近纯色的无纹理区域,在弱纹理、无纹理处灰度窗较大。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>1.3 代价聚合</b></h4>
                <div class="p1">
                    <p id="68">以相邻视差窗到邻域灰度窗最后到中心像素作为单条优化路径,中心像素p的4邻域内每个灰度窗都产生一条优化聚合路径。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911004_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 代价聚合路径图" src="Detail/GetImg?filename=images/DZKK201911004_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 代价聚合路径图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911004_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2. Cost aggregation path</p>

                </div>
                <div class="p1">
                    <p id="70">如图2所示,其中深色区域为同一视差,浅色色区域为同一视差,白色像素点为中心像素点p;以蓝色区域为例,浅色区域为连通的同时视差像素块,记为视差窗<i><b>U</b></i>;用<i><b>v</b></i>表示视差窗<i><b>U</b></i>内灰度值相似的连通灰度窗,图中<i><b>v</b></i>为<i>p</i>的相邻灰度窗,即<i><b>v</b></i>内存在像素<i>q</i>为<i>p</i>的4领域<i>N</i><sub><i>p</i></sub>内像素点</p>
                </div>
                <div class="p1">
                    <p id="71">∃<i>q</i>∈<i><b>v</b></i>&amp;<i>q</i>∈<i>N</i><sub><i>p</i></sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="72"><i><b>O</b></i>定义为<i><b>v</b></i>在视差窗所有点的集合<i><b>U</b></i>的绝对补集<i><b>O</b></i>=<i>C</i><sub><i>U</i></sub><i><b>v</b></i><b> ,</b><i><b>v</b></i>和<i><b>O</b></i>的边界能有效的拟合<i><b>U</b></i>内灰度变化。图2中的箭头所示, <i><b>O</b></i><b>-</b><i><b>v</b></i>-<i>p</i>为一条完整的视差优化路径,箭头方向为优化方向。与中心像素<i>p</i>相邻的每一个灰度窗都可生成一条路径。匹配代价将通过单条或多条路径通过类DP优化的算法聚合到<i>p</i>点。为了尽可能的保留每个像素的视差信息,将<i><b>v</b></i>和<i><b>O</b></i>的匹配代价定义为所有像素的匹配代价均值</p>
                </div>
                <div class="p1">
                    <p id="73"><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mi>Ο</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>U</mi></msub><mo>-</mo><mi>n</mi><msub><mrow></mrow><mi>v</mi></msub></mrow></mfrac><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>U</mi></mrow></munder><mi>e</mi></mstyle><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>v</mi></mrow></munder><mi>e</mi></mstyle><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo></mrow><mo>)</mo></mrow></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>v</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>v</mi></mrow></munder><mi>e</mi></mstyle><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="75">式(9)中<i>e</i>(<i>q</i>)为<i>q</i>点的匹配代价,<i>n</i><sub><i>U</i></sub>和<i>n</i><sub><i>v</i></sub>为<i>U</i>和<i>v</i>的元素数。式(9)中<i>E</i><sub><i>v</i></sub>为<i><b>O</b></i>向<i>v</i>传递匹配代价<i>E</i><sub><i>O</i></sub>,<i>E</i><sub><i>v</i></sub>更新为</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><msup><mi>E</mi><mo>′</mo></msup><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>+</mo><mrow><mi>min</mi></mrow><mo stretchy="false">(</mo><mi>E</mi><msub><mrow></mrow><mi>Ο</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>,</mo><mi>E</mi><msub><mrow></mrow><mi>Ο</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo>±</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>+</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>i</mi></munder><mi>E</mi><msub><mrow></mrow><mi>Ο</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Ρ</mi><msubsup><mrow></mrow><mn>2</mn><mo>*</mo></msubsup><mi>ξ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ο</mi><mo>,</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77"><i>P</i><sub>1</sub>和<i>P</i><sub>2</sub>为相邻视差平滑系数和间隔视差平滑系数,<i>P</i><sub>1</sub>≤<i>P</i><sub>2</sub>;式(11)中第一项为灰度层匹配代价,第二项为视差层匹配代价平滑传递项。平滑系数<i>P</i><sub>1</sub>越小视差越光滑,<i>P</i><sub>2</sub>越小视差边缘越准确,平滑模型采用广泛应用的Potts模型。<i>ξ</i>(<i><b>O</b></i><b>,</b><i><b>v</b></i>)为<i><b>O</b></i>和<i><b>v</b></i>接触边缘梯度影响因子,当边缘梯度越大,其为视差边缘的概率越大,<i><b>O</b></i>对<i><b>v</b></i>影响应越小,梯度影响因子越小。</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ξ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ο</mi><mo>,</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ο</mi><mo>,</mo><mi>j</mi><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>v</mi></msub><mo>,</mo><mi>j</mi><mo>∈</mo><mi>v</mi></mrow></munder><mi>g</mi></mstyle><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>g</mi></msub><mo>×</mo><mi>σ</mi></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="79">其中,<i>n</i><sub><i>g</i></sub>为<i><b>O</b></i>和<i><b>v</b></i>相邻边个数,<i>σ</i>控制参数,<i>σ</i>越大<i>ξ</i>(<i><b>O</b></i><b>,</b><i><b>v</b></i>)越大。</p>
                </div>
                <div class="p1">
                    <p id="80">获得灰度窗的视差匹配代价后,中心像素聚合所有邻域灰度窗的匹配代价,<i>p</i>的聚合代价定义为</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mo>∈</mo><mi mathvariant="bold-italic">S</mi></mrow></munder><mrow><mi>min</mi></mrow></mstyle><mo stretchy="false">(</mo><msup><mi>E</mi><mo>′</mo></msup><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>,</mo><msup><mi>E</mi><mo>′</mo></msup><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo>±</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>i</mi></munder><msup><mi>E</mi><mo>′</mo></msup><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中,<i><b>S</b></i>为<i>p</i>的相邻灰度窗集合。</p>
                </div>
                <div class="p1">
                    <p id="83">聚合代价计算完成后,采用WTA策略来更新视差,视差更新定义为</p>
                </div>
                <div class="p1">
                    <p id="84"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><munder><mstyle mathsize="140%" displaystyle="true"><mtext>n</mtext></mstyle><mrow><mi>d</mi><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></munder><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (14)</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>1.4 视差精调</b></h4>
                <div class="p1">
                    <p id="86">立体图像对是摄像机分置左右间隔一定距离拍摄而成的,左右视野不一。左右图像都是相似场景的映射,左图中的像素能够在右图中找到点。由于前景遮挡后景的原因,后景的一些点被前景遮挡,在目标图像中不存在对应点,这会引起误匹配。本文采用左右一致性检验(Left-Right Consistency,LRC)来检测视差图中的遮挡点,同时对遮挡点视差值进行填充。</p>
                </div>
                <div class="p1">
                    <p id="87">左右一致性检验的步骤如下:将左图作为参考图像右图作为目标图像计算得到视差图<i><b>D</b></i><sub><i>L</i></sub>,将右图作为参考图像左图作为目标图像计算得到视差图<i><b>D</b></i><sub><i>R</i></sub>;如果<i>p</i>点的视差值计算正确,则应满足如下关系</p>
                </div>
                <div class="p1">
                    <p id="88"><i><b>D</b></i><sub><i>L</i></sub>(<i>p</i>)=<i><b>D</b></i><sub><i>R</i></sub>(<i>p</i>-<i><b>D</b></i><sub><i>L</i></sub>(<i>p</i>))      (15)</p>
                </div>
                <div class="p1">
                    <p id="89">为了提高一般性,可以设置一个阈值<i>ε</i>,将上式改写为</p>
                </div>
                <div class="p1">
                    <p id="90"><image href="images/DZKK201911004_114.jpg" type="" display="inline" placement="inline"><alt></alt></image><i><b>D</b></i><sub><i>L</i></sub>(<i>p</i>)-<i><b>D</b></i><sub><i>R</i></sub>(<i>p</i>-<i><b>D</b></i><sub><i>L</i></sub>(<i>p</i>))&lt;<i>ε</i>      (16)</p>
                </div>
                <div class="p1">
                    <p id="91">满足条件的点为有效点,不满足为无效点,将该点视差值剔除。检验完成后需要对遮挡点进行填充:以无效点为中心,分别向左和向右寻找有效点,选取第一个有效点为候选视差,在左右候选视差中选取较小的一个作为该点视差值,即选取被遮挡的后景作为遮挡部分的视差值。</p>
                </div>
                <div class="p1">
                    <p id="92">对于深度不连续区域,由于左右一致性检验并不能完全剔除错误匹配点,需采用区域迭代投票的方式进一步去除深度边缘的错误匹配,使边缘视差更加贴合灰度边缘。对于视差图上的每一个点,以该点<i>p</i>为中心,建立一个<i>m</i> pixel×<i>m</i> pixel的动态窗口,使用箱体数为<i>d</i><sub>max</sub>+1(<i>d</i><sub>max</sub>为最大视差范围)的直方图<i>Hp</i>统计窗口内像素<i>q</i>的投票结果。如果像素<i>q</i>与<i>p</i>的灰度距离<i>L</i>(<i>p</i>,<i>q</i>)小于灰度阈值<image href="images/DZKK201911004_115.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub>color</sub>,即满足</p>
                </div>
                <div class="p1">
                    <p id="93"><i>L</i>(<i>p</i>,<i>q</i>)=‖<i><b>I</b></i><sub><i>r</i></sub>(<i>p</i>)-<i><b>I</b></i><sub><i>r</i></sub>(<i>q</i>)‖<sub>2</sub>&lt;<image href="images/DZKK201911004_116.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub>color</sub>      (17)</p>
                </div>
                <div class="p1">
                    <p id="94"><i>H</i><sub><i>p</i></sub>(<i><b>D</b></i><sub><i>L</i></sub>(<i>p</i>))的投票值加1;<i><b>D</b></i><sub><i>L</i></sub>(<i>p</i>)取<i>H</i><sub><i>p</i></sub>投票结果最大值</p>
                </div>
                <div class="p1">
                    <p id="95"><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>arg</mi></mrow><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>d</mi></munder><mi>Η</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (18)</p>
                </div>
                <div class="p1">
                    <p id="96">一般区域投票迭代两次就接近收敛,深度边缘匹配误差基本消除。</p>
                </div>
                <h3 id="97" name="97" class="anchor-tag"><b>2 实验与分析</b></h3>
                <div class="p1">
                    <p id="98">为了验证本文算法的匹配精度,使用C++对算法进行编程实现,实验平台为Intel® Core<sup>TM</sup> i5 2.5 GHz的中央处理器(CPU),4 GB内存,Windows 10操作系统,Visual Studio 2015集成开发环境。为定量的检验本文算法的有效性,在Middlebury立体匹配评估平台进行测试。选取该平台提供的4组基准彩色图像对:Tsukuba、Venus、Teddy 和Cones,分辨率分别为384 pixel×288 pixel、434 pixel×383 pixel、450 pixel×375 pixel 和450 pixel×375 pixel。实验时,将4组图像的视差搜索范围分别设置为0～15 pixel、0～19 pixel、0～59 pixel和0～59 pixel。若无特殊说明,实验中算法相关参数设置为{<i>α</i>,<image href="images/DZKK201911004_118.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub>color</sub>,<i>P</i><sub>1</sub>,<i>P</i><sub>2</sub> , ,ε}={0.4,12,1,30,2}。</p>
                </div>
                <div class="p1">
                    <p id="99">图3所示为本文算法在Tsukuba、Venus、Teddy、Cones标准图像上得到的视差图。其中图3(a)为待匹配标准图像对左图,图3(b)为对应的真实视差图,图3(c)为本文算法的到的视差图,图3(d)为误匹配像素图。其中大片白色区域为正确匹配点,灰色区域为遮挡区域误匹配点,黑色区域为非遮挡区域误匹配点。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911004_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法在Middlebury图像对上的实验结果" src="Detail/GetImg?filename=images/DZKK201911004_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法在Middlebury图像对上的实验结果
  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911004_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3. Testing results on the Middlebury stereo vision 
</p>
                                <p class="img_note">(a)原始左图 (b)真实视差图 (c)视差精调后视差图 (d)误匹配像素图</p>
                                <p class="img_note">(a) Original left image (b) Ground truth image (c) Disparity image after post processing (d) Error matching image</p>

                </div>
                <div class="p1">
                    <p id="101">从图3中可以看到,Teddy图中的重复纹理(砖墙部分和报纸部分)、弱纹理(背景板和小屋屋顶)和边缘无纹理(小熊的右边和叶子右边部分)比较多,局部区域深度变化复杂,因此在4对标准图中表现最差;Cones物品较多,深度结构更为复杂,由于圆锥数目较多,视差遮挡区域较大,因此匹配精度较低;Tsukuba 图中,视差层次分级明显,但深度不连续区域较多,在灯的电线区域与背景较为相似且细长,降低了视差图整体精度;Venus图无纹理区域较多,但构图较为简单,在4幅图中精度最高。</p>
                </div>
                <div class="p1">
                    <p id="102">为更好的体现本文算法的有效性,将本文算法视差图与其他4种较为先进的匹配算法得到的视差图作比较,如图4所示,图中亮白色部分为非遮挡区域错误匹配的像素点。其中,CostFilter<citation id="129" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>算法通过对初始匹配代价快速滤波,得到较为平滑的视差图;IterAdaptWgt<citation id="130" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>算法通过自适应权重惩罚本地窗口内的视差差异,不断的迭代细化,不依赖图像分割和平面拟合就可以获得较高质量的视差图;HistoAggr2<citation id="131" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>算法探讨了一种基于直方图改进的自适应代价聚合的方法,得到较为平滑的时差图;TwoStep<citation id="132" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>算法提出了一种基于图像特征的立体匹配算法,进一步结合图像分割和种子点生长理论得到稠密时差图。从图4中可以看出,本文算法根据现有视差图获取连通的同视差支持窗,支持窗有较高的窗口视差支撑强度,对重复纹理和弱纹理、无纹理都有理想的效果,如Teddy图中砖墙、报纸重复纹理区域和小熊右边无纹理区域,其他算法都有大片亮白色不匹配点;在视差支持窗下进一步生成的灰度窗,能有效的拟合视差边缘,在深度不连续区域有良好表现,如Cones图中其他算法在多个圆锥边缘、笔筒区域都有大量误匹配点,本文算法得到的视差图效果更佳。</p>
                </div>
                <div class="p1">
                    <p id="103">在匹配点与真实视差图视差值相差1 pixel以上时,记为误匹配点。表1为本文算法与其他先进算法的误匹配像素百分比,包括局部自适应权重算法CostFilter、HistoAggr2、IterAdaptWgt、TwoStep、CrossTrees+SP<citation id="133" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,改进Census变换(MCT<citation id="134" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>)算法与全局算法GlobalGCP<citation id="135" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。其中“n-occ”、“all”、“disc”、“Average error”分别代表非遮挡区域误匹配率、总体误匹配率和深度不连续区域误匹配率、平均误匹配率。从表1中可以看出,在深度不连续区域比较多的Teddy和Cones图中非遮挡区域误匹配率分别为5.42%和2.34%,在所有算法中非遮挡区域误匹配率最低,可见本文算法有效的提高了非遮挡区域的匹配精度。同时,本文算法的平均误匹配率为5.15%,优于其他7种匹配算法,可见本文算法在遮挡区域和视差不连续区域也有良好的表现。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZKK201911004_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 5种算法得到的视差图" src="Detail/GetImg?filename=images/DZKK201911004_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 5种算法得到的视差图
  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZKK201911004_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4. Disparity map obtained by five algorithms
</p>
                                <p class="img_note">(a)CostFilter算法 (b)IterAdaptWgt算法 (c)HistoAggr2算法 (d)TwoStep算法 (f)本文算法</p>
                                <p class="img_note">(a) CostFilter algorithm (b) IterAdaptWgt algorithm (c) HistoAggr2 algorithm (d) TwoStep algorithm (f) Ours</p>

                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表</b>1 <b>不同算法误匹配率比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">Table 1. Comparison of error matching rates of different algorithm.</p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="3"><br />Tsukuba</td><td colspan="3">Venus</td><td colspan="3">Teddy</td><td colspan="3">Cones</td><td rowspan="2">Average Error</td></tr><tr><td><br />n-occ</td><td>all</td><td>disc</td><td>n-occ</td><td>all</td><td>disc</td><td>n-occ</td><td>all</td><td>disc</td><td>n-occ</td><td>all</td><td>disc</td></tr><tr><td><br />本文算法</td><td>1.29</td><td>1.78</td><td>6.99</td><td>0.26</td><td>0.45</td><td>2.38</td><td>5.42</td><td>10.49</td><td>15.02</td><td>2.34</td><td>8.63</td><td>6.77</td><td>5.15</td></tr><tr><td><br />CostFilter</td><td>1.51</td><td>1.85</td><td>7.61</td><td>0.20</td><td>0.39</td><td>2.42</td><td>6.16</td><td>11.80</td><td>16.00</td><td>2.71</td><td>8.24</td><td>7.66</td><td>5.55</td></tr><tr><td><br />MCT</td><td>1.41</td><td>1.74</td><td>6.86</td><td>0.27</td><td>0.38</td><td>3.04</td><td>5.94</td><td>11.40</td><td>14.90</td><td>2.64</td><td>7.86</td><td>7.37</td><td>5.31</td></tr><tr><td><br />GlobalGCP</td><td>0.87</td><td>2.54</td><td>4.69</td><td>0.46</td><td>0.53</td><td>2.22</td><td>6.44</td><td>11.50</td><td>16.20</td><td>3.59</td><td>9.49</td><td>8.90</td><td>5.60</td></tr><tr><td><br />CrossTrees+SP</td><td>1.68</td><td>1.99</td><td>7.82</td><td>0.22</td><td>0.32</td><td>2.84</td><td>6.23</td><td>11.70</td><td>14.80</td><td>2.52</td><td>7.71</td><td>7.50</td><td>5.44</td></tr><tr><td><br />HistoAggr2</td><td>1.93</td><td>2.30</td><td>6.39</td><td>0.16</td><td>0.46</td><td>2.22</td><td>5.88</td><td>11.30</td><td>14.70</td><td>2.41</td><td>7.78</td><td>6.89</td><td>5.20</td></tr><tr><td><br />IterAdaptWgt</td><td>0.85</td><td>1.28</td><td>4.59</td><td>0.35</td><td>0.86</td><td>4.53</td><td>7.60</td><td>14.50</td><td>17.30</td><td>3.20</td><td>9.36</td><td>8.49</td><td>6.08</td></tr><tr><td><br />TwoStep</td><td>2.91</td><td>3.68</td><td>13.30</td><td>0.27</td><td>0.45</td><td>2.63</td><td>7.42</td><td>12.60</td><td>18.00</td><td>4.09</td><td>10.10</td><td>10.30</td><td>7.14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="106" name="106" class="anchor-tag"><b>3 结束语</b></h3>
                <div class="p1">
                    <p id="107">本文提出一种基于视差和灰度双层支持窗的立体匹配算法。相较于传统的基于窗口的局部立体匹配算法,该算法可根据视差图获得连通的同视差支持窗,提高了对窗口视差得支撑强度。实验结果表明,该方法对重复纹理和弱纹理、无纹理都有不错的效果;同时根据连通灰度相似得像素有相同视差得假设,在视差窗内获取灰度窗,拟合视差窗内灰度变化;然后,按照从中心像素邻域视差窗到邻域灰度窗最后到中心像素的路径优化聚合匹配代价,对重复纹理、弱纹理和无纹理区域起到了较好的效果。实验结果证明,所提算法综合匹配精度优于经典的局部匹配算法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201603017&amp;v=MTgxMzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkza1Vidk9JVGZBWmJHNEg5Zk1ySTlFWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 张力,黄影平.实时双目立体视觉系统的实现[J].电子科技,2016,29(3):68-70.Zhang Li,Huang Yingping.Implementation of real-time binocular stereo vision system[J].Electronic Science and Technology,2016,29(3):68-70.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Cost-Volume Filtering for Visual Correspondence and Beyond">

                                <b>[2]</b> Hosni A,Rhemann C,Bleyer M,et al.Fast cost-volume filtering for visual correspondence and beyond[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(2):504-11.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201108005&amp;v=MTAxNDBJVGZBWmJHNEg5RE1wNDlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkza1Vidk8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 林川.基于新扫描策略的快速立体匹配算法[J].电子科技,2011,24(8):12-15.Lin Chuan.Fast stereo matching algorithm based on new scanning strategy[J].Electronic Science and Technology,2011,24(8):12-15.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A stereo matching algorithm with an adaptive window: Theory and experiment">

                                <b>[4]</b> Kanade T,Okutomi M.A stereo matching algorithm with an adaptive window:Theory and experiment[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1994,16(9):920-932.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive support-weight approach for correspondence search">

                                <b>[5]</b> Yoon K J,Kweon I S.Adaptive support-weight approach for correspondence search[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(4):650-656.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo Processing by Semiglobal Matching and Mutual Information">

                                <b>[6]</b> Hirschmuller H.Stereo processing by semiglobal matching and mutual information[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2008,30(2):328-341.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300162493&amp;v=MDYwMTZRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmpJSlY4V2JoUT1OaWZPZmJLOUg5UE9ySTlGWmUwTkNIVTZvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Cheng F,Zhang H,Sun M,et al.Cross-trees,edge and superpixel priors-based cost aggregation for Stereo matching.[J].Pattern Recognition,2015,48(7):2269-2278.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201702007&amp;v=MTEzMzZVYnZPTHp6WmZMRzRIOWJNclk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5M2s=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 刘杰,张建勋,代煜.基于区域增长的稠密立体匹配[J].机器人,2017,39(2):182-188.Liu Jie,Zhang Jianxun,Dai Yu.Dense stereo matching based on region growing[J].Robot,2017,39(2):182-188.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint histogram-based cost aggregation for stereo matching">

                                <b>[9]</b> Min D,Lu J,Do M N.Joint histogram-based cost aggregation for stereo matching[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(10):2539-2545.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD14123000000280&amp;v=MTIyMzVXYmhRPU5pZkRhcks4SDlQUHI0OUZaT3NQRG5RNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUpWOA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Wang L Q,Liu Z,Zhang Z H.Feature based stereo matching using two-step expansion[J].Mathematical Problems in Engineering,2014(14):452803-452809.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201604028&amp;v=MjEwMjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeTNrVWJ2T0lqWFRiTEc0SDlmTXE0OUhiSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 祝世平,闫利那,李政.基于改进 Census 变换和动态规划的立体匹配算法[J].光学学报,2016,36(4):0415001-415018.Zhu Shiping,Yan Lina,Li Zheng.Stereo Matching algorithm based on Improved census transform and dynamic programming[J].Acta Optics Sin,2016,36(4):0415001-0415018.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MTk3NDVYcVJyeG94Y01IN1I3cWRaK1p1RmkvbFZyN0tKRmc9Tmo3QmFyTzRIdEhPcDR4Rlkra0xZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision,2002,47(1-3):7-42.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph cut based continuous stereo matching using locally shared labels">

                                <b>[13]</b> Taniai T,Matsushita Y,Naemura T.Graph cut based continuous stereo matching using locally shared labels[C].Columbus:IEEE Conference on Computer Vision and Pattern Recognition,IEEE Computer Society,2014.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matching">

                                <b>[14]</b> Yang Q.A non-local cost aggregation method for stereo matching[C].Providence:Computer Vision and Pattern Recognition,IEEE,2012.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time Accurate Stereo Matching Using Modified Two-pass Aggregation and Winner-take-all Guided Dynamic Programming">

                                <b>[15]</b> Chang X,Zhou Z,Wang L,et al.Real-time accurate stereo matching using modified two-pass aggregation and winner-take-all guided dynamic programming[C].Hangzhou:International Conference on 3D Imaging,Modeling,Processing,Visualization and Transmission,2011.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast cost-volume filtering for visual correspondence and beyond">

                                <b>[16]</b> Rhemann C,Hosni A,Bleyer M,et al.Fast cost-volume filtering for visual correspondence and beyond[C].Colorado Springs:IEEE Conference on Computer Vision and Pattern Recognition,2011.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A local iterative refinement method for adaptive support-weight stereo matching">

                                <b>[17]</b> Psota E T,Kowalczuk J,Carlson J,et al.A local iterative refinement method for adaptive support-weight stereo matching[C].Las Vegas:Proceedings of the International Conference on Image Processing,Computer Vision,and Pattern Recognition (IPCV),2011.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global stereo matching leveraged by sparse ground control points">

                                <b>[18]</b> Wang L,Yang R.Global stereo matching leveraged by sparse ground control points[C].Colorado Springs:IEEE Conference on Computer Vision and Pattern Recognition,2011.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZKK201911004" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201911004&amp;v=MDE4NTlPSVRmQVpiRzRIOWpOcm85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5M2tVYnY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZONEl2ZXMzNXJlUkZjWFB4bHhwYz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=vAscMyvIPP9NePnbGPkqJ0A5tHOvnzl65tGuRDsf9xg1" rel="stylesheet"/>

</body>
</html>

