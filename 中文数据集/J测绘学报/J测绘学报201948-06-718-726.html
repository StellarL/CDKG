<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142622218076250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201906007%26RESULT%3d1%26SIGN%3dYpAgKb6rxesu1HUg57Qp3xJxs58%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906007&amp;v=MjM3MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTNsVXJ6SUppWFRiTEc0SDlqTXFZOUZZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="1 方 法 ">1 方 法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="1.1 可变形卷积网络">1.1 可变形卷积网络</a></li>
                                                <li><a href="#110" data-title="1.2 全连接条件随机场后处理">1.2 全连接条件随机场后处理</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="2 试验结果与分析 ">2 试验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#117" data-title="2.1 数据集">2.1 数据集</a></li>
                                                <li><a href="#120" data-title="2.2 数据预处理">2.2 数据预处理</a></li>
                                                <li><a href="#123" data-title="2.3 分割试验">2.3 分割试验</a></li>
                                                <li><a href="#132" data-title="2.4 分析与评价">2.4 分析与评价</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="3 结 论 ">3 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="图1 标准卷积与可变形卷积">图1 标准卷积与可变形卷积</a></li>
                                                <li><a href="#109" data-title="图2 带有编码器和解码器的SegNet架构">图2 带有编码器和解码器的SegNet架构</a></li>
                                                <li><a href="#115" data-title="图3 语义分割流程">图3 语义分割流程</a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;表1 ISPRS Vaihingen数据集语义分割结果&lt;/b&gt;"><b>表1 ISPRS Vaihingen数据集语义分割结果</b></a></li>
                                                <li><a href="#138" data-title="图4 试验结果">图4 试验结果</a></li>
                                                <li><a href="#143" data-title="图5 可变形滤波器的采样位置">图5 可变形滤波器的采样位置</a></li>
                                                <li><a href="#144" data-title="图6 可变形卷积与条件随机场的分割对比">图6 可变形卷积与条件随机场的分割对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" 刘婧, 李培军.结合结构和光谱特征的高分辨率影像分割方法[J].测绘学报, 2014, 43 (5) :466-473.DOI:10.13485/j.cnki.112089.2014.0087.LIU Jing, LI Peijun.A high resolution image segmentation method by combined structural and spectral characteristics[J].Acta Geodaetica et Cartographica Sinica, 2014, 43 (5) :466-473.DOI:10.13485/j.cnki.11-2089.2014.0087." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201405006&amp;v=MTA1NTZVcnpJSmlYVGJMRzRIOVhNcW85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2w=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         刘婧, 李培军.结合结构和光谱特征的高分辨率影像分割方法[J].测绘学报, 2014, 43 (5) :466-473.DOI:10.13485/j.cnki.112089.2014.0087.LIU Jing, LI Peijun.A high resolution image segmentation method by combined structural and spectral characteristics[J].Acta Geodaetica et Cartographica Sinica, 2014, 43 (5) :466-473.DOI:10.13485/j.cnki.11-2089.2014.0087.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     周成虎, 骆剑承.高分辨率卫星遥感影像地学计算[M].北京:科学出版社, 2009.ZHOU Chenghu, LUO Jiancheng.Geo-computing of high resolution satellite remote sensing image[M].Beijing:Science Press, 2009.</a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston, MA, USA:IEEE, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[3]</b>
                                         LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston, MA, USA:IEEE, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" WANG Hongzhen, WANG Ying, ZHANG Qian, et al.Gated convolutional neural network for semantic segmentation in high-resolution images[J].Remote Sensing, 2017, 9 (5) :446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated convolutional neural network for sematic segmentation in high-resolution images">
                                        <b>[4]</b>
                                         WANG Hongzhen, WANG Ying, ZHANG Qian, et al.Gated convolutional neural network for semantic segmentation in high-resolution images[J].Remote Sensing, 2017, 9 (5) :446.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" GARCIA-GARCIA A, ORTS-ESCOLANO S, OPREA S, et al.A review on deep learning techniques applied to semantic segmentation[J].arXiv:1704.06857, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A review on deep learning techniques applied to semantic segmentation">
                                        <b>[5]</b>
                                         GARCIA-GARCIA A, ORTS-ESCOLANO S, OPREA S, et al.A review on deep learning techniques applied to semantic segmentation[J].arXiv:1704.06857, 2017.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" HUBEL D H.The visual cortex of the brain[J].Scientific American, 1963, 209:54-62.DOI:10.1038/scientificamerican1163-54." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=THE VISUAL CORTEX OF THE BRAIN">
                                        <b>[6]</b>
                                         HUBEL D H.The visual cortex of the brain[J].Scientific American, 1963, 209:54-62.DOI:10.1038/scientificamerican1163-54.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" KR&#196;HENB&#220;HL P, KOLTUN V.Efficient inference in fully connected crfs with gaussian edge potentials[J].arXiv:1210.5644, 2012:109-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient inference in fully connected crfs with gaussian edge potentials">
                                        <b>[7]</b>
                                         KR&#196;HENB&#220;HL P, KOLTUN V.Efficient inference in fully connected crfs with gaussian edge potentials[J].arXiv:1210.5644, 2012:109-117.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" SHOTTON J, WINN J, ROTHER C, et al.Textonboost for image understanding:multi- class object recognition and segmentation by jointly modeling texture, layout, and context[J].International Journal of Computer Vision, 2009, 81 (1) :2-23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110100309604&amp;v=MDgzODBaK3NHQ253OW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUpWNFZhUkk9Tmo3QmFySzlIOURNcm85Rg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         SHOTTON J, WINN J, ROTHER C, et al.Textonboost for image understanding:multi- class object recognition and segmentation by jointly modeling texture, layout, and context[J].International Journal of Computer Vision, 2009, 81 (1) :2-23.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" DAI Jifeng, QI Haozhi, XIONG Yuwen, et al.Deformable convolutional networks[J].arXiv:1703.06211, 2017:764-773." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deformable convolutional networks">
                                        <b>[9]</b>
                                         DAI Jifeng, QI Haozhi, XIONG Yuwen, et al.Deformable convolutional networks[J].arXiv:1703.06211, 2017:764-773.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" LUO Wenjie, LI Yujia, URTASUN R, et al.Understanding the effective receptive field in deep convolutional neural networks[J].arXiv:1701.04128, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the effective receptive field in deep convolutional neural networks">
                                        <b>[10]</b>
                                         LUO Wenjie, LI Yujia, URTASUN R, et al.Understanding the effective receptive field in deep convolutional neural networks[J].arXiv:1701.04128, 2017.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[11]</b>
                                         SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" HOLSCHNEIDER M, KRONLAND-MARTINET R, MORLET J, et al.A real-time algorithm for signal analysis with the help of the wavelet transform[M].COMBES J M, GROSSMANN A, TCHAMITCHIAN P.Wavelets:Time-Frequency Methods and Phase Space.Berlin:Springer, 1990:286-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A real-time algorithm forsignal analysis with the help of the wavelet transform">
                                        <b>[12]</b>
                                         HOLSCHNEIDER M, KRONLAND-MARTINET R, MORLET J, et al.A real-time algorithm for signal analysis with the help of the wavelet transform[M].COMBES J M, GROSSMANN A, TCHAMITCHIAN P.Wavelets:Time-Frequency Methods and Phase Space.Berlin:Springer, 1990:286-297.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" DENG Jia, DONG Wei, SOCHER R, et al.Imagenet:a large-scale hierarchical image database[C]//Proceedings of 2009 IEEE Conference on Computer Vision and Pattern Recognition.Miami, FL, USA:IEEE, 2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:A Large-Scale Hierarchical Image Database">
                                        <b>[13]</b>
                                         DENG Jia, DONG Wei, SOCHER R, et al.Imagenet:a large-scale hierarchical image database[C]//Proceedings of 2009 IEEE Conference on Computer Vision and Pattern Recognition.Miami, FL, USA:IEEE, 2009.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     DAI Jifeng, LI Yi, HE Kaiming, et al.R-FCN:object detection via region-based fully convolutional networks[C]//Proceedings of the 30th International Conference on Neural Information Processing Systems.Barcelona, Spain:ACM, 2016:379-387.</a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" BADRINARAYANAN V, KENDALL A, CIPOLLA R.SegNet:a deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (12) :2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">
                                        <b>[15]</b>
                                         BADRINARAYANAN V, KENDALL A, CIPOLLA R.SegNet:a deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (12) :2481-2495.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" CHEN L C, PAPANDREOU G, KOKKINOS I, et al.Semantic image segmentation with deep convolutional nets and fully connected CRFs[C]//Proceedings of the International Conference on Learning Representations.San Diego, CA:Computational and Biological Learning Society, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic image segmentation with deep convolutional nets and fully connected CRFS">
                                        <b>[16]</b>
                                         CHEN L C, PAPANDREOU G, KOKKINOS I, et al.Semantic image segmentation with deep convolutional nets and fully connected CRFs[C]//Proceedings of the International Conference on Learning Representations.San Diego, CA:Computational and Biological Learning Society, 2015.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" CHATFIELD K, SIMONYAN K, VEDALDI A, et al.Return of the devil in the details:delving deep into convolutional nets[C]//Proceedings of the British Machine Vision Conference.Dundee, Britain:BMVA Press, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Return of the Devil in the Details:Delving Deep into Convolutional Nets">
                                        <b>[17]</b>
                                         CHATFIELD K, SIMONYAN K, VEDALDI A, et al.Return of the devil in the details:delving deep into convolutional nets[C]//Proceedings of the British Machine Vision Conference.Dundee, Britain:BMVA Press, 2014.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[18]</b>
                                         SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_19" title=" IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.Lille, France:JMLR, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[19]</b>
                                         IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.Lille, France:JMLR, 2015:448-456.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_20" title=" HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Delving deep into rectifiers:surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE International Conference on Computer Vision.Santiago, Chile:IEEE, 2015:1026-1034." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving deep into rectifiers:Surpassing human-level performance on imagenet classification">
                                        <b>[20]</b>
                                         HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Delving deep into rectifiers:surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE International Conference on Computer Vision.Santiago, Chile:IEEE, 2015:1026-1034.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_21" title=" CLEVERT D A, UNTERTHINER T, HOCHREITER S.Fast and accurate deep network learning by exponential linear units (ELUs) [C]//Proceedings of the International Conference on Learning Representations.San Diego, CA:Computational and Biological Learning Society, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and accurate deep network learning by exponential linear units (ELUs)">
                                        <b>[21]</b>
                                         CLEVERT D A, UNTERTHINER T, HOCHREITER S.Fast and accurate deep network learning by exponential linear units (ELUs) [C]//Proceedings of the International Conference on Learning Representations.San Diego, CA:Computational and Biological Learning Society, 2015.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_22" title=" EIGEN D, FERGUS R.Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture[C]//Proceedings of the IEEE International Conference on Computer Vision.Santiago, Chile:IEEE, 2015:2650-2658." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture">
                                        <b>[22]</b>
                                         EIGEN D, FERGUS R.Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture[C]//Proceedings of the IEEE International Conference on Computer Vision.Santiago, Chile:IEEE, 2015:2650-2658.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                     MOSTAJABI M, YADOLLAHPOUR P, SHAKHNAROVICH G.Feedforward semantic segmentation with zoom-out features[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston, MA, USA:IEEE, 2015:3376-3385.</a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_24" title=" ROTHER C, KOLMOGOROV V, BLAKE A.Grabcut:interactive foreground extraction using iterated graph cuts[C]//Proceedings of the ACM SIGGRAPH 2004.Los Angeles, California:ACM, 2004:309-314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Grabcut-interactive foreground extraction using iterated graph cuts">
                                        <b>[24]</b>
                                         ROTHER C, KOLMOGOROV V, BLAKE A.Grabcut:interactive foreground extraction using iterated graph cuts[C]//Proceedings of the ACM SIGGRAPH 2004.Los Angeles, California:ACM, 2004:309-314.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_25" title=" KOHLI P, LADICKY L, TORR P H S.Robust higher order potentials for enforcing label consistency[C]//Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition.Anchorage, AK, USA:IEEE, 2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust higher order potentials for enforcing labelconsistency">
                                        <b>[25]</b>
                                         KOHLI P, LADICKY L, TORR P H S.Robust higher order potentials for enforcing label consistency[C]//Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition.Anchorage, AK, USA:IEEE, 2009.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_26" title=" KR&#196;HENB&#220;HL P, KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of the 25th annual conference on Neural Information Processing Systems.Granada:NIF, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient inference in fully connected CRFs w ith gaussian edge potentials">
                                        <b>[26]</b>
                                         KR&#196;HENB&#220;HL P, KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of the 25th annual conference on Neural Information Processing Systems.Granada:NIF, 2011.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_27" title=" ADAMS A, BAEK J, DAVIS M A.Fast high-dimensional filtering using the permutohedral lattice[J].Computer Graphics Forum, 2010, 29 (2) :753-762." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000018133&amp;v=MjQ3MTBPNEh0SE1yNDVOWmVnTVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RnlubFZyL0pJMTQ9TmlmY2Fy&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         ADAMS A, BAEK J, DAVIS M A.Fast high-dimensional filtering using the permutohedral lattice[J].Computer Graphics Forum, 2010, 29 (2) :753-762.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_28" title=" GERKE M, ROTTENSTEINER F, WEGNER J D, et al.ISPRS semantic labeling contest[C]//Proceedings of PCV-Photogrammetric Computer Vision.[S.l.]:ISPRS, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ISPRS semantic labeling contest">
                                        <b>[28]</b>
                                         GERKE M, ROTTENSTEINER F, WEGNER J D, et al.ISPRS semantic labeling contest[C]//Proceedings of PCV-Photogrammetric Computer Vision.[S.l.]:ISPRS, 2014.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_29" title=" GERKE M.Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen) [C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .Boston, MA, USA:IEEE, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen)">
                                        <b>[29]</b>
                                         GERKE M.Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen) [C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .Boston, MA, USA:IEEE, 2015.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_30" title=" PAISITKRIANGKRAI S, SHERRAH J, JANNEY P, et al.Effective semantic pixel labelling with convolutional networks and conditional random fields[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.Boston, MA, USA:IEEE, 2015:36-43." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective semantic pixel labelling with convolutional networks and conditional random fields">
                                        <b>[30]</b>
                                         PAISITKRIANGKRAI S, SHERRAH J, JANNEY P, et al.Effective semantic pixel labelling with convolutional networks and conditional random fields[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.Boston, MA, USA:IEEE, 2015:36-43.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_31" title=" AUDEBERT N, LE SAUX B, LEF&#200;VRE S.Semantic segmentation of earth observation data using multimodal and multi-scale deep networks[M]//LAI S H, LEPETIT V, NISHINO K, et al.Computer Vision-ACCV 2016.Cham:Springer, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation of earth observation data using multimodal and multi-scale deep networks">
                                        <b>[31]</b>
                                         AUDEBERT N, LE SAUX B, LEF&#200;VRE S.Semantic segmentation of earth observation data using multimodal and multi-scale deep networks[M]//LAI S H, LEPETIT V, NISHINO K, et al.Computer Vision-ACCV 2016.Cham:Springer, 2017.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_32" title=" ZHOU Hao, ZHANG Jun, LEI Jun, et al.Image semantic segmentation based on FCN-CRF model[C]//Proceedings of International Conference on Image, Vision and Computing.Portsmouth, UK:IEEE, 2016:9-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Semantic Segmentation Based on FCN-CRF Model">
                                        <b>[32]</b>
                                         ZHOU Hao, ZHANG Jun, LEI Jun, et al.Image semantic segmentation based on FCN-CRF model[C]//Proceedings of International Conference on Image, Vision and Computing.Portsmouth, UK:IEEE, 2016:9-14.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(06),718-726 DOI:10.11947/j.AGCS.2019.20170740            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合可变形卷积与条件随机场的遥感影像语义分割方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B7%A6%E5%AE%97%E6%88%90&amp;code=42097125&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">左宗成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%87&amp;code=09036434&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B8%9C%E6%98%A0&amp;code=42097127&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张东映</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E9%81%A5%E6%84%9F%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学遥感信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AC%A7%E7%89%B9%E5%85%8B(%E4%B8%AD%E5%9B%BD)%E8%BD%AF%E4%BB%B6%E7%A0%94%E5%8F%91%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0779226&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">欧特克(中国)软件研发有限公司</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%83%91%E5%B7%9E%E5%A4%A7%E5%AD%A6%E6%B0%B4%E5%88%A9%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑州大学水利与环境学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>当前, 深度卷积神经网络在遥感影像语义分割领域取得了长足的发展。标准的卷积神经网络由于卷积核的几何形状是固定的, 导致对几何变换的模拟能力受到限制。本文引入一种可变形卷积来增强卷积网络对空间变换的适应能力。由于神经网络架构中使用了池化层操作, 这会导致在输出层未能充分地对局部对象进行准确的分割。为了克服这种特性, 本文将神经网络输出层的粗糙预测分割结果通过全连接的条件随机场来进行处理, 以此来提高对影像细节的分割能力。本文方法易于采用标准的反向传播算法进行端到端的方式训练。ISPRS数据集上的测试试验结果表明本文方法可以有效地克服遥感影像中分割对象的复杂结构对分割结果的影响, 并在该数据集上获得了当前最好的语义分割结果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高分辨率遥感影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可变形卷积网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件随机场;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    左宗成 (1988—) , 男, 硕士, 工程师, 研究方向为高分辨遥感影像处理及信息提取、模式识别与机器学习。;
                                </span>
                                <span>
                                    *张文, E-mail:wen_zhang@whu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFC0405806);</span>
                    </p>
            </div>
                    <h1>A remote sensing image semantic segmentation method by combining deformable convolution with conditional random fields</h1>
                    <h2>
                    <span>ZUO Zongcheng</span>
                    <span>ZHANG Wen</span>
                    <span>ZHANG Dongying</span>
            </h2>
                    <h2>
                    <span>School of Remote Sensing and Information Engineering, Wuhan University</span>
                    <span>Autodesk (China) Software Research and Development Co.Ltd.</span>
                    <span>College of Water Conservancy & Environmental Engineering, Zhengzhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Currently, deep convolutional neural networks have made great progress in the field of semantic segmentation. Because of the fixed convolution kernel geometry, standard convolution neural networks have been limited the ability to simulate geometric transformations. Therefore, a deformable convolution is introduced to enhance the adaptability of convolutional networks to spatial transformation. Considering that the deep convolutional neural networks cannot adequately segment the local objects at the output layer due to using the pooling layers in neural networks architecture. To overcome this shortcoming, the rough prediction segmentation results of the neural network output layer will be processed by fully connected conditional random fields to improve the ability of image segmentation. The proposed method can easily be trained by end-to-end using standard backpropagation algorithms. Finally, the proposed method is tested on the ISPRS dataset. The results show that the proposed method can effectively overcome the influence of the complex structure of the segmentation object and obtain state-of-the-art accuracy on the ISPRS Vaihingen 2 D semantic labeling dataset.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=high-resolution%20remote%20sensing%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">high-resolution remote sensing image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deformable%20convolution%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deformable convolution network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=conditions%20random%20fields&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">conditions random fields;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZUO Zongcheng (1988—) , male, master, engineer, majors in high resolution remote sensing image processing and information extraction, pattern recognition and machine learning.E-mail: jason.zuo@autodesk.com;
                                </span>
                                <span>
                                    ZHANG Wen, E-mail: wen_zhang@whu.edu.cn;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2017-12-23</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National Key Research and Development Program of China (No.2017YFC0405806);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="66">高分辨率遥感影像已经在制图、城市规划、灾害监测、房地产管理、计量经济学、作物分类和气候研究等多领域得到应用<citation id="147" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。遥感影像语义分割作为高分辨率遥感影像信息提取与目标识别的前提和基础, 是实现从数据到信息的对象化提取的过渡环节和关键步骤, 具有十分重要的意义<citation id="148" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。图像语义分割不同于图像分类或物体检测等任务, 图像语义分割是一个空间密集型的预测任务, 换言之, 这需要预测一幅图像中所有像素点的类别<citation id="149" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。语义分割旨在分类每一个像素到指定的类别, 是一种对于理解和推理对象以及场景中物体之间关系的重要任务。作为通向高级任务的桥梁, 在计算机视觉和遥感领域中, 语义分割被用在了多种应用中, 例如自动驾驶、姿态估计、遥感影像解译及3D重建等<citation id="150" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。传统的特征提取难以应付空间变换的要求, 并且手工特征设计难度大。自从FCNs<citation id="151" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>首次被用于图像的语义分割之后, 新的方法不断被创造。近年来, 由于深度卷积神经网络的广泛使用使得密集型预测的语义分割取得了长足的发展。最近的工作都表明, 在许多图像处理任务中, 深度学习模型往往显著优于传统的方法<citation id="152" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="67">标准的卷积操作通常是在特征图谱固定的位置上进行采样, 对于复杂目标对象的检测来说不是很合理, 这是因为不同位置对应的目标大小是不同的。如果能够使得感受野<citation id="153" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在不同位置的大小进行自适应调整, 那么对于语义分割任务必然有很大的帮助。</p>
                </div>
                <div class="p1">
                    <p id="68">其次, 对于深度卷积神经网络来说, 从分类器获取以对象为中心的决策需要空间不变性, 其内在特性限制了深度卷积神经网络 (DCNNs) 的空间精度模型, 并且DCNNs的最后一层通常没有充分地对局部对象进行分割, 所以本文通过采用全连接的条件随机场来提升本文方法捕获细节的能力<citation id="154" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。条件随机场 (CRF) 已广泛应用于语义分割, 以便将由多方向分类器计算的类别得分与由像素和边缘的局部交互所捕获的低级信息进行综合<citation id="155" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。因此本文提出一种融合可变形卷积与条件随机场的方法来解决标准卷积操作对空间自适应能力的欠缺以及采用条件随机场的方法来提高局部分割精度。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag">1 方 法</h3>
                <div class="p1">
                    <p id="70">本文方法主要分为3个步骤:①对常规的标准卷积添加了一项二维偏移量到采样网格中, 得到一种可变形卷积网络<citation id="156" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 该项偏移量的添加使得卷积网络可以自由形成变形, 这些偏移量是通过卷积层抽取的特征图谱进行学习而来;②训练的网络采用VGGNet网络的参数进行初始化, 反卷积网络作为分割结果层进行预测每个像素的分类结果;③对输出的粗糙预测分割结果进行CRF操作, 得到精细化的分割图。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">1.1 可变形卷积网络</h4>
                <h4 class="anchor-tag" id="72" name="72">1.1.1 可变形卷积</h4>
                <div class="p1">
                    <p id="73">传统的二维图像上的卷积操作包含两个步骤:①在输入特征图谱<i>x</i>上采用常规网格<i>R</i>进行采样;②由<i>w</i>加权的采样值的求和。网格<i>R</i>定义了感受野的大小和步幅。例如:<i>R</i>={ (-1, -1) , (-1, 0) , (-1, 1) , (0, 1) , (0, 0) , (0, -1) , (1, 1) , (1, 0) , (1, -1) }定义了一个步幅为1的3×3卷积核。对于输出特征图谱<i>y</i>上的每个位置<i>p</i><sub>0</sub>可得到</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo>∈</mo><mi>R</mi></mrow></munder><mi>w</mi></mstyle><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>×</mo><mi>x</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="76">可变形卷积使用偏移{Δ<i>p</i><sub><i>n</i></sub>|<i>n</i>=1, …, <i>N</i>}扩充了常规网格<i>R</i>, 这里的<i>N</i>=|<i>R</i>|。上述方程变为</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo>∈</mo><mi>R</mi></mrow></munder><mi>w</mi></mstyle><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>×</mo><mi>x</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo>+</mo><mtext>Δ</mtext><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">现在, 采样在不规则和偏移位置<i>p</i><sub><i>n</i></sub>+Δ<i>p</i><sub><i>n</i></sub>之上。由于偏移Δ<i>p</i><sub><i>n</i></sub>通常是小数形式的, 式 (2) 通过双线性插值变换之后变为</p>
                </div>
                <div class="p1">
                    <p id="79"><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>q</mi></munder><mi>G</mi></mstyle><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>p</mi><mo stretchy="false">) </mo><mo>×</mo><mi>x</mi><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="81">式中, <i>p</i>代表一个任意位置 (<i>p</i>=<i>p</i><sub>0</sub>+<i>p</i><sub><i>n</i></sub>+Δ<i>p</i><sub><i>n</i></sub>) ;<i>q</i>枚举特征图谱<i>x</i>中的所有的积分空间位置;<i>G</i> (., .) 代表双线性插值核。这里的<i>G</i>是二维的, 它可以被分为两个一维的内核</p>
                </div>
                <div class="p1">
                    <p id="82"><i>G</i> (<i>q</i>, <i>p</i>) =<i>g</i> (<i>q</i><sub><i>x</i></sub>, <i>p</i><sub><i>x</i></sub>) <i>g</i> (<i>q</i><sub><i>y</i></sub>, <i>p</i><sub><i>y</i></sub>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="83">式中, <i>g</i> (<i>a</i>, <i>b</i>) =max (0, 1-|<i>a</i>-<i>b</i>|) 。式 (4) 是快速计算, 因为<i>G</i> (<i>q</i>, <i>p</i>) 仅对于部分<i>q</i>的取值是非零的。</p>
                </div>
                <div class="p1">
                    <p id="84">如式 (2) 所示, 可变形卷积是通过在相同的输入特征图谱上应用卷积层来获得偏移。可变形卷积核与当前卷积层也是具有相同的空间分辨率。输出偏移量与输入特征图谱具有相同的空间分辨率。当通道维数为2<i>N</i>时, 需要编码<i>N</i>个二维偏移向量。在神经网络训练期间, 可以学习用于产生输出特征和产生偏移量的卷积内核。在可变形卷积模块上执行的梯度可以通过式 (3) 和式 (4) 中的双线性运算来反向传播。</p>
                </div>
                <div class="p1">
                    <p id="85">文献<citation id="157" type="reference">[<a class="sup">10</a>]</citation>发现, 感受野中并不是所有的像素都有助于输出单元响应。由于靠近中心的像素具有更大的影响, 所以有效感受野只占理论感受野中的小部分, 并服从高斯分布。虽然理论感受野尺寸随着卷积层的数量线性增加, 但让人感到意外的是, 有效感受野尺寸是随着卷积层数量的平方根线性增加, 因此以比预期慢得多的速率进行收敛<citation id="158" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。这一发现表明, 即便是CNNs中的顶层单元也可能没有足够大的感受野。这部分也解释了为什么空洞卷积<citation id="159" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>被广泛应用于视觉任务。它揭示了可自适应性感受野学习的必要性。可变形卷积能够自适应地学习感受野, 如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="86">在标准卷积中的空间采样位置增加了额外的偏移量就可以得到可变形卷积。这些偏移量是从目标任务驱动的数据中学习的。当可变形模块堆叠成多层时, 复合变形的影响是巨大的。相较于标准卷积而言, 对于复杂目标可变形卷积有着强大的自适应提取能力。如图1所示, 标准卷积滤波器中的感受野和采样位置在上层特征图谱上是固定的 (图1 (a) ) ;当使用可变形卷积时, 它们会根据物体的尺度和形状进行自适应调整 (图1 (b) ) 。特别是对于非刚性物体增强了目标定位能力。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906007_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 标准卷积与可变形卷积" src="Detail/GetImg?filename=images/CHXB201906007_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 标准卷积与可变形卷积  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906007_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Standard convolution and deformable convolution</p>

                </div>
                <h4 class="anchor-tag" id="88" name="88">1.1.2 可变形卷积的网络结构</h4>
                <div class="p1">
                    <p id="89">可变形卷积与标准卷积有着相同的输入和输出。因此, 在神经网络结构中可变形卷积可以很容易替换标准卷积操作。在训练期间, 可以学习用于产生输出特征和产生偏移量的卷积内核。在可变形卷积模块上执行的梯度可以通过式 (3) 中的双线性运算来反向传播。他们通过反向传播训练, 通过方程式中的双线性插值运算, 所得到的CNNs称为可变形网络 (deformable ConvNets) 。可变形网络自动学习预测影像中物体对象的位置, 它是基于增强空间采样位置的能力为考量并使用非监督方法从目标任务中学习偏移量。</p>
                </div>
                <div class="p1">
                    <p id="90">为了将可变形ConvNets与现有的CNNs架构相结合, 本文提出由3个阶段组成的流程来实现可变形卷积网络。首先, 深度全卷积网络 (DFCNs) 在整个输入图像上生成特征图谱;其次, 从特征图谱中生成分割结果;最后, 对于粗糙的结果进行反卷积得到精细化的分割图。以下详细说明这3个步骤。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"> (1) 可变形卷积的特征提取。</h4>
                <div class="p1">
                    <p id="92">本文采用广泛使用且具有良好性能的特征提取架构VGGNet<citation id="160" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>作为特征提取层。网络的初始化参数在ImageNet<citation id="161" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>分类数据集上进行了预训练。这个VGGNet模型是由卷积层、平均池化层和一个用于ImageNet分类的1000路全连接层组成。本文删除了平均池化层和全连接层。像通常的做法<citation id="162" type="reference"><link href="28" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>一样, 本文将最后卷积块中的有效步幅从32像素减少到16像素, 以增加特征映射分辨率。本文中可变形卷积层被施加在最后3层作为特征提取层。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"> (2) 语义分割网络。</h4>
                <div class="p1">
                    <p id="94">当前有许多可用的语义分割的网络架构。本文选择SegNet架构<citation id="163" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation> (图2) , 因为它提供了精度与计算成本之间的良好平衡。以此结构为基础, 嵌入可变形卷积结构。SegNet的对称架构使得它对池化层及反卷积的使用非常有效, 这对于遥感数据来说至关重要。除了SegNet, 还对DeepLab<citation id="164" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>进行了初步试验, 结果显示并没有显著改善甚至没有改善。因此, 没有必要切换到更昂贵的计算架构。请注意, 本文方法可以轻松地适应其他架构, 而不是限定于SegNet的架构。SegNet具有VGG-16的卷积层的编码器与解码器架构<citation id="168" type="reference"><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>。编码器由一系列卷积层组成, 每个卷积层后面紧跟着的是批量归一化<citation id="165" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和整流线性单元。卷积块之后是池化层, 本文采用步幅为2的最大池化层操作。没有试验其他的激活函数, 如PReLU<citation id="166" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>或ELU<citation id="167" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 也没有进一步改变SegNet架构。这样做的目的是为了在后续试验中对比可变形卷积的性能。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"> (3) 反卷积网络。</h4>
                <div class="p1">
                    <p id="96">解码器的结构与编码器是对称的, 解码器对于输入结果进行上采样, 本文使用文献<citation id="169" type="reference">[<a class="sup">24</a>]</citation>中的策略随机初始化解码器中网络的权重。在一般的CNNs结构中, 如AlexNet、VGGNet均使用了池化操作来缩小输出图片的尺寸, 例如VGGNet, 5次池化操作后输入图像的尺寸被缩小了32倍。本文目的是得到一个与原图像尺寸相同的分割图, 因此需要对最后一层特征提取层进行上采样。卷积网络输出的特征图谱是缩小的尺寸, 因为最终预测的分割图是基于像素的, 所以需要对卷积网络的输出进行反卷积操作。反卷积就是卷积计算的逆过程, 可以从卷积过程中来推导反卷积的过程</p>
                </div>
                <div class="p1">
                    <p id="97"><i>W</i><sup><i>n</i>+1</sup>= (<i>W</i><sup><i>n</i></sup>+2<i>p</i>-<i>k</i>) /<i>s</i>+1      (5) </p>
                </div>
                <div class="p1">
                    <p id="98"><i>H</i><sup><i>n</i>+1</sup>= (<i>H</i><sup><i>n</i></sup>+2<i>p</i>-<i>k</i>) /<i>s</i>+1      (6) </p>
                </div>
                <div class="p1">
                    <p id="99">式中, <i>W</i>表示宽度;<i>p</i>表示填充像素大小;<i>k</i>表示卷积核尺寸;<i>s</i>表示卷积的步幅。可以很容易推导到反卷积的公式</p>
                </div>
                <div class="p1">
                    <p id="100"><i>W</i><sup><i>n</i></sup>=<i>s</i>+<i>k</i>-2<i>p</i> (<i>W</i><sup><i>n</i>+1</sup>-1)      (7) </p>
                </div>
                <div class="p1">
                    <p id="101"><i>H</i><sup><i>n</i></sup>=<i>s</i>+<i>k</i>-2<i>pH</i><sup><i>n</i>+1</sup>      (8) </p>
                </div>
                <div class="p1">
                    <p id="102">本文使用卷积特征图谱的后3层作为反卷积的输入特征。</p>
                </div>
                <div class="p1">
                    <p id="103">综合以上描述, 可以得到一个详细的网络模型结构 (图2) 。令<i>N</i>为图像中的像素数, <i>k</i>为类别的数量, 对于指定的像素<i>i</i>, 令<i>y</i><sup><i>i</i></sup>表示其标签类别, [<i>z</i><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, …, <i>z</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup></mrow></math></mathml>]为预测向量, 最小化输入影像上Softmax输出的多项式Logistic损失的归一化和</p>
                </div>
                <div class="p1">
                    <p id="106"><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>l</mtext><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>y</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup><mrow><mi>log</mi></mrow><mrow><mo> (</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>z</mi><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>z</mi><msubsup><mrow></mrow><mi>l</mi><mi>i</mi></msubsup><mo stretchy="false">) </mo></mrow></mfrac></mtd></mtr><mtr><mtd></mtd></mtr></mtable><mo>) </mo></mrow></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="108">式 (9) 即为训练网络的损失函数。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906007_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 带有编码器和解码器的SegNet架构" src="Detail/GetImg?filename=images/CHXB201906007_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 带有编码器和解码器的SegNet架构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906007_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 SegNet architecture with an encoder and a decoder</p>

                </div>
                <h4 class="anchor-tag" id="110" name="110">1.2 全连接条件随机场后处理</h4>
                <div class="p1">
                    <p id="111">DCNNs得分图可以可靠地预测图像中已知的对象和粗略位置, 但不太适合用于指向其精确的轮廓。卷积网络的分类准确度和定位精度之间有一个合适的权衡:具有多个最大池化层的深层模型在分类任务中被证明是成功的, 然而过多的池化层所带来的对空间不变性的提升与有效感受野增大的同时也使得从网络输出层根据得分推断位置变得更具挑战性。</p>
                </div>
                <div class="p1">
                    <p id="112">解决这个问题目前有两种方法:第1种方法是利用卷积网络中多尺度的信息来更好地估计对象边界<citation id="171" type="reference"><link href="6" rel="bibliography" /><link href="44" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">22</a>]</sup></citation>;第2种方法是采用超像素表示, 基本上将定位任务委托给低级分割方法, 这种解决思路由文献<citation id="170" type="reference">[<a class="sup">23</a>]</citation>提出。</p>
                </div>
                <div class="p1">
                    <p id="113">条件随机场已被广泛应用于平滑噪声分割图<citation id="174" type="reference"><link href="48" rel="bibliography" /><link href="50" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>。通常, 这些模型包含耦合相邻节点的能量项, 有利于对空间邻近像素的类别分配<citation id="172" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。本质上, 这些短距离CRF的主要功能是清理基于局部手工设计的弱分类器的虚假预测。严格来说, 这种模式适合于有效的近似概率推理<citation id="173" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。本文通过融合DCNNs的识别能力和全连接的条件随机场 (CRF) 的细粒度定位精度来寻求新的替代方向, 通过试验表明它在解决逐像素定位与分类方面取得了良好的结果, 产生了准确的语义分割结果以及恢复对象边界的细节能力。</p>
                </div>
                <div class="p1">
                    <p id="114">图3所示为模型的分割流程。通过可变形卷积采样POI区域, 然后得到特征图谱, 使用特征图谱来生成粗糙的缩小分割图, 再通过反卷积插值得到同尺寸的预测分割图, 最后将此粗糙的分割图使用CRF进行精细化分割, 得到精细化的分割图。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906007_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 语义分割流程" src="Detail/GetImg?filename=images/CHXB201906007_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 语义分割流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906007_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 The flowchart of the proposed approach</p>

                </div>
                <h3 id="116" name="116" class="anchor-tag">2 试验结果与分析</h3>
                <h4 class="anchor-tag" id="117" name="117">2.1 数据集</h4>
                <div class="p1">
                    <p id="118">本文使用ISPRS Vaihingen 2D语义标签遥感数据集<citation id="175" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>来评估本文提出的方法。这是一个开放的基准数据集。数据集由33幅大小不同的图像组成, 每幅图像为300万至1000万像素, 图像是在德国Vaihingen地区拍摄的高分辨率正射影像, 影像的平均尺寸约2493×2063像素, 分辨率为9 cm。除了正射影像之外, 数据集中还包含具有相同空间分辨率的数字表面模型 (DSM) 图像。此外, 文献<citation id="176" type="reference">[<a class="sup">29</a>]</citation>提供了归一化的DSM, 以限制不同地面高度的影响。33幅图像中的16幅图像已被正确标注, 其中所有像素都被标记, 总共分为6个类别, 即道路、建筑物、植被、树木、车辆及杂类地物 (例如集装箱、网球场、游泳池等) 。如以前在其他方法中所做的那样, 试验中不包括杂类, 因为杂类的像素面积仅占总图像像素的0.88%。</p>
                </div>
                <div class="p1">
                    <p id="119">ISPRS只提供16幅标注的图像进行训练, 而其余17幅图像未标注用于评估提交的方法。为了评估本文方法, 有标签分类的数据集被分为训练集和验证集。按照文献<citation id="177" type="reference">[<a class="sup">30</a>]</citation>的例子, 训练集包含11幅图像 (图幅区域为1、3、5、7、13、17、21、23、26、32、37) ;验证集包含5幅图像 (图幅区域为11、15、28、30、34) 。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">2.2 数据预处理</h4>
                <div class="p1">
                    <p id="121">高分辨率遥感图像通常尺寸较大, 无法整幅图像通过卷积进行处理。例如, 来自Vaihingen数据集的ISPRS瓦片的平均尺寸为2493×2063像素, 而大多数卷积操作的分辨率为256×256。鉴于目前的GPU内存限制, 本文使用滑动窗口将原始的遥感影像分割成较小的图像块。如果卷积步幅小于图像块尺寸, 在连续图像块重叠的情况下, 对多个预测进行平均, 以获得重叠像素的最终分类。这可以平滑每个图像块边界的预测, 并消除可能出现的不连续性。</p>
                </div>
                <div class="p1">
                    <p id="122">本文的目标是将当前计算机视觉领域中的典型的人工神经网络结构应用到地球观测数据中, 因此, 使用最初为RGB数据设计的人工神经网络, 处理后的图像必须遵守这种3通道格式。ISPRS数据集包含Vaihingen的IRRG图像, 因此, 3个通道 (近红外、红色和绿色) 将被处理为RGB图像。该数据集包含从空载激光传感器获取的数字表面模型 (DSM) 的数据。本文还将使用文献<citation id="178" type="reference">[<a class="sup">30</a>]</citation>中的归一化数字表面模型 (NDSM) , 然后从近红外和红外通道计算归一化差异植被指数 (NDVI) , 最终使用DSM, NDSM和NDVI信息为每个IRRG图像构建一个相对应的合成图像。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">2.3 分割试验</h4>
                <div class="p1">
                    <p id="124">本试验中深度学习网络的训练设备为4核心8线程Intel I7-7700K CPU;32 GB内存;NVIDIA GTX 1080显卡, 8 G显存。软件环境为Ubuntu16.04.01操作系统;开发平台是Anaconda 4.3.1;内置的Python版本为3.6.1;深度学习软件框架是TensorFlow1.2。本次试验训练迭代次数为5<i>e</i><sup>5</sup>, 总训练时间为32 h 30 min。</p>
                </div>
                <div class="p1">
                    <p id="125">本文使用随机梯度下降方法训练网络。学习率采用固定大小, 本文使用的学习率为5<i>e</i><sup>-3</sup>, 学习率过大容易无法收敛, 太小训练时间过长。批处理数量 (batch) 受到显存大小的限制, 所以本文设置batch=10。本文网络特征提取层的权重使用了VGGNet<citation id="179" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>上进行预训练的权重, 迁移预训练的权重可以增强网络在特征提取的性能。</p>
                </div>
                <div class="p1">
                    <p id="126">本文使用ISPRS<citation id="180" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>定义的评估方法来评估结果, 用<i>F</i><sub>1</sub>分数来评估试验的结果</p>
                </div>
                <div class="p1">
                    <p id="127"><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mrow><mn>1</mn><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mn>2</mn><mo>×</mo><mfrac><mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub><mrow><mo>×</mo><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mrow><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="129">式中, <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mrow><mtext>t</mtext><mtext>p</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>;<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mrow><mtext>t</mtext><mtext>p</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>。tp<sub><i>i</i></sub>为类别<i>i</i>正确识别的像素个数;<i>C</i><sub><i>i</i></sub>为属于类别<i>i</i>类的像素数个数;<i>P</i><sub><i>i</i></sub>为模型识别出类别<i>i</i>的像素个数。此外, 根据ISPRS的评估, 测试标签图像中的物体的边界被半径为3像素的圆形区域所侵蚀。这些被侵蚀的区域在评估过程中被忽视, 以减少不确定边界定义的影响。因此, 测试集上的性能略好于验证集。</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">2.4 分析与评价</h4>
                <h4 class="anchor-tag" id="133" name="133">2.4.1 分割结果的比较</h4>
                <div class="p1">
                    <p id="134">本文模型在ISPRSVaihingen数据集上获得了良好的分割结果, 分割结果参见表1。表1比较了采用CNNs、FCNs及SegNet作为自编码器的网络。图4展示了采用本文方法与采用其他当前主流人工神经网络结构的分割结果, 分割图中白色代表道路, 蓝色代表建筑物, 青色代表植被, 绿色代表树木, 黄色代表车辆。其中图4 (c) 为文献<citation id="181" type="reference">[<a class="sup">31</a>]</citation>中提出的传统CNNs方法的影像分割结果;图4 (d) 为文献<citation id="182" type="reference">[<a class="sup">32</a>]</citation>中提出的FCNs方法的影像分割结果;图4 (e) 为本文提出的方法的影像分割结果。</p>
                </div>
                <div class="area_img" id="135">
                    <p class="img_tit"><b>表1 ISPRS Vaihingen数据集语义分割结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.1 ISPRS Vaihingen dataset semantic segmentation results</b></p>
                    <p class="img_note"> (%) </p>
                    <table id="135" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="5"><br />分类精度</td><td rowspan="2">整体精度</td></tr><tr><td><br />道路</td><td>建筑物</td><td>植被</td><td>树木</td><td>车辆</td></tr><tr><td><br />CNNs文献[31]</td><td>87.8</td><td>91.9</td><td>77.8</td><td>86.2</td><td>50.7</td><td>85.9</td></tr><tr><td><br />FCNs文献[32]</td><td>89.2</td><td>92.5</td><td>81.6</td><td>86.9</td><td>57.3</td><td>87.3</td></tr><tr><td><br />SegNet</td><td>90.5</td><td>93.7</td><td>82.7</td><td>89.2</td><td>70.6</td><td>89.1</td></tr><tr><td><br />SegNet+CRF</td><td>90.3</td><td>92.3</td><td>82.5</td><td>89.5</td><td>82.5</td><td>88.5</td></tr><tr><td><br />SegNet+DC</td><td>92.2</td><td>94.3</td><td>83.4</td><td>89.9</td><td>76.3</td><td>89.4</td></tr><tr><td><br />SegNet+DC+CRF (Iter=1) </td><td>91.1</td><td>93.5</td><td>84.4</td><td>89.1</td><td>77.8</td><td>89.8</td></tr><tr><td><br />SegNet+DC+CRF (Iter=10) </td><td>91.5</td><td>94.7</td><td>85.1</td><td>89.3</td><td>85.7</td><td>90.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="136">从试验结果中可以看出, CNNs算法难以很好地分割不同地物类别的边界, 且整体分割精度较低, 仅为85.9%。由图4 (c) 可以看出植被、建筑物与道路之间的分割边界很模糊并且不规整;其次是分割的地物中出现了很多类别错误, 例如在植被区域出现了图斑状的树木类别, 导致植被的分割精度只有77.8%。究其原因, 文献<citation id="183" type="reference">[<a class="sup">31</a>]</citation>中已经提及这是由于CNNs网络对于空间变换没有很强的自适应能力。FCNs方法在边界分割能力上有一定的提升。从图4 (d) 可以看出, 相较于CNNs方法, FCNs方法的分割边界更加清晰并且更加规整, 但是依然存在较多的分割错误, 例如房屋的边界不连续并且在车辆类别上的分割精度只有57.3%。从文献<citation id="184" type="reference">[<a class="sup">32</a>]</citation>中得知, FCNs某种程度上解决了CNNs的一些缺点。该方法的内在本质是利用现有的CNNs作为强大的视觉模型, 使得网络能够学习特征的层次结构。尽管FCNs模型具有强大的性能和灵活性, 但它仍然具有局限性, 其内在的空间不变性并没有考虑有用的全局上下文信息, 导致FCNs对细节不敏感。究其原因主要有两点:一是固定尺寸的感受野, 对于大尺度目标而言, 只能获得该目标的局部信息, 导致目标的某些部分将被错误分类, 对于小尺度目标而言, 很容易被忽略或当成背景处理;二是目标的细节结构容易被丢失, 导致边缘信息不充分, 这是由于FCNs得到的特征图谱过于粗糙, 这样用于上采样操作的信息过于简单。</p>
                </div>
                <div class="p1">
                    <p id="137">本文提出的可变形卷积在空间变换上具有很强的自适应性。因为在不同区域对应的目标尺寸是不相同的, 如果感受野在不同区域的尺寸能够进行自适应调整, 这对于语义分割必然有很大帮助。从图4 (e) 中可以看出, 分割的边界相较于CNNs与FCNs更加清晰规整, 整体分割精度达到了90.7%。可以看到本文方法相比以CNNs和FCNs作为自编码器网络的方法在各个类别的分割精度上均有提升, 尤其在车辆这个类别上提升最多达到了85.7%的分割精度, 说明本文方法对于小目标物体的分割有着很高的适应性。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906007_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 试验结果" src="Detail/GetImg?filename=images/CHXB201906007_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 试验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906007_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 The experimental results</p>

                </div>
                <h4 class="anchor-tag" id="139" name="139">2.4.2 可变形卷积的理解</h4>
                <div class="p1">
                    <p id="140">可变形卷积的理念是建立在一个容易理解的想法之上的。可变形卷积是将空间采样位置增加额外的偏移量, 这些偏移量是以目标任务为驱动的数据中学习到的。当可变形模块堆叠成多层时, 复合变形的作用是显著的。对于同一区域的5幅图像, 本文选取了不同尺度大小的地物 (依次为车辆、建筑物、树木、植被和道路) 作为激活单元, 分别展示不同地物的采样位置, 以此来可视化可变形卷积滤波器的工作原理。如图5所示, 其中绿色点代表激活单元的位置, 蓝色点代表对该地物的采样位置, 每幅图像中展示了9<sup>3</sup>=729个采样点的位置。</p>
                </div>
                <h4 class="anchor-tag" id="141" name="141">2.4.3 可变形卷积与CRF的性能比较</h4>
                <div class="p1">
                    <p id="142">为了定量量化本文提出的两个模块对于分割精度的作用, 将CRF模块与可变形卷积模块单独嵌入SegNet网络中进行试验, 以此来定量对比模块的效果。CRF可以细化模型的分割输出并提高其捕捉细粒度细节的能力。CRF能够将低级图像信息 (如像素间的交互) 与产生逐个像素的类别得分的多类推理系统的输出进行组合。这种组合对于捕获卷积层未能考虑的远程依赖关系特别重要, 并且可以保留良好的局部细节。从图6 (b) 中可以看到采用全连接的CRF显著改善了分割结果, 使模型能够准确捕获复杂的对象边界, 尤其是车辆这类小目标物体, 可以看出相对于单独使用SegNet网络分割精度提升了11.9%。从局部分割图中也可以看出, 分割的边界更加清晰, 并且错误分类的孤立图斑更少。经过试验比较, 一般对CRF的迭代次数取为10次, 多于10次以上性能提升不明显, 应用CRF操作之后整体精度可以提升1%～2%左右。从图6 (c) 可以看出对于道路、建筑物及树木这些不规则的地物, 可变形卷积有着很好的提升效果, 尤其在道路的分割精度上提升了1.7%。这也证明了可变形卷积在空间变换上有着很好的自适应能力。</p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906007_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 可变形滤波器的采样位置" src="Detail/GetImg?filename=images/CHXB201906007_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 可变形滤波器的采样位置  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906007_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Sampling locations of deformable filters</p>

                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906007_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 可变形卷积与条件随机场的分割对比" src="Detail/GetImg?filename=images/CHXB201906007_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 可变形卷积与条件随机场的分割对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906007_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Comparing deformable convolution with conditional random field segmentation</p>

                </div>
                <h3 id="145" name="145" class="anchor-tag">3 结 论</h3>
                <div class="p1">
                    <p id="146">本文研究了深度卷积神经网络在遥感影像语义分割中的应用。当前使用卷积网络来分割影像已经取得了很大的进展, 但是由于传统的卷积方式无法有效地模拟几何变换, 导致分割能力受到限制。相比之下, 本文采用的可变形卷积方法对空间变换有着很强的自适应能力。准确捕获复杂地物的边界一直都是语义分割的难点, 为此本文在神经网络的输出层加入了结构化的后处理步骤——条件随机场, 并通过试验验证其有效性。本文也展示了如何为这种级联方法构建训练实例, 并在ISPRS 2D Vaihingen数据集上验证了本文方法。由表1可以看出, 本文方法获得了良好的分割结果。今后, 残差校正是否能改善不同拓扑网络的分割性能是下一步研究目标。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201405006&amp;v=MTM0Mjl1ZHZGeTNsVXJ6SUppWFRiTEc0SDlYTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 刘婧, 李培军.结合结构和光谱特征的高分辨率影像分割方法[J].测绘学报, 2014, 43 (5) :466-473.DOI:10.13485/j.cnki.112089.2014.0087.LIU Jing, LI Peijun.A high resolution image segmentation method by combined structural and spectral characteristics[J].Acta Geodaetica et Cartographica Sinica, 2014, 43 (5) :466-473.DOI:10.13485/j.cnki.11-2089.2014.0087.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 周成虎, 骆剑承.高分辨率卫星遥感影像地学计算[M].北京:科学出版社, 2009.ZHOU Chenghu, LUO Jiancheng.Geo-computing of high resolution satellite remote sensing image[M].Beijing:Science Press, 2009.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[3]</b> LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston, MA, USA:IEEE, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated convolutional neural network for sematic segmentation in high-resolution images">

                                <b>[4]</b> WANG Hongzhen, WANG Ying, ZHANG Qian, et al.Gated convolutional neural network for semantic segmentation in high-resolution images[J].Remote Sensing, 2017, 9 (5) :446.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A review on deep learning techniques applied to semantic segmentation">

                                <b>[5]</b> GARCIA-GARCIA A, ORTS-ESCOLANO S, OPREA S, et al.A review on deep learning techniques applied to semantic segmentation[J].arXiv:1704.06857, 2017.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=THE VISUAL CORTEX OF THE BRAIN">

                                <b>[6]</b> HUBEL D H.The visual cortex of the brain[J].Scientific American, 1963, 209:54-62.DOI:10.1038/scientificamerican1163-54.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient inference in fully connected crfs with gaussian edge potentials">

                                <b>[7]</b> KRÄHENBÜHL P, KOLTUN V.Efficient inference in fully connected crfs with gaussian edge potentials[J].arXiv:1210.5644, 2012:109-117.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15110100309604&amp;v=MTEzMTRNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVNzdJSlY0VmFSST1OajdCYXJLOUg5RE1ybzlGWitzR0NudzlvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> SHOTTON J, WINN J, ROTHER C, et al.Textonboost for image understanding:multi- class object recognition and segmentation by jointly modeling texture, layout, and context[J].International Journal of Computer Vision, 2009, 81 (1) :2-23.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deformable convolutional networks">

                                <b>[9]</b> DAI Jifeng, QI Haozhi, XIONG Yuwen, et al.Deformable convolutional networks[J].arXiv:1703.06211, 2017:764-773.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the effective receptive field in deep convolutional neural networks">

                                <b>[10]</b> LUO Wenjie, LI Yujia, URTASUN R, et al.Understanding the effective receptive field in deep convolutional neural networks[J].arXiv:1701.04128, 2017.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[11]</b> SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A real-time algorithm forsignal analysis with the help of the wavelet transform">

                                <b>[12]</b> HOLSCHNEIDER M, KRONLAND-MARTINET R, MORLET J, et al.A real-time algorithm for signal analysis with the help of the wavelet transform[M].COMBES J M, GROSSMANN A, TCHAMITCHIAN P.Wavelets:Time-Frequency Methods and Phase Space.Berlin:Springer, 1990:286-297.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:A Large-Scale Hierarchical Image Database">

                                <b>[13]</b> DENG Jia, DONG Wei, SOCHER R, et al.Imagenet:a large-scale hierarchical image database[C]//Proceedings of 2009 IEEE Conference on Computer Vision and Pattern Recognition.Miami, FL, USA:IEEE, 2009.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 DAI Jifeng, LI Yi, HE Kaiming, et al.R-FCN:object detection via region-based fully convolutional networks[C]//Proceedings of the 30th International Conference on Neural Information Processing Systems.Barcelona, Spain:ACM, 2016:379-387.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">

                                <b>[15]</b> BADRINARAYANAN V, KENDALL A, CIPOLLA R.SegNet:a deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (12) :2481-2495.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic image segmentation with deep convolutional nets and fully connected CRFS">

                                <b>[16]</b> CHEN L C, PAPANDREOU G, KOKKINOS I, et al.Semantic image segmentation with deep convolutional nets and fully connected CRFs[C]//Proceedings of the International Conference on Learning Representations.San Diego, CA:Computational and Biological Learning Society, 2015.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Return of the Devil in the Details:Delving Deep into Convolutional Nets">

                                <b>[17]</b> CHATFIELD K, SIMONYAN K, VEDALDI A, et al.Return of the devil in the details:delving deep into convolutional nets[C]//Proceedings of the British Machine Vision Conference.Dundee, Britain:BMVA Press, 2014.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[18]</b> SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[19]</b> IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.Lille, France:JMLR, 2015:448-456.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving deep into rectifiers:Surpassing human-level performance on imagenet classification">

                                <b>[20]</b> HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Delving deep into rectifiers:surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE International Conference on Computer Vision.Santiago, Chile:IEEE, 2015:1026-1034.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and accurate deep network learning by exponential linear units (ELUs)">

                                <b>[21]</b> CLEVERT D A, UNTERTHINER T, HOCHREITER S.Fast and accurate deep network learning by exponential linear units (ELUs) [C]//Proceedings of the International Conference on Learning Representations.San Diego, CA:Computational and Biological Learning Society, 2015.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture">

                                <b>[22]</b> EIGEN D, FERGUS R.Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture[C]//Proceedings of the IEEE International Conference on Computer Vision.Santiago, Chile:IEEE, 2015:2650-2658.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                 MOSTAJABI M, YADOLLAHPOUR P, SHAKHNAROVICH G.Feedforward semantic segmentation with zoom-out features[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston, MA, USA:IEEE, 2015:3376-3385.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Grabcut-interactive foreground extraction using iterated graph cuts">

                                <b>[24]</b> ROTHER C, KOLMOGOROV V, BLAKE A.Grabcut:interactive foreground extraction using iterated graph cuts[C]//Proceedings of the ACM SIGGRAPH 2004.Los Angeles, California:ACM, 2004:309-314.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust higher order potentials for enforcing labelconsistency">

                                <b>[25]</b> KOHLI P, LADICKY L, TORR P H S.Robust higher order potentials for enforcing label consistency[C]//Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition.Anchorage, AK, USA:IEEE, 2009.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient inference in fully connected CRFs w ith gaussian edge potentials">

                                <b>[26]</b> KRÄHENBÜHL P, KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of the 25th annual conference on Neural Information Processing Systems.Granada:NIF, 2011.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000018133&amp;v=MDg3MTc0NU5aZWdNWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGeW5sVnIvSkkxND1OaWZjYXJPNEh0SE1y&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> ADAMS A, BAEK J, DAVIS M A.Fast high-dimensional filtering using the permutohedral lattice[J].Computer Graphics Forum, 2010, 29 (2) :753-762.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ISPRS semantic labeling contest">

                                <b>[28]</b> GERKE M, ROTTENSTEINER F, WEGNER J D, et al.ISPRS semantic labeling contest[C]//Proceedings of PCV-Photogrammetric Computer Vision.[S.l.]:ISPRS, 2014.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen)">

                                <b>[29]</b> GERKE M.Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen) [C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .Boston, MA, USA:IEEE, 2015.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective semantic pixel labelling with convolutional networks and conditional random fields">

                                <b>[30]</b> PAISITKRIANGKRAI S, SHERRAH J, JANNEY P, et al.Effective semantic pixel labelling with convolutional networks and conditional random fields[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.Boston, MA, USA:IEEE, 2015:36-43.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation of earth observation data using multimodal and multi-scale deep networks">

                                <b>[31]</b> AUDEBERT N, LE SAUX B, LEFÈVRE S.Semantic segmentation of earth observation data using multimodal and multi-scale deep networks[M]//LAI S H, LEPETIT V, NISHINO K, et al.Computer Vision-ACCV 2016.Cham:Springer, 2017.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Semantic Segmentation Based on FCN-CRF Model">

                                <b>[32]</b> ZHOU Hao, ZHANG Jun, LEI Jun, et al.Image semantic segmentation based on FCN-CRF model[C]//Proceedings of International Conference on Image, Vision and Computing.Portsmouth, UK:IEEE, 2016:9-14.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201906007" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906007&amp;v=MjM3MjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTNsVXJ6SUppWFRiTEc0SDlqTXFZOUZZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
