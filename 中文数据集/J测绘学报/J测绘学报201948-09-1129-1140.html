<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142610600107500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201909007%26RESULT%3d1%26SIGN%3dTmXlgxSaagsUE6yD9k7ijusEe1c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201909007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201909007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201909007&amp;v=MDgzNTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBSmlYVGJMRzRIOWpNcG85Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 本文方法 ">1 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="1.1 结构自适应特征点双向匹配">1.1 结构自适应特征点双向匹配</a></li>
                                                <li><a href="#84" data-title="1.2 双重核线约束的结构自适应特征点匹配">1.2 双重核线约束的结构自适应特征点匹配</a></li>
                                                <li><a href="#90" data-title="1.3 特征点匹配扩展">1.3 特征点匹配扩展</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="2 试验及结果分析 ">2 试验及结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="2.1 试验数据">2.1 试验数据</a></li>
                                                <li><a href="#105" data-title="2.2 参数设置">2.2 参数设置</a></li>
                                                <li><a href="#108" data-title="2.3 试验结果分析">2.3 试验结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="3 结 论 ">3 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 本文方法整体流程">图1 本文方法整体流程</a></li>
                                                <li><a href="#68" data-title="图2 结构自适应特征点双向匹配流程">图2 结构自适应特征点双向匹配流程</a></li>
                                                <li><a href="#75" data-title="图4 几何结构方向显著点的确定">图4 几何结构方向显著点的确定</a></li>
                                                <li><a href="#77" data-title="图3 特征点几何结构方向的表达">图3 特征点几何结构方向的表达</a></li>
                                                <li><a href="#78" data-title="图5 特征点支撑区域">图5 特征点支撑区域</a></li>
                                                <li><a href="#81" data-title="图6 特征区域和特征描述符计算过程">图6 特征区域和特征描述符计算过程</a></li>
                                                <li><a href="#88" data-title="图7 核线约束的特征点支撑区域">图7 核线约束的特征点支撑区域</a></li>
                                                <li><a href="#94" data-title="图8 第1类未匹配特征点匹配扩展">图8 第1类未匹配特征点匹配扩展</a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表1 试验数据详细信息&lt;/b&gt;"><b>表1 试验数据详细信息</b></a></li>
                                                <li><a href="#107" data-title="图9 试验数据">图9 试验数据</a></li>
                                                <li><a href="#112" data-title="图10 对比试验统计结果">图10 对比试验统计结果</a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;表2 三视影像匹配统计结果&lt;/b&gt;"><b>表2 三视影像匹配统计结果</b></a></li>
                                                <li><a href="#119" data-title="图11 三视影像1的三度重叠匹配结果">图11 三视影像1的三度重叠匹配结果</a></li>
                                                <li><a href="#120" data-title="图12 三视影像2的三度重叠匹配结果">图12 三视影像2的三度重叠匹配结果</a></li>
                                                <li><a href="#123" data-title="图13 ASIFT方法(下采样模式)在三视影像1上的两两匹配结果">图13 ASIFT方法(下采样模式)在三视影像1上的两两匹配结果</a></li>
                                                <li><a href="#125" data-title="图14 ASIFT方法(普通模式)在三视影像1上的两两匹配结果">图14 ASIFT方法(普通模式)在三视影像1上的两两匹配结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="163">


                                    <a id="bibliography_1" title=" 戴激光,宋伟东,贾永红,等.一种新的异源高分辨率光学卫星遥感影像自动匹配算法[J].测绘学报,2013,42(1):80-86.DAI Jiguang,SONG Weidong,JIA Yonghong,et al.A new automatically matching algorithm for multi-source high resolution optical satellite images[J].Acta Geodaetica et Cartographica Sinica,2013,42(1):80-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201301014&amp;v=MDIzMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFKaVhUYkxHNEg5TE1ybzlFWUlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         戴激光,宋伟东,贾永红,等.一种新的异源高分辨率光学卫星遥感影像自动匹配算法[J].测绘学报,2013,42(1):80-86.DAI Jiguang,SONG Weidong,JIA Yonghong,et al.A new automatically matching algorithm for multi-source high resolution optical satellite images[J].Acta Geodaetica et Cartographica Sinica,2013,42(1):80-86.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_2" title=" 叶沅鑫,单杰,彭剑威,等.利用局部自相似进行多光谱遥感图像自动配准[J].测绘学报,2014,43(3):268-275.DOI:10.13485/j.cnki.11-2089.2014.0039.YE Yuanxin,SHAN Jie,PENG Jianwei,et al.Automated multispectral remote sensing image registration using local self-similarity[J].Acta Geodaetica et Cartographica Sinica,2014,43(3):268-275.DOI:10.13485/j.cnki.11-2089.2014.0039." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201403009&amp;v=MjQ3OThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFKaVhUYkxHNEg5WE1ySTlGYlk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         叶沅鑫,单杰,彭剑威,等.利用局部自相似进行多光谱遥感图像自动配准[J].测绘学报,2014,43(3):268-275.DOI:10.13485/j.cnki.11-2089.2014.0039.YE Yuanxin,SHAN Jie,PENG Jianwei,et al.Automated multispectral remote sensing image registration using local self-similarity[J].Acta Geodaetica et Cartographica Sinica,2014,43(3):268-275.DOI:10.13485/j.cnki.11-2089.2014.0039.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_3" title=" 岳春宇,江万寿.几何约束和改进SIFT的SAR影像和光学影像自动配准方法[J].测绘学报,2012,41(4):570-576.YUE Chunyu,JIANG Wanshou.An automatic registration algorithm for SAR and optical images based on geometry constraint and improved SIFT[J].Acta Geodaetica et Cartographica Sinica,2012,41(4):570-576." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201204019&amp;v=MjMwMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFKaVhUYkxHNEg5UE1xNDlFYllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         岳春宇,江万寿.几何约束和改进SIFT的SAR影像和光学影像自动配准方法[J].测绘学报,2012,41(4):570-576.YUE Chunyu,JIANG Wanshou.An automatic registration algorithm for SAR and optical images based on geometry constraint and improved SIFT[J].Acta Geodaetica et Cartographica Sinica,2012,41(4):570-576.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_4" title=" 陈敏,朱庆,朱军,等.SAR影像与光学影像的高斯伽玛型边缘强度特征匹配法[J].测绘学报,2016,45(3):318-325.DOI:10.11947/j.AGCS.2016.20150084.CHEN Min,ZHU Qing,ZHU Jun,et al.Feature matching for SAR and optical images based on Gaussian-Gamma-shaped edge strength map[J].Acta Geodaetica et Cartographica Sinica,2016,45(3):318-325.DOI:10.11947/j.AGCS.2016.20150084." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201603012&amp;v=MDQ4NDdmWnVkdkZ5cmhVcnZBSmlYVGJMRzRIOWZNckk5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         陈敏,朱庆,朱军,等.SAR影像与光学影像的高斯伽玛型边缘强度特征匹配法[J].测绘学报,2016,45(3):318-325.DOI:10.11947/j.AGCS.2016.20150084.CHEN Min,ZHU Qing,ZHU Jun,et al.Feature matching for SAR and optical images based on Gaussian-Gamma-shaped edge strength map[J].Acta Geodaetica et Cartographica Sinica,2016,45(3):318-325.DOI:10.11947/j.AGCS.2016.20150084.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_5" title=" 梁焕青,谢意,付四洲.颜色不变量与AKAZE特征相结合的无人机影像匹配算法[J].测绘学报,2017,46(7):900-909.DOI:10.11947/j.AGCS.2017.20160436.LIANG Huanqing,XIE Yi,FU Sizhou.UAV image registration algorithm using color invariant and AKAZE feature[J].Acta Geodaetica et Cartographica Sinica,2017,46(7):900-909.DOI:10.11947/j.AGCS.2017.20160436." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201707015&amp;v=MTkwODBKaVhUYkxHNEg5Yk1xSTlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         梁焕青,谢意,付四洲.颜色不变量与AKAZE特征相结合的无人机影像匹配算法[J].测绘学报,2017,46(7):900-909.DOI:10.11947/j.AGCS.2017.20160436.LIANG Huanqing,XIE Yi,FU Sizhou.UAV image registration algorithm using color invariant and AKAZE feature[J].Acta Geodaetica et Cartographica Sinica,2017,46(7):900-909.DOI:10.11947/j.AGCS.2017.20160436.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_6" title=" ZITOV&#193; B,FLUSSER J.Image registration methods:a survey[J].Image and Vision Computing,2003,21(11):977-1000." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349910&amp;v=MjAwNjBOaWZPZmJLN0h0RE9yWTlFWis4R0JYMDVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3N0lJbG9WYmhvPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         ZITOV&#193; B,FLUSSER J.Image registration methods:a survey[J].Image and Vision Computing,2003,21(11):977-1000.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_7" title=" XIONG Zhen,ZHANG Yun.A novel interest-point-matching algorithm for high-resolution satellite images[J].IEEE Transactions on Geoscience and Remote Sensing,2009,47(12):4189-4200." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel interest-point-matching algorithm for high-resolution satellite images">
                                        <b>[7]</b>
                                         XIONG Zhen,ZHANG Yun.A novel interest-point-matching algorithm for high-resolution satellite images[J].IEEE Transactions on Geoscience and Remote Sensing,2009,47(12):4189-4200.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_8" title=" GRUEN A.Development and status of image matching in photogrammetry[J].The Photogrammetric Record,2012,27(137):36-57." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD120427011557&amp;v=MDI0MjBGeWprVTczTUlWb2NOaWZjYXJLNkh0WE9xSTlFWmU0S0N4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         GRUEN A.Development and status of image matching in photogrammetry[J].The Photogrammetric Record,2012,27(137):36-57.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_9" title=" LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=Mjg5NjJxUnJ4b3hjTUg3UjdxZForWnVGeW5sVWJ2SkpGWT1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNY&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_10" title=" BAY H,ESS A,TUYTELAARS T,et al.Speeded-up robust features (SURF)[J].Computer Vision and Image Understanding,2008,110(3):346-359." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MjY5MTlJSWxvVmJobz1OaWZPZmJLN0h0RE5xbzlFWk9NTUJIUXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3Nw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         BAY H,ESS A,TUYTELAARS T,et al.Speeded-up robust features (SURF)[J].Computer Vision and Image Understanding,2008,110(3):346-359.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_11" title=" TOLA E,LEPETIT V,FUA P.Daisy:an efficient dense descriptor applied to wide-baseline stereo[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(5):815-830." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DAISY: An efficient dense descriptor applied to wide-baseline stereo">
                                        <b>[11]</b>
                                         TOLA E,LEPETIT V,FUA P.Daisy:an efficient dense descriptor applied to wide-baseline stereo[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(5):815-830.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_12" title=" MOREL J M,YU Guoshen.ASIFT:a new framework for fully affine invariant image comparison[J].SIAM Journal on Imaging Sciences,2009,2(2):438-469." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ASIFT: A New Framework for Fully Affine Invariant Image Comparison">
                                        <b>[12]</b>
                                         MOREL J M,YU Guoshen.ASIFT:a new framework for fully affine invariant image comparison[J].SIAM Journal on Imaging Sciences,2009,2(2):438-469.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_13" title=" HU Han,ZHU Qing,DU Zhiqiang,et al.Reliable spatial relationship constrained feature point matching of oblique aerial images[J].Photogrammetric Engineering &amp;amp; Remote Sensing,2015,81(1):49-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD85D5880ACA6B19DDEF5F422FFC56179&amp;v=MTc5NDhHNlhKcDRkRkZaaCtDZzQ0eG1Kbm4wbDRQbnZncm1SRENyZVNSTDJXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1OWxoeHJ5K3hLQT1OaWZPZmNldw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         HU Han,ZHU Qing,DU Zhiqiang,et al.Reliable spatial relationship constrained feature point matching of oblique aerial images[J].Photogrammetric Engineering &amp;amp; Remote Sensing,2015,81(1):49-58.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_14" title=" 肖雄武,郭丙轩,李德仁,等.一种具有仿射不变性的倾斜影像快速匹配方法[J].测绘学报,2015,44(4):414-421.DOI:10.11947/j.AGCS.2015.20140048.XIAO Xiongwu,GUO Bingxuan,LI Deren,et al.A quick and affine invariance matching method for oblique images[J].Acta Geodaetica et Cartographica Sinica,2015,44(4):414-421.DOI:10.11947/j.AGCS.2015.20140048." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201504012&amp;v=MTA2NTh0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBSmlYVGJMRzRIOVRNcTQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         肖雄武,郭丙轩,李德仁,等.一种具有仿射不变性的倾斜影像快速匹配方法[J].测绘学报,2015,44(4):414-421.DOI:10.11947/j.AGCS.2015.20140048.XIAO Xiongwu,GUO Bingxuan,LI Deren,et al.A quick and affine invariance matching method for oblique images[J].Acta Geodaetica et Cartographica Sinica,2015,44(4):414-421.DOI:10.11947/j.AGCS.2015.20140048.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_15" title=" 肖雄武,李德仁,郭丙轩,等.一种具有视点不变性的倾斜影像快速匹配方法[J].武汉大学学报(信息科学版),2016,41(9):1151-1159.XIAO Xiongwu,LI Deren,GUO Bingxuan,et al.A robust and rapid viewpoint-invariant matching method for oblique images[J].Geomatics and Information Science of Wuhan University,2016,41(9):1151-1159." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH201609003&amp;v=Mjk1ODVyRzRIOWZNcG85Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBTWlYSVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         肖雄武,李德仁,郭丙轩,等.一种具有视点不变性的倾斜影像快速匹配方法[J].武汉大学学报(信息科学版),2016,41(9):1151-1159.XIAO Xiongwu,LI Deren,GUO Bingxuan,et al.A robust and rapid viewpoint-invariant matching method for oblique images[J].Geomatics and Information Science of Wuhan University,2016,41(9):1151-1159.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_16" title=" 闫利,费亮,叶志云,等.大范围倾斜多视影像连接点自动提取的区域网平差法[J].测绘学报,2016,45(3):310-317.DOI:10.11947/j.AGCS.2016.20140673.YAN Li,FEI Liang,YE Zhiyun,et al.Automatic tie-points extraction for triangulation of large-scale oblique multi-view images[J].Acta Geodaetica et Cartographica Sinica,2016,45(3):310-317.DOI:10.11947/j.AGCS.2016.20140673." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201603011&amp;v=MTk3NjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJoVXJ2QUppWFRiTEc0SDlmTXJJOUVaWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         闫利,费亮,叶志云,等.大范围倾斜多视影像连接点自动提取的区域网平差法[J].测绘学报,2016,45(3):310-317.DOI:10.11947/j.AGCS.2016.20140673.YAN Li,FEI Liang,YE Zhiyun,et al.Automatic tie-points extraction for triangulation of large-scale oblique multi-view images[J].Acta Geodaetica et Cartographica Sinica,2016,45(3):310-317.DOI:10.11947/j.AGCS.2016.20140673.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_17" title=" 张力,艾海滨,许彪,等.基于多视影像匹配模型的倾斜航空影像自动连接点提取及区域网平差方法[J].测绘学报,2017,46(5):554-564.DOI:10.11947/j.AGCS.2017.20160571.ZHANG Li,AI Haibin,XU Biao,et al.Automatic tie-point extraction based on multiple-image matching and bundle adjustment of large block of oblique aerial images[J].Acta Geodaetica et Cartographica Sinica,2017,46(5):554-564.DOI:10.11947/j.AGCS.2017.20160571." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201705005&amp;v=MDY5NjNUYkxHNEg5Yk1xbzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFKaVg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         张力,艾海滨,许彪,等.基于多视影像匹配模型的倾斜航空影像自动连接点提取及区域网平差方法[J].测绘学报,2017,46(5):554-564.DOI:10.11947/j.AGCS.2017.20160571.ZHANG Li,AI Haibin,XU Biao,et al.Automatic tie-point extraction based on multiple-image matching and bundle adjustment of large block of oblique aerial images[J].Acta Geodaetica et Cartographica Sinica,2017,46(5):554-564.DOI:10.11947/j.AGCS.2017.20160571.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_18" title=" SUN Yanbiao,ZHAO Liang,HUANG Shoudong,et al.L2-SIFT:SIFT feature extraction and matching for large images in large-scale aerial photogrammetry[J].ISPRS Journal of Photogrammetry and Remote Sensing,2014(91):1-16." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300100892&amp;v=MjM3OTZiaG89TmlmT2ZiSzhIdExPckk5Rlplc1BCSFU3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVNzdJSWxvVg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         SUN Yanbiao,ZHAO Liang,HUANG Shoudong,et al.L2-SIFT:SIFT feature extraction and matching for large images in large-scale aerial photogrammetry[J].ISPRS Journal of Photogrammetry and Remote Sensing,2014(91):1-16.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_19" title=" JIANG San,JIANG Wanshou.On-board GNSS/IMU assisted feature extraction and matching for oblique UAV images[J].Remote Sensing,2017,9(8):813." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On-Board GNSS/IMU Assisted Feature Extraction and Matching for Oblique UAV Images">
                                        <b>[19]</b>
                                         JIANG San,JIANG Wanshou.On-board GNSS/IMU assisted feature extraction and matching for oblique UAV images[J].Remote Sensing,2017,9(8):813.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_20" title=" YU Yinan,HUANG Kaiqi,CHEN Wei,et al.A novel algorithm for view and illumination invariant image matching[J].IEEE Transactions on Image Processing,2012,21(1):229-240." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Novel Algorithm for View and Illumination Invariant Image Matching">
                                        <b>[20]</b>
                                         YU Yinan,HUANG Kaiqi,CHEN Wei,et al.A novel algorithm for view and illumination invariant image matching[J].IEEE Transactions on Image Processing,2012,21(1):229-240.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_21" title=" 仇春平,于瑞鹏,丁翠,等.面向倾斜立体影像的尺度不变特征匹配[J].遥感信息,2016,31(1):43-47.QIU Chunping,YU Ruipeng,DING Cui,et al.Oblique stereo image matching based on scale invariant feature[J].Remote Sensing Information,2016,31(1):43-47." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201601007&amp;v=MDc5NTh0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBUENyVGRyRzRIOWZNcm85Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         仇春平,于瑞鹏,丁翠,等.面向倾斜立体影像的尺度不变特征匹配[J].遥感信息,2016,31(1):43-47.QIU Chunping,YU Ruipeng,DING Cui,et al.Oblique stereo image matching based on scale invariant feature[J].Remote Sensing Information,2016,31(1):43-47.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_22" title=" FISCHLER M A,BOLLES R C.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM,1981,24(6):381-395." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025011&amp;v=MjUxOTVpclJkR2VycVFUTW53WmVadEZpbmxVNzdJSWxvVmJobz1OaWZJWTdLN0h0ak5yNDlGWk9rS0RIMDRvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         FISCHLER M A,BOLLES R C.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM,1981,24(6):381-395.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_23" title=" LI Kai,YAO Jian.Line segment matching and reconstruction via exploiting coplanar cues[J].ISPRS Journal of Photogrammetry and Remote Sensing,2017(125):33-49." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES2ABD146E53351F412D135C532F181A61&amp;v=MjM0MDV4QkRlTHFWTkx5ZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNTlsaHhyeSt4S0E9TmlmT2ZiSEpiS1hOcTRrd1llZ01DWDFQeXhjUm5qNStUUXpucg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         LI Kai,YAO Jian.Line segment matching and reconstruction via exploiting coplanar cues[J].ISPRS Journal of Photogrammetry and Remote Sensing,2017(125):33-49.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_24" title=" CHEN Min,QIN Rongjun,HE Haiqing,et al.A local distinctive features matching method for remote sensing images with repetitive patterns[J].Photogrammetric Engineering &amp;amp; Remote Sensing,2018,84(8):513-524." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A local distinctive features matching method for remote sensing images with repetitive patterns">
                                        <b>[24]</b>
                                         CHEN Min,QIN Rongjun,HE Haiqing,et al.A local distinctive features matching method for remote sensing images with repetitive patterns[J].Photogrammetric Engineering &amp;amp; Remote Sensing,2018,84(8):513-524.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_25" title=" MIKOLAJCZYK K,SCHMID C.Scale &amp;amp; affine invariant interest point detectors[J].International Journal of Computer Vision,2004,60(1):63-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830896&amp;v=MTgxMzMzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RnlubFVidkpKRlk9Tmo3QmFyTzRIdEhPcDR4RmJPSUpZ&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         MIKOLAJCZYK K,SCHMID C.Scale &amp;amp; affine invariant interest point detectors[J].International Journal of Computer Vision,2004,60(1):63-86.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_26" title=" MATAS J,CHUM O,URBAN M,et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image and Vision Computing,2004,22(10):761-767." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MjUzNTBubFU3N0lJbG9WYmhvPU5pZk9mYks3SHRET3JZOUVaKzhHQzNRN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         MATAS J,CHUM O,URBAN M,et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image and Vision Computing,2004,22(10):761-767.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_27" title=" HARRIS C,STEPHENS M.A combined corner and edge detector[C]//Proceedings of the 4th Alvey Vision Conference.Manchester,Britain:[s.n.],1988:147-152." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Combined Corner and Edge Detector">
                                        <b>[27]</b>
                                         HARRIS C,STEPHENS M.A combined corner and edge detector[C]//Proceedings of the 4th Alvey Vision Conference.Manchester,Britain:[s.n.],1988:147-152.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_28" title=" VON GIOI R G,JAKUBOWICZ J,MOREL J M,et al.LSD:a fast line segment detector with a false detection control[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(4):722-732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD: A Fast Line Segment Detector with a False Detection Control">
                                        <b>[28]</b>
                                         VON GIOI R G,JAKUBOWICZ J,MOREL J M,et al.LSD:a fast line segment detector with a false detection control[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(4):722-732.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_29" title=" WU Changchang.SiftGPU:a GPU implementation of scale invariant feature transform[EB/OL].(2011).http://ccwu.me/code.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SiftGPU:a GPU implementation of scale invariant feature transform">
                                        <b>[29]</b>
                                         WU Changchang.SiftGPU:a GPU implementation of scale invariant feature transform[EB/OL].(2011).http://ccwu.me/code.html.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_30" title=" YU Guoshen,MOREL J M.ASIFT:an algorithm for fully affine invariant comparison[J].Image Processing On Line,2011(1):11-38." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ASIFT:An Algorithm for Fully Affine Invariant Comparison">
                                        <b>[30]</b>
                                         YU Guoshen,MOREL J M.ASIFT:an algorithm for fully affine invariant comparison[J].Image Processing On Line,2011(1):11-38.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(09),1129-1140 DOI:10.11947/j.AGCS.2019.20180266            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向城区宽基线立体像对视角变化的结构自适应特征点匹配</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%95%8F&amp;code=09177301&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E5%BA%86&amp;code=10994934&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱庆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E6%B5%B7%E6%B8%85&amp;code=31055104&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何海清</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E5%B0%91%E5%8D%8E&amp;code=37801894&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严少华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%80%A1%E6%B6%9B&amp;code=39251608&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵怡涛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%83%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0218487&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南交通大学地球科学与环境工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E6%B5%8B%E7%BB%98%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0042235&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东华理工大学测绘工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种基于结构自适应特征的城区宽基线影像特征点匹配方法。首先,对影像提取点特征和直线特征,挖掘点特征与其邻域内直线特征之间的几何关系,构建结构自适应的特征区域和特征描述符,并通过双向匹配策略获得初始匹配结果。然后,基于初匹配结果估计影像基础矩阵,构建核线约束的结构自适应特征匹配算法进行二次匹配。最后,将已匹配特征作为控制基础设计匹配扩展算法,进一步增加匹配点数量。本文方法以特征点邻域几何结构为出发点,构建自适应的特征区域,能够在显著的影像视角变化下,为同名特征点提取影像内容一致的特征区域,进而获得相似的特征描述符。试验结果证明,与传统算法相比,本文方法在城区宽基线影像上能够同时获得更多的正确匹配特征和更高的匹配正确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9F%8E%E5%8C%BA%E5%AE%BD%E5%9F%BA%E7%BA%BF%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">城区宽基线影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%82%B9%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征点匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%93%E6%9E%84%E8%87%AA%E9%80%82%E5%BA%94%E7%89%B9%E5%BE%81%E5%8C%BA%E5%9F%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">结构自适应特征区域;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%92%E5%8F%98%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视角变化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%B9%E9%85%8D%E6%89%A9%E5%B1%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">匹配扩展;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈敏(1986—),男,博士,副教授,研究方向为多源遥感影像处理与分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-08</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(41631174;41971411;41501492);</span>
                                <span>国家重点研发计划课题(2016YFB0502603;2016YFB0501403);</span>
                                <span>中央高校基本科研业务费专项资金(2682017CX084;2682016CY02);</span>
                    </p>
            </div>
                    <h1>Structureadaptive feature point matching for urban area wide-baseline images with viewpoint variation</h1>
                    <h2>
                    <span>CHEN Min</span>
                    <span>ZHU Qing</span>
                    <span>HE Haiqing</span>
                    <span>YAN Shaohua</span>
                    <span>ZHAO Yitao</span>
            </h2>
                    <h2>
                    <span>Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University</span>
                    <span>School of Geomatics, East China University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A feature point matching method based on structureadaptive feature is proposed for urban area wide-baseline images. Firstly, interest points and straight lines are detected from images. Structure adaptive feature region and descriptor are constructed by exploring the geometric relationship between the interest point and the straight lines located in the local neighborhood of the point, and initial matching results are obtained by using bi-directional matching strategy. Secondly, the fundamental matrix is estimated from the initial matching results. An epipolar geometry constrained structure adaptive feature matching method is proposed to match those features have not been matched in the initial matching step. Finally, a matching expansion method is proposed based on the previous matching results to improve the matching performance.The proposed matching method can generate similar feature regions and descriptors for corresponding features under significant image viewpoint variation benefiting from the proposed structure adaptive feature region construction method. The experimental results demonstrate that the proposed method provides significant improvements in correct matches number and matching precision compared with other traditional matching methods for urban area wide-baseline images(e.g. unmanned aerial vehicle images and oblique images) with viewpoint change and occlusion.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=urban%20area%20wide-baseline%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">urban area wide-baseline image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20point%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature point matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=structure%20adaptive%20feature%20region&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">structure adaptive feature region;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=viewpoint%20variation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">viewpoint variation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=matching%20expansion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">matching expansion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Min(1986—),male,PhD,associate professor,majors in multi-source remote sensing images processing and analysis.E-mail: minchen@home.swjtu.edu.cn;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-06-08</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National Natural Science Foundation of China(Nos.41631174;41971411;41501492);</span>
                                <span>The National Key Research and Development Program of China(Nos.2016YFB0502603; 2016YFB0501403);</span>
                                <span>The Fundamental Research Funds for the Central Universities(Nos.2682017CX084; 2682016CY02);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="60">影像匹配是遥感影像处理与应用的关键步骤之一。经过数十年的发展,研究人员提出了许多适用于不同类型影像的匹配方法<citation id="225" type="reference"><link href="163" rel="bibliography" /><link href="165" rel="bibliography" /><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。现有影像匹配方法大体上可分为两类:基于灰度的匹配方法和基于特征的匹配方法<citation id="226" type="reference"><link href="173" rel="bibliography" /><link href="175" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。基于灰度的匹配方法能够获得亚像素级精度,但对影像灰度变化和几何变形比较敏感<citation id="223" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。相对而言,基于特征的匹配方法能够较好地克服基于灰度的匹配方法对影像灰度变化和几何变形稳健性不足的问题。随着SIFT(scale invariant feature transform)算法<citation id="224" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的成功,基于特征的匹配方法受到了越来越多的关注<citation id="227" type="reference"><link href="181" rel="bibliography" /><link href="183" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="61">针对影像视角变化,研究人员通过模拟影像仿射或投影空间,并在模拟空间进行特征匹配,获得了较好的匹配结果<citation id="228" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。但这类方法时间效率较低,在实际应用中受到限制。在摄影测量领域,高精度POS(position and orientation system)数据通常被用来辅助大视角变化影像(如倾斜影像)的匹配,即利用POS信息对影像进行粗纠正,整体上降低影像几何变形的影响,再采用传统方法进行特征点匹配<citation id="229" type="reference"><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。这类方法能够在一定程度上改善影像匹配的效果,但全局变换难以准确描述影像之间的局部几何变形。针对这个问题,将整幅影像分成多个子区域,分别对子区域进行特征点检测和匹配,可以克服全局影像几何纠正的不足,增加匹配点的数量<citation id="230" type="reference"><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>。对于无高精度POS数据的情况,可通过初匹配获取一定数量的匹配点来估计立体像对之间的几何变换模型,进而对影像进行粗纠正<citation id="231" type="reference"><link href="199" rel="bibliography" /><link href="201" rel="bibliography" /><link href="203" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>。基于影像粗纠正的方法虽然能够改善影像匹配效果,但仍然存在以下问题:①影像纠正只能在一定程度上缓解平面场景的几何变形,城区影像由于存在显著的遮挡问题,对于位于视差不连续处的特征点(如建筑物角点或边缘附近的特征点),无论经过影像全局几何纠正还是分区域处理,都难以在同名点之间获得影像内容一致的特征区域,进而产生错误匹配;②通过影像初匹配进行几何纠正的方法依赖于初始匹配结果,对于存在大视角变化的城区宽基线影像,现有方法难以获得可靠的初匹配结果,导致最终匹配结果不可靠。</p>
                </div>
                <div class="p1">
                    <p id="62">为此,本文面向无高精度POS信息的城区宽基线影像,针对传统方法为同名点计算的特征区域影像内容不一致,导致同名点特征描述符相似度低、匹配失败的问题,充分挖掘特征点邻域几何结构信息,提出结构自适应的特征区域计算方法,在不同视角情况下获得影像内容一致性的同名特征区域和相似特征描述符,并设计可靠的特征点匹配算法,提高正确匹配点对的数量和匹配正确率。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">1 本文方法</h3>
                <div class="p1">
                    <p id="64">城区影像场景多为人工建筑物,能够提取大量的点特征和直线特征。本文方法通过挖掘点特征与直线特征之间的几何关系来实现城区宽基线影像特征点匹配:首先,对立体像对提取点特征和直线特征,利用特征点邻域内的直线特征表达特征点的几何结构方向信息,为特征点计算结构自适应的不变特征区域和特征描述符,通过双向匹配策略获取可靠的初匹配点,并估计立体像对核线几何关系;其次,针对部分特征点难以仅利用几何结构方向信息构建视角不变特征区域的问题,联合特征点几何结构方向信息和核线几何约束,构建视角不变特征区域进行特征匹配;最后,设计特征匹配扩展算法增加匹配点数量,提高特征匹配率,并通过RANSAC(random sample consensus)算法<citation id="232" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>剔除错误匹配。本文方法的整体流程如图1所示,其中斜体标记步骤为本文方法的关键步骤。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法整体流程" src="Detail/GetImg?filename=images/CHXB201909007_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法整体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flow chart of the proposed method</p>

                </div>
                <h4 class="anchor-tag" id="66" name="66">1.1 结构自适应特征点双向匹配</h4>
                <div class="p1">
                    <p id="67">结构自适应的特征点双向匹配是本文方法的初匹配步骤,目的是获取一定数量的匹配点并估计立体像对核线几何关系,其算法流程如图2所示。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 结构自适应特征点双向匹配流程" src="Detail/GetImg?filename=images/CHXB201909007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 结构自适应特征点双向匹配流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Flow chart of the structure adaptive bidirectional feature point matching method</p>

                </div>
                <div class="p1">
                    <p id="69">具体匹配方法如下:</p>
                </div>
                <div class="p1">
                    <p id="70">(1) 利用直线特征表达特征点的几何结构方向信息。如图3所示,以特征点<i><b>p</b></i><sub><i>i</i></sub>为中心,确定大小为<i>m</i>×<i>m</i>的局部邻域<i><b>R</b></i><sub><i>i</i></sub>,提取与<i><b>R</b></i><sub><i>i</i></sub>相交的非平行直线特征。以特征点为原点,以与直线特征平行的方向向量表示该特征点的几何结构方向。在每个方向上,以邻域内与该方向一致且最长的直线特征的长度,作为该几何结构方向的向量长度。</p>
                </div>
                <div class="p1">
                    <p id="71">(2) 挖掘几何结构方向信息构建特征点支撑区域。根据几何结构方向数量差异,将特征点分以下3种情况进行处理:</p>
                </div>
                <div class="p1">
                    <p id="72">1) 如果特征点具有3个及以上几何结构方向,对其中任意两个满足夹角<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>∈</mo><mrow><mo>[</mo><mrow><mi>θ</mi><mo>,</mo><mtext>π</mtext><mo>-</mo><mi>θ</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>的几何结构方向,分别在对应的几何结构方向上寻找显著点。其中,在每个方向上寻找显著点的方法如图4所示<citation id="233" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。图4中,<i><b>p</b></i><sub><i>i</i></sub>为特征点,<i><b>O</b></i><sub><i>i</i></sub>表示<i><b>p</b></i><sub><i>i</i></sub>的一个几何结构方向,为<i><b>O</b></i><sub><i>i</i></sub>确定一个长度为|<i><b>O</b></i><sub><i>i</i></sub>|+<i>S</i>,宽度为2<i>S</i>的向量支撑区域(其中,|<i><b>O</b></i><sub><i>i</i></sub>|表示向量<i><b>O</b></i><sub><i>i</i></sub>的模,参数<i>S</i>用于控制向量支撑区域的尺寸)。如果向量支撑区域内存在直线特征且与该几何结构方向的交点也位于向量支撑区域内,则认为该交点是该几何结构方向上的一个显著点。由两个方向上的显著点与特征点构成平行四边形区域,即为特征点支撑区域(图5(a))。如果在一个几何结构方向上存在多个显著点,则每个显著点分别用于构建特征点支撑区域,得到多个支撑区域。</p>
                </div>
                <div class="p1">
                    <p id="73">2) 如果特征点具有两个几何结构方向,首先沿各个方向寻找显著点,然后以特征点为中心,在几何结构方向的反方向确定对称点作为虚拟显著点,最后分别由显著点、虚拟显著点和特征点确定支撑区域(图5(b))。</p>
                </div>
                <div class="p1">
                    <p id="74">3) 如果特征点少于两个几何结构方向,则无法利用上述方法构建视角不变支撑区域。后续1.3节的特征点匹配扩展步骤将对这类特征点进行处理。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 几何结构方向显著点的确定" src="Detail/GetImg?filename=images/CHXB201909007_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 几何结构方向显著点的确定  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Stable point determination on astructure orientation</p>

                </div>
                <div class="p1">
                    <p id="76">图5所示为构建特征点支撑区域示意图。图5(a)所示为特征点具有3个几何结构方向的情况,由于<i><b>O</b></i><sub>1</sub>的向量支撑区域内没有直线特征,即该方向上没有显著点,因此无法与其他几何结构方向一起构建特征点支撑区域。<i><b>O</b></i><sub>2</sub>和<i><b>O</b></i><sub>3</sub>的向量支撑区域内分别存在直线特征且交点也位于向量支撑区域内,因此可以在<i><b>O</b></i><sub>2</sub>和<i><b>O</b></i><sub>3</sub>所夹范围内确定一个特征点支撑区域(蓝色虚线标记区域)。图5(b)所示为特征点具有两个几何结构方向的情况,获得显著点<i><b>p</b></i><sub>1</sub>和<i><b>p</b></i><sub>2</sub>以后,分别在几何结构方向的反方向确定对称点<i><b>q</b></i><sub>1</sub>和<i><b>q</b></i><sub>2</sub>作为虚拟显著点,形成两个特征点支撑区域(橙色和蓝色虚线标记区域)。分别由显著点和虚拟显著点确定特征点支撑区域可以保证当特征点位于视差不连续区域时至少有一个支撑区域具备视角不变性。在图5(b)所示情况中,显著点确定的支撑区域(橙色虚线标记区域)的影像内容不具备视角不变性,而虚拟显著点确定的支撑区域(蓝色虚线标记区域)的影像内容具备视角不变性。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 特征点几何结构方向的表达" src="Detail/GetImg?filename=images/CHXB201909007_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 特征点几何结构方向的表达  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Interest point structure orientation</p>

                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 特征点支撑区域" src="Detail/GetImg?filename=images/CHXB201909007_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 特征点支撑区域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Interest point support region</p>

                </div>
                <div class="p1">
                    <p id="79">(3) 基于特征点支撑区域计算特征区域和特征描述符。如果一个特征点存在多个支撑区域,将该特征点视作多个不同的特征点分别分配一个支撑区域。将平行四边形支撑区域归一化得到正方形特征区域。在支撑区域归一化时,为所有特征点设置相同的特征区域尺寸<i>T</i><sub><i>r</i></sub>×<i>T</i><sub><i>r</i></sub>,并分别将支撑区域中特征点及其对角线顶点映射到正方形特征区域的左下角和右上角顶点。由支撑区域与特征区域4个顶点的对应关系计算单应性矩阵进行特征区域归一化。将特征区域划分为16个子区域,在每个子区域内计算8个方向的梯度方向直方图,得到128维特征描述符。最后,对特征描述符进行归一化处理,提高特征描述符对影像光照变化的稳健性<citation id="234" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="80">通过本文提出的特征区域归一化处理,一方面可以避免建筑物同一顶(侧)面上不同位置的角点因为对应于同一个支撑区域而产生错误匹配,另一方面能够消除影像旋转变化的影响。如图6所示,特征点<i><b>p</b></i><sub>1</sub>和<i><b>p</b></i><sub>2</sub>虽然支撑区域相同,但是经过本文方法得到的特征区域和特征描述符具有较强的可区分性,能够避免错误匹配。虽然特征点<i><b>p</b></i><sub>1</sub>和<i><b>p</b></i><sub>3</sub>所在的影像存在旋转变化,但是本文提出的特征区域归一化方法能够消除影像旋转变化,得到相似的特征区域和特征描述符,提高特征匹配率。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 特征区域和特征描述符计算过程" src="Detail/GetImg?filename=images/CHXB201909007_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 特征区域和特征描述符计算过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Feature region and descriptor computation</p>

                </div>
                <div class="p1">
                    <p id="82">(4) 利用双向的NNDR(nearest neighbor distance ratio)匹配策略<citation id="235" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>进行特征点匹配,并结合RANSAC算法剔除错误匹配,得到初匹配集合MSet<sub>1</sub>和基础矩阵<i><b>F</b></i>。</p>
                </div>
                <div class="p1">
                    <p id="83">虽然初匹配能够正确匹配部分特征点,但是该方法只对满足以下条件的特征点有效:特征点具有至少两个满足夹角约束条件并且能够获得显著点(图4所示方法)的几何结构方向。该条件导致初匹配步骤获得的匹配点数量有限。为了提高匹配点数量,本文方法在初匹配之后分别设计双重核线约束的结构自适应特征点匹配(1.2节)和特征点匹配扩展(1.3节)算法。前者用于处理具有两个及两个以上几何结构方向的未匹配特征点;后者用于处理经过前面两个匹配步骤仍然未匹配成功的特征点。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">1.2 双重核线约束的结构自适应特征点匹配</h4>
                <div class="p1">
                    <p id="85">首先,对于参考影像上未匹配且具有两个及以上几何结构方向的特征点<i>p</i><sub><i>i</i></sub>,由特征点与任意两个夹角<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>∈</mo><mrow><mo>[</mo><mrow><mi>θ</mi><mo>,</mo><mtext>π</mtext><mo>-</mo><mi>θ</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>的几何结构方向的终点<i>s</i><sub>1</sub>和<i>s</i><sub>2</sub>确定一个平行四边形,作为特征点<i>p</i><sub><i>i</i></sub>的支撑区域(图7(a))。其次,基于基础矩阵<i><b>F</b></i>分别计算点<i><b>p</b></i><sub><i><b>i</b></i></sub>、<i><b>s</b></i><sub>1</sub>和<i><b>s</b></i><sub>2</sub>在搜索影像上对应的核线<i><b>e</b></i><sub><i>pi</i></sub>=<i><b>F</b></i><sub><i><b>p</b></i></sub><sub><i>i</i></sub>、<i><b>e</b></i><sub><i>s</i></sub><sub>1</sub>=<i><b>F</b></i><sub><i><b>s</b></i></sub><sub>1</sub>和<i><b>e</b></i><sub><i>s</i></sub><sub>2</sub>=<i><b>F</b></i><sub><i><b>s</b></i></sub><sub>2</sub>。然后,在搜索影像上筛选到核线<i><b>e</b></i><sub><i>pi</i></sub>的距离小于阈值<i>T</i><sub><i>e</i></sub>,并且具有两个及以上几何结构方向的特征点,得到候选匹配点集合<i>C</i><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>o</mi></msubsup></mrow></math></mathml>。最后,对候选匹配点<i><b>q</b></i><sub><i>j</i></sub>∈<i>C</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>o</mi></msubsup></mrow></math></mathml>,取其任意两个夹角<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo>∈</mo><mrow><mo>[</mo><mrow><mi>θ</mi><mo>,</mo><mtext>π</mtext><mo>-</mo><mi>θ</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>的几何结构方向,分别计算几何结构方向与核线<i><b>e</b></i><sub><i>s</i></sub><sub>1</sub>和<i><b>e</b></i><sub><i>s</i></sub><sub>2</sub>的交点<i><b>Q</b></i><sub><i>s</i></sub><sub>1</sub>和<i><b>Q</b></i><sub><i>s</i></sub><sub>2</sub>,由点<i><b>q</b></i><sub><i>j</i></sub>、<i><b>Q</b></i><sub><i>s</i></sub><sub>1</sub>和<i><b>Q</b></i><sub><i>s</i></sub><sub>2</sub>为<i><b>q</b></i><sub><i>j</i></sub>确定一个平行四边形支撑区域。由于在匹配之前特征点<i><b>p</b></i><sub><i>i</i></sub>和<i><b>q</b></i><sub><i>j</i></sub>的几何结构方向对应关系未知,本文方法在计算交点<i><b>Q</b></i><sub><i>s</i></sub><sub>1</sub>和<i><b>Q</b></i><sub><i>s</i></sub><sub>2</sub>时分别考虑如图7(b)和(c)所示的两种情况,构建两个相应的支撑区域,避免由错误的几何结构方向对应关系造成同名特征点支撑区域不一致的问题。此外,鉴于核线估计存在误差导致计算的交点不可靠,本文方法在交点<i><b>Q</b></i><sub><i>s</i></sub><sub>1</sub>和<i><b>Q</b></i><sub><i>s</i></sub><sub>2</sub>的邻域内(邻域大小为2<i>T</i><sub><i>e</i></sub>)计算显著点来代替<i><b>Q</b></i><sub><i>s</i></sub><sub>1</sub>和<i><b>Q</b></i><sub><i>s</i></sub><sub>2</sub>作为支撑区域顶点。显著点计算方法为:对交点邻域内任意像素<i><b>g</b></i><sub><i>k</i></sub>,按式(1)计算其像素显著性<i>S</i>(<i><b>g</b></i><sub><i>k</i></sub>),交点邻域中显著性值最大的像素即为显著点</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>e</mtext><mtext>r</mtext><mtext>a</mtext><mtext>g</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">(</mo><mi>G</mi><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup><mo stretchy="false">(</mo><mi>Ν</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>v</mtext><mtext>e</mtext><mtext>r</mtext><mtext>a</mtext><mtext>g</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false">(</mo><mi>G</mi><msubsup><mrow></mrow><mi>k</mi><mi>r</mi></msubsup><mo stretchy="false">(</mo><mi>Ν</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">式中,<i>G</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup></mrow></math></mathml>(<i>N</i>)和<i>G</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>r</mi></msubsup></mrow></math></mathml>(<i>N</i>)分别表示以点<i><b>g</b></i><sub><i>k</i></sub>为中心,沿对应的几何结构方向两侧<i>N</i>个像素的集合(本文方法中参数<i>N</i>设置为5);<i>I</i><sub>average</sub>()表示像素集合的灰度值均值。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 核线约束的特征点支撑区域" src="Detail/GetImg?filename=images/CHXB201909007_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 核线约束的特征点支撑区域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Feature support region computation based on epipolar geometric constraint</p>

                </div>
                <div class="p1">
                    <p id="89">获得参考影像特征点及其候选同名特征点的支撑区域以后,采用1.1节提出的方法计算特征区域和特征描述符,并基于NNDR匹配策略<citation id="236" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>得到特征点匹配集合MSet<sub>2</sub>。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">1.3 特征点匹配扩展</h4>
                <div class="p1">
                    <p id="91">根据特征点是否位于某个已匹配的特征点支撑区域内,将未匹配的特征点分成两类(第1类:是;第2类:否),并分别进行匹配:</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">1.3.1 结合几何约束和特征描述符相似性匹配第1类特征点</h4>
                <div class="p1">
                    <p id="93">在几何约束方面,如图8所示,假设(<i><b>p</b></i><sub><i>i</i></sub>,<i><b>q</b></i><sub><i>i</i></sub>)为一对已匹配的特征点,图中实线平行四边形区域为其支撑区域。<i><b>X</b></i>为参考影像上位于点<i><b>p</b></i><sub><i>i</i></sub>支撑区域内的一个未匹配特征点,由搜索影像上所有位于特征点<i><b>q</b></i><sub><i>i</i></sub>支撑区域内的未匹配特征点构成<i><b>X</b></i>的候选匹配点集合<i>C</i><sub><i>X</i></sub>。由于特征点支撑区域是基于直线特征确定的局部区域,可以近似为平面区域。由仿射几何可知,如果特征点<i><b>X</b></i>与特征点<i><b>Y</b></i>∈<i>C</i><sub><i>X</i></sub>为一对同名点,则<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">A</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">B</mi></mrow><mo>|</mo></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">E</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">F</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>,且<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">C</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">D</mi></mrow><mo>|</mo></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">G</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>。其中,点<i><b>A</b></i><b>、</b><i><b>B</b></i><b>、</b><i><b>C</b></i><b>、</b><i><b>D</b></i>分别为过特征点<i><b>X</b></i>且与点<i><b>p</b></i><sub><i>i</i></sub>的支撑区域的边平行的直线与点<i><b>p</b></i><sub><i>i</i></sub>的支撑区域的交点;点<i><b>E</b></i><b>、</b><i><b>F</b></i><b>、</b><i><b>G</b></i><b>、</b><i><b>H</b></i>分别为过特征点<i><b>Y</b></i>且与点<i><b>q</b></i><sub><i>i</i></sub>的支撑区域的边平行的直线与点<i><b>q</b></i><sub><i>i</i></sub>的支撑区域的交点。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 第1类未匹配特征点匹配扩展" src="Detail/GetImg?filename=images/CHXB201909007_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 第1类未匹配特征点匹配扩展  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 The matching expansion for the unmatched point in the first class</p>

                </div>
                <div class="p1">
                    <p id="95">计算特征描述符时,首先,根据<i><b>X</b></i>在<i><b>p</b></i><sub><i>i</i></sub>的支撑区域中的位置确定<i><b>X</b></i>的支撑区域:如果<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">A</mi></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">B</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>,则点<i><b>B</b></i>被视作一个显著点,否则点<i><b>A</b></i>被视作显著点;如果<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">C</mi></mrow><mo>|</mo></mrow><mo>≤</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">D</mi></mrow><mo>|</mo></mrow></mrow></math></mathml>,则点<i><b>D</b></i>被视作第2个显著点,否则点<i><b>C</b></i>被视作第2个显著点。由点<i><b>X</b></i>与两个显著点共同确定特征点<i><b>X</b></i>的支撑区域;然后,根据显著点的对应关系(<i><b>A</b></i><b>→</b><i><b>E</b></i><b>、</b><i><b>B</b></i><b>→</b><i><b>F</b></i><b>、</b><i><b>C</b></i><b>→</b><i><b>G</b></i>和<i><b>D</b></i><b>→</b><i><b>H</b></i>),确定候选匹配点对应的显著点及支撑区域;最后,采用1.1节所述方法计算特征点<i><b>X</b></i>及其候选匹配点的特征区域和特征描述符。</p>
                </div>
                <div class="p1">
                    <p id="96">按式(2)计算参考影像特征点与所有候选同名特征点的相似性度量值sim(<i><b>X</b></i><b>,</b><i><b>Y</b></i>),并寻找最相似的候选匹配点,如果其相似性度量值大于阈值<i>T</i><sub>sim</sub>,则认为该特征点与参考特征点为一对匹配点。完成第1类特征点匹配以后,得到匹配集合MSet<sub>3</sub>。</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>s</mtext><mtext>i</mtext><mtext>m</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>,</mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mtext>a</mtext><mtext>b</mtext><mtext>s</mtext><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">A</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">B</mi></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mo>/</mo><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">E</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">F</mi></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mo>-</mo><mn>1</mn></mrow><mo>]</mo></mrow><mo>&gt;</mo><mi>τ</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mspace width="0.25em" /><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mtext>a</mtext><mtext>b</mtext><mtext>s</mtext><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">C</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">D</mi></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mo>/</mo><mrow><mo>(</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">G</mi></mrow><mo>|</mo></mrow><mo>/</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">Η</mi></mrow><mo>|</mo></mrow></mrow><mo>)</mo></mrow><mo>-</mo><mn>1</mn></mrow><mo>]</mo></mrow><mo>&gt;</mo><mi>τ</mi></mtd></mtr><mtr><mtd><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold">D</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">c</mi><msub><mrow></mrow><mi>X</mi></msub><mo>-</mo><mi mathvariant="bold">D</mi><mi mathvariant="bold">e</mi><mi mathvariant="bold">s</mi><mi mathvariant="bold">c</mi><msub><mrow></mrow><mi>Y</mi></msub></mrow><mo>|</mo></mrow></mrow></msup><mo>,</mo><mtext> </mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">式中,<i>τ</i>为仿射不变量阈值;<b>Desc</b><sub><i>X</i></sub>和<b>Desc</b><sub><i>Y</i></sub>分别为特征点<i><b>X</b></i>及其候选匹配点<i><b>Y</b></i>的特征描述符。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">1.3.2 基于单应变换匹配第2类特征点</h4>
                <div class="p1">
                    <p id="100">首先,基于前面所有匹配结果{MSet<sub><i>i</i></sub>,<i>i</i>=1,2,3}估计影像之间的单应矩阵<i><b>H</b></i>;然后,为参考影像上所有第2类未匹配特征点确定以特征点为中心的正方形特征区域,基于单应变换为搜索影像上所有第2类未匹配特征点确定以特征点为中心的四边形特征区域,并将四边形特征区域归一化为正方形区域;最后,计算所有未匹配特征点的特征描述符,并在核线约束下通过NNDR匹配策略得到匹配集合MSet<sub>4</sub>。利用RANSAC算法对所有匹配结果{MSet<sub><i>i</i></sub>,<i>i</i>=1,…,4}剔除错误匹配,得到最终匹配结果。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">2 试验及结果分析</h3>
                <h4 class="anchor-tag" id="102" name="102">2.1 试验数据</h4>
                <div class="p1">
                    <p id="103">为了验证本文算法的有效性,分别采用6对典型的局部影像块、一组原始大小的三视航空倾斜影像和一组原始大小的三视无人机影像进行特征点匹配试验。如图9所示:(a)和(b)所示分别为平房和广场区域倾斜像对,其中参考影像为下视影像,搜索影像为斜视影像;(c)所示为无人机影像对;(d)—(f)所示均为倾斜影像,其中(d)和(e)中参考影像为下视影像,搜索影像为斜视影像,(f)中参考影像和搜索影像均为斜视影像;(g)所示为三视航空倾斜影像;(h)所示为三视无人机影像。试验数据详细信息见表1。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表1 试验数据详细信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.1 Details of experimental datasets</b></p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td><br />编号</td><td>拍摄系统</td><td>拍摄地点</td><td>相对航高/m</td><td>地面采样间隔/cm</td><td>交会角/(°)</td><td>影像大小/pixel</td></tr><tr><td><br />影像对1</td><td>IQ180</td><td>深圳</td><td>800</td><td>8</td><td>51.4</td><td>1000×1000</td></tr><tr><td><br />影像对2</td><td>IQ180</td><td>深圳</td><td>800</td><td>8</td><td>53.6</td><td>1000×1000</td></tr><tr><td><br />影像对3</td><td>Nikon D810</td><td>深圳</td><td>300</td><td>6</td><td>36.9</td><td>1024×1024</td></tr><tr><td><br />影像对4</td><td>SWDC-5</td><td>贵阳</td><td>600</td><td>8</td><td>49.6</td><td>1000×1000</td></tr><tr><td><br />影像对5</td><td>SWDC-5</td><td>贵阳</td><td>600</td><td>8</td><td>47.7</td><td>1200×1200</td></tr><tr><td><br />影像对6</td><td>SWDC-5</td><td>贵阳</td><td>600</td><td>8</td><td>94.4</td><td>1200×1200</td></tr><tr><td><br />三视影像1</td><td>SWDC-5</td><td>贵阳</td><td>600</td><td>8</td><td>—</td><td>8176×6132</td></tr><tr><td><br />三视影像2</td><td>Canon IXUS127</td><td>南昌</td><td>240</td><td>8</td><td>—</td><td>4608×3456</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="105" name="105">2.2 参数设置</h4>
                <div class="p1">
                    <p id="106">本文方法相关参数设置为:构建特征点邻域几何结构方向向量时,特征点邻域大小<i>m</i>×<i>m</i>=11×11像素;邻域几何结构方向向量支撑区域参数<i>s</i>=20像素;邻域几何结构方向向量夹角阈值<i>θ</i>=10°;归一化特征区域尺寸<i>T</i><sub><i>r</i></sub>×<i>T</i><sub><i>r</i></sub>=65×65像素;点到核线的距离阈值<i>T</i><sub><i>e</i></sub>=20像素;特征匹配扩展算法中,仿射不变量阈值<i>τ</i>=0.3,特征描述符相似性阈值<i>T</i><sub>sim</sub>=0.65。在本文试验中,Harris算子<citation id="237" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>和LSD算子<citation id="238" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>分别被用于提取点特征和直线特征。本文试验中所有对比方法的参数均按原文献作者推荐的参数值进行设置。本文所有试验均在相同平台环境(Windows 10,Intel Core i7 3.6 GHz,RAM 32 GB)下完成。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 试验数据" src="Detail/GetImg?filename=images/CHXB201909007_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 试验数据  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Experimental datasets</p>

                </div>
                <h4 class="anchor-tag" id="108" name="108">2.3 试验结果分析</h4>
                <h4 class="anchor-tag" id="109" name="109">2.3.1 局部影像像对匹配结果分析</h4>
                <div class="p1">
                    <p id="110">本文试验首先基于局部影像对(图9(a)—(f))进行两两匹配,将本文方法与多种匹配算法进行对比分析以验证本文方法对典型影像区域的有效性。对比方法包括:分别将Harris-Affine算子<citation id="239" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、Hessian-Affine算子<citation id="240" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、MSER算子<citation id="241" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>和DoG算子<citation id="242" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>与SIFT特征描述符和NNDR匹配策略组合而成的4种特征匹配算法(HarAff、HesAff、MSER和SIFT算法)、ASIFT算法<citation id="243" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,以及基于影像粗纠正的ISIFT算法<citation id="244" type="reference"><link href="199" rel="bibliography" /><link href="201" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="111">在这部分试验中,以正确匹配特征对数和匹配正确率(正确匹配特征对数/总匹配对数)为评价指标,统计结果如图10所示。其中,通过人工检查的方式来统计正确匹配特征对数。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 对比试验统计结果" src="Detail/GetImg?filename=images/CHXB201909007_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 对比试验统计结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Statistic results of the comparative experiments</p>

                </div>
                <div class="p1">
                    <p id="113">从图10(a)所示的正确匹配对数可以看出,HarAff、HesAff、MSER和SIFT 4种方法在所有像对上都只能获得少量的匹配对,表明这4种方法难以适用于视角变化较大且存在遮挡问题的城区宽基线影像。相对于上述4种方法,ASIFT和ISIFT算法通过改进匹配策略来提高算法的稳健性。其中,ASIFT算法通过模拟影像仿射空间来消除影像之间的几何变形,获得了更好的匹配效果。影像对2中场景深度变化不显著,ASIFT算法模拟的仿射空间能够较好地拟合影像局部区域,因此ASIFT算法在影像对2上获得了最多的匹配对。但是ASIFT方法中模拟的仿射空间不连续,在影像视角变化和场景深度变化较大的影像对4、5和6上,许多局部区域没有被模拟的仿射空间所覆盖,即难以通过模拟仿射空间来消除这些局部区域的几何变形,因此ASIFT方法在这3对影像上获得的特征对数较少。此外,ASIFT算法为特征点分配规则特征区域的方法难以适用于地物遮挡和视差不连续的情况。ISIFT算法也是基于影像模拟和粗纠正的思想,但是ISIFT算法只对整幅影像进行一次模拟,当参考影像与待匹配影像视角变化大且影像场景深度变化较大时,整幅影像之间不服从同一个全局变换模型,ISIFT方法获得的几何变换模型只能纠正影像中的部分区域。因此ISIFT方法虽然能够改善SIFT方法的匹配结果,但是其改善程度有限。此外,ISIFT算法依赖于初匹配的结果。如图中所示影像对5的匹配结果,ISIFT算法因为初匹配结果难以准确估计影像之间的几何变换模型,最终匹配失败。</p>
                </div>
                <div class="p1">
                    <p id="114">相对于以上方法,本文方法在除影像对2以外的所有5对影像上都得到了最多的正确匹配。这主要得益于本文方法能够根据特征点邻域结构自适应地获取影像内容一致的特征区域。无论特征点位于平面区域或是视差不连续区域,本文方法得到的同名特征区域之间都具有较高的相似度,更容易在特征匹配过程中被正确识别出来。此外,本文方法中的特征匹配扩展算法有助于获得更多的匹配对。</p>
                </div>
                <div class="p1">
                    <p id="115">从图10(b)所示的匹配正确率可以看出,本文方法和ASIFT算法的匹配正确率优于其他方法。当影像初匹配能够获得一定数量的正确匹配用于估计影像几何变换模型时,ISIFT算法也能获得较高的匹配正确率,但是当影像视角变化导致无法通过初匹配来估计影像几何变换模型时,ISIFT算法将匹配失败。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">2.3.2 完整三视影像匹配结果分析</h4>
                <div class="p1">
                    <p id="117">除了采用局部影像像对进行算法验证以外,本文试验还利用两组三视影像测试本文方法的匹配性能。鉴于SIFT算法的广泛应用以及ASIFT算法对影像视角变化的稳健性,这部分试验将本文方法与SIFT算法和ASIFT算法进行对比分析。在具体实施时,考虑到原始SIFT算法和ASIFT算法在处理较大尺寸影像时计算内存开销非常大,且算法时间效率极低,本文采用更加高效的GPU版本的SIFT算法(SIFTGPU<citation id="245" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>)以及下采样模式的ASIFT算法(先将原始影像下采样为800×600像素大小的影像进行匹配,再将匹配结果反算回原始影像<citation id="246" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>)。此部分试验以三度重叠匹配数量和匹配效率为评价指标。3种方法在两组三视影像上匹配的统计结果如下表2所示,三度重叠匹配如图11和图12所示。由于ASIFT方法的三度重叠匹配数量为0,因此图11中只列出SIFTGPU方法和本文方法的结果。</p>
                </div>
                <div class="area_img" id="118">
                    <p class="img_tit"><b>表2 三视影像匹配统计结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.2 Statistical results on three-view images</b></p>
                    <p class="img_note"></p>
                    <table id="118" border="1"><tr><td colspan="2"><br /></td><td>SIFTGPU</td><td>ASIFT</td><td>本文方法</td></tr><tr><td rowspan="2"><br />三视<br />影像1</td><td><br />三度重叠匹配数量</td><td>10</td><td>0</td><td>463</td></tr><tr><td><br />运算时间/s</td><td>15</td><td>2392</td><td>1537</td></tr><tr><td colspan="5"><br /></td></tr><tr><td rowspan="2"><br />三视<br />影像2</td><td><br />三度重叠匹配数量</td><td>593</td><td>327</td><td>1303</td></tr><tr><td><br />运算时间/s</td><td>10</td><td>227</td><td>139</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 三视影像1的三度重叠匹配结果" src="Detail/GetImg?filename=images/CHXB201909007_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 三视影像1的三度重叠匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 Three-time overlapped matches on three-view image dataset 1</p>

                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 三视影像2的三度重叠匹配结果" src="Detail/GetImg?filename=images/CHXB201909007_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 三视影像2的三度重叠匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.12 Three-time overlapped matches on three-view image dataset 2</p>

                </div>
                <div class="p1">
                    <p id="121">从表2统计结果可以看出,在三度重叠匹配数量方面,本文方法在两组数据上获得的三度重叠匹配数量都远超SIFTGPU和ASIFT算法。尤其在三视影像1上,影像之间视角变化大,且影像场景为密集建筑区域,大量特征点位于视差不连续的边缘附近,SIFTGPU和ASIFT算法几乎匹配失败,而本文方法仍然能够获得463个三度重叠匹配;在算法时间效率方面,SIFTGPU算法的时间效率最高。本文方法在分步匹配中利用初匹配估计同名核线来约束后续匹配过程,时间效率优于ASIFT算法,但相对于SIFTGPU而言,运算效率仍然较低。</p>
                </div>
                <div class="p1">
                    <p id="122">此外,表2统计结果显示ASIFT方法获得的三度重叠匹配少于SIFTGPU方法,尤其在三视影像1上的三度重叠匹配数量为0。对ASIFT方法在该数据集上的匹配结果进行仔细检查发现:ASIFT方法在三视影像集1中三对立体像对之间获得的两两匹配数量分别为108对、33对和222对,如图13所示。其中,影像1和影像3由于视角差异太大(图13(b)),ASIFT方法获得的匹配点数量非常少,直接影响了三度重叠匹配的数量。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 ASIFT方法(下采样模式)在三视影像1上的两两匹配结果" src="Detail/GetImg?filename=images/CHXB201909007_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 ASIFT方法(下采样模式)在三视影像1上的两两匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.13 Matches of ASIFT (down-sampling mode) on image pairs in three-view image dataset 1</p>

                </div>
                <div class="p1">
                    <p id="124">为了进一步验证ASIFT算法在三视影像1上的匹配效果,使用普通模式(直接在原始影像上进行匹配)的ASIFT方法对三视影像1进行匹配试验。具体实施时,采用OpenCV中的ASIFT算子,分别在三视影像1中的3幅影像上提取了2 523 709、3 746 247和3 402 273个特征点。数百万个特征点进行盲匹配和穷举搜索带来了巨大的时间开销(约67个小时),然而三度重叠匹配数量仍然是0。3幅影像两两匹配的结果如图14所示。从图14(b)可以看出,对于视角变化非常大的影像1和影像3,ASIFT算法获得的匹配点非常少,与图13(b)的结果一致。此外,从图14(a)和图14(c)所示匹配结果可以看出,虽然ASIFT方法在两两影像之间能够获得一些匹配点,但是匹配点在多视影像上的重复率非常低。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201909007_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 ASIFT方法(普通模式)在三视影像1上的两两匹配结果" src="Detail/GetImg?filename=images/CHXB201909007_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 ASIFT方法(普通模式)在三视影像1上的两两匹配结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201909007_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.14 Matches of ASIFT (general mode) on image pairs in three-view image dataset 1</p>

                </div>
                <div class="p1">
                    <p id="126">综上所述,本文方法在匹配效果方面优于SIFTGPU方法和ASIFT方法,在匹配时间效率方面优于ASIFT方法,但低于SIFTGPU方法。笔者将在后续研究中通过算法和程序优化提高本文方法的时间效率。</p>
                </div>
                <h3 id="127" name="127" class="anchor-tag">3 结 论</h3>
                <div class="p1">
                    <p id="128">本文针对城区宽基线影像视角变化导致传统方法难以为同名点计算影像内容一致的特征区域和相似特征描述符,进而导致匹配失败的问题,提出了一种结构自适应的特征点匹配方法。本文方法的创新之处在于利用城区影像点特征与直线特征的几何关系定义了特征点几何结构方向信息,构建了结构自适应的特征区域和特征描述符,在此基础上设计特征匹配和扩展算法,实现了城区宽基线影像的可靠匹配。以上改进使得本文方法能够较好地处理因影像视角变化导致的几何变形和遮挡问题,对于大视角变化的城区宽基线影像能够获得较好的匹配结果。但是,由于本文方法在特征匹配时利用了粗略的核线约束,因此匹配结果中的误匹配都满足该约束条件。在后续剔除误匹配时,部分误匹配难以通过(基于基础矩阵的)RANSAC算法来剔除。此外,本文方法的运算效率仍需进一步提高。后续工作将研究如何有效剔除错误匹配,并通过算法和程序优化提高本文方法的运算效率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="163">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201301014&amp;v=MDM2MjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBSmlYVGJMRzRIOUxNcm85RVlJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 戴激光,宋伟东,贾永红,等.一种新的异源高分辨率光学卫星遥感影像自动匹配算法[J].测绘学报,2013,42(1):80-86.DAI Jiguang,SONG Weidong,JIA Yonghong,et al.A new automatically matching algorithm for multi-source high resolution optical satellite images[J].Acta Geodaetica et Cartographica Sinica,2013,42(1):80-86.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201403009&amp;v=MzE5NzJGeXJoVXJ2QUppWFRiTEc0SDlYTXJJOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 叶沅鑫,单杰,彭剑威,等.利用局部自相似进行多光谱遥感图像自动配准[J].测绘学报,2014,43(3):268-275.DOI:10.13485/j.cnki.11-2089.2014.0039.YE Yuanxin,SHAN Jie,PENG Jianwei,et al.Automated multispectral remote sensing image registration using local self-similarity[J].Acta Geodaetica et Cartographica Sinica,2014,43(3):268-275.DOI:10.13485/j.cnki.11-2089.2014.0039.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201204019&amp;v=MjExOTJxNDlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFKaVhUYkxHNEg5UE0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 岳春宇,江万寿.几何约束和改进SIFT的SAR影像和光学影像自动配准方法[J].测绘学报,2012,41(4):570-576.YUE Chunyu,JIANG Wanshou.An automatic registration algorithm for SAR and optical images based on geometry constraint and improved SIFT[J].Acta Geodaetica et Cartographica Sinica,2012,41(4):570-576.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201603012&amp;v=MjEzNDhadWR2RnlyaFVydkFKaVhUYkxHNEg5Zk1ySTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 陈敏,朱庆,朱军,等.SAR影像与光学影像的高斯伽玛型边缘强度特征匹配法[J].测绘学报,2016,45(3):318-325.DOI:10.11947/j.AGCS.2016.20150084.CHEN Min,ZHU Qing,ZHU Jun,et al.Feature matching for SAR and optical images based on Gaussian-Gamma-shaped edge strength map[J].Acta Geodaetica et Cartographica Sinica,2016,45(3):318-325.DOI:10.11947/j.AGCS.2016.20150084.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201707015&amp;v=MjA2NDVMRzRIOWJNcUk5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBSmlYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 梁焕青,谢意,付四洲.颜色不变量与AKAZE特征相结合的无人机影像匹配算法[J].测绘学报,2017,46(7):900-909.DOI:10.11947/j.AGCS.2017.20160436.LIANG Huanqing,XIE Yi,FU Sizhou.UAV image registration algorithm using color invariant and AKAZE feature[J].Acta Geodaetica et Cartographica Sinica,2017,46(7):900-909.DOI:10.11947/j.AGCS.2017.20160436.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349910&amp;v=MDI1MTNobz1OaWZPZmJLN0h0RE9yWTlFWis4R0JYMDVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3N0lJbG9WYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> ZITOVÁ B,FLUSSER J.Image registration methods:a survey[J].Image and Vision Computing,2003,21(11):977-1000.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel interest-point-matching algorithm for high-resolution satellite images">

                                <b>[7]</b> XIONG Zhen,ZHANG Yun.A novel interest-point-matching algorithm for high-resolution satellite images[J].IEEE Transactions on Geoscience and Remote Sensing,2009,47(12):4189-4200.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD120427011557&amp;v=MDAyNDFlWnZGeWprVTczTUlWb2NOaWZjYXJLNkh0WE9xSTlFWmU0S0N4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZa&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> GRUEN A.Development and status of image matching in photogrammetry[J].The Photogrammetric Record,2012,27(137):36-57.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjEzMzMzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RnlubFVidkpKRlk9Tmo3QmFyTzRIdEhPcDR4RmJlc09Z&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MDU1ODJLN0h0RE5xbzlFWk9NTUJIUXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3N0lJbG9WYmhvPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> BAY H,ESS A,TUYTELAARS T,et al.Speeded-up robust features (SURF)[J].Computer Vision and Image Understanding,2008,110(3):346-359.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DAISY: An efficient dense descriptor applied to wide-baseline stereo">

                                <b>[11]</b> TOLA E,LEPETIT V,FUA P.Daisy:an efficient dense descriptor applied to wide-baseline stereo[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(5):815-830.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ASIFT: A New Framework for Fully Affine Invariant Image Comparison">

                                <b>[12]</b> MOREL J M,YU Guoshen.ASIFT:a new framework for fully affine invariant image comparison[J].SIAM Journal on Imaging Sciences,2009,2(2):438-469.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD85D5880ACA6B19DDEF5F422FFC56179&amp;v=MTYwMzVubjBsNFBudmdybVJEQ3JlU1JMMldDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDU5bGh4cnkreEtBPU5pZk9mY2V3RzZYSnA0ZEZGWmgrQ2c0NHhtSg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> HU Han,ZHU Qing,DU Zhiqiang,et al.Reliable spatial relationship constrained feature point matching of oblique aerial images[J].Photogrammetric Engineering &amp; Remote Sensing,2015,81(1):49-58.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201504012&amp;v=MjM1MTF2RnlyaFVydkFKaVhUYkxHNEg5VE1xNDlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 肖雄武,郭丙轩,李德仁,等.一种具有仿射不变性的倾斜影像快速匹配方法[J].测绘学报,2015,44(4):414-421.DOI:10.11947/j.AGCS.2015.20140048.XIAO Xiongwu,GUO Bingxuan,LI Deren,et al.A quick and affine invariance matching method for oblique images[J].Acta Geodaetica et Cartographica Sinica,2015,44(4):414-421.DOI:10.11947/j.AGCS.2015.20140048.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH201609003&amp;v=MTczNjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFNaVhJWnJHNEg5Zk1wbzlGWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 肖雄武,李德仁,郭丙轩,等.一种具有视点不变性的倾斜影像快速匹配方法[J].武汉大学学报(信息科学版),2016,41(9):1151-1159.XIAO Xiongwu,LI Deren,GUO Bingxuan,et al.A robust and rapid viewpoint-invariant matching method for oblique images[J].Geomatics and Information Science of Wuhan University,2016,41(9):1151-1159.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201603011&amp;v=MzA5NzQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBSmlYVGJMRzRIOWZNckk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 闫利,费亮,叶志云,等.大范围倾斜多视影像连接点自动提取的区域网平差法[J].测绘学报,2016,45(3):310-317.DOI:10.11947/j.AGCS.2016.20140673.YAN Li,FEI Liang,YE Zhiyun,et al.Automatic tie-points extraction for triangulation of large-scale oblique multi-view images[J].Acta Geodaetica et Cartographica Sinica,2016,45(3):310-317.DOI:10.11947/j.AGCS.2016.20140673.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201705005&amp;v=MjAwNDZHNEg5Yk1xbzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFKaVhUYkw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 张力,艾海滨,许彪,等.基于多视影像匹配模型的倾斜航空影像自动连接点提取及区域网平差方法[J].测绘学报,2017,46(5):554-564.DOI:10.11947/j.AGCS.2017.20160571.ZHANG Li,AI Haibin,XU Biao,et al.Automatic tie-point extraction based on multiple-image matching and bundle adjustment of large block of oblique aerial images[J].Acta Geodaetica et Cartographica Sinica,2017,46(5):554-564.DOI:10.11947/j.AGCS.2017.20160571.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300100892&amp;v=MzE1OTJlcnFRVE1ud1plWnRGaW5sVTc3SUlsb1ZiaG89TmlmT2ZiSzhIdExPckk5Rlplc1BCSFU3b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> SUN Yanbiao,ZHAO Liang,HUANG Shoudong,et al.L2-SIFT:SIFT feature extraction and matching for large images in large-scale aerial photogrammetry[J].ISPRS Journal of Photogrammetry and Remote Sensing,2014(91):1-16.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On-Board GNSS/IMU Assisted Feature Extraction and Matching for Oblique UAV Images">

                                <b>[19]</b> JIANG San,JIANG Wanshou.On-board GNSS/IMU assisted feature extraction and matching for oblique UAV images[J].Remote Sensing,2017,9(8):813.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Novel Algorithm for View and Illumination Invariant Image Matching">

                                <b>[20]</b> YU Yinan,HUANG Kaiqi,CHEN Wei,et al.A novel algorithm for view and illumination invariant image matching[J].IEEE Transactions on Image Processing,2012,21(1):229-240.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201601007&amp;v=MDI3MjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyaFVydkFQQ3JUZHJHNEg5Zk1ybzlGWTRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 仇春平,于瑞鹏,丁翠,等.面向倾斜立体影像的尺度不变特征匹配[J].遥感信息,2016,31(1):43-47.QIU Chunping,YU Ruipeng,DING Cui,et al.Oblique stereo image matching based on scale invariant feature[J].Remote Sensing Information,2016,31(1):43-47.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025011&amp;v=MDAwNTF0RmlubFU3N0lJbG9WYmhvPU5pZklZN0s3SHRqTnI0OUZaT2tLREgwNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> FISCHLER M A,BOLLES R C.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM,1981,24(6):381-395.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES2ABD146E53351F412D135C532F181A61&amp;v=MTY2MTRpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDU5bGh4cnkreEtBPU5pZk9mYkhKYktYTnE0a3dZZWdNQ1gxUHl4Y1JuajUrVFF6bnJ4QkRlTHFWTkx5ZUNPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> LI Kai,YAO Jian.Line segment matching and reconstruction via exploiting coplanar cues[J].ISPRS Journal of Photogrammetry and Remote Sensing,2017(125):33-49.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A local distinctive features matching method for remote sensing images with repetitive patterns">

                                <b>[24]</b> CHEN Min,QIN Rongjun,HE Haiqing,et al.A local distinctive features matching method for remote sensing images with repetitive patterns[J].Photogrammetric Engineering &amp; Remote Sensing,2018,84(8):513-524.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830896&amp;v=MTQ2NTRZPU5qN0Jhck80SHRIT3A0eEZiT0lKWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGeW5sVWJ2SkpG&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> MIKOLAJCZYK K,SCHMID C.Scale &amp; affine invariant interest point detectors[J].International Journal of Computer Vision,2004,60(1):63-86.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MDE0MjZvPU5pZk9mYks3SHRET3JZOUVaKzhHQzNRN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUlsb1ZiaA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> MATAS J,CHUM O,URBAN M,et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image and Vision Computing,2004,22(10):761-767.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Combined Corner and Edge Detector">

                                <b>[27]</b> HARRIS C,STEPHENS M.A combined corner and edge detector[C]//Proceedings of the 4th Alvey Vision Conference.Manchester,Britain:[s.n.],1988:147-152.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD: A Fast Line Segment Detector with a False Detection Control">

                                <b>[28]</b> VON GIOI R G,JAKUBOWICZ J,MOREL J M,et al.LSD:a fast line segment detector with a false detection control[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(4):722-732.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SiftGPU:a GPU implementation of scale invariant feature transform">

                                <b>[29]</b> WU Changchang.SiftGPU:a GPU implementation of scale invariant feature transform[EB/OL].(2011).http://ccwu.me/code.html.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ASIFT:An Algorithm for Fully Affine Invariant Comparison">

                                <b>[30]</b> YU Guoshen,MOREL J M.ASIFT:an algorithm for fully affine invariant comparison[J].Image Processing On Line,2011(1):11-38.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201909007" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201909007&amp;v=MDgzNTc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmhVcnZBSmlYVGJMRzRIOWpNcG85Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
