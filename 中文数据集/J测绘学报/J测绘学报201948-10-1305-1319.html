<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142607777920000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201910012%26RESULT%3d1%26SIGN%3dAeZsyvg49tEvcYAVkUpe6Gj7ib8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201910012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201910012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201910012&amp;v=MTk1MDRiTEc0SDlqTnI0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT0ppWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#77" data-title="1 基于2D地图的位姿优化校正方法 ">1 基于2D地图的位姿优化校正方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="1.1 图像校正与特征提取">1.1 图像校正与特征提取</a></li>
                                                <li><a href="#108" data-title="1.2 计算位姿假设集合">1.2 计算位姿假设集合</a></li>
                                                <li><a href="#128" data-title="1.3 筛选最优位姿">1.3 筛选最优位姿</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="2 试验与结果分析 ">2 试验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#132" data-title="2.1 试验数据准备">2.1 试验数据准备</a></li>
                                                <li><a href="#135" data-title="2.2 试验结果分析">2.2 试验结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#158" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 在(a)中,用户仍需要努力辨别信息与现实物体的对应关系;(b)中,多种地理内容如3D建筑轮廓、兴趣点/区域、道路、街道名称等混合在同一个虚实高精度配准的增强现实视图中,直观增强用户对周围环境的感知">图1 在(a)中,用户仍需要努力辨别信息与现实物体的对应关系;(b)中,多种地理内容如3D建筑轮廓、......</a></li>
                                                <li><a href="#75" data-title="图2 视觉辅助位姿优化与地理配准精度改进效果对比">图2 视觉辅助位姿优化与地理配准精度改进效果对比</a></li>
                                                <li><a href="#80" data-title="图3 基于2D地图的视觉辅助6DOF绝对位姿优化整体流程">图3 基于2D地图的视觉辅助6DOF绝对位姿优化整体流程</a></li>
                                                <li><a href="#97" data-title="图4 图像语义分割及建筑区域提取">图4 图像语义分割及建筑区域提取</a></li>
                                                <li><a href="#98" data-title="图5 建筑立面线特征提取及消隐点估计">图5 建筑立面线特征提取及消隐点估计</a></li>
                                                <li><a href="#102" data-title="图6 建筑左、右角点垂直边缘边界估计">图6 建筑左、右角点垂直边缘边界估计</a></li>
                                                <li><a href="#103" data-title="图7 建筑左、中、右角点垂直边缘识别提取">图7 建筑左、中、右角点垂直边缘识别提取</a></li>
                                                <li><a href="#106" data-title="图8 角点垂直边缘线水平分布与对应关系">图8 角点垂直边缘线水平分布与对应关系</a></li>
                                                <li><a href="#138" data-title="图9 相机初始位姿及2D位姿假设集合">图9 相机初始位姿及2D位姿假设集合</a></li>
                                                <li><a href="#139" data-title="图10 筛选得到相机2D位姿最优解">图10 筛选得到相机2D位姿最优解</a></li>
                                                <li><a href="#140" data-title="图11 本文的测试场景案例:纵向上每3幅图像对应一组测试场景,其中横向第1层为从原始图像中自动提取得到的3条角点边缘线(经滚动倾角校正);中间层为原始图像语义分割结果;第3层为传感器初始2D位姿和优化的2D位姿">图11 本文的测试场景案例:纵向上每3幅图像对应一组测试场景,其中横向第1层为从原始图像中自动提取得......</a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表1 以EP&lt;/b&gt;&lt;i&gt;&lt;b&gt;n&lt;/b&gt;&lt;/i&gt;&lt;b&gt;P定位3DOF位置为参考,本文方法位置精度优化比较&lt;/b&gt;"><b>表1 以EP</b><i><b>n</b></i><b>P定位3DOF位置为参考,本文方法位置精度优化比较</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;表2 以EP&lt;/b&gt;&lt;i&gt;&lt;b&gt;n&lt;/b&gt;&lt;/i&gt;&lt;b&gt;P定位3DOF姿态为参考,本文方法姿态精度优化比较&lt;/b&gt;"><b>表2 以EP</b><i><b>n</b></i><b>P定位3DOF姿态为参考,本文方法姿态精度优化比较</b></a></li>
                                                <li><a href="#145" data-title="图12 每一组场景水平位置精度改进对比">图12 每一组场景水平位置精度改进对比</a></li>
                                                <li><a href="#148" data-title="图13 每一组场景水平偏航角精度改进对比">图13 每一组场景水平偏航角精度改进对比</a></li>
                                                <li><a href="#154" data-title="图14 虚实配准量化精度求解原理">图14 虚实配准量化精度求解原理</a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表3 地理配准技术方案及其配准精度比较&lt;/b&gt;"><b>表3 地理配准技术方案及其配准精度比较</b></a></li>
                                                <li><a href="#160" data-title="图15 每一组试验场景虚实地理配准精度优化前和优化后的量化折线图">图15 每一组试验场景虚实地理配准精度优化前和优化后的量化折线图</a></li>
                                                <li><a href="#161" data-title="图16 场景2虚实地理配准精度优化前与优化后的效果对比">图16 场景2虚实地理配准精度优化前与优化后的效果对比</a></li>
                                                <li><a href="#162" data-title="图17 场景4虚实地理配准精度优化前与优化后的效果对比">图17 场景4虚实地理配准精度优化前与优化后的效果对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" 杜清运,刘涛.户外增强现实地理信息系统原型设计与实现[J].武汉大学学报(信息科学版),2007,32(11):1046-1049.DU Qingyun,LIU Tao.Design and implementation of a prototype outdoor augmented reality GIS[J].Geomatics and Information Science of Wuhan University,2007,32(11):1046-1049." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH200711024&amp;v=Mjc5NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT01pWElackc0SHRiTnJvOUhZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         杜清运,刘涛.户外增强现实地理信息系统原型设计与实现[J].武汉大学学报(信息科学版),2007,32(11):1046-1049.DU Qingyun,LIU Tao.Design and implementation of a prototype outdoor augmented reality GIS[J].Geomatics and Information Science of Wuhan University,2007,32(11):1046-1049.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" 孙敏,陈秀万,张飞舟,等.增强现实地理信息系统[J].北京大学学报(自然科学版),2004,40(6):906-913.SUN Min,CHEN Xiuwan,ZHANG Feizhou,et al.Augment reality geographical information system[J].Acta Scientiarum Naturalium Universitatis Pekinensis,2004,40(6):906-913." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJDZ200406008&amp;v=Mjg3NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdWTC9PSnlmUGRMRzRIdFhNcVk5RmJJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         孙敏,陈秀万,张飞舟,等.增强现实地理信息系统[J].北京大学学报(自然科学版),2004,40(6):906-913.SUN Min,CHEN Xiuwan,ZHANG Feizhou,et al.Augment reality geographical information system[J].Acta Scientiarum Naturalium Universitatis Pekinensis,2004,40(6):906-913.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" 刘经南,高柯夫.增强现实及其在导航与位置服务中的应用[J].地理空间信息,2013,11(2):1-6,14.LIU Jingnan,GAO Kefu.Augmented reality and its applications for navigation and location based service[J].Geospatial Information,2013,11(2):1-6,14." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DXKJ201302002&amp;v=MjUzOTZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT0lUWEFaTEc0SDlMTXJZOUY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         刘经南,高柯夫.增强现实及其在导航与位置服务中的应用[J].地理空间信息,2013,11(2):1-6,14.LIU Jingnan,GAO Kefu.Augmented reality and its applications for navigation and location based service[J].Geospatial Information,2013,11(2):1-6,14.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" 罗斌,王涌天,沈浩,等.增强现实混合跟踪技术综述[J].自动化学报,2013,39(8):1185-1201.LUO Bin,WANG Yongtian,SHEN Hao,et al.Overview of hybrid tracking in augmented reality[J].Acta Automatica Sinica,2013,39(8):1185-1201." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308004&amp;v=MTg2ODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZML09LQ0xmWWJHNEg5TE1wNDlGWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         罗斌,王涌天,沈浩,等.增强现实混合跟踪技术综述[J].自动化学报,2013,39(8):1185-1201.LUO Bin,WANG Yongtian,SHEN Hao,et al.Overview of hybrid tracking in augmented reality[J].Acta Automatica Sinica,2013,39(8):1185-1201.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" YOU Xiong,ZHANG Weiwei,MA Meng,et al.Survey on urban warfare augmented reality[J].ISPRS International Journal of Geo-Information,2018,7(2):46." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Survey on Urban Warfare Augmented Reality">
                                        <b>[5]</b>
                                         YOU Xiong,ZHANG Weiwei,MA Meng,et al.Survey on urban warfare augmented reality[J].ISPRS International Journal of Geo-Information,2018,7(2):46.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" 刘浩敏,章国锋,鲍虎军.基于单目视觉的同时定位与地图构建方法综述[J].计算机辅助设计与图形学学报,2016,28(6):855-868.LIU Haomin,ZHANG Guofeng,BAO Hujun.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2016,28(6):855-868." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201606001&amp;v=MDEzNzJGeXJnVkwvT0x6N0JhTEc0SDlmTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         刘浩敏,章国锋,鲍虎军.基于单目视觉的同时定位与地图构建方法综述[J].计算机辅助设计与图形学学报,2016,28(6):855-868.LIU Haomin,ZHANG Guofeng,BAO Hujun.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2016,28(6):855-868.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" 曲毅,李爱光,徐旺,等.基于位姿传感器的户外ARGIS注册技术[J].测绘科学技术学报,2017,34(1):106-110.QU Yi,LI Aiguang,XU Wang,et al.Outdoor ARGIS registration techniques based on position-posture sensor[J].Journal of Geomatics Science and Technology,2017,34(1):106-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFJC201701021&amp;v=MTk3MTR6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT0x5dkJiYkc0SDliTXJvOUhaWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         曲毅,李爱光,徐旺,等.基于位姿传感器的户外ARGIS注册技术[J].测绘科学技术学报,2017,34(1):106-110.QU Yi,LI Aiguang,XU Wang,et al.Outdoor ARGIS registration techniques based on position-posture sensor[J].Journal of Geomatics Science and Technology,2017,34(1):106-110.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" 曲毅,李爱光,徐旺,等.面向ArGIS的多传感器混合跟踪注册研究[J].测绘与空间地理信息,2017,40(3):114-117,121.QU Yi,LI Aiguang,XU Wang,et al.Research on multi sensor hybrid tracking registration for ArGIS[J].Geomatics &amp;amp; Spatial Information Technology,2017,40(3):114-117,121." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBCH201703031&amp;v=MjgxNDNJWnJHNEg5Yk1ySTlHWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZML09JUy8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         曲毅,李爱光,徐旺,等.面向ArGIS的多传感器混合跟踪注册研究[J].测绘与空间地理信息,2017,40(3):114-117,121.QU Yi,LI Aiguang,XU Wang,et al.Research on multi sensor hybrid tracking registration for ArGIS[J].Geomatics &amp;amp; Spatial Information Technology,2017,40(3):114-117,121.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" 王俊.户外增强现实GIS的应用研究[D].重庆:西南大学,2014.WANG Jun.Application research on outdoor augmented reality GIS[D].Chongqing:Southwest University,2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014264657.nh&amp;v=MDYzOTZxQnRHRnJDVVI3cWZadWR2RnlyZ1ZML09WRjI2R3JHK0d0ZkpxSkViUElRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         王俊.户外增强现实GIS的应用研究[D].重庆:西南大学,2014.WANG Jun.Application research on outdoor augmented reality GIS[D].Chongqing:Southwest University,2014.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" SUN Min,LIU Lei,HUANG Wei,et al.Interactive registration for augmented reality GIS[C]//Proceedings of 2012 International Conference on Computer Vision in Remote Sensing.Xiamen,China:IEEE,2012:246-251." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interactive registration for Augmented Reality GIS">
                                        <b>[10]</b>
                                         SUN Min,LIU Lei,HUANG Wei,et al.Interactive registration for augmented reality GIS[C]//Proceedings of 2012 International Conference on Computer Vision in Remote Sensing.Xiamen,China:IEEE,2012:246-251.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" HUANG Wei,SUN Min,LI Songnian.A 3D GIS-based interactive registration mechanism for outdoor augmented reality system[J].Expert Systems with Applications,2016,55(8):48-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7C0ED1802EC2EFDFA82551F774CD9C59&amp;v=MTk5NzdvZEZacDU4RGdsUHUyQmk0ajE0VFg2VXF4VXhDc2FkTnIrV0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNTlsaHhyMjR3SzQ9TmlmT2ZiVExIcVM0cg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         HUANG Wei,SUN Min,LI Songnian.A 3D GIS-based interactive registration mechanism for outdoor augmented reality system[J].Expert Systems with Applications,2016,55(8):48-58.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" VENTURA J,H&#214;LLERER T.Wide-area scene mapping for mobile visual tracking[C]//Proceedings of 2012 IEEE International Symposium on Mixed and Augmented Reality.Atlanta,GA:IEEE,2012:3-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wide-area Scene Mapping for Mobile Visual Tracking">
                                        <b>[12]</b>
                                         VENTURA J,H&#214;LLERER T.Wide-area scene mapping for mobile visual tracking[C]//Proceedings of 2012 IEEE International Symposium on Mixed and Augmented Reality.Atlanta,GA:IEEE,2012:3-12.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" MIDDELBERG S,SATTLER T,UNTZELMANN O,et al.Scalable 6-DOF localization on mobile devices[C]//Proceedings of the 13th European conference on computer vision.Zurich,Switzerland:Springer,2014:268-283." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable6-DOF Localization on Mobile Devices">
                                        <b>[13]</b>
                                         MIDDELBERG S,SATTLER T,UNTZELMANN O,et al.Scalable 6-DOF localization on mobile devices[C]//Proceedings of the 13th European conference on computer vision.Zurich,Switzerland:Springer,2014:268-283.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" ARTH C,VENTURA J,SCHMALSTIEG D.Geospatial management and utilization of large-scale urban visual reconstructions[C]//Proceedings of the 4th International Conference on Computing for Geospatial Research and Application.San Jose,CA:IEEE,2013:64-69." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Geospatial management and utilization of large-scale urban visual reconstructions">
                                        <b>[14]</b>
                                         ARTH C,VENTURA J,SCHMALSTIEG D.Geospatial management and utilization of large-scale urban visual reconstructions[C]//Proceedings of the 4th International Conference on Computing for Geospatial Research and Application.San Jose,CA:IEEE,2013:64-69.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" 段利亚.移动增强现实大范围定位与注册关键技术研究[D].武汉:华中科技大学,2013.DUAN Liya.Study on key technology of wide area localization and registration for mobile augmented reality systems[D].Wuhan:Huazhong University of Science and Technology," target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014024686.nh&amp;v=MTE1NzRyZ1ZML09WRjI2R3JPNkd0ZkVxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         段利亚.移动增强现实大范围定位与注册关键技术研究[D].武汉:华中科技大学,2013.DUAN Liya.Study on key technology of wide area localization and registration for mobile augmented reality systems[D].Wuhan:Huazhong University of Science and Technology,
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_2013" >
                                        <b>[2013]</b>
                                    .</a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_16" title=" VENTURA J,ARTH C,REITMAYR G,et al.Global localization from monocular SLAM on a mobile phone[J].IEEE Transactions on Visualization and Computer Graphics,2014,20(4):531-539." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global localization from monocular SLAM on a mobile phone.">
                                        <b>[16]</b>
                                         VENTURA J,ARTH C,REITMAYR G,et al.Global localization from monocular SLAM on a mobile phone[J].IEEE Transactions on Visualization and Computer Graphics,2014,20(4):531-539.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_17" title=" LIU Haomin,ZHANG Guofeng,BAO Hujun.Robust keyframe-based monocular SLAM for augmented reality[C]//Proceedings of 2016 IEEE International Symposium on Mixed and Augmented Reality.Merida,Mexico:IEEE,2016:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Keyframe-based Monocular SLAM for Augmented Reality">
                                        <b>[17]</b>
                                         LIU Haomin,ZHANG Guofeng,BAO Hujun.Robust keyframe-based monocular SLAM for augmented reality[C]//Proceedings of 2016 IEEE International Symposium on Mixed and Augmented Reality.Merida,Mexico:IEEE,2016:1-10.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_18" title=" LI Peiliang,QIN Tong,HU Botao,et al.Monocular visual-inertial state estimation for mobile augmented reality[C]//Proceedings of 2017 IEEE International Symposium on Mixed and Augmented Reality.Nantes,France:IEEE,2017:11-21." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monocular visual-inertial state estimation for mobile augmented reality">
                                        <b>[18]</b>
                                         LI Peiliang,QIN Tong,HU Botao,et al.Monocular visual-inertial state estimation for mobile augmented reality[C]//Proceedings of 2017 IEEE International Symposium on Mixed and Augmented Reality.Nantes,France:IEEE,2017:11-21.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_19" title=" MENOZZI A,CLIPP B,WENGER E,et al.Development of vision-aided navigation for a wearable outdoor augmented reality system[C]//Proceedings of 2014 IEEE/ION Position,Location and Navigation Symposium - PLANS 2014.Monterey,CA:IEEE,2014:460-472." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Development of vision-aided navigation for a wearable outdoor augmented reality system">
                                        <b>[19]</b>
                                         MENOZZI A,CLIPP B,WENGER E,et al.Development of vision-aided navigation for a wearable outdoor augmented reality system[C]//Proceedings of 2014 IEEE/ION Position,Location and Navigation Symposium - PLANS 2014.Monterey,CA:IEEE,2014:460-472.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_20" title=" GANS E,ROBERTS D,BENNETT M,et al.Augmented reality technology for day/night situational awareness for the dismounted soldier[C]//Proceedings Volume 9470,Display Technologies and Applications for Defense,Security,and Avionics IX;and Head- and Helmet-Mounted Displays XX.Baltimore,Maryland,United States:SPIE,2015:947004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Augmented reality technology for day/night situational awareness for the dismounted soldier">
                                        <b>[20]</b>
                                         GANS E,ROBERTS D,BENNETT M,et al.Augmented reality technology for day/night situational awareness for the dismounted soldier[C]//Proceedings Volume 9470,Display Technologies and Applications for Defense,Security,and Avionics IX;and Head- and Helmet-Mounted Displays XX.Baltimore,Maryland,United States:SPIE,2015:947004.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_21" title=" CHAM T J,CIPTADI A,TAN W C,et al.Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map[C]//Proceedings of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.San Francisco,CA:IEEE,2010:366-373." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Estimating camera pose from a single urban ground-view omnidirectional image and a2D building outline map">
                                        <b>[21]</b>
                                         CHAM T J,CIPTADI A,TAN W C,et al.Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map[C]//Proceedings of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.San Francisco,CA:IEEE,2010:366-373.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_22" title=" CHU Hang,GALLAGHER A,CHEN T.GPS refinement and camera orientation estimation from a single image and a 2D map[C]//Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Columbus,OH:IEEE,2014:171-178." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPS refinement and camera orientation estimation from a single image and a 2D map">
                                        <b>[22]</b>
                                         CHU Hang,GALLAGHER A,CHEN T.GPS refinement and camera orientation estimation from a single image and a 2D map[C]//Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Columbus,OH:IEEE,2014:171-178.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_23" title=" GALLAGHER A C.Using vanishing points to correct camera rotation in images[C]//Proceedings of the 2nd Canadian Conference on Computer and Robot Vision.Victoria,BC,Canada:IEEE,2005:460-467." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using Vanishing Points To Correct Camera Rotation In Images">
                                        <b>[23]</b>
                                         GALLAGHER A C.Using vanishing points to correct camera rotation in images[C]//Proceedings of the 2nd Canadian Conference on Computer and Robot Vision.Victoria,BC,Canada:IEEE,2005:460-467.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                     ZHANG Zhengyou.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2000,22(11):1330-1334.</a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_25" title=" LOBO J,DIAS J.Relative pose calibration between visual and inertial sensors[J].The International Journal of Robotics Research,2007,26(6):561-575." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relative pose calibration between visual and inertial sensors">
                                        <b>[25]</b>
                                         LOBO J,DIAS J.Relative pose calibration between visual and inertial sensors[J].The International Journal of Robotics Research,2007,26(6):561-575.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_26" title=" ZHAO Hengshuang,SHI Jianping,QI Xiaojuan,et al.Pyramid scene parsing network[C]//Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition.Honolulu,HI:IEEE,2017:2881-2890." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid scene parsing network">
                                        <b>[26]</b>
                                         ZHAO Hengshuang,SHI Jianping,QI Xiaojuan,et al.Pyramid scene parsing network[C]//Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition.Honolulu,HI:IEEE,2017:2881-2890.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_27" title=" VON GIOI R G,JAKUBOWICZ J,MOREL J M,et al.LSD:a fast line segment detector with a false detection control[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(4):722-732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD: A Fast Line Segment Detector with a False Detection Control">
                                        <b>[27]</b>
                                         VON GIOI R G,JAKUBOWICZ J,MOREL J M,et al.LSD:a fast line segment detector with a false detection control[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(4):722-732.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_28" title=" ZHONG Baojiang,XU Dongsheng,YANG Jiwen.Vertical corner line detection on buildings in quasi-Manhattan world[C]//Proceedings of 2013 IEEE International Conference on Image Processing.Melbourne,VIC,Australia:IEEE,2013:3064-3068." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vertical corner line detection on buildings in quasi-Manhattan world">
                                        <b>[28]</b>
                                         ZHONG Baojiang,XU Dongsheng,YANG Jiwen.Vertical corner line detection on buildings in quasi-Manhattan world[C]//Proceedings of 2013 IEEE International Conference on Image Processing.Melbourne,VIC,Australia:IEEE,2013:3064-3068.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_29" title=" TARDIF J P.Non-iterative approach for fast and accurate vanishing point detection[C]//Proceedings of the 12th International Conference on Computer Vision.Kyoto,Japan:IEEE,2009:1250-1257." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-iterative approach for fast and accurate vanishingpoint detection">
                                        <b>[29]</b>
                                         TARDIF J P.Non-iterative approach for fast and accurate vanishing point detection[C]//Proceedings of the 12th International Conference on Computer Vision.Kyoto,Japan:IEEE,2009:1250-1257.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_30" title=" LEPETIT V,MORENO-NOGUER F,FUA P.EP&lt;i&gt;n&lt;/i&gt;P:an accurate &lt;i&gt;O&lt;/i&gt;(&lt;i&gt;n&lt;/i&gt;) solution to the P&lt;i&gt;n&lt;/i&gt;P problem[J].International Journal of Computer Vision,2009,81(2):155-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003394813&amp;v=MjkxNTE5U1hxUnJ4b3hjTUg3UjdxZForWnVGeW5sVWJyUElGZz1OajdCYXJPNEh0SFBySVpCYk9vTVkzazV6QmRoNGo5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                         LEPETIT V,MORENO-NOGUER F,FUA P.EP&lt;i&gt;n&lt;/i&gt;P:an accurate &lt;i&gt;O&lt;/i&gt;(&lt;i&gt;n&lt;/i&gt;) solution to the P&lt;i&gt;n&lt;/i&gt;P problem[J].International Journal of Computer Vision,2009,81(2):155-166.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_31" title=" CHULLIAT A,MACMILLAN S,ALKEN P,et al.The US/UK world magnetic model for 2015—2020:technical report[R].Boulder,CO:NOAA,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The US/UK world magnetic model for 2015—2020:technical report">
                                        <b>[31]</b>
                                         CHULLIAT A,MACMILLAN S,ALKEN P,et al.The US/UK world magnetic model for 2015—2020:technical report[R].Boulder,CO:NOAA,2015.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_32" title=" ARTH C,PIRCHHEIM C,VENTURA J,et al.Instant outdoor localization and SLAM initialization from 2.5D maps[J].IEEE Transactions on Visualization and Computer Graphics,2015,21(11):1309-1318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Instant outdoor localization and SLAM initialization from 2.5D maps">
                                        <b>[32]</b>
                                         ARTH C,PIRCHHEIM C,VENTURA J,et al.Instant outdoor localization and SLAM initialization from 2.5D maps[J].IEEE Transactions on Visualization and Computer Graphics,2015,21(11):1309-1318.
                                    </a>
                                </li>
                                <li id="68">


                                    <a id="bibliography_33" title=" ARTH C,PIRCHHEIM C,VENTURA J,et al.Global 6DOF pose estimation from untextured 2D city models[J].Computer Science,2015,25(1):1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global 6DOF pose estimation from untextured 2D city models">
                                        <b>[33]</b>
                                         ARTH C,PIRCHHEIM C,VENTURA J,et al.Global 6DOF pose estimation from untextured 2D city models[J].Computer Science,2015,25(1):1-8.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(10),1305-1319 DOI:10.11947/j.AGCS.2019.20190007            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于2D地图的城市户外ARGIS视觉辅助地理配准技术</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%93%E6%99%A8&amp;code=31611220&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邓晨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B8%B8%E9%9B%84&amp;code=20996373&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">游雄</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%A8%81%E5%B7%8D&amp;code=20328247&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张威巍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%99%BA%E6%A2%85%E9%9C%9E&amp;code=43124174&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">智梅霞</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学地理空间信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E6%B5%8B%E7%BB%98%E9%81%A5%E6%84%9F%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学测绘遥感信息工程国家重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对当前便携式位姿传感器获取的户外6DOF绝对位姿精度通常不足,致使基于位姿传感器的户外AR地理配准精度不高,提出了一种以2D地图作为参考数据进行户外视觉辅助定位进而提高ARGIS系统地理配准精度的方法和技术框架。在位姿传感器获取初始位姿的基础上,详细阐述了基于2D地图进行图像位姿优化和辅助校正的基本原理;并通过试验验证了该方法可以对位姿传感器获取的初始位姿进行优化,进而提高户外AR地理配准精度的有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%88%B7%E5%A4%96%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">户外增强现实;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">增强现实地理信息系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%B0%E7%90%86%E9%85%8D%E5%87%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">地理配准;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%BE%85%E5%8A%A9%E9%85%8D%E5%87%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉辅助配准;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B7%E5%90%88%E9%85%8D%E5%87%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">混合配准;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8D%E5%A7%BF%E4%BC%A0%E6%84%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">位姿传感器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%85%8D%E5%87%86%E7%B2%BE%E5%BA%A6%E8%AF%84%E4%BC%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">配准精度评估;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%B0%E7%90%86%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">地理定位;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    邓晨(1990—),男,博士生,研究方向为虚拟现实与战场环境仿真、增强现实与位置服务,E-mail: 906739553@qq.com。;
                                </span>
                                <span>
                                    *游雄,E-mail: youarexiong@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-06</p>

            </div>
                    <h1>A vision-aided geo-registration method for outdoor ARGIS in urban environments based on 2D maps</h1>
                    <h2>
                    <span>DENG Chen</span>
                    <span>YOU Xiong</span>
                    <span>ZHANG Weiwei</span>
                    <span>ZHI Meixia</span>
            </h2>
                    <h2>
                    <span>Institute of Geospatial Information, Information Engineering University</span>
                    <span>State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The accuracy of the outdoor 6 DOF absolute pose obtained by the current portable pose sensor is usually insufficient in urban environments, which makes the geo-registration accuracy of outdoor AR poorly. Aiming at this issue, a method and technical framework for outdoor vision-aided localization is proposed with 2D maps to improve the outdoor ARGIS geo-registration accuracy. Based on the initial pose obtained by the pose sensor, the basic principles of image localization and pose optimization based on 2 D maps are expounded in detail. The experimental results show that the proposed method can effectively optimize the initial pose obtained by the pose sensor, and thus improve the geo-registration accuracy of outdoor AR.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=outdoor%20augmented%20reality&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">outdoor augmented reality;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ARGIS&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ARGIS;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=geo-registration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">geo-registration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=vision-aided%20registration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">vision-aided registration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hybrid%20registration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hybrid registration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=position-posture%20sensor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">position-posture sensor;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=evaluation%20of%20registration%20accuracy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">evaluation of registration accuracy;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=geo-localization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">geo-localization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    DENG Chen(1990—), male,PhD candidate,majors in virtual reality and battlefield environment simulation, augmented reality and location based service,E-mail: 906739553@qq.com;;
                                </span>
                                <span>
                                    YOU Xiong,E-mail: youarexiong@163.com;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-06</p>
                            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="70">近年来,增强现实(augmented reality,AR)技术与地理信息系统(geographic information system,GIS)结合应用开始受到广泛的关注<citation id="166" type="reference"><link href="2" rel="bibliography" /><link href="4" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>,特别是在城市环境中,通过将3D城市建筑模型、路网信息、兴趣点信息、交通信息、街道导航信息等数据逐层精确的叠加到对应的建筑物和街道上,可以在户外大范围空间中为用户提供AR平视导航与位置服务,方便用户日常出行<citation id="165" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="71">支持上述增强现实地理信息系统(augmented reality geographic information system,ARGIS)应用的一项关键技术是户外增强现实地理配准技术(geo-registration),即在真实的地理空间中,将具有三维位置的地理信息叠加在AR显示器上,使用户从不同视角观察时,虚拟信息都能够与真实世界中的对象准确套合。如图1所示,地理配准技术的突出特点是可以户外大范围空间中,将大量相关的地理信息直观呈现在用户视野中,显然,虚拟信息与现实世界对象精确套合(配准)的程度,即虚实配准的精确度,对户外AR应用的可用性有显著的影响。实现高精度的地理配准才能为用户提供精细、精确、与位置相关的信息内容服务。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 在(a)中,用户仍需要努力辨别信息与现实物体的对应关系;(b)中,多种地理内容如3D建筑轮廓、兴趣点/区域、道路、街道名称等混合在同一个虚实高精度配准的增强现实视图中,直观增强用户对周围环境的感知" src="Detail/GetImg?filename=images/CHXB201910012_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 在(a)中,用户仍需要努力辨别信息与现实物体的对应关系;(b)中,多种地理内容如3D建筑轮廓、兴趣点/区域、道路、街道名称等混合在同一个虚实高精度配准的增强现实视图中,直观增强用户对周围环境的感知  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 In (a),the user still needs to work hard to distinguish the correspondence between the information and the real objects; in (b), a variety of geographic content such as 3D building outlines, points/ areas of interest, roads, street names, etc. are mixed in the same high-accuracy augmented reality view, and intuitively enhance the user’s perception of the surrounding environment</p>

                </div>
                <div class="p1">
                    <p id="73">当前,户外ARGIS系统地理配准技术主要有3类<citation id="169" type="reference"><link href="8" rel="bibliography" /><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。第1类是基于位姿传感器的地理配准方法。目前文献<citation id="170" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</citation>等基于各类便携式位姿传感器在户外环境中进行了实时定位与地理配准研究,但试验结果表明普通便携式位姿传感器易受测量噪声和干扰因素的影响而存在测量误差,例如普通的GPS、北斗等卫星定位接收终端(如手机中集成的定位芯片)通常会有米级到10 m级的定位误差,磁力计受磁干扰影响通常也存在5°～10°的航向角偏差,因而直接使用移动便携式位姿传感器测量的6自由度(6 degree of freedom,6DOF)位姿往往难以实现较高精度的地理配准,仍需要通过人工交互方式<citation id="167" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>改善配准精度。第2类方法是基于预先准备的具有地理坐标参考的点云数据,根据P<i>n</i>P的基本原理进行视觉地理定位进而实现地理配准<citation id="171" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。这种方法通常可以达到较高的配准精度,但是其必备的前提是必须要在试验区域预先采集大量的参考图像并重建大量的点云数据库,而后及时更新。根据文献<citation id="168" type="reference">[<a class="sup">14</a>]</citation>的分析,即使在较小场景中这类方法所需的视觉参考库也将是非常巨大的,例如,街景图像采集设备在100 m的距离上平均采集的图像数据约为3～5 GB,重建生成点云参考数据也通常也达数GB以上;在户外较大的环境中,移动终端本身往往难以存储所有的点云参考数据,因而必须借助服务器和网络给移动终端实时传送视觉参考数据<citation id="172" type="reference"><link href="30" rel="bibliography" /><link href="34" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>,这很容易给原本复杂、耗时、耗能的视觉计算带来更大的延迟和消耗。第3类方法是在上述两种方法基础上实现视觉-位姿传感器混合定位与配准的方法,但目前这类方法研究较多的是将IMU与视觉SLAM方法结合进行混合位姿跟踪<citation id="173" type="reference"><link href="36" rel="bibliography" /><link href="38" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>。其特点是在临时设置的局部坐标系中实时、准确的跟踪用户(或相机)的相对位姿,较少顾及用户(或相机)相对于地理坐标系的初始绝对6DOF位姿。近年来,文献<citation id="174" type="reference">[<a class="sup">19</a>,<a class="sup">20</a>]</citation>提出了一种在野外条件下基于DEM数据进行视觉定位进而校正位姿传感器初始定位结果的思路,特别的是该方法能通过视觉定向自动校正位姿传感器水平航向角的测量误差,辅助提高基于位姿传感器的AR地理配准绝对精度。受这一工作的启发,本文试图建立一种在城市环境中借助广泛易得的2D地图数据为参考进行视觉定位,并自动校正位姿传感器的初始6DOF定位结果,进而提高户外AR地理配准精度的技术框架与方法。</p>
                </div>
                <div class="p1">
                    <p id="74">当前,城市环境中2D地图可以通过公开的地图数据源(如通过OpenStreetMap)广泛获取,经过适当的地理定位校正后,即可具备较高精度的地理参考。但是地图只包括场景顶视图的轮廓结构信息,没有场景的表面纹理信息,必须要通过图像处理方法提取图像中的特定特征(如线特征)与2D地图进行对应才能求解图像位姿。文献<citation id="175" type="reference">[<a class="sup">21</a>]</citation>率先提出了一种从图像中提取建筑分段线性轮廓结构与2D地图匹配求解图像位姿的方法,但该方法定位精度不高,且需要使用宽视野的全景图像为输入;文献<citation id="176" type="reference">[<a class="sup">22</a>]</citation>提出了一种使用图像中建筑垂直轮廓线与2D地图中建筑角点对应求解单张图像位姿的方法,该方法可以对普通视野图像进行处理并得到较高的定位精度,但是需要通过人工辅助以准确提取建筑垂直轮廓线,尚不能完全自动实现。在参考借鉴文献<citation id="177" type="reference">[<a class="sup">21</a>,<a class="sup">22</a>]</citation>工作的基础上,本文借助当前性能较好的图像处理方法,实现了一种自动提取图像上同一建筑的3条可见垂直边缘轮廓线,并与2D地图上相应建筑的3个角点对应,求解相机在平面上的2D位置和水平航向角yaw(本文中称为2D平面位姿)的方法;进一步与重力加速度计测得的俯仰角pitch和滚动角roll,以及预先设定的高程值<i>h</i>(或者GNSS测量的高程值)组合后,可以得到优化的相机6DOF绝对位姿(本文中称为3D位姿),用于户外AR地理配准。如图2所示,是本文研究的输入与输出:视觉辅助位姿优化及相应户外AR地理配准结果对比效果图。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 视觉辅助位姿优化与地理配准精度改进效果对比" src="Detail/GetImg?filename=images/CHXB201910012_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 视觉辅助位姿优化与地理配准精度改进效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Effects of visual aided pose optimization and geo-registration accuracy improvement</p>

                </div>
                <div class="p1">
                    <p id="76">使用局部区域中的2D地图,特别是只使用2D地图中的建筑角点信息进行视觉定位时,所需参考的数据量将到达非常小的量级(如KB级),因为在100 m×100 m区域范围内定位参考数据一般不超过十几KB,并且容易准备和获得;这非常吻合本文研究的最终目标,即实现一种“轻量”的视觉定位方法对传感器获取的初始粗略6DOF绝对地理位姿进行“离散”的校正和优化,以期最终提高户外AR“连续”地理配准的精度。下面具体对基于2D地图的视觉辅助位姿优化方法进行介绍。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag">1 基于2D地图的位姿优化校正方法</h3>
                <div class="p1">
                    <p id="78">如图3所示,是基于2D地图数据对图像进行定位进而对位姿传感器获取的初始6DOF位姿进行优化的整体框架,其中,位姿传感器获取的粗略6DOF位姿是视觉定位的初始输入,而视觉定位的结果反过来可以对传感器的测量结果进行校正。</p>
                </div>
                <div class="p1">
                    <p id="79">2D地图中的建筑底图角点通常对应于现实场景中建筑不同立面的垂直边缘交线,通过将建筑底图中的角点与图像中提取的垂直边缘线对应,可以求解出当前图像的平面2D位置和方向。但是在实际过程中,一方面从图像多而杂的线特征中准确、自动地识别出建筑垂直轮廓线就是相当具有挑战性的工作<citation id="178" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>;另一方面,因为2D地图并没有场景表面描述信息,即使准确识别出建筑垂直轮廓线,也难以直接通过特征描述符匹配建立垂直边缘轮廓线与地图建筑角点的对应关系。因此,本文一方面借助当前较好的图像处理方法,实现自动识别图像中的建筑垂直轮廓线;另一方面,采用先假设后验证的方式,在局部地图范围内先假设垂直边缘轮廓线与地图建筑角点的对应关系,建立起图像所有可能的2D位姿假设集合,然后根据传感器初始的位姿输入筛选出可能的最优解,最终实现对初始位姿的优化校正。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于2D地图的视觉辅助6DOF绝对位姿优化整体流程" src="Detail/GetImg?filename=images/CHXB201910012_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于2D地图的视觉辅助6DOF绝对位姿优化整体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Overall framework of vision-assisted 6DOF absolute pose optimization method based on 2D map</p>

                </div>
                <div class="p1">
                    <p id="81">因此,本文的位姿优化方法实际上可以分为以下3个步骤:①对输入图像进行校正和特征提取;②根据假设的图像特征与2D地图建筑角点对应关系,计算可能的2D位姿假设集合;③根据位姿传感器的初始位姿,筛选最优解。下面具体对每一个步骤进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">1.1 图像校正与特征提取</h4>
                <h4 class="anchor-tag" id="83" name="83">1.1.1 图像滚动倾角校正</h4>
                <div class="p1">
                    <p id="84">因为在实际观察时,相机拍摄容易存在一定的滚动倾斜,会对建筑垂直轮廓线的准确识别带来影响,因此需要校正。文献<citation id="179" type="reference">[<a class="sup">22</a>,<a class="sup">23</a>]</citation>提出了一种基于图像特征自动校正滚动倾角的方法,一方面这种方法相对复杂,且能够自动校正的倾角阈值范围较小;另一方面,在相对静态条件下,重力加速度计测得的俯仰角和滚动角通常具有较高的精度,因此本文使用重力加速度测得的滚动角对图像进行校正,使得从校正后的图像中提取的垂直直线能够汇聚交于以图像主点为原点的<i>y</i>轴上,水平消隐点的连线与图像<i>x</i>轴平行。后续的试验表明,在相对静态条件下,重力加速度计测量的倾角具有较高的精度,可以基本满足本文需求。</p>
                </div>
                <div class="p1">
                    <p id="85">此外,摄像机的主点、焦距等内参数可以通过离线校正<citation id="180" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>得到,相机与位姿传感器的相对位姿可以通过文献<citation id="181" type="reference">[<a class="sup">25</a>]</citation>中的方法进行离线校正。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">1.1.2 语义分割及建筑区域提取</h4>
                <div class="p1">
                    <p id="87">经过滚动校正后,图像的基本姿态得到“归正”,但是图像中的户外现实场景通常都是包含各类遮挡物的嘈杂场景,为了减小干扰和便于提取图像中的建筑垂直轮廓线,需要先对图像进行过滤,提取出图像中的建筑区域进行后续处理。考虑到户外场景通常光照明暗变化显著、或遮挡严重,为了应对户外复杂场景,本文使用目前性能较好的开源深度语义分割网络PSPNet<citation id="182" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>对户外场景图像进行语义分割,以语义分割后的场景类别图像为掩码,对原始输入图像进行过滤,可以快速高效的提取出图像中的建筑区域,如图4(a)、(b)、(c)所示。</p>
                </div>
                <div class="p1">
                    <p id="88">另外,考虑到现实场景中通常会有树木、车辆等遮挡物对建筑区域进行遮挡,上述提取的建筑区域往往会存在“空洞”而导致建筑区域提取不全。为了弥补上述“空洞”并且兼顾算法效率,根据建筑区域在图像平面上通常沿图像<i>y</i>轴正向连续延伸的特点,对上述语义分割后的图像掩码进行补充,然后再对原始输入图像进行过滤,可以提取出相对完整、且独立的建筑区域,如图4(d)所示。这为本文后续自动提取建筑角点边缘线特征打下了良好基础。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">1.1.3 线特征提取及消隐点估计</h4>
                <div class="p1">
                    <p id="90">为了提取建筑的角点边缘线,首先需要提取建筑区域中的所有特征线,并计算水平和垂直消隐点。本文使用相对成熟的LSD直线段提取算法<citation id="183" type="reference"><link href="56" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>,对上述提取的建筑区域进行线特征提取,如图5(a)所示(只在未受遮挡的建筑区域中提取线特征)。考虑到建筑是一类特殊的人工建筑,其立面通常铅垂于水平面(或地面),因此可以假设认为建筑立面中提取的大多直线段为现实场景中水平或垂直直线<citation id="184" type="reference"><link href="58" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="91">为了计算水平和垂直消隐点,使用J-Linkage模型<citation id="185" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>对上述提取的直线段进行分组,而后通过随机选取同一组直线中的两条直线计算可能的备选消隐点,循环迭代多次,取其中包含的长直线最多的消隐点(即备选消隐点到同组每条直线距离小于一定阈值的所有直线的长度和最大)为最优消隐点,求解得到每一组直线较为可靠的消隐点位置。根据消隐点在图像平面上的坐标,可以区分得到垂直直线组的消隐点和水平直线组的消隐点,通常垂直消隐点只有一个,在图像上方或下方(本案例垂直消隐点在图像上方);而水平消隐点会在图像左右两边各有一个,分别对应不同建筑立面上提取的水平直线组。如图5(b)所示,不同颜色的直线组分别对应不同的消隐点,其中,绿色对应垂直消隐点,红色对应图像右边水平消隐点,蓝色对应图像左边水平消隐点。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">1.1.4 左右及中间垂直角点边缘线识别</h4>
                <div class="p1">
                    <p id="93">在图像建筑立面上提取特征线并估计垂直和水平消隐点后,下一步是要从提取的特征线中识别建筑垂直角点边缘线。为了能够相对准确地识别建筑角点边缘线,本文假设被分析的图像中建筑区域位于图像中间,且能够观察到建筑的3条垂直角点边缘线,其中左、右角点边缘线分别位于图像主点的左右两侧,中间角点边缘位于左右边缘之间。显然,这一假设是合理且具有可操作性的,因为在实际拍摄图像时,只需要用户有意识地将建筑区域大致放于图像中部区域即可。</p>
                </div>
                <div class="p1">
                    <p id="94">从视觉上分析,建筑垂直角点边缘通常是建筑两个不同立面的交线,并且可以与2D地图上的建筑底图轮廓角点一一对应;从图像处理的角度分析,建筑左、右垂直角点边缘线附近水平方向的颜色梯度通常会有较大的突变,如图4(d)所示,在经过语义分析处理的图像上,背景区域和建筑区域的交界处会有显著的颜色梯度变化,且建筑区域的垂直长度也会发生较大的突变(这是需要对受遮挡的建筑区域补全的原因)。因此,根据图像每一列像素的颜色梯度与该列建筑区域长度梯度之和,求取其最大值,可以作为判定建筑左右角点边缘位置的参考依据。本文使用如下公式,计算图像上每一列作为左、右角点边缘的评估值,有</p>
                </div>
                <div class="p1">
                    <p id="95">FinalScores(<i>j</i>)=ColorScores(<i>j</i>)+MaskScores(<i>j</i>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="96">式中,<i>j</i>代表的是经过滚动倾斜校正后图像的第<i>j</i>列,需要注意的是,列<i>j</i>上对应的所有图像像素并非是图像上第<i>j</i>列沿<i>y</i>轴方向上的所有像素,因为在没有经过俯仰倾斜校正前,建筑立面中提取的垂直直线与图像<i>y</i>轴是不平行的,图像<i>y</i>轴方向的列像素并不能表达现实世界中的垂直线特征;因此,列<i>j</i>上对应的像素,是垂直消隐点(本示例在图像上方)与图像第<i>j</i>列最底部像素的连线与图像平面的所有交点,这条连线与建筑立面中提取的垂直直线是平行的。FinalScores(<i>j</i>)是第<i>j</i>列作为角点垂直边缘的最终评估值(即评估值加和),ColorScores(<i>j</i>)是第<i>j</i>列所有在建筑立面上的像素RGB三通道水平方向梯度之和,MaskScores(<i>j</i>)是第<i>j</i>列所有在建筑立面上的像素个数的梯度值(即相邻列像素个数差值)。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 图像语义分割及建筑区域提取" src="Detail/GetImg?filename=images/CHXB201910012_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 图像语义分割及建筑区域提取  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Image semantic segmentation and building area extraction</p>

                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 建筑立面线特征提取及消隐点估计" src="Detail/GetImg?filename=images/CHXB201910012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 建筑立面线特征提取及消隐点估计  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Line segments extraction on building facades and vanishing points estimation</p>

                </div>
                <div class="p1">
                    <p id="99">如图6所示,当计算出图像的FinalScores(<i>j</i>)后,根据图像中心位置(主点横坐标)将FinalScores(<i>j</i>)分为LeftFinalS(<i>j</i>)和RightFinalS(<i>j</i>)两部分,分别求取两部分的最大值对应的列坐标<i>j</i>,分别作为图中建筑的左右角点垂直边缘线的边界参考。</p>
                </div>
                <div class="p1">
                    <p id="100">依据上述得到的左、右边界范围,建筑左、右角点垂直边缘线的确切位置,可以在建筑立面上提取的垂直直线中,分别选取最接近左、右边界的垂直直线作为最终的左右角点垂直边缘线,如图7(a)所示。</p>
                </div>
                <div class="p1">
                    <p id="101">一旦左、右角点垂直边缘确定后,中间角点边缘的位置也可以确定,因为中间角点边缘通常位于左右角点边缘中间,且具有如下特征:①中间角点边缘位于建筑不同立面水平直线的交点上(如图7(a)所示);②该边缘线位置通常也是建筑区域垂直长度(列像素个数)极大值点处(如图7(b)所示)。本文综合上述两个条件,在保证位于两组水平直线交点上的前提下,取建筑区域垂直长度最长的列为中间角点垂直边缘的最佳位置,如图7(c)所示。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 建筑左、右角点垂直边缘边界估计" src="Detail/GetImg?filename=images/CHXB201910012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 建筑左、右角点垂直边缘边界估计  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Estimation of the boundary of vertical edges corresponding to the left and right corner of the building</p>

                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 建筑左、中、右角点垂直边缘识别提取" src="Detail/GetImg?filename=images/CHXB201910012_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 建筑左、中、右角点垂直边缘识别提取  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Recognition and extraction of the left, middle and right corner vertical edge of building</p>

                </div>
                <h4 class="anchor-tag" id="104" name="104">1.1.5 角点边缘线水平分布特征</h4>
                <div class="p1">
                    <p id="105">事实上,如图7(a)中所示,建筑两个不同立面上的两组水平直线的交点为可以基本确定中间角点边缘的位置,而同组直线间的交点则分别是图像的左、右两个水平消隐点,连接两个水平消隐点,则得到水平消隐线,即过相机光心的水平面在图像平面上的成像线,如图8(a)所示的水平线,3条垂直边缘线的分布特征可以通过水平消隐线与3条垂直边缘线交点的横坐标进行表征。根据2D-1D透视投影原理,如图8(b)所示,图像上3个交点可以分别和2D地图上相应建筑轮廓角点进行对应,据此对应关系可以进一步求解相机中心的位置(<i>x</i>,<i>y</i>)和主轴水平方向yaw等2D位姿。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 角点垂直边缘线水平分布与对应关系" src="Detail/GetImg?filename=images/CHXB201910012_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 角点垂直边缘线水平分布与对应关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Horizontal distribution and correspondence of vertical edge of building corner points</p>

                </div>
                <div class="p1">
                    <p id="107">需要注意的是,图8(a)中的图像拍摄时存在俯仰倾斜,3个交点的横坐标(<i>u</i><sub>左</sub>,<i>u</i><sub>中</sub>,<i>u</i><sub>右</sub>)会随图像的俯仰角度变化而变化,因而,图8(b)中所示的主轴方向是主轴在水平面上的投影方向,相机光心(相机原点)沿该方向与图像平面相交的水平直线长度为<i>f</i><sub><i>x</i></sub>/cos(<i>φ</i>),这是经过俯仰倾斜角度校正后的相机横轴焦距,其中<i>f</i><sub><i>x</i></sub>是相机横轴方向的原始焦距,可以通过离线校正得到;<i>φ</i>是图像的俯仰倾斜角,可以使用重力加速度计等传感器进行测量。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">1.2 计算位姿假设集合</h4>
                <div class="p1">
                    <p id="109">如图8(b)所示,根据中心透视投影物点、像点和相机光心3点共线的规律,过相机光心<i>O</i><sub><i>c</i></sub>及上述图像上交点<i>P</i><sub><i>i</i></sub>的反向投影直线必然会与现实场景中的建筑垂直边缘相交;如果从顶部向下观看,在相机光心<i>O</i><sub><i>c</i></sub>所在的水平面上,建筑的每一个轮廓角点<i>C</i><sub><i>i</i></sub>会在过相机中心<i>O</i><sub><i>c</i></sub>与图像中交点<i>P</i><sub><i>i</i></sub>的反向投影直线上。事实上,只考虑相机光心<i>O</i><sub><i>c</i></sub>所在的水平面时,这是将2D平面空间投影变换到1D线性空间中,是退化的2D-1D透视投影(相对于常见的3D-2D透视投影而言)。</p>
                </div>
                <div class="p1">
                    <p id="110">相机在水平面上的2D位姿包含3个参数,即相机中心的平面坐标位置<i>O</i><sub><i>c</i></sub>-(<i>x</i>,<i>y</i>),和相机坐标系相对于平面坐标系(地图平面坐标系)的旋转角度yaw-<i>θ</i>(也即相机主轴<i>O</i><sub><i>c</i></sub><i>O</i><sub><i>o</i></sub>相对于平面坐标系<i>Y</i>轴的旋转角度)。在有3对图像交点与建筑角点对应的条件下可以求解相机2D平面位姿的3个参数(<i>x</i>,<i>y</i>,<i>θ</i>)。</p>
                </div>
                <div class="p1">
                    <p id="111">现在,定义建筑角点及其在世界平面坐标系上的坐标为<i>C</i><sub>1</sub>-(<i>c</i><sub><i>x</i></sub><sub>1</sub>,<i>c</i><sub><i>y</i></sub><sub>1</sub>),<i>C</i><sub>2</sub>-(<i>c</i><sub><i>x</i></sub><sub>2</sub>,<i>c</i><sub><i>y</i></sub><sub>2</sub>),<i>C</i><sub>3</sub>-(<i>c</i><sub><i>x</i></sub><sub>3</sub>,<i>c</i><sub><i>y</i></sub><sub>3</sub>),相应的图像像点及其列(横)坐标为<i>P</i><sub>1</sub>-<i>u</i><sub>左</sub>,<i>P</i><sub>2</sub>-<i>u</i><sub>中</sub>,<i>P</i><sub>3</sub>-<i>u</i><sub>右</sub>,即</p>
                </div>
                <div class="p1">
                    <p id="112"><i>u</i><sub>左</sub>↔(<i>c</i><sub><i>x</i></sub><sub>1</sub>,<i>c</i><sub><i>y</i></sub><sub>1</sub>)<sup>T</sup>,<i>u</i><sub>中</sub>↔(<i>c</i><sub><i>x</i></sub><sub>2</sub>,<i>c</i><sub><i>y</i></sub><sub>2</sub>)<sup>T</sup>,<i>u</i><sub>右</sub>↔(<i>c</i><sub><i>x</i></sub><sub>3</sub>,<i>c</i><sub><i>y</i></sub><sub>3</sub>)<sup>T</sup></p>
                </div>
                <div class="p1">
                    <p id="113">根据中心透视投影的规律,经过相机坐标变换和投影变换即可将现实世界中的物点变换为像平面中的像点,对应的变换公式为</p>
                </div>
                <div class="p1">
                    <p id="114"><i><b>K</b></i><b>(</b><i><b>R</b></i><sub><i>θ</i></sub><i>C</i><sub><i>i</i></sub>+<i><b>T</b></i>))      (2)</p>
                </div>
                <div class="p1">
                    <p id="115">式中</p>
                </div>
                <div class="area_img" id="116">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910012_11600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="118">式中,<i>k</i><sub><i>u</i></sub>=<i>f</i><sub><i>x</i></sub>/cos(<i>φ</i>),是经过俯仰倾斜校正的横轴相机焦距,<i>u</i><sub>0</sub>是图像主点的横坐标(可以通过离线校正得到);<i><b>R</b></i><sub><i>θ</i></sub>是由世界平面坐标到相机平面坐标的2D旋转变换矩阵,<i><b>T</b></i>是世界坐标系原点在相机平面坐标系中的2D位置。相机原点在世界坐标系中的位置可以通过式(4)计算,即</p>
                </div>
                <div class="p1">
                    <p id="119">(<i>x</i>,<i>y</i>)<sup>T</sup>=-<i><b>R</b></i><sup>T</sup><sub><i>θ</i></sub><i><b>T</b></i>=-<i><b>R</b></i><sup>T</sup><sub><i>θ</i></sub>(<i>T</i><sub><i>x</i></sub>,<i>T</i><sub><i>y</i></sub>)<sup>T</sup>      (4)</p>
                </div>
                <div class="p1">
                    <p id="120">事实上,式(2)中求解得到的是像点横坐标<i>u</i><sub><i>i</i></sub>的齐次坐标形式,根据齐次坐标与原始坐标的转换关系,可以求得</p>
                </div>
                <div class="p1">
                    <p id="121"><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>θ</mi></msub><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">)</mo><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi mathvariant="bold-italic">Κ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>θ</mi></msub><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false">)</mo><msub><mrow></mrow><mn>1</mn></msub></mrow></mfrac></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="122">式中,(*)<sub>0</sub>、(*)<sub>1</sub>分别表示向量(*)的第1维分量和第2维分量,而<i>u</i><sub><i>i</i></sub>∈(<i>u</i><sub>左</sub>,<i>u</i><sub>中</sub>,<i>u</i><sub>右</sub>)。</p>
                </div>
                <div class="p1">
                    <p id="123">根据式(5),对每一对<i>u</i><sub><i>i</i></sub>↔(<i>c</i><sub><i>xi</i></sub>,<i>c</i><sub><i>yi</i></sub>)<sup>T</sup>,事实上可以得到一个关于(<i>T</i><sub><i>x</i></sub>,<i>T</i><sub><i>y</i></sub>,<i>θ</i>)的三元一次方程,当有3对对应点时,可以得到3个方程组。尽管这些方程是非线性的(因为有三角函数参数),但是通过方程组之间的线性变换(如高斯-约丹(Gauss-Jordan)消元法),可以将3个方程组化简为如下形式</p>
                </div>
                <div class="area_img" id="124">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910012_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="126">对于式(6)中的最后一行,有:<i>a</i>sin <i>θ</i>+<i>b</i>cos <i>θ</i>=<i>d</i>,结合cos<sup>2</sup><i>θ</i>+sin<sup>2</sup><i>θ</i>=1,即可以求得<i>θ</i>值,代入式(6)的前两行,即可以求得<i>T</i><sub><i>x</i></sub>、<i>T</i><sub><i>y</i></sub>;但通常<i>θ</i>会有两个解,对应可以求解得到两组<i>T</i><sub><i>x</i></sub>、<i>T</i><sub><i>y</i></sub>,分别对应两组相机原点位置(<i>x</i>,<i>y</i>)。这两组解互为对称,相机方向恰好相反,其中在2D世界坐标系中距离建筑中间角点距离更近的(<i>x</i>,<i>y</i>,<i>θ</i>)是本文要求的相机2D位姿,而另一组位姿可以舍弃。</p>
                </div>
                <div class="p1">
                    <p id="127">综合上述,在已知3组2D地图建筑角点与图像像点对应的情况下,可以求解图像2D位姿,但实际情况是2D地图没有现实世界的表面特征信息,难以通过特征描述符计算实现准确的2D地图建筑角点与图像垂直轮廓线对应。因此,如图9所示,本文以初始定位位置(如GNSS定位位置)为参考,提取附近一定范围的2D地图(本文选取100 m×100 m范围)并记录范围内的有所建筑角点;对每一栋建筑,依次取其中的3个角点作为图像中3条垂直边缘线的对应点,根据前述方法,计算假设的相机2D位姿;通过循环迭代,遍历选取范围内的所有建筑角点,可以求解得到一个相机2D位姿的假设集合(图9中红点表示相机中心平面位置,蓝线表示相机平面朝向)。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">1.3 筛选最优位姿</h4>
                <div class="p1">
                    <p id="129">通过前述方法求解得到的2D位姿假设集合,事实上是包含(<i>x</i>,<i>y</i>,<i>θ</i>)3个元素的数组,为了判断哪一组数据是相机真实位姿或者最优解,需要参考位姿传感器获取的初始位姿。通常而言,在大多数普通的环境中,磁力计测量的水平航向角度误差为5°～10°,普通GNSS接收终端的水平定位精度会在米级到10米级左右。本文首先通过初始航向角角度对位姿假设集合进行过滤,只保留与磁力计测量的初始航向角绝对差值不超过30°的位姿数据组,这样可以过滤掉大部分无关的假设数据;然后,在剩下的假设集合中,逐个计算假设集合中的(<i>x</i>,<i>y</i>)与传感器初始位置(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)的两点距离,选取距离初始位置距离最近的位姿数据组作为相机2D位姿的最优解。如图10所示,短红线表示的是与初始位姿方向相近的假设位姿,长红线是得到的最优位姿。</p>
                </div>
                <div class="p1">
                    <p id="130">一旦得到优化后的2D位姿后,结合加速度计等位姿传感器测量的滚动倾角<i>ψ</i>和俯仰倾角<i>φ</i>,以及设定的高程值<i>h</i>(如本文设定相机高度距离地面1.6 m),组合得到优化的6DOF绝对位姿(<i>x</i>,<i>y</i>,<i>h</i>,<i>φ</i>,<i>ψ</i>,<i>θ</i>)(本文称为3D位姿),用于户外AR地理配准。</p>
                </div>
                <h3 id="131" name="131" class="anchor-tag">2 试验与结果分析</h3>
                <h4 class="anchor-tag" id="132" name="132">2.1 试验数据准备</h4>
                <div class="p1">
                    <p id="133">上文中提出了一种使用位姿传感器获取位姿数据为初始输入,然后基于2D地图数据进行视觉辅助位姿校正,最终用以提高户外AR地理配准精度的方法框架。本节中进行试验,用于验证:①基于2D地图的视觉定位方法可以对位姿传感器获取的初始2D位姿进行优化校正;②使用优化后组合得到的6DOF位姿进行地理配准可以提高原始配准精度,从而证明本文所提方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="134">试验使用的2D地图是通过ArcGIS软件从倾斜摄影三维重建生成的正射图中采集、编辑得到2D矢量地图,为了保证地图地理参考的准确性,三维重建时在地面布设了差分控制点。事实上,试验使用的2D地图也可以来源于OpenStreetMap等网络公开地图数据源,即使其初始地理定位有所偏移,但经过适当的整体地理位置纠正,仍然可以得到比较精确的地理位置参考。试验使用的位姿传感器和图像采集设备是小米5X手机自带的集成传感器和摄像机,其中集成的加速度计是BMI120,磁力计是AK09918。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">2.2 试验结果分析</h4>
                <h4 class="anchor-tag" id="136" name="136">2.2.1 位姿精度优化分析</h4>
                <div class="p1">
                    <p id="137">本次试验共在校区范围内采集了10组不同场景的数据(包括图像、传感器位姿数据等)以进行位姿精度优化和地理配准精度优化分析。因为在语义分割阶段使用开源深度网络PSPNet对图像进行语义分割,即使对于遮挡较为严重、明暗变化显著的复杂场景,也能较为准确地提取图像中的建筑区域,这为准确识别图像中的边缘线特征带来了便利。对于测试的10组场景,或光照明暗变化显著、或遮挡严重,本文方法都能自动、准确地提取其中角点垂直边缘特征,而文献<citation id="186" type="reference">[<a class="sup">22</a>]</citation>的方法难以处理这类复杂场景,必须要借助人工干预才能实现特征线识别与提取,这是本文工作的一项重要改进和发展。对于1280×720分辨率大小的输入图像,在MSI笔记本电脑上(CPU为intel core i7-6700HQ 2.6 GHz,内存8 GB),在Matlab环境中算法对每张图像处理的整体时间为3～4 s(不计PSPNet对图像进行语义分割处理的时间),为算法后续在其他平台中的集成应用奠定了基础。如图11所示,是本文试验测试的10组场景案例,是在保持AR用户户外自然观察的前提下在“离散”时刻选取的,其基本要求是满足本文算法的基本假设,即可以看到目标建筑的3条角点边缘、且左右边缘线大致位于图像主点的两侧等。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 相机初始位姿及2D位姿假设集合" src="Detail/GetImg?filename=images/CHXB201910012_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 相机初始位姿及2D位姿假设集合  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Initial 2D camera pose and the hypothetical 2D pose set</p>

                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 筛选得到相机2D位姿最优解" src="Detail/GetImg?filename=images/CHXB201910012_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 筛选得到相机2D位姿最优解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Screening to get the optimal 2D pose solution</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 本文的测试场景案例:纵向上每3幅图像对应一组测试场景,其中横向第1层为从原始图像中自动提取得到的3条角点边缘线(经滚动倾角校正);中间层为原始图像语义分割结果;第3层为传感器初始2D位姿和优化的2D位姿" src="Detail/GetImg?filename=images/CHXB201910012_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 本文的测试场景案例:纵向上每3幅图像对应一组测试场景,其中横向第1层为从原始图像中自动提取得到的3条角点边缘线(经滚动倾角校正);中间层为原始图像语义分割结果;第3层为传感器初始2D位姿和优化的2D位姿  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 The test scenarios of this experiment: each of the three images in the vertical direction corresponds to a set of test scenes, where in the first horizontal layer is the three corner edges automatically extracted from the original images (corrected by rolling inclination); the middle horizontal layer is the semantic segmentation results of original images; and the third horizontal layer is the sensor’s initial 2D pose and optimized 2D pose</p>

                </div>
                <div class="p1">
                    <p id="141">为了能够对比分析传感器获取的位姿与图像定位方法校正的位姿精度,本文使用EP<i>n</i>P<citation id="187" type="reference"><link href="62" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>定位得到的6DOF位姿作为每组场景的参考真实值,分别计算传感器获取的6DOF位姿和视觉定位得到的2D平面位姿与参考真值的误差,计算它们在各自由度上的误差最大值、最小值、平均值作为精度评价指标,结果如表1、表2所示。</p>
                </div>
                <div class="area_img" id="142">
                                            <p class="img_tit">
                                                <b>表1 以EP</b><i><b>n</b></i><b>P定位3DOF位置为参考,本文方法位置精度优化比较</b>
                                                    <br />
                                                <b>Tab.1 The 3DOF position accuracy of our localization methods compared with the position acquired by EP</b><i><b>n</b></i><b>P localization methods</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910012_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">m</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 以EPnP定位3DOF位置为参考,本文方法位置精度优化比较" src="Detail/GetImg?filename=images/CHXB201910012_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="143">
                                            <p class="img_tit">
                                                <b>表2 以EP</b><i><b>n</b></i><b>P定位3DOF姿态为参考,本文方法姿态精度优化比较</b>
                                                    <br />
                                                <b>Tab.2 The 3DOF attitude accuracy of our methods compared with the attitude acquired by image-based localization</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910012_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">（°）</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 以EPnP定位3DOF姿态为参考,本文方法姿态精度优化比较" src="Detail/GetImg?filename=images/CHXB201910012_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="144">在位置精度方面,从表1中可以看出,在普通的城市建筑环境中,直接使用普通的卫星定位信号接收终端(如手机GNSS芯片)获取的位置,其水平单方向上的平均误差接近3～4 m,这可能是因为近年来卫星定位系统快速发展(如卫星数量增多,芯片性能增强,定位信号在电离层、对流层传输误差模型得到改进等原因),卫星定位的精度水平整体有所提升;但显然,这一精度不能满足户外AR高精度地理配准的需求(相关分析参见下文地理配准精度分析)。相比较而言,在上述初始位置的基础上,基于2D地图的视觉定位方法可以对水平初始位置进行校正,如表1中所示,经过视觉方法校正后,水平定位精度显著提升,平均误差在1 m以内,且最大的误差值显著较低,其中每一组场景的水平位置精度改进对比如图12所示。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 每一组场景水平位置精度改进对比" src="Detail/GetImg?filename=images/CHXB201910012_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 每一组场景水平位置精度改进对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.12 Improvement of horizontal position accuracy of each set of scenes</p>

                </div>
                <div class="p1">
                    <p id="146">在姿态精度方面,如表2所示,使用普通手机自带的加速度计(BMI120)和磁力计(AK09918),在相对静态条件下获取的绝对姿态,在某些自由度上也具有较高的精度,如使用重力加速计测得水平俯仰角和滚动角误差大多不超过2°,平均值误差在1°左右,这验证了本文在相对静态条件下,使用重力加速度计测得俯仰角和滚动角进行图像滚动倾角校正的可行性。</p>
                </div>
                <div class="p1">
                    <p id="147">但是,通过磁力计等传感器直接测得的航向角误差通常都相对较大,在没有显著磁干扰的环境中,平均误差4°～5°,且测量结果不稳定,易出现较大误差值;如表2中所示,与之相比,使用基于2D地图的视觉定位方法可以显著提高航向角的测量精度,平均误差降低到接近0.5°,且最大值误差也显著降低,达到2°以内,其中每一组场景的水平航向角精度改进对比如图13所示。</p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 每一组场景水平偏航角精度改进对比" src="Detail/GetImg?filename=images/CHXB201910012_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 每一组场景水平偏航角精度改进对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.13 Improvement of yaw angle accuracy for each set of scenes</p>

                </div>
                <div class="p1">
                    <p id="149">在试验过程中,笔者发现使用加速度计结合陀螺仪等传感器,即使在运动状态下也可以获取相对准确的俯仰和滚动姿态值,但是直接使用磁力计测量航向角的结果极容易受环境中的磁性材质的干扰而不准确、甚至不可用,而这种磁干扰在现实世界中是普遍存在的。例如接近普通笔记本电脑内置的扩音器或接近停在路旁的汽车时,磁力计会迅速偏转导致测量误差明显增大,这使得使用磁力计在户外测量偏航角的稳定性、准确性难以保证,因而必须要借助其他方法进行“间断性”的校正。这也正是本文提出使用视觉定位方法辅助校正位姿传感器位姿(特别是航向角)的出发点和着眼点,而本文的实验结果验证了这一思路和方法的可行性和有效性。</p>
                </div>
                <div class="p1">
                    <p id="150">需要说明的是,本试验中得到的偏航角误差是经过磁偏角校正后的误差,试验中使用的磁偏角大小是-5.033 3°,即磁北方向相对当地真北西偏5.033 3°,这是根据当地经纬度和测试日期时间使用2015—2020地球磁场模型<citation id="188" type="reference"><link href="64" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>计算得到的结果。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151">2.2.2 地理配准精度优化分析</h4>
                <div class="p1">
                    <p id="152">本文研究的着眼点和最终目的是提高户外AR地理配准的精度,前文中已经分析了基于2D地图的视觉定位方法对位姿传感器获取的初始位姿的优化校正作用,现在需要进一步分析这种位姿优化对最终地理配准精度优化的影响。</p>
                </div>
                <div class="p1">
                    <p id="153">本文以虚实对象在合成图像上的视角夹角作为评价虚实对象配准精度的量化指标,如图14所示,虚实对象视线夹角<i>λ</i>越大, 则配准误差越大;夹角<i>λ</i>越小,则虚实对象叠加越接近。因为角度可以和弧度互换,1毫弧度(mard),约等于0.057°,约等于横向0.1 m与纵向100 m形成的夹角值;10 mard约等于0.57°,即约等于横向1 m与纵向100 m形成的夹角值。考虑到使用毫弧度(mard)作为评估虚实配准精度的单位,可以方便地将虚实视角误差与虚实配准的现实意义联系起来,因此本文同时给出虚实视线夹角的角度值和毫弧度值。</p>
                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 虚实配准量化精度求解原理" src="Detail/GetImg?filename=images/CHXB201910012_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图14 虚实配准量化精度求解原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.14 Schematic diagram for quantitative registration accuracy evaluation</p>

                </div>
                <div class="p1">
                    <p id="155">如表3所示,是10组场景分别使用位姿传感器获取的初始6DOF和经过视觉位姿优化校正组合得到6DOF位姿进行虚实地理配准的量化精度的最大值、最小值和平均值,而图15则给出了相应每一组场景虚实地理配准精度优化前和优化后的量化折线图。</p>
                </div>
                <div class="area_img" id="156">
                                            <p class="img_tit">
                                                <b>表3 地理配准技术方案及其配准精度比较</b>
                                                    <br />
                                                <b>Tab.3 Comparison of geo-registration schemes and their registration accuracy</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910012_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 地理配准技术方案及其配准精度比较" src="Detail/GetImg?filename=images/CHXB201910012_15600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="157">从表3可以看出,使用位姿传感器获取的初始6DOF位姿进行地理配准的误差整体相对较高,平均配准误差在120 mard左右,且最大值达到近200 mard,相当于在距离目标100 m距离观察时,虚实配准偏差接近20 m,实际观察时会显著影响AR用户体验;而经过视觉位姿校正后,地理配准精度显著提高,平均配准误差达到20 mard以下,且最大配准误差也提高到35 mard左右,这显然有助于支撑户外AR为用户提供精确、精细的信息内容服务。如图16、图17所示,分别给出了两组户外场景实际地理配准精度优化前和优化后的地理配准效果定性对比。</p>
                </div>
                <h3 id="158" name="158" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="159">高精度的虚实地理配准技术是户外ARGIS为用户提供精细、精确信息内容服务的基础支撑,本文研究并实现了一种以2D地图为参考数据对传感器获取的初始粗略6DOF绝对位姿进行辅助校正,进而提高户外ARGIS地理配准精度的方法和技术框架。试验表明,在城市户外环境中,该方法可以有效对便携式位姿传感器获取的初始6DOF位姿进行优化,显著提高基于位姿传感器的户外AR初始地理配准精度,为户外ARGIS用户提供精确、精细信息内容服务提供重要基础支撑。当然,本文目前的测试场景仍具有一定的局限性,因为算法具有较强的约束假设,但是这些测试数据集仍是在保持AR用户户外自然观察的前提下在“离散”时刻选取的。对于更为复杂的户外三维场景,例如弧形结构建筑,或者树木等遮挡物遮挡目标建筑的一个“角点”的情况,本文的算法尚不能鲁棒处理。在2D地图数据的基础上,进一步利用建筑的高度数据(即概略的2.5D地图<citation id="189" type="reference"><link href="66" rel="bibliography" /><link href="68" rel="bibliography" /><sup>[<a class="sup">32</a>,<a class="sup">33</a>]</sup></citation>),对上述更为复杂的情况进一步研究是本文后续改进的主要工作之一。</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 每一组试验场景虚实地理配准精度优化前和优化后的量化折线图" src="Detail/GetImg?filename=images/CHXB201910012_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图15 每一组试验场景虚实地理配准精度优化前和优化后的量化折线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.15 Quantitative line graph of geo-registration accuracy before and after optimization for each set of scenes</p>

                </div>
                <div class="area_img" id="161">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 场景2虚实地理配准精度优化前与优化后的效果对比" src="Detail/GetImg?filename=images/CHXB201910012_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图16 场景2虚实地理配准精度优化前与优化后的效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_161.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.16 Effects of geo-registration accuracy in scene 2 before and after optimization</p>

                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910012_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图17 场景4虚实地理配准精度优化前与优化后的效果对比" src="Detail/GetImg?filename=images/CHXB201910012_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图17 场景4虚实地理配准精度优化前与优化后的效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910012_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.17 Effects of geo-registration accuracy in scene 4 before and after optimization</p>

                </div>
                <div class="p1">
                    <p id="163">结合惯性、视觉SLAM<citation id="190" type="reference"><link href="36" rel="bibliography" /><link href="38" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>等相对位姿连续跟踪方法,“间断性”地采用上述方法对连续跟踪的位姿进行绝对初始化和“离散”的优化校正,实现户外ARGIS“连续、持续的”高精度虚实地理配准,进一步提高算法的实际可用性和系统性,将是后续需要进一步研究的内容。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH200711024&amp;v=MDk2Njl0Yk5ybzlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZML09NaVhJWnJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 杜清运,刘涛.户外增强现实地理信息系统原型设计与实现[J].武汉大学学报(信息科学版),2007,32(11):1046-1049.DU Qingyun,LIU Tao.Design and implementation of a prototype outdoor augmented reality GIS[J].Geomatics and Information Science of Wuhan University,2007,32(11):1046-1049.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJDZ200406008&amp;v=MTkyMzA1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZML09KeWZQZExHNEh0WE1xWTlGYklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 孙敏,陈秀万,张飞舟,等.增强现实地理信息系统[J].北京大学学报(自然科学版),2004,40(6):906-913.SUN Min,CHEN Xiuwan,ZHANG Feizhou,et al.Augment reality geographical information system[J].Acta Scientiarum Naturalium Universitatis Pekinensis,2004,40(6):906-913.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DXKJ201302002&amp;v=MDA5OTdCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT0lUWEFaTEc0SDlMTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 刘经南,高柯夫.增强现实及其在导航与位置服务中的应用[J].地理空间信息,2013,11(2):1-6,14.LIU Jingnan,GAO Kefu.Augmented reality and its applications for navigation and location based service[J].Geospatial Information,2013,11(2):1-6,14.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201308004&amp;v=MjMzNzE0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT0tDTGZZYkc0SDlMTXA0OUZZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 罗斌,王涌天,沈浩,等.增强现实混合跟踪技术综述[J].自动化学报,2013,39(8):1185-1201.LUO Bin,WANG Yongtian,SHEN Hao,et al.Overview of hybrid tracking in augmented reality[J].Acta Automatica Sinica,2013,39(8):1185-1201.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Survey on Urban Warfare Augmented Reality">

                                <b>[5]</b> YOU Xiong,ZHANG Weiwei,MA Meng,et al.Survey on urban warfare augmented reality[J].ISPRS International Journal of Geo-Information,2018,7(2):46.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201606001&amp;v=MDY5OTlHRnJDVVI3cWZadWR2RnlyZ1ZML09MejdCYUxHNEg5Zk1xWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 刘浩敏,章国锋,鲍虎军.基于单目视觉的同时定位与地图构建方法综述[J].计算机辅助设计与图形学学报,2016,28(6):855-868.LIU Haomin,ZHANG Guofeng,BAO Hujun.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp; Computer Graphics,2016,28(6):855-868.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFJC201701021&amp;v=MjYxNTlPTHl2QmJiRzRIOWJNcm85SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdWTC8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 曲毅,李爱光,徐旺,等.基于位姿传感器的户外ARGIS注册技术[J].测绘科学技术学报,2017,34(1):106-110.QU Yi,LI Aiguang,XU Wang,et al.Outdoor ARGIS registration techniques based on position-posture sensor[J].Journal of Geomatics Science and Technology,2017,34(1):106-110.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBCH201703031&amp;v=MDI5NzVxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdWTC9PSVMvSVpyRzRIOWJNckk5R1pZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 曲毅,李爱光,徐旺,等.面向ArGIS的多传感器混合跟踪注册研究[J].测绘与空间地理信息,2017,40(3):114-117,121.QU Yi,LI Aiguang,XU Wang,et al.Research on multi sensor hybrid tracking registration for ArGIS[J].Geomatics &amp; Spatial Information Technology,2017,40(3):114-117,121.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014264657.nh&amp;v=MjE2NTF2RnlyZ1ZML09WRjI2R3JHK0d0ZkpxSkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 王俊.户外增强现实GIS的应用研究[D].重庆:西南大学,2014.WANG Jun.Application research on outdoor augmented reality GIS[D].Chongqing:Southwest University,2014.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interactive registration for Augmented Reality GIS">

                                <b>[10]</b> SUN Min,LIU Lei,HUANG Wei,et al.Interactive registration for augmented reality GIS[C]//Proceedings of 2012 International Conference on Computer Vision in Remote Sensing.Xiamen,China:IEEE,2012:246-251.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7C0ED1802EC2EFDFA82551F774CD9C59&amp;v=MTEzNTRVeENzYWROcitXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1OWxoeHIyNHdLND1OaWZPZmJUTEhxUzRyb2RGWnA1OERnbFB1MkJpNGoxNFRYNlVxeA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> HUANG Wei,SUN Min,LI Songnian.A 3D GIS-based interactive registration mechanism for outdoor augmented reality system[J].Expert Systems with Applications,2016,55(8):48-58.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wide-area Scene Mapping for Mobile Visual Tracking">

                                <b>[12]</b> VENTURA J,HÖLLERER T.Wide-area scene mapping for mobile visual tracking[C]//Proceedings of 2012 IEEE International Symposium on Mixed and Augmented Reality.Atlanta,GA:IEEE,2012:3-12.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable6-DOF Localization on Mobile Devices">

                                <b>[13]</b> MIDDELBERG S,SATTLER T,UNTZELMANN O,et al.Scalable 6-DOF localization on mobile devices[C]//Proceedings of the 13th European conference on computer vision.Zurich,Switzerland:Springer,2014:268-283.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Geospatial management and utilization of large-scale urban visual reconstructions">

                                <b>[14]</b> ARTH C,VENTURA J,SCHMALSTIEG D.Geospatial management and utilization of large-scale urban visual reconstructions[C]//Proceedings of the 4th International Conference on Computing for Geospatial Research and Application.San Jose,CA:IEEE,2013:64-69.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014024686.nh&amp;v=MTI3NTMzenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZML09WRjI2R3JPNkd0ZkVxWkViUElRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 段利亚.移动增强现实大范围定位与注册关键技术研究[D].武汉:华中科技大学,2013.DUAN Liya.Study on key technology of wide area localization and registration for mobile augmented reality systems[D].Wuhan:Huazhong University of Science and Technology,
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_2013" >
                                    <b>[2013]</b>
                                .
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global localization from monocular SLAM on a mobile phone.">

                                <b>[16]</b> VENTURA J,ARTH C,REITMAYR G,et al.Global localization from monocular SLAM on a mobile phone[J].IEEE Transactions on Visualization and Computer Graphics,2014,20(4):531-539.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Keyframe-based Monocular SLAM for Augmented Reality">

                                <b>[17]</b> LIU Haomin,ZHANG Guofeng,BAO Hujun.Robust keyframe-based monocular SLAM for augmented reality[C]//Proceedings of 2016 IEEE International Symposium on Mixed and Augmented Reality.Merida,Mexico:IEEE,2016:1-10.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monocular visual-inertial state estimation for mobile augmented reality">

                                <b>[18]</b> LI Peiliang,QIN Tong,HU Botao,et al.Monocular visual-inertial state estimation for mobile augmented reality[C]//Proceedings of 2017 IEEE International Symposium on Mixed and Augmented Reality.Nantes,France:IEEE,2017:11-21.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Development of vision-aided navigation for a wearable outdoor augmented reality system">

                                <b>[19]</b> MENOZZI A,CLIPP B,WENGER E,et al.Development of vision-aided navigation for a wearable outdoor augmented reality system[C]//Proceedings of 2014 IEEE/ION Position,Location and Navigation Symposium - PLANS 2014.Monterey,CA:IEEE,2014:460-472.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Augmented reality technology for day/night situational awareness for the dismounted soldier">

                                <b>[20]</b> GANS E,ROBERTS D,BENNETT M,et al.Augmented reality technology for day/night situational awareness for the dismounted soldier[C]//Proceedings Volume 9470,Display Technologies and Applications for Defense,Security,and Avionics IX;and Head- and Helmet-Mounted Displays XX.Baltimore,Maryland,United States:SPIE,2015:947004.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Estimating camera pose from a single urban ground-view omnidirectional image and a2D building outline map">

                                <b>[21]</b> CHAM T J,CIPTADI A,TAN W C,et al.Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map[C]//Proceedings of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.San Francisco,CA:IEEE,2010:366-373.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPS refinement and camera orientation estimation from a single image and a 2D map">

                                <b>[22]</b> CHU Hang,GALLAGHER A,CHEN T.GPS refinement and camera orientation estimation from a single image and a 2D map[C]//Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops.Columbus,OH:IEEE,2014:171-178.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using Vanishing Points To Correct Camera Rotation In Images">

                                <b>[23]</b> GALLAGHER A C.Using vanishing points to correct camera rotation in images[C]//Proceedings of the 2nd Canadian Conference on Computer and Robot Vision.Victoria,BC,Canada:IEEE,2005:460-467.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                 ZHANG Zhengyou.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2000,22(11):1330-1334.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relative pose calibration between visual and inertial sensors">

                                <b>[25]</b> LOBO J,DIAS J.Relative pose calibration between visual and inertial sensors[J].The International Journal of Robotics Research,2007,26(6):561-575.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid scene parsing network">

                                <b>[26]</b> ZHAO Hengshuang,SHI Jianping,QI Xiaojuan,et al.Pyramid scene parsing network[C]//Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition.Honolulu,HI:IEEE,2017:2881-2890.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD: A Fast Line Segment Detector with a False Detection Control">

                                <b>[27]</b> VON GIOI R G,JAKUBOWICZ J,MOREL J M,et al.LSD:a fast line segment detector with a false detection control[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(4):722-732.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vertical corner line detection on buildings in quasi-Manhattan world">

                                <b>[28]</b> ZHONG Baojiang,XU Dongsheng,YANG Jiwen.Vertical corner line detection on buildings in quasi-Manhattan world[C]//Proceedings of 2013 IEEE International Conference on Image Processing.Melbourne,VIC,Australia:IEEE,2013:3064-3068.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-iterative approach for fast and accurate vanishingpoint detection">

                                <b>[29]</b> TARDIF J P.Non-iterative approach for fast and accurate vanishing point detection[C]//Proceedings of the 12th International Conference on Computer Vision.Kyoto,Japan:IEEE,2009:1250-1257.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003394813&amp;v=MTk3MDViclBJRmc9Tmo3QmFyTzRIdEhQcklaQmJPb01ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZ5bmxV&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b> LEPETIT V,MORENO-NOGUER F,FUA P.EP<i>n</i>P:an accurate <i>O</i>(<i>n</i>) solution to the P<i>n</i>P problem[J].International Journal of Computer Vision,2009,81(2):155-166.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The US/UK world magnetic model for 2015—2020:technical report">

                                <b>[31]</b> CHULLIAT A,MACMILLAN S,ALKEN P,et al.The US/UK world magnetic model for 2015—2020:technical report[R].Boulder,CO:NOAA,2015.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Instant outdoor localization and SLAM initialization from 2.5D maps">

                                <b>[32]</b> ARTH C,PIRCHHEIM C,VENTURA J,et al.Instant outdoor localization and SLAM initialization from 2.5D maps[J].IEEE Transactions on Visualization and Computer Graphics,2015,21(11):1309-1318.
                            </a>
                        </p>
                        <p id="68">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global 6DOF pose estimation from untextured 2D city models">

                                <b>[33]</b> ARTH C,PIRCHHEIM C,VENTURA J,et al.Global 6DOF pose estimation from untextured 2D city models[J].Computer Science,2015,25(1):1-8.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201910012" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201910012&amp;v=MTk1MDRiTEc0SDlqTnI0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVkwvT0ppWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
