<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142621677607500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201906005%26RESULT%3d1%26SIGN%3dkrPTo9tSuwmF4yGVcyEiDPbKkmw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906005&amp;v=MTgzNTVrVzd6T0ppWFRiTEc0SDlqTXFZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#66" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="2 深度度量学习方法 ">2 深度度量学习方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="1.1 模型概述">1.1 模型概述</a></li>
                                                <li><a href="#76" data-title="1.2 均值中心度量模型">1.2 均值中心度量模型</a></li>
                                                <li><a href="#93" data-title="1.3 模型优化">1.3 模型优化</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="3 试验结果与分析 ">3 试验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="3.1 数据集">3.1 数据集</a></li>
                                                <li><a href="#108" data-title="3.2 评价指标">3.2 评价指标</a></li>
                                                <li><a href="#119" data-title="3.3 试验参数与环境">3.3 试验参数与环境</a></li>
                                                <li><a href="#122" data-title="3.4 数据集试验">3.4 数据集试验</a></li>
                                                <li><a href="#139" data-title="3.5 结果讨论">3.5 结果讨论</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="4 总 结 ">4 总 结</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="图1 基于VGGNet-16的深度卷积神经网络模型结构">图1 基于VGGNet-16的深度卷积神经网络模型结构</a></li>
                                                <li><a href="#80" data-title="图2 均值中心度量方法">图2 均值中心度量方法</a></li>
                                                <li><a href="#124" data-title="图3 不同&lt;i&gt;λ&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;参数配置下的分类准确率变化对比曲线">图3 不同<i>λ</i><sub>1</sub>参数配置下的分类准确率变化对比曲线</a></li>
                                                <li><a href="#125" data-title="图4 不同“margin”参数配置下的分类准确率变化对比曲线">图4 不同“margin”参数配置下的分类准确率变化对比曲线</a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表1 不同方法对RSSCN7数据集的分类准确率&lt;/b&gt;"><b>表1 不同方法对RSSCN7数据集的分类准确率</b></a></li>
                                                <li><a href="#129" data-title="图5 RSSCN7数据集以20%为训练样本的混淆矩阵">图5 RSSCN7数据集以20%为训练样本的混淆矩阵</a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表2 不同方法对UC Merced数据集的分类准确率&lt;/b&gt;"><b>表2 不同方法对UC Merced数据集的分类准确率</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表3 不同方法对NWPU-RESISC45数据集的分类准确率&lt;/b&gt;"><b>表3 不同方法对NWPU-RESISC45数据集的分类准确率</b></a></li>
                                                <li><a href="#137" data-title="图6 UC Merced数据集以50%为训练样本的混淆矩阵">图6 UC Merced数据集以50%为训练样本的混淆矩阵</a></li>
                                                <li><a href="#138" data-title="图7 NWPU-RESISC45数据集以20%为训练样本的混淆矩阵">图7 NWPU-RESISC45数据集以20%为训练样本的混淆矩阵</a></li>
                                                <li><a href="#141" data-title="图8 混淆场景对比影像">图8 混淆场景对比影像</a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表4 消融试验在RSSCN7、UCMerced和NWPU-RESISC45数据集上的综合平均准确率&lt;/b&gt;"><b>表4 消融试验在RSSCN7、UCMerced和NWPU-RESISC45数据集上的综合平均准确率</b></a></li>
                                                <li><a href="#147" data-title="图9 RSSCN7数据集的测试样本输出特征的2D映射特征可视化图">图9 RSSCN7数据集的测试样本输出特征的2D映射特征可视化图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" CHENG Gong, HAN Junwei, LU Xiaoqiang.Remote sensing image scene classification:benchmark and state of the art[J].Proceedings of the IEEE, 2017, 105 (10) :1865-1883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Remote Sensing Image Scene Classification:Benchmark and State of the Art">
                                        <b>[1]</b>
                                         CHENG Gong, HAN Junwei, LU Xiaoqiang.Remote sensing image scene classification:benchmark and state of the art[J].Proceedings of the IEEE, 2017, 105 (10) :1865-1883.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" 钱晓亮, 李佳, 程塨, 等.特征提取策略对高分辨率遥感图像场景分类性能影响的评估[J].遥感学报, 2018, 22 (5) :758-776.QIAN Xiaoliang, LI Jia, CHENG Gong, et al.Evaluation of the effect of feature extraction strategy on the performance of high-resolution remote sensing image scene classification[J].Journal of Remote Sensing, 2018, 22 (5) :758-776." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXB201805005&amp;v=MDAwODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2tXN3pPUENyVGJMRzRIOW5NcW85RllZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         钱晓亮, 李佳, 程塨, 等.特征提取策略对高分辨率遥感图像场景分类性能影响的评估[J].遥感学报, 2018, 22 (5) :758-776.QIAN Xiaoliang, LI Jia, CHENG Gong, et al.Evaluation of the effect of feature extraction strategy on the performance of high-resolution remote sensing image scene classification[J].Journal of Remote Sensing, 2018, 22 (5) :758-776.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint arXiv:1409.1556, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[3]</b>
                                         SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint arXiv:1409.1556, 2014.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" SZEGEDY C, LIU Wei, JIA Yangqing, et al.Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston:IEEE, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[4]</b>
                                         SZEGEDY C, LIU Wei, JIA Yangqing, et al.Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston:IEEE, 2015:1-9.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[5]</b>
                                         HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" 郑卓, 方芳, 刘袁缘, 等.高分辨率遥感影像场景的多尺度神经网络分类法[J].测绘学报, 2018, 47 (5) :620-630.DOI:10.11947/j.AGCS.2018.20170191.ZHENG Zhuo, FANG Fang, LIU Yuanyuan, et al.Joint multi-scale convolution neural network for scene classification of high resolution remote sensing imagery[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (5) :620-630.DOI:10.11947/j.AGCS.2018.20170191." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201805009&amp;v=MTA5MDhadWR2Rnkza1c3ek9KaVhUYkxHNEg5bk1xbzlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         郑卓, 方芳, 刘袁缘, 等.高分辨率遥感影像场景的多尺度神经网络分类法[J].测绘学报, 2018, 47 (5) :620-630.DOI:10.11947/j.AGCS.2018.20170191.ZHENG Zhuo, FANG Fang, LIU Yuanyuan, et al.Joint multi-scale convolution neural network for scene classification of high resolution remote sensing imagery[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (5) :620-630.DOI:10.11947/j.AGCS.2018.20170191.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" HU Fan, XIA Guisong, HU Jingwen, et al.Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery[J].Remote Sensing, 2015, 7 (11) :14680-14707." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery">
                                        <b>[7]</b>
                                         HU Fan, XIA Guisong, HU Jingwen, et al.Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery[J].Remote Sensing, 2015, 7 (11) :14680-14707.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" CHENG Gong, YANG Ceyuan, YAO Xiwen, et al.When deep learning meets metric learning:remote sensing image scene classification via learning discriminative CNNs[J].IEEE Transactions on Geoscience and Remote Sensing, 2018, 56 (5) :2811-2821." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=When Deep Learning Meets Metric Larning:Remote Sensing Image Scene Classification via Learning Discriminative CNNs">
                                        <b>[8]</b>
                                         CHENG Gong, YANG Ceyuan, YAO Xiwen, et al.When deep learning meets metric learning:remote sensing image scene classification via learning discriminative CNNs[J].IEEE Transactions on Geoscience and Remote Sensing, 2018, 56 (5) :2811-2821.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" LIU Yishu, HUANG Chao.Scene classification via triplet networks[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018, 11 (1) :220-237." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene Classification via Triplet Networks">
                                        <b>[9]</b>
                                         LIU Yishu, HUANG Chao.Scene classification via triplet networks[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018, 11 (1) :220-237.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" BAO Jiangfeng, CHI Mingmin, BENEDIKTSSON J A.Spectral derivative features for classification of hyperspectral remote sensing images:experimental evaluation[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2013, 6 (2) :594-601." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Spectral Derivative Features for Classification of Hyperspectral Remote Sensing Images:Experimental Evaluation, &amp;quot;">
                                        <b>[10]</b>
                                         BAO Jiangfeng, CHI Mingmin, BENEDIKTSSON J A.Spectral derivative features for classification of hyperspectral remote sensing images:experimental evaluation[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2013, 6 (2) :594-601.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" JIAO Hongzan, ZHONG Yanfei, ZHANG Liangpei.Artificial DNA computing-based spectral encoding and matching algorithm for hyperspectral remote sensing data[J].IEEE Transactions on Geoscience and Remote Sensing, 2012, 50 (10) :4085-4104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Artificial DNA computing-based spectral encoding and matching algorithm for hyperspectral remote sensing data">
                                        <b>[11]</b>
                                         JIAO Hongzan, ZHONG Yanfei, ZHANG Liangpei.Artificial DNA computing-based spectral encoding and matching algorithm for hyperspectral remote sensing data[J].IEEE Transactions on Geoscience and Remote Sensing, 2012, 50 (10) :4085-4104.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" FRANKLIN S E, HALL R J, MOSKAL L M, et al.Incorporating texture into classification of forest species composition from airborne multispectral images[J].International Journal of Remote Sensing, 2000, 21 (1) :61-79." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD713860311&amp;v=MDM2MzMzenFxQnRHRnJDVVI3cWZadWR2Rnkza1c3ek9Oam5CYXJTNUhkbktyNHhFWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         FRANKLIN S E, HALL R J, MOSKAL L M, et al.Incorporating texture into classification of forest species composition from airborne multispectral images[J].International Journal of Remote Sensing, 2000, 21 (1) :61-79.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" CLAUSI D A, DENG Huang.Design-based texture feature fusion using Gabor filters and co-occurrence probabilities[J].IEEE Transactions on Image Processing, 2005, 14 (7) :925-936." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design-based texture feature fusion using Gabor filters and co-occurrence probabilities">
                                        <b>[13]</b>
                                         CLAUSI D A, DENG Huang.Design-based texture feature fusion using Gabor filters and co-occurrence probabilities[J].IEEE Transactions on Image Processing, 2005, 14 (7) :925-936.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" ZHANG Liangpei, HUANG Xin, HUANG Bo, et al.A pixel shape index coupled with spectral information for classification of high spatial resolution remotely sensed imagery[J].IEEE Transactions on Geoscience and Remote Sensing, 2006, 44 (10) :2950-2961." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A pixel shape index coupled with spectral information for classification of high spatial resolution remotely sensed imagery">
                                        <b>[14]</b>
                                         ZHANG Liangpei, HUANG Xin, HUANG Bo, et al.A pixel shape index coupled with spectral information for classification of high spatial resolution remotely sensed imagery[J].IEEE Transactions on Geoscience and Remote Sensing, 2006, 44 (10) :2950-2961.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" YANG Yi, NEWSAM S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems.New York:ACM, 2010:270-279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bag of visual words and spatial extensions for land-use classification">
                                        <b>[15]</b>
                                         YANG Yi, NEWSAM S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems.New York:ACM, 2010:270-279.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" CHEN Shizhi, TIAN Yingli.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2014, 53 (4) :1947-1957." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pyramid of spatial relatons for scene-level land use classification">
                                        <b>[16]</b>
                                         CHEN Shizhi, TIAN Yingli.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2014, 53 (4) :1947-1957.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" ZHAO Lijun, TANG Ping, HUO Lianzhi.Land-use scene classification using a concentric circle-structured multiscale bag-of-visual-words model[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017, 7 (12) :4620-4631." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Land-use scene classification using a concentric circle-structured multiscale bag-of-visual-words model">
                                        <b>[17]</b>
                                         ZHAO Lijun, TANG Ping, HUO Lianzhi.Land-use scene classification using a concentric circle-structured multiscale bag-of-visual-words model[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017, 7 (12) :4620-4631.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" LIENOU M, MAITRE H, DATCU M.Semantic annotation of satellite images using latent dirichlet allocation[J].IEEE Geoscience and Remote Sensing Letters, 2010, 7 (1) :28-32." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Annotation of Satellite Images Using Latent Dirichlet Allocation">
                                        <b>[18]</b>
                                         LIENOU M, MAITRE H, DATCU M.Semantic annotation of satellite images using latent dirichlet allocation[J].IEEE Geoscience and Remote Sensing Letters, 2010, 7 (1) :28-32.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_19" title=" ZHONG Yanfei, ZHU Qiqi, ZHANG Liangpei.Scene classification based on the multifeature fusion probabilistic topic model for high spatial resolution remote sensing imagery[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (11) :6207-6222." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene Classification Based on the Multifeature Fusion Probabilistic Topic Model for High Spatial Resolution Remote Sensing Imagery">
                                        <b>[19]</b>
                                         ZHONG Yanfei, ZHU Qiqi, ZHANG Liangpei.Scene classification based on the multifeature fusion probabilistic topic model for high spatial resolution remote sensing imagery[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (11) :6207-6222.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_20" title=" LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[20]</b>
                                         LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_21" title=" LUUS F P S, SALMON B P, VAN DEN BERGH F, et al.Multiview deep learning for land-use classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (12) :2448-2452." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiview Deep Learning for LandUse Classification">
                                        <b>[21]</b>
                                         LUUS F P S, SALMON B P, VAN DEN BERGH F, et al.Multiview deep learning for land-use classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (12) :2448-2452.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_22" title=" ZHANG Fan, DU Bo, ZHANG Liangpei.Scene classification via a gradient boosting random convolutional network framework[J].IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (3) :1793-1802." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene classification via a gradient Boosting random convolutional network framework">
                                        <b>[22]</b>
                                         ZHANG Fan, DU Bo, ZHANG Liangpei.Scene classification via a gradient boosting random convolutional network framework[J].IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (3) :1793-1802.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_23" title=" WU Hang, LIU Baozhen, SU Weihua, et al.Deep filter banks for land-use scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (12) :1895-1899." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Filter Banks for Land-Use Scene Classification">
                                        <b>[23]</b>
                                         WU Hang, LIU Baozhen, SU Weihua, et al.Deep filter banks for land-use scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (12) :1895-1899.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_24" title=" ZENG Dan, CHEN Shuaijun, CHEN Boyang, et al.Improving remote sensing scene classification by integrating global-context and local-object features[J].Remote Sensing, 2018, 10 (5) :734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving remote sensing scene classification by integrating global-context and local-object features">
                                        <b>[24]</b>
                                         ZENG Dan, CHEN Shuaijun, CHEN Boyang, et al.Improving remote sensing scene classification by integrating global-context and local-object features[J].Remote Sensing, 2018, 10 (5) :734.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_25" title=" XIA Guisong, HU Jingwen, HU Fan, et al.AID:A benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AID:ABenchmark Data Set for Performance Evaluation of Aerial Scene Classification">
                                        <b>[25]</b>
                                         XIA Guisong, HU Jingwen, HU Fan, et al.AID:A benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_26" title=" YE Lihua, WANG Lei, SUN Yaxin, et al.Aerial scene classification via an ensemble extreme learning machine classifier based on discriminative hybrid convolutional neural networks features[J].International Journal of Remote Sensing, 2018, 40 (7) :2759-2783." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aerial scene classification via an ensemble extreme learning machine classifier based on discriminative hybrid convolutional neural networks features">
                                        <b>[26]</b>
                                         YE Lihua, WANG Lei, SUN Yaxin, et al.Aerial scene classification via an ensemble extreme learning machine classifier based on discriminative hybrid convolutional neural networks features[J].International Journal of Remote Sensing, 2018, 40 (7) :2759-2783.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_27" title=" 许夙晖, 慕晓冬, 赵鹏, 等.利用多尺度特征与深度网络对遥感影像进行场景分类[J].测绘学报, 2016, 45 (7) :834-840.DOI:10.11947/j.AGCS.2016.20150623.XU Suhui, MU Xiaodong, ZHAO Peng, et al.Scene classification of remote sensing image based on multi-scale feature and deep neural network[J].Acta Geodaetica et Cartographica Sinica, 2016, 45 (7) :834-840.DOI:10.11947/j.AGCS.2016.20150623." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201607012&amp;v=MjQ0MDZxZlp1ZHZGeTNrVzd6T0ppWFRiTEc0SDlmTXFJOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         许夙晖, 慕晓冬, 赵鹏, 等.利用多尺度特征与深度网络对遥感影像进行场景分类[J].测绘学报, 2016, 45 (7) :834-840.DOI:10.11947/j.AGCS.2016.20150623.XU Suhui, MU Xiaodong, ZHAO Peng, et al.Scene classification of remote sensing image based on multi-scale feature and deep neural network[J].Acta Geodaetica et Cartographica Sinica, 2016, 45 (7) :834-840.DOI:10.11947/j.AGCS.2016.20150623.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_28" title=" YE Lihua, WANG Lei, SUN Yaxin, et al.Parallel multi-stage features fusion of deep convolutional neural networks for aerial scene classification[J].Remote Sensing Letters, 2018, 9 (3) :294-303." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDC18D2DD29988F2F30EE30B036A201279&amp;v=MjAxNDFibTN3NjQ9TmpuQmFzQzVGcVhPMi90SGJlSUhCQW83dVJVVG4wcCtTQTNpcnhSRWU3S1ZSNzJXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1OWxodw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         YE Lihua, WANG Lei, SUN Yaxin, et al.Parallel multi-stage features fusion of deep convolutional neural networks for aerial scene classification[J].Remote Sensing Letters, 2018, 9 (3) :294-303.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_29" title=" WEN Yandong, ZHANG Kaipeng, LI Zhifeng, et al.A discriminative feature learning approach for deep face recognition[C]//Proceedings of the 14th European Conference on Computer Vision.Amsterdam, The Netherlands:Springer, 2016:499-515." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Discrim-inative Feature Learning Approach for Deep Face Recognition">
                                        <b>[29]</b>
                                         WEN Yandong, ZHANG Kaipeng, LI Zhifeng, et al.A discriminative feature learning approach for deep face recognition[C]//Proceedings of the 14th European Conference on Computer Vision.Amsterdam, The Netherlands:Springer, 2016:499-515.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_30" title=" BIAN Xiaoyong, CHEN Chen, TIAN Long, et al.Fusing local and global features for high-resolution scene classification[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017, 10 (6) :2889-2901." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fusing local and global features for high-resolution scene classification">
                                        <b>[30]</b>
                                         BIAN Xiaoyong, CHEN Chen, TIAN Long, et al.Fusing local and global features for high-resolution scene classification[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017, 10 (6) :2889-2901.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(06),698-707 DOI:10.11947/j.AGCS.2019.20180434            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>高分辨率光学遥感场景分类的深度度量学习方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E5%88%A9%E5%8D%8E&amp;code=37393967&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶利华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%A3%8A&amp;code=10090875&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%87%E6%96%87&amp;code=35273215&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张文文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%B0%B8%E5%88%9A&amp;code=07764160&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李永刚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E8%B5%A0%E5%87%AF&amp;code=42097124&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王赠凯</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%8C%E6%B5%8E%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0118734&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">同济大学电子与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%98%89%E5%85%B4%E5%AD%A6%E9%99%A2%E6%95%B0%E7%90%86%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0137300&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">嘉兴学院数理与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对高分辨率光学遥感影像场景具有同类型内部差异大、不同类型间相似度高导致部分场景识别困难的问题, 本文提出了一种深度度量学习方法。首先在深度学习模型的特征输出层上为每类预设聚类中心, 其次基于欧氏距离方法设计均值中心度量损失项, 最后联合交叉熵损失项以及权重与偏置正则项构成模型的损失函数。该方法的目标是在特征空间上使同类型特征聚集并扩大类型间的距离以提高分类准确率。试验结果表明, 本文方法有效地提升了分类准确率。在RSSCN7、UC Merced和NWPU-RESISC45数据集上, 与现有方法相比, 分类准确率分别提高了1.46%、1.09%和2.51%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">度量学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9D%87%E5%80%BC%E4%B8%AD%E5%BF%83%E5%BA%A6%E9%87%8F%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">均值中心度量损失;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%BA%E6%99%AF%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">场景分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    叶利华 (1978—) , 男, 博士生, 讲师, 研究方向为机器学习, 深度学习, 遥感影像解译。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFE0100900);</span>
                                <span>浙江省自然科学基金 (LY19F020017;LY18F020021);</span>
                    </p>
            </div>
                    <h1>Deep metric learning method for high resolution remote sensing image scene classification</h1>
                    <h2>
                    <span>YE Lihua</span>
                    <span>WANG Lei</span>
                    <span>ZHANG Wenwen</span>
                    <span>LI Yonggang</span>
                    <span>WANG Zengkai</span>
            </h2>
                    <h2>
                    <span>College of Electronic and Information Engineering, Tongji University</span>
                    <span>College of Mathematics, Physics and Information Engineering, Jiaxing University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to the similarity of intra-class and dissimilarity of inter-class of high-resolution remote sensing image scene, it is difficult to identify some image scene class. In this paper, a new classification approach for high-resolution remote sensing image scene is proposed based on deep learning and metric learning. Firstly, a clustering center of each class is preset on the output features of deep learning model. Secondly, the Euclidean distance method is used to calculate the average central metric loss. Finally, the final loss function consists of a central metric loss term, a cross entropy loss term, and a weight and bias term. The goal of this method is to improve the classification accuracy by forcing intra-class compactness and inter-class separability. The experimental results show that the proposed method significantly improves the classification accuracy. Compared with state-of-the-art results, the classification accuracy ratios on RSSCN7, UC Merced and NWPU-RESISC45 datasets are increased by 1.46%, 1.09% and 2.51%, respectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=metric%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">metric learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=average%20center%20metric%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">average center metric loss;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scene%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scene classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YE Lihua (1978—) , male, PhD candidate, lecturer, majors in machine learning, deep learning and remote sensing image interpretation.E-mail: 9604ylh@tongji.edu.cn;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-17</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National Key Research and Develepment Program of China (No.2017YFE0100900);</span>
                                <span>The National Natural Science Foundation of Zhejiang Province of China (Nos.LY19F020017; LY18F020021);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="62">随着遥感影像技术的快速发展, 高分辨率光学遥感影像数据量获得快速增长。与中低分辨率遥感影像相比, 高分影像包含的信息更丰富, 如空间信息、纹理信息、地物的几何结构信息等。影像中的地物目标具有同类差异大和部分类间相似度高的特点, 因而如何有效地自动解译影像已吸引众多研究者的关注<citation id="148" type="reference"><link href="2" rel="bibliography" /><link href="4" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="63">为实现计算机视觉技术自动解译高分辨率光学遥感影的目标, 很多处理方法被提出, 主要可分为人工设计特征法和深度学习法。描述场景信息采用人工设计特征方法提取时, 由于缺乏语义信息, 导致这些方法的识别准确率与实际应用要求有较大差距。当前, 由于出色的性能, 深度学习方法已成为人工智能与模式识别领域的研究热点。针对图像分类问题, 大量深度学习模型被构建, 其中深度卷积神经网络模型的效果最好, 如VGGNet<citation id="149" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、GoogLeNet<citation id="150" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、ResNet<citation id="151" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。在遥感影像场景分类任务中, 基于深度学习方法的分类准确率获得大幅度地提高<citation id="152" type="reference"><link href="4" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">6</a>]</sup></citation>。与人工设计特征方法相比, 深度学习方法需要更多的标注样本。在标注样本较少的应用中, 迁移学习方法能有效地解决样本缺少问题<citation id="153" type="reference"><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="64">然而, 在识别同类差异大和类间相似度高的任务时, 现有的深度学习模型仍不能准确地区分各类场景。因此, 深度学习模型的特征学习能力仍需进一步提高。文献<citation id="154" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>采用三元组损失的度量学习与深度学习结合的方法, 提高模型的学习能力。该方法的输入由3幅图片构成, 分别为主样本、同类型样本和不同类型样本, 通过度量同类和异类之间的特征距离以提升模型对相似场景的分辨能力。三元组深度度量学习方法存在以下问题:训练样本量急剧增长, 假定有<i>C</i>种类型且每种类型有<i>N</i>个样本, 则训练样本数由<i>C</i>×<i>N</i>变为<i>C</i><sup>2</sup><sub><i>N</i></sub>×<i>N</i>× (<i>C</i>-1) ;训练时运行3个相同网络结构, 收敛速度慢;仅对不同类型之间特征距离进行约束而同类型内部未进行约束, 同类特征在空间上未聚类。</p>
                </div>
                <div class="p1">
                    <p id="65">针对上述三元组方法存在的问题, 本文提出均值中心度量方法以提升深度学习模型的学习能力。该方法通过增加<i>C</i>个均值聚类中心来改进现有深度学习模型。与现有的遥感影像场景分类方法相比, 本文方法的特点如下:①单输入方式实现深度学习与度量学习相结合的遥感影像场景分类;②改进深度学习模型的损失函数, 新损失函数由交叉熵损失项、权重与偏置正则项和均值中心度量损失项组成;③与现有方法相比, 在3个公开遥感数据集上都取得最高的分类准确率。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="67">早期遥感影像场景分类采用基于低层特征的方法, 包括光谱特征、纹理特征、形状特征等。文献<citation id="158" type="reference">[<a class="sup">10</a>,<a class="sup">11</a>]</citation>分别提取光谱微分特征和脱氧核糖核酸编码光谱特征的方法进行分类。针对对象纹理信息差异的特点, 文献<citation id="155" type="reference">[<a class="sup">12</a>]</citation>提出光谱与纹理相结合的场景特征表示。单独或融合的纹理特征可有效地表征高分遥感影像信息, 如灰度共生矩阵、Gabor小波纹理等<citation id="156" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。文献<citation id="157" type="reference">[<a class="sup">14</a>]</citation>融合像素上下文的形状结构特征与光谱特征提高了分类准确率。</p>
                </div>
                <div class="p1">
                    <p id="68">若用语义信息描述能力差的低层特征来表征富含语义信息的遥感影像场景, 识别性能存在局限性。视觉词袋模型 (bag of visual words, BoVW) 描述的特征含有中层语义信息, 该方法在图像分类领域获得广泛应用。文献<citation id="159" type="reference">[<a class="sup">15</a>]</citation>采用该模型显著地提高了遥感影像场景的分类准确率。文献<citation id="160" type="reference">[<a class="sup">16</a>]</citation>采用空间金字塔模型, 将分层图像的BoVW特征级联组成最终特征用于描述图像特征。文献<citation id="161" type="reference">[<a class="sup">17</a>]</citation>基于BoVW提出一种空间共生矩阵核来表示相对空间信息并采用同心圆的划分方式解决图像旋转敏感问题。然而, BoVW模型仅利用图像局部特征的统计信息但忽略这些信息之间关联关系。为挖掘这些关联信息, 文献<citation id="162" type="reference">[<a class="sup">18</a>]</citation>提出主题模型实现场景的语义标注, 文献<citation id="163" type="reference">[<a class="sup">19</a>]</citation>则利用多种低层特征分别构建主题模型来实现。基于主题模型方法使场景分类的准确率获得大幅度地提高, 然而, 上述方法在复杂场景上的分类准确率仍很低<citation id="164" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="69">随着高性能计算技术的快速发展, 深度学习方法被广泛地应用于各领域的研究并取得巨大成功, 其原因在于深度学习方法能从原始数据中自动地学习高层语义信息<citation id="165" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。与其他图像分类任务一样, 基于深度学习的高分遥感影像场景分类准确率也获得大幅提升。文献<citation id="166" type="reference">[<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>]</citation>分别构建端到端的深度学习模型用于高分遥感影像场景分类任务。VGGNet、ResNet等优秀深度学习模型直接地用于高分遥感影像场景分类可获得更好的性能<citation id="167" type="reference"><link href="48" rel="bibliography" /><link href="50" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>。针对遥感影像数据集标注数据量少的问题, 基于迁移学习的方法有效地提高了分类准确率<citation id="168" type="reference"><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><link href="52" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">26</a>]</sup></citation>。融合不同的深度特征也可有效地提高分类准确率<citation id="169" type="reference"><link href="54" rel="bibliography" /><link href="56" rel="bibliography" /><sup>[<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="70">尽管深度学习方法极大地提高了高分辨率光学遥感影像场景分类的准确率, 但面对相似程度较高的场景时区分能力仍不足。因此, 度量学习被引入用于改进深度学习模型, 其目的在于改进特征在空间上的分布, 降低相似场景之间的混淆比率。文献<citation id="170" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>采用三元组深度度量学习方法提高深度学习模型的区分能力。</p>
                </div>
                <div class="p1">
                    <p id="71">综上所述, 现有高分辨率光学遥感影像场景分类方法中, 人工设计特征方法的特点是对标注数据量要求低、模型简单、运行速度快等, 但是表征能力差、知识迁移困难、分类准确率低等;而深度学习方法则正相反, 特征学习能力强、迁移学习较易及分类准确率高, 但需更多的计算资源。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag">2 深度度量学习方法</h3>
                <h4 class="anchor-tag" id="73" name="73">1.1 模型概述</h4>
                <div class="p1">
                    <p id="74">本文的深度卷积神经网络模型如图1所示。该模型基于VGGNet-16<citation id="171" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>构建, 虚线框部分继承原有模型, 由卷积层、池化层及全连接层构成, 增加全连接层 (1×1×<i>N</i>, <i>N</i>为类型数) 以及联合损失层构成本文模型。图1中, “224×224×3”代表有3个通道, 输入数据的尺寸为224×224。本文模型由13个卷积层、5个池化层、3个全连接层以及联合损失层组成。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于VGGNet-16的深度卷积神经网络模型结构" src="Detail/GetImg?filename=images/CHXB201906005_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于VGGNet-16的深度卷积神经网络模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Structure diagram of deep convolutional neural network model based on VGGNet-16</p>

                </div>
                <h4 class="anchor-tag" id="76" name="76">1.2 均值中心度量模型</h4>
                <div class="p1">
                    <p id="77">在特征空间上不同场景类型的间距越大, 相似场景之间混淆的可能性就越低。因此, 改进深度学习模型输出特征的空间分布, 对提高总体分类准确率有积极意义。在文献<citation id="172" type="reference">[<a class="sup">8</a>,<a class="sup">29</a>]</citation>的启发下, 本文提出均值中心度量方法以改进特征的空间分布, 实现提升模型场景识别能力的目标。图2展示了本文深度度量学习方法的核心思想。“<i>d</i>”代表各聚类中心之间欧氏距离的平方, 计算方法如式 (1) 所示</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>i</mi></msub><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (1) </p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 均值中心度量方法" src="Detail/GetImg?filename=images/CHXB201906005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 均值中心度量方法  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 The diagram of average center metric method</p>

                </div>
                <div class="p1">
                    <p id="81">式中, <i>i</i>和<i>j</i>为类型编号;<i>N</i>为数据样本类型数目;<b><i>c</i></b><sub><i>ik</i></sub>为第<i>i</i>类均值聚类中心向量的第<i>k</i>维;“margin”超参数为均值聚类中心之间的最小距离。该方法为各场景类型添加聚类中心, 聚类中心的值在训练过程中按批次进行动态调整。</p>
                </div>
                <div class="p1">
                    <p id="82">模型的损失函数由3部分构成, 包括交叉熵损失项 (<i>L</i><sub><i>s</i></sub>) 、均值中心度量损失项 (<i>L</i><sub><i>cm</i></sub>) 和权重 (<b><i>W</i></b>) 与偏置 (<b><i>b</i></b>) 正则项。各项的作用分别是:交叉熵损失项使不相同类型样本分离;中心度量损失项使同类型聚集并扩大各类聚集中心之间的最小间距;权重与偏置正则项是为防止模型过拟合。模型的损失函数定义如式 (2) 所示</p>
                </div>
                <div class="area_img" id="181">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201906005_18100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="85">式中, <i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>为权重系数。</p>
                </div>
                <div class="p1">
                    <p id="86">假定数据集有<i>N</i>个样本<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mtext>R</mtext></mrow></mrow><msup><mrow></mrow><mi>d</mi></msup><mo>, </mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mi>Κ</mi></msup><mo stretchy="false">}</mo><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></mrow></math></mathml>。其中<i>d</i>为模型输出特征的维数;<i>K</i>是样本类型数;<b><i>x</i></b><sub><i>i</i></sub>=[<i>x</i><sub><i>i</i>1</sub>, <i>x</i><sub><i>i</i>2</sub>, …, <i>x</i><sub><i>id</i></sub>]是<i>d</i>×1的输出特征向量;<b><i>y</i></b><sub><i>i</i></sub>=[<i>y</i><sub><i>i</i>1</sub>, <i>y</i><sub><i>i</i>2</sub>, …, <i>y</i><sub><i>iK</i></sub>]是与样本<b><i>x</i></b><sub><i>i</i></sub>对应的类型标签向量, 其中仅有一个值为1剩余的为0。式 (2) 中的<i>L</i><sub><i>s</i></sub>项定义如下</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mfrac><mrow><mi>e</mi><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup></mrow></mfrac></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="90">式中, <i>m</i>为每批次的样本数;<i>y</i><sub><i>i</i></sub>为第<i>i</i>个样本的类型编号。根据上述均值中心度量方法的描述, <i>L</i><sub><i>cm</i></sub>定义如式 (4) 所示</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>m</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo> (</mo><mtable columnalign="left"><mtr><mtd></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false"> (</mo></mstyle><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mrow></mtd></mtr><mtr><mtd><mrow><mtable columnalign="left"><mtr><mtd></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mi>h</mi><mo stretchy="false"> (</mo><mtext>m</mtext><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>i</mtext><mtext>n</mtext><mo>-</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>, </mo><mn>0</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd></mtd></mtr></mtable><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">式中, <b><i>c</i></b><sub><i>y</i><sub><i>i</i></sub></sub>为<b><i>y</i></b><sub><i>i</i></sub>类型的均值聚类中心, 其值采用梯度下降方法通过训练求得;<i>h</i> (<i>x</i>) 定义为max (<i>x</i>, 0) 。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">1.3 模型优化</h4>
                <div class="p1">
                    <p id="94">针对式 (2) 的优化问题, 本文采用随机梯度下降 (stochastic gradient descent, SGD) 方法进行求解。根据SGD求解原理, <i>L</i><sub><i>cm</i></sub>项的<b><i>x</i></b><sub><i>i</i></sub>偏导数及均值聚类中心的更新梯度如式 (5) 和式 (6) 所示</p>
                </div>
                <div class="p1">
                    <p id="95"><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>m</mi></mrow></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></math></mathml>      (5) </p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201906005_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="98">式中, <i>δ</i>函数在条件满足时返回1否则返回0;<i>n</i>为类型序号。<b><i>c</i></b><sub><i>n</i></sub>值的更新方法如式 (7) 所示</p>
                </div>
                <div class="p1">
                    <p id="99"><b><i>c</i></b><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>=<b><i>c</i></b><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>t</mi></msubsup></mrow></math></mathml>-Δ<b><i>c</i></b><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>t</mi></msubsup></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="103">式中, <i>t</i>为迭代次序。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">3 试验结果与分析</h3>
                <h4 class="anchor-tag" id="105" name="105">3.1 数据集</h4>
                <div class="p1">
                    <p id="106">为验证所提方法的有效性, 本文选取其中3个各有特点的数据集进行试验, 包括RSSCN7、UC Merced和NWPU-RESISC45数据集<citation id="173" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。RSSCN7数据集共有7类场景, 每类场景都有400幅400×400像素的影像并平均分为4种尺度。UC Merced数据集有21类场景且只有一个尺度, 各类型有100幅256×256像素的影像。NWPU-RESISC45是当前规模最大、种类最多的公开遥感场景影像数据集共有45类场景, 分别包含700幅256×256像素的影像。该数据集的特点是同类内部差异大和异类之间相似度高, 对高分遥感图像场景分类方法有很高的挑战性。</p>
                </div>
                <div class="p1">
                    <p id="107">为公平比较, 数据集的设置与其他方法一致<citation id="174" type="reference"><link href="2" rel="bibliography" /><link href="48" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">24</a>]</sup></citation>, 随机选取各类型的两种比例作为训练样本, 剩余为测试样本, 其中RSSCN7为20%和50%、UC Merced为50%和80%以及NWPU-RESISC45为10%和20%。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">3.2 评价指标</h4>
                <div class="p1">
                    <p id="109">试验结果采用平均总体分类准确率、标准差和混淆矩阵作为分类性能的评估方法。总体分类准确率的计算方法如式 (8) 所示</p>
                </div>
                <div class="p1">
                    <p id="110"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mi>Τ</mi><mi>Ν</mi></mfrac><mo>⋅</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mrow></math></mathml>      (8) </p>
                </div>
                <div class="p1">
                    <p id="112">式中, N为测试样本的总数;T为各类型分类正确数的总和。平均总体分类准确率和标准差的计算如式 (9) 和式 (10) 所示</p>
                </div>
                <div class="p1">
                    <p id="113"><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ρ</mi><mo stretchy="true">¯</mo></mover><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>Ρ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="115"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>Ρ</mi><mo stretchy="true">¯</mo></mover><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="117">式中, <i>M</i>为重复试验的次数, 本文<i>M</i>为10。</p>
                </div>
                <div class="p1">
                    <p id="118">混淆矩阵能直观地展示各类型之间的混淆比率, 矩阵的行为真实类型而列为预测类型。矩阵的对角线元素为各类型的分类准确率, 其他任意元素<i>x</i><sub><i>i</i>, <i>j</i></sub>代表第<i>i</i>类被误识为第<i>j</i>类场景占该类型的比率。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.3 试验参数与环境</h4>
                <div class="p1">
                    <p id="120">试验中有两类参数需要配置。一类是损失函数中的超参数, 包括<i>λ</i><sub>1</sub>、<i>λ</i><sub>2</sub>和“margin”。参考文献<citation id="175" type="reference">[<a class="sup">8</a>]</citation>, <i>λ</i><sub>2</sub>设为0.000 5;<i>λ</i><sub>1</sub>和“margin”的值, 本文通过在NWPU-RESISC45数据集上以10%样本测试确定。首先, 设定“margin”参数为1并分别设置<i>λ</i><sub>1</sub>的值为{0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}进行试验, 见图3, <i>λ</i><sub>1</sub>设置为0.3。然后, 将<i>λ</i><sub>1</sub>设置为0.3, 分别设置“margin”参数为{0.01, 0.05, 0.1, 0.5, 1, 2}进行试验, 见图4, “margin”设置为1。另一类是训练参数, 设置如下:学习率 (0.0005) 、更新策略 (“inv”) 、迭代次数 (20000) 、批次大小 (20) 等。</p>
                </div>
                <div class="p1">
                    <p id="121">软硬件环境如下:Ubuntu 16.04操作系统、Caffe深度学习框架、Python2.7编程语言、Intel I5 3.4 GHz双核CPU、16 GB RAM和GTX1070显卡。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">3.4 数据集试验</h4>
                <div class="p1">
                    <p id="123">本文下列所有试验结果都是基于迁移学习ImageNet数据集获得。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同λ1参数配置下的分类准确率变化对比曲线" src="Detail/GetImg?filename=images/CHXB201906005_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同<i>λ</i><sub>1</sub>参数配置下的分类准确率变化对比曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 The overall accuracies of the proposed method using different <i>λ</i><sub>1</sub> settings</p>

                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同“margin”参数配置下的分类准确率变化对比曲线" src="Detail/GetImg?filename=images/CHXB201906005_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同“margin”参数配置下的分类准确率变化对比曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 The overall accuracies of the proposed method using different “margin” settings</p>

                </div>
                <h4 class="anchor-tag" id="126" name="126">3.4.1 RSSCN7数据集试验</h4>
                <div class="p1">
                    <p id="127">表1列出近期相关方法以及本文方法的准确率。在训练样本分别为20%和50%时, 本文方法的准确率分别达到93.93%和96.01%, 高于其他方法的结果。尽管增强数据的量比文献<citation id="176" type="reference">[<a class="sup">24</a>]</citation>少, 本文方法的准确率仍分别提高了1.46%和0.42%。图5显示了20%训练样本的分类结果混淆矩阵, 结果表明仅有田与草地、工业区与停车场的混淆比率相对较高。</p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit"><b>表1 不同方法对RSSCN7数据集的分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.1 Overall accuracy (%) and standard deviations of the proposed method and the comparison of state-of-the-art methods on the RSSCN7dataset</b></p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td rowspan="2"><br />方法</td><td rowspan="2">年份</td><td colspan="2"><br />训练样本占比</td></tr><tr><td><br />20%</td><td>50%</td></tr><tr><td><br />Deep Filter Banks[23]</td><td>2016</td><td>—</td><td>90.04±0.6</td></tr><tr><td><br />VGGNet-16[25]</td><td>2017</td><td>83.98±0.87</td><td>87.18±0.94</td></tr><tr><td><br />LOFs+GCFs[24]<sup>1</sup></td><td>2018</td><td>92.47±0.29</td><td>95.59±0.49</td></tr><tr><td><br />本文方法<sup>2</sup></td><td>—</td><td><b>93.93±0.42</b></td><td><b>96.01±0.58</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:1 通过旋转0°、90°、180°和270°、左右和上下翻转以及随机添加白高斯噪声方式增强训练样本。2 通过旋转0°、90°、180°和270°方式增强训练样本。</p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 RSSCN7数据集以20%为训练样本的混淆矩阵" src="Detail/GetImg?filename=images/CHXB201906005_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 RSSCN7数据集以20%为训练样本的混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Confusion matrix of the RSSCN7 dataset with a 20% ratio as training samples</p>

                </div>
                <h4 class="anchor-tag" id="130" name="130">3.4.2 UC Merced数据集试验</h4>
                <div class="p1">
                    <p id="131">表2列出近期公开方法以及本文方法的分类结果。在训练样本为80%时, 本文方法的准确率略高于目前最好结果;而在训练样本为50%时, 与文献<citation id="177" type="reference">[<a class="sup">9</a>]</citation>相比, 准确率提升了1.09%。图6显示了50%训练样本的分类结果混淆矩阵, 除密集住宅、中密度住宅和稀疏住宅之间容易混淆外, 其他场景都能较好地识别。与文献<citation id="178" type="reference">[<a class="sup">24</a>]</citation>中的混淆矩阵相比, 密集住宅和中密度住宅的混淆比率大幅地降低, 从18%降至6%。</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表2 不同方法对UC Merced数据集的分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.2 Overall accuracy (%) and standard deviations of the proposed method and the comparison of state-of-the-art methods on the UC Merced dataset</b></p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td rowspan="2"><br />方法</td><td rowspan="2">年份</td><td colspan="2"><br />训练样本占比</td></tr><tr><td><br />50%</td><td>80%</td></tr><tr><td><br />VGG-VD-16[25]</td><td>2016</td><td>94.14±0.69</td><td>95.21±1.20</td></tr><tr><td><br />salM3 LBP-CLM[30]</td><td>2017</td><td>94.21±0.75</td><td>95.75±0.80</td></tr><tr><td><br />Fine-tuned VGGNet-16+SVM[8]</td><td>2018</td><td>—</td><td>97.14±0.10</td></tr><tr><td><br />Triplet networks[9]<sup>1</sup></td><td>2018</td><td>—</td><td>97.99±0.53</td></tr><tr><td><br />D-CNN with VGGNet-16[8]<sup>2</sup></td><td>2018</td><td>—</td><td>98.93±0.10</td></tr><tr><td><br />LOFs+GCFs[24]<sup>3</sup></td><td>2018</td><td>97.37±0.44</td><td>99.00±0.35</td></tr><tr><td><br />本文方法<sup>3</sup></td><td>-</td><td><b>98.46±0.18</b></td><td><b>99.15±0.29</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:1四个角上按75%和50%覆盖方式分别裁剪与中间按50%覆盖方式裁剪, 实现九倍训练样本增强。2 每次迭代训练中, 随机选择2 (<i>C</i>-1) 幅图像以单独生成<i>C</i>-1个同类和异类的影像对得到批训练样本, 其中<i>C</i>为类型数。3 数据增强方法与表1一致。</p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">3.4.3 NWPU-RESISC45数据集试验</h4>
                <div class="p1">
                    <p id="134">表3列出了最新研究结果, 本文方法与其他方法相比在分类准确率上有显著地提高。与文献<citation id="179" type="reference">[<a class="sup">8</a>]</citation>相比, 分类准确率分别提高2.51%和1.58%;与文献<citation id="180" type="reference">[<a class="sup">9</a>]</citation>相比, 分类准确率提高了1.14%。图7为20%训练样本的分类结果混淆矩阵。</p>
                </div>
                <div class="p1">
                    <p id="135">分析混淆矩阵发现教堂易被识为宫殿和商业区、宫殿易被识为教堂、铁路易被识为火车站。图8列出3类误识的对比场景影像, (a) 教堂、 (b) 宫殿和 (c) 铁路分别被误识为宫殿、教堂和火车站, 与对应样例 (d) 宫殿、 (e) 教堂和 (f) 铁路非常相似。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表3 不同方法对NWPU-RESISC45数据集的分类准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.3 Overall accuracy (%) and standard deviations of the proposed method and the comparison of state-of-the-art methods on the NWPU-RESISC45 dataset</b></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td rowspan="2"><br />方法</td><td rowspan="2">年份</td><td colspan="2"><br />训练样本占比</td></tr><tr><td><br />10%</td><td>20%</td></tr><tr><td><br />Fine-tuned VGGNet-16[1]</td><td>2017</td><td>87.15±0.45</td><td>90.36±0.18</td></tr><tr><td><br />D-CNN with VGGNet-<br />16[8]<sup>*</sup></td><td>2018</td><td>89.22±0.50</td><td>91.89±0.22</td></tr><tr><td><br />Triplet networks[9]<sup>*</sup></td><td>2018</td><td>—</td><td>92.33±0.20</td></tr><tr><td><br />本文方法<sup>*</sup></td><td>—</td><td><b>91.73±0.21</b></td><td><b>93.47±0.30</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">注:* 数据增强方法与表2一致。</p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 UC Merced数据集以50%为训练样本的混淆矩阵" src="Detail/GetImg?filename=images/CHXB201906005_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 UC Merced数据集以50%为训练样本的混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Confusion matrix of the UC Merced dataset with a 50% ratio as training samples</p>

                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 NWPU-RESISC45数据集以20%为训练样本的混淆矩阵" src="Detail/GetImg?filename=images/CHXB201906005_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 NWPU-RESISC45数据集以20%为训练样本的混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Confusion matrix of the NWPU-RESISC45 dataset with a 20% ratio as training samples</p>

                </div>
                <h4 class="anchor-tag" id="139" name="139">3.5 结果讨论</h4>
                <div class="p1">
                    <p id="140">在3个数据集上的试验结果表明, 本文方法的准确率明显高于其他方法。为进一步分析本文方法的性能, 表4列出消融试验结果。试验结果说明调优训练、数据增强及均值中心度量方法都能有效地提高分类准确率。</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 混淆场景对比影像" src="Detail/GetImg?filename=images/CHXB201906005_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 混淆场景对比影像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Thecontrast image of confused scene</p>

                </div>
                <div class="area_img" id="142">
                                            <p class="img_tit">
                                                <b>表4 消融试验在RSSCN7、UCMerced和NWPU-RESISC45数据集上的综合平均准确率</b>
                                                    <br />
                                                <b>Tab.4 Overall accuracy (%) of Ablation study on RSSCN7, UC Merced and NWPU-RESISC45 datasets</b> (%) 
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201906005_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 消融试验在RSSCN7、UCMerced和NWPU-RESISC45数据集上的综合平均准确率" src="Detail/GetImg?filename=images/CHXB201906005_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="143">为分析本文方法对特征空间分布的影响, 采用LargeVis算法将第2个全链接层的4096维输出映射成二维向量。图9是RSSCN7以50%训练样本在没有增强条件下获取的特征分布对比图, 每个圈代表不同类型大概聚集范围。图9 (b) 的特征聚集程度显著地提升, 不同类型间的界线更清晰;图9 (a) 的特征分布范围是50×60而图9 (b) 为120×200, 类型间距离明显地增大。表4和图9的结果验证了本文方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="144">分析表1至表4发现在训练样本比例较低的情况下准确率获得更明显的提升, 说明本文方法在少样本的应用中适用性更强。</p>
                </div>
                <h3 id="145" name="145" class="anchor-tag">4 总 结</h3>
                <div class="p1">
                    <p id="146">针对高分场景图像分类存在相似场景之间容易混淆的问题, 本文提出深度学习与度量学习相结合的方法来降低混淆比率。新模型的损失函数由交叉熵损失项、均值中心度量损失项以及权重与偏置正则项组成。试验结果表明, 本文方法与现有其他方法相比在分类准确率上有明显的提高。在RSSCN7、UC Merced和NWPU-RESISC45数据集上以较小比例为训练样本时, 分类准确率分别达到93.93%、98.46%和91.73%。尽管如此, 在处理规模更大、类型更多、场景更复杂的高分遥感场景影像分类的任务中, 分类准确率还有待改进。改进方法可从两个方面:一是改进模型以提升局部细节信息的学习能力, 构建全局特征与局部细节特征相结合的学习模型;二是应用多特征融合方法, 如多个深度模型特征或人工设计特征。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906005_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 RSSCN7数据集的测试样本输出特征的2D映射特征可视化图" src="Detail/GetImg?filename=images/CHXB201906005_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 RSSCN7数据集的测试样本输出特征的2D映射特征可视化图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906005_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 2D feature visualization of image representations of the RSSCN7 dataset</p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Remote Sensing Image Scene Classification:Benchmark and State of the Art">

                                <b>[1]</b> CHENG Gong, HAN Junwei, LU Xiaoqiang.Remote sensing image scene classification:benchmark and state of the art[J].Proceedings of the IEEE, 2017, 105 (10) :1865-1883.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXB201805005&amp;v=MDUzNzdCdEdGckNVUjdxZlp1ZHZGeTNrVzd6T1BDclRiTEc0SDluTXFvOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 钱晓亮, 李佳, 程塨, 等.特征提取策略对高分辨率遥感图像场景分类性能影响的评估[J].遥感学报, 2018, 22 (5) :758-776.QIAN Xiaoliang, LI Jia, CHENG Gong, et al.Evaluation of the effect of feature extraction strategy on the performance of high-resolution remote sensing image scene classification[J].Journal of Remote Sensing, 2018, 22 (5) :758-776.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[3]</b> SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].arXiv preprint arXiv:1409.1556, 2014.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[4]</b> SZEGEDY C, LIU Wei, JIA Yangqing, et al.Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Boston:IEEE, 2015:1-9.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[5]</b> HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201805009&amp;v=MDM4MTdCdEdGckNVUjdxZlp1ZHZGeTNrVzd6T0ppWFRiTEc0SDluTXFvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 郑卓, 方芳, 刘袁缘, 等.高分辨率遥感影像场景的多尺度神经网络分类法[J].测绘学报, 2018, 47 (5) :620-630.DOI:10.11947/j.AGCS.2018.20170191.ZHENG Zhuo, FANG Fang, LIU Yuanyuan, et al.Joint multi-scale convolution neural network for scene classification of high resolution remote sensing imagery[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (5) :620-630.DOI:10.11947/j.AGCS.2018.20170191.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery">

                                <b>[7]</b> HU Fan, XIA Guisong, HU Jingwen, et al.Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery[J].Remote Sensing, 2015, 7 (11) :14680-14707.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=When Deep Learning Meets Metric Larning:Remote Sensing Image Scene Classification via Learning Discriminative CNNs">

                                <b>[8]</b> CHENG Gong, YANG Ceyuan, YAO Xiwen, et al.When deep learning meets metric learning:remote sensing image scene classification via learning discriminative CNNs[J].IEEE Transactions on Geoscience and Remote Sensing, 2018, 56 (5) :2811-2821.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene Classification via Triplet Networks">

                                <b>[9]</b> LIU Yishu, HUANG Chao.Scene classification via triplet networks[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2018, 11 (1) :220-237.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Spectral Derivative Features for Classification of Hyperspectral Remote Sensing Images:Experimental Evaluation, &amp;quot;">

                                <b>[10]</b> BAO Jiangfeng, CHI Mingmin, BENEDIKTSSON J A.Spectral derivative features for classification of hyperspectral remote sensing images:experimental evaluation[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2013, 6 (2) :594-601.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Artificial DNA computing-based spectral encoding and matching algorithm for hyperspectral remote sensing data">

                                <b>[11]</b> JIAO Hongzan, ZHONG Yanfei, ZHANG Liangpei.Artificial DNA computing-based spectral encoding and matching algorithm for hyperspectral remote sensing data[J].IEEE Transactions on Geoscience and Remote Sensing, 2012, 50 (10) :4085-4104.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD713860311&amp;v=MjM3MzVxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2tXN3pPTmpuQmFyUzVIZG5LcjR4RVpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> FRANKLIN S E, HALL R J, MOSKAL L M, et al.Incorporating texture into classification of forest species composition from airborne multispectral images[J].International Journal of Remote Sensing, 2000, 21 (1) :61-79.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design-based texture feature fusion using Gabor filters and co-occurrence probabilities">

                                <b>[13]</b> CLAUSI D A, DENG Huang.Design-based texture feature fusion using Gabor filters and co-occurrence probabilities[J].IEEE Transactions on Image Processing, 2005, 14 (7) :925-936.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A pixel shape index coupled with spectral information for classification of high spatial resolution remotely sensed imagery">

                                <b>[14]</b> ZHANG Liangpei, HUANG Xin, HUANG Bo, et al.A pixel shape index coupled with spectral information for classification of high spatial resolution remotely sensed imagery[J].IEEE Transactions on Geoscience and Remote Sensing, 2006, 44 (10) :2950-2961.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bag of visual words and spatial extensions for land-use classification">

                                <b>[15]</b> YANG Yi, NEWSAM S.Bag-of-visual-words and spatial extensions for land-use classification[C]//Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems.New York:ACM, 2010:270-279.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pyramid of spatial relatons for scene-level land use classification">

                                <b>[16]</b> CHEN Shizhi, TIAN Yingli.Pyramid of spatial relatons for scene-level land use classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2014, 53 (4) :1947-1957.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Land-use scene classification using a concentric circle-structured multiscale bag-of-visual-words model">

                                <b>[17]</b> ZHAO Lijun, TANG Ping, HUO Lianzhi.Land-use scene classification using a concentric circle-structured multiscale bag-of-visual-words model[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017, 7 (12) :4620-4631.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Annotation of Satellite Images Using Latent Dirichlet Allocation">

                                <b>[18]</b> LIENOU M, MAITRE H, DATCU M.Semantic annotation of satellite images using latent dirichlet allocation[J].IEEE Geoscience and Remote Sensing Letters, 2010, 7 (1) :28-32.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene Classification Based on the Multifeature Fusion Probabilistic Topic Model for High Spatial Resolution Remote Sensing Imagery">

                                <b>[19]</b> ZHONG Yanfei, ZHU Qiqi, ZHANG Liangpei.Scene classification based on the multifeature fusion probabilistic topic model for high spatial resolution remote sensing imagery[J].IEEE Transactions on Geoscience and Remote Sensing, 2015, 53 (11) :6207-6222.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[20]</b> LECUN Y, BENGIO Y, HINTON G.Deep learning[J].Nature, 2015, 521 (7553) :436-444.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiview Deep Learning for LandUse Classification">

                                <b>[21]</b> LUUS F P S, SALMON B P, VAN DEN BERGH F, et al.Multiview deep learning for land-use classification[J].IEEE Geoscience and Remote Sensing Letters, 2015, 12 (12) :2448-2452.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene classification via a gradient Boosting random convolutional network framework">

                                <b>[22]</b> ZHANG Fan, DU Bo, ZHANG Liangpei.Scene classification via a gradient boosting random convolutional network framework[J].IEEE Transactions on Geoscience and Remote Sensing, 2016, 54 (3) :1793-1802.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Filter Banks for Land-Use Scene Classification">

                                <b>[23]</b> WU Hang, LIU Baozhen, SU Weihua, et al.Deep filter banks for land-use scene classification[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (12) :1895-1899.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving remote sensing scene classification by integrating global-context and local-object features">

                                <b>[24]</b> ZENG Dan, CHEN Shuaijun, CHEN Boyang, et al.Improving remote sensing scene classification by integrating global-context and local-object features[J].Remote Sensing, 2018, 10 (5) :734.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AID:ABenchmark Data Set for Performance Evaluation of Aerial Scene Classification">

                                <b>[25]</b> XIA Guisong, HU Jingwen, HU Fan, et al.AID:A benchmark data set for performance evaluation of aerial scene classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (7) :3965-3981.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aerial scene classification via an ensemble extreme learning machine classifier based on discriminative hybrid convolutional neural networks features">

                                <b>[26]</b> YE Lihua, WANG Lei, SUN Yaxin, et al.Aerial scene classification via an ensemble extreme learning machine classifier based on discriminative hybrid convolutional neural networks features[J].International Journal of Remote Sensing, 2018, 40 (7) :2759-2783.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201607012&amp;v=MDY2OTVxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2tXN3pPSmlYVGJMRzRIOWZNcUk5RVpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 许夙晖, 慕晓冬, 赵鹏, 等.利用多尺度特征与深度网络对遥感影像进行场景分类[J].测绘学报, 2016, 45 (7) :834-840.DOI:10.11947/j.AGCS.2016.20150623.XU Suhui, MU Xiaodong, ZHAO Peng, et al.Scene classification of remote sensing image based on multi-scale feature and deep neural network[J].Acta Geodaetica et Cartographica Sinica, 2016, 45 (7) :834-840.DOI:10.11947/j.AGCS.2016.20150623.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDC18D2DD29988F2F30EE30B036A201279&amp;v=MDg0NDNHUWxmQnJMVTA1OWxod2JtM3c2ND1Oam5CYXNDNUZxWE8yL3RIYmVJSEJBbzd1UlVUbjBwK1NBM2lyeFJFZTdLVlI3MldDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> YE Lihua, WANG Lei, SUN Yaxin, et al.Parallel multi-stage features fusion of deep convolutional neural networks for aerial scene classification[J].Remote Sensing Letters, 2018, 9 (3) :294-303.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Discrim-inative Feature Learning Approach for Deep Face Recognition">

                                <b>[29]</b> WEN Yandong, ZHANG Kaipeng, LI Zhifeng, et al.A discriminative feature learning approach for deep face recognition[C]//Proceedings of the 14th European Conference on Computer Vision.Amsterdam, The Netherlands:Springer, 2016:499-515.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fusing local and global features for high-resolution scene classification">

                                <b>[30]</b> BIAN Xiaoyong, CHEN Chen, TIAN Long, et al.Fusing local and global features for high-resolution scene classification[J].IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2017, 10 (6) :2889-2901.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201906005" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906005&amp;v=MTgzNTVrVzd6T0ppWFRiTEc0SDlqTXFZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
