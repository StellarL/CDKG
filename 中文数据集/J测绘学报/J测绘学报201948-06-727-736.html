<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142622471045000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201906008%26RESULT%3d1%26SIGN%3dddgCSEcouFN%252bgAdWxNFPX8ku0ZI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906008&amp;v=MDYzODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2xVcnJQSmlYVGJMRzRIOWpNcVk5RmJJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 基于Siamese网络的影像相似性检测 ">1 基于Siamese网络的影像相似性检测</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="2 多模态遥感影像模板匹配流程 ">2 多模态遥感影像模板匹配流程</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="3 试 验 ">3 试 验</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="3.1 试验准备">3.1 试验准备</a></li>
                                                <li><a href="#96" data-title="3.2 训练数据集准备">3.2 训练数据集准备</a></li>
                                                <li><a href="#98" data-title="3.3 模型训练">3.3 模型训练</a></li>
                                                <li><a href="#100" data-title="3.4 性能对比">3.4 性能对比</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="4 结 论 ">4 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 Siamese网络结构">图1 Siamese网络结构</a></li>
                                                <li><a href="#68" data-title="图2 本文网络模型">图2 本文网络模型</a></li>
                                                <li><a href="#83" data-title="图3 方法对比">图3 方法对比</a></li>
                                                <li><a href="#85" data-title="图4 试验影像">图4 试验影像</a></li>
                                                <li><a href="#86" data-title="图5 水平方向相似性曲线">图5 水平方向相似性曲线</a></li>
                                                <li><a href="#89" data-title="图6 匹配流程">图6 匹配流程</a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表1 试验数据介绍&lt;/b&gt;"><b>表1 试验数据介绍</b></a></li>
                                                <li><a href="#95" data-title="图7 试验数据">图7 试验数据</a></li>
                                                <li><a href="#103" data-title="图8 相似性图">图8 相似性图</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表2 试验结果对比&lt;/b&gt;"><b>表2 试验结果对比</b></a></li>
                                                <li><a href="#107" data-title="图9 试验结果">图9 试验结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="137">


                                    <a id="bibliography_1" title="闫利, 王紫琦, 叶志云.顾及灰度和梯度信息的多模态影像配准算法[J].测绘学报, 2018, 47 (1) :71-81.DOI:10.11947/j.AGCS.2018.20170368.YAN Li, WANG Ziqi, YE Zhiyun.Multimodal image registration algorithm considering grayscale and gradient information[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (1) :71-81.DOI:10.11947/j.AGCS.2018.20170368." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201801010&amp;v=MTQ0ODg2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTNsVXJyUEppWFRiTEc0SDluTXJvOUVaSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        闫利, 王紫琦, 叶志云.顾及灰度和梯度信息的多模态影像配准算法[J].测绘学报, 2018, 47 (1) :71-81.DOI:10.11947/j.AGCS.2018.20170368.YAN Li, WANG Ziqi, YE Zhiyun.Multimodal image registration algorithm considering grayscale and gradient information[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (1) :71-81.DOI:10.11947/j.AGCS.2018.20170368.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_2" title="YE Yuanxin, SHAN Jie, BRUZZONE L, et al.Robust registration of multimodal remote sensing images based on structural similarity[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (5) :2941-2958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity">
                                        <b>[2]</b>
                                        YE Yuanxin, SHAN Jie, BRUZZONE L, et al.Robust registration of multimodal remote sensing images based on structural similarity[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (5) :2941-2958.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_3" title="ZITOVB, FLUSSER J.Image registration methods:a survey[J].Image and Vision Computing, 2003, 21 (11) :977-1000." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349910&amp;v=MTA1MzhVPU5pZk9mYks3SHRET3JZOUVaKzhHQlgwNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUpWNFZieA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        ZITOVB, FLUSSER J.Image registration methods:a survey[J].Image and Vision Computing, 2003, 21 (11) :977-1000.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_4" title="余先川, 吕中华, 胡丹.遥感图像配准技术综述[J].光学精密工程, 2013, 21 (11) :2960-2972.YU Xianchuan, LZhonghua, HU Dan.Review of remote sensing image registration techniques[J].Optics and Precision Engineering, 2013, 21 (11) :2960-2972." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201311030&amp;v=MjMyNjU3cWZadWR2RnkzbFVyclBJalhCWTdHNEg5TE5ybzlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        余先川, 吕中华, 胡丹.遥感图像配准技术综述[J].光学精密工程, 2013, 21 (11) :2960-2972.YU Xianchuan, LZhonghua, HU Dan.Review of remote sensing image registration techniques[J].Optics and Precision Engineering, 2013, 21 (11) :2960-2972.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_5" title="MORAVEC H P.Towards automatic visual obstacle avoidance[C]∥Proceedings of the 5th International Joint Conference on Artificial Intelligence.Massachusetts US:MIT, 1977." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards automatic visual obstacle avoidance">
                                        <b>[5]</b>
                                        MORAVEC H P.Towards automatic visual obstacle avoidance[C]∥Proceedings of the 5th International Joint Conference on Artificial Intelligence.Massachusetts US:MIT, 1977.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_6" title="HARRIS C, STEPHENS M.A combined corner and edge detector[C]∥Proceedings of the 4th Alvey Vision Conference.Manchester, UK:Organising Committee AVC, 1988:147-151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Combined Corner and Edge Detector">
                                        <b>[6]</b>
                                        HARRIS C, STEPHENS M.A combined corner and edge detector[C]∥Proceedings of the 4th Alvey Vision Conference.Manchester, UK:Organising Committee AVC, 1988:147-151.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_7" title="ZIOU D, TABBONE S.Edge detection techniques:an overview[J].International Journal of Pattern Recognition and Image Analysis, 1998 (8) :537-559." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Edge detection techniques:an overview">
                                        <b>[7]</b>
                                        ZIOU D, TABBONE S.Edge detection techniques:an overview[J].International Journal of Pattern Recognition and Image Analysis, 1998 (8) :537-559.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_8" title="ZHANG Dengrong, YU Le, CAI Zhigang.Automatic registration for ASAR and TM images based on region features[C]∥Proceedings of SPIE 6752, Geoinformatics 2007:Remotely Sensed Data and Information.Nanjing:SPIE, 2007:6752." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic registration for ASAR and TM images based on region features">
                                        <b>[8]</b>
                                        ZHANG Dengrong, YU Le, CAI Zhigang.Automatic registration for ASAR and TM images based on region features[C]∥Proceedings of SPIE 6752, Geoinformatics 2007:Remotely Sensed Data and Information.Nanjing:SPIE, 2007:6752.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_9" title="张宝尚, 田铮, 延伟东.基于分割区域的SAR图像配准方法研究[J].工程数学学报, 2011, 28 (1) :7-14.ZHANG Baoshang, TIAN Zheng, YAN Weidong.An SAR image registration algorithm based on segmentationderived regions[J].Chinese Journal of Engineering Mathematics, 2011, 28 (1) :7-14." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GCSX201101004&amp;v=MDk0NDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnkzbFVyclBJaTdZZHJHNEg5RE1ybzlGWUlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        张宝尚, 田铮, 延伟东.基于分割区域的SAR图像配准方法研究[J].工程数学学报, 2011, 28 (1) :7-14.ZHANG Baoshang, TIAN Zheng, YAN Weidong.An SAR image registration algorithm based on segmentationderived regions[J].Chinese Journal of Engineering Mathematics, 2011, 28 (1) :7-14.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_10" title="LOWE D G.Object recognition from local scale-invariant features[C]∥Proceeding of the 7th International Conference on Computer Vision.Kerkyra, Corfu, Greece:IEEE, 1999:1150-1157." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object recognition from local scale-invariant features">
                                        <b>[10]</b>
                                        LOWE D G.Object recognition from local scale-invariant features[C]∥Proceeding of the 7th International Conference on Computer Vision.Kerkyra, Corfu, Greece:IEEE, 1999:1150-1157.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_11" title="LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTE1MThrPU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGeW5sVnIvSkpW&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_12" title="BELONGIE S, MALIK J, PUZICHA J.Shape matching and object recognition using shape contexts[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape matching and object recognition using shape contexts">
                                        <b>[12]</b>
                                        BELONGIE S, MALIK J, PUZICHA J.Shape matching and object recognition using shape contexts[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_13" title="SURI S, REINARTZ P.Mutual-information-based registration of TerraSAR-X and Ikonos imagery in urban areas[J].IEEE Transactions on Geoscience and Remote Sensing, 2010, 48 (2) :939-949." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mutual-information-based registration of TerraSAR-X and ikonos imagery in Urban areas">
                                        <b>[13]</b>
                                        SURI S, REINARTZ P.Mutual-information-based registration of TerraSAR-X and Ikonos imagery in urban areas[J].IEEE Transactions on Geoscience and Remote Sensing, 2010, 48 (2) :939-949.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_14" title="MARTINEZ A, GARCIA-CONSUEGRA J, ABAD F.Acorrelation-symbolic approach to automatic remotely sensed image rectification[C]∥Proceedings of IEEE 1999International Geoscience and Remote Sensing Symposium.Hamburg, Germany:IEEE, 1999:336-338." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A correlation-symbolic approach to automatic remotely sensed image rectification">
                                        <b>[14]</b>
                                        MARTINEZ A, GARCIA-CONSUEGRA J, ABAD F.Acorrelation-symbolic approach to automatic remotely sensed image rectification[C]∥Proceedings of IEEE 1999International Geoscience and Remote Sensing Symposium.Hamburg, Germany:IEEE, 1999:336-338.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_15" title="JOHNSON K, COLE-RHODES A, ZAVORIN I, et al.Mutual information as a similarity measure for remote sensing image registration[C]∥Proceedings of SPIE4383, Geo-Spatial Image and Data ExploitationⅡ.Orlando, FL, United States:SPIE, 2001:51-61." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mutual information as a similarity measure for remote sensing image registration">
                                        <b>[15]</b>
                                        JOHNSON K, COLE-RHODES A, ZAVORIN I, et al.Mutual information as a similarity measure for remote sensing image registration[C]∥Proceedings of SPIE4383, Geo-Spatial Image and Data ExploitationⅡ.Orlando, FL, United States:SPIE, 2001:51-61.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_16" title="YE Yuanxin, SHAN Jie, HAO Siyuan, et al.A local phase based invariant feature for remote sensing image matching[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2018 (142) :205-221." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES408212CE209B01E9BA69B2253BFDE9DC&amp;v=MDU5MzhOdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1OWxod2JpK3hhOD1OaWZPZmJlNEZ0UE5yZnd3WnVzR2ZudzR1aDlobXpsME9uM2dxUkZIRDhiaFRNN3NDTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                        YE Yuanxin, SHAN Jie, HAO Siyuan, et al.A local phase based invariant feature for remote sensing image matching[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2018 (142) :205-221.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_17" title="ZAGORUYKO S, KOMODAKIS N.Learning to compare image patches via convolutional neural networks[C]∥Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston:IEEE, 2015:4353-4361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to compare image patches via convolutional neural networks">
                                        <b>[17]</b>
                                        ZAGORUYKO S, KOMODAKIS N.Learning to compare image patches via convolutional neural networks[C]∥Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston:IEEE, 2015:4353-4361.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_18" title="范大昭, 董杨, 张永生.卫星影像匹配的深度卷积神经网络方法[J].测绘学报, 2018, 47 (6) :844-853.DOI:10.11947/j.AGCS.2018.20170627.FAN Dazhao, DONG Yang, ZHANG Yongsheng.Satellite image matching method based on deep convolution neural network[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (6) :844-853.DOI:10.11947/j.AGCS.2018.20170627." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806018&amp;v=MjkyODJYVGJMRzRIOW5NcVk5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2xVcnJQSmk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                        范大昭, 董杨, 张永生.卫星影像匹配的深度卷积神经网络方法[J].测绘学报, 2018, 47 (6) :844-853.DOI:10.11947/j.AGCS.2018.20170627.FAN Dazhao, DONG Yang, ZHANG Yongsheng.Satellite image matching method based on deep convolution neural network[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (6) :844-853.DOI:10.11947/j.AGCS.2018.20170627.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_19" title="HE Haiqing, CHEN Min, CHEN Ting, et al.Matching of remote sensing images with complex background variations via Siamese convolutional neural network[J].Remote Sensing, 2018, 10 (3) :355." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Matching of remote sensing images with complex background variations via Siamese convolutional neural network">
                                        <b>[19]</b>
                                        HE Haiqing, CHEN Min, CHEN Ting, et al.Matching of remote sensing images with complex background variations via Siamese convolutional neural network[J].Remote Sensing, 2018, 10 (3) :355.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_20" title="BROMLEY J, GUYON I, LECUN Y, et al.Signature verification using a“Siamese”time delay neural network[C]∥Proceedings of the 6th International Conference on Neural Information Processing Systems.Denver, Colorado:Morgan Kaufmann Publishers Inc, 1993." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Signature verification using a&amp;quot;Siamese&amp;quot;time delay neural network">
                                        <b>[20]</b>
                                        BROMLEY J, GUYON I, LECUN Y, et al.Signature verification using a“Siamese”time delay neural network[C]∥Proceedings of the 6th International Conference on Neural Information Processing Systems.Denver, Colorado:Morgan Kaufmann Publishers Inc, 1993.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_21" title="CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with Application to Face Verification∥Proceeding of 2005IEEE Computer Society Conference on Computer Vision and Pattern Recognition.San Diego, CA:IEEE, 2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a similarity metric discriminatively with application to face verification">
                                        <b>[21]</b>
                                        CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with Application to Face Verification∥Proceeding of 2005IEEE Computer Society Conference on Computer Vision and Pattern Recognition.San Diego, CA:IEEE, 2005.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_22" title="许慧敏.基于深度学习U-Net模型的高分辨率遥感影像分类方法研究[D].成都:西南交通大学, 2018.XU Huimin.Method research of high resolution remote sensing imagery classification based on U-Net model of deep learning[D].Chengdu:Southwest Jiaotong University, 2018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018709715.nh&amp;v=Mjg2MzViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnkzbFVyclBWRjI2RnJTNEY5Yk5xcEU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        许慧敏.基于深度学习U-Net模型的高分辨率遥感影像分类方法研究[D].成都:西南交通大学, 2018.XU Huimin.Method research of high resolution remote sensing imagery classification based on U-Net model of deep learning[D].Chengdu:Southwest Jiaotong University, 2018.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_23" title="周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251.ZHOU Feiyan, JIN Linpeng, DONG Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=Mjk5NTFNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2xVcnJQTHo3QmRyRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                        周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251.ZHOU Feiyan, JIN Linpeng, DONG Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_24" title="HADSELL R, CHOPRA S, LECUN Y.Dimensionality reduction by learning an invariant mapping[C]∥Proceedins of 2006IEEE Computer Society Conference on Computer Vision and Pattern Recognition.New York:IEEE, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dimensionality Reduction by Learning an Invariant Mapping">
                                        <b>[24]</b>
                                        HADSELL R, CHOPRA S, LECUN Y.Dimensionality reduction by learning an invariant mapping[C]∥Proceedins of 2006IEEE Computer Society Conference on Computer Vision and Pattern Recognition.New York:IEEE, 2006.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_25" title="TRAPPEY A J C, HSU F C, TRAPPEY C V, et al.Development of a patent document classification and search platform using a back-propagation network[J].Expert Systems with Applications, 2006, 31 (4) :755-765." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501643843&amp;v=MTE0NzlsVTc3SUpWNFZieFU9TmlmT2ZiSzdIdEROcW85RVl1OE1CSGc2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        TRAPPEY A J C, HSU F C, TRAPPEY C V, et al.Development of a patent document classification and search platform using a back-propagation network[J].Expert Systems with Applications, 2006, 31 (4) :755-765.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_26" title="YE Yuanxin, SHAN Jie.A local descriptor based registration method for multispectral remote sensing images with nonlinear intensity differences[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2014 (90) :83-95." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300100884&amp;v=MTIyMTZUTW53WmVadEZpbmxVNzdJSlY0VmJ4VT1OaWZPZmJLOEh0TE9ySTlGWmVzUEJIUTlvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        YE Yuanxin, SHAN Jie.A local descriptor based registration method for multispectral remote sensing images with nonlinear intensity differences[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2014 (90) :83-95.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_27" title="林善明, 朱小艳, 周建华, 等.一种结合互信息和模板匹配的配准方法[J].计算机工程, 2010, 36 (14) :198-200.LIN Shanming, ZHU Xiaoyan, ZHOU Jianhua, et al.Registration method with mutual information and template matching[J].Computer Engineering, 2010, 36 (14) :198-200." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201014075&amp;v=MjY0NDFyQ1VSN3FmWnVkdkZ5M2xVcnJQTHo3QmJiRzRIOUhOcTQ5Q1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                        林善明, 朱小艳, 周建华, 等.一种结合互信息和模板匹配的配准方法[J].计算机工程, 2010, 36 (14) :198-200.LIN Shanming, ZHU Xiaoyan, ZHOU Jianhua, et al.Registration method with mutual information and template matching[J].Computer Engineering, 2010, 36 (14) :198-200.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(06),727-736 DOI:10.11947/j.AGCS.2019.20180432            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>深度卷积特征表达的多模态遥感影像模板匹配方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%97%E8%BD%B2&amp;code=41064483&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南轲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BD%90%E5%8D%8E&amp;code=09180661&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">齐华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E6%B2%85%E9%91%AB&amp;code=29818727&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶沅鑫</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%83%E7%A7%91%E5%AD%A6%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0218487&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南交通大学地球科学与环境工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>多模态遥感影像间 (光学、红外、SAR等) 存在显著的非线性辐射差异, 传统方法难以有效地提取影像间的共有特征, 匹配效果不佳。鉴于此, 本文将深度学习方法引入影像匹配中, 提出了一种基于Siamese网络提取多模态影像共有特征的匹配方法。首先通过去除Siamese网络中的池化层和抽取特征来优化该网络, 保持特征信息的完整性和位置精度, 使其可有效地提取多模态影像间的共有特征, 然后采用模板匹配策略, 实现多模态遥感影像高精度匹配。通过利用多组多模态遥感影像进行试验, 结果表明, 本文方法的匹配正确率和匹配精度都优于传统的模板匹配方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A8%A1%E6%80%81%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多模态影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BD%B1%E5%83%8F%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">影像匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Siamese%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Siamese网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    南轲 (1994—) , 男, 硕士生, 研究方向为摄影测量与遥感。;
                                </span>
                                <span>
                                    *叶沅鑫, E-mail:yeyuanxin@home.swjtu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>四川省科技计划 (2017SZ0027);</span>
                    </p>
            </div>
                    <h1>A template matching method of multimodal remote sensing images based on deep convolutional feature representation</h1>
                    <h2>
                    <span>NAN Ke</span>
                    <span>QI Hua</span>
                    <span>YE Yuanxin</span>
            </h2>
                    <h2>
                    <span>Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to significant non-linear radiometric differences between multimodal remote sensing images (e.g., optical, infrared, and SAR) , traditional methods cannot efficiently extract common features between such images, and are vulnerable for image matching. To address that, the deep learning technique is introduced into the present study to design a matching method based on Siamese network, which aims to extract common features between multimodal images. The network is first optimized by removing the pooling layer and extracting the feature layer from Siamese network to maintain the integrity and positional accuracy of the feature information, making it possible the effective extraction of common features between multimodal images. Then, the template matching strategy is adopted to achieve high-precision matching of multimodal images. The proposed method is evaluated by using multiple multimodal remote sensing images. The results show that the proposed method outperforms traditional template-matching methods in both the matching correct ratio and matching accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multimodal%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multimodal image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Siamese%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Siamese network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    NAN Ke (1994—) , male, postgraduate, majors in photogrammetry and remote sensing.E-mail: nanke1994@163.com;
                                </span>
                                <span>
                                    YE Yuanxin, E-mail: yeyuanxin@home.swjtu.edu.cn;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-14</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The Science and Technology Program of Sichuan Province (No.2017SZ0027);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="58">随着遥感技术的发展, 从遥感影像上获取所需信息已经成为一种非常重要的信息获取手段。不同的卫星传感器对地观测可以为同一地区提供多光谱、多时相、多分辨率的多模态遥感影像 (光学、红外、SAR等) 。这些多模态遥感影像反映了地物的不同特征, 能够为地表监测提供互补的信息, 弥补单一影像源的不足, 提高影像的信息量<citation id="191" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。利用多模态遥感影像进行数据分析和处理前, 必须进行严格的匹配或配准。虽然目前的遥感影像利用轨道参数和严格几何定位模型进行粗纠正, 可消除影像间的旋转和尺度等几何形变<citation id="192" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 但是由于多模态影像间非线性辐射差异较大, 即同一地物呈现出完全不同的灰度信息, 导致同名点的匹配十分困难, 所以多模态遥感影像的自动匹配仍然非常具有挑战性。</p>
                </div>
                <div class="p1">
                    <p id="59">目前, 影像匹配方法大致可分为两种, 基于特征的方法和基于区域的方法<citation id="198" type="reference"><link href="141" rel="bibliography" /><link href="143" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。基于特征的方法首先是对影像特征进行提取, 再利用特征间的相似性进行匹配。常见的特征包括点特征 (Moravec算子<citation id="193" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、Harris算子<citation id="194" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等) 、线特征<citation id="195" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation> (道路、建筑物的边缘或轮廓) 、面特征<citation id="199" type="reference"><link href="151" rel="bibliography" /><link href="153" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation> (湖泊、草地等) 和局部不变性特征 (SIFT特征<citation id="200" type="reference"><link href="155" rel="bibliography" /><link href="157" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>、Shape Context<citation id="196" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等) 。这些特征受影像间的非线性辐射差异影响较大, 难以反映多模态影像间的共有属性, 特征检测的重复率往往较低, 从而导致匹配效率低下, 不能较好应用于多模态遥感影像的自动匹配<citation id="197" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="60">基于区域的方法也称为模板匹配方法。首先在参考影像上选择适当尺寸的模板窗口, 然后在待匹配影像的搜索域中利用某种相似性测度进行匹配, 选择匹配窗口的中心作为同名点。常用的相似性度量有归一化互相关 (normalized cross correlation, NCC) <citation id="201" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、互信息 (mutual information, MI) <citation id="202" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等。这些相似性测度直接利用影像灰度信息计算, 对灰度信息变化敏感, 受辐射差异影响较大, 在多模态遥感影像匹配中表现不佳<citation id="203" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。最近, 有学者利用影像间的几何结构特征构建相似性测度, 该算法可以较好克服多模态影像间的辐射差异, 获得较高的匹配正确率, 但是其提取的特征冗余, 表达力不够, 而且当影像的几何结构特征不够丰富时, 其匹配性能可能会有所下降<citation id="204" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="61">随着人工智能技术的发展, 深度学习方法已深入应用于计算机视觉、图像处理和大数据等方面。在影像匹配领域, 目前已有学者利用深度学习方法开展相关研究。文献<citation id="205" type="reference">[<a class="sup">17</a>]</citation>分析了不同神经网络模型用于影像匹配的性能, 文献<citation id="206" type="reference">[<a class="sup">18</a>]</citation>将空间尺度卷积层加入卷积神经网络, 以加强整体网络的抗尺度特性。文献<citation id="207" type="reference">[<a class="sup">19</a>]</citation>利用卷积神经网络开展复杂背景条件下的影像匹配研究。目前大多数应用深度学习的匹配研究采用的是基于特征的方法, 即先利用特征提取算子在影像间检测特征点, 再利用深度学习技术构建特征点的描述子, 最后根据描述子之间的相似性识别同名点。由于首先要进行特征点检测, 特征检测的性能将大大地影响匹配效率。考虑到多模态影像间显著的辐射差异和噪声干扰, 特征检测的重复率往往较低, 即无法有效地检测到共有特征, 因此这些方法不能有效地应用于多模态影像的匹配。</p>
                </div>
                <div class="p1">
                    <p id="62">Siamese网络 (孪生网络) <citation id="208" type="reference"><link href="175" rel="bibliography" /><link href="177" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>是一种检测图像相似性的深度学习网络模型。Siamese网络具有深层次特征提取能力, 被认为是一种高效的深层网络, 并且它可以通过深层网络学习来提取影像间的共有特征, 提高影像在不同模态情况下的相似性检测性能, 能有效地抵抗影像间的非线性辐射差异。鉴于此, 本文将Siamese网络应用于多模态遥感影像匹配, 通过对其进行优化, 使该网络能有效地提取影像间的共有特征, 然后采用模板匹配策略, 避免特征检测重复率的影响, 来实现多模态遥感影像高精度匹配。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">1 基于Siamese网络的影像相似性检测</h3>
                <div class="p1">
                    <p id="64">Siamese网络是一种提取训练样本对的深层次特征用于影像相似性检测的深度学习网络模型。它包含两个分支网络和一个决策网, 如图1<citation id="209" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>所示。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Siamese网络结构" src="Detail/GetImg?filename=images/CHXB201906008_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Siamese网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Siamese network structure schematics</p>

                </div>
                <div class="p1">
                    <p id="66">在Siamese网络中, 分支网络包含卷积层、池化层、全连接层等, 决策网络可以是全连接层, 也可以是某种相似性度量算法<citation id="210" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。卷积层用于提取训练影像对的特征。池化层一方面可以非常有效地缩小矩阵尺寸, 降低输入影像或特征空间的空间分辨率, 减少特征和参数, 简化网络计算复杂度, 提高计算速度;另一方面, 池化层可以进行特征压缩, 减少特征信息, 提取主要特征, 理想状态下保留显著特征, 保持特征空间的某种不变性 (平移、旋转等) <citation id="212" type="reference"><link href="179" rel="bibliography" /><link href="181" rel="bibliography" /><sup>[<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>。全连接层相当于一个特征空间变换, 可以把特征信息进行整合, 降低特征维数, 再加上激活函数的非线性映射, 多层 (两层及以上) 全连接层理论上可以模拟任何非线性变换<citation id="211" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。Siamese网络是对影像对同时训练的网络模型, 两个分支网络的权值共享。它同时提取训练样本对的深层次特征, 然后根据特征相似性判断影像是否相似。因此, Siamese网络提取的深层次特征可反映影像间的共有属性, 深层次特征可看作样本对的共有特征。传统的Siamese网络中包含池化层, 这些网络可以较好地应用于基于特征的匹配方法。在模板匹配过程中, 池化层减少特征信息, 提取主要特征, 保持的特征不变性会使Siamese网络对小范围偏移的影像相似性检测不敏感, 无法满足模板匹配的逐像素滑动匹配的精度要求, 造成匹配正确率降低。</p>
                </div>
                <div class="p1">
                    <p id="67">本文在传统Siamese网络模型的基础上构建了一种影像相似性检测模型。该网络模型去除了特征提取过程中的池化层, 将每一个卷积层生成的特征空间完整的表达并向下层传递, 使模型提取的特征的空间信息更加完整, 定位精度更高。该网络模型可以有效地抵抗多模态遥感影像间非线性辐射差异和噪声干扰, 提取影像间共有特征的同时保证其定位精度, 实现多模态遥感影像相似性的准确检测, 并且满足模板匹配的定位精度需求, 模型如图2所示。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文网络模型" src="Detail/GetImg?filename=images/CHXB201906008_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文网络模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 The proposed network model</p>

                </div>
                <div class="p1">
                    <p id="69">该网络模型的两个分支网络完全相同且权值共享, 每个分支网络包含两部分, 特征提取和特征降维, 特征提取由6个卷积层组成, 特征降维由3个全连接层组成, 单个分支网络如图2所示。分支网络的详细参数如下</p>
                </div>
                <div class="p1">
                    <p id="70"><i>C</i> (8, 3, 1) -Relu-BN (8) -<i>C</i> (16, 7, 1) -Relu-BN (16) -<i>C</i> (32, 7, 1) -Relu-BN (32) -<i>C</i> (128, 7, 1) -Relu-BN (128) -<i>C</i> (256, 7, 1) -Relu-BN (256) -<i>C</i> (64, 7, 1) -Relu-BN (64) -F (512) -Relu-F (512) -Relu-F (300) </p>
                </div>
                <div class="p1">
                    <p id="71">式中, <i>C</i> (<i>n</i>, <i>m</i>, <i>k</i>) 表示卷积层有<i>n</i>个卷积核, 卷积核大小为<i>m</i>×<i>m</i>, 步长为<i>k</i>;Relu表示激活函数为Relu激活函数;BN (<i>n</i>) 表示<i>n</i>维样本批量标准化;<i>F</i> (<i>n</i>) 表示全连接层输出为<i>n</i>维。</p>
                </div>
                <div class="p1">
                    <p id="72">用于网络模型训练的训练影像对可分为正样本对和负样本对, 正负样本对均有标签进行区别。其中, 正样本对对应标签为0, 负样本对对应标签为1。训练损失函数如下<citation id="213" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="73"><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>ω</mi></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mi>L</mi></mstyle><mo stretchy="false"> (</mo><mi>W</mi><mo>, </mo><mo stretchy="false"> (</mo><mi>Y</mi><mo>, </mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="75"><i>L</i> (<i>W</i>, (<i>Y</i>, <i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>) <sup><i>i</i></sup>) = (1-<i>Y</i>) <i>L</i><sub><i>S</i></sub> (<i>D</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mi>i</mi></msubsup></mrow></math></mathml>) +<i>YL</i><sub><i>D</i></sub> (<i>D</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mi>i</mi></msubsup></mrow></math></mathml>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="78">式中, (<i>Y</i>, <i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>) <sup><i>i</i></sup>是第<i>i</i>对带标签的样本对;<i>L</i><sub><i>S</i></sub>代表一个正样本对的局部损失函数;<i>L</i><sub><i>D</i></sub>代表一个负样本对的局部损失函数;<i>P</i>是每次批训练的样本对的数量;<i>D</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mi>i</mi></msubsup></mrow></math></mathml>是第<i>i</i>对带标签的样本对的相似性度量。</p>
                </div>
                <div class="p1">
                    <p id="80">在训练过程中, 最小化来自正样本对的损失函数值, 最大化来自负样本对的损失函数值, 从而提取的共有特征对正负样本对有很好的区分效果, 进而提高影像对相似性检测的准确性。当样本对导入该网络模型后, 分支网络中卷积层提取样本共有特征, 全连接层将共有特征降维并以<i>n</i>维特征向量的形式表达。决策网络以欧氏距离作为相似性度量, 对训练样本对提取的共有特征计算欧氏距离并输出结果。根据欧氏距离的大小反映样本之间的相似性。</p>
                </div>
                <div class="p1">
                    <p id="81">在试验过程中发现, 特征降维会对匹配性能精度产生影响。笔者分析认为, 模板匹配过程中, 全连接层破坏特征空间结构, 使特征信息丢失表达不全面, 造成模板匹配正确率降低。因此, 这里直接将网络模型中特征提取的最后一层卷积层输出的特征代入决策网络中, 以欧氏距离作为相似性度量, 进行模板匹配, 取得了较高的匹配精度。下面通过对比试验对此进行分析。</p>
                </div>
                <div class="p1">
                    <p id="82">图3是本文方法与传统Siamese网络 (包含一个池化层) 的模板匹配正确率的进行对比。由曲线可以看出, 池化和特征降维均无法较好地应用于模板匹配, 池化和特征降维会使特征定位精度降低, 特征信息表达不完整, 曲线趋于平缓后匹配正确率分别为80%、9%。根据本文方法提取未降维特征的匹配正确率曲线在网络模型训练12轮之后趋于平缓, 匹配正确率稳定在98%, 更加适用于多模态遥感影像的匹配。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 方法对比" src="Detail/GetImg?filename=images/CHXB201906008_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 方法对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Comparison of the proposed method and traditional method</p>

                </div>
                <div class="p1">
                    <p id="84">图4是用于试验的一对SAR和光学影像, 模板尺寸为72×72像素, 通过对比水平方向偏移的相似性曲线来说明降低特征矩阵分辨率 (池化层) 对匹配性能的影响。试验过程中, 传统Siamese网络采用一层最大池化层, 池化窗口大小为2×2。从图5中可以看出, 池化层进行特征压缩, 减少特征信息, 提取主要特征, 降低特征空间分辨率, 保持特征空间不变性使网络模型对逐像素滑动匹配敏感度降低, 造成相似性曲线的峰值区域区分度不明显, 峰值无法出现在正确位置上。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 试验影像" src="Detail/GetImg?filename=images/CHXB201906008_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 试验影像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Experiment image</p>

                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 水平方向相似性曲线" src="Detail/GetImg?filename=images/CHXB201906008_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 水平方向相似性曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Horizontal similarity curves</p>

                </div>
                <h3 id="87" name="87" class="anchor-tag">2 多模态遥感影像模板匹配流程</h3>
                <div class="p1">
                    <p id="88">利用上述优化的Siamese网络模型提取多模态遥感影像间的共有特征用于影像匹配。根据模板匹配的思想, 在参考影像合适位置构建匹配模板, 在待匹配影像确定搜索域, 随后提取共有特征并计算二者的欧氏距离, 搜索域中欧氏距离最小的位置便是匹配最佳的位置。本文提出的影像匹配方法分为两个阶段, 模型训练阶段和模板匹配阶段。在模型训练阶段, 利用训练数据进行模型训练。训练数据中正样本对来自已经配准好的影像, 负样本对来自未配准的影像。每对正负样本对均包含独立标签, 用于网络模型训练。在模板匹配阶段, 首先在参考影像中提取Harris特征点, 确定模板区域, 在输入影像上确定搜索区域, 利用训练好的网络模型提取模板区域和搜索区域的共有特征并计算欧氏距离, 将欧氏距离最小的位置作为最匹配点, 直到所有特征点找到对应的匹配点。匹配流程如图6所示。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 匹配流程" src="Detail/GetImg?filename=images/CHXB201906008_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 匹配流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Matching process</p>

                </div>
                <h3 id="90" name="90" class="anchor-tag">3 试 验</h3>
                <div class="p1">
                    <p id="91">本文进行了多组试验用于验证本文方法性能, 分别从匹配正确率、均方根误差 (root mean square error, RMSE) 和相似性图等方面进行详细评价和分析并与NCC、MI、表示几何结构相似性的HOPC (histogram of orientated phase congruency) <citation id="214" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>方法以及基于传统Siamese网络的深度学习方法进行对比。试验硬件平台CPU为Inter (R) Xeon (R) E5-2640 v4 2.40 GHz, GPU为NVIDIA Tesla P40 24 GB, 内存为128 GB配置, 采用PyTorch深度学习框架实现具体卷积神经网络训练和匹配试验。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">3.1 试验准备</h4>
                <div class="p1">
                    <p id="93">本节共设置4组多模态遥感影像试验, 两组试验为光学和红外遥感影像试验, 两组试验为光学和雷达遥感影像试验 (图7) 。每组数据都利用严格几何定位模型和轨道参数进行粗纠正, 消除了影像间的旋转和尺度等几何形变, 影像间仅存在一定量的平移差异。尽管如此, 影像间仍然存在显著的非线性辐射差异, 匹配难度较大。具体试验数据如表1所示。</p>
                </div>
                <div class="area_img" id="94">
                                            <p class="img_tit">
                                                <b>表1 试验数据介绍</b>
                                                    <br />
                                                <b>Tab.1 Details of experiment data</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201906008_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 试验数据介绍" src="Detail/GetImg?filename=images/CHXB201906008_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 试验数据" src="Detail/GetImg?filename=images/CHXB201906008_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 试验数据  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Experiment data</p>

                </div>
                <h4 class="anchor-tag" id="96" name="96">3.2 训练数据集准备</h4>
                <div class="p1">
                    <p id="97">准备4组已配准的多模态遥感影像数据, 分别采集用于网络模型训练和验证的正负样本对。样本尺寸为65×65像素。每组正样本对即相同位置的影像图块。每组负样本对是随机平移若干个像素模拟未配准情况。每组样本对赋予一对相同数字的标签用以区分。在试验1中, 用于训练的正负样本对分别为59 535对和59 051对, 用于验证的正负样本对分别为4990对和4980对。在试验2中, 用于训练的正负样本对分别为62 500对和58 186对, 用于验证的正负样本对分别为5300对和5152对。在试验3中, 用于训练的正负样本对分别为60 577对和50 278对, 用于验证的正负样本对分别为5530对和5400对。在试验4中, 用于训练的正负样本对分别为56 147对和56 147对, 用于验证的正负样本对分别为5210对和5330对。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">3.3 模型训练</h4>
                <div class="p1">
                    <p id="99">每组试验网络模型训练的过程相同。以试验1数据为例, 模型训练以64个样本对为一组做批次训练, 一轮训练需1852批次 (根据样本对数量而定) , 模型训练的迭代轮数最大为50轮, 当每轮训练损失函数值之差小于0.001时停止训练。梯度优化算法使用随机梯度下降 (stochastic gradient descent, SGD) , 学习率为0.001。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">3.4 性能对比</h4>
                <div class="p1">
                    <p id="101">为了验证本文方法的匹配性能, 这里与其他模板匹配方法 (NCC、MI、HOPC) 在匹配正确率 (容差为1.5个像素) 、RMSE和相似性图进行结果对比和详细分析。由于本文方法的训练样本和匹配模板均使用65×65像素, 为了保证不同方法性能对比的一致性和客观性, 其他模板匹配方法也使用相同尺寸的模板窗口。在匹配过程中, 首先采用Harris特征点在参考影像上提取均匀分布<citation id="215" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>的200个特征点, 然后在待匹配影像上确定21×21像素的搜索区域, 并进行模板匹配。</p>
                </div>
                <div class="p1">
                    <p id="102">为了更直观地定性比较几种方法的匹配效果, 本文将滑动匹配过程中计算得到的搜索域的欧氏距离可视化表达成相似性图, 将NCC、MI、HOPC和传统Siamese网络方法类似地进行可视化表达。为了统一标准和便于比较, 本文首先对不同数据归一化处理然后再进行可视化表达。相似性如图8所示, 自上至下分别为4组试验, 第1列为模板影像, 第7列为待匹配影像, 第2至6列分别为NCC、MI、HOPC、传统Siamese网络方法和本文方法的相似性图。红色表示相似性越高, 蓝色表示相似性越低。参考影像和待匹配影像是完全配准的, 因此当模板滑动位于待匹配影像中心位置时, 模板处于正确的匹配位置。从图8中易看出, NCC、MI方法抵抗多模态影像非线性辐射差异效果不佳, 未能找到正确匹配位置;HOPC方法和传统Siamese网络方法的匹配位置与中心位置有偏差, 相似性图存在双峰或多峰或峰值不明显, 峰值范围大;本文方法相似性图清晰简单, 峰值位置处于搜索域中心位置, 峰值范围小, 定位准确, 性能最优。</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 相似性图" src="Detail/GetImg?filename=images/CHXB201906008_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 相似性图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Similarity maps</p>

                </div>
                <div class="p1">
                    <p id="104">表2中显示了4组试验不同方法的匹配正确率和RMSE。总体来看, NCC方法在4组试验中匹配正确率最低。这是因为NCC方法对于影像间的灰度只具有线性不变性, 当辐射差异较大, 尤其是非线性的辐射差异时, NCC方法通常不能得到满意的匹配效果。MI方法匹配正确率虽优于NCC方法, 但整体匹配正确率较低, 4组试验中匹配正确率最高为40.5%, 最低仅22%, 无法满足模板匹配的准确率要求。这是因为MI方法是根据像元灰度值的概率分布计算信息熵实现匹配的, 需要大量计算影像灰度直方图, 容易出现局部极值产生误匹配现象<citation id="216" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。HOPC方法利用影像间的几何结构信息, 获得较高的匹配正确率。但是, 由于4组试验影像的几何结构信息丰富程度不尽相同, 4组试验匹配正确率起伏较大, 该方法同样存在局限性。基于传统Siamese网络的深度学习方法由于网络中存在池化层使得特征信息减少, 特征定位精度降低, 从而造成该方法匹配正确率较低。</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表2 试验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.2 Comparison of experiment results</b></p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br /></td><td></td><td>NCC</td><td>MI</td><td>HOPC</td><td>传统Siamese网络</td><td>本文方法</td></tr><tr><td rowspan="2"><br />试验1</td><td>匹配正确率/ (%) </td><td>7 </td><td>23</td><td>81.5</td><td>79 </td><td>99 </td></tr><tr><td><br />RMSE</td><td>9.369</td><td>7.859</td><td>2.658</td><td>3.355</td><td>0.854</td></tr><tr><td colspan="7"><br /></td></tr><tr><td rowspan="2"><br />试验2</td><td>匹配正确率/ (%) </td><td>16.5</td><td>26</td><td>97 </td><td>92.5</td><td>98.5</td></tr><tr><td><br />RMSE</td><td>9.504</td><td>6.408</td><td>1.568</td><td>1.530</td><td>0.510</td></tr><tr><td colspan="7"><br /></td></tr><tr><td rowspan="2"><br />试验3</td><td>匹配正确率/ (%) </td><td>3 </td><td>60</td><td>90 </td><td>86.5</td><td>97 </td></tr><tr><td><br />RMSE</td><td>10.780</td><td>5.642</td><td>2.776</td><td>1.856</td><td>1.432</td></tr><tr><td colspan="7"><br /></td></tr><tr><td rowspan="2"><br />试验4</td><td>匹配正确率/ (%) </td><td> 6.5</td><td>22</td><td>89.5</td><td>80.5</td><td>97.5</td></tr><tr><td><br />RMSE</td><td>10.023</td><td>7.210</td><td>1.718</td><td>3.11</td><td>0.959</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="106">本文方法在匹配正确率上表现最佳, 均保持95%以上的匹配正确率。这说明了本文方法通过对Siamese网络模型进行优化, 可有效地提取多模态影像间的共有特征, 增强了影像匹配的稳健性。另外本文方法的RMSE最小, 这说明本文方法的匹配精度最高。如图9所示, 本文方法的匹配点都非常正确地定位在多模态影像的同名区域。这些试验表明了本文方法在匹配稳定性和精确度方面都优于其他4种方法, 可有效地实现多模态遥感影像的自动匹配。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906008_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 试验结果" src="Detail/GetImg?filename=images/CHXB201906008_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 试验结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906008_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Experiment results</p>

                </div>
                <h3 id="108" name="108" class="anchor-tag">4 结 论</h3>
                <div class="p1">
                    <p id="109">针对多模态遥感影像匹配中非线性辐射差异引起的困难问题, 本文将深度学习方法引入模板匹配中, 构建了一种提取影像间的共有特征的Siamese网络模型, 对此进行优化, 使其适用于多模态遥感影像匹配。为了验证本文方法的性能和表现, 将其与NCC、MI、HOPC、基于传统Siamese网络的深度学习等方法进行详细的对比与分析, 结果表明:本文方法在多组多模态遥感影像匹配试验中匹配正确率最高, 匹配精度较传统的NCC、MI和传统Siamese网络方法有明显提升, 并且优于目前精度较高的HOPC方法。这说明本文方法可以有效抵抗多模态遥感影像间辐射差异, 获得高精度的同名匹配点, 从而获得可靠的匹配精度。</p>
                </div>
                <div class="p1">
                    <p id="110">由于本文方法需要事先人工配准影像进行训练数据的采集, 因此, 本文方法在模型训练阶段较为耗时, 存在局限性。此外, 模型结构仍有优化空间, 训练样本的数量和质量与匹配准确度之间的关系还需要进一步的探究。本文的试验前提是利用卫星影像的轨道参数和严格定位模型对其进行粗纠正, 消除影像间的旋转和尺度差异。因此, 对于具有显著旋转和尺度变化的多模态数据的匹配, 有待进一步研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="137">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201801010&amp;v=MDA0MjVMRzRIOW5Ncm85RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2xVcnJQSmlYVGI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>闫利, 王紫琦, 叶志云.顾及灰度和梯度信息的多模态影像配准算法[J].测绘学报, 2018, 47 (1) :71-81.DOI:10.11947/j.AGCS.2018.20170368.YAN Li, WANG Ziqi, YE Zhiyun.Multimodal image registration algorithm considering grayscale and gradient information[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (1) :71-81.DOI:10.11947/j.AGCS.2018.20170368.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Registration of Multimodal Remote Sensing Images Based on Structural Similarity">

                                <b>[2]</b>YE Yuanxin, SHAN Jie, BRUZZONE L, et al.Robust registration of multimodal remote sensing images based on structural similarity[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (5) :2941-2958.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349910&amp;v=MTA4MjBIdERPclk5RVorOEdCWDA1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVNzdJSlY0VmJ4VT1OaWZPZmJLNw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>ZITOVB, FLUSSER J.Image registration methods:a survey[J].Image and Vision Computing, 2003, 21 (11) :977-1000.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201311030&amp;v=MjI2NDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnkzbFVyclBJalhCWTdHNEg5TE5ybzlHWklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>余先川, 吕中华, 胡丹.遥感图像配准技术综述[J].光学精密工程, 2013, 21 (11) :2960-2972.YU Xianchuan, LZhonghua, HU Dan.Review of remote sensing image registration techniques[J].Optics and Precision Engineering, 2013, 21 (11) :2960-2972.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards automatic visual obstacle avoidance">

                                <b>[5]</b>MORAVEC H P.Towards automatic visual obstacle avoidance[C]∥Proceedings of the 5th International Joint Conference on Artificial Intelligence.Massachusetts US:MIT, 1977.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Combined Corner and Edge Detector">

                                <b>[6]</b>HARRIS C, STEPHENS M.A combined corner and edge detector[C]∥Proceedings of the 4th Alvey Vision Conference.Manchester, UK:Organising Committee AVC, 1988:147-151.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Edge detection techniques:an overview">

                                <b>[7]</b>ZIOU D, TABBONE S.Edge detection techniques:an overview[J].International Journal of Pattern Recognition and Image Analysis, 1998 (8) :537-559.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic registration for ASAR and TM images based on region features">

                                <b>[8]</b>ZHANG Dengrong, YU Le, CAI Zhigang.Automatic registration for ASAR and TM images based on region features[C]∥Proceedings of SPIE 6752, Geoinformatics 2007:Remotely Sensed Data and Information.Nanjing:SPIE, 2007:6752.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GCSX201101004&amp;v=MjY3MDBJaTdZZHJHNEg5RE1ybzlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnkzbFVyclA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>张宝尚, 田铮, 延伟东.基于分割区域的SAR图像配准方法研究[J].工程数学学报, 2011, 28 (1) :7-14.ZHANG Baoshang, TIAN Zheng, YAN Weidong.An SAR image registration algorithm based on segmentationderived regions[J].Chinese Journal of Engineering Mathematics, 2011, 28 (1) :7-14.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object recognition from local scale-invariant features">

                                <b>[10]</b>LOWE D G.Object recognition from local scale-invariant features[C]∥Proceeding of the 7th International Conference on Computer Vision.Kerkyra, Corfu, Greece:IEEE, 1999:1150-1157.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjkyMzhubFZyL0pKVms9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZ5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape matching and object recognition using shape contexts">

                                <b>[12]</b>BELONGIE S, MALIK J, PUZICHA J.Shape matching and object recognition using shape contexts[J].IEEETransactions on Pattern Analysis and Machine Intelligence, 2002, 24 (4) :509-522.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mutual-information-based registration of TerraSAR-X and ikonos imagery in Urban areas">

                                <b>[13]</b>SURI S, REINARTZ P.Mutual-information-based registration of TerraSAR-X and Ikonos imagery in urban areas[J].IEEE Transactions on Geoscience and Remote Sensing, 2010, 48 (2) :939-949.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A correlation-symbolic approach to automatic remotely sensed image rectification">

                                <b>[14]</b>MARTINEZ A, GARCIA-CONSUEGRA J, ABAD F.Acorrelation-symbolic approach to automatic remotely sensed image rectification[C]∥Proceedings of IEEE 1999International Geoscience and Remote Sensing Symposium.Hamburg, Germany:IEEE, 1999:336-338.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mutual information as a similarity measure for remote sensing image registration">

                                <b>[15]</b>JOHNSON K, COLE-RHODES A, ZAVORIN I, et al.Mutual information as a similarity measure for remote sensing image registration[C]∥Proceedings of SPIE4383, Geo-Spatial Image and Data ExploitationⅡ.Orlando, FL, United States:SPIE, 2001:51-61.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES408212CE209B01E9BA69B2253BFDE9DC&amp;v=MTk5OTFobXpsME9uM2dxUkZIRDhiaFRNN3NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDU5bGh3YmkreGE4PU5pZk9mYmU0RnRQTnJmd3dadXNHZm53NHVoOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b>YE Yuanxin, SHAN Jie, HAO Siyuan, et al.A local phase based invariant feature for remote sensing image matching[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2018 (142) :205-221.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to compare image patches via convolutional neural networks">

                                <b>[17]</b>ZAGORUYKO S, KOMODAKIS N.Learning to compare image patches via convolutional neural networks[C]∥Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston:IEEE, 2015:4353-4361.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806018&amp;v=MDcxMTMzenFxQnRHRnJDVVI3cWZadWR2RnkzbFVyclBKaVhUYkxHNEg5bk1xWTlFYklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b>范大昭, 董杨, 张永生.卫星影像匹配的深度卷积神经网络方法[J].测绘学报, 2018, 47 (6) :844-853.DOI:10.11947/j.AGCS.2018.20170627.FAN Dazhao, DONG Yang, ZHANG Yongsheng.Satellite image matching method based on deep convolution neural network[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (6) :844-853.DOI:10.11947/j.AGCS.2018.20170627.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Matching of remote sensing images with complex background variations via Siamese convolutional neural network">

                                <b>[19]</b>HE Haiqing, CHEN Min, CHEN Ting, et al.Matching of remote sensing images with complex background variations via Siamese convolutional neural network[J].Remote Sensing, 2018, 10 (3) :355.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Signature verification using a&amp;quot;Siamese&amp;quot;time delay neural network">

                                <b>[20]</b>BROMLEY J, GUYON I, LECUN Y, et al.Signature verification using a“Siamese”time delay neural network[C]∥Proceedings of the 6th International Conference on Neural Information Processing Systems.Denver, Colorado:Morgan Kaufmann Publishers Inc, 1993.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a similarity metric discriminatively with application to face verification">

                                <b>[21]</b>CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with Application to Face Verification∥Proceeding of 2005IEEE Computer Society Conference on Computer Vision and Pattern Recognition.San Diego, CA:IEEE, 2005.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018709715.nh&amp;v=MjU2MzNwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTNsVXJyUFZGMjZGclM0RjliTnE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>许慧敏.基于深度学习U-Net模型的高分辨率遥感影像分类方法研究[D].成都:西南交通大学, 2018.XU Huimin.Method research of high resolution remote sensing imagery classification based on U-Net model of deep learning[D].Chengdu:Southwest Jiaotong University, 2018.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MTM5NTVxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2xVcnJQTHo3QmRyRzRIOWJNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b>周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251.ZHOU Feiyan, JIN Linpeng, DONG Jun.Review of convolutional neural network[J].Chinese Journal of Computers, 2017, 40 (6) :1229-1251.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dimensionality Reduction by Learning an Invariant Mapping">

                                <b>[24]</b>HADSELL R, CHOPRA S, LECUN Y.Dimensionality reduction by learning an invariant mapping[C]∥Proceedins of 2006IEEE Computer Society Conference on Computer Vision and Pattern Recognition.New York:IEEE, 2006.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501643843&amp;v=MTU4ODNHZXJxUVRNbndaZVp0RmlubFU3N0lKVjRWYnhVPU5pZk9mYks3SHRETnFvOUVZdThNQkhnNm9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>TRAPPEY A J C, HSU F C, TRAPPEY C V, et al.Development of a patent document classification and search platform using a back-propagation network[J].Expert Systems with Applications, 2006, 31 (4) :755-765.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300100884&amp;v=MjY3NzhubFU3N0lKVjRWYnhVPU5pZk9mYks4SHRMT3JJOUZaZXNQQkhROW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>YE Yuanxin, SHAN Jie.A local descriptor based registration method for multispectral remote sensing images with nonlinear intensity differences[J].ISPRS Journal of Photogrammetry and Remote Sensing, 2014 (90) :83-95.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201014075&amp;v=MTE3NTBkdkZ5M2xVcnJQTHo3QmJiRzRIOUhOcTQ5Q1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b>林善明, 朱小艳, 周建华, 等.一种结合互信息和模板匹配的配准方法[J].计算机工程, 2010, 36 (14) :198-200.LIN Shanming, ZHU Xiaoyan, ZHOU Jianhua, et al.Registration method with mutual information and template matching[J].Computer Engineering, 2010, 36 (14) :198-200.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201906008" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906008&amp;v=MDYzODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2xVcnJQSmlYVGJMRzRIOWpNcVk5RmJJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
