<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142603001201250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201911008%26RESULT%3d1%26SIGN%3dmLhOIg7j8EqCm0HVjKAoOhZn2Vs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201911008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201911008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201911008&amp;v=MTY4MTFOcm85RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cm5Xci9BSmlYVGJMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="1 图像空间向地理空间的映射关系模型 ">1 图像空间向地理空间的映射关系模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="2 前景目标及其跟踪轨迹的提取与表达 ">2 前景目标及其跟踪轨迹的提取与表达</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="3 前景动态目标与地理信息的融合模式 ">3 前景动态目标与地理信息的融合模式</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="4 试 验 ">4 试 验</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="5 结 论 ">5 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 实时检测跟踪流程">图1 实时检测跟踪流程</a></li>
                                                <li><a href="#102" data-title="图2 地理信息与视频动态前景目标信息的融合模式">图2 地理信息与视频动态前景目标信息的融合模式</a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表1 世界坐标与图像坐标对应点对&lt;/b&gt;"><b>表1 世界坐标与图像坐标对应点对</b></a></li>
                                                <li><a href="#115" data-title="图3 监控视频地理映射前后对比">图3 监控视频地理映射前后对比</a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;表2 多目标跟踪量化评价结果&lt;/b&gt;"><b>表2 多目标跟踪量化评价结果</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表3 映射坐标对应及误差分析&lt;/b&gt;"><b>表3 映射坐标对应及误差分析</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表4 实时性分析结果&lt;/b&gt;"><b>表4 实时性分析结果</b></a></li>
                                                <li><a href="#128" data-title="图4 原始视频与跟踪视频对应帧">图4 原始视频与跟踪视频对应帧</a></li>
                                                <li><a href="#129" data-title="图5 多目标跟踪轨迹结果">图5 多目标跟踪轨迹结果</a></li>
                                                <li><a href="#130" data-title="图6 试验区底图">图6 试验区底图</a></li>
                                                <li><a href="#131" data-title="图7 4种融合模式对比">图7 4种融合模式对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" 韩志刚,孔云峰,秦耀辰.地理表达研究进展[J].地理科学进展,2011,30(2):141-148.HAN Zhigang,KONG Yunfeng,QIN Yaochen.Research on geographic representation:a review[J].Progress in Geography,2011,30(2):141-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DLKJ201102001&amp;v=MTUyNTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJuV3IvQUlTSEFaTEc0SDlETXJZOUZaWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         韩志刚,孔云峰,秦耀辰.地理表达研究进展[J].地理科学进展,2011,30(2):141-148.HAN Zhigang,KONG Yunfeng,QIN Yaochen.Research on geographic representation:a review[J].Progress in Geography,2011,30(2):141-148.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" YUAN M,MARK D M,EGENHOFER M,et al.Extensions to geographic representations[M]//MCMASTER R,USERY L.A Research Agenda for Geographic Information Science.Boca Raton,FL:CRC Press,2004:129-156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extensions to geographic representations">
                                        <b>[2]</b>
                                         YUAN M,MARK D M,EGENHOFER M,et al.Extensions to geographic representations[M]//MCMASTER R,USERY L.A Research Agenda for Geographic Information Science.Boca Raton,FL:CRC Press,2004:129-156.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" LIPPMAN A.Movie-maps:An application of the optical videodisc to computer graphics[J].ACM SIGGRAPH Computer Graphics,1980,14(3):32-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000048162&amp;v=MzAzNzRud1plWnRGaW5sVTc3SUlsd2RhaG89TmlmSVk3SzdIdGpOcjQ5RlpPOEhEWG83b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         LIPPMAN A.Movie-maps:An application of the optical videodisc to computer graphics[J].ACM SIGGRAPH Computer Graphics,1980,14(3):32-42.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" BILL R.Multimedia GIS-definition,requirements and applications[M]//The 1994 European GIS Yearbook.Oxford:Blackwell,1994:151-154." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimedia GIS:definition, requirements and applications">
                                        <b>[4]</b>
                                         BILL R.Multimedia GIS-definition,requirements and applications[M]//The 1994 European GIS Yearbook.Oxford:Blackwell,1994:151-154.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" BERRY J K.Capture “where” and “when” on video-based GIS[J].GeoWorld,2000,13(9):26-27." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Capture′Where′and′When′on video-based GIS">
                                        <b>[5]</b>
                                         BERRY J K.Capture “where” and “when” on video-based GIS[J].GeoWorld,2000,13(9):26-27.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" 谢潇,朱庆,张叶廷,等.多层次地理视频语义模型[J].测绘学报,2015,44(5):555-562.DOI:10.11947/j.AGCS.2015.20140176.XIE Xiao,ZHU Qing,ZHANG Yeting,et al.Hierarchical semantic model of geovideo[J].Acta Geodaetica et Cartographica Sinica,2015,44(5):555-562.DOI:10.11947/j.AGCS.2015.20140176." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201505014&amp;v=MDEwNTZxQnRHRnJDVVI3cWZadWR2RnlybldyL0FKaVhUYkxHNEg5VE1xbzlFWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         谢潇,朱庆,张叶廷,等.多层次地理视频语义模型[J].测绘学报,2015,44(5):555-562.DOI:10.11947/j.AGCS.2015.20140176.XIE Xiao,ZHU Qing,ZHANG Yeting,et al.Hierarchical semantic model of geovideo[J].Acta Geodaetica et Cartographica Sinica,2015,44(5):555-562.DOI:10.11947/j.AGCS.2015.20140176.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" 尹宏鹏,陈波,柴毅,等.基于视觉的目标检测与跟踪综述[J].自动化学报,2016,42(10):1466-1489.YIN Hongpeng,CHEN Bo,CHAI Yi,et al.Vision-based object detection and tracking:a review[J].Acta Automatica Sinica,2016,42(10):1466-1489." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610002&amp;v=MTMzOTVGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlybldyL0FLQ0xmWWJHNEg5Zk5yNDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         尹宏鹏,陈波,柴毅,等.基于视觉的目标检测与跟踪综述[J].自动化学报,2016,42(10):1466-1489.YIN Hongpeng,CHEN Bo,CHAI Yi,et al.Vision-based object detection and tracking:a review[J].Acta Automatica Sinica,2016,42(10):1466-1489.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" 徐光祐,曹媛媛.动作识别与行为理解综述[J].中国图象图形学报,2009,14(2):189-195.XU Guangyou,CAO Yuanyuan.Action recognition and activity understanding:a review[J].Journal of Image and Graphics,2009,14(2):189-195." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200902002&amp;v=MDczMDNVUjdxZlp1ZHZGeXJuV3IvQVB5cmZiTEc0SHRqTXJZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         徐光祐,曹媛媛.动作识别与行为理解综述[J].中国图象图形学报,2009,14(2):189-195.XU Guangyou,CAO Yuanyuan.Action recognition and activity understanding:a review[J].Journal of Image and Graphics,2009,14(2):189-195.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" LUO Wenhan,XING Junjiang,MILAN A,et al.Multiple object tracking:a literature review[Z].arXiv:1409.7618,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiple object tracking:a literature review">
                                        <b>[9]</b>
                                         LUO Wenhan,XING Junjiang,MILAN A,et al.Multiple object tracking:a literature review[Z].arXiv:1409.7618,2015.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" GIRGENSOHN A,SHIPMAN F,TURNER T,et al.Effects of presenting geographic context on tracking activity between cameras[C]//Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.San Jose,California,USA:ACM,2007:1167-1176." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effects of presenting geographic context on tracking activity between cameras">
                                        <b>[10]</b>
                                         GIRGENSOHN A,SHIPMAN F,TURNER T,et al.Effects of presenting geographic context on tracking activity between cameras[C]//Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.San Jose,California,USA:ACM,2007:1167-1176.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" LEWIS P,FOTHERINGHAM S,WINSTANLEY A.Spatial video and GIS[J].International Journal of Geographical Information Science,2011,25(5):697-716." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD120604028048&amp;v=MTI2NTl2Rnlqa1U3M0tLVjRjTmpuQmFySzZIdGZNcTQ5SGJPc0xCQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVa&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         LEWIS P,FOTHERINGHAM S,WINSTANLEY A.Spatial video and GIS[J].International Journal of Geographical Information Science,2011,25(5):697-716.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" SOURIMANT G,MORIN L,BOUATOUCH K.GPS,GIS and video registration for building reconstruction[C]//Proceedings of 2007 IEEE International Conference on Image Processing.San Antonio,TX,USA:IEEE,2007:VI - 401-VI - 404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPS, GIS and Video Registration for Building Reconstruction">
                                        <b>[12]</b>
                                         SOURIMANT G,MORIN L,BOUATOUCH K.GPS,GIS and video registration for building reconstruction[C]//Proceedings of 2007 IEEE International Conference on Image Processing.San Antonio,TX,USA:IEEE,2007:VI - 401-VI - 404.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" XIE Yujia,WANG Meizhen,LIU Xuejun,et al.Surveillance video synopsis in GIS[J].ISPRS International Journal of Geo-Information,2017,6(11):333." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Surveillance video synopsis in GIS">
                                        <b>[13]</b>
                                         XIE Yujia,WANG Meizhen,LIU Xuejun,et al.Surveillance video synopsis in GIS[J].ISPRS International Journal of Geo-Information,2017,6(11):333.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" 宋宏权,孔云峰.Flex框架下网络视频GIS设计与实现[J].测绘科学,2010,35(5):208-210,151.SONG Hongquan,KONG Yunfeng.Design and implementation of a web-based VideoGIS using Adobe Flex[J].Science of Surveying &amp;amp; Mapping,2010,35(5):208-210,151." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHKD201005072&amp;v=MDg3NzVDWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlybldyL0FKaVhBYXJHNEg5SE1xbzk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         宋宏权,孔云峰.Flex框架下网络视频GIS设计与实现[J].测绘科学,2010,35(5):208-210,151.SONG Hongquan,KONG Yunfeng.Design and implementation of a web-based VideoGIS using Adobe Flex[J].Science of Surveying &amp;amp; Mapping,2010,35(5):208-210,151.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" 韩志刚,曾明,孔云峰.校园地理视频监控WebGIS系统设计与实现[J].测绘科学,2012,37(1):195-197.HAN Zhigang,ZENG Ming,KONG Yunfeng.Design and implementation of the campus geovideo monitoring Web GIS system[J].Science of Surveying &amp;amp; Mapping,2012,37(1):195-197." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHKD201201068&amp;v=MTQ0NTF2RnlybldyL0FKaVhBYXJHNEg5UE1ybzlEYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         韩志刚,曾明,孔云峰.校园地理视频监控WebGIS系统设计与实现[J].测绘科学,2012,37(1):195-197.HAN Zhigang,ZENG Ming,KONG Yunfeng.Design and implementation of the campus geovideo monitoring Web GIS system[J].Science of Surveying &amp;amp; Mapping,2012,37(1):195-197.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" CHEN Kuanwen,LEE P J,HUNG Y P.Egocentric view transition for video monitoring in a distributed camera network[C]//Proceedings of International Conference on Advances in Multimedia Modeling.Berlin:Springer-Verlag,2011:171-181." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Egocentric view transition for video monitoring in a distributed camera network">
                                        <b>[16]</b>
                                         CHEN Kuanwen,LEE P J,HUNG Y P.Egocentric view transition for video monitoring in a distributed camera network[C]//Proceedings of International Conference on Advances in Multimedia Modeling.Berlin:Springer-Verlag,2011:171-181.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" WANG Yi,BOWMAN D A.Effects of navigation design on contextualized video interfaces[C]//Proceedings of 2011 IEEE Symposium on 3D User Interfaces.Singapore:IEEE,2011:27-34." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effects of navigation design on contextualized video interfaces">
                                        <b>[17]</b>
                                         WANG Yi,BOWMAN D A.Effects of navigation design on contextualized video interfaces[C]//Proceedings of 2011 IEEE Symposium on 3D User Interfaces.Singapore:IEEE,2011:27-34.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" ZHANG Z.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2000,22(11):1330-1334." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Flexible New Technique for Camera Calibration">
                                        <b>[18]</b>
                                         ZHANG Z.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2000,22(11):1330-1334.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_19" title=" LEPETIT V,MORENO-NOGUER F,FUA P.EP&lt;i&gt;n&lt;/i&gt;P:An accurate O(&lt;i&gt;n&lt;/i&gt;)solution to the P&lt;i&gt;n&lt;/i&gt;P problem[J].International Journal of Computer Vision,2009,81(2):155-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003394813&amp;v=MDU1MzZTWHFScnhveGNNSDdSN3FkWitadUZ5bmxVYjNCSUZZPU5qN0Jhck80SHRIUHJJWkJiT29NWTNrNXpCZGg0ajk5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         LEPETIT V,MORENO-NOGUER F,FUA P.EP&lt;i&gt;n&lt;/i&gt;P:An accurate O(&lt;i&gt;n&lt;/i&gt;)solution to the P&lt;i&gt;n&lt;/i&gt;P problem[J].International Journal of Computer Vision,2009,81(2):155-166.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_20" title=" PENATE-SANCHEZ A,ANDRADE-CETTO J,MORENO-NOGUER F.Exhaustive linearization for robust camera pose and focal length estimation[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2013,35(10):2387-2400." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation">
                                        <b>[20]</b>
                                         PENATE-SANCHEZ A,ANDRADE-CETTO J,MORENO-NOGUER F.Exhaustive linearization for robust camera pose and focal length estimation[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2013,35(10):2387-2400.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_21" title=" ZIVKOVIC Z.Improved adaptive Gaussian mixture model for background subtraction[C]//Proceedings of the 17th International Conference on Pattern Recognition.Cambridge,UK:IEEE,2004:28-31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved adaptive Gaussian mixture model for background subtraction">
                                        <b>[21]</b>
                                         ZIVKOVIC Z.Improved adaptive Gaussian mixture model for background subtraction[C]//Proceedings of the 17th International Conference on Pattern Recognition.Cambridge,UK:IEEE,2004:28-31.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     WOJKE N,BEWLEY A,PAULUS D.Simple online and realtime tracking with a deep association metric[C]//Proceedings of 2017 IEEE International Conference on Image Processing.Beijing,China:IEEE,2017:3645-3649.</a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_23" title=" KALMAN R E.A new approach to linear filtering and prediction problems[J].Journal of Basic Engineering,1960,82(1):35-45." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new approach to linear filtering and prediction problems">
                                        <b>[23]</b>
                                         KALMAN R E.A new approach to linear filtering and prediction problems[J].Journal of Basic Engineering,1960,82(1):35-45.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_24" title=" KUHNH W.The Hungarian method for the assignment problem[J].Naval Research Logistics Quarterly,1955,2(1-2):83-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The hungarian method for the assignment problem">
                                        <b>[24]</b>
                                         KUHNH W.The Hungarian method for the assignment problem[J].Naval Research Logistics Quarterly,1955,2(1-2):83-97.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_25" title=" BERNARDIN K,STIEFELHAGEN R.Evaluating multiple object tracking performance:the clear MOT metrics[J].EURASIP Journal on Image &amp;amp; Video Processing,2008(1):246309." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluating multipe object tracking performance: theCLEARMOT metrics">
                                        <b>[25]</b>
                                         BERNARDIN K,STIEFELHAGEN R.Evaluating multiple object tracking performance:the clear MOT metrics[J].EURASIP Journal on Image &amp;amp; Video Processing,2008(1):246309.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_26" title=" 葛宝义,左宪章,胡永江,视觉目标跟踪方法研究综述[J].中国图像图形学报,2008,23(8):1091-1107.GE Baoyi,ZUO Xianzhang,HU Yongjiang.Review of visual object tracking technology[J].Journal of Image and Graphics,2018,23(8):1091-1107." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201808001&amp;v=MjA5ODZxZlp1ZHZGeXJuV3IvQVB5cmZiTEc0SDluTXA0OUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         葛宝义,左宪章,胡永江,视觉目标跟踪方法研究综述[J].中国图像图形学报,2008,23(8):1091-1107.GE Baoyi,ZUO Xianzhang,HU Yongjiang.Review of visual object tracking technology[J].Journal of Image and Graphics,2018,23(8):1091-1107.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(11),1415-1423 DOI:10.11947/j.AGCS.2019.20180572            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>监控视频中动态目标与地理空间信息的融合与可视化方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%97%AD&amp;code=43201361&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张旭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%9D%E5%90%91%E9%98%B3&amp;code=20313407&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郝向阳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BB%BA%E8%83%9C&amp;code=20421498&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李建胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%9C%8B%E6%9C%88&amp;code=41879085&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李朋月</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%88%AA%E5%A4%A9%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6&amp;code=1702146&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">航天工程大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学地理空间信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>监控视频的动态前景目标智能分析是平安城市、智慧园区等安防建设的重要基础,将监控视频与地理空间数据融合可为静态的地理数据赋予动态属性。针对传统监控视频与地理信息数据集成仅仅将视频数据投射至地理空间,造成存储难、视频内容理解难度大等问题,本文提出了前景动态目标与地理空间信息的融合模型,通过推导出的映射模型将图像空间中的动态前景目标及跟踪轨迹映射至地理空间中,达到将监控视频与地理信息有机融合的目的。根据不同的应用需求,本文设计了4种多图层融合显示模式,实现了监控视频中的动态前景目标在地理空间的可视化。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%91%E6%8E%A7%E8%A7%86%E9%A2%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">监控视频;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E6%80%81%E5%89%8D%E6%99%AF%E7%9B%AE%E6%A0%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动态前景目标;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">地理空间映射;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">地理空间信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E8%A7%86%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可视化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张旭(1994—),男,硕士生,研究方向为计算机视觉、导航定位与位置服务。E-mail:xuzhang_2016@126.com;
                                </span>
                                <span>
                                    *郝向阳 E-mail:xiangyanghao2004@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家863计划(2015AA7034057A);</span>
                                <span>国家自然科学基金(61173077);</span>
                    </p>
            </div>
                    <h1>Fusion and visualization method of dynamic targets in surveillance video with geospatial information</h1>
                    <h2>
                    <span>ZHANG Xu</span>
                    <span>HAO Xiangyang</span>
                    <span>LI Jiansheng</span>
                    <span>LI Pengyue</span>
            </h2>
                    <h2>
                    <span>Space Enginerring University</span>
                    <span>Institute of Geographical Spatial Information,Information Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The dynamic foreground target analysis of surveillance video is an important basis for security construction in safe cities and smart campuses. The dynamic attributes can be assigned to static geographic data by integrating surveillance video with geospatial data. For the integration of traditional surveillance video and geographic information data, only video data is projected into geospatial, which causes storage issue and difficult in understanding video content. The model that integrates geographic information and foreground dynamic targets, and integrates surveillance video with geographic information is proposed in this paper. And the dynamic foreground target and the tracking trajectory in the image are projected into the geospatial by the derived mapping model. The multi-layer fusion mode is utilized to distribute the video dynamic summary content in geospatial space.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=surveillance%20video&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">surveillance video;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dynamic%20foreground%20target&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dynamic foreground target;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=geospatialm%20apping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">geospatialm apping;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=geospatial%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">geospatial information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=information%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">information fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visualization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visualization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Xu(1994—),male,postgraduate,majors in navigation, positioning and location-based services.E-mail: xuzhang_2016@126.com;
                                </span>
                                <span>
                                    HAO Xiangyang E-mail: xiangyanghao2004@163.com;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-10</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National High-tech Research and Development Program of China(863 Program)(No.2015AA7034057A);</span>
                                <span>The National Natural Science Foundation of China(No.61173077);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="54">地理信息的表达内容涉及地理实体及其空间关系、不确定性、地理动态及地理本体等方面<citation id="133" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。地理数据包括空间位置、属性特征及时态特征3个部分,是对于不同的地理实体、地理要素、地理现象、地理事件、地理过程等的表达<citation id="134" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。传统的地理信息表达以二维地图为主导,展现地理空间的静态属性。为解决地理数据的多维表达,使以地理方式看待世界时更加贴近人的视角,同时附加所不具备的地理属性,将视频与地理信息融合的地理超媒体的表达方式成为研究的新方向<citation id="135" type="reference"><link href="6" rel="bibliography" /><link href="8" rel="bibliography" /><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="55">目前对于监控视频的智能分析,无论是目标检测<citation id="136" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>还是跟踪行为理解<citation id="137" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等计算机视觉任务,仅仅是基于视频影像本身,其中影像目标的检测与跟踪<citation id="138" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等的精度是研究者们追求的主要目标,所得到的分析结果也仅仅是影像坐标。以监控视频的目标跟踪为例,监控者们更想得到的是实际地理位置和目标的动态方位、速度、运动轨迹等信息,而单纯的视频目标跟踪无法完成该任务,将地理空间信息与视频融合可有效解决这一问题。文献<citation id="139" type="reference">[<a class="sup">10</a>]</citation>描绘出摄像机的位置以及视图方向将视频影像置入“附近”视图,来进行跨摄像机的跟踪。文献<citation id="140" type="reference">[<a class="sup">11</a>]</citation>认为空间视频具有巨大的潜力,在GIS中使用基本数据类型来建模空间视频,使用Viewpoint数据结构表示视频帧来进行视频的地理空间分析。文献<citation id="141" type="reference">[<a class="sup">12</a>]</citation>提出了一个系统,用于从未标定的视频中利用地理空间数据进行相机姿态估计,通过GPS数据、序列影像和建筑物粗糙模型进行建筑物精细三维建模。文献<citation id="142" type="reference">[<a class="sup">13</a>]</citation>提出了基于视频运动物体和GIS的集成模型,通过空间定位和聚类运动物体的轨迹,构建运动物体的虚拟视野和表达模型,在虚拟场景中逐帧重建运动对象的子图。</p>
                </div>
                <div class="p1">
                    <p id="56">传统的监控视频与地理信息的融合模型分为两类,即位置映射模型和视频影像映射模型。前者仅仅将监控相机的位置集成在地理信息的框架中,用统一的地理坐标参考系将处于该区域范围内的监控相机建立相关联系。监控视频与地理信息的集成仅仅处于松集成阶段,起到的作用更多是示意功能<citation id="143" type="reference"><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。后者则是在此基础上将视频影像通过相机的内外参数等信息映射至地理空间中,与地理场景叠加进行处理分析<citation id="144" type="reference"><link href="22" rel="bibliography" /><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。虽然将更多信息映射至地理空间中,但是由于监控视频存在大量的信息冗余,无法使监控人员快速捕获真正感兴趣的目标信息。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">1 图像空间向地理空间的映射关系模型</h3>
                <div class="p1">
                    <p id="58">监控视频中含有静态环境背景信息和动态前景目标信息。首先对静态背景建模,将背景提取并映射至地理空间,然后建立前景目标和轨迹信息与地理空间的映射关系模型,通过该映射关系模型将其映射至地理空间,从而建立地理信息与前景目标相融合的可视化模型。通过提取监控视频中动态前景目标信息,既可减少视频的冗余信息,也可改善可视化效果。设某空间点<i>P</i>的世界坐标为(<i>X</i><sub><i>w</i></sub>,<i>Y</i><sub><i>w</i></sub>,<i>Z</i><sub><i>w</i></sub>),通过旋转矩阵<i><b>R</b></i>和平移向量<i><b>t</b></i>可以转换为相机坐标系下的坐标(<i>X</i><sub><i>c</i></sub>,<i>Y</i><sub><i>c</i></sub>,<i>Z</i><sub><i>c</i></sub>),坐标(<i>X</i><sub><i>c</i></sub>,<i>Y</i><sub><i>c</i></sub>,<i>Z</i><sub><i>c</i></sub>)与其对应图像坐标(<i>u</i>,<i>v</i>)存在如式(1)所示透视投影关系</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ζ</mi><msub><mrow></mrow><mi>c</mi></msub><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn><mo>/</mo><mi>d</mi><msub><mrow></mrow><mi>x</mi></msub></mtd><mtd><mn>0</mn></mtd><mtd><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn><mo>/</mo><mi>d</mi><msub><mrow></mrow><mi>y</mi></msub></mtd><mtd><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi><msub><mrow></mrow><mi>x</mi></msub></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>f</mi><msub><mrow></mrow><mi>y</mi></msub></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">R</mi></mtd><mtd><mi mathvariant="bold-italic">t</mi></mtd></mtr><mtr><mtd><mn>0</mn><msup><mrow></mrow><mtext>Τ</mtext></msup></mtd><mtd><mi mathvariant="bold-italic">Ι</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>X</mi><msub><mrow></mrow><mi>w</mi></msub></mtd></mtr><mtr><mtd><mi>Y</mi><msub><mrow></mrow><mi>w</mi></msub></mtd></mtr><mtr><mtd><mi>Ζ</mi><msub><mrow></mrow><mi>w</mi></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mi mathvariant="bold-italic">Κ</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">R</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">]</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>w</mi></msub><mo>=</mo><mi mathvariant="bold-italic">Ρ</mi><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>w</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">式中,<i>f</i><sub><i>x</i></sub>、<i>f</i><sub><i>y</i></sub>为相机焦距;<i>d</i><sub><i>x</i></sub>、<i>d</i><sub><i>y</i></sub>为相机传感器在水平与垂直方向的像元物理尺寸;<i>u</i><sub>0</sub>、<i>v</i><sub>0</sub>为图像像素主点坐标;<i><b>K</b></i>是仅与相机内部结构相关的参数所决定的内参数矩阵;[<i><b>R</b></i><b>|</b><i><b>t</b></i>]是由相机相对世界坐标系的旋转矩阵<i><b>R</b></i>和平移向量<i><b>t</b></i>所决定的外参数矩阵;<i><b>P</b></i>为相机投影矩阵。</p>
                </div>
                <div class="p1">
                    <p id="61">不妨假设地面为一平面,将世界坐标系中的点映射至图像坐标系中,假设图像中一点<i><b>m</b></i>,对应世界坐标点为<i><b>M</b></i>,则</p>
                </div>
                <div class="p1">
                    <p id="62"><i><b>m</b></i>=[<i>x y</i> 1]<sup>T</sup>      (2)</p>
                </div>
                <div class="p1">
                    <p id="63"><i><b>M</b></i>=[<i>X</i><sub><i>w</i></sub><i>Y</i><sub><i>w</i></sub> 0 1]<sup>T</sup>      (3)</p>
                </div>
                <div class="p1">
                    <p id="64"><i><b>m</b></i>=<i><b>HM</b></i>      (4)</p>
                </div>
                <div class="p1">
                    <p id="65">即</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Κ</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">]</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>X</mi><msub><mrow></mrow><mi>w</mi></msub></mtd></mtr><mtr><mtd><mi>Y</mi><msub><mrow></mrow><mi>w</mi></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">Μ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">式中</p>
                </div>
                <div class="area_img" id="68">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201911008_06800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="70">上述所求解的<i><b>H</b></i>矩阵是将平面上的物方空间点透视变化至图像空间中的映射矩阵,为了求解图像空间点投射至物方空间中,需要对<i><b>H</b></i>矩阵求逆,即</p>
                </div>
                <div class="area_img" id="71">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201911008_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="73"><i><b>H</b></i><sup>-1</sup>=(<i><b>K</b></i><b>[</b><i><b>r</b></i><sub>1</sub>,<i><b>r</b></i><sub>2</sub>,<i><b>t</b></i>])<sup>-1</sup>      (8)</p>
                </div>
                <div class="p1">
                    <p id="74">当假设世界坐标高程为0时,即将其看作平面时,通过计算相机内参矩阵<i><b>K</b></i>与外参矩阵[<i><b>r</b></i><sub>1</sub>,<i><b>r</b></i><sub>2</sub>,<i><b>t</b></i>]求解出<i><b>H</b></i>矩阵,内参矩阵的求解是利用张正友标定法<citation id="145" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>拍摄10～20张标定板图像进行标定,同时可获取相机畸变参数;外参矩阵可通过多点透视问题(perspective-n-points,PNP)进行求解,本文采用的是精度较高也较为流行的EPNP+Iteration<citation id="146" type="reference"><link href="38" rel="bibliography" /><link href="40" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>的方法。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">2 前景目标及其跟踪轨迹的提取与表达</h3>
                <div class="p1">
                    <p id="76">在固定场景的监控视频中,背景信息并不引起人们关注,前景运动目标才是关注的重点,也是视频智能分析的关键信息,因此运动目标的提取尤为重要。本文采用MOG2<citation id="147" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>算法进行前景目标的提取,并根据差分检测策略筛选出含前景目标的视频帧,通过跨帧检测显著提高了检测效率,然后将提取出的前景运动目标的轮廓作为地图符号置入地理空间中进行可视化表达。在多目标跟踪任务中,则利用基于深度学习的YOLOv3算法进行目标检测并利用DeepSort<citation id="148" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>算法实现多目标跟踪。将视频流输入差分筛选器剔除无需检测帧后,置入YOLOv3检测器中,输出检测框、类别与置信度,将该输出再次置于DeepSort<citation id="149" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>多目标跟踪器中,通过改进的递归卡尔曼滤波<citation id="150" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>预测位置并跟踪,根据马氏距离与深度描述子的余弦距离作为融合后的度量,采用匈牙利算法<citation id="151" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>进行级联匹配,输出动态跟踪定位信息。具体流程见图1。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 实时检测跟踪流程" src="Detail/GetImg?filename=images/CHXB201911008_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 实时检测跟踪流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Real-time detection and tracking flowchart</p>

                </div>
                <div class="p1">
                    <p id="78">视频所输出的跟踪结果为所跟踪目标在视频流图像中的位置、大小、身份识别信息等组成,由于这种结果无法被人们很直观地感受到,因此在进行目标跟踪时需要同时绘制出目标的运动轨迹。目前通常以目标检测框的中心为轨迹节点,虽然这种表达方式能够显示目标的运动轨迹,但不能满足量测定位的精度要求。为此,本文以目标(以人为例)的双足中心作为轨迹节点的初值,然后根据相机相对地平面的位姿与目标在图像中所占比例大小进行轨迹校正。假设由多目标跟踪器中获得的当前帧<i>t</i><sub><i>i</i></sub>中某一目标<i>O</i><sub><i>m</i></sub>的检测框结果为(<i>u</i>,<i>v</i>,<i>γ</i>,<i>h</i>),分别对应检测框的左下点的横纵坐标、宽高比例以及高度,则<i>t</i><sub><i>i</i></sub>帧中目标<i>O</i><sub><i>m</i></sub>在图像中的轨迹节点<i>T</i><sub><i>j</i></sub>(<i>u</i>′,<i>v</i>′)可由式(9)求得</p>
                </div>
                <div class="area_img" id="79">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201911008_07900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="81">式中,<i>δ</i>、<i>σ</i>为校正值。由于由此获得的目标<i>O</i><sub><i>m</i></sub>的轨迹节点<i>T</i><sub><i>j</i></sub>存在误差,因此连接轨迹节点<i>T</i><sub><i>j</i></sub>所得到的轨迹Tra<i>j</i><sub><i>n</i></sub>出现抖动现象,需要对所有轨迹节点进行拟合以取得光滑的跟踪轨迹。本文采用式(10)所示的3次多项式进行轨迹拟合</p>
                </div>
                <div class="p1">
                    <p id="82"><i>y</i>=<i>a</i><sub>0</sub>+<i>a</i><sub>1</sub><i>x</i>+…+<i>a</i><sub><i>k</i></sub><i>x</i><sup><i>k</i></sup>(<i>k</i>=3)      (10)</p>
                </div>
                <div class="p1">
                    <p id="83">各节点到该曲线的偏差平方和为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">[</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mo stretchy="false">(</mo><mi>a</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">经过求偏导化简后得到式(12)的矩阵表达形式</p>
                </div>
                <div class="area_img" id="86">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201911008_08600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="88">即</p>
                </div>
                <div class="p1">
                    <p id="89"><i><b>X</b></i>×<i><b>A</b></i>=<i><b>Y</b></i>      (13)</p>
                </div>
                <div class="p1">
                    <p id="90"><i><b>A</b></i>=(<i><b>X</b></i><sup>T</sup><i><b>X</b></i>)<sup>-1</sup><i><b>X</b></i><sup>T</sup><i><b>Y</b></i>      (14)</p>
                </div>
                <div class="p1">
                    <p id="91">结合前文所计算得到的映射矩阵,当获得图像空间中的跟踪目标<i>O</i><sub><i>m</i></sub>的轨迹节点点集</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>⋯</mo><mi>j</mi><mo>,</mo><mo>⋯</mo><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><msup><mi>u</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msup><mi>u</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub></mtd><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><msup><mi>v</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msup><mi>v</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub></mtd><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">通过映射矩阵可计算得到物方空间中该目标<i>O</i><sub><i>m</i></sub>的轨迹节点的对应点集,经过上述3次多项式拟合后得到的地理空间中平滑轨迹节点集为</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><msubsup><mrow></mrow><mi>w</mi><msup><mi>j</mi><mo>′</mo></msup></msubsup><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mi>Τ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>×</mo><mi mathvariant="bold-italic">A</mi><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Κ</mi><mo stretchy="false">[</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⋅</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><msup><mi>u</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msup><mi>u</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub></mtd><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><msup><mi>v</mi><mo>′</mo></msup><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><msup><mi>v</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub></mtd><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>×</mo><mi mathvariant="bold-italic">A</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h3 id="95" name="95" class="anchor-tag">3 前景动态目标与地理信息的融合模式</h3>
                <div class="p1">
                    <p id="96">在安防监控中,以井字格形式的监控视频最为常见,也广泛应用于公安系统、校园、小区等场所,然而这种以原始视频影像序列作为信息源的方式费时费力且未利用空间相关信息。同时,仅仅将监控视频映射至地理空间的模式也无法克服视频数据的冗余性缺点,难以突出视频的主要信息。与传统的监控视频相比,将视频的动态前景目标信息或者是管理者们感兴趣的信息提取出来,将处理分析后的结果,本文根据应用需求的差异性,共提出了4种融合模式,分别为:</p>
                </div>
                <div class="p1">
                    <p id="97">融合模式1:轨迹要素层+前景动态目标图层+背景层+真实地图图层;</p>
                </div>
                <div class="p1">
                    <p id="98">融合模式2:轨迹要素层+前景动态目标图层+真实地图图层;</p>
                </div>
                <div class="p1">
                    <p id="99">融合模式3:轨迹要素层+前景动态目标图层+背景层+矢量地图图层;</p>
                </div>
                <div class="p1">
                    <p id="100">融合模式4:轨迹要素层+前景动态目标图层+矢量地图图层。</p>
                </div>
                <div class="p1">
                    <p id="101">图2是以融合模式1为例的示意图,将多目标跟踪的轨迹信息映射至地理空间,利用前景目标提取算法提取目标与轨迹相关联,以真实的场景作为固定背景信息,实现地理信息与视频影像动态前景目标信息的融合。该模式包含要素最全,背景层的融合可利用视频所提供的背景对真实场景更新,可体现前景动态目标在真实场景下的定位结果以及轨迹的位置分布,提供了更多超媒体信息;模式2主要用于以遥感地图为参考底图的动态目标定位跟踪任务,当背景与真实地图场景差别较小时适用该模式,使得可视化效果更加真实;模式3与前两者相比将真实地图层更换为矢量地图层,对于相机可视范围外场景不被关注的情况下较为适用,同时使表达更为简洁;模式4适用于只关心地理信息表达,忽视场景中超媒体信息的任务,可视化效果也更为直观,突出前景动态目标的定位跟踪结果。同时模式3与模式4相比于前两者均更重视动态前景目标在地理空间中的数据分析、可量测、可查询统计等目的。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 地理信息与视频动态前景目标信息的融合模式" src="Detail/GetImg?filename=images/CHXB201911008_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 地理信息与视频动态前景目标信息的融合模式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Fusion mode of geographic information and video dynamic foreground summary information</p>

                </div>
                <h3 id="103" name="103" class="anchor-tag">4 试 验</h3>
                <div class="p1">
                    <p id="104">为验证所提出的模型及方法的可行性,笔者采集了部分监控视频影像进行试验,所采集视频场景为校园某一区域,所采用真实地图为无人机在120 m高度拍摄的遥感影像,像素分辨率为0.08 m。硬件环境Intel XEON CPU E5-1607 3.00 GHz,16 GB内存,NVIDIA GTX1060 6 GB显卡。</p>
                </div>
                <div class="p1">
                    <p id="105">试验数据是由USB外置相机采集,像素为640×480,相机内参矩阵为</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Κ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>8</mn><mn>3</mn><mn>0</mn><mo>.</mo><mn>2</mn><mn>1</mn><mn>5</mn><mtext> </mtext><mn>5</mn></mtd><mtd><mo>-</mo><mn>2</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>0</mn><mtext> </mtext><mn>5</mn></mtd><mtd><mn>2</mn><mn>7</mn><mn>9</mn><mo>.</mo><mn>7</mn><mn>1</mn><mn>4</mn><mtext> </mtext><mn>8</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>8</mn><mn>3</mn><mn>4</mn><mo>.</mo><mn>8</mn><mn>1</mn><mn>4</mn><mtext> </mtext><mn>7</mn></mtd><mtd><mn>2</mn><mn>7</mn><mn>0</mn><mo>.</mo><mn>6</mn><mn>6</mn><mn>4</mn><mtext> </mtext><mn>3</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">畸变参数为</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>3</mn><mn>8</mn><mn>5</mn><mtext> </mtext><mn>5</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>2</mn><mn>6</mn><mn>1</mn><mtext> </mtext><mn>0</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>3</mn><mtext> </mtext><mn>7</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>3</mn><mtext> </mtext><mn>7</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">由于该相机存在桶形畸变,一定程度上影响了映射结果,因此对视频影像首先进行畸变校正,再将畸变校正后的结果选取对应点进行PNP的计算,获取相机外参数矩阵。对应点对如表1所示。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表1 世界坐标与图像坐标对应点对</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.1 Point pairs corresponding to world coordinates and image coordinates</b></p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td><br />世界坐标</td><td>图像坐标</td></tr><tr><td><br />(460 471.188 545,3 853 986.285 574,0)</td><td>(411.572 036,52.501 202)</td></tr><tr><td><br />(460 477.726 312,3 853 997.308 430,0)</td><td>(64.504 398,6.506 124)</td></tr><tr><td><br />(460 457.092 360,3 853 990.164 838,0)</td><td>(295.583 308,419.436 340)</td></tr><tr><td><br />(460 457.107 798,3 853 986.005 468,0)</td><td>(607.761 291,401.538 110)</td></tr><tr><td><br />(460 469.792 619,3 853 994.166 355,0)</td><td>(125.351 499,86.497 767)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="111">标定出的相机在地理空间中的坐标为(460 449.504 6,3 853 990.102,7.625 628 456),相机3个旋转角为[-178.157° 55.467 1° 91.332 6°],外参数矩阵为</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">[</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">]</mo><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>3</mn><mtext> </mtext><mn>3</mn><mn>7</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>8</mn><mtext> </mtext><mn>5</mn><mn>9</mn><mn>5</mn><mtext> </mtext><mn>8</mn><mn>4</mn></mtd><mtd><mn>3</mn><mtext> </mtext><mn>8</mn><mn>5</mn><mn>4</mn><mtext> </mtext><mn>6</mn><mn>4</mn><mn>9</mn><mo>.</mo><mn>1</mn><mn>5</mn><mn>4</mn><mtext> </mtext><mn>6</mn><mn>5</mn><mn>3</mn><mtext> </mtext><mn>2</mn><mn>5</mn></mtd></mtr><mtr><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mn>6</mn><mn>6</mn><mtext> </mtext><mn>7</mn><mn>2</mn><mn>6</mn><mtext> </mtext><mn>3</mn><mn>9</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>4</mn><mn>9</mn><mtext> </mtext><mn>7</mn><mn>3</mn><mn>7</mn><mtext> </mtext><mn>3</mn><mn>5</mn></mtd><mtd><mn>6</mn><mn>9</mn><mtext> </mtext><mn>2</mn><mn>6</mn><mn>7</mn><mo>.</mo><mn>8</mn><mn>9</mn><mn>6</mn><mtext> </mtext><mn>5</mn><mn>2</mn><mn>8</mn><mtext> </mtext><mn>0</mn><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>.</mo><mn>8</mn><mn>2</mn><mn>3</mn><mtext> </mtext><mn>8</mn><mn>0</mn><mn>0</mn><mtext> </mtext><mn>5</mn><mn>8</mn><mn>3</mn></mtd><mtd><mo>-</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>4</mn><mn>3</mn><mtext> </mtext><mn>4</mn><mn>0</mn><mn>3</mn><mtext> </mtext><mn>9</mn><mn>4</mn></mtd><mtd><mo>-</mo><mn>4</mn><mn>4</mn><mn>9</mn><mtext> </mtext><mn>5</mn><mn>9</mn><mn>4</mn><mo>.</mo><mn>5</mn><mn>9</mn><mn>9</mn><mtext> </mtext><mn>0</mn><mn>8</mn><mn>2</mn><mtext> </mtext><mn>8</mn><mn>3</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn><mn>0</mn><mn>0</mn><mo>.</mo><mn>6</mn><mn>8</mn><mn>7</mn><mtext> </mtext><mn>5</mn><mn>6</mn><mn>7</mn><mtext> </mtext><mn>9</mn><mn>7</mn></mtd><mtd><mn>1</mn><mtext> </mtext><mn>6</mn><mn>0</mn><mn>5</mn><mo>.</mo><mn>2</mn><mn>2</mn><mn>5</mn><mtext> </mtext><mn>7</mn><mn>5</mn><mn>0</mn><mtext> </mtext><mn>7</mn><mn>9</mn></mtd><mtd><mn>4</mn><mn>6</mn><mn>0</mn><mtext> </mtext><mn>4</mn><mn>7</mn><mn>6</mn><mo>.</mo><mn>8</mn><mn>1</mn><mn>0</mn><mtext> </mtext><mn>2</mn><mn>9</mn><mn>7</mn><mtext> </mtext><mn>9</mn><mn>9</mn></mtd></mtr><mtr><mtd><mn>8</mn><mn>4</mn><mn>2</mn><mo>.</mo><mn>7</mn><mn>3</mn><mn>2</mn><mtext> </mtext><mn>1</mn><mn>4</mn><mn>3</mn><mtext> </mtext><mn>5</mn><mn>8</mn></mtd><mtd><mn>1</mn><mn>3</mn><mtext> </mtext><mn>4</mn><mn>3</mn><mn>5</mn><mo>.</mo><mn>9</mn><mn>9</mn><mn>1</mn><mtext> </mtext><mn>1</mn><mn>2</mn><mn>4</mn><mtext> </mtext><mn>4</mn><mn>1</mn></mtd><mtd><mn>3</mn><mtext> </mtext><mn>8</mn><mn>5</mn><mn>3</mn><mtext> </mtext><mn>9</mn><mn>9</mn><mn>9</mn><mo>.</mo><mn>2</mn><mn>5</mn><mn>7</mn><mtext> </mtext><mn>7</mn><mn>7</mn><mn>0</mn><mtext> </mtext><mn>8</mn><mn>2</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>0</mn><mtext> </mtext><mn>2</mn><mn>1</mn><mn>8</mn><mtext> </mtext><mn>6</mn><mn>7</mn></mtd><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>0</mn><mn>3</mn><mtext> </mtext><mn>4</mn><mn>8</mn><mn>6</mn><mtext> </mtext><mn>2</mn><mn>5</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">图3为监控视频背景通过地理映射模型投射前后的对比图,(a)为视频图像空间中的视角,(b)为地理物方空间中的视角,此时视频的背景已具有地理信息,可实现查询、量测等功能。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 监控视频地理映射前后对比" src="Detail/GetImg?filename=images/CHXB201911008_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 监控视频地理映射前后对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 The comparison chart of surveillance video before and after geographical mapping</p>

                </div>
                <div class="p1">
                    <p id="116">图4是在试验视频数据中截取的部分帧与所对应的跟踪结果,其中白色框是由递归卡尔曼滤波所得到的预测框,蓝色框是由差分YOLOv3算法所计算得出的检测框,绿色的ID号为通过匈牙利匹配所确定的跟踪结果。</p>
                </div>
                <div class="p1">
                    <p id="117">视频的多目标跟踪量化评价见表2,分别采用了YOLOv2与YOLOv3两种检测器作为多目标跟踪的目标检测算法。评估的度量标准为MOT CHALLENGE所提供的方法CLEAR MOT<citation id="152" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>,其中MOTA是结合了丢失目标,虚警率,ID转换误配数,表示了多目标跟踪的准确度,而MOTP是所有跟踪目标的平均边框重叠率IOU,表示了多目标跟踪的精确度。由表2可看出,当采用YOLOv2时多目标跟踪的MOTA与MOTP可达78.4与79.8,采用YOLOv3时有一定增长,分别为87.5与83.6,可以发现当目标检测的精度提高时,多目标跟踪的精度会随之提高,同时量化指标MOTA 87.5以及MOTP83.5体现了本文算法对于所实验视频的适用性。</p>
                </div>
                <div class="area_img" id="118">
                    <p class="img_tit"><b>表2 多目标跟踪量化评价结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.2 The quantitative evaluation result of multi-target tracking tasks</b></p>
                    <p class="img_note"></p>
                    <table id="118" border="1"><tr><td><br />Detector</td><td>IDF1</td><td>Rcll</td><td>Prcn</td><td>FAR</td><td>MT</td><td>FP</td><td>FN</td><td>IDs</td><td>FM</td><td>MOTA</td><td>MOTP</td><td>MOTAL</td></tr><tr><td><br />YOLOv2</td><td>87.8</td><td>78.6</td><td>99.8</td><td>0.01</td><td>1</td><td>4</td><td>438</td><td>0</td><td>18</td><td>78.4</td><td>79.8</td><td>78.4</td></tr><tr><td><br />YOLOv3</td><td>92.1</td><td>89.3</td><td>98.1</td><td>0.05</td><td>4</td><td>35</td><td>218</td><td>3</td><td>22</td><td>87.5</td><td>83.5</td><td>87.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="119">试验中共选取了均匀分布的18个点作为图像空间与地理空间的对应点,其中表1中的对应点对作为映射模型计算的输入值,剩下的13个点对作为测试点对进行映射模型的精度评定,测试的对应点对坐标及映射后的坐标见表3,并计算其均方误差,其中最大误差为0.117 4 m,最小为0.017 7 m,平均均方差为0.071 3 m。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表3 映射坐标对应及误差分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.3 Mapping coordinate correspondence and error analysis</b></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td colspan="2"><br />图像坐标系</td><td colspan="3">世界坐标系(真实对应点)</td><td colspan="3">世界坐标系(映射结果)</td><td rowspan="2">均方差/m</td></tr><tr><td><br /><i>x</i></td><td><i>y</i></td><td></td><td><br /><i>x</i></td><td><i>y</i></td><td></td><td><br /><i>x</i></td><td><i>y</i></td></tr><tr><td><br />72.518 8</td><td>12.512 8</td><td colspan="2">460 475.044 0</td><td>3 853 996.605 2</td><td colspan="2">460 475.031 6</td><td>3 853 996.540 4</td><td>0.066 0</td></tr><tr><td><br />286.536 5</td><td>168.400 3</td><td colspan="2">460 464.176 8</td><td>3 853 990.189 8</td><td colspan="2">460 464.111 8</td><td>3 853 990.171 0</td><td>0.067 7</td></tr><tr><td><br />199.485 0</td><td>4.482 2</td><td colspan="2">460 475.043 1</td><td>3 853 992.598 4</td><td colspan="2">460 475.124 8</td><td>3 853 992.640 1</td><td>0.091 7</td></tr><tr><td><br />308.700 6</td><td>141.516 0</td><td colspan="2">460 465.290 8</td><td>3 853 989.621 1</td><td colspan="2">460 465.253 0</td><td>3 853 989.687 7</td><td>0.076 6</td></tr><tr><td><br />483.542 3</td><td>158.458 2</td><td colspan="2">460 464.075 5</td><td>3 853 986.389 3</td><td colspan="2">460 464.096 6</td><td>3 853 986.305 6</td><td>0.086 3</td></tr><tr><td><br />593.348 2</td><td>472.508 8</td><td colspan="2">460 456.177 3</td><td>3 853 986.711 7</td><td colspan="2">460 456.128 9</td><td>3 853 986.724 7</td><td>0.050 2</td></tr><tr><td><br />61.495 8</td><td>12.498 0</td><td colspan="2">460 475.043 6</td><td>3 853 996.810 0</td><td colspan="2">460 475.095 8</td><td>3 853 996.893 8</td><td>0.098 8</td></tr><tr><td><br />75.528 9</td><td>6.498 9</td><td colspan="2">460 475.627 6</td><td>3 853 996.568 2</td><td colspan="2">460 475.635 3</td><td>3 853 996.563 3</td><td>0.009 1</td></tr><tr><td><br />505.965 5</td><td>202.045 9</td><td colspan="2">460 462.473 1</td><td>3 853 986.187 2</td><td colspan="2">460 462.390 3</td><td>3 853 986.269 4</td><td>0.116 7</td></tr><tr><td><br />599.978 2</td><td>195.934 3</td><td colspan="2">460 462.455 6</td><td>3 853 984.586 5</td><td colspan="2">460 462.435 1</td><td>3 853 984.580 9</td><td>0.021 2</td></tr><tr><td><br />399.498 5</td><td>59.499 7</td><td colspan="2">460 469.735 8</td><td>3 853 987.245 9</td><td colspan="2">4604 69.619 5</td><td>3 853 987.229 9</td><td>0.117 4</td></tr><tr><td><br />286.485 5</td><td>268.494 1</td><td colspan="2">460 460.745 3</td><td>3 853 990.271 3</td><td colspan="2">460 460.644 2</td><td>3 853 990.236 3</td><td>0.107 0</td></tr><tr><td><br />610.626 9</td><td>71.466 7</td><td colspan="2">460 468.106 2</td><td>3 853 982.455 9</td><td colspan="2">460 468.117 0</td><td>3 85 3982.470 0</td><td>0.017 7</td></tr><tr><td><br />平均均方差</td><td></td><td colspan="2"></td><td></td><td colspan="2"></td><td></td><td>0.071 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="121">图5中(a)、(b)分别对应了多目标跟踪轨迹在图像空间中与地理物方空间中的可视化表达;图5(c)、(d)分别是原始跟踪轨迹节点与通过三次多项式拟合后的误差较小的平滑轨迹。</p>
                </div>
                <div class="p1">
                    <p id="122">图6展示了融合模式中两种不同的底图,分别是(a)中的无人机遥感影像图,该底图可最大程度反映真实的场景,但由于更新的原因,在特殊情况下可与背景层叠加显示;(b)为矢量地图,以其作为底图层,可突出前景动态目标信息,使表达更加简明。红色框为相机可视域范围。</p>
                </div>
                <div class="p1">
                    <p id="123">图7中(a)、(d)分别对应融合模式中的①到④,4种融合模式分别适用于不同的任务需求,具有不同的可视化表达效果,在突出重点有差异的情况下,可根据各模式的特点灵活选择。</p>
                </div>
                <div class="p1">
                    <p id="124">试验所采集视频时长22 s,共670帧,为验证所提方法的实时性,进行了各部分耗时统计,结果如表4所示,其中目标检测部分利用监控视频冗余特性,通过差分筛选剔除无须检测帧提高检测速度,共耗时16.96 s,跟踪部分耗时4.27 s,映射总耗时0.13 s,其中检测每帧平均耗时2.5 ms,跟踪每帧平均耗时0.6 ms,总速度可达31.36 fps,可知所提出方法在完成任务需求的同时满足实时性要求,同时当视频前景变化较小时,可改变筛选阈值,进一步提高检测速度以提高实时性。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表4 实时性分析结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.4 The real-time analysis results</b></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />检测速度/(ms/帧)</td><td>跟踪速度/(ms/帧)</td><td>总速度/fps</td></tr><tr><td><br />2.5</td><td>0.6</td><td>31.36</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="126" name="126" class="anchor-tag">5 结 论</h3>
                <div class="p1">
                    <p id="127">本文在多目标跟踪的基础上,提出融合地理信息与动态前景目标的模型,设计了4种多图层融合模式将监控视频中动态前景目标与跟踪轨迹通过地理映射模型投射至地理空间中,与传统视频与地理信息的结合方式相比,减少了视频数据传输中的冗余,极大程度上降低了数据的存储量,智能化提取了视频动态前景目标信息,减轻了监控人员的工作强度。实现了监控视频动态前景目标在真实地理空间中的表达,解决了传统目标跟踪任务仅仅处于图像空间中,无法实现真实地理空间中可量测、可定位的问题。从试验结果来看,多目标跟踪的准确度MOTA可达87.5,精确度MOTP可达83.5,图像空间向地理空间的映射模型精度为0.071 3 m,处理速度为31.36 fps,在精度与实时性上可满足任务需求,监控视频的动态前景目标信息融合至地理空间的可视化效果良好,4种融合模式也可为不同的需求提供相应映射方案。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 原始视频与跟踪视频对应帧" src="Detail/GetImg?filename=images/CHXB201911008_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 原始视频与跟踪视频对应帧  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 The corresponding frame of original video and tracking video</p>

                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 多目标跟踪轨迹结果" src="Detail/GetImg?filename=images/CHXB201911008_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 多目标跟踪轨迹结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 The results of multi-target tracking trajectory</p>

                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 试验区底图" src="Detail/GetImg?filename=images/CHXB201911008_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 试验区底图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 The base map of experimental area</p>

                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201911008_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 4种融合模式对比" src="Detail/GetImg?filename=images/CHXB201911008_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 4种融合模式对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201911008_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 The comparison chart of four fusion modes</p>

                </div>
                <div class="p1">
                    <p id="132">监控视频多用于以平面为主的场景,因此二维映射也可适用于大多情况,但未来的研究还可以从多方面展开,如利用不同平面高程约束实现三维映射;也可通过标注真值的数据集,来评估跟踪及映射的精度,不断提高跟踪算法与映射模型的精度;实现多相机的多目标跟踪在统一的地理参考场景下的融合表达。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DLKJ201102001&amp;v=MTg1NTVGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlybldyL0FJU0hBWkxHNEg5RE1yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 韩志刚,孔云峰,秦耀辰.地理表达研究进展[J].地理科学进展,2011,30(2):141-148.HAN Zhigang,KONG Yunfeng,QIN Yaochen.Research on geographic representation:a review[J].Progress in Geography,2011,30(2):141-148.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extensions to geographic representations">

                                <b>[2]</b> YUAN M,MARK D M,EGENHOFER M,et al.Extensions to geographic representations[M]//MCMASTER R,USERY L.A Research Agenda for Geographic Information Science.Boca Raton,FL:CRC Press,2004:129-156.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000048162&amp;v=MTE0NTBNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVNzdJSWx3ZGFobz1OaWZJWTdLN0h0ak5yNDlGWk84SERYbzdvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> LIPPMAN A.Movie-maps:An application of the optical videodisc to computer graphics[J].ACM SIGGRAPH Computer Graphics,1980,14(3):32-42.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimedia GIS:definition, requirements and applications">

                                <b>[4]</b> BILL R.Multimedia GIS-definition,requirements and applications[M]//The 1994 European GIS Yearbook.Oxford:Blackwell,1994:151-154.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Capture′Where′and′When′on video-based GIS">

                                <b>[5]</b> BERRY J K.Capture “where” and “when” on video-based GIS[J].GeoWorld,2000,13(9):26-27.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201505014&amp;v=MDI0MTBUTXFvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJuV3IvQUppWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 谢潇,朱庆,张叶廷,等.多层次地理视频语义模型[J].测绘学报,2015,44(5):555-562.DOI:10.11947/j.AGCS.2015.20140176.XIE Xiao,ZHU Qing,ZHANG Yeting,et al.Hierarchical semantic model of geovideo[J].Acta Geodaetica et Cartographica Sinica,2015,44(5):555-562.DOI:10.11947/j.AGCS.2015.20140176.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610002&amp;v=Mjk5NTFOcjQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cm5Xci9BS0NMZlliRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 尹宏鹏,陈波,柴毅,等.基于视觉的目标检测与跟踪综述[J].自动化学报,2016,42(10):1466-1489.YIN Hongpeng,CHEN Bo,CHAI Yi,et al.Vision-based object detection and tracking:a review[J].Acta Automatica Sinica,2016,42(10):1466-1489.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200902002&amp;v=MDQ5OTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cm5Xci9BUHlyZmJMRzRIdGpNclk5Rlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 徐光祐,曹媛媛.动作识别与行为理解综述[J].中国图象图形学报,2009,14(2):189-195.XU Guangyou,CAO Yuanyuan.Action recognition and activity understanding:a review[J].Journal of Image and Graphics,2009,14(2):189-195.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiple object tracking:a literature review">

                                <b>[9]</b> LUO Wenhan,XING Junjiang,MILAN A,et al.Multiple object tracking:a literature review[Z].arXiv:1409.7618,2015.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effects of presenting geographic context on tracking activity between cameras">

                                <b>[10]</b> GIRGENSOHN A,SHIPMAN F,TURNER T,et al.Effects of presenting geographic context on tracking activity between cameras[C]//Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.San Jose,California,USA:ACM,2007:1167-1176.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD120604028048&amp;v=MDg1OTdCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2Rnlqa1U3M0tLVjRjTmpuQmFySzZIdGZNcTQ5SGJPc0xC&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> LEWIS P,FOTHERINGHAM S,WINSTANLEY A.Spatial video and GIS[J].International Journal of Geographical Information Science,2011,25(5):697-716.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPS, GIS and Video Registration for Building Reconstruction">

                                <b>[12]</b> SOURIMANT G,MORIN L,BOUATOUCH K.GPS,GIS and video registration for building reconstruction[C]//Proceedings of 2007 IEEE International Conference on Image Processing.San Antonio,TX,USA:IEEE,2007:VI - 401-VI - 404.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Surveillance video synopsis in GIS">

                                <b>[13]</b> XIE Yujia,WANG Meizhen,LIU Xuejun,et al.Surveillance video synopsis in GIS[J].ISPRS International Journal of Geo-Information,2017,6(11):333.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHKD201005072&amp;v=MjEwMzdCdEdGckNVUjdxZlp1ZHZGeXJuV3IvQUppWEFhckc0SDlITXFvOUNab1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 宋宏权,孔云峰.Flex框架下网络视频GIS设计与实现[J].测绘科学,2010,35(5):208-210,151.SONG Hongquan,KONG Yunfeng.Design and implementation of a web-based VideoGIS using Adobe Flex[J].Science of Surveying &amp; Mapping,2010,35(5):208-210,151.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHKD201201068&amp;v=MTA0OTF2RnlybldyL0FKaVhBYXJHNEg5UE1ybzlEYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 韩志刚,曾明,孔云峰.校园地理视频监控WebGIS系统设计与实现[J].测绘科学,2012,37(1):195-197.HAN Zhigang,ZENG Ming,KONG Yunfeng.Design and implementation of the campus geovideo monitoring Web GIS system[J].Science of Surveying &amp; Mapping,2012,37(1):195-197.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Egocentric view transition for video monitoring in a distributed camera network">

                                <b>[16]</b> CHEN Kuanwen,LEE P J,HUNG Y P.Egocentric view transition for video monitoring in a distributed camera network[C]//Proceedings of International Conference on Advances in Multimedia Modeling.Berlin:Springer-Verlag,2011:171-181.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effects of navigation design on contextualized video interfaces">

                                <b>[17]</b> WANG Yi,BOWMAN D A.Effects of navigation design on contextualized video interfaces[C]//Proceedings of 2011 IEEE Symposium on 3D User Interfaces.Singapore:IEEE,2011:27-34.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Flexible New Technique for Camera Calibration">

                                <b>[18]</b> ZHANG Z.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2000,22(11):1330-1334.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003394813&amp;v=MTYzODlyTzRIdEhQcklaQmJPb01ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZ5bmxVYjNCSUZZPU5qN0Jh&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> LEPETIT V,MORENO-NOGUER F,FUA P.EP<i>n</i>P:An accurate O(<i>n</i>)solution to the P<i>n</i>P problem[J].International Journal of Computer Vision,2009,81(2):155-166.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation">

                                <b>[20]</b> PENATE-SANCHEZ A,ANDRADE-CETTO J,MORENO-NOGUER F.Exhaustive linearization for robust camera pose and focal length estimation[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2013,35(10):2387-2400.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved adaptive Gaussian mixture model for background subtraction">

                                <b>[21]</b> ZIVKOVIC Z.Improved adaptive Gaussian mixture model for background subtraction[C]//Proceedings of the 17th International Conference on Pattern Recognition.Cambridge,UK:IEEE,2004:28-31.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 WOJKE N,BEWLEY A,PAULUS D.Simple online and realtime tracking with a deep association metric[C]//Proceedings of 2017 IEEE International Conference on Image Processing.Beijing,China:IEEE,2017:3645-3649.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new approach to linear filtering and prediction problems">

                                <b>[23]</b> KALMAN R E.A new approach to linear filtering and prediction problems[J].Journal of Basic Engineering,1960,82(1):35-45.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The hungarian method for the assignment problem">

                                <b>[24]</b> KUHNH W.The Hungarian method for the assignment problem[J].Naval Research Logistics Quarterly,1955,2(1-2):83-97.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluating multipe object tracking performance: theCLEARMOT metrics">

                                <b>[25]</b> BERNARDIN K,STIEFELHAGEN R.Evaluating multiple object tracking performance:the clear MOT metrics[J].EURASIP Journal on Image &amp; Video Processing,2008(1):246309.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201808001&amp;v=MjY4NzBkdkZ5cm5Xci9BUHlyZmJMRzRIOW5NcDQ5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> 葛宝义,左宪章,胡永江,视觉目标跟踪方法研究综述[J].中国图像图形学报,2008,23(8):1091-1107.GE Baoyi,ZUO Xianzhang,HU Yongjiang.Review of visual object tracking technology[J].Journal of Image and Graphics,2018,23(8):1091-1107.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201911008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201911008&amp;v=MTY4MTFOcm85RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cm5Xci9BSmlYVGJMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
