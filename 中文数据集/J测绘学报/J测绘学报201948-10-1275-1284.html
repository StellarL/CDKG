<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142606577920000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201910009%26RESULT%3d1%26SIGN%3dcAy5150T2yqzQuOsHCm0JXuPpuc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201910009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201910009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201910009&amp;v=MjU3NjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZyL09KaVhUYkxHNEg5ak5yNDlGYllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 本文算法 ">1 本文算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="1.1 ROI提取">1.1 ROI提取</a></li>
                                                <li><a href="#68" data-title="1.2 混合受限波尔兹曼机">1.2 混合受限波尔兹曼机</a></li>
                                                <li><a href="#87" data-title="1.3 细节—语义特征融合网络">1.3 细节—语义特征融合网络</a></li>
                                                <li><a href="#96" data-title="1.4 结合上下文信息">1.4 结合上下文信息</a></li>
                                                <li><a href="#103" data-title="1.5 目标分类预测与定位">1.5 目标分类预测与定位</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="2 试验与分析 ">2 试验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="2.1 试验数据集及评价指标">2.1 试验数据集及评价指标</a></li>
                                                <li><a href="#116" data-title="2.2 试验参数设置及优化">2.2 试验参数设置及优化</a></li>
                                                <li><a href="#120" data-title="2.3 算法可行性验证分析">2.3 算法可行性验证分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#138" data-title="3 结 论 ">3 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 VOC2007数据集与NWPU VHR-10数据集对比">图1 VOC2007数据集与NWPU VHR-10数据集对比</a></li>
                                                <li><a href="#65" data-title="图2 本文目标检测模型架构示意">图2 本文目标检测模型架构示意</a></li>
                                                <li><a href="#89" data-title="图3 细节—语义特征融合网络">图3 细节—语义特征融合网络</a></li>
                                                <li><a href="#119" data-title="图4 特征提取网络结构及参数设置">图4 特征提取网络结构及参数设置</a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表1 不同层特征融合试验对比&lt;/b&gt;"><b>表1 不同层特征融合试验对比</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表2 结合上下文试验对比&lt;/b&gt;"><b>表2 结合上下文试验对比</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表3 不同RBM模型(局部—上下文特征融合)测试结果对比&lt;/b&gt;"><b>表3 不同RBM模型(局部—上下文特征融合)测试结果对比</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表4 对比算法和本文算法在测试集上性能比较&lt;/b&gt;"><b>表4 对比算法和本文算法在测试集上性能比较</b></a></li>
                                                <li><a href="#140" data-title="图5 本文算法在测试集图像上检测结果">图5 本文算法在测试集图像上检测结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" 温奇,李苓苓,刘庆杰,等.基于视觉显著性和图分割的高分辨率遥感影像中人工目标区域提取[J].测绘学报,2013,42(6):831-837.WEN Qi,LI Lingling,LIU Qingjie,et al.A man-made object area extraction method based on visual saliency detection and graph-cut segmentation for high resolution remote sensing imagery[J].Acta Geodaetica et Cartographica Sinica,2013,42(6):831-837." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201306009&amp;v=MjA3NzBMTXFZOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVnIvT0ppWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         温奇,李苓苓,刘庆杰,等.基于视觉显著性和图分割的高分辨率遥感影像中人工目标区域提取[J].测绘学报,2013,42(6):831-837.WEN Qi,LI Lingling,LIU Qingjie,et al.A man-made object area extraction method based on visual saliency detection and graph-cut segmentation for high resolution remote sensing imagery[J].Acta Geodaetica et Cartographica Sinica,2013,42(6):831-837.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" GIRSHICK R,DONAHUE J,DARRELL T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition.Columbus,OH:IEEE,2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[2]</b>
                                         GIRSHICK R,DONAHUE J,DARRELL T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition.Columbus,OH:IEEE,2014:580-587.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" GIRSHICK R.Fast R-CNN[C]//Proceedings of 2015 IEEE International Conference on Computer Vision.Santiago:IEEE,2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[3]</b>
                                         GIRSHICK R.Fast R-CNN[C]//Proceedings of 2015 IEEE International Conference on Computer Vision.Santiago:IEEE,2015:1440-1448.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" REN Shaoqing,HE Kaiming,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2015:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards realtime object detection with region proposal networks">
                                        <b>[4]</b>
                                         REN Shaoqing,HE Kaiming,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2015:91-99.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     REDMON J,DIVVALA S,GIRSHICK R,et al.You only look once:unified,real-time object detection[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas,NV:IEEE,2016:779-788.</a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" LIU Wei,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector[C]//Proceedings of the 14th European Conference on Computer Vision.Amsterdam:Springer,2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[6]</b>
                                         LIU Wei,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector[C]//Proceedings of the 14th European Conference on Computer Vision.Amsterdam:Springer,2016:21-37.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" EVERINGHAM M,VAN GOOL L,WILLIAMS C K I,et al.The Pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision,2010,88(2):303-338." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=MDMyNjFGZz1OajdCYXJPNEh0SFBxWWRIWStJTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RnlubFVick5J&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         EVERINGHAM M,VAN GOOL L,WILLIAMS C K I,et al.The Pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision,2010,88(2):303-338.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" CHENG Gong,HAN Junwei,ZHOU Peicheng,et al.Multi-class geospatial object detection and geographic image classification based on collection of part detectors[J].ISPRS Journal of Photogrammetry and Remote Sensing,2014,98(12):119-132." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700308532&amp;v=MDIwNTNJOUZaK3NIQ1g4N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUlsc1JhaFE9TmlmT2ZiSzhIOURNcQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         CHENG Gong,HAN Junwei,ZHOU Peicheng,et al.Multi-class geospatial object detection and geographic image classification based on collection of part detectors[J].ISPRS Journal of Photogrammetry and Remote Sensing,2014,98(12):119-132.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" 伍广明,陈奇,SHIBASAKI R,等.基于U形卷积神经网络的航空影像建筑物检测[J].测绘学报,2018,47(6):864-872.DOI:10.11947/j.AGCS.2018.20170651.WU Guangming,CHEN Qi,SHIBASAKI R,et al.High precision building detection from aerial imagery using a U-Net like convolutional architecture[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):864-872.DOI:10.11947/j.AGCS.2018.20170651." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806020&amp;v=MTQyMzFNcVk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdWci9PSmlYVGJMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         伍广明,陈奇,SHIBASAKI R,等.基于U形卷积神经网络的航空影像建筑物检测[J].测绘学报,2018,47(6):864-872.DOI:10.11947/j.AGCS.2018.20170651.WU Guangming,CHEN Qi,SHIBASAKI R,et al.High precision building detection from aerial imagery using a U-Net like convolutional architecture[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):864-872.DOI:10.11947/j.AGCS.2018.20170651.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" 戴玉超,张静,PORIKLI F,等.深度残差网络的多光谱遥感图像显著目标检测[J].测绘学报,47(6):873-881.DOI:10.11947/j.AGCS.2018.20170633.DAI Yuchao,ZHANG Jing,PORIKLI F,et al.Salient object detection from multi-spectral remote sensing images with deep residual network[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):873-881.DOI:10.11947/j.AGCS.2018.20170633." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806021&amp;v=MDA0MDBGckNVUjdxZlp1ZHZGeXJnVnIvT0ppWFRiTEc0SDluTXFZOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         戴玉超,张静,PORIKLI F,等.深度残差网络的多光谱遥感图像显著目标检测[J].测绘学报,47(6):873-881.DOI:10.11947/j.AGCS.2018.20170633.DAI Yuchao,ZHANG Jing,PORIKLI F,et al.Salient object detection from multi-spectral remote sensing images with deep residual network[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):873-881.DOI:10.11947/j.AGCS.2018.20170633.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" XU Yuelei,ZHU Mingming,XIN Peng,et al.Rapid airplane detection in remote sensing images based on multilayer feature fusion in fully convolutional neural networks[J].Sensors,2018,18(7):2335." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rapid Airplane Detection in Remote Sensing Images Based on Multilayer Feature Fusion in Fully Convolutional Neural Networks">
                                        <b>[11]</b>
                                         XU Yuelei,ZHU Mingming,XIN Peng,et al.Rapid airplane detection in remote sensing images based on multilayer feature fusion in fully convolutional neural networks[J].Sensors,2018,18(7):2335.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" YANG Yiding,ZHUANG Yin,BI Fukun,et al.M-FCN:effective fully convolutional network-based airplane detection framework[J].IEEE Geoscience and Remote Sensing Letters,2017,14(8):1293-1297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=M-FCN Effective Fully Convolutional Network-based Airplane Detection Framework">
                                        <b>[12]</b>
                                         YANG Yiding,ZHUANG Yin,BI Fukun,et al.M-FCN:effective fully convolutional network-based airplane detection framework[J].IEEE Geoscience and Remote Sensing Letters,2017,14(8):1293-1297.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" TANG Tianyu,ZHOU Shilin,DENG Zhipeng,et al.Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining[J].Sensors,2017,17(2):336." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle Detection in Aerial Images Based on Region Convolutional Neural Networks and Hard Negative Example Mining">
                                        <b>[13]</b>
                                         TANG Tianyu,ZHOU Shilin,DENG Zhipeng,et al.Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining[J].Sensors,2017,17(2):336.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" GUO Wei,YANG Wen,ZHANG Haijian,et al.Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network[J].Remote Sensing,2018,10(1):131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network">
                                        <b>[14]</b>
                                         GUO Wei,YANG Wen,ZHANG Haijian,et al.Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network[J].Remote Sensing,2018,10(1):131.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" CHEN Zhong,ZHANG Ting,OUYANG Chao.End-to-end airplane detection using transfer learning in remote sensing images[J].Remote Sensing,2018,10(1):139." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Airplane Detection Using Transfer Learning in Remote Sensing Images">
                                        <b>[15]</b>
                                         CHEN Zhong,ZHANG Ting,OUYANG Chao.End-to-end airplane detection using transfer learning in remote sensing images[J].Remote Sensing,2018,10(1):139.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition.Boston,MA:IEEE,2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">
                                        <b>[16]</b>
                                         LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition.Boston,MA:IEEE,2015:3431-3440.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" UIJLINGS J R R,VAN DE SANDE K E A,GEVERS T,et al.Selective search for object recognition[J].International Journal of Computer Vision,2013,104(2):154-171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MDg4OTlJSWxzUmFoUT1OajdCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3Nw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         UIJLINGS J R R,VAN DE SANDE K E A,GEVERS T,et al.Selective search for object recognition[J].International Journal of Computer Vision,2013,104(2):154-171.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" ZEILER M D,FERGUS R.Visualizing and understanding convolutional networks[C]//Proceedings of the 13th European Conference on Computer Vision.Zurich:Springer,2014:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[18]</b>
                                         ZEILER M D,FERGUS R.Visualizing and understanding convolutional networks[C]//Proceedings of the 13th European Conference on Computer Vision.Zurich:Springer,2014:818-833.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_19" title=" ZITNICK C L,DOLL&#193;R P.Edge boxes:locating object proposals from edges[C]//Proceedings of the 13th European Conference on Computer Vision.Zurich:Springer,2014:391-405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Edge boxes:locating object proposals from edges">
                                        <b>[19]</b>
                                         ZITNICK C L,DOLL&#193;R P.Edge boxes:locating object proposals from edges[C]//Proceedings of the 13th European Conference on Computer Vision.Zurich:Springer,2014:391-405.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_20" title=" KUO Weicheng,HARIHARAN B,MALIK J.DeepBox:learning objectness with convolutional networks[C]//Proceedings of 2015 IEEE International Conference on Computer Vision.Santiago:IEEE,2015:2479-2487." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepbox:Learning Objectness with Convolutional Networks">
                                        <b>[20]</b>
                                         KUO Weicheng,HARIHARAN B,MALIK J.DeepBox:learning objectness with convolutional networks[C]//Proceedings of 2015 IEEE International Conference on Computer Vision.Santiago:IEEE,2015:2479-2487.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_21" title=" WANG Li,LU Yao,WANG Hong,et al.Evolving boxes for fast vehicle detection[C]//Proceedings of 2017 IEEE International Conference on Multimedia and Expo.Hong Kong,China:IEEE,2017:1135-1140." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evolving boxes for fast vehicle detection">
                                        <b>[21]</b>
                                         WANG Li,LU Yao,WANG Hong,et al.Evolving boxes for fast vehicle detection[C]//Proceedings of 2017 IEEE International Conference on Multimedia and Expo.Hong Kong,China:IEEE,2017:1135-1140.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_22" title=" HINTON G E,OSINDERO S,TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18(7):1527-1554." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MjYxNzVSYWhRPU5pZkpaYks5SHRqTXFvOUZaT29OQ1g4eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUlscw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         HINTON G E,OSINDERO S,TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18(7):1527-1554.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_23" title=" LAROCHELLE H,MANDEL M,PASCANU R,et al.Learning algorithms for the classification restricted Boltzmann machine[J].Journal of Machine Learning Research,2012,13(1):643-669." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning algorithms for the classification restricted Boltzmann machine">
                                        <b>[23]</b>
                                         LAROCHELLE H,MANDEL M,PASCANU R,et al.Learning algorithms for the classification restricted Boltzmann machine[J].Journal of Machine Learning Research,2012,13(1):643-669.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_24" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.Lake Tahoe:ACM,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classifi cation with deep convolutional neural networks">
                                        <b>[24]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.Lake Tahoe:ACM,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_25" title=" FELZENSZWALB P F,GIRSHICK R B,MCALLESTER D,et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(9):1627-1645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">
                                        <b>[25]</b>
                                         FELZENSZWALB P F,GIRSHICK R B,MCALLESTER D,et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(9):1627-1645.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_26" title=" CHENG Gong,ZHOU Peicheng,HAN Junwei.Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing,2016,54(12):7405-7415." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images">
                                        <b>[26]</b>
                                         CHENG Gong,ZHOU Peicheng,HAN Junwei.Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing,2016,54(12):7405-7415.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(10),1275-1284 DOI:10.11947/j.AGCS.2019.20180431            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>多层特征与上下文信息相结合的光学遥感影像目标检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E4%B8%81&amp;code=29528674&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈丁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%87%E5%88%9A&amp;code=20892308&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">万刚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%A7%91&amp;code=20777759&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李科</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目标检测是遥感影像分析的基础和关键。针对光学遥感影像中目标尺度多样、小目标居多、相似性及背景复杂等问题,本文提出一种将卷积神经网络(CNN)和混合波尔兹曼机(HRBM)相结合的遥感影像目标检测方法。首先设计细节—语义特征融合网络(D-SFN)提取卷积神经网络低层和高层融合特征,提升目标特征的判别力,特别是小目标;其次考虑上下文信息对目标检测的影响,结合上下文信息进一步加强目标表征的准确性,提升检测精度。在NWPU数据集上试验表明,本文方法能够显著提升目标检测精度且具有一定程度的稳健性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">受限玻尔兹曼机;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈丁(1990—),男,博士生,研究方向为遥感影像分析和虚拟地理环境,E-mail: fwind_email@163.com。;
                                </span>
                                <span>
                                    *李科,E-mail: like19771223@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(41871322);</span>
                                <span>国家国防基金项目(3601015);</span>
                    </p>
            </div>
                    <h1>Object detection in optical remote sensing images based on combination of multi-layer feature and context information</h1>
                    <h2>
                    <span>CHEN Ding</span>
                    <span>WAN Gang</span>
                    <span>LI Ke</span>
            </h2>
                    <h2>
                    <span>Information Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Object detection is the basic and key step of remote sensing image analysis. In optical remote sensing images, object detection faced many challenges such as multi-scale and small objects, appearance ambiguity and complicated background. To address these problems, a new method of object detection based on convolutional neural networks(CNN) and hybrid restricted boltzmann machine(HRBM) is proposed. Firstly, the detail-semantic feature fusion network(D-SFN) is designed to extract fusion features from low-level and high-level CNNs, which can make the target representation more distinguishable, especially for small objects. Secondly, context information is incorporated to further boost feature discrimination, which also improves the detection accuracy. Experiments on NWPU datasets show that the proposed method can significantly improve the accuracy of object detection and has certain robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing%20images&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing images;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RBM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RBM;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Ding (1990—),male,PhD candidate,majors in remote sensing image analysis and virtual geographic environment,E-mail: fwind_email@163.com;;
                                </span>
                                <span>
                                    LI Ke,E-mail: like19771223@163.com;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-13</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National Natural Science Foundation of China(No.41871322);</span>
                                <span>National Defense Foundation of China(No.3601015);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="54">光学遥感影像的目标检测,是遥感影像分析中一项极具挑战的研究内容,其广泛应用于土地规划、环境监测、城市安防、交通规划和军事指挥等众多领域,近年来受到越来越多的关注<citation id="151" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。与此同时,深度学习作为大数据、高性能计算环境下快速发展的一门新兴机器学习技术,具备超强的数据学习能力和高度的特征抽象能力,如何将深度学习应用于遥感影像的目标检测成为新的研究热点。</p>
                </div>
                <div class="p1">
                    <p id="55">在自然图像领域,深度学习的应用极大地提升了目标检测的性能,并出现了许多优秀的算法,主要分为基于区域推荐(region proposals)的检测方法和基于回归的检测方法<citation id="154" type="reference"><link href="4" rel="bibliography" /><link href="6" rel="bibliography" /><link href="8" rel="bibliography" /><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。然而,与自然图像不同,深度学习应用于遥感影像的目标检测更具挑战性。如图1所示,对比VOC2007数据集<citation id="152" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和NWPU VHR-10数据集<citation id="153" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,可以看出:</p>
                </div>
                <div class="p1">
                    <p id="56">(1) 目标尺度多样。遥感影像拍摄高度从几百米到上万米,即使是同类目标成像也大小不一。</p>
                </div>
                <div class="p1">
                    <p id="57">(2) 小目标居多。VOC数据集中目标占据图像的大部分区域,而遥感影像中多为小目标。小目标携带信息量较小,而CNN采样会让信息量进一步减少甚至无法区分。</p>
                </div>
                <div class="p1">
                    <p id="58">(3) 目标外观相似性。如图1所示,道路和桥梁以及篮球场和田径场场地,相似外观增加了区分的难度,此时需要利用上下文(河流和跑道)辅助目标的判别。</p>
                </div>
                <div class="p1">
                    <p id="59">(4) 目标背景复杂。光学遥感影像是对全空间全天候地物信息的真实反映,目标所处的背景信息十分复杂,因此要求检测算法具有较强的抗复杂背景干扰能力。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 VOC2007数据集与NWPU VHR-10数据集对比" src="Detail/GetImg?filename=images/CHXB201910009_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 VOC2007数据集与NWPU VHR-10数据集对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Comparison of VOC2007 dataset with NWPU VHR-10 dataset</p>

                </div>
                <div class="p1">
                    <p id="61">利用深度学习解决遥感影像目标检测问题,学者们已经做了许多工作。文献<citation id="155" type="reference">[<a class="sup">9</a>]</citation>在CNN中添加反卷积层融合CNN网络深层和浅层特征,用于遥感影像中建筑物的检测;文献<citation id="156" type="reference">[<a class="sup">10</a>]</citation>引入优化的ResNet模型解决遥感影像的显著性检测问题;文献<citation id="157" type="reference">[<a class="sup">11</a>]</citation>依据影像中飞机成像大小选取感受野适中的CNN特征,并采样深层CNN特征和浅层CNN特征进行叠加融合;文献<citation id="158" type="reference">[<a class="sup">12</a>]</citation>引入了马尔可夫随机场和全卷积神经网络生成高质量的候选区域;文献<citation id="159" type="reference">[<a class="sup">13</a>]</citation>融合多层CNN特征来描述遥感影像中车辆目标,并改用层级boost分类器判别取得了较好的效果;文献<citation id="160" type="reference">[<a class="sup">14</a>]</citation>利用CNN不同层特征分别检测不同尺度的目标,并结合上下文信息改进了检测效果;文献<citation id="161" type="reference">[<a class="sup">15</a>]</citation>对样本数据进行扩充并且结合目标上下文特征用于遥感影像中飞机目标的检测。可以看出:针对遥感影像中目标尺度多样性和小目标问题,融合CNN网络中不同卷积层所对应的特征,即融合CNN中浅卷积层富含的细节信息和深卷积层富含的语义信息进行特征提取是一种很好的思路<citation id="162" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>,但采用维度拼接或逐像素相加/乘的形式融合多层特征较少考虑不同层特征的分布及尺度差异,特征融合仍是研究的难点;另外,遥感影像中背景复杂性对目标检测干扰较大,需要重点关注上下文信息对目标检测的影响。</p>
                </div>
                <div class="p1">
                    <p id="62">基于上述分析,本文参考区域卷积神经网络(region convolutional neural network,R-CNN)方法的两阶段检测框架,利用混合受限玻尔兹曼机(hybrid restricted boltzmann machine,HRBM)模型,提出一种能够融合CNN多层特征和上下文信息的目标检测方法。本文的主要工作包括:一是提出细节—语义特征融合网络,构建富含细节信息和语义信息的高级目标特征表示,强化特征描述能力;二是结合上下文信息,辅助补充目标特征表示,进一步提高特征判别能力;三是采用难负样本挖掘(hard negative mining)策略,加速SVM分类器训练,有效提升模型检测性能。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">1 本文算法</h3>
                <div class="p1">
                    <p id="64">本文算法的整体结构如图2所示,算法实现主要分为4个步骤:给定一幅图像,首先利用选择性搜索(Selective Search)算法<citation id="163" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>生成一系列感兴趣区域(regions of interest,ROI);其次选取1.0×和1.5×的ROI区域,如图2中上下两个分支所示,作为目标局部(local)区域和目标上下文(context)区域,分别提取能够融合细节信息(conv3)和语义信息(conv5)的目标局部特征和目标上下文特征(与人类视觉类似,CNN自动学习的过程也呈分层特性,浅层如conv3学习到的特征反映物体形状、边缘、纹理等细节信息,而深层如conv5则能学习到对物体位置和尺度等敏感性更低的抽象特征,反映语义信息<citation id="164" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>);然后利用两层HRBM模型融合目标局部特征和目标上下文特征,获取最终用于检测的特征表示;最后将局部—上下文融合特征分别输入SVM分类器和位置回归模型,实现目标类别预测和边界框位置修正。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文目标检测模型架构示意" src="Detail/GetImg?filename=images/CHXB201910009_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文目标检测模型架构示意  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 The architecture of our geospatial object detection model</p>

                </div>
                <h4 class="anchor-tag" id="66" name="66">1.1 ROI提取</h4>
                <div class="p1">
                    <p id="67">传统目标检测方法中通常利用不同尺度的滑动窗口实现“穷举”式检测目标,算法简单但存在大量冗余计算。显然,滑动窗口用于CNN目标检测,高昂的计算代价是无法接受的。因此,迫切希望能够采用一种高效的ROI提取方法。Selective Search算法是一种十分有效的ROI提取方法。算法首先通过图像分割获得超像素构成的子区域,然后利用颜色、纹理、大小等多种合并策略实现层次化的子区域合并,得到较少的可能存在目标的ROI<citation id="165" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。Selective Search算法通过分割和层次化合并保证了ROI的多尺度,而多样化的合并策略则保证了ROI的多适应性。此外比较常用的ROI提取方法还有文献<citation id="166" type="reference">[<a class="sup">19</a>]</citation>提出的采用BING特征的Edge Box算法,以及最近提出的利用CNN特征的ROI提取方法<citation id="167" type="reference"><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>。本文选用Selective Search算法来生成遥感影像中的ROI。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">1.2 混合受限波尔兹曼机</h4>
                <div class="p1">
                    <p id="69">受限玻尔兹曼机(RBM)具有优秀的数据拟合能力,常用来描述变量间的高阶相互作用,研究人员常采用单层或多层RBM结构来提取特征。文献<citation id="168" type="reference">[<a class="sup">22</a>]</citation>中系统介绍了RBM模型的训练,将RBM表示为基于能量的模型,其数学形式如下</p>
                </div>
                <div class="p1">
                    <p id="70"><i>E</i>(<i><b>y</b></i><b>,</b><i><b>v</b></i><b>,</b><i><b>h</b></i>;<i>θ</i>)=-<i><b>h</b></i><sup>T</sup><i><b>Wv</b></i>-<i><b>b</b></i><sup>T</sup><i><b>v</b></i>-<i><b>c</b></i><sup>T</sup><i><b>h</b></i>-<i><b>d</b></i><sup>T</sup><i><b>y</b></i>-<i><b>h</b></i><sup>T</sup><i><b>Uy</b></i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="71">式中,<i><b>v</b></i>=(<i>v</i><sub>1</sub>,<i>v</i><sub>2</sub>,…,<i>v</i><sub><i>n</i></sub>)为可见层变量;<i><b>h</b></i>=(<i>h</i><sub>1</sub>,<i>h</i><sub>2</sub>,…,<i>h</i><sub><i>m</i></sub>)为隐层变量;<i><b>y</b></i>表示样本的标签,采用独热(one-hot)编码;其余变量为系统构建参数<i>θ</i>=(<i><b>W</b></i><b>,</b><i><b>U</b></i><b>,</b><i><b>b</b></i><b>,</b><i><b>c</b></i><b>,</b><i><b>d</b></i><b>),</b><i><b>W</b></i><b>、</b><i><b>U</b></i>表示联结权重,<i><b>b</b></i><b>、</b><i><b>c</b></i><b>、</b><i><b>d</b></i>则分别表示可视层、隐层节点和标签层的偏差量。</p>
                </div>
                <div class="p1">
                    <p id="72">那么,给定样本<i>k</i>,隐层和可见层变量的联合概率分布可定义为</p>
                </div>
                <div class="area_img" id="73">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910009_07300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="75">式中,<i>Z</i>(<i>θ</i>)为归一化常量,代表整个模型系统的能量总和,用于确保将能量函数转化成有效的概率分布形式。</p>
                </div>
                <div class="p1">
                    <p id="76">相应的,可以给出单个样本(<i><b>v</b></i><b>,</b><i><b>h</b></i><b>,</b><i><b>y</b></i>)的条件概率公式</p>
                </div>
                <div class="p1">
                    <p id="77"><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">v</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">h</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><mo>∏</mo><mi>σ</mi></mstyle><mo stretchy="false">(</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">h</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">y</mi></mrow><mo>,</mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><mo>∏</mo><mi>σ</mi></mstyle><mo stretchy="false">(</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>U</mi><msub><mrow></mrow><mrow><mi>j</mi><mo>,</mo><mi>y</mi><mo>=</mo><mn>1</mn></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79"><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">h</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∏</mo><mi>k</mi></munder><mrow><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>U</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mo>,</mo><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup></mrow></msub><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>y</mi></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mi>y</mi></msub><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>U</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mo>,</mo><mi>y</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="80">式中,<i>σ</i>(<i>x</i>)=1/(1+exp(-<i>x</i>),代表sigmoid函数。</p>
                </div>
                <div class="p1">
                    <p id="81">可知,RBM模型的训练目标就是通过最大化似然求解优化参数集<i>θ</i>。根据学习方式不同,RBM又可分为生成式模型(generative RBM,GRBM)和判别式模型(discriminative RBM,DRBM),对应目标函数定义如下</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>e</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">(</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow></munderover><mrow><mi>log</mi></mrow></mstyle><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>θ</i>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="83"><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>i</mtext><mtext>s</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false">(</mo><mi>D</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow></munderover><mrow><mi>log</mi></mrow></mstyle><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mo>|</mo><mrow><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>θ</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="84">二者本质区别在于建模对象不同,GRBM可以增量学习,能够应付数据不完整的情况,而DRBM模型的优势在于容易学习且生成特征更能反映类间差异性<citation id="169" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。为了结合两种模型的优点,本文采用HRBM模型,目标函数定义如下</p>
                </div>
                <div class="p1">
                    <p id="85"><i>L</i><sub>hybrid</sub>(<i>D</i><sub>train</sub>)=<i>αL</i><sub>gen</sub>(<i>D</i><sub>train</sub>)+<i>L</i><sub>disc</sub>(<i>D</i><sub>train</sub>)      (8)</p>
                </div>
                <div class="p1">
                    <p id="86">式中,<i>α</i>为权重参数,用来平衡<i>L</i><sub>disc</sub>和<i>L</i><sub>gen</sub>在目标函数中的比重。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">1.3 细节—语义特征融合网络</h4>
                <div class="p1">
                    <p id="88">如前所述,CNN自动学习的过程呈分层特性,学习到的浅层特征反映细节信息,深层特征反映语义信息。而对于目标检测的两个子任务,分类任务希望特征更多反映语义信息,位置回归任务则希望特征更多反映细节信息,因此需要学习同时保留图像语义信息和细节信息的特征表示。如图3所示,本文设计了细节—语义特征融合网络(detail-semantic feature fusion network,D-SFN),利用HRBM模型的去噪和特征变换能力,实现CNN中细节特征和语义特征的融合。基础网络采用AlexNet模型<citation id="170" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>,保留前5个卷积层(第1和第2卷积层后跟有池化层),各卷积层通道(channel)数分别为96、256、384、384、256。第1层卷积核尺寸(size)为5×5,其余各层卷积核size=3×3,卷积步长(stride)固定为1。池化层采用最大池化(max-pooling)方式,窗口size=2×2,stride=2。后续连接3个全连接层,用于对模型进行微调优化,在微调后将被弃用。最后一个全连接层(fc8)节点数由目标类别决定,本文为11(10类目标+背景)。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 细节—语义特征融合网络" src="Detail/GetImg?filename=images/CHXB201910009_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 细节—语义特征融合网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Detail-semantic feature fusion network</p>

                </div>
                <div class="p1">
                    <p id="90">D-SFN模型实现主要包括3个步骤:①利用优化的CNN提取图像特征,输入检测图像提取conv3(384张)和conv5(256张)层的对应特征图;②ROI特征采样,利用文献<citation id="171" type="reference">[<a class="sup">16</a>]</citation>中的方法,将CNN特征图上采样(up-resampling)至原图大小,随后根据采样区域坐标(1.0倍ROI和1.5倍ROI)截取特征图并映射至3×3大小;③细节—语义特征融合,首先对conv3和conv5层特征分别去噪优化,独立训练HRBM模型<i>f</i><sub>denoise</sub>,学习到的隐层数据就是优化特征<i><b>F</b></i>′<sub>roi</sub></p>
                </div>
                <div class="p1">
                    <p id="91"><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">F</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>e</mtext><mtext>n</mtext><mtext>o</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub><mrow><mo>[</mo><mrow><mtext>r</mtext><mtext>e</mtext><mtext>s</mtext><mtext>h</mtext><mtext>a</mtext><mtext>p</mtext><mtext>e</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>i</mtext></mrow></msub><mo>,</mo><mn>3</mn><mo>×</mo><mn>3</mn><mo>×</mo><mi>C</mi><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="92">式中,<i>C</i>表示特征图的通道数,将特征重组成一维向量训练HRBM。</p>
                </div>
                <div class="p1">
                    <p id="93">然后将式(9)得到的优化的conv3′和conv5′特征拼接作为可见层,训练HRBM模型<i>f</i><sub>fuse</sub>,学习细节—语义融合特征<i><b>F</b></i><sub>fuse</sub></p>
                </div>
                <div class="p1">
                    <p id="94"><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub><mrow><mo>[</mo><mrow><mtext>n</mtext><mtext>o</mtext><mtext>r</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext><mtext>i</mtext><mtext>z</mtext><mtext>e</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>i</mtext></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><msup><mn>3</mn><mo>′</mo></msup></mrow></msubsup><mo>⊕</mo><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>i</mtext></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><msup><mn>5</mn><mo>′</mo></msup></mrow></msubsup><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="95">式中,⊕表示将<i><b>F</b></i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>i</mtext></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><msup><mn>3</mn><mo>′</mo></msup></mrow></msubsup></mrow></math></mathml>和<i><b>F</b></i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>r</mtext><mtext>o</mtext><mtext>i</mtext></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>v</mtext><msup><mn>5</mn><mo>′</mo></msup></mrow></msubsup></mrow></math></mathml>直接拼接在一起;normalize为归一化操作。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">1.4 结合上下文信息</h4>
                <div class="p1">
                    <p id="97">遥感影像中的复杂背景信息对目标检测的影响是不容忽视的,本节将上下文信息引入目标检测模型以提高目标检测的精度。设图像大小<i>w</i>,<i>h</i>,ROI区域坐标(<i>x</i><sub>min</sub>,<i>y</i><sub>min</sub>,<i>x</i><sub>max</sub>,<i>y</i><sub>max</sub>),尺度比例<i>s</i>,定义对应上下文区域</p>
                </div>
                <div class="area_img" id="98">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910009_09800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="100">如图2中上下两个分支所示,上面分支对应目标局部区域(1.0倍ROI),下面分支对应目标上下文区域(1.5倍ROI),分别提取融合细节—语义信息的目标局部特征和目标上下文特征。与1.3节类似,目标局部特征和上下文特征的融合也采用HRBM模型,不同之处是增加了对上下文影响的控制。将学习到的局部特征<i><b>F</b></i><sub>local</sub>和上下文特征<i><b>F</b></i><sub>context</sub>拼接作为新的可见层数据训练HRBM模型,由于归一化操作会导致目标局部特征和上下文特征的激活响应相等,考虑到背景信息的复杂性,引入可学习抑制参数<i>λ</i>=(<i>λ</i><sub>1</sub>,<i>λ</i><sub>2</sub>…,<i>λ</i><sub><i>m</i></sub>),将上下文特征激活值分成<i>m</i>组分别对其进行约束(本文取<i>m</i>=14),得到最终的目标特征表示,本文称之为局部—上下文特征,如公式(12)所示</p>
                </div>
                <div class="p1">
                    <p id="101"><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mtext>j</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mrow><mtext>j</mtext><mtext>o</mtext><mtext>i</mtext><mtext>n</mtext><mtext>t</mtext></mrow></msub><mrow><mo>[</mo><mrow><mtext>n</mtext><mtext>o</mtext><mtext>r</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext><mtext>i</mtext><mtext>z</mtext><mtext>e</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>⊕</mo><mi mathvariant="bold-italic">λ</mi><mo>*</mo><mi mathvariant="bold-italic">F</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>t</mtext><mtext>e</mtext><mtext>x</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="102">式中,<i>f</i><sub>joint</sub>代表HRBM模型;<i>λ</i>*<i><b>F</b></i><sub>context</sub>表示组内逐元素(element-wise)相乘;⊕代表拼接操作。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103">1.5 目标分类预测与定位</h4>
                <div class="p1">
                    <p id="104">本文使用线性SVM对目标特征进行判定,给出每个候选框的类别预测。对每类目标都训练一个线性SVM分类器,训练样本的采集以候选框与该类目标真实边界框的交并比(intersection over union,IoU)为依据,IoU大于0.9的候选框为正样本,IoU小于0.3的候选框为负样本,其余则忽略。由于采集到的负样本数量远远多于正样本,正负样本不均衡会导致训练的分类器产生较多误判,因此本文使用了难负样本挖掘(hard negative mining)策略<citation id="172" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>,在训练过程中,保证正负样本比例1∶3,训练步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="105">(1) 使用所有的正样本,并从IoU小于0.1的负样本中随机挑选一部分训练分类器。</p>
                </div>
                <div class="p1">
                    <p id="106">(2) 用训练好的分类器对所有负样本进行检测,挑出误判为目标的负样本作为难负样本。</p>
                </div>
                <div class="p1">
                    <p id="107">(3) 使用所有正样本,从IoU小于0.1的负样本和步骤(2)获取的难负样本构成的集合中随机挑选一部分,重新训练分类器。</p>
                </div>
                <div class="p1">
                    <p id="108">(4) 重复步骤(2)、(3),直到分类器性能不再提升。</p>
                </div>
                <div class="p1">
                    <p id="109">使用难负样本挖掘策略,可以有效减少分类器误判情况,加快训练速度。同时为了提高目标定位的精度,本文利用最终获取的目标特征训练了一个线性回归模型来修正候选框的位置。</p>
                </div>
                <h3 id="110" name="110" class="anchor-tag">2 试验与分析</h3>
                <h4 class="anchor-tag" id="111" name="111">2.1 试验数据集及评价指标</h4>
                <div class="p1">
                    <p id="112">本文使用试验数据来源于NWPU VHR-10遥感影像数据集,经过进一步整理得到。NWPU VHR-10由西北工业大学公开,共包含800张图像,图像采集自谷歌地球并由领域专家进行标注。数据集共包含10类目标:飞机、舰船、油罐、棒球场、网球场、篮球场、田径场、港口、桥梁和汽车。本文使用了其中650张含有目标的图像(另外150张图像为背景图像,用于半监督或弱监督学习),由于数据集中图像尺寸差异较大(最小533×597像素,而最大1728×1028像素),通过对图像进行裁剪得到仍然包含10类目标图像1172张(图像大小统一为400×400像素)。从中选取879张影像(约为数据集的75%)作为训练集,剩余的293张影像作为测试集,进行后续试验验证分析。试验环境主要包括Intel Core i7 CPU、NvidiaTitan X GPU和64 GB内存,操作系统为Ubuntu16.04 LTS。</p>
                </div>
                <div class="p1">
                    <p id="113">为了定量评价试验结果,本文采用平均准确率(average precision,AP)和类别均值平均准确率(mean average precision,mAP)作为算法检测性能分析的主要指标。准确率计算的是在所有预测为正例的数据中,真正例所占的比例,如式(13)所示</p>
                </div>
                <div class="p1">
                    <p id="114"><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>p</mtext><mtext>r</mtext><mtext>e</mtext><mtext>c</mtext><mtext>i</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo>=</mo><mfrac><mrow><mtext>Τ</mtext><mtext>Ρ</mtext></mrow><mrow><mo stretchy="false">(</mo><mtext>Τ</mtext><mtext>Ρ</mtext><mo>+</mo><mtext>F</mtext><mtext>Ρ</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="115">式中,TP、FP分别代表预测结果中真正例、伪正例的数量,当候选框与真实框的IoU大于0.5为正例,反之为负例。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">2.2 试验参数设置及优化</h4>
                <div class="p1">
                    <p id="117">本文CNN特征提取模型在ImageNet数据集预训练的AlexNet模型基础上进行微调优化,最后一个全连接层节点数改为11,前面5个卷积层直接使用预训练参数,全连接层参数采用“xavier”策略进行初始化,偏置设为0。采用随机梯度下降法更新权值,并设置动量0.9,权值衰减0.000 5。设置最大迭代次数70 000次,学习率为0.001,迭代50 000次,然后学习率设为0.000 1,迭代10 000次,最后学习率设为0.000 01,再迭代10 000次。为扩充训练集,对所有训练样本图像进行旋转增强,旋转角度{20°,40°,…,340°}。</p>
                </div>
                <div class="p1">
                    <p id="118">本文特征提取网络采用逐层贪婪预训练方式学习深度HRBM模型,隐层节点数量和平衡影响因子<i>α</i>采用交叉验证方式确定。经过试验,参数设置见图4。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 特征提取网络结构及参数设置" src="Detail/GetImg?filename=images/CHXB201910009_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 特征提取网络结构及参数设置  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Overall architecture and parameters of feature extraction network</p>

                </div>
                <h4 class="anchor-tag" id="120" name="120">2.3 算法可行性验证分析</h4>
                <h4 class="anchor-tag" id="121" name="121">2.3.1 细节—语义特征融合试验及分析</h4>
                <div class="p1">
                    <p id="122">为了验证细节—语义融合特征用于目标检测的有效性,如表1所示,设计了7组对比试验,分别选择CNN单层特征以及不同层融合特征作为目标特征表示,输入SVM分类器和回归模型进行训练,然后在测试集上测试。从测试结果可以发现,在飞机、棒球场、田径场、篮球场、网球场和港口等6类目标上使用CNN单层特征(conv3、conv4或conv5)检测精度(AP)差别不大;但在汽车、桥梁、舰船和油罐等4类目标上使用conv5层特征检测精度明显较差;相比单层特征,使用融合特征检测各类目标,检测精度有不同程度提升。表1最后一列给出了各组试验在测试集上的mAP,可以发现融合特征相比单层特征的mAP提升显著,说明多层特征融合能获得更好的特征表示。其中,融合3、4、5层特征的mAP最好,融合3、4层特征mAP提升不明显,融合3、5层特征相比融合4、5层特征检测精度有0.3的提升。考虑到多层特征融合的计算成本,本文选择融合3、5层特征,后续试验也都是基于融合3、5层特征的方式。</p>
                </div>
                <div class="area_img" id="123">
                                            <p class="img_tit">
                                                <b>表1 不同层特征融合试验对比</b>
                                                    <br />
                                                <b>Tab.1 Comparison of feature fusion from different layers</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910009_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">(%)</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 不同层特征融合试验对比" src="Detail/GetImg?filename=images/CHXB201910009_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="124" name="124">2.3.2 结合上下文试验及分析</h4>
                <div class="p1">
                    <p id="125">本小节试验在2.3.1小节试验基础上进行,用于验证融合细节—语义特征后,结合上下文信息能否进一步提高算法检测精度。细节—语义特征融合采用conv3+conv5方式,如表2所示,分别取<i>s</i>=1.0,<i>s</i>=1.5和<i>s</i>=2.0倍的ROI进行特征采样,设计了6组对比试验。通过试验可以发现:</p>
                </div>
                <div class="p1">
                    <p id="126">(1) 1、2、3组试验对比。扩大候选区域算法检测精度降低,表明候选区域过大会导致不能有效提取目标特征。此外,可以发现候选区域变化对油罐、汽车等小目标检测精度影响较大,表明上下文信息对小目标检测尤为重要。</p>
                </div>
                <div class="p1">
                    <p id="127">(2) 1、2、5组试验对比。针对网球场、篮球场、田径场、港口、桥梁和汽车等目标,使用局部—上下文联合特征检测效果提升显著,表明引入上下文信息能进一步增强了目标特征表示的判别能力。</p>
                </div>
                <div class="p1">
                    <p id="128">(3) 4、5组试验对比。试验差别在于边界框位置回归使用特征不同,第4组仅使用目标局部特征,而第5组使用局部—上下文特征。两组试验在测试集上检测精度相差只有0.7个百分点,说明本文算法提取的上下文信息对位置回归影响不大。</p>
                </div>
                <div class="p1">
                    <p id="129">(4) 5、6组试验对比。上下文区域过大算法检测精度降低。说明由于目标背景的复杂性,目标特征表示中上下文信息占比不宜过大/过多,以免产生干扰。根据试验结果,本文选用上下文区域的比例<i>s</i>=1.5。</p>
                </div>
                <div class="area_img" id="130">
                                            <p class="img_tit">
                                                <b>表2 结合上下文试验对比</b>
                                                    <br />
                                                <b>Tab.2 Comparison of experiments in the context</b>(%)
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910009_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 结合上下文试验对比" src="Detail/GetImg?filename=images/CHXB201910009_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="131" name="131">2.3.3 不同RBM模型对比</h4>
                <div class="p1">
                    <p id="132">本节试验用于验证不同RBM模型对特征融合的影响,由于特征提取网络包含多个RBM模型,为了保证公平性,试验算法主干网络保持一致,仅修改目标局部特征和上下文特征融合RBM模型(即图2中最后一层RBM模型),分别使用GRBM、DRBM和HRBM模型。训练好的模型在测试集上的检测结果如表3所示,可以看出使用DRBM模型的检测精度要好于使用GRBM模型,而使用HRBM模型的检测精度要显著优于GRBM和DRBM模型。试验结果表明HRBM模型确实能够综合GRBM模型和DRBM模型各自的优势,更好地实现特征融合。</p>
                </div>
                <div class="area_img" id="133">
                                            <p class="img_tit">
                                                <b>表3 不同RBM模型(局部—上下文特征融合)测试结果对比</b>
                                                    <br />
                                                <b>Tab.3 Comparison of different RBM models for local-context feature fusion</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910009_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">(%)</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 不同RBM模型(局部—上下文特征融合)测试结果对比" src="Detail/GetImg?filename=images/CHXB201910009_13300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="134" name="134">2.3.4 与其他方法对比试验</h4>
                <div class="p1">
                    <p id="135">为进一步验证本文方法有效性,将本文方法与几种已有方法进行对比:COPD<citation id="173" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Transferred CNN<citation id="174" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、RICNN<citation id="175" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>和R-CNN<citation id="176" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。为保证试验可比性,几种方法参照原有训练参数设置,在本文的训练集上重新训练。如表4所示,是几种方法在测试集上的测试结果,其中文本加粗项代表本类目标最高检测精度。本文方法在飞机、舰船、棒球场、田径场、港口和桥梁等6类目标的AP值均为最高,特别是对舰船、港口和桥梁3类目标的检测效果相比对比方法提高显著。从整体看,本文方法mAP超过对比方法至少6个百分点,充分验证了本文方法的有效性。此外,试验6是未经过微调而直接使用预训练AlexNet模型提取特征的试验结果,与试验7的mAP相差近3个百分点,这也证明了自然图像和遥感影像数据存在较大差异,表明针对任务数据的微调十分必要。表中最后一列给出了几种方法检测单张图像所需时间,相比对比方法,本文方法由于利用多层RBM模型提取特征,检测耗时有所增加。</p>
                </div>
                <div class="area_img" id="136">
                                            <p class="img_tit">
                                                <b>表4 对比算法和本文算法在测试集上性能比较</b>
                                                    <br />
                                                <b>Tab.4 Performance comparison of competing methods and the proposed method</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910009_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 对比算法和本文算法在测试集上性能比较" src="Detail/GetImg?filename=images/CHXB201910009_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="137">如图5所示,是本文方法在测试集部分图像上的检测结果,其中红色框代表真正例目标(正确检测),绿色代表伪正例目标(误判),黄色代表伪负例目标(漏判)。边界框左上角数字1—10代表预测类别,依次为:飞机、舰船、油罐、棒球场、网球场、篮球场、田径场、港口、桥梁和汽车。从图中可以看出:影像中10类目标的尺度差异很大(田径场等目标充满整幅图像,而汽车、舰船等目标仅占图像很小范围);同类目标纹理形状差异较大,而不同类目标却存在一定相似性;目标分布在复杂的背景中。面对这些挑战,本文方法能够成功检测出图像中大部分的目标,检测性能稳定,证明了本文方法的有效性。</p>
                </div>
                <h3 id="138" name="138" class="anchor-tag">3 结 论</h3>
                <div class="p1">
                    <p id="139">本文设计了基于CNN和HRBM的一体化特征提取网络,用于生成融合细节—语义信息和上下文信息的目标特征表示,融合细节—语义特征解决目标尺度多样性和小目标问题,结合上下文信息解决目标外观相似和背景复杂问题。通过在10类目标构成的NWPU数据集上进行比较试验,验证了本文方法的有效性。尽管本文方法在测试集上检测效果提升显著,相比对比方法提高6个百分点以上,但仍存在一些问题没有解决:①检测较为耗时,本文方法虽然检测精度有了明显提高,但计算量相对较大;②上下文利用不够,如图5中飞机、油罐等目标的分布存在一定的线性、集聚特点,更好地利用这些特点可能进一步提高检测精度。后续工作中将研究候选区域生成方法,提高候选区域生成质量并减少计算量,同时深入挖掘上下文信息应用,探索目标间分布关联关系以及上下文信息在位置回归中的利用,进一步提高光学遥感影像目标检测的精度。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910009_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文算法在测试集图像上检测结果" src="Detail/GetImg?filename=images/CHXB201910009_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文算法在测试集图像上检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910009_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Selected detection results of the proposed method on test set</p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201306009&amp;v=MzEwMzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdWci9PSmlYVGJMRzRIOUxNcVk5RmI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 温奇,李苓苓,刘庆杰,等.基于视觉显著性和图分割的高分辨率遥感影像中人工目标区域提取[J].测绘学报,2013,42(6):831-837.WEN Qi,LI Lingling,LIU Qingjie,et al.A man-made object area extraction method based on visual saliency detection and graph-cut segmentation for high resolution remote sensing imagery[J].Acta Geodaetica et Cartographica Sinica,2013,42(6):831-837.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[2]</b> GIRSHICK R,DONAHUE J,DARRELL T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition.Columbus,OH:IEEE,2014:580-587.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[3]</b> GIRSHICK R.Fast R-CNN[C]//Proceedings of 2015 IEEE International Conference on Computer Vision.Santiago:IEEE,2015:1440-1448.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards realtime object detection with region proposal networks">

                                <b>[4]</b> REN Shaoqing,HE Kaiming,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge,MA:MIT Press,2015:91-99.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 REDMON J,DIVVALA S,GIRSHICK R,et al.You only look once:unified,real-time object detection[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas,NV:IEEE,2016:779-788.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[6]</b> LIU Wei,ANGUELOV D,ERHAN D,et al.SSD:single shot multibox detector[C]//Proceedings of the 14th European Conference on Computer Vision.Amsterdam:Springer,2016:21-37.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003682794&amp;v=Mjg2NDBTWHFScnhveGNNSDdSN3FkWitadUZ5bmxVYnJOSUZnPU5qN0Jhck80SHRIUHFZZEhZK0lMWTNrNXpCZGg0ajk5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> EVERINGHAM M,VAN GOOL L,WILLIAMS C K I,et al.The Pascal visual object classes (VOC) challenge[J].International Journal of Computer Vision,2010,88(2):303-338.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700308532&amp;v=MjU5Mzg0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3N0lJbHNSYWhRPU5pZk9mYks4SDlETXFJOUZaK3NIQ1g4N29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> CHENG Gong,HAN Junwei,ZHOU Peicheng,et al.Multi-class geospatial object detection and geographic image classification based on collection of part detectors[J].ISPRS Journal of Photogrammetry and Remote Sensing,2014,98(12):119-132.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806020&amp;v=MDAwOTE0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVnIvT0ppWFRiTEc0SDluTXFZOUhaSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 伍广明,陈奇,SHIBASAKI R,等.基于U形卷积神经网络的航空影像建筑物检测[J].测绘学报,2018,47(6):864-872.DOI:10.11947/j.AGCS.2018.20170651.WU Guangming,CHEN Qi,SHIBASAKI R,et al.High precision building detection from aerial imagery using a U-Net like convolutional architecture[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):864-872.DOI:10.11947/j.AGCS.2018.20170651.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806021&amp;v=MjYzMjU3cWZadWR2RnlyZ1ZyL09KaVhUYkxHNEg5bk1xWTlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 戴玉超,张静,PORIKLI F,等.深度残差网络的多光谱遥感图像显著目标检测[J].测绘学报,47(6):873-881.DOI:10.11947/j.AGCS.2018.20170633.DAI Yuchao,ZHANG Jing,PORIKLI F,et al.Salient object detection from multi-spectral remote sensing images with deep residual network[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):873-881.DOI:10.11947/j.AGCS.2018.20170633.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rapid Airplane Detection in Remote Sensing Images Based on Multilayer Feature Fusion in Fully Convolutional Neural Networks">

                                <b>[11]</b> XU Yuelei,ZHU Mingming,XIN Peng,et al.Rapid airplane detection in remote sensing images based on multilayer feature fusion in fully convolutional neural networks[J].Sensors,2018,18(7):2335.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=M-FCN Effective Fully Convolutional Network-based Airplane Detection Framework">

                                <b>[12]</b> YANG Yiding,ZHUANG Yin,BI Fukun,et al.M-FCN:effective fully convolutional network-based airplane detection framework[J].IEEE Geoscience and Remote Sensing Letters,2017,14(8):1293-1297.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle Detection in Aerial Images Based on Region Convolutional Neural Networks and Hard Negative Example Mining">

                                <b>[13]</b> TANG Tianyu,ZHOU Shilin,DENG Zhipeng,et al.Vehicle detection in aerial images based on region convolutional neural networks and hard negative example mining[J].Sensors,2017,17(2):336.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network">

                                <b>[14]</b> GUO Wei,YANG Wen,ZHANG Haijian,et al.Geospatial object detection in high resolution satellite images based on multi-scale convolutional neural network[J].Remote Sensing,2018,10(1):131.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Airplane Detection Using Transfer Learning in Remote Sensing Images">

                                <b>[15]</b> CHEN Zhong,ZHANG Ting,OUYANG Chao.End-to-end airplane detection using transfer learning in remote sensing images[J].Remote Sensing,2018,10(1):139.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">

                                <b>[16]</b> LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition.Boston,MA:IEEE,2015:3431-3440.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MjAzNzhud1plWnRGaW5sVTc3SUlsc1JhaFE9Tmo3QmFySzdIdG5Nclk5RlpPb01Dbjg5b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> UIJLINGS J R R,VAN DE SANDE K E A,GEVERS T,et al.Selective search for object recognition[J].International Journal of Computer Vision,2013,104(2):154-171.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[18]</b> ZEILER M D,FERGUS R.Visualizing and understanding convolutional networks[C]//Proceedings of the 13th European Conference on Computer Vision.Zurich:Springer,2014:818-833.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Edge boxes:locating object proposals from edges">

                                <b>[19]</b> ZITNICK C L,DOLLÁR P.Edge boxes:locating object proposals from edges[C]//Proceedings of the 13th European Conference on Computer Vision.Zurich:Springer,2014:391-405.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepbox:Learning Objectness with Convolutional Networks">

                                <b>[20]</b> KUO Weicheng,HARIHARAN B,MALIK J.DeepBox:learning objectness with convolutional networks[C]//Proceedings of 2015 IEEE International Conference on Computer Vision.Santiago:IEEE,2015:2479-2487.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evolving boxes for fast vehicle detection">

                                <b>[21]</b> WANG Li,LU Yao,WANG Hong,et al.Evolving boxes for fast vehicle detection[C]//Proceedings of 2017 IEEE International Conference on Multimedia and Expo.Hong Kong,China:IEEE,2017:1135-1140.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MTQzMDRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3N0lJbHNSYWhRPU5pZkpaYks5SHRqTXFvOUZaT29OQ1g4eA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> HINTON G E,OSINDERO S,TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation,2006,18(7):1527-1554.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning algorithms for the classification restricted Boltzmann machine">

                                <b>[23]</b> LAROCHELLE H,MANDEL M,PASCANU R,et al.Learning algorithms for the classification restricted Boltzmann machine[J].Journal of Machine Learning Research,2012,13(1):643-669.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classifi cation with deep convolutional neural networks">

                                <b>[24]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.Lake Tahoe:ACM,2012:1097-1105.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">

                                <b>[25]</b> FELZENSZWALB P F,GIRSHICK R B,MCALLESTER D,et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(9):1627-1645.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images">

                                <b>[26]</b> CHENG Gong,ZHOU Peicheng,HAN Junwei.Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images[J].IEEE Transactions on Geoscience and Remote Sensing,2016,54(12):7405-7415.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201910009" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201910009&amp;v=MjU3NjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ZyL09KaVhUYkxHNEg5ak5yNDlGYllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
