<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142621921045000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201906006%26RESULT%3d1%26SIGN%3d0zf8qQaFUHtsXMLYPbdPWRKBihM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201906006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906006&amp;v=MTA5MTN5M2tXN3ZJSmlYVGJMRzRIOWpNcVk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#60" data-title="1 总体流程与关键技术 ">1 总体流程与关键技术</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="1.1 总体流程">1.1 总体流程</a></li>
                                                <li><a href="#66" data-title="1.2 运动模型追踪参考关键帧">1.2 运动模型追踪参考关键帧</a></li>
                                                <li><a href="#78" data-title="1.3 逆深度滤波器">1.3 逆深度滤波器</a></li>
                                                <li><a href="#98" data-title="1.4 后端混合局部优化框架">1.4 后端混合局部优化框架</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="2 试验与分析 ">2 试验与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="2.1 试验数据与评价方法">2.1 试验数据与评价方法</a></li>
                                                <li><a href="#121" data-title="2.2 计算效率分析">2.2 计算效率分析</a></li>
                                                <li><a href="#126" data-title="2.3 相机轨迹误差分析">2.3 相机轨迹误差分析</a></li>
                                                <li><a href="#131" data-title="2.4 三维重建效果">2.4 三维重建效果</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="3 结 论 ">3 结 论</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 本文方法总体流程">图1 本文方法总体流程</a></li>
                                                <li><a href="#68" data-title="图2 运动模型追踪参考关键帧流程">图2 运动模型追踪参考关键帧流程</a></li>
                                                <li><a href="#84" data-title="图3 深度估计误差模型">图3 深度估计误差模型</a></li>
                                                <li><a href="#97" data-title="图4 图像块模型与极线搜索">图4 图像块模型与极线搜索</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表1 试验数据基本信息&lt;/b&gt;"><b>表1 试验数据基本信息</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表2 系统总体运行时间对比&lt;/b&gt;"><b>表2 系统总体运行时间对比</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表3 不同方法位姿估计轨迹误差比较&lt;/b&gt;"><b>表3 不同方法位姿估计轨迹误差比较</b></a></li>
                                                <li><a href="#134" data-title="图5 直接法深度优化因子图">图5 直接法深度优化因子图</a></li>
                                                <li><a href="#135" data-title="图6 不同类型地图点">图6 不同类型地图点</a></li>
                                                <li><a href="#136" data-title="图7 试验数据">图7 试验数据</a></li>
                                                <li><a href="#139" data-title="图8 关键帧特征点匹配率">图8 关键帧特征点匹配率</a></li>
                                                <li><a href="#140" data-title="图9 相机平面轨迹与地面真值">图9 相机平面轨迹与地面真值</a></li>
                                                <li><a href="#169" data-title="图10 本文方法重建效果">图10 本文方法重建效果</a></li>
                                                <li><a href="#169" data-title="图10 本文方法重建效果">图10 本文方法重建效果</a></li>
                                                <li><a href="#143" data-title="图11 UAV_Lab数据局部重建细节对比">图11 UAV_Lab数据局部重建细节对比</a></li>
                                                <li><a href="#144" data-title="图12 UAV_Building数据局部重建细节对比">图12 UAV_Building数据局部重建细节对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" 刘浩敏, 章国锋, 鲍虎军.基于单目视觉的同时定位与地图构建方法综述[J].计算机辅助设计与图形学学报, 2016, 28 (6) :855-868.LIU Haomin, ZHANG Guofeng, BAO Hujun.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2016, 28 (6) :855-868." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201606001&amp;v=MzA2NDNVUjdxZlp1ZHZGeTNrVzd2SUx6N0JhTEc0SDlmTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         刘浩敏, 章国锋, 鲍虎军.基于单目视觉的同时定位与地图构建方法综述[J].计算机辅助设计与图形学学报, 2016, 28 (6) :855-868.LIU Haomin, ZHANG Guofeng, BAO Hujun.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics, 2016, 28 (6) :855-868.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" 邸凯昌, 万文辉, 赵红颖, 等.视觉SLAM技术的进展与应用[J].测绘学报, 2018, 47 (6) :770-779.DOI:10.11947/j.AGCS.2018.20170652.DI Kaichang, WAN Wenhui, ZHAO Hongying, et al.Progress and applications of visual SLAM[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (6) :770-779.DOI:10.11947/j.AGCS.2018.20170652." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806010&amp;v=MTc3OTA1NE8zenFxQnRHRnJDVVI3cWZadWR2Rnkza1c3dklKaVhUYkxHNEg5bk1xWTlFWklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         邸凯昌, 万文辉, 赵红颖, 等.视觉SLAM技术的进展与应用[J].测绘学报, 2018, 47 (6) :770-779.DOI:10.11947/j.AGCS.2018.20170652.DI Kaichang, WAN Wenhui, ZHAO Hongying, et al.Progress and applications of visual SLAM[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (6) :770-779.DOI:10.11947/j.AGCS.2018.20170652.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" CADENA C, CARLONE L, CARRILLO H, et al.Past, present, and future of simultaneous localization and mapping:toward the robust-perception age[J].IEEE Transactions on Robotics, 2017, 32 (6) :1309-1332." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Past,Present,and Future of Simultaneous Localization and Mapping:Toward the Robust-Perception Age">
                                        <b>[3]</b>
                                         CADENA C, CARLONE L, CARRILLO H, et al.Past, present, and future of simultaneous localization and mapping:toward the robust-perception age[J].IEEE Transactions on Robotics, 2017, 32 (6) :1309-1332.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" DAVISON A J, REID I D, MOLTON N D, et al.MonoSLAM:real-time single camera SLAM[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (6) :1052-1067." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MonoSLAM: Real-time single camera SLAM">
                                        <b>[4]</b>
                                         DAVISON A J, REID I D, MOLTON N D, et al.MonoSLAM:real-time single camera SLAM[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (6) :1052-1067.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" KLEIN G, MURRAY D.Parallel tracking and mapping for small AR workspaces[C]//Proceedings of 2007 IEEE and ACM International Symposium on Mixed and Augmented Reality.Nara, Japan:IEEE, 2007:225-234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel tracking and mapping for small AR workspace">
                                        <b>[5]</b>
                                         KLEIN G, MURRAY D.Parallel tracking and mapping for small AR workspaces[C]//Proceedings of 2007 IEEE and ACM International Symposium on Mixed and Augmented Reality.Nara, Japan:IEEE, 2007:225-234.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" STRASDAT H, DAVISON A J, MONTIEL J M M, et al.Double window optimisation for constant time visual SLAM[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2352-2359." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Double window optimisation for constant time visual SLAM">
                                        <b>[6]</b>
                                         STRASDAT H, DAVISON A J, MONTIEL J M M, et al.Double window optimisation for constant time visual SLAM[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2352-2359.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" TRIGGS B, MCLAUCHLAN P F, HARTLEY R I, et al.Bundle adjustment—a modern synthesis[M].TRIGGS B, ZISSERMAN A, SZELISKI R.Vision Algorithms:Theory and Practice.Berlin, Heidelberg:Springer, 2000:298-372." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bundle Adjustment-A Modern Synthesis">
                                        <b>[7]</b>
                                         TRIGGS B, MCLAUCHLAN P F, HARTLEY R I, et al.Bundle adjustment—a modern synthesis[M].TRIGGS B, ZISSERMAN A, SZELISKI R.Vision Algorithms:Theory and Practice.Berlin, Heidelberg:Springer, 2000:298-372.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" MUR-ARTAL R, MONTIEL J M M, TARD&#211;S J D.ORB-SLAM:a versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics, 2015, 31 (5) :1147-1163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM: A Versatile and Accurate Monocular SLAM System">
                                        <b>[8]</b>
                                         MUR-ARTAL R, MONTIEL J M M, TARD&#211;S J D.ORB-SLAM:a versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics, 2015, 31 (5) :1147-1163.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     MUR-ARTAL R, TARD&#211;S J D.ORB-SLAM2:an open-source SLAM system for monocular, stereo, and RGB-D cameras[J].IEEE Transactions on Robotics, 2016, 33 (5) :1255-1262.</a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2564-2571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">
                                        <b>[10]</b>
                                         RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2564-2571.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" DUBBELMAN G, BROWNING B.COP-SLAM:closed-form online pose-chain optimization for visual SLAM[J].IEEE Transactions on Robotics, 2015, 31 (5) :1194-1213." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=COP-SLAM:Closed-Form Online Pose-Chain Optimization for Visual SLAM">
                                        <b>[11]</b>
                                         DUBBELMAN G, BROWNING B.COP-SLAM:closed-form online pose-chain optimization for visual SLAM[J].IEEE Transactions on Robotics, 2015, 31 (5) :1194-1213.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" VOGIATZIS G, HERN&#193;NDEZ C.Video-based, real-time multi-view stereo[J].Image and Vision Computing, 2011, 29 (7) :434-441." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201348807&amp;v=MDU2MzljYmhJPU5pZk9mYks3SHRET3JZOUVaKzhIQkh3K29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUpWOA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         VOGIATZIS G, HERN&#193;NDEZ C.Video-based, real-time multi-view stereo[J].Image and Vision Computing, 2011, 29 (7) :434-441.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" FORSTER C, ZHANG Zichao, GASSNER M, et al.SVO:semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics, 2017, 33 (2) :249-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SVO:Semidirect Visual Odometry for Monocular and Multicamera Systems">
                                        <b>[13]</b>
                                         FORSTER C, ZHANG Zichao, GASSNER M, et al.SVO:semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics, 2017, 33 (2) :249-265.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" ENGEL J, STURM J, CREMERS D.Semi-dense visual odometry for a monocular camera[C]//Proceedings of 2013 IEEE International Conference on Computer Vision.Sydney:IEEE, 2013:1449-1456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-dense visual odometry for a monocular camera">
                                        <b>[14]</b>
                                         ENGEL J, STURM J, CREMERS D.Semi-dense visual odometry for a monocular camera[C]//Proceedings of 2013 IEEE International Conference on Computer Vision.Sydney:IEEE, 2013:1449-1456.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" KERL C, STURM J, CREMERS D.Robust odometry estimation for RGB-D cameras[C]//Proceedings of 2013 IEEE International Conference on Robotics and Automation.Karlsruhe:IEEE, 2013:3748-3754." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust odometry estimation for RGB-D cameras">
                                        <b>[15]</b>
                                         KERL C, STURM J, CREMERS D.Robust odometry estimation for RGB-D cameras[C]//Proceedings of 2013 IEEE International Conference on Robotics and Automation.Karlsruhe:IEEE, 2013:3748-3754.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" ST&#220;HMER J, GUMHOLD S, CREMERS D.Real-time dense geometry from a handheld camera[C]//Proceedings of Joint Pattern Recognition Symposium.Darmstadt:Springer, 2010:11-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time dense geometry from a handheld camera">
                                        <b>[16]</b>
                                         ST&#220;HMER J, GUMHOLD S, CREMERS D.Real-time dense geometry from a handheld camera[C]//Proceedings of Joint Pattern Recognition Symposium.Darmstadt:Springer, 2010:11-20.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" NEWCOMBE R A, LOVEGROVE S J, DAVISON A J.DTAM:dense tracking and mapping in real-time[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2320-2327." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DTAM:dense tracking and mapping in real-time">
                                        <b>[17]</b>
                                         NEWCOMBE R A, LOVEGROVE S J, DAVISON A J.DTAM:dense tracking and mapping in real-time[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2320-2327.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" ENGEL J, SCH&#214;PS T, CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[C]//Proceedings of European Conference on Computer Vision.Zurich:Springer, 2014:834-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lsdslam:Large-scale direct monocular slam">
                                        <b>[18]</b>
                                         ENGEL J, SCH&#214;PS T, CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[C]//Proceedings of European Conference on Computer Vision.Zurich:Springer, 2014:834-849.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_19" title=" ENGEL J, KOLTUN V, CREMERS D.Direct sparse odometry[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (3) :611-625." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Direct Sparse Odometry">
                                        <b>[19]</b>
                                         ENGEL J, KOLTUN V, CREMERS D.Direct sparse odometry[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (3) :611-625.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_20" title=" MUR-ARTAL R, TARD&#211;S J D.Probabilistic semi-dense mapping from highly accurate feature-based monocular SLAM[C]//Proceedings of Conference on Robotics:Science and Systems.Rome:Universidad Zaragoza, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic Semi-Dense Mapping from Highly Accurate Feature-Based Monocular SLAM">
                                        <b>[20]</b>
                                         MUR-ARTAL R, TARD&#211;S J D.Probabilistic semi-dense mapping from highly accurate feature-based monocular SLAM[C]//Proceedings of Conference on Robotics:Science and Systems.Rome:Universidad Zaragoza, 2015.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_21" title=" CIVERA J, DAVISON A J, MONTIEL J M M.Inverse depth parametrization for monocular SLAM[J].IEEE Transactions on Robotics, 2008, 24 (5) :932-945." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inverse Depth Parametrization for Monocular SLAM">
                                        <b>[21]</b>
                                         CIVERA J, DAVISON A J, MONTIEL J M M.Inverse depth parametrization for monocular SLAM[J].IEEE Transactions on Robotics, 2008, 24 (5) :932-945.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_22" title=" 高翔, 张涛, 刘毅, 等.视觉SLAM十四讲——从理论到实践[M].北京:电子工业出版社, 2017:340-341.GAO Xiang, ZHANG Tao, LIU Yi, et al.Visual SLAM fourteen lectures-from theory to practice[M].Beijing:China Machine Press, 2017:340-341." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121311048000&amp;v=MTEyMjZVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5amtVN3JKS0ZvVVhGcXpHYks2SDlMTnJvOUJiT3NQREJNOHp4&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         高翔, 张涛, 刘毅, 等.视觉SLAM十四讲——从理论到实践[M].北京:电子工业出版社, 2017:340-341.GAO Xiang, ZHANG Tao, LIU Yi, et al.Visual SLAM fourteen lectures-from theory to practice[M].Beijing:China Machine Press, 2017:340-341.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_23" title=" 于英, 张永生, 薛武, 等.影像连接点均衡化高精度自动提取[J].测绘学报, 2017, 46 (1) :90-97.DOI:10.11947/j.AGCS.2017.20160320.YU Ying, ZHANG Yongsheng, XUE Wu, et al.Automatic tie points extraction with uniform distribution and high precision[J].Acta Geodaetica et Cartographica Sinica, 2017, 46 (1) :90-97.DOI:10.11947/j.AGCS.2017.20160320." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201701014&amp;v=Mjk4MzQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2tXN3ZJSmlYVGJMRzRIOWJNcm8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         于英, 张永生, 薛武, 等.影像连接点均衡化高精度自动提取[J].测绘学报, 2017, 46 (1) :90-97.DOI:10.11947/j.AGCS.2017.20160320.YU Ying, ZHANG Yongsheng, XUE Wu, et al.Automatic tie points extraction with uniform distribution and high precision[J].Acta Geodaetica et Cartographica Sinica, 2017, 46 (1) :90-97.DOI:10.11947/j.AGCS.2017.20160320.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_24" title=" STURM J, ENGELHARD N, ENDRES F, et al.A benchmark for the evaluation of RGB-D SLAM systems[C]//Proceedings of 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.Vilamoura-Algarve:IEEE, 2012:573-580." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A benchmark for the evaluation of RGB-D SLAM systems">
                                        <b>[24]</b>
                                         STURM J, ENGELHARD N, ENDRES F, et al.A benchmark for the evaluation of RGB-D SLAM systems[C]//Proceedings of 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.Vilamoura-Algarve:IEEE, 2012:573-580.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_25" title=" ContextCapture.Create 3D models from simple photographs[EB/OL]. (2018-08-31) .https://www.bentley.com/en/products/brands/contextcapture." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Create 3D models from simple photographs">
                                        <b>[25]</b>
                                         ContextCapture.Create 3D models from simple photographs[EB/OL]. (2018-08-31) .https://www.bentley.com/en/products/brands/contextcapture.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_26" title=" MUR-ARTAL R, TARD&#211;S J D.Fast relocalisation and loop closing in keyframe-based SLAM[C]//Proceedings of 2014 IEEE International Conference on Robotics and Automation.Hong Kong:IEEE, 2014:846-853." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast relocalisation and loop closing in keyframe-based SLAM">
                                        <b>[26]</b>
                                         MUR-ARTAL R, TARD&#211;S J D.Fast relocalisation and loop closing in keyframe-based SLAM[C]//Proceedings of 2014 IEEE International Conference on Robotics and Automation.Hong Kong:IEEE, 2014:846-853.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(06),708-717 DOI:10.11947/j.AGCS.2019.20180421            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>特征法视觉SLAM逆深度滤波的三维重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B8%80&amp;code=28614660&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张一</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E6%8C%BA&amp;code=20616112&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜挺</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E5%88%9A%E6%AD%A6&amp;code=20686968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江刚武</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E5%B2%B8%E7%AB%B9&amp;code=26360843&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余岸竹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E8%8B%B1&amp;code=24201910&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于英</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学地理空间信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有特征法视觉SLAM只能重建稀疏点云、非关键帧对地图点深度估计无贡献等问题, 本文提出一种特征法视觉SLAM逆深度滤波的三维重建方法, 可利用视频序列影像实时、增量式地构建相对稠密的场景结构。具体来说, 设计了一种基于运动模型的关键帧追踪流程, 能够提供精确的相对位姿关系;采用一种基于概率分布的逆深度滤波器, 地图点通过多帧信息累积、更新得到, 而不再由两帧三角化直接获取;提出一种基于特征法与直接法的后端混合优化框架, 以及基于平差约束的地图点筛选策略, 可以准确、高效解算相机位姿与场景结构。试验结果表明, 与现有方法相比, 本文方法具有更高的计算效率和位姿估计精度, 而且能够重建出全局一致的较稠密点云地图。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E5%8D%B3%E6%97%B6%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%9C%B0%E5%9B%BE%E6%9E%84%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉即时定位与地图构建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%86%E6%B7%B1%E5%BA%A6%E6%BB%A4%E6%B3%A2%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">逆深度滤波器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%90%E5%8A%A8%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">运动模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%90%8E%E7%AB%AF%E6%B7%B7%E5%90%88%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">后端混合优化框架;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张一 (1989—) , 男, 博士生, 研究方向为数字摄影测量与视觉SLAM。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (41501482;41471387;41801388);</span>
                    </p>
            </div>
                    <h1>3D reconstruction with inverse depth filter of feature-based visual SLAM</h1>
                    <h2>
                    <span>ZHANG Yi</span>
                    <span>JIANG Ting</span>
                    <span>JIANG Gangwu</span>
                    <span>YU Anzhu</span>
                    <span>YU Ying</span>
            </h2>
                    <h2>
                    <span>Institute of Surveying and Mapping, Information Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem that the current feature-based visual SLAM can only reconstruct a sparse point cloud and the ordinary frame does not contribute to point depth estimation, a novel 3 D reconstruction method with inverse depth filter of feature-based visual SLAM is proposed, which utilizes video sequence to incrementally build a denser scene structure in real-time. Specifically, a motion model based keyframe tracking approach is designed to provide accurate relative pose relationship. The map point is no longer calculated directly by two-frame-triangulation, instead it is accumulated and updated by information of several frames with an inverse depth filter based on probability distribution. A back-end hybrid optimization framework composed of feature and direct method is introduced, as well as an adjustment constraint based point screening strategy, which can precisely and efficiently solve camera pose and structure. The experimental results demonstrate the superiority of proposed method on computational speed and pose estimation accuracy compared with existing methods. Meanwhile, it is shown that our method can reconstruct a denser globally consistent point cloud map.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20simultaneous%20localization%20and%20mapping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual simultaneous localization and mapping;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=inverse%20depth%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">inverse depth filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=motion%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">motion model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=back-end%20hybrid%20optimization%20framework&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">back-end hybrid optimization framework;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Yi (1989—) , male, PhD candidate, majors in digital photogrammetry and visual SLAM.E-mail: 276690308@qq.com;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-06</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National Natural Science Foundation of China (Nos.41501482; 41471387; 41801388);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="54">视觉即时定位与地图构建 (visual simultaneous localization and mapping, VSLAM) 技术, 以视频序列影像为输入, 能够在恢复相机运动轨迹的同时, 实时重建未知场景的三维结构, 可用于无人机、车辆、机器人等平台的智能环境感知、自动驾驶与导航<citation id="145" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 也可用于应急测绘、灾害与突发事件监测、虚拟与增强现实等场景, 具有广阔的应用前景与市场潜力<citation id="146" type="reference"><link href="4" rel="bibliography" /><link href="6" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="55">目前视觉SLAM算法主要采用基于关键帧优化的特征法<citation id="148" type="reference"><link href="8" rel="bibliography" /><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>进行实现, 其基本流程是:首先从每幅影像中提取出具有可重复性与显著区分性的点、线特征;使用不变性描述符在后续帧中对特征进行匹配, 利用多视图几何原理恢复相机位姿与结构, 然后通过光束法平差<citation id="147" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>进行联合优化。</p>
                </div>
                <div class="p1">
                    <p id="56">在诸多特征法SLAM中最具代表性的是ORB-SLAM<citation id="152" type="reference"><link href="16" rel="bibliography" /><link href="18" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>, 它采用统一的ORB<citation id="149" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> (oriented FAST and rotated BRIEF) 特征进行计算, 在室内外环境中均能稳健、实时运行, 可在大视角差异下完成重定位与闭环检测, 并且由像点匹配累积的共视<citation id="150" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>信息可用于构建强壮的平差与图优化<citation id="151" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>网络, 有利于得到全局最优解。ORB-SLAM的主要缺陷在于, 地图点通过两关键帧三角化直接得到, 对于短基线视频影像, 其深度不确定性较大, 为避免粗差必须采取严格的筛选策略, 导致所构点云地图十分稀疏。此外, ORB-SLAM的非关键帧在追踪完后即被抛弃, 其对地图点深度信息无贡献, 造成了资源浪费。</p>
                </div>
                <div class="p1">
                    <p id="57">从应用层面看, 稀疏点云仅可用于传感器的自身定位, 而较高级的视觉导航、避障、虚拟增强现实、模型重建、语义识别等应用, 都需要更加稠密的地图表达。为此, 直接法相关技术获得了更多关注, 它根据像素亮度差异直接构建优化问题求解相机位姿, 并采用一种基于概率分布的深度滤波器模型<citation id="153" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>, 利用多帧影像更新地图点深度信息。早期的直接法多根据稠密深度图进行计算, 以最大化利用影像信息并增加稳健性<citation id="154" type="reference"><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>, 这类算法一般需要GPU加速来满足计算要求。文献<citation id="155" type="reference">[<a class="sup">14</a>,<a class="sup">18</a>]</citation>使用具有显著梯度的像素, 在CPU上完成了较稠密的实时重建;文献<citation id="156" type="reference">[<a class="sup">13</a>,<a class="sup">19</a>]</citation>提出了仅需少量像素的稀疏直接法, 使算法具有更强的适用性。</p>
                </div>
                <div class="p1">
                    <p id="58">直接法不必进行特征提取与匹配, 因而具备了更高的计算效率, 但它易受相机曝光、环境光源等因素影响, 当场景亮度变化剧烈时可能导致求解失败, 稳健性不足<citation id="157" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。另外, 直接法优化基于像素本身进行操作, 该过程需要完整的影像数据, 对于大规模场景来说, 由于设备存储空间有限, 直接法难以进行全局质量控制。为此文献<citation id="158" type="reference">[<a class="sup">20</a>]</citation>将概率分布构图方法应用于ORB-SLAM中, 使得其在保留特征法优点的同时, 能够重建出精确的稠密场景结构。但由于其构图模块独立于ORB-SLAM系统并具有若干关键帧的延迟, 严格意义上并不属于在线处理, 并且构图结果无法用于后续帧的位姿追踪, 作用有限。</p>
                </div>
                <div class="p1">
                    <p id="59">在总结前文算法的基础上, 本文提出一种特征法视觉SLAM逆深度滤波的三维重建方法, 无须GPU加速即可实时、增量式地重建出较为稠密的点云地图。在前端中, 设计了一种基于运动模型的参考关键帧追踪流程, 能够有效利用视频影像先验约束获取精确的相对位姿关系;建模并分析了深度估计误差, 采用一种基于概率分布的逆深度滤波器构图方法, 可得到更加稠密的地图点。在后端中, 提出一种基于特征法与直接法混合的优化框架以及基于平差约束的点位筛选策略, 可以准确、高效地恢复相机位姿与场景结构。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">1 总体流程与关键技术</h3>
                <h4 class="anchor-tag" id="61" name="61">1.1 总体流程</h4>
                <div class="p1">
                    <p id="62">本文核心思路是:地图点不再由两帧三角化直接得到, 而是先构造为种子点, 经多帧信息融合直至深度收敛后再插入地图。为此设计总体流程如图1所示, 系统以ORB-SLAM为基础框架, 仍采用前后端结合的三线程结构, 主要改动集中于追踪与局部构图线程, 以粗体表示。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法总体流程" src="Detail/GetImg?filename=images/CHXB201906006_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法总体流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Flowchart of the proposed algorithm</p>

                </div>
                <div class="p1">
                    <p id="64">对于输入的每一帧视频序列影像, 首先在前端追踪线程中提取ORB特征, 利用运动模型提供的帧间先验约束, 将参考关键帧的地图点投影至当前帧, 进行特征匹配与位姿估计;然后在逆深度滤波器中对参考关键帧的种子点进行极线匹配并更新其深度;最后判断该当前帧是否应作为新关键帧插入局部构图线程。</p>
                </div>
                <div class="p1">
                    <p id="65">后端局部构图线程收到新关键帧后, 首先将原参考关键帧中所有深度收敛的种子点激活为新的地图点, 并与邻近关键帧的其他局部地图点一起投影至新关键帧以建立更多匹配;随后根据各地图点的观测情况, 采用一种基于特征法与直接法的混合框架优化方法, 联合求解局部相机位姿与场景三维结构;在剔除掉地图野点与重复关键帧后, 将新关键帧作为后续影像的追踪参考帧, 其未匹配的特征点构造为新的种子点;最后, 将新关键帧送入闭环线程以实现最优全局一致性。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 运动模型追踪参考关键帧</h4>
                <div class="p1">
                    <p id="67">准确的极线几何关系是本文逆深度滤波器顺利更新种子点深度的前提, 因此前端系统需提供当前帧<i>I</i><sub><i>c</i></sub>关于参考关键帧<i>I</i><sub><i>r</i></sub>的精确相对位姿<b><i>T</i></b><sub><i>cr</i></sub>∈SE (3) 。然而ORB-SLAM采用逐帧追踪的方式, 当前帧并不直接与参考帧进行匹配, <b><i>T</i></b><sub><i>cr</i></sub>只能通过间接方式求出, 由于存在误差累积等因素, 该方法用于逆深度滤波器的效果较差。为此本文设计了一种新的追踪方式, 可直接计算当前帧关于参考关键帧的相对位姿<b><i>T</i></b><sub><i>cr</i></sub>, 基本流程如图2所示。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 运动模型追踪参考关键帧流程" src="Detail/GetImg?filename=images/CHXB201906006_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 运动模型追踪参考关键帧流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Flowchart of tracking reference keyframe with motion model</p>

                </div>
                <div class="p1">
                    <p id="69">由于视频影像通常具有较高帧率, 相机运动一般比较平稳, 具备丰富的先验信息, 能够为追踪系统提供良好的初值。常速运动模型假设短时间内相机帧间相对位姿保持不变, 即</p>
                </div>
                <div class="p1">
                    <p id="70"><b><i>T</i></b><sub><i>c</i>, <i>c</i>-1</sub>=<b><i>T</i></b><sub><i>c</i>-1, <i>c</i>-2</sub>=<b><i>T</i></b><sub><i>v</i></sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="71">式中, <b><i>T</i></b><sub><i>v</i></sub>∈<i>SE</i> (3) 即为常速运动模型。由于参考帧<i>I</i><sub><i>r</i></sub>位姿<b><i>T</i></b><sub><i>rw</i></sub>已知, 上一帧<i>I</i><sub><i>c</i>-1</sub>的相对位姿<b><i>T</i></b><sub><i>c</i>-1, <i>r</i></sub>=<b><i>T</i></b><sub><i>c</i>-1, <i>w</i></sub>·<b><i>T</i></b><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>r</mi><mi>w</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>也为已知, 于是当前帧<i>I</i><sub><i>c</i></sub>与参考帧<i>I</i><sub><i>r</i></sub>的相对位姿初值<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>˜</mo></mover></math></mathml><sub><i>c</i>, <i>r</i></sub>就可以按照式 (2) 计算</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>˜</mo></mover><msub><mrow></mrow><mrow><mi>c</mi><mo>, </mo><mi>r</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mi>v</mi></msub><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>c</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>r</mi></mrow></msub></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="76">由此就得到了一个<mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>˜</mo></mover></math></mathml><sub><i>c</i>, <i>r</i></sub>的初值假设, 考虑到相机不规则运动的可能性, 还需构建多种其他假设, 包括双倍运动假设、零运动假设等。算法首先采用常速假设, 将参考关键帧的地图点投影至当前帧, 得到预测的像点坐标, 然后在一定半径范围内搜索最优特征匹配, 如果匹配数量不足就扩大搜索范围重新搜索。当取得足够数量的匹配后, 进行位姿优化并采用自由度为2、置信度95%的<i>χ</i><sup>2</sup>测试剔除野点。如果上述过程失败, 就采用其他假设继续尝试。该方法能够准确、稳健地恢复当前帧关于参考帧的相对位姿<b><i>T</i></b><sub><i>cr</i></sub>, 同时能够充分利用先验信息加速匹配过程。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">1.3 逆深度滤波器</h4>
                <div class="p1">
                    <p id="79">由于视频序列影像帧间基线较短, 直接通过两帧计算的像点深度误差往往较大, 深度滤波器的基本思想是通过多次观测融合更新像素深度。假设像点<i>P</i>的深度<i>d</i><sub><i>p</i></sub>服从均值为<i>μ</i>, 方差为<i>δ</i><sup>2</sup>的正态分布, 且系统每追踪一帧, 都可以通过极线搜索寻找该点在新帧上的匹配点, 进而观测到该点的深度<i>μ</i><sub><i>obs</i></sub>并估算方差<i>δ</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>o</mi><mi>b</mi><mi>s</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>, 假设这次观测仍服从正态分布, 进行高斯融合可以得到</p>
                </div>
                <div class="area_img" id="81">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201906006_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="83">由式 (3) 可知, 观测融合后深度估计的方差会减小, 当其小于某一阈值并趋于稳定时, 就认为该点的深度值收敛。本文采用文献<citation id="159" type="reference">[<a class="sup">13</a>]</citation>提出的方法对深度估计误差进行建模。如图3所示, 设左影像<i>I</i><sub><i>r</i></sub>为参考关键帧, 右影像<i>I</i><sub><i>c</i></sub>为已追踪的当前帧, <i>O</i><sub>1</sub>、<i>O</i><sub>2</sub>分别为两帧相机光心位置, 基线长<i>b</i>, <i>P</i>为同名像点<i>p</i><sub>1</sub><i>p</i><sub>2</sub>经三角化得到的空间点, 其在左影像中的深度值为<i>d</i><sub><i>p</i></sub>, <i>P</i>、<i>O</i><sub>1</sub>、<i>O</i><sub>2</sub> 3点的夹角分别记为<i>α</i>、<i>β</i>、<i>γ</i>, <b><i>l</i></b><sub>2</sub>为<i>p</i><sub>1</sub>在当前帧<i>I</i><sub><i>c</i></sub>中对应的极线。</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度估计误差模型" src="Detail/GetImg?filename=images/CHXB201906006_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度估计误差模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Error model of depth estimation</p>

                </div>
                <div class="p1">
                    <p id="85">假设在<i>I</i><sub><i>c</i></sub>上经过极线匹配得到的同名点<i>p</i><sub>2</sub>存在一个像素的误差, 相应的深度值及夹角变为<i>d</i>′<sub><i>p</i></sub>、<i>β</i>′与<i>γ</i>′。此时<i>P</i>点深度<i>d</i><sub><i>p</i></sub>的误差<i>δ</i><sub><i>p</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="86"><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><msup><mi>d</mi><mo>′</mo></msup><msub><mrow></mrow><mi>p</mi></msub></mrow><mo>|</mo></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>b</mi><mfrac><mrow><mi>sin</mi><mtext> </mtext><msup><mi>β</mi><mo>′</mo></msup></mrow><mrow><mi>sin</mi><mtext> </mtext><msup><mi>γ</mi><mo>′</mo></msup></mrow></mfrac></mrow><mo>|</mo></mrow></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="88">理论上, 深度滤波器只需不断将当前帧得到的深度估值与方差作为新的观测, 与已有数据进行融合即可。然而实际过程中, 由于位姿估计不够准确、匹配错误等诸多原因, 观测值可能存在粗差, 将会对融合结果产生较大影响, 因此必须对该过程进行优化。</p>
                </div>
                <div class="p1">
                    <p id="89">首先, 文献<citation id="160" type="reference">[<a class="sup">21</a>]</citation>发现逆深度 (即深度值的倒数) 的统计直方图更接近正态分布, 因此在计算出<i>P</i>点深度值<i>d</i><sub><i>p</i></sub>后, 将其换算为逆深度<i>ρ</i><sub><i>p</i></sub>=1/<i>d</i><sub><i>p</i></sub>, 相应的误差计算公式为</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mi>ρ</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>ρ</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>ρ</mi><msub><mrow></mrow><msup><mi>p</mi><mo>′</mo></msup></msub></mrow><mo>|</mo></mrow><mo>=</mo><mrow><mo>|</mo><mrow><mi>ρ</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mfrac><mrow><mi>sin</mi><mtext> </mtext><msup><mi>γ</mi><mo>′</mo></msup></mrow><mrow><mi>b</mi><mtext>s</mtext><mtext>i</mtext><mtext>n</mtext><mtext> </mtext><msup><mi>β</mi><mo>′</mo></msup></mrow></mfrac></mrow><mo>|</mo></mrow></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="92">其次, 考虑到单像素的亮度与梯度没有明显的可区分性, 易受噪声影响, 本文采用文献<citation id="161" type="reference">[<a class="sup">19</a>]</citation>提出的8像素-图像块模型, 用各像素的梯度均值作为中心像素的梯度, 并按照距离平方和 (sum of squared distance, SSD) 测度进行极线搜索匹配, 能够保证较高的准确度并兼顾效率, 如图4所示, 其中<i>p</i><sub>1</sub>为待匹配点, <i>p</i><sub>min</sub>与<i>p</i><sub>max</sub>分别是按照最大与最小逆深度值 (<i>ρ</i><sub><i>p</i></sub>±3<i>δ</i><sub><i>ρ</i></sub>) 计算的空间点<i>P</i>投影到当前帧的像点, 两点连线即为极线段向量<b><i>l</i></b><sub>2</sub>。</p>
                </div>
                <div class="p1">
                    <p id="93">此外, 匹配点像素梯度与极线的夹角也对深度滤波器有重要影响, 随着像素梯度与极线段夹角的增加, 极线匹配的不确定性也会增加<citation id="162" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。由于视频影像帧率较高, 帧间相对旋转较小, 可将参考帧<i>I</i><sub><i>r</i></sub>上<i>p</i><sub>1</sub>点的图像块平均梯度<b><i>g</i></b><sub><i>p</i></sub>作为当前帧<i>I</i><sub><i>c</i></sub>匹配点<i>p</i><sub>2</sub>梯度的近似, 于是由像素梯度与极线夹角造成的误差可被定义为</p>
                </div>
                <div class="p1">
                    <p id="94"><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">n</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi mathvariant="bold-italic">l</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></mfrac></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="96">式中, <b><i>n</i></b><sub>2</sub>表示极线段<b><i>l</i></b><sub>2</sub>的法向量。由式 (6) 可知, 当<b><i>l</i></b><sub>2</sub>与<b><i>g</i></b><sub><i>p</i></sub>平行时, <i>Ep</i>取得最小值, 反之当<b><i>l</i></b><sub>2</sub>与<b><i>g</i></b><sub><i>p</i></sub>垂直时<i>Ep</i>取得最大值。在实际计算过程中, 若<i>Ep</i>小于某一阈值, 并且极线搜索的最优匹配SSD小于次优匹配的一半以上, 就认为该点的深度观测有效, 可以对其进行高斯融合。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 图像块模型与极线搜索" src="Detail/GetImg?filename=images/CHXB201906006_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 图像块模型与极线搜索  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Patch model and epipolar line search</p>

                </div>
                <h4 class="anchor-tag" id="98" name="98">1.4 后端混合局部优化框架</h4>
                <div class="p1">
                    <p id="99">后端优化是维持SLAM系统一致性的关键, 在ORB-SLAM中该问题可以通过局部光束法平差解决, 但由于本文方法地图点是通过深度滤波算法构造的, 在初始化时仅能够被当前关键帧观测, 无法按照重投影误差最小化原理进行优化。为此本文提出一种基于特征法与直接法的后端混合局部优化框架。</p>
                </div>
                <div class="p1">
                    <p id="100">首先, 对于观测数大于等于3的地图点, 采用光束法平差联合求解相机位姿与地图点位置。然后, 对每个观测数小于3的地图点, 将其投影到其他关键帧, 按照直接法辐射误差最小化原理, 对该点深度值进行优化。该过程可以用因子图的形式表达, 如图5所示, MP<sub>1</sub>与MP<sub>2</sub>表示待优化的地图点, HostKF表示构造这两个地图点的主帧, LocalKF表示与主帧存在共视连接的局部关键帧, Ob表示地图点投影到目标关键帧形成了一次观测 (观测的主帧与目标帧分别用红线和蓝线表示) 。</p>
                </div>
                <div class="p1">
                    <p id="101">假设图像块模型中的各像素具有相同的逆深度值, 以MP<sub>1</sub>为例, 其深度优化目标函数为</p>
                </div>
                <div class="p1">
                    <p id="102"><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi></mrow></mstyle><mrow><mi>ρ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mrow><mi>min</mi></mrow><mspace width="0.25em" /><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mtext>Ο</mtext><mtext>b</mtext></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mtext>Ρ</mtext><mtext>a</mtext><mtext>t</mtext><mtext>c</mtext><mtext>h</mtext></mrow></munderover><mi>v</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="104">以及对应的误差项</p>
                </div>
                <div class="p1">
                    <p id="105"><i>v</i><sub><i>ij</i></sub>=<i>I</i><sub><i>H</i></sub> (<i>p</i><sub><i>i</i></sub>) -<i>I</i><sub><i>j</i></sub> (<i>p</i>′<sub><i>i</i></sub>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="106">式中, 优化目标<i>ρ</i><sub>1</sub>为地图点MP<sub>1</sub>在主帧HostKF中的逆深度;<i>I</i><sub><i>H</i></sub>表示主帧影像灰度函数;<i>I</i><sub><i>j</i></sub>表示第<i>j</i>个观测目标帧的影像灰度函数;<i>p</i><sub><i>i</i></sub>与<i>p</i>′<sub><i>i</i></sub>分别表示MP<sub>1</sub>投影在<i>I</i><sub><i>H</i></sub>和<i>I</i><sub><i>j</i></sub>上的8像素-图像块的第<i>i</i>个像素。建立误差方程式并线性化后, 可按照最小二乘原理迭代求解。</p>
                </div>
                <div class="p1">
                    <p id="107">采用该优化框架的好处非常明显:首先, 观测数较多的地图点往往具有较好的稳健性, 平差系统能够减少野点风险, 准确恢复相机位姿与场景结构<citation id="163" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 如图6 (a) ;其次, 这类地图点数量稀疏, 平差负担小;而经直接法优化的地图点, 如图6 (b) , 尽管比较稠密, 但未知数仅有一个, 且各点解算过程相互独立, 可采用并行策略进行加速, 从而提高优化效率。</p>
                </div>
                <div class="p1">
                    <p id="108">由于误匹配等原因, 系统在运行过程中不可避免会出现地图野点, 需予以剔除。对于新构建的地图点, ORB-SLAM根据其在后续若干关键帧中的匹配情况进行筛选, 该策略无法用于本文直接法优化的地图点 (匹配数不足, 总是会被剔除) 。为此, 提出一种基于平差约束的筛选策略, 每次后端优化完成后, 检查所有新构建的地图点, 如果满足以下任一条件, 就会被标记为野点并剔除。</p>
                </div>
                <div class="p1">
                    <p id="109"> (1) 经局部光束法平差优化后, 重投影误差未通过<i>χ</i><sup>2</sup>测试被标记为野点的;</p>
                </div>
                <div class="p1">
                    <p id="110"> (2) 经直接法优化后, 能量函数超过阈值被标记为野点的;</p>
                </div>
                <div class="p1">
                    <p id="111">此外, 如果一个地图点被标记为野点, 并且其主帧是最新的两个关键帧之一, 就重新将该点构造为种子点进行深度滤波。本文策略能够在排除野点的同时, 有效增加点云密度。</p>
                </div>
                <h3 id="112" name="112" class="anchor-tag">2 试验与分析</h3>
                <h4 class="anchor-tag" id="113" name="113">2.1 试验数据与评价方法</h4>
                <div class="p1">
                    <p id="114">利用3组数据进行验证, 如图7所示, 第1组数据选自开源TUM RGB-D数据集<citation id="164" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 采用移动机器人搭载Kinect相机拍摄, 拍摄环境为室内, 本文仅利用其中的RGB数据, 数据集提供了完整的相机参数以及由高精度IMU获取的位姿数据作为评价真值。</p>
                </div>
                <div class="p1">
                    <p id="115">第2、3组数据由大疆精灵4无人机相机获取, 其中第2组采用手持方式, 拍摄对象为室内机房, 第3组采用航拍方式, 拍摄对象为室外建筑, 相机预先经过标定并对原始影像进行了几何畸变校正, 3组数据均包含完整闭合回路, 基本信息见表1。依据本文方法, 在Ubuntu16.04系统下开发了验证算法ZY-SLAM, 所用PC配置为:Inter Core i7 2.6 GHz、DDR3 16 GB, 未采用GPU加速。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表1 试验数据基本信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Tab.1 Data description of test images</b></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />测试数据</td><td>影像大小/像素</td><td>帧率/ (帧/s) </td><td>影像帧数</td><td>平面范围/m<sup>2</sup></td><td>拍摄环境</td></tr><tr><td><br />TUM_Desk</td><td>640×480</td><td>30</td><td>2964</td><td>5×6</td><td>室内</td></tr><tr><td><br />UAV_Lab</td><td>1280×720</td><td>20</td><td>3231</td><td>18×12</td><td>室内</td></tr><tr><td><br />UAV_Building</td><td>1920×1080</td><td>10</td><td>1840</td><td>160×120</td><td>室外</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">由于本文方法基于ORB-SLAM框架实现, 故以其作为主要比较对象, 考虑到ORB-SLAM只能得到稀疏点云, 而较为稠密的三维结构表达又是直接法视觉里程计 (visual odometry, VO) 的研究热点, 因此采用当前具有代表性的DSO<citation id="165" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation> (后文称DSO-VO, 区别于SLAM) 共同比较, 按如下3种方法分析本文方法性能:</p>
                </div>
                <div class="p1">
                    <p id="118"> (1) 通过比较系统处理影像的整体及关键环节运行时间, 验证算法的计算效率;</p>
                </div>
                <div class="p1">
                    <p id="119"> (2) 通过计算轨迹误差, 分析算法的相机位姿估计精度;</p>
                </div>
                <div class="p1">
                    <p id="120"> (3) 通过目视检查, 评价算法的重建效果。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121">2.2 计算效率分析</h4>
                <div class="p1">
                    <p id="122">表2显示了采用本文方法与ORB-SLAM对TUM_Desk数据进行处理的系统运行时间对比 (由于DSO计算流程存在较大差异, 这里未进行对比) 。本文方法的总体效率优于ORB-SLAM, 每帧平均处理时间约36 ms, 帧率28 fps, 基本满足实时要求。</p>
                </div>
                <div class="area_img" id="123">
                                            <p class="img_tit">
                                                <b>表2 系统总体运行时间对比</b>
                                                    <br />
                                                <b>Tab.2 Comparison of system overall runtime</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201906006_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 系统总体运行时间对比" src="Detail/GetImg?filename=images/CHXB201906006_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="124">考虑到本文方法地图点数量超过ORB-SLAM的20倍, 综合效率提升非常显著, 这主要得益于:①追踪线程中, 本文方法采用追踪参考关键帧的方式进行位姿估计, 不再逐帧追踪局部地图, 节省了大量资源;②局部构图线程中, 影响计算效率的主要是局部平差。本文算法采用的混合优化框架, 能够在准确恢复相机位姿与场景结构的同时, 有效降低平差规模, 不仅可以减少局部平差优化被中断的次数, 也可以更好地平衡追踪与局部构图这两个并行线程的关系, 使系统更加流畅。</p>
                </div>
                <div class="p1">
                    <p id="125">上述试验结果也证明由深度滤波构造地图点的方式更适合SLAM系统。从效率角度看, 深度滤波利用若干相邻帧影像更新深度, 由于基线较短, 极线搜索范围小, 匹配成功率高, 计算负担低, 系统运行更平滑, 更符合增量式重建的特点, 而ORB-SLAM为保证深度估计的可靠性, 只在关键帧上进行极线匹配, 虽然基线较长, 但搜索范围大, 耗时较长, 同时由于影像函数的高度非凸性, 可能存在若干接近最优匹配的“次优匹配”结果。从实用性角度看, 深度滤波过程中, 倘若某次观测出现粗差, 只需跳过该次观测, 系统仍可利用后续观测准确估计深度, 而在ORB-SLAM中, 地图点由两关键帧直接计算, 为避免粗差就需采用多种组合策略进行约束, 实际能够通过测试的点对非常少, 这也是ORB-SLAM只能输出稀疏点云的主要原因。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">2.3 相机轨迹误差分析</h4>
                <div class="p1">
                    <p id="127">位姿估计是SLAM系统的核心, 其精度受地图点精度的影响, 并且反过来也直接影响所构建的地图点。由于本文方法属于单目视觉SLAM, 存在尺度不确定性, 且位姿估值可以被定义在任何坐标系下, 无法直接与真值进行比较, 本文采用轨迹误差来衡量算法的位姿估计精度。其思路是, 先利用最小二乘得到位姿估值序列与真值序列的相似变换<b><i>S</i></b>∈Sim (3) , 将估值换算至同尺度真值坐标系下再计算各帧的相对变换。轨迹误差一般只计算平移分量, 这是因为它更加直观, 并且也潜在地受到角度差异的影响<citation id="166" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。此外, 对于第2、3组无人机影像数据, 由于缺乏高精度POS实测的位姿真值, 本文利用ContextCapture<citation id="167" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>软件对影像数据进行三维建模并将其空三结果作为评价真值使用。结果如表3所示。</p>
                </div>
                <div class="area_img" id="128">
                                            <p class="img_tit">
                                                <b>表3 不同方法位姿估计轨迹误差比较</b>
                                                    <br />
                                                <b>Tab.3 Trajectory error comparison of pose estimation among different methods</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201906006_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">cm</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 不同方法位姿估计轨迹误差比较" src="Detail/GetImg?filename=images/CHXB201906006_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="129">本文方法的位姿估计精度较ORB-SLAM有所提升, 分析原因如下:①由深度滤波获取的地图点融合了多帧观测结果, 相比于直接通过两帧三角化得到地图点的方式, 信息利用更加充分, 三维位置更加准确, 对位姿估计产生积极影响;②由于地图点数量较多, 后端光束法平差可以仅采用观测数多的点估计相机位姿, 有助于减小野点对最小二乘系统的影响;③特征点匹配率更高, 一定程度上增加了多余观测, 能够提高位姿估计的稳健性。图8显示了不同数据集的关键帧特征点匹配率, 本文方法在大部分情况下均高于ORB-SLAM。</p>
                </div>
                <div class="p1">
                    <p id="130">此外, 本文方法与ORB-SLAM的位姿估计精度明显高于DSO-VO, 这是因为前两种方法均可利用闭合回路约束减少累积误差影响, 而DSO-VO不具备此功能, 轨迹误差的累积效果在图9中尤为明显。</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2.4 三维重建效果</h4>
                <div class="p1">
                    <p id="132">在准确恢复相机位姿的前提下, 本文方法无须GPU加速即可重建出较为稠密的场景三维结构, 如图10所示, 从而使虚拟现实、机器感知、语义识别等更高级别的地图应用成为可能。这主要得益于, 逆深度滤波器可以构造大量地图点, 且一个地图点无论观测情况如何, 都可以通过后端混合框架进行优化。观察可发现, 本文方法点云地图中包含许多非角点的物体轮廓、边缘信息, 这是ORB-SLAM无法实现的。</p>
                </div>
                <div class="p1">
                    <p id="133">与此同时, 本文方法可以准确地识别并进行闭环改正<citation id="168" type="reference"><link href="52" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 得到全局一致的位姿与地图信息, 而DSO尽管也具备优异的局部重建性能, 但由于其采用的直接法平差需利用完整影像数据, 在资源受限的情况下难以进行全局质量控制, 导致所构建地图出现重影, 如图11、图12所示, 影响重建效果与准确性。</p>
                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 直接法深度优化因子图" src="Detail/GetImg?filename=images/CHXB201906006_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 直接法深度优化因子图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Factor graph of direct depth optimization</p>

                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同类型地图点" src="Detail/GetImg?filename=images/CHXB201906006_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同类型地图点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.6 Different types of map points</p>

                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 试验数据" src="Detail/GetImg?filename=images/CHXB201906006_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 试验数据  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Data examples</p>

                </div>
                <h3 id="137" name="137" class="anchor-tag">3 结 论</h3>
                <div class="p1">
                    <p id="138">本文提出一种特征法视觉SLAM逆深度滤波的三维重建方法, 关键在于地图点深度不再由两帧三角化直接获取, 而是由基于概率分布的逆深度滤波器累计更新得出。除了计算效率与位姿估计精度方面的优势, 更重要的是, 该方法能够在保证全局一致性的前提下, 显著提升重建点云密度, 准确、高效地恢复场景结构细节, 为视觉SLAM在机器人智能感知、自动驾驶、应急测绘等领域的进一步应用提供了新思路。未来将针对点云的去噪策略与平滑约束、重建精度定量评价等问题继续开展研究, 同时也将采用更多类型的影像数据进行更加广泛的测试。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 关键帧特征点匹配率" src="Detail/GetImg?filename=images/CHXB201906006_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 关键帧特征点匹配率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.8 Matching ratio of keyframe feature points</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 相机平面轨迹与地面真值" src="Detail/GetImg?filename=images/CHXB201906006_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 相机平面轨迹与地面真值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.9 Camera plane trajectories and ground truth</p>

                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 本文方法重建效果" src="Detail/GetImg?filename=images/CHXB201906006_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 本文方法重建效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_16900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Reconstruction examples of proposed method</p>

                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_16901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 本文方法重建效果" src="Detail/GetImg?filename=images/CHXB201906006_16901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 本文方法重建效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_16901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.10 Reconstruction examples of proposed method</p>

                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 UAV_Lab数据局部重建细节对比" src="Detail/GetImg?filename=images/CHXB201906006_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 UAV_Lab数据局部重建细节对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.11 Local reconstruction details of UAV_Lab data</p>

                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201906006_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 UAV_Building数据局部重建细节对比" src="Detail/GetImg?filename=images/CHXB201906006_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 UAV_Building数据局部重建细节对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201906006_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.12 Local reconstruction details of UAV_Building data</p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201606001&amp;v=MjMyMTlMTHo3QmFMRzRIOWZNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5M2tXN3Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 刘浩敏, 章国锋, 鲍虎军.基于单目视觉的同时定位与地图构建方法综述[J].计算机辅助设计与图形学学报, 2016, 28 (6) :855-868.LIU Haomin, ZHANG Guofeng, BAO Hujun.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp; Computer Graphics, 2016, 28 (6) :855-868.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806010&amp;v=MDI5NDRiTEc0SDluTXFZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeTNrVzd2TEppWFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 邸凯昌, 万文辉, 赵红颖, 等.视觉SLAM技术的进展与应用[J].测绘学报, 2018, 47 (6) :770-779.DOI:10.11947/j.AGCS.2018.20170652.DI Kaichang, WAN Wenhui, ZHAO Hongying, et al.Progress and applications of visual SLAM[J].Acta Geodaetica et Cartographica Sinica, 2018, 47 (6) :770-779.DOI:10.11947/j.AGCS.2018.20170652.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Past,Present,and Future of Simultaneous Localization and Mapping:Toward the Robust-Perception Age">

                                <b>[3]</b> CADENA C, CARLONE L, CARRILLO H, et al.Past, present, and future of simultaneous localization and mapping:toward the robust-perception age[J].IEEE Transactions on Robotics, 2017, 32 (6) :1309-1332.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MonoSLAM: Real-time single camera SLAM">

                                <b>[4]</b> DAVISON A J, REID I D, MOLTON N D, et al.MonoSLAM:real-time single camera SLAM[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (6) :1052-1067.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel tracking and mapping for small AR workspace">

                                <b>[5]</b> KLEIN G, MURRAY D.Parallel tracking and mapping for small AR workspaces[C]//Proceedings of 2007 IEEE and ACM International Symposium on Mixed and Augmented Reality.Nara, Japan:IEEE, 2007:225-234.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Double window optimisation for constant time visual SLAM">

                                <b>[6]</b> STRASDAT H, DAVISON A J, MONTIEL J M M, et al.Double window optimisation for constant time visual SLAM[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2352-2359.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bundle Adjustment-A Modern Synthesis">

                                <b>[7]</b> TRIGGS B, MCLAUCHLAN P F, HARTLEY R I, et al.Bundle adjustment—a modern synthesis[M].TRIGGS B, ZISSERMAN A, SZELISKI R.Vision Algorithms:Theory and Practice.Berlin, Heidelberg:Springer, 2000:298-372.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM: A Versatile and Accurate Monocular SLAM System">

                                <b>[8]</b> MUR-ARTAL R, MONTIEL J M M, TARDÓS J D.ORB-SLAM:a versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics, 2015, 31 (5) :1147-1163.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 MUR-ARTAL R, TARDÓS J D.ORB-SLAM2:an open-source SLAM system for monocular, stereo, and RGB-D cameras[J].IEEE Transactions on Robotics, 2016, 33 (5) :1255-1262.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">

                                <b>[10]</b> RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2564-2571.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=COP-SLAM:Closed-Form Online Pose-Chain Optimization for Visual SLAM">

                                <b>[11]</b> DUBBELMAN G, BROWNING B.COP-SLAM:closed-form online pose-chain optimization for visual SLAM[J].IEEE Transactions on Robotics, 2015, 31 (5) :1194-1213.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201348807&amp;v=MDkwNzQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFU3N0lKVjhjYmhFPU5pZk9mYks3SHRET3JZOUVaKzhIQkh3K29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> VOGIATZIS G, HERNÁNDEZ C.Video-based, real-time multi-view stereo[J].Image and Vision Computing, 2011, 29 (7) :434-441.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SVO:Semidirect Visual Odometry for Monocular and Multicamera Systems">

                                <b>[13]</b> FORSTER C, ZHANG Zichao, GASSNER M, et al.SVO:semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics, 2017, 33 (2) :249-265.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-dense visual odometry for a monocular camera">

                                <b>[14]</b> ENGEL J, STURM J, CREMERS D.Semi-dense visual odometry for a monocular camera[C]//Proceedings of 2013 IEEE International Conference on Computer Vision.Sydney:IEEE, 2013:1449-1456.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust odometry estimation for RGB-D cameras">

                                <b>[15]</b> KERL C, STURM J, CREMERS D.Robust odometry estimation for RGB-D cameras[C]//Proceedings of 2013 IEEE International Conference on Robotics and Automation.Karlsruhe:IEEE, 2013:3748-3754.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time dense geometry from a handheld camera">

                                <b>[16]</b> STÜHMER J, GUMHOLD S, CREMERS D.Real-time dense geometry from a handheld camera[C]//Proceedings of Joint Pattern Recognition Symposium.Darmstadt:Springer, 2010:11-20.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DTAM:dense tracking and mapping in real-time">

                                <b>[17]</b> NEWCOMBE R A, LOVEGROVE S J, DAVISON A J.DTAM:dense tracking and mapping in real-time[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE, 2011:2320-2327.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lsdslam:Large-scale direct monocular slam">

                                <b>[18]</b> ENGEL J, SCHÖPS T, CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[C]//Proceedings of European Conference on Computer Vision.Zurich:Springer, 2014:834-849.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Direct Sparse Odometry">

                                <b>[19]</b> ENGEL J, KOLTUN V, CREMERS D.Direct sparse odometry[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (3) :611-625.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic Semi-Dense Mapping from Highly Accurate Feature-Based Monocular SLAM">

                                <b>[20]</b> MUR-ARTAL R, TARDÓS J D.Probabilistic semi-dense mapping from highly accurate feature-based monocular SLAM[C]//Proceedings of Conference on Robotics:Science and Systems.Rome:Universidad Zaragoza, 2015.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inverse Depth Parametrization for Monocular SLAM">

                                <b>[21]</b> CIVERA J, DAVISON A J, MONTIEL J M M.Inverse depth parametrization for monocular SLAM[J].IEEE Transactions on Robotics, 2008, 24 (5) :932-945.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121311048000&amp;v=MDgxODhLRm9YWEZxekdiSzZIOUxOcm85QmJPc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWmVadkZ5amtVN3JK&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> 高翔, 张涛, 刘毅, 等.视觉SLAM十四讲——从理论到实践[M].北京:电子工业出版社, 2017:340-341.GAO Xiang, ZHANG Tao, LIU Yi, et al.Visual SLAM fourteen lectures-from theory to practice[M].Beijing:China Machine Press, 2017:340-341.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201701014&amp;v=MjEyMjU3cWZadWR2Rnkza1c3dkxKaVhUYkxHNEg5Yk1ybzlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 于英, 张永生, 薛武, 等.影像连接点均衡化高精度自动提取[J].测绘学报, 2017, 46 (1) :90-97.DOI:10.11947/j.AGCS.2017.20160320.YU Ying, ZHANG Yongsheng, XUE Wu, et al.Automatic tie points extraction with uniform distribution and high precision[J].Acta Geodaetica et Cartographica Sinica, 2017, 46 (1) :90-97.DOI:10.11947/j.AGCS.2017.20160320.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A benchmark for the evaluation of RGB-D SLAM systems">

                                <b>[24]</b> STURM J, ENGELHARD N, ENDRES F, et al.A benchmark for the evaluation of RGB-D SLAM systems[C]//Proceedings of 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.Vilamoura-Algarve:IEEE, 2012:573-580.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Create 3D models from simple photographs">

                                <b>[25]</b> ContextCapture.Create 3D models from simple photographs[EB/OL]. (2018-08-31) .https://www.bentley.com/en/products/brands/contextcapture.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast relocalisation and loop closing in keyframe-based SLAM">

                                <b>[26]</b> MUR-ARTAL R, TARDÓS J D.Fast relocalisation and loop closing in keyframe-based SLAM[C]//Proceedings of 2014 IEEE International Conference on Robotics and Automation.Hong Kong:IEEE, 2014:846-853.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201906006" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201906006&amp;v=MTA5MTN5M2tXN3ZJSmlYVGJMRzRIOWpNcVk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
