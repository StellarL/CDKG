<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637142605967920000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dCHXB201910007%26RESULT%3d1%26SIGN%3dR6TtZxy7f%252bBzrVuLdebI0iAPK2k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201910007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=CHXB201910007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201910007&amp;v=MTI4NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDlqTnI0OUZZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#75" data-title="1 全景相机的成像、检校与优化模型 ">1 全景相机的成像、检校与优化模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="1.1 多镜头组合式全景相机模型">1.1 多镜头组合式全景相机模型</a></li>
                                                <li><a href="#86" data-title="1.2 鱼眼相机的检校">1.2 鱼眼相机的检校</a></li>
                                                <li><a href="#95" data-title="1.3 全景相机单像解析和前方交会">1.3 全景相机单像解析和前方交会</a></li>
                                                <li><a href="#114" data-title="1.4 全景相机的非线性优化方法">1.4 全景相机的非线性优化方法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="2 全景SLAM流程 ">2 全景SLAM流程</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#131" data-title="2.1 初始化">2.1 初始化</a></li>
                                                <li><a href="#133" data-title="2.2 地图点跟踪">2.2 地图点跟踪</a></li>
                                                <li><a href="#148" data-title="2.3 关键帧选取">2.3 关键帧选取</a></li>
                                                <li><a href="#156" data-title="2.4 局部地图构建">2.4 局部地图构建</a></li>
                                                <li><a href="#158" data-title="2.5 闭环探测与全局优化">2.5 闭环探测与全局优化</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#168" data-title="3 试验结果和分析 ">3 试验结果和分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#169" data-title="3.1 试验设计">3.1 试验设计</a></li>
                                                <li><a href="#172" data-title="3.2 试验结果和分析">3.2 试验结果和分析</a></li>
                                                <li><a href="#192" data-title="3.3 待改进之处">3.3 待改进之处</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#195" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="图1 全景成像模型">图1 全景成像模型</a></li>
                                                <li><a href="#128" data-title="图2 构建的优化图(三角形节点代表位姿,圆形节点代表地图点,边代表误差项)">图2 构建的优化图(三角形节点代表位姿,圆形节点代表地图点,边代表误差项)</a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;表1 本文使用的鱼眼相机检校方法检校结果与文献&lt;/b&gt;&lt;b&gt;对比&lt;/b&gt;"><b>表1 本文使用的鱼眼相机检校方法检校结果与文献</b><b>对比</b></a></li>
                                                <li><a href="#178" data-title="图3 连续三帧图像所匹配的ORB特征点">图3 连续三帧图像所匹配的ORB特征点</a></li>
                                                <li><a href="#184" data-title="&lt;b&gt;表2 Kashiwa图像序列与Omiya图像序列数据集使用全景SLAM跟踪所得结果,仅用前两帧GPS/IMU数据对齐SLAM轨迹&lt;/b&gt;"><b>表2 Kashiwa图像序列与Omiya图像序列数据集使用全景SLAM跟踪所得结果,仅用前两帧GPS/IMU数据对齐SLAM轨迹</b></a></li>
                                                <li><a href="#185" data-title="&lt;b&gt;表3 Kashiwa图像序列与Omiya图像序列使用全景SLAM跟踪所得结果,用最小数量均匀分布的GPS点7参数变换对齐SLAM轨迹&lt;/b&gt;"><b>表3 Kashiwa图像序列与Omiya图像序列使用全景SLAM跟踪所得结果,用最小数量均匀分布的GPS点7参数变换对齐SLAM轨迹</b></a></li>
                                                <li><a href="#189" data-title="图4 两套数据集上所探测出的闭环">图4 两套数据集上所探测出的闭环</a></li>
                                                <li><a href="#190" data-title="图5 Kashiwa与Omiya序列使用全景SLAM系统跟踪后得到的全局地图总览(关键帧及其连接关系,地图点分布情况)">图5 Kashiwa与Omiya序列使用全景SLAM系统跟踪后得到的全局地图总览(关键帧及其连接关系......</a></li>
                                                <li><a href="#191" data-title="图6 两个区域的SLAM结果以及加入3/15个控制点后的定位结果分别与GPS参考的对比">图6 两个区域的SLAM结果以及加入3/15个控制点后的定位结果分别与GPS参考的对比</a></li>
                                                <li><a href="#194" data-title="图7 反向行驶拍摄的全景图像">图7 反向行驶拍摄的全景图像</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="2">


                                    <a id="bibliography_1" title=" 龚健雅,季顺平.摄影测量与深度学习[J].测绘学报,2018,47(6):693-704.DOI:10.11947/j.AGCS.2018.20170640.GONG Jianya,JI Shunping.Photogrammetry and deep learning[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):693-704.DOI:10.11947/j.AGCS.2018.20170640." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806003&amp;v=MDg3MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdVYi9QSmlYVGJMRzRIOW5NcVk5Rlo0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         龚健雅,季顺平.摄影测量与深度学习[J].测绘学报,2018,47(6):693-704.DOI:10.11947/j.AGCS.2018.20170640.GONG Jianya,JI Shunping.Photogrammetry and deep learning[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):693-704.DOI:10.11947/j.AGCS.2018.20170640.
                                    </a>
                                </li>
                                <li id="4">


                                    <a id="bibliography_2" title=" 邸凯昌,万文辉,赵红颖,等.视觉SLAM技术的进展与应用[J].测绘学报,2018,47(6):770-779.DOI:10.11947/j.AGCS.2018.20170652.DI Kaichang,WANG Wenhui,ZHAO hongying,et al.Progress and application of visual SLAM[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):770-779.DOI:10.11947/j.AGCS.2018.20170652." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806010&amp;v=MDk3NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ViL1BKaVhUYkxHNEg5bk1xWTlFWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         邸凯昌,万文辉,赵红颖,等.视觉SLAM技术的进展与应用[J].测绘学报,2018,47(6):770-779.DOI:10.11947/j.AGCS.2018.20170652.DI Kaichang,WANG Wenhui,ZHAO hongying,et al.Progress and application of visual SLAM[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):770-779.DOI:10.11947/j.AGCS.2018.20170652.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_3" title=" LIU H M,ZHANG G F,BAO H J.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2016,28(6):855-868." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201606001&amp;v=Mjc1Mjl1ZHZGeXJnVWIvUEx6N0JhTEc0SDlmTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         LIU H M,ZHANG G F,BAO H J.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2016,28(6):855-868.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_4" title=" DAVISON A J,REID I D,MOLTON N D,et al.MonoSLAM:real-time single camera SLAM[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2007,29(6):1052-1067." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MonoSLAM: Real-Time Single Camera SLAM">
                                        <b>[4]</b>
                                         DAVISON A J,REID I D,MOLTON N D,et al.MonoSLAM:real-time single camera SLAM[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2007,29(6):1052-1067.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_5" title=" MUR-ARTAL R,MONTIEL J M M,TARD&#211;S J D.ORB-SLAM:A versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics,2017,31(5):1147-1163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM:A Versatile and Accurate Monocular SLAM System">
                                        <b>[5]</b>
                                         MUR-ARTAL R,MONTIEL J M M,TARD&#211;S J D.ORB-SLAM:A versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics,2017,31(5):1147-1163.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_6" title=" ENGEL J,SCH&#214;PS T,CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[M]//FLEET D,PAJDLA T,SCHIELE B,et al.Computer Vision-ECCV 2014.Zurich:Cham- Springer,2014:834-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD-SLAM:Large-scale direct monocular SLAM">
                                        <b>[6]</b>
                                         ENGEL J,SCH&#214;PS T,CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[M]//FLEET D,PAJDLA T,SCHIELE B,et al.Computer Vision-ECCV 2014.Zurich:Cham- Springer,2014:834-849.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_7" title=" FORSTER C,PIZZOLI M,SCARAMUZZA D.SVO:Fast semi-direct monocular visual odometry[C]//Proceedings of IEEE International Conference on Robotics and Automation.Hong Kong:IEEE,2014:15-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SVO:fast semi-direct monocular visual odometry">
                                        <b>[7]</b>
                                         FORSTER C,PIZZOLI M,SCARAMUZZA D.SVO:Fast semi-direct monocular visual odometry[C]//Proceedings of IEEE International Conference on Robotics and Automation.Hong Kong:IEEE,2014:15-22.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_8" title=" FORSTER C,ZHANG Z C,GASSNER M,et al.SVO:semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics,2017,33(2):249-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SVO:Semidirect Visual Odometry for Monocular and Multicamera Systems">
                                        <b>[8]</b>
                                         FORSTER C,ZHANG Z C,GASSNER M,et al.SVO:semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics,2017,33(2):249-265.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_9" title=" GAO X,ZHANG T.Robust RGB-D simultaneous localization and mapping using planar point features[J].Robotics and Autonomous Systems,2015,72(10):1-14." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES60024CCA6DF70F2C7F2EA12264F5E4D4&amp;v=MDA3NzhmaFFjNmJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDU5bGh4cjI5d0s4PU5pZk9mYlc0SHRQSTNQdzBZcDk1QzN4UHpXVVVuRDBJT1g3Z3JoUXhENw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         GAO X,ZHANG T.Robust RGB-D simultaneous localization and mapping using planar point features[J].Robotics and Autonomous Systems,2015,72(10):1-14.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_10" title=" NEWMAN P,HO K.SLAM-loop closing with visually salient features[C]//Proceedings of 2005 IEEE International Conference on Robotics and Automation.Barcelona:IEEE,2005:635-642." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLAM-Loop Closing with Visually Salient Features">
                                        <b>[10]</b>
                                         NEWMAN P,HO K.SLAM-loop closing with visually salient features[C]//Proceedings of 2005 IEEE International Conference on Robotics and Automation.Barcelona:IEEE,2005:635-642.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_11" title=" RUBLEE E,RABAUD V,KONOLIGE K,et al.ORB:An efficient alternative to SIFT or SURF[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE,2011:2564-2571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">
                                        <b>[11]</b>
                                         RUBLEE E,RABAUD V,KONOLIGE K,et al.ORB:An efficient alternative to SIFT or SURF[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE,2011:2564-2571.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_12" title=" 季顺平,史云.多镜头组合型全景相机两种成像模型的定位精度比较[J].测绘学报,2014,43(12):1252-1258.DOI:10.13485/j.cnki.11-2089.2014.0169.JI Shunping,SHI Yun.Comparison of two sensor models for multi-camera rig system in measurements[J].Acta Geodaetica et Cartographica Sinica,2014,43(12):1252-1258.DOI:10.13485/j.cnki.11-2089.2014.0169." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201412008&amp;v=MTIzMTZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDlYTnJZOUY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         季顺平,史云.多镜头组合型全景相机两种成像模型的定位精度比较[J].测绘学报,2014,43(12):1252-1258.DOI:10.13485/j.cnki.11-2089.2014.0169.JI Shunping,SHI Yun.Comparison of two sensor models for multi-camera rig system in measurements[J].Acta Geodaetica et Cartographica Sinica,2014,43(12):1252-1258.DOI:10.13485/j.cnki.11-2089.2014.0169.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_13" title=" 季顺平,史云.高噪声环境下基于参考影像的车载序列影像定位方法[J].测绘学报,2014,43(11):1174-1181.DOI:10.13485/j.cnki.11-2089.2014.0181.JI Shunping,SHI Yun.Georegistration of ground sequential imagery with geo-referenced aerial images in high noise environments[J].Acta Geodaetica et Cartographica Sinica,2014,43(11):1174-1181.DOI:10.13485/j.cnki.11-2089.2014.0181." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201411012&amp;v=MDMwMzVxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdVYi9QSmlYVGJMRzRIOVhOcm85RVpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         季顺平,史云.高噪声环境下基于参考影像的车载序列影像定位方法[J].测绘学报,2014,43(11):1174-1181.DOI:10.13485/j.cnki.11-2089.2014.0181.JI Shunping,SHI Yun.Georegistration of ground sequential imagery with geo-referenced aerial images in high noise environments[J].Acta Geodaetica et Cartographica Sinica,2014,43(11):1174-1181.DOI:10.13485/j.cnki.11-2089.2014.0181.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_14" title=" RITUERTO A,PUIG L,GUERRERO J J.Visual SLAM with an omnidirectional camera[C]//Proceedings of the 20th International Conference on Pattern Recognition.Istanbul,Turkey:IEEE,2010:348-351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual SLAM with an omnidirectional camera">
                                        <b>[14]</b>
                                         RITUERTO A,PUIG L,GUERRERO J J.Visual SLAM with an omnidirectional camera[C]//Proceedings of the 20th International Conference on Pattern Recognition.Istanbul,Turkey:IEEE,2010:348-351.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_15" title=" LEMAIRE T,LACROIX S.SLAM with panoramic vision[J].Journal of Field Robotics,2010,24(1-2):91-111." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001252476&amp;v=MjIxOTBxZForWnVGeW5sVWJyS0lGaz1OaWZjYXJPNEh0SE5yWXBIWU93SlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         LEMAIRE T,LACROIX S.SLAM with panoramic vision[J].Journal of Field Robotics,2010,24(1-2):91-111.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_16" title=" JI Shunping,SHI Yun,SHAN Jie,et al.Particle filtering methods for georeferencing panoramic image sequence in complex urban scenes[J].ISPRS Journal of Photogrammetry and Remote Sensing,2015,105(7):1-12." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFA7A9A8979587CBECA23597899EBD201&amp;v=MDE4NDJmT2ZjWEpHYURGM29kTVkrSUtCSHRLdldOZ216MStUWGJscEJzOERNRGdSN3FlQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1OWxoeHIyOXdLOD1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         JI Shunping,SHI Yun,SHAN Jie,et al.Particle filtering methods for georeferencing panoramic image sequence in complex urban scenes[J].ISPRS Journal of Photogrammetry and Remote Sensing,2015,105(7):1-12.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_17" title=" CARUSO D,ENGEL J,CREMERS D.Large-scale direct SLAM for omnidirectional cameras[C]//Proceedings of 2015 IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems.Hamburg:IEEE,2015:141-148." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale direct slam for omnidirectional cameras">
                                        <b>[17]</b>
                                         CARUSO D,ENGEL J,CREMERS D.Large-scale direct SLAM for omnidirectional cameras[C]//Proceedings of 2015 IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems.Hamburg:IEEE,2015:141-148.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_18" title=" 谢东海,钟若飞,吴俣,等.球面全景影像相对定向与精度验证[J].测绘学报,2017,46(11):1822-1829.DOI:10.11947/j.AGCS.2017.20160645.XIE Donghai,ZHONG Ruofei,WU Yu,et al.Relative pose estimation and accuracy verification of spherical panoramic image[J].Acta Geodaetica et Cartographica Sinica,2017,46(11):1822-1829.DOI:10.11947/j.AGCS.2017.20160645." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201711005&amp;v=MDAyODhadWR2RnlyZ1ViL1BKaVhUYkxHNEg5Yk5ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         谢东海,钟若飞,吴俣,等.球面全景影像相对定向与精度验证[J].测绘学报,2017,46(11):1822-1829.DOI:10.11947/j.AGCS.2017.20160645.XIE Donghai,ZHONG Ruofei,WU Yu,et al.Relative pose estimation and accuracy verification of spherical panoramic image[J].Acta Geodaetica et Cartographica Sinica,2017,46(11):1822-1829.DOI:10.11947/j.AGCS.2017.20160645.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_19" title=" 闫利,王奕丹.扩展共线方程结合车载组合全景相机影像区域网平差[J].测绘学报,2017,46(4):460-467.DOI:10.11947/j.AGCS.2017.20160464.YAN Li,WANG Yidan.Block adjustment of vehicle-borne multi-camera rig images using extended collinearity equations[J].Acta Geodaetica et Cartographica Sinica,2017,46(4):460-467.DOI:10.11947/j.AGCS.2017.20160464." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201704010&amp;v=MzAxMDZxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDliTXE0OUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         闫利,王奕丹.扩展共线方程结合车载组合全景相机影像区域网平差[J].测绘学报,2017,46(4):460-467.DOI:10.11947/j.AGCS.2017.20160464.YAN Li,WANG Yidan.Block adjustment of vehicle-borne multi-camera rig images using extended collinearity equations[J].Acta Geodaetica et Cartographica Sinica,2017,46(4):460-467.DOI:10.11947/j.AGCS.2017.20160464.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_20" title=" 石丽梅,赵红蕊,李明海,等.车载移动测图系统外方位元素标定方法[J].测绘学报,2015,44(1):52-58.DOI:10.11947/j.AGCS.2015.20130203.SHI Limei,ZHAO Hongrui,LI Minghai,et al.Extrinsic calibration for vehicle-based mobile mapping system[J].Acta Geodaetica et Cartographica Sinica,2015,44(1):52-58.DOI:10.11947/j.AGCS.2015.20130203." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201501011&amp;v=MDkwNDBGckNVUjdxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDlUTXJvOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         石丽梅,赵红蕊,李明海,等.车载移动测图系统外方位元素标定方法[J].测绘学报,2015,44(1):52-58.DOI:10.11947/j.AGCS.2015.20130203.SHI Limei,ZHAO Hongrui,LI Minghai,et al.Extrinsic calibration for vehicle-based mobile mapping system[J].Acta Geodaetica et Cartographica Sinica,2015,44(1):52-58.DOI:10.11947/j.AGCS.2015.20130203.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_21" title=" GEYER C,DANIILIDIS K.Catadioptric camera calibration[C]//Proceedings of the 7th IEEE International Conference on Computer Vision.Kerkyra,Greece,Greece:IEEE,1999:398-404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Catadioptric camera calibration">
                                        <b>[21]</b>
                                         GEYER C,DANIILIDIS K.Catadioptric camera calibration[C]//Proceedings of the 7th IEEE International Conference on Computer Vision.Kerkyra,Greece,Greece:IEEE,1999:398-404.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_22" title=" SATO T,IKEDA S,YOKOYA N.Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system[C]//Proceedings of European Conference on Computer Vision.Prague,Czech Republic:Springer,2004:326-340." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extrinsic Camera Parameter Recovery from Multiple Image Sequences Captured by An Omni-directional Multi-camera System">
                                        <b>[22]</b>
                                         SATO T,IKEDA S,YOKOYA N.Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system[C]//Proceedings of European Conference on Computer Vision.Prague,Czech Republic:Springer,2004:326-340.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_23" title=" MUR-ARTAL R,TARD&#211;S J D.ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEE Transactions on Robotics,2017,33(5):1255-1262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras">
                                        <b>[23]</b>
                                         MUR-ARTAL R,TARD&#211;S J D.ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEE Transactions on Robotics,2017,33(5):1255-1262.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_24" title=" IKEDA S,SATO T,YOKOYA N.High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system[C]//Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems,MFI2003.Tokyo,Japan:IEEE,2003." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system">
                                        <b>[24]</b>
                                         IKEDA S,SATO T,YOKOYA N.High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system[C]//Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems,MFI2003.Tokyo,Japan:IEEE,2003.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_25" title=" GUTIERREZ D,RITUERTO A,MONTIEL J M M,et al.Adapting a real-time monocular visual SLAM from conventional to omnidirectional cameras[C]//Proceedings of 2011 IEEE International Conference on Computer Vision Workshops.Barcelona,Spain:IEEE,2011:343-350." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adapting A Real-time Monocular Visual SLAM from Conventional to Omnidirectional Cameras">
                                        <b>[25]</b>
                                         GUTIERREZ D,RITUERTO A,MONTIEL J M M,et al.Adapting a real-time monocular visual SLAM from conventional to omnidirectional cameras[C]//Proceedings of 2011 IEEE International Conference on Computer Vision Workshops.Barcelona,Spain:IEEE,2011:343-350.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_26" title=" PAY&#193; L,FERN&#193;NDEZ L,GIL A,et al.Map building and monte carlo localization using global appearance of omnidirectional images[J].Sensors,2010,10(12):11468-11497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Map Building and Monte Carlo Localization Using Global Appearance of Omnidirectional Images">
                                        <b>[26]</b>
                                         PAY&#193; L,FERN&#193;NDEZ L,GIL A,et al.Map building and monte carlo localization using global appearance of omnidirectional images[J].Sensors,2010,10(12):11468-11497.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_27" title=" CUMMINS M,NEWMAN P.FAB-MAP:appearance-based place recognition and mapping using a learned visual vocabulary model[C]//Proceedings of the 27th International Conference on Machine Learning.Haifa,Israel:[s.n.],2010." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FAB-MAP:Appearancebased place recognition and mapping using a learned visual vocabulary model">
                                        <b>[27]</b>
                                         CUMMINS M,NEWMAN P.FAB-MAP:appearance-based place recognition and mapping using a learned visual vocabulary model[C]//Proceedings of the 27th International Conference on Machine Learning.Haifa,Israel:[s.n.],2010.
                                    </a>
                                </li>
                                <li id="56">


                                    <a id="bibliography_28" title=" SATO T,YOKOYA N.Efficient hundreds-baseline stereo by counting interest points for moving omni-directional multi-camera system[J].Journal of Visual Communication and Image Representation,2010,21(5-6):416-426." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501263499&amp;v=MDg5NDIvaXJSZEdlcnFRVE1ud1plWnRGaW5sVTc3SUlsc1dhaFU9TmlmT2ZiSzdIdEROcW85RVp1ME1DSFV3b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         SATO T,YOKOYA N.Efficient hundreds-baseline stereo by counting interest points for moving omni-directional multi-camera system[J].Journal of Visual Communication and Image Representation,2010,21(5-6):416-426.
                                    </a>
                                </li>
                                <li id="58">


                                    <a id="bibliography_29" title=" LEPETIT V,MORENO-NOGUER F,FUA P.EP&lt;i&gt;n&lt;/i&gt;P:an accurate &lt;i&gt;O&lt;/i&gt;(&lt;i&gt;n&lt;/i&gt;) solution to the P&lt;i&gt;n&lt;/i&gt;P problem[J].International Journal of Computer Vision,2009,81(2):155-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003394813&amp;v=MTkyNjROajdCYXJPNEh0SFBySVpCYk9vTVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RnlubFVicktJRms9&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                         LEPETIT V,MORENO-NOGUER F,FUA P.EP&lt;i&gt;n&lt;/i&gt;P:an accurate &lt;i&gt;O&lt;/i&gt;(&lt;i&gt;n&lt;/i&gt;) solution to the P&lt;i&gt;n&lt;/i&gt;P problem[J].International Journal of Computer Vision,2009,81(2):155-166.
                                    </a>
                                </li>
                                <li id="60">


                                    <a id="bibliography_30" title=" K&#220;MMERLE R,GRISETTI G,STRASDAT H,et al.G&lt;sup&gt;2&lt;/sup&gt;o:a general framework for graph optimization[C]//Proceedings of 2011 IEEE International Conference on Robotics and Automation.Shanghai,China:IEEE,2011:3607-3613." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=g2o:A general frame-work for graph optimization">
                                        <b>[30]</b>
                                         K&#220;MMERLE R,GRISETTI G,STRASDAT H,et al.G&lt;sup&gt;2&lt;/sup&gt;o:a general framework for graph optimization[C]//Proceedings of 2011 IEEE International Conference on Robotics and Automation.Shanghai,China:IEEE,2011:3607-3613.
                                    </a>
                                </li>
                                <li id="62">


                                    <a id="bibliography_31" title=" STRASDAT H,MONTIEL J M M,DAVISON A J.Scale drift-aware large scale monocular SLAM[M]// Robotics:Science and Systems.Zurich:The MIT Press,2010." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale Drift-aware Large Scale MonocularSLAM">
                                        <b>[31]</b>
                                         STRASDAT H,MONTIEL J M M,DAVISON A J.Scale drift-aware large scale monocular SLAM[M]// Robotics:Science and Systems.Zurich:The MIT Press,2010.
                                    </a>
                                </li>
                                <li id="64">


                                    <a id="bibliography_32" title=" HORN B K P,HILDENH M,NEGAHDARIPOUR S.Closed-form solution of absolute orientation using orthonormal matrices[J].Journal of the Optical Society of America A,1998,5(7):1127-1135." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Closed-form solution of absolute orientation using orthonormal matrices">
                                        <b>[32]</b>
                                         HORN B K P,HILDENH M,NEGAHDARIPOUR S.Closed-form solution of absolute orientation using orthonormal matrices[J].Journal of the Optical Society of America A,1998,5(7):1127-1135.
                                    </a>
                                </li>
                                <li id="66">


                                    <a id="bibliography_33" title=" KANNALA J,BRANDT S S.A generic camera model and calibration method for conventional,wide-angle,and fish-eye lenses[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(8):1335-1340." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses">
                                        <b>[33]</b>
                                         KANNALA J,BRANDT S S.A generic camera model and calibration method for conventional,wide-angle,and fish-eye lenses[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(8):1335-1340.
                                    </a>
                                </li>
                                <li id="68">


                                    <a id="bibliography_34" title=" ZHANG Guofeng,LIU Haomin,DONG Zilong,et al.Efficient non-consecutive feature tracking for robust structure-from-motion[J].IEEE Transactions on Image Processing,2016,25(12):5957-5970." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ENFT: Efficient Non-Consecutive Feature Tracking for Robust Structure-from-Motion">
                                        <b>[34]</b>
                                         ZHANG Guofeng,LIU Haomin,DONG Zilong,et al.Efficient non-consecutive feature tracking for robust structure-from-motion[J].IEEE Transactions on Image Processing,2016,25(12):5957-5970.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=CHXB" target="_blank">测绘学报</a>
                2019,48(10),1254-1265 DOI:10.11947/j.AGCS.2019.20180443            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>多镜头组合式相机的全景SLAM</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%A3%E9%A1%BA%E5%B9%B3&amp;code=11067968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">季顺平</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%A7%A6%E6%A2%93%E6%9D%B0&amp;code=41790940&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">秦梓杰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E9%81%A5%E6%84%9F%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学遥感信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>同步定位与地图构建(simultaneous localization and mapping,SLAM)是摄影测量、计算机视觉和机器人学的研究热点,并广泛应用于移动测图系统、机器人、无人驾驶车等。本文提出一种针对多镜头组合式全景相机的基于特征的SLAM解决方案。首先,本文建立了鱼眼相机的高精度检校模型,以保证鱼眼相机与全景相机之间的高精度坐标转换;然后,将多镜头组合式全景成像模型嵌入SLAM的初始化、局部地图生成、关键帧选取、图优化等各个流程中。此外,考虑全景相机变形大、基线长的不利因素,本文在特征匹配、平差、特征点跟踪等SLAM的各个步骤都进行了针对性改进。本文在两套车载全景数据集共8000余张全景影像上进行试验。结果表明,本文所提出的全景SLAM很好地实现了全景相机的自动定位与地图构建功能,并达到了接近GPS参考的极高定位精度而无须借助GPS/IMU组合导航系统。相对于主流的基于平面相机的各类SLAM系统,如Mono-SLAM、Stereo-SLAM以及RGB-D SLAM,本文提出的全景SLAM可作为良好的补充,并为GPS信号失锁时的传感器定位提供廉价的自动解决方案。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SLAM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SLAM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E6%99%AF%E7%9B%B8%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全景相机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%B1%BC%E7%9C%BC%E7%9B%B8%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鱼眼相机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E6%9C%BA%E6%A3%80%E6%A0%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相机检校;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    季顺平(1979—),男,博士,教授,研究方向为智能摄影测量与计算机视觉,E-mail: jishunping@whu.edu.cn。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(41471288);</span>
                                <span>国家重点研发计划(2018YFB0505003);</span>
                    </p>
            </div>
                    <h1>Panoramic SLAM for multi-camera rig</h1>
                    <h2>
                    <span>JI Shunping</span>
                    <span>QIN Zijie</span>
            </h2>
                    <h2>
                    <span>School of Remote Sensing and Information Engineering, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Simultaneous localization and mapping(SLAM) is a research hotspot in fields of photogrammetry, computer vision and robotics, and has been widely applied in mobile mapping system, robots, driverless car, etc. This paper presents a fully automated feature based SLAM solution for a panoramic imaging system consisted of multi-camera rig. First, we developed a fisheye camera calibration model for guaranteeing high accurate coordinate transformation between the fisheye camera and the panoramic camera. Second, we imbedded the panoramic camera model into the SLAM process including initialization, local map building, key frame selection, graph optimization and bundle adjustment. In addition, we developed the algorithm in the processes of feature matching, bundle adjustment, frame tracking considering the disadvantages from the large image distortion and long baseline of the panoramic camera system. Experiments are executed on two data sets with more than 8000 panoramic images. Results show that the proposed panoramic SLAM solution achieves automatic camera localization and map construction, and the localization accuracy approaches the GPS reference. With respect to the mainstream SLAM systems based on conventional cameras, such as Mono-SLAM, Stereo-SLAM and RGB-D SLAM, our proposed panoramic SLAM system could serve as a beneficial supplement and supplies a cheap solution for GPS denied localization problem.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SLAM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SLAM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=panoramic%20camera&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">panoramic camera;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fisheye%20camera&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fisheye camera;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=camera%20calibration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">camera calibration;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JI Shunping(1979—),male,PhD,professor,majors in intelligent photogrammetry and computer vision,E-mail: jishunping@whu.edu.cn;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-28</p>
                                    <p>
                            <b>Fund：</b>
                                                        <span>The National Science Foundation(No.41471288);</span>
                                <span>The National Key Research and Development Program of China(No.2018YFB0505003);</span>
                    </p>
            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="70">随着传感器、自动化和平台技术的发展,利用平台上安置的光学或距离传感器,同时实现自我定位与环境感知的智能系统成为摄影测量、计算机视觉和机器人学的新型研究方向<citation id="201" type="reference"><link href="2" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>;并在移动测图系统、无人驾驶汽车、火星和月球的深空探测、无人机侦察、室内导航等领域发挥着关键的作用<citation id="208" type="reference"><link href="4" rel="bibliography" /><link href="6" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。若以光学传感器为主要信息获取源,这种系统通常称为基于视觉的自动定位与地图构建(simultaneous localization and mapping,SLAM)<citation id="202" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。主流的视觉SLAM系统有两种分类模式。一类按照所采用的传感器划分。包括单目SLAM(mono-SLAM)、双目SLAM(stereo-SLAM)和4D相机SLAM(RGBD-SLAM)。另一类按照所采用的方法划分,主流是基于点特征的SLAM(feature-based SLAM)和基于图像自身的SLAM(direct SLAM)。成熟的、具代表性的SLAM系统如:基于特征点的ORB-SLAM<citation id="203" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,直接法的LSD-SLAM<citation id="204" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,将特征点法与直接法混用的SVO<citation id="205" type="reference"><link href="14" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、SVO2.0<citation id="206" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,以及RBG-D SLAM的代表作RTAB-MAP<citation id="207" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,等。</p>
                </div>
                <div class="p1">
                    <p id="71">这些主流SLAM架构采用传统框幅式的相机或摄像机作为视觉信息获取装备。单目SLAM视场狭窄,尺度估计受累积误差的影响,较依赖于闭环条件<citation id="209" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。双目SLAM虽然克服了尺度漂移,但是视差狭窄依然没有改变。若局部成像区域信息较少(即提取的ORB<citation id="210" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>或其他特征较少),或者出现较大的视角变化,则会引起跟踪频繁丢失。这也是SLAM技术尚未广泛应用于测绘行业的地面移动测图系统(mobile mapping system,MMS)的关键因素。</p>
                </div>
                <div class="p1">
                    <p id="72">大视场成像设备,如鱼眼镜头和全景镜头,理论上能够克服视场狭窄的问题。全景视觉成像具有360°全视角成像的优势,已经在测绘、机器人、计算机视觉等相关领域中逐步得到应用<citation id="215" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>,如用于城市测图、视频监控、交通监督、虚拟现实、机器人导航、场景重建等。装载于移动测图系统或普通汽车上的全景成像装置通常只被用于街景收集,如谷歌和百度的街景图像,无法实现量测功能;基于全景视觉的检校,几何定位或完整的SLAM系统也有较多的研究<citation id="216" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="30" rel="bibliography" /><link href="32" rel="bibliography" /><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><link href="38" rel="bibliography" /><link href="40" rel="bibliography" /><link href="42" rel="bibliography" /><link href="44" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。文献<citation id="217" type="reference">[<a class="sup">18</a>,<a class="sup">19</a>]</citation>采用超广角单目鱼眼相机,使用卡尔曼滤波作为优化方法;文献<citation id="211" type="reference">[<a class="sup">17</a>]</citation>与SVO2.0<citation id="212" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>研究了多镜头组合系统的SLAM算法,前者采用了球面模型的粒子滤波作为优化方法,而SVO2.0采用了多个平面相机模型,以光束法平差进行优化;文献<citation id="213" type="reference">[<a class="sup">16</a>]</citation>使用直接法光流跟踪作为前端的视觉里程计,光束法平差作为后端优化方法;因此也继承了直接法的优点与缺点。本文尝试将多镜头组合式全景相机与基于特征的SLAM方法(ORB-SLAM2<citation id="214" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>)相结合,发展一套基于特征的三线程单目全景SLAM系统。</p>
                </div>
                <div class="p1">
                    <p id="73">生产完美的球形镜头受到当前制造工艺的限制。移动全景视觉成像主要采用3种替代模式:多镜头组合式、旋转式、折反射式<citation id="218" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。其中,多镜头组合式是目前最流行的移动全景成像技术,它借助一系列的鱼眼相机分别成像,再拼接为无缝全景图像。目前已经实现了商业化生产,如PointGrey公司的Ladybug系列,中国测绘科学研究院也有相关产品。旋转式成像借助一根CCD的高速旋转,实现360°的连续成像。文献<citation id="219" type="reference">[<a class="sup">14</a>]</citation>曾研究过旋转式全景相机的传感器成像模型和自检校方法。然而,只有2000年前后德国的Noblex公司生产过少量的相关产品。折反射全景相机包括两个组分:镜子和透镜。镜子首先折射周围的光至透镜,透镜再实现成像。由于复杂的成像机理和制造工艺,折反射全景相机应用相对较少,也没有商业上的推广<citation id="220" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。本文的全景SLAM以多镜头组合式全景相机作为视觉信息的获取装备。</p>
                </div>
                <div class="p1">
                    <p id="74">目前,视觉SLAM已经得到了蓬勃的发展,基于单目、双目和RGB-D相机的SLAM技术日臻成熟,能应用于全景相机的SLAM系统也有较多的研究。多镜头组合式全景相机的原理已经由<citation id="221" type="reference">[<a class="sup">25</a>,<a class="sup">26</a>,<a class="sup">27</a>]</citation>等文献建立,鉴于全景视觉的独特优势,发展一套高度自动化的、基于特征的多镜头组合式全景相机的SLAM系统并实现高精度的量测功能,无论在测绘行业还是机器人与计算机视觉都具有积极的意义。本文从全景构像方程、鱼眼相机检校、初始化、局部地图构建、关键帧选择、全图优化等各个步骤探讨基于全景相机的完整的SLAM方法,并在每个步骤中都做出了大量创新性的工作。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">1 全景相机的成像、检校与优化模型</h3>
                <h4 class="anchor-tag" id="76" name="76">1.1 多镜头组合式全景相机模型</h4>
                <div class="p1">
                    <p id="77">多镜头组合式全景相机由一系列独立、固定的鱼眼镜头组成,多个镜头独立成像,再拼接为全景图。如图1(b)所示,每个镜头具有各自的投影中心<i>C</i>,在实际制造过程中尚难以保证与球心<i>S</i>完全重合。可见,物理上的三点共线是<i>C</i>、<i>u</i><sub><i>c</i></sub>和<i>P</i>′。这是独立镜头的成像方程。为了实现统一的全景坐标系统,需要将实际像素坐标<i>u</i><sub><i>c</i></sub>投影到某个指定半径的球面上,得到<i>u</i>。此时,所有的鱼眼图像坐标就可投影至统一的球面坐标。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 全景成像模型" src="Detail/GetImg?filename=images/CHXB201910007_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 全景成像模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.1 Panoramic imaging model</p>

                </div>
                <div class="p1">
                    <p id="79">因此,第1步需要将独立相机的图像坐标转换为统一的全景球面坐标。描述单相机像点<i>u</i><sub><i>c</i></sub>(其坐标用<i><b>x</b></i>表示)计算全景球面像点<i>u</i>(其坐标用<i><b>x</b></i>′表示)的过程如式(1)所示</p>
                </div>
                <div class="p1">
                    <p id="80"><i><b>x</b></i>′=<i>m</i><i><b>R</b></i><sub><i>i</i></sub><i>K</i><sub><i>i</i></sub>(<i><b>x</b></i>)+<i><b>T</b></i><sub><i>i</i></sub>      (1)</p>
                </div>
                <div class="p1">
                    <p id="81"><i>x</i>′<sup>2</sup>+<i>y</i>′<sup>2</sup>+<i>z</i>′<sup>2</sup>=<i>r</i><sup>2</sup>      (2)</p>
                </div>
                <div class="p1">
                    <p id="82">式中,<i>K</i><sub><i>i</i></sub>是第<i>i</i>个鱼眼镜头的内参函数,参量包含镜头畸变差、CCD畸变差、焦距、主点等;<i><b>R</b></i><sub><i>i</i></sub>和<i><b>T</b></i><sub><i>i</i></sub>分别表示第<i>i</i>个鱼眼镜头投影中心在全景坐标系中的旋转矩阵和偏移矢量;<i>K</i><sub><i>i</i></sub>、<i><b>R</b></i><sub><i>i</i></sub>、<i><b>T</b></i><sub><i>i</i></sub>通过严格标定之后即为固定值;<i>m</i>为球面半径<i>r</i>所确定的比例系数,联合如式(2)所示的球面方程可同时解得<i>m</i>和<i><b>x</b></i>′。</p>
                </div>
                <div class="p1">
                    <p id="83">从图1可见,用于表述共线条件方程的光束是<i>CuP</i>′而非<i>SuP</i>。首先表达真实的射线(<i><b>x</b></i>′-<i><b>T</b></i><sub><i>i</i></sub>),并将其平移至统一的球面坐标(加上平移量<i><b>T</b></i><sub><i>i</i></sub>),得到共线条件方程</p>
                </div>
                <div class="p1">
                    <p id="84"><i><b>T</b></i><sub><i>i</i></sub>+<i>λ</i>(<i><b>x</b></i>′-<i><b>T</b></i><sub><i>i</i></sub>)=[<i><b>R</b></i>|<i><b>T</b></i>]<i><b>X</b></i><sub><i><b>w</b></i></sub>      (3)</p>
                </div>
                <div class="p1">
                    <p id="85">当<i><b>T</b></i><sub><i>i</i></sub>取值为0时,式(3)就表达一个完美的球面成像模型(图1(a)),许多文献都采用近似的理想成像模型<citation id="222" type="reference"><link href="30" rel="bibliography" /><link href="44" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">22</a>]</sup></citation>。本文将成像模型用于SLAM系统,需要像点、世界点之间的精确的正反算,因此保留参数<i><b>T</b></i><sub><i>i</i></sub>。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">1.2 鱼眼相机的检校</h4>
                <div class="p1">
                    <p id="87">内参函数<i>K</i><sub><i>i</i></sub>可采用不同的相机检校模型。鱼眼镜头的内参数同样需要精确的检校才能保证在一系列坐标转换中保持高精度。文献<citation id="223" type="reference">[<a class="sup">28</a>]</citation>提出了一种通用鱼眼相机检校方法并得到广泛的应用。如式(4)所示</p>
                </div>
                <div class="p1">
                    <p id="88"><i><b>X</b></i><sub><i>d</i></sub>=(<i>k</i><sub>1</sub><i>θ</i>+<i>k</i><sub>2</sub><i>θ</i><sup>3</sup>)<i><b>u</b></i><sub><i>r</i></sub>(<i>φ</i>)+<i>Δ</i><sub><i>r</i></sub>(<i>θ</i>,<i>φ</i>)<i><b>u</b></i><sub><i>r</i></sub>(<i>φ</i>)+</p>
                </div>
                <div class="p1">
                    <p id="89"><i>Δ</i><sub><i>t</i></sub>(<i>θ</i>,<i>φ</i>)<i>μ</i><sub><i>φ</i></sub>(<i>φ</i>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="90">式中</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Δ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>l</mi><msub><mrow></mrow><mn>1</mn></msub><mi>θ</mi><mo>+</mo><mi>l</mi><msub><mrow></mrow><mn>2</mn></msub><mi>θ</mi><msup><mrow></mrow><mn>3</mn></msup><mo>+</mo><mi>l</mi><msub><mrow></mrow><mn>3</mn></msub><mi>θ</mi><msup><mrow></mrow><mn>5</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>i</mi><msub><mrow></mrow><mn>1</mn></msub><mi>cos</mi><mtext> </mtext><mi>φ</mi><mo>+</mo><mi>i</mi><msub><mrow></mrow><mn>2</mn></msub><mi>sin</mi><mtext> </mtext><mi>φ</mi><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mi>i</mi><msub><mrow></mrow><mn>3</mn></msub><mi>cos</mi><mtext> </mtext><mn>2</mn><mi>φ</mi><mo>+</mo><mi>i</mi><msub><mrow></mrow><mn>4</mn></msub><mi>sin</mi><mtext> </mtext><mn>2</mn><mi>φ</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Δ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub><mi>θ</mi><mo>+</mo><mi>m</mi><msub><mrow></mrow><mn>2</mn></msub><mi>θ</mi><msup><mrow></mrow><mn>3</mn></msup><mo>+</mo><mi>m</mi><msub><mrow></mrow><mn>3</mn></msub><mi>θ</mi><msup><mrow></mrow><mn>5</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>j</mi><msub><mrow></mrow><mn>1</mn></msub><mi>cos</mi><mtext> </mtext><mi>φ</mi><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mtext> </mtext><mtext> </mtext><mi>j</mi><msub><mrow></mrow><mn>2</mn></msub><mi>sin</mi><mtext> </mtext><mi>φ</mi><mo>+</mo><mi>j</mi><msub><mrow></mrow><mn>3</mn></msub><mi>cos</mi><mtext> </mtext><mn>2</mn><mi>φ</mi><mo>+</mo><mi>j</mi><msub><mrow></mrow><mn>4</mn></msub><mi>sin</mi><mtext> </mtext><mn>2</mn><mi>φ</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92"><i><b>X</b></i><sub><i>d</i></sub>=(<i>x</i><sub><i>d</i></sub><i>y</i><sub><i>d</i></sub>)<sup>T</sup>是鱼眼相机坐标;<i>θ</i>和<i>φ</i>分别是天顶角和水平角。<i>l</i><sub><i>k</i></sub>、<i>m</i><sub><i>k</i></sub>、<i>i</i><sub><i>k</i></sub>、<i>j</i><sub><i>k</i></sub>为多项式系数。试验表明,基于该方法检校鱼眼相机并生成核线立体像对,其核线误差约1～1.5像素。为使得核线误差降低到1像素以下,本文对改模型加以改进,提出一种更为精确的鱼眼相机检校方法。如式(5)所示</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mi>λ</mi><mrow><mo>[</mo><mrow><mi>f</mi><mfrac><mrow><mi>arctan</mi><mrow><mo>(</mo><mrow><mfrac><mi>r</mi><mi>f</mi></mfrac></mrow><mo>)</mo></mrow></mrow><mi>r</mi></mfrac><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub><mi>θ</mi><mo>+</mo><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub><mi>θ</mi><msup><mrow></mrow><mn>3</mn></msup><mo stretchy="false">)</mo><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Δ</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">Δ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mi>A</mi><msub><mrow></mrow><mn>1</mn></msub><mi>r</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mrow><mrow><mi>A</mi><msub><mrow></mrow><mn>2</mn></msub><mi>r</mi><msup><mrow></mrow><mn>4</mn></msup><mo>+</mo><mi>A</mi><msub><mrow></mrow><mn>3</mn></msub><mi>r</mi><msup><mrow></mrow><mn>6</mn></msup><mo stretchy="false">)</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub><mo>+</mo><mrow><mo>(</mo><mrow><mtable><mtr><mtd><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mrow><mo>)</mo></mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中,<i>θ</i>与<i>φ</i>可以由像素坐标计算得出。此模型共23个参数,因此需要12个或以上原始图像与纠正后图像的同名点坐标,以求解模型参数。本文显式地加入了鱼眼相机的成像过程(等式右边的第1项),并将式(4)作为其系统误差的附加参数项。这样就减轻了通用多项式模型的拟合负担(即拟合微小的系统误差而不是将鱼眼投影模型都看作系统误差)。本文选取全图像均匀分布的72个点进行最小二乘求解,相机检校精度均在1个像素以内。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">1.3 全景相机单像解析和前方交会</h4>
                <div class="p1">
                    <p id="96">全景相机的定位全部在统一的全景球面坐标系下,即在原始鱼眼相机提取的特征点先根据式(3)转换到球面坐标系下。两个全景球上的同名特征点同样满足对极几何关系<citation id="224" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,也即共面条件方程,如式(6)所示</p>
                </div>
                <div class="p1">
                    <p id="97"><i><b>x</b></i><sup>T</sup><sub>2</sub><i><b>Ex</b></i><sub>1</sub>=0      (6)</p>
                </div>
                <div class="p1">
                    <p id="98">从鱼眼相机坐标计算得到的球面坐标点已经包含了相机内参信息。直接解算式(6)可得到两个全景球之间的本质矩阵<i><b>E</b></i>。使用SVD分解获取旋转矩阵<i><b>R</b></i>与平移分量<i><b>t</b></i>,分解得到4个解。通过局部坐标系下球面特征点及其地图点在同一个方向作为判别依据,可选出正确的<i><b>R</b></i>与<i><b>t</b></i>。平移量<i><b>t</b></i>具有尺度不确定性,本文将当前场景的平均深度归一化从而确定<i><b>t</b></i>的尺度,并使用李代数<i>SE</i>(3)的形式表示相机位姿。</p>
                </div>
                <div class="p1">
                    <p id="99">获取相机位姿的同时,需要根据匹配的特征点计算其地图点。球面坐标的前方交会公式如式(7)所示</p>
                </div>
                <div class="area_img" id="100">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910007_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="102">将同名点代入式(7)可解算得到地图点坐标。式(7)中,<i><b>T</b></i><b>=[</b><i><b>R</b></i><b>|</b><i><b>t</b></i>]为变换矩阵,<i><b>x</b></i><sub>1</sub>与<i><b>x</b></i><sub>2</sub>为同名球面特征点。</p>
                </div>
                <div class="p1">
                    <p id="103">当已知相机位姿的初始值以及全景球面上的特征点所对应的地图点时,采用最小二乘法构建目标函数使球面重投影误差最小</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>e</mi><mo>=</mo><mrow><mi>min</mi></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>s</mi><mi>i</mi></msubsup><mo>-</mo><mfrac><mi>r</mi><mrow><mrow><mo>|</mo><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>ξ</mi><msup><mrow></mrow><mo>^</mo></msup><mo stretchy="false">)</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>w</mi><mi>i</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mi>exp</mi><mo stretchy="false">(</mo><mi>ξ</mi><msup><mrow></mrow><mo>^</mo></msup><mo stretchy="false">)</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>w</mi><mi>i</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="105">式中,<i>ξ</i>∈<i>SE</i>(3)为当前位姿的李代数表示;<i><b>X</b></i><sub><i>w</i></sub>为地图点的世界坐标,两者为未知参数;<i><b>X</b></i><sub><i>s</i></sub>为特征点的球面坐标观测值;<i>r</i>为全景球半径。</p>
                </div>
                <div class="p1">
                    <p id="106">获取有效初值是最小二乘法的关键步骤。若已知图像特征点与地图点的对应关系,文献<citation id="225" type="reference">[<a class="sup">29</a>]</citation>所提出Efficient Perspective-<i>n</i>-Point(EP<i>n</i>P)算法可用于解算位姿的初始值。EP<i>n</i>P在跟踪失败后的重定位过程发挥着重要的作用,本文提出并建立了球面模型上的EP<i>n</i>P方法,用以求解位姿初始值。根据球面投影关系</p>
                </div>
                <div class="area_img" id="107">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910007_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="109">式中,<i>t</i>为比例因子;(<i>x</i><sub><i>j</i></sub>,<i>y</i><sub><i>j</i></sub>,<i>z</i><sub><i>j</i></sub>)为控制点坐标;<i>α</i><sub><i>j</i></sub>为坐标权重;(<i>u</i>,<i>v</i>,<i>w</i>)为球面点坐标。消元可得到球面坐标系下控制点坐标的齐次方程</p>
                </div>
                <div class="area_img" id="110">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910007_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="113">式中,向量<i><b>X</b></i>具有12维,设有<i>n</i>个地图点参与计算,则<i><b>M</b></i>矩阵具有3<i>n</i>维,<i><b>M</b></i>矩阵的核空间变为向量<i><b>X</b></i>的解。至此,4个控制点在全景球坐标系下的坐标及其在世界坐标系下的坐标已求出。再根据式(9)可得出所有正确的3D-2D匹配点在全景球坐标系及对应世界坐标系下的坐标。根据文献<citation id="226" type="reference">[<a class="sup">29</a>]</citation>所述的坐标对齐方法即可解除位姿估值[<i><b>R</b></i><b>|</b><i><b>t</b></i>]。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">1.4 全景相机的非线性优化方法</h4>
                <div class="p1">
                    <p id="115">本文的非线性优化方法基于g2o库<citation id="227" type="reference"><link href="60" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>。而g2o中的所有优化方法只针对普通平面相机模型。本文对相关的优化算法进行改进,使其能够处理全景成像模型。借助于图论,待优化变量,即位姿、地图点设定为图的顶点,重投影误差(如式(8))设定为边,得到优化图(图2)。结合g2o并给出误差函数式(8)的解析导数形式,即误差函数对于位姿及地图点的雅克比矩阵,即可进行优化求解。假定球面位姿表达为<i>ξ</i>∈<i>SE</i>(3),<i><b>P</b></i><sub><i>C</i></sub>=exp(<i>ξ</i><sup>^</sup>),地图点为<i><b>P</b></i><sub><i>W</i></sub>=(<i>X Y Z</i>)<sup>T</sup>且<i>L</i>=<i>X</i><sup>2</sup>+<i>Y</i><sup>2</sup>+<i>Z</i><sup>2</sup>,<i>e</i>是代价函数。则<i>e</i>对于位姿的雅克比矩阵为</p>
                </div>
                <div class="p1">
                    <p id="116"><mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">J</mi><msub><mrow></mrow><mi>ξ</mi></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>e</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow><mrow><mo>∂</mo><mi>δ</mi><mi>ζ</mi></mrow></mfrac></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="117">式中</p>
                </div>
                <div class="area_img" id="118">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910007_11800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="120">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/CHXB201910007_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="122"><i>e</i>对于地图点的雅克比矩阵为</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">J</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>e</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>W</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>e</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>W</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>e</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">R</mi><mo>⋅</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub><mo>+</mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>e</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>C</mi></msub></mrow></mfrac><mo>⋅</mo><mi mathvariant="bold-italic">R</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">式中,[<i><b>R</b></i><b>|</b><i><b>t</b></i>]=exp(<i>ξ</i><sup>^</sup>)。</p>
                </div>
                <div class="p1">
                    <p id="125">本文实现的优化算法包含4类:单帧位姿优化(仅根据匹配的地图点计算当前帧精确的位姿,图2(a))、局部地图优化和平差(根据局部共视关键帧优化位姿与局部地图点,图2(b))、图优化(即essential graph优化,用于检测到闭环后,对全局关键帧的位姿进行调整,图2(c))、全局光束法平差(优化所有位姿和地图点,图2(d))。</p>
                </div>
                <div class="p1">
                    <p id="126">鱼眼图像上不同位置投射到球面上的变形不同,例如,在本文试验中相机中心位置<i>x</i><sub>0</sub>处一个像素的宽度,投影到球面上为0.037 6 m,图像边缘处一个像素宽度投射到球面上表现为0.014 45 m,因此,不同的点<i>x</i>采用不同的误差域值,阈值<i>s</i>设定为</p>
                </div>
                <div class="p1">
                    <p id="127"><mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>2</mn><mrow><mo>[</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">|</mo></mrow><mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></math></mathml>      (15)</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 构建的优化图(三角形节点代表位姿,圆形节点代表地图点,边代表误差项)" src="Detail/GetImg?filename=images/CHXB201910007_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 构建的优化图(三角形节点代表位姿,圆形节点代表地图点,边代表误差项)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.2 Constructed optimization graph (in which the triangle node represents the pose, the circular node represents the map point, and the edge represents the error term)</p>

                </div>
                <h3 id="129" name="129" class="anchor-tag">2 全景SLAM流程</h3>
                <div class="p1">
                    <p id="130">系统将分为3个线程并行工作,分别为跟踪(tracking),局部地图构建(local mapping)与闭环(loop closing)。跟踪线程主要负责系统初始化,地图点跟踪,关键帧选取与位姿优化等功能,局部地图构建线程主要负责特征点三角化,误匹配点与冗余关键帧剔除,局部地图优化等功能,闭环线程主要负责闭环探测,闭环关键帧位姿改正与全局地图优化等功能。以下将逐一介绍各模块的算法策略。</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2.1 初始化</h4>
                <div class="p1">
                    <p id="132">本文将ORB(oriented FAST and rotated BRIEF)特征<citation id="228" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>用于SLAM系统的特征提取。本文针对全景相机做了系统初始化的优化工作。首先,鱼眼相机变形较大,匹配难度高,误匹配数量大。因此采用三帧而非两帧进行初始化。确定第1帧为参考帧,提取ORB特征点,等待系统连续传入两帧,进行第1帧与第2帧,第2帧与第3帧的特征匹配。然后结合RANSAC剔除误匹配点,保留3帧共视点,计算1-2帧和2-3帧的基础矩阵<i><b>F</b></i>。1-2、2-3、1-3组合分别进行三角化,得到3组3D地图点坐标值,设定3个地图点之间的空间距离阈值为0.08 m若有任意两个地图点坐标值的空间距离大于0.08 m,则认为此地图点不稳定,将其剔除。若成功三角化的点数量大于正确匹配点数量的75%,则调用优化模块进行平差并剔除误匹配点。若平差后正确匹配点数量大于30个则视为初始化成功,否则判定为失败,将第2帧作为参考帧,等待系统传入第4帧,使用2、3、4三帧进行初始化,依次迭代,直到初始化成功。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">2.2 地图点跟踪</h4>
                <div class="p1">
                    <p id="134">地图点的跟踪是SLAM自动化的关键环节。在算法1中,列出了全景相机地图跟踪的算法。</p>
                </div>
                <div class="p1">
                    <p id="135">算法1 跟踪地图点</p>
                </div>
                <div class="p1">
                    <p id="136">(1) 采用恒速模型实现3D地图点到当前2D图像的匹配定位。若成功,则结束,否则进入(2)。</p>
                </div>
                <div class="p1">
                    <p id="137">(2) 采用2D图像到2D图像的立体匹配定位。若成功,则结束,否则进入(3)。</p>
                </div>
                <div class="p1">
                    <p id="138">(3) 采用多视图像匹配,作为初值,再进行3D地图点到2D当前图像的匹配。若成功,则结束,否则进入(4)。</p>
                </div>
                <div class="p1">
                    <p id="139">(4) 重定位。采用视觉词袋和P<i>n</i>P解法,实现匹配并定位。</p>
                </div>
                <div class="p1">
                    <p id="140">(5) 只要(1)—(4)有一步成功,调用位姿优化模块进行位姿优化。</p>
                </div>
                <div class="p1">
                    <p id="141">步骤1:采用恒速模型(速度和角度不变)跟踪下一帧地图点。首先,利用恒速模型估计当前相机位姿的初始值,将上一帧观察到的地图点投影到当前帧的全景球面上。然后,根据球面特征点坐标与鱼眼相机的外方位元素,判断该点所在的鱼眼相机编号。最后根据模型(3)反投影到鱼眼图像上设定搜索窗口进行特征匹配。同时计算地图点到全景球面的距离。本文假定当前恒速模型的水平旋转角度绝对值为<i>θ</i>,地图点到相机中心距离为<i>d</i>,则窗口大小<i>W</i>自适应地设为</p>
                </div>
                <div class="p1">
                    <p id="142"><mathml id="200"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo>=</mo><mn>3</mn><mn>0</mn><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mi>θ</mi><mo>-</mo><mn>5</mn></mrow><mrow><mn>1</mn><mn>5</mn></mrow></mfrac></mrow><mo>)</mo></mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>d</mi><mo>-</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (16)</p>
                </div>
                <div class="p1">
                    <p id="143">确定搜索窗口后,在窗口内搜索同名特征点。这里的匹配阈值设置为:最邻近次临近之比大于0.7且描述子距离小于70。对所有地图点进行逐个匹配,若匹配数大于20则认为匹配成功并结束跟踪步骤。否则进入第2步。</p>
                </div>
                <div class="p1">
                    <p id="144">步骤2:在步骤1,即3D地图点到2D图像的匹配失败时执行。采用图像到图像匹配策略,方法是视觉词袋(bag-of-words,BoW)技术。在当前帧与上一帧的特征点间进行匹配,若匹配点数小于10则进入第3步。若匹配点数在10～25之间,则扩大搜索范围,匹配从当前帧直到上一个关键帧所观察到的所有地图特征点。若匹配点数大于等于25,设上一帧的位姿为当前帧的位姿,执行位姿优化同时剔除误匹配粗差点(图2(a)),统计优化后的正确匹配点个数。若小于15,进入第3步;否则判定为匹配成功,返回位姿并结束。</p>
                </div>
                <div class="p1">
                    <p id="145">步骤3:采用多视匹配获取当前帧位姿初值并用于匹配。使用当前帧与前两帧的所有特征点进行匹配,并提取三度重叠点。计算当前帧与前一帧的本质矩阵<i><b>E</b></i>,SVD分解后得到位姿初值,使用该初值将前一帧地图点投影到当前帧,并进行匹配。搜索窗口大小设置方式同式(16)。若匹配点数小于30则判定失败,进入第4步。若匹配点数大于30,则执行位姿优化模块。剔除粗差后若匹配点数再次小于30,则采用第2步BoW的匹配点执行位姿优化模块同时剔除误匹配点。若正确匹配点数大于15则判定成功,返回位姿;否则进入第4步。</p>
                </div>
                <div class="p1">
                    <p id="146">步骤4:在以上匹配都失败时,进行重定位(re-localization)。使用BoW搜索上一帧到上一个关键帧的所有匹配点,调用P<i>n</i>P配合RANSAC进行重定位。</p>
                </div>
                <div class="p1">
                    <p id="147">步骤5:以上步骤任意一步成功即可得到当前帧的位姿的初始值。利用该值,将局部地图点全部投影至当前帧进行匹配,匹配完成后进行局部优化同时剔除误匹配点(图2(b))。统计正确匹配点数,若大于20则判定为成功,若小于20判定为失败,传入下一帧,执行步骤4。</p>
                </div>
                <h4 class="anchor-tag" id="148" name="148">2.3 关键帧选取</h4>
                <div class="p1">
                    <p id="149">正确跟踪当前帧之后,判断当前帧是否作为关键帧。本文的全景影像框幅高达8000×4000像素,由于车辆高速行驶,存在内存读写的限制,因此采用较大的采样间隔(1～3 m)。判别当前全景影像为关键帧的条件为满足算法2列表中的任意一点。</p>
                </div>
                <div class="p1">
                    <p id="150">算法2 关键帧选取的判别规则</p>
                </div>
                <div class="p1">
                    <p id="151">(1) 超过8帧未插入关键帧,则确定当前帧为关键帧。</p>
                </div>
                <div class="p1">
                    <p id="152">(2) 当前帧新插入的地图点大于其跟踪到的地图点的1.7倍。</p>
                </div>
                <div class="p1">
                    <p id="153">(3) 当前帧跟踪到的地图点数小于上一帧跟踪到的点数的85%且局部地图构建线程空闲。</p>
                </div>
                <div class="p1">
                    <p id="154">(4) 当前帧跟踪到的地图点数小于上一帧跟踪到的50%,中断局部地图构建并插入关键帧。</p>
                </div>
                <div class="p1">
                    <p id="155">(5) 局部地图构建(local mapping)线程空闲。</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156">2.4 局部地图构建</h4>
                <div class="p1">
                    <p id="157">当局部地图线程检测到新插入的关键帧后,首先应更新当前关键帧的连接关系,找出与其具有一定共视点数的关键帧作为共视关键帧。根据全景相机的特殊情形,本文设定共视关键帧必须同时满足:① 与当前关键帧的共视点数≥50个;② 与当前关键帧在图像序列中的间隔关键帧不超过40个;③ 其与当前关键帧的共视点在两帧所在金字塔层数差异&gt;2的特征点数不超过总共视点数的85%。</p>
                </div>
                <h4 class="anchor-tag" id="158" name="158">2.5 闭环探测与全局优化</h4>
                <div class="p1">
                    <p id="159">使用词袋模型进行闭环探测的技术较为成熟,本文使用文献<citation id="229" type="reference">[<a class="sup">28</a>,<a class="sup">31</a>]</citation>提出的闭环探测方法,利用ORB特征点结合DBoW2库进行闭环探测,流程如算法3所示。</p>
                </div>
                <div class="p1">
                    <p id="160">算法3 闭环探测流程</p>
                </div>
                <div class="p1">
                    <p id="161">检测到新关键帧插入:</p>
                </div>
                <div class="p1">
                    <p id="162">(1) 计算当前关键帧的BoW向量,用于表示当前帧的场景信息。</p>
                </div>
                <div class="p1">
                    <p id="163">(2) 通过相邻关键帧计算相似性阈值。</p>
                </div>
                <div class="p1">
                    <p id="164">(3) 通过关键帧相连关系计算相似性累计得分,结合2的阈值选取闭环候选帧。</p>
                </div>
                <div class="p1">
                    <p id="165">(4) 连续性检验:认为孤立的候选组具有很强的不确定性。</p>
                </div>
                <div class="p1">
                    <p id="166">(5) 通过连续性检验的候选关键帧均认为与当前帧构成闭环。</p>
                </div>
                <div class="p1">
                    <p id="167">闭环关键帧选取完毕后,依次与当前关键帧使用BoW搜索匹配特征点,若匹配点数小于30,则剔除此闭环关键帧,继续下一帧的匹配。匹配成功后,按照文献<citation id="230" type="reference">[<a class="sup">32</a>]</citation>的方法计算闭环帧到当前帧的相似变换群,即sim(3)变换,此时便得到了闭环关键帧与当前帧的转换关系<i>S</i>,通过转化关系再次搜索匹配点并使用局部光束法平差优化<i>S</i>。最后,调整当前帧及其相连关键帧的位姿与地图点,更新关键帧链接关系并传递调整位姿,构建Essential Graph进行全局位姿优化(图2(c))并调整相应地图点。</p>
                </div>
                <h3 id="168" name="168" class="anchor-tag">3 试验结果和分析</h3>
                <h4 class="anchor-tag" id="169" name="169">3.1 试验设计</h4>
                <div class="p1">
                    <p id="170">本文试验选取了两组数据均采集自车载Ladybug3相机,分别称为Kashiwa和Omiya图像序列。Ladybug拍摄鱼眼影像大小为1616×1232像素,像元分辨率为0.009 mm,颜色分辨率为8 bit量化的RGB色彩空间。鱼眼相机的焦距为3.3 mm,投影中心相互距离约40 mm;共有6个鱼眼相机,5个相机用于360°水平成像,1个相机指向天空。全景影像球面成像于距离球心20 m处。拼接后的全景影像大小为4000×2000像素。</p>
                </div>
                <div class="p1">
                    <p id="171">Kashiwa图像序列包含498张全景影像,摄影间隔约2 m;Omiya图像序列包含7464张全景影像,拍摄间隔约为1 m。由于天空上没有特征点,匹配在独立的水平鱼眼相机中执行,而<i><b>F</b></i>矩阵计算、光束法平差都是在统一的全景坐标系中进行。参考值由高精度的GPS/IMU组合导航系统获得,且GPS经过CORS站的事后改正,其绝对定位精度高于10 cm。GPS/IMU数据不参与SLAM流程。本文同时采用两种精度评价方法进行评价,首先按照经典的SLAM算法,给定初始位置,姿态与尺度信息将相对位姿统一到给定的坐标系下,之后不再给出任何控制信息,并计算绝对定位均方误差以衡量SLAM系统的实时动态定位精度。考虑到在测绘中的应用,全局定位精度更受关注,因此本文选取均匀分布的少量GPS点作为控制点,根据7参数相似变换将所有的SLAM后得到的外方位元素转换到绝对坐标系中,并计算所有相片外方位元素与参考值间的绝对值,作为精度评定指标。本文的SLAM系统运行在Ubantu16.04环境下,使用Qt5.8作为集成开发环境。CPU为Intel Core i7-7700HQ@2.8 GHz,内存为8 GB,执行效率约每秒1帧。</p>
                </div>
                <h4 class="anchor-tag" id="172" name="172">3.2 试验结果和分析</h4>
                <h4 class="anchor-tag" id="173" name="173">3.2.1 相机检校结果</h4>
                <div class="p1">
                    <p id="174">表1比较了全景相机中鱼眼镜头的检校结果。采用63个均匀分布的校准点,根据不同的检校模型,解算对应的模型参数,并评估了检校中误差。本文方法的精度为0.56像素,明显高于文献<citation id="231" type="reference">[<a class="sup">33</a>]</citation>提出的通用相机检校模型,并后继保证了优于一个像素的核线精度。</p>
                </div>
                <div class="area_img" id="175">
                                            <p class="img_tit">
                                                <b>表1 本文使用的鱼眼相机检校方法检校结果与文献</b><citation id="232" type="reference">[<a class="sup">33</a>]</citation><b>对比</b>
                                                    <br />
                                                <b>Tab.1 Calibration result of generic camera model and the proposed method</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910007_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">像素</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 本文使用的鱼眼相机检校方法检校结果与文献[33]对比" src="Detail/GetImg?filename=images/CHXB201910007_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="176" name="176">3.2.2 特征提取与匹配结果</h4>
                <div class="p1">
                    <p id="177">图3所示为Ladybug数据集使用三帧同时匹配的结果,由于鱼眼相机变形较大,距离相机越近的地物变形越大,可以看出,特征点基本分布于距相机相对较远的地物上。图中3号相机只匹配出了极少的特征点。这表明基于单相机SLAM容易受到匹配条件的困扰而追踪失败。而本文使用的多镜头组合式全景相机可依赖其余镜头捕捉到的足够的特征点完成跟踪和后继平差。此外,由于本文根据几何关系设定了恰当的搜索区间(算法1),0号相机和4号相机的同名点也能够进行正确匹配。</p>
                </div>
                <div class="area_img" id="178">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 连续三帧图像所匹配的ORB特征点" src="Detail/GetImg?filename=images/CHXB201910007_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 连续三帧图像所匹配的ORB特征点  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.3 Matched ORB features on three adjacent frames</p>

                </div>
                <h4 class="anchor-tag" id="179" name="179">3.2.3 闭环探测结果</h4>
                <div class="p1">
                    <p id="180">如图4所示,蓝色连线表示所探测的闭环,Kashiwa数据集共探测出52个闭环,Omiya数据集共探测出203个闭环。在SLAM系统中,一旦成功探测出闭环,将进行相似变换关系的计算,传播位姿并更新连接关系,连接关系更新后,便可避免重复的路径中对于冗余的闭环探测过程(蓝线所示),重复路径中的关键帧将更新与先前帧的相连关系,并统一参与位姿图的优化。</p>
                </div>
                <h4 class="anchor-tag" id="181" name="181">3.2.4 定位结果</h4>
                <div class="p1">
                    <p id="182">表2为Kashiwa和Omiya图像序列使用全景SLAM跟踪所得位姿精度。仅用前两帧的GPS、IMU数据作为SLAM轨迹的对齐,其他所有的GPS/IMU数据作为参考和精度评价。Kashiwa序列(1 km,498张全景影像)定位平均精度达到30 cm。Kashiwa序列具有很好的共视条件,不使用闭环纠正其精度依旧能达到46 cm。Omiya序列(9 km,7492张影像)仅在图4所示处存在闭环,且运行里程较长,因此闭环约束的范围有限,无闭环纠正精度为42.7 m,加入闭环后精度为19 m。</p>
                </div>
                <div class="p1">
                    <p id="183">事实上,在测绘中更加关注静态的全局定位精度,而非机器人领域的动态当前定位精度。因此本文分别在Kashiwa与Omiya图像序列上分别加入3个与15个均匀分布的GPS控制点进行7参数转换与整体平差,将SLAM轨迹纳入到绝对坐标系下,并统计更符合实际应用的定位精度。如表3所示,两套数据集的定位精度分别达到13 cm与16 cm。在角度上,车辆的姿态基本仅在水平方向上有旋转,误差主要集中在水平旋转角Rz上。</p>
                </div>
                <div class="area_img" id="184">
                                            <p class="img_tit">
                                                <b>表2 Kashiwa图像序列与Omiya图像序列数据集使用全景SLAM跟踪所得结果,仅用前两帧GPS/IMU数据对齐SLAM轨迹</b>
                                                    <br />
                                                <b>Tab.2 Pose and map point location accuracy of the trajectories of Kashiwa and Omiya sequences, aligned by the given initial pose and scale</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_18400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910007_18400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_18400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 Kashiwa图像序列与Omiya图像序列数据集使用全景SLAM跟踪所得结果,仅用前两帧GPS/IMU数据对齐SLAM轨迹" src="Detail/GetImg?filename=images/CHXB201910007_18400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="185">
                                            <p class="img_tit">
                                                <b>表3 Kashiwa图像序列与Omiya图像序列使用全景SLAM跟踪所得结果,用最小数量均匀分布的GPS点7参数变换对齐SLAM轨迹</b>
                                                    <br />
                                                <b>Tab.3 Pose and map point location accuracy of the trajectories of Kashiwa and Omiya sequences, minimum number of even distributed GPS points are used to align SLAM trajectories</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/CHXB201910007_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 Kashiwa图像序列与Omiya图像序列使用全景SLAM跟踪所得结果,用最小数量均匀分布的GPS点7参数变换对齐SLAM轨迹" src="Detail/GetImg?filename=images/CHXB201910007_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="186">特征法SLAM可得到稀疏地图点云,如图5所示。由于没有实际的控制点评价物方定位精度,本文采用误差传播的方式,利用摄站定位误差估计物方点误差。表2中的地图点残差是与GPS/IMU作为真值前方交会出的地图点坐标进行比较的结果。</p>
                </div>
                <div class="p1">
                    <p id="187">图5给出了Kashiwa与Omiya序列的使用全景SLAM系统得到的全局地图全览(包括关键帧的位姿及其连接关系与地图点分布情况示意图),图中Kashiwa序列(图5(a))当前帧为接近终点的第492帧,在无闭环纠正的情况下,尺度的漂移非常小。这归功于全景相机的360°成像。图5中绿色的线条显示了关键帧的连接关系,同时也是位姿优化中重要的图连接关系。</p>
                </div>
                <div class="p1">
                    <p id="188">图6表示通过给定初始位姿态与方向,将SLAM的轨迹通过7参数转换到地理参考坐标系下与GPS的轨迹对比。其中,蓝色曲线为GPS参考位置,红色曲线为全景SLAM所跟踪出的位姿结果。可以看出,Kahiswa序列由于极好的共视关系,跟踪定位位置与GPS参考位置近乎完全重合,Omiya序列由于运行里程较长、共视关系较差且无较好的闭环约束,出现了一定程度的尺度漂移。然而,在加入极少量(15个)控制点后,定位精度变具有明显的改善,与GPS参考轨迹一致(图6(d))。</p>
                </div>
                <div class="area_img" id="189">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_189.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 两套数据集上所探测出的闭环" src="Detail/GetImg?filename=images/CHXB201910007_189.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 两套数据集上所探测出的闭环  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_189.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.4 Detected loops on two datasets</p>

                </div>
                <div class="area_img" id="190">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Kashiwa与Omiya序列使用全景SLAM系统跟踪后得到的全局地图总览(关键帧及其连接关系,地图点分布情况)" src="Detail/GetImg?filename=images/CHXB201910007_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Kashiwa与Omiya序列使用全景SLAM系统跟踪后得到的全局地图总览(关键帧及其连接关系,地图点分布情况)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_190.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.5 Overall of global map of Kashiwa and Omiya sequences by panoramic SLAM system (status of current frame, tracked map points, tracked key frames and their connections)</p>

                </div>
                <div class="area_img" id="191">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 两个区域的SLAM结果以及加入3/15个控制点后的定位结果分别与GPS参考的对比" src="Detail/GetImg?filename=images/CHXB201910007_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 两个区域的SLAM结果以及加入3/15个控制点后的定位结果分别与GPS参考的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig. 6 The comparison between GPS reference and SLAM results before and after GCP alignment</p>

                </div>
                <h4 class="anchor-tag" id="192" name="192">3.3 待改进之处</h4>
                <div class="p1">
                    <p id="193">本文采用BoW词袋模型并参考ORB-SLAM闭环探测的方法对全景图像序列进行闭环探测。文献<citation id="233" type="reference">[<a class="sup">34</a>]</citation>指出在复杂场景中闭环探测具有相当的难度,ORB-SLAM使用的闭环探测方法具有很大的局限性。这在全景SLAM中问题更加突出。如图7所示,除了球面投影形变,地面物体的分布也发生了剧烈的变化。不过,归功于全景相机的大范围视场,未加入闭环约束的位姿跟踪已可达到较为满意的效果。</p>
                </div>
                <div class="area_img" id="194">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/CHXB201910007_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 反向行驶拍摄的全景图像" src="Detail/GetImg?filename=images/CHXB201910007_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 反向行驶拍摄的全景图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/CHXB201910007_194.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Fig.7 Two panoramic images shot in the reverse directions</p>

                </div>
                <h3 id="195" name="195" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="196">本文提出了基于多镜头组合式全景相机的全自动SLAM系统。该系统能够实现全景图像系列的定位和稀疏地图重建,其定位精度接近GPS参考精度。相对于国际上主流的基于传统平面相机的各类SLAM系统,如Mono-SLAM、Stereo-SLAM以及RGB-D SLAM,本文提出的全景SLAM可作为良好的补充,辅助或取代昂贵的GPS/IMU导航系统,特别是在GPS信号失锁时。后期的工作将集中于全景相机影像的闭合探测,以增强几何约束,得到更好的定位精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="2">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806003&amp;v=MjI2NDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDluTXFZOUZaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 龚健雅,季顺平.摄影测量与深度学习[J].测绘学报,2018,47(6):693-704.DOI:10.11947/j.AGCS.2018.20170640.GONG Jianya,JI Shunping.Photogrammetry and deep learning[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):693-704.DOI:10.11947/j.AGCS.2018.20170640.
                            </a>
                        </p>
                        <p id="4">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201806010&amp;v=Mjc0NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdVYi9QSmlYVGJMRzRIOW5NcVk5RVpJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 邸凯昌,万文辉,赵红颖,等.视觉SLAM技术的进展与应用[J].测绘学报,2018,47(6):770-779.DOI:10.11947/j.AGCS.2018.20170652.DI Kaichang,WANG Wenhui,ZHAO hongying,et al.Progress and application of visual SLAM[J].Acta Geodaetica et Cartographica Sinica,2018,47(6):770-779.DOI:10.11947/j.AGCS.2018.20170652.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201606001&amp;v=MjM4MDU3cWZadWR2RnlyZ1ViL1BMejdCYUxHNEg5Zk1xWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> LIU H M,ZHANG G F,BAO H J.A survey of monocular simultaneous localization and mapping[J].Journal of Computer-Aided Design &amp; Computer Graphics,2016,28(6):855-868.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MonoSLAM: Real-Time Single Camera SLAM">

                                <b>[4]</b> DAVISON A J,REID I D,MOLTON N D,et al.MonoSLAM:real-time single camera SLAM[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2007,29(6):1052-1067.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM:A Versatile and Accurate Monocular SLAM System">

                                <b>[5]</b> MUR-ARTAL R,MONTIEL J M M,TARDÓS J D.ORB-SLAM:A versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics,2017,31(5):1147-1163.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD-SLAM:Large-scale direct monocular SLAM">

                                <b>[6]</b> ENGEL J,SCHÖPS T,CREMERS D.LSD-SLAM:large-scale direct monocular SLAM[M]//FLEET D,PAJDLA T,SCHIELE B,et al.Computer Vision-ECCV 2014.Zurich:Cham- Springer,2014:834-849.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SVO:fast semi-direct monocular visual odometry">

                                <b>[7]</b> FORSTER C,PIZZOLI M,SCARAMUZZA D.SVO:Fast semi-direct monocular visual odometry[C]//Proceedings of IEEE International Conference on Robotics and Automation.Hong Kong:IEEE,2014:15-22.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SVO:Semidirect Visual Odometry for Monocular and Multicamera Systems">

                                <b>[8]</b> FORSTER C,ZHANG Z C,GASSNER M,et al.SVO:semidirect visual odometry for monocular and multicamera systems[J].IEEE Transactions on Robotics,2017,33(2):249-265.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES60024CCA6DF70F2C7F2EA12264F5E4D4&amp;v=MTU4OTB1SFlmT0dRbGZCckxVMDU5bGh4cjI5d0s4PU5pZk9mYlc0SHRQSTNQdzBZcDk1QzN4UHpXVVVuRDBJT1g3Z3JoUXhEN2ZoUWM2YkNPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> GAO X,ZHANG T.Robust RGB-D simultaneous localization and mapping using planar point features[J].Robotics and Autonomous Systems,2015,72(10):1-14.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLAM-Loop Closing with Visually Salient Features">

                                <b>[10]</b> NEWMAN P,HO K.SLAM-loop closing with visually salient features[C]//Proceedings of 2005 IEEE International Conference on Robotics and Automation.Barcelona:IEEE,2005:635-642.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">

                                <b>[11]</b> RUBLEE E,RABAUD V,KONOLIGE K,et al.ORB:An efficient alternative to SIFT or SURF[C]//Proceedings of 2011 International Conference on Computer Vision.Barcelona:IEEE,2011:2564-2571.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201412008&amp;v=MDY3Mjk5WE5yWTlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ViL1BKaVhUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 季顺平,史云.多镜头组合型全景相机两种成像模型的定位精度比较[J].测绘学报,2014,43(12):1252-1258.DOI:10.13485/j.cnki.11-2089.2014.0169.JI Shunping,SHI Yun.Comparison of two sensor models for multi-camera rig system in measurements[J].Acta Geodaetica et Cartographica Sinica,2014,43(12):1252-1258.DOI:10.13485/j.cnki.11-2089.2014.0169.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201411012&amp;v=MjQzNzBYTnJvOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 季顺平,史云.高噪声环境下基于参考影像的车载序列影像定位方法[J].测绘学报,2014,43(11):1174-1181.DOI:10.13485/j.cnki.11-2089.2014.0181.JI Shunping,SHI Yun.Georegistration of ground sequential imagery with geo-referenced aerial images in high noise environments[J].Acta Geodaetica et Cartographica Sinica,2014,43(11):1174-1181.DOI:10.13485/j.cnki.11-2089.2014.0181.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual SLAM with an omnidirectional camera">

                                <b>[14]</b> RITUERTO A,PUIG L,GUERRERO J J.Visual SLAM with an omnidirectional camera[C]//Proceedings of the 20th International Conference on Pattern Recognition.Istanbul,Turkey:IEEE,2010:348-351.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001252476&amp;v=MTUzNTBubFVicktJRms9TmlmY2FyTzRIdEhOcllwSFlPd0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZ5&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> LEMAIRE T,LACROIX S.SLAM with panoramic vision[J].Journal of Field Robotics,2010,24(1-2):91-111.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFA7A9A8979587CBECA23597899EBD201&amp;v=MTUwMDZERjNvZE1ZK0lLQkh0S3ZXTmdtejErVFhibHBCczhETURnUjdxZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNTlsaHhyMjl3Szg9TmlmT2ZjWEpHYQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> JI Shunping,SHI Yun,SHAN Jie,et al.Particle filtering methods for georeferencing panoramic image sequence in complex urban scenes[J].ISPRS Journal of Photogrammetry and Remote Sensing,2015,105(7):1-12.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale direct slam for omnidirectional cameras">

                                <b>[17]</b> CARUSO D,ENGEL J,CREMERS D.Large-scale direct SLAM for omnidirectional cameras[C]//Proceedings of 2015 IEEE/RSJ IEEE International Conference on Intelligent Robots and Systems.Hamburg:IEEE,2015:141-148.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201711005&amp;v=MzExMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ViL1BKaVhUYkxHNEg5Yk5ybzlGWVlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 谢东海,钟若飞,吴俣,等.球面全景影像相对定向与精度验证[J].测绘学报,2017,46(11):1822-1829.DOI:10.11947/j.AGCS.2017.20160645.XIE Donghai,ZHONG Ruofei,WU Yu,et al.Relative pose estimation and accuracy verification of spherical panoramic image[J].Acta Geodaetica et Cartographica Sinica,2017,46(11):1822-1829.DOI:10.11947/j.AGCS.2017.20160645.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201704010&amp;v=MjQzOTMzenFxQnRHRnJDVVI3cWZadWR2RnlyZ1ViL1BKaVhUYkxHNEg5Yk1xNDlFWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 闫利,王奕丹.扩展共线方程结合车载组合全景相机影像区域网平差[J].测绘学报,2017,46(4):460-467.DOI:10.11947/j.AGCS.2017.20160464.YAN Li,WANG Yidan.Block adjustment of vehicle-borne multi-camera rig images using extended collinearity equations[J].Acta Geodaetica et Cartographica Sinica,2017,46(4):460-467.DOI:10.11947/j.AGCS.2017.20160464.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201501011&amp;v=MDM1NDhIOVRNcm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVkdkZ5cmdVYi9QSmlYVGJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 石丽梅,赵红蕊,李明海,等.车载移动测图系统外方位元素标定方法[J].测绘学报,2015,44(1):52-58.DOI:10.11947/j.AGCS.2015.20130203.SHI Limei,ZHAO Hongrui,LI Minghai,et al.Extrinsic calibration for vehicle-based mobile mapping system[J].Acta Geodaetica et Cartographica Sinica,2015,44(1):52-58.DOI:10.11947/j.AGCS.2015.20130203.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Catadioptric camera calibration">

                                <b>[21]</b> GEYER C,DANIILIDIS K.Catadioptric camera calibration[C]//Proceedings of the 7th IEEE International Conference on Computer Vision.Kerkyra,Greece,Greece:IEEE,1999:398-404.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extrinsic Camera Parameter Recovery from Multiple Image Sequences Captured by An Omni-directional Multi-camera System">

                                <b>[22]</b> SATO T,IKEDA S,YOKOYA N.Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system[C]//Proceedings of European Conference on Computer Vision.Prague,Czech Republic:Springer,2004:326-340.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras">

                                <b>[23]</b> MUR-ARTAL R,TARDÓS J D.ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEE Transactions on Robotics,2017,33(5):1255-1262.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system">

                                <b>[24]</b> IKEDA S,SATO T,YOKOYA N.High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system[C]//Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems,MFI2003.Tokyo,Japan:IEEE,2003.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adapting A Real-time Monocular Visual SLAM from Conventional to Omnidirectional Cameras">

                                <b>[25]</b> GUTIERREZ D,RITUERTO A,MONTIEL J M M,et al.Adapting a real-time monocular visual SLAM from conventional to omnidirectional cameras[C]//Proceedings of 2011 IEEE International Conference on Computer Vision Workshops.Barcelona,Spain:IEEE,2011:343-350.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Map Building and Monte Carlo Localization Using Global Appearance of Omnidirectional Images">

                                <b>[26]</b> PAYÁ L,FERNÁNDEZ L,GIL A,et al.Map building and monte carlo localization using global appearance of omnidirectional images[J].Sensors,2010,10(12):11468-11497.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FAB-MAP:Appearancebased place recognition and mapping using a learned visual vocabulary model">

                                <b>[27]</b> CUMMINS M,NEWMAN P.FAB-MAP:appearance-based place recognition and mapping using a learned visual vocabulary model[C]//Proceedings of the 27th International Conference on Machine Learning.Haifa,Israel:[s.n.],2010.
                            </a>
                        </p>
                        <p id="56">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501263499&amp;v=MTk2MjlNbndaZVp0RmlubFU3N0lJbHNXYWhVPU5pZk9mYks3SHRETnFvOUVadTBNQ0hVd29CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> SATO T,YOKOYA N.Efficient hundreds-baseline stereo by counting interest points for moving omni-directional multi-camera system[J].Journal of Visual Communication and Image Representation,2010,21(5-6):416-426.
                            </a>
                        </p>
                        <p id="58">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003394813&amp;v=MTI2MjNCYk9vTVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RnlubFVicktJRms9Tmo3QmFyTzRIdEhQckla&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b> LEPETIT V,MORENO-NOGUER F,FUA P.EP<i>n</i>P:an accurate <i>O</i>(<i>n</i>) solution to the P<i>n</i>P problem[J].International Journal of Computer Vision,2009,81(2):155-166.
                            </a>
                        </p>
                        <p id="60">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=g2o:A general frame-work for graph optimization">

                                <b>[30]</b> KÜMMERLE R,GRISETTI G,STRASDAT H,et al.G<sup>2</sup>o:a general framework for graph optimization[C]//Proceedings of 2011 IEEE International Conference on Robotics and Automation.Shanghai,China:IEEE,2011:3607-3613.
                            </a>
                        </p>
                        <p id="62">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale Drift-aware Large Scale MonocularSLAM">

                                <b>[31]</b> STRASDAT H,MONTIEL J M M,DAVISON A J.Scale drift-aware large scale monocular SLAM[M]// Robotics:Science and Systems.Zurich:The MIT Press,2010.
                            </a>
                        </p>
                        <p id="64">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Closed-form solution of absolute orientation using orthonormal matrices">

                                <b>[32]</b> HORN B K P,HILDENH M,NEGAHDARIPOUR S.Closed-form solution of absolute orientation using orthonormal matrices[J].Journal of the Optical Society of America A,1998,5(7):1127-1135.
                            </a>
                        </p>
                        <p id="66">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses">

                                <b>[33]</b> KANNALA J,BRANDT S S.A generic camera model and calibration method for conventional,wide-angle,and fish-eye lenses[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,28(8):1335-1340.
                            </a>
                        </p>
                        <p id="68">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ENFT: Efficient Non-Consecutive Feature Tracking for Robust Structure-from-Motion">

                                <b>[34]</b> ZHANG Guofeng,LIU Haomin,DONG Zilong,et al.Efficient non-consecutive feature tracking for robust structure-from-motion[J].IEEE Transactions on Image Processing,2016,25(12):5957-5970.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="CHXB201910007" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201910007&amp;v=MTI4NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1ZHZGeXJnVWIvUEppWFRiTEc0SDlqTnI0OUZZNFE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM28vVUR0R2xyamVab0dVVnFzS1A1UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
