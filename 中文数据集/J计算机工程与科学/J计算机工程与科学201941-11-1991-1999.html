<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132340873467500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJK201911013%26RESULT%3d1%26SIGN%3d98V6s%252btnbHgz0i54A4XSm1j4xUo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911013&amp;v=MzAzMDhIOWpOcm85RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnZKTHo3QlpiRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#164" data-title="(1)通过空间金字塔池化SPP(Spatial Pyramid Pooling)操作,既提高了模型对输入图像形变的鲁棒性,又提升了模型对目标物品的识别精度。">(1)通过空间金字塔池化SPP(Spatial Pyramid Pooling)操作,既提高了模型对......</a></li>
                                                <li><a href="#165" data-title="(2)在实际应用中,商品图像的数据规模都是百万级的,因此商品图像检索方法对检索速度十分敏感。">(2)在实际应用中,商品图像的数据规模都是百万级的,因此商品图像检索方法对检索速度十分敏感。</a></li>
                                                <li><a href="#166" data-title="(3)在商品图像中,不同种类间图像的数据规模往往存在明显差异。">(3)在商品图像中,不同种类间图像的数据规模往往存在明显差异。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="&lt;b&gt;2 本文方法&lt;/b&gt; "><b>2 本文方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;2.1 整体框架&lt;/b&gt;"><b>2.1 整体框架</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;2.2 SHN模型&lt;/b&gt;"><b>2.2 SHN模型</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.3 相似性度量与检索策略&lt;/b&gt;"><b>2.3 相似性度量与检索策略</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;3.1 实验数据与评价标准&lt;/b&gt;"><b>3.1 实验数据与评价标准</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;3.2 实验性能分析&lt;/b&gt;"><b>3.2 实验性能分析</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.3 加入金字塔池化层和FQ损失层的性能对比&lt;/b&gt;"><b>3.3 加入金字塔池化层和FQ损失层的性能对比</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;3.4 检索结果&lt;/b&gt;"><b>3.4 检索结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="&lt;b&gt;4 结束语&lt;/b&gt; "><b>4 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="图1 本文方法流程图">图1 本文方法流程图</a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表1 卷积子网络配置&lt;/b&gt;"><b>表1 卷积子网络配置</b></a></li>
                                                <li><a href="#75" data-title="图2 金字塔池化层结构图">图2 金字塔池化层结构图</a></li>
                                                <li><a href="#107" data-title="图3 训练集中各类别数量分布">图3 训练集中各类别数量分布</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表2 各哈希检索方法的&lt;/b&gt;&lt;i&gt;&lt;b&gt;mAP&lt;/b&gt;&lt;/i&gt;&lt;b&gt;值对比&lt;/b&gt; (&lt;b&gt;返回最近邻个数为50&lt;/b&gt;)"><b>表2 各哈希检索方法的</b><i><b>mAP</b></i><b>值对比</b> (<b>返回最近邻个数为50</b>)</a></li>
                                                <li><a href="#116" data-title="图4 各方法检索性能对比">图4 各方法检索性能对比</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表3 按汉明距离排序的&lt;/b&gt;&lt;i&gt;&lt;b&gt;mAP&lt;/b&gt;&lt;/i&gt;&lt;b&gt;值(返回最近邻个数为100&lt;/b&gt;)"><b>表3 按汉明距离排序的</b><i><b>mAP</b></i><b>值(返回最近邻个数为100</b>)</a></li>
                                                <li><a href="#125" data-title="图5 本文方法不同哈希码位数时检索性能比较图">图5 本文方法不同哈希码位数时检索性能比较图</a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表4 不同位数的得分情况&lt;/b&gt;"><b>表4 不同位数的得分情况</b></a></li>
                                                <li><a href="#130" data-title="图6 检索结果">图6 检索结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="5">


                                    <a id="bibliography_1" title=" Kiapour M H,Han X,Lazebnik S,et al.Where to buy it:Matching street clothing photos in online shops[C]//Proc of 2015 IEEE International Conference on Computer Vision(ICCV),2015:3343-3351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Where to buy it:Matching street clothing photos in online shops">
                                        <b>[1]</b>
                                         Kiapour M H,Han X,Lazebnik S,et al.Where to buy it:Matching street clothing photos in online shops[C]//Proc of 2015 IEEE International Conference on Computer Vision(ICCV),2015:3343-3351.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_2" title=" Gong Y,Wang L,Guo R,et al.Multi-scale orderless pooling of deep convolutional activation features[C]//Proc of European Conference on Computer Vision,2014:392-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale orderless pooling of deep convolutional activation features">
                                        <b>[2]</b>
                                         Gong Y,Wang L,Guo R,et al.Multi-scale orderless pooling of deep convolutional activation features[C]//Proc of European Conference on Computer Vision,2014:392-407.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_3" title=" Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDE0ODZ4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDRGxWTDNOSkY4PU5qN0Jhck80SHRIT3A0&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_4" title=" Oliva A,Torralba A.Modeling the shape of the scene:A holistic representation of the spatial envelope[J].International Journal of Computer Vision,2001,42(3):145-175." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830679&amp;v=MTM1ODNGWXV3R1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZMM05KRjg9Tmo3QmFyTzRIdEhPcDR4&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Oliva A,Torralba A.Modeling the shape of the scene:A holistic representation of the spatial envelope[J].International Journal of Computer Vision,2001,42(3):145-175.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_5" title=" Perronnin F,Dance C.Fisher kernels on visual vocabularies for image categorization[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition,2007:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fisher Kernels on Visual Vocabularies for Image Categorization">
                                        <b>[5]</b>
                                         Perronnin F,Dance C.Fisher kernels on visual vocabularies for image categorization[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition,2007:1-8.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_6" title=" Wang J,Yang J,Kai Y,et al.Locality-constrained linear coding for image classification[C]//Proc of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2010:3360-3367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locality-constrained linear coding for image classification">
                                        <b>[6]</b>
                                         Wang J,Yang J,Kai Y,et al.Locality-constrained linear coding for image classification[C]//Proc of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2010:3360-3367.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_7" title=" Pramod K P,Vadakkepat P,Poh L A.Fuzzy-rough discriminative feature selection and classification algorithm,with application to microarray and image datasets[J].Applied Soft Computing,2011,11(4):3429-3440." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300297830&amp;v=Mjg1NDBaZVp1SHlqbVVMZklKMXdSYmhNPU5pZk9mYks3SHRETnJJOUZadUlJQkg4NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Pramod K P,Vadakkepat P,Poh L A.Fuzzy-rough discriminative feature selection and classification algorithm,with application to microarray and image datasets[J].Applied Soft Computing,2011,11(4):3429-3440.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_8" title=" Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C]//Proc of European Conference on Computer Vision(ECCV 2014),2014:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[8]</b>
                                         Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C]//Proc of European Conference on Computer Vision(ECCV 2014),2014:818-833.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_9" title=" Kan S,Cen Y,He Z,et al.Supervised deep feature embedding with handcrafted feature[J].IEEE Transactions on Image Processing,2019,28(12):5809-5823." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised deep feature embedding with handcrafted feature">
                                        <b>[9]</b>
                                         Kan S,Cen Y,He Z,et al.Supervised deep feature embedding with handcrafted feature[J].IEEE Transactions on Image Processing,2019,28(12):5809-5823.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Xiong Chang-zhen,Shan Yan-mei,Guo Fen-hong.Image retrival method based on image principal part detection[J].Optics and Precision Engineering,2017,25(3):792-798.(in Chinese)</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Dong Rong-sheng,Cheng De-qiang,Li Feng-ying.Aggregating deep convolutional features for image retrieval using multi-regional cross weighting[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2018,30(4):659-665.(in Chinese)</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     Yuan Hui,Liao Kai-yang,Zheng Yuan-lin,et al.Image retrieval based on CNN feature weighting and region integration[J].Computer Engineering &amp;amp; Science,2019,41(1):113-121.(in Chinese)</a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Lin T Y,Goyal P,Girshick R,et al.Focal loss for dense object detection[C]//Proc of 2017 IEEE International Conference on Computer Vision,2017:2999-3007.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_14" title=" Lai H J,Pan Y,Liu Y,et al.Simultaneous feature learning and hash coding with deep neural networks[C]//Proc of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),2015:3270-3278." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous feature learning and hash coding with deep neural networks.">
                                        <b>[14]</b>
                                         Lai H J,Pan Y,Liu Y,et al.Simultaneous feature learning and hash coding with deep neural networks[C]//Proc of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),2015:3270-3278.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_15" title=" Lu J,Liong V E,Zhou J.Deep hashing for scalable image search[J].IEEE Transactions on Image Processing,2017,26(5):2352-2369." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep hashing for scalable image search">
                                        <b>[15]</b>
                                         Lu J,Liong V E,Zhou J.Deep hashing for scalable image search[J].IEEE Transactions on Image Processing,2017,26(5):2352-2369.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_16" title=" Wan J,Wang D,Hoi C H,et al.Deep learning for content-based image retrieval:A comprehensive study[C]//Proc of ACM International Conference on Multimedia,2014:303-313." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for contentbased image retrieval:A comprehensive study">
                                        <b>[16]</b>
                                         Wan J,Wang D,Hoi C H,et al.Deep learning for content-based image retrieval:A comprehensive study[C]//Proc of ACM International Conference on Multimedia,2014:303-313.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_17" title=" Liu W,Wang J,Ji R,et al.Supervised hashing with kernels[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition,2012:2074-2081." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised hashing with kernels">
                                        <b>[17]</b>
                                         Liu W,Wang J,Ji R,et al.Supervised hashing with kernels[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition,2012:2074-2081.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_18" title=" Gong Y,Lazebnik S,Gordo A,et al.Iterative quantization:A procrustean approach to learning binary codes for large-scale image retrieval[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2916-2929." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Iterative Quantization:A Procrustean Approach to Learning Binary Codes for Large-Scale Image Retrieval">
                                        <b>[18]</b>
                                         Gong Y,Lazebnik S,Gordo A,et al.Iterative quantization:A procrustean approach to learning binary codes for large-scale image retrieval[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2916-2929.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_19" title=" Liu H,Wang R,Shan S,et al.Deep supervised hashing for fast image retrieval[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:2064-2072." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Supervised Hashing for Fast Image Retrieval&amp;quot;">
                                        <b>[19]</b>
                                         Liu H,Wang R,Shan S,et al.Deep supervised hashing for fast image retrieval[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:2064-2072.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_20" title=" Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//Proc of 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2015:27-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning of Binary Hash Codes for Fast Image Retrieval">
                                        <b>[20]</b>
                                         Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//Proc of 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2015:27-35.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_21" title=" Li Q,Sun Z,He R,et al.Deep supervised discrete hashing[C]//Proc of Advances in Neural Information Processing Systems(NIPS 2017),2017:1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Supervised Discrete Hashing">
                                        <b>[21]</b>
                                         Li Q,Sun Z,He R,et al.Deep supervised discrete hashing[C]//Proc of Advances in Neural Information Processing Systems(NIPS 2017),2017:1.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     Shen F,Xu Y,Liu L,et al.Unsupervised deep hashing with similarity-adaptive and discrete optimization[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2018,40(12):3034-3044.附中文参考文献:</a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_10" title=" 熊昌镇,单艳梅,郭芬红.结合主体检测的图像检索方法[J].光学精密工程,2017,25(3):792-798." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703034&amp;v=MDE0MzJGeS9uVnJ2SklqWEJZN0c0SDliTXJJOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm0=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         熊昌镇,单艳梅,郭芬红.结合主体检测的图像检索方法[J].光学精密工程,2017,25(3):792-798.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_11" title=" 董荣胜,程德强,李凤英.用于图像检索的多区域交叉加权聚合深度卷积特征[J].计算机辅助设计与图形学学报,2018,30(4):659-665." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201804014&amp;v=MjI4NTlKTHo3QmFMRzRIOW5NcTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         董荣胜,程德强,李凤英.用于图像检索的多区域交叉加权聚合深度卷积特征[J].计算机辅助设计与图形学学报,2018,30(4):659-665.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_12" title=" 袁晖,廖开阳,郑元林,等.基于CNN特征加权和区域整合的图像检索[J].计算机工程与科学,2019,41(1):113-121." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201901015&amp;v=MTA2NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnZKTHo3QlpiRzRIOWpNcm85RVlZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         袁晖,廖开阳,郑元林,等.基于CNN特征加权和区域整合的图像检索[J].计算机工程与科学,2019,41(1):113-121.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(11),1991-1999 DOI:10.3969/j.issn.1007-130X.2019.11.013            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于SHN模型的商品图像检索方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BA%E5%91%A8%E9%9B%A8&amp;code=43344733&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贺周雨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%97%AD%E9%B9%8F&amp;code=32929442&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯旭鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%88%A9%E5%86%9B&amp;code=10537962&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘利军</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E9%9D%92%E6%9D%BE&amp;code=07895919&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄青松</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0242668&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学信息工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%8C%96%E5%BB%BA%E8%AE%BE%E7%AE%A1%E7%90%86%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学信息化建设管理中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BA%91%E5%8D%97%E7%9C%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学云南省计算机技术应用重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>近年来电子商务行业快速发展,如何通过图像信息在庞大的商品库中快速、准确地找到所需要的商品具有重要的应用价值。针对商品图像数据规模大、类间数据量差异大、被拍摄商品的尺度相差较大以及压缩图像会损失掉细节信息的特点,提出了一个融合金字塔池化策略与哈希学习的空间金字塔池化哈希网络SHN模型,作为本文商品图像检索方法的特征提取部分。为了提高模型对图像形变的鲁棒性,采用金字塔池化策略实现多尺度特征融合;为了使学习到的哈希码具有更好的独立性,使用量化误差损失及附加权值对哈希编码进行约束。本文方法保留了原始图像信息,解决了图像尺度变化所带来的负面影响,通过哈希编码能够实现快速的商品图像检索,商品图像检索实验中的<i>mAP</i>值达到91.986 3%,完成一次检索所用时间为0.034 856 s,检索性能优于当前主流方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%95%86%E5%93%81%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">商品图像检索;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%B1%A0%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度池化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%93%88%E5%B8%8C%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈希学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    贺周雨(1995-),女,贵州兴义人,硕士生,研究方向为深度学习和图像处理。E-mail:he535040@qq.com,通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                                <span>
                                    冯旭鹏(1986-),男,河南巩义人,硕士,实验师,CCF会员(51769M),研究方向为深度学习。E-mail:281364805@qq.com;
                                </span>
                                <span>
                                    刘利军(1978-),男,河南辉县人,硕士,副教授,CCF会员(49941M),研究方向为医学图像智能识别。E-mail:cloneiq@126.com,通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                                <span>
                                    *黄青松(1962-),男,湖南长沙人,硕士,教授,研究方向为智能信息系统。E-mail:kmustailab@hotmail.com,通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(81860318,81560296);</span>
                    </p>
            </div>
                    <h1><b>A product image retrieval method based on SHN model</b></h1>
                    <h2>
                    <span>HE Zhou-yu</span>
                    <span>FENG Xu-peng</span>
                    <span>LIU Li-jun</span>
                    <span>HUANG Qing-song</span>
            </h2>
                    <h2>
                    <span>Faculty of Information Engineering and Automation,Kunming University of Science and Technology</span>
                    <span>Information Technology Center,Kunming University of Science and Technology</span>
                    <span>Yunnan Key Laboratory of Computer Technology Applications,Kunming University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In recent years, with the rapid development of the e-commerce industry, how to quickly and accurately find the required goods through image information in the huge product library has important application value.Aiming at the characteristics such as the large scale of product image data, the large difference of data between classes, the large difference between the scales of photographed products and the loss of detailed information in compressed images, an Spatial pyramid pooling-Hash-Net(SHN)model combining spatial pyramid pooling strategy and hash learning is proposed as the feature extraction part of the product image retrieval method. In order to improve the robustness of the model to image deformation, the spatial pyramid pooling strategy is adopted to achieve multi-scale feature fusion. In order to make the learned hash code have better independence, the quantization error loss and additional weights are used to constrain the hash code.The method preserves the original image information and solves the negative effects caused by image scale changes, and it can realize fast product image retrieval through hash coding.The experimental results show that the <i>mAP</i> value of this method reaches 91.986 3%, and the time for completing a search is 0.034 856 s. The image retrieval performance is better than the current mainstream methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=product%20image%20retrieval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">product image retrieval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20convolutional%20neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep convolutional neural networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20pooling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale pooling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hash%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hash learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HE Zhou-yu,born in 1995,MS candidate,her research interests include deep learning,and image processing.Address:Faculty of Information Engineering and Automation,Kunming University of Science and Technology,Kunming 650500, Yunnan,P.R.China;
                                </span>
                                <span>
                                    FENG Xu-peng,born in 1986,MS,experimentalist,CCF member(51769M),his research interest includes deep learning.;
                                </span>
                                <span>
                                    LIU Li-jun,born in 1978,MS,associate professor,CCF member(49941M),his research interests include intelligent medical image recognition.Address:Faculty of Information Engineering and Automation,Kunming University of Science and Technology,Kunming 650500, Yunnan,P.R.China;
                                </span>
                                <span>
                                    HUANG Qing-song,born in 1962,MS, professor,his research interest includes intelligent information system.Address:Faculty of Information Engineering and Automation,Kunming University of Science and Technology,Kunming 650500, Yunnan,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="56">随着大数据时代的到来以及移动互联网的普及,电子商务行业得到了迅速的发展。商品图像检索CMIR(Commodity Image Retrieval)作为电子商务的重要技术支持,得到了更多的关注。如何提升商品图像检索的速度和准确率以满足用户的需求是亟待解决的问题<citation id="133" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。传统的图像检索系统主要是基于文本的图像检索TBIR(Text-Based Image Retrieval)。然而,文本数据具有很大的局限性,很难准确描述出图像信息,且依赖用户输入的搜索关键词是否准确<citation id="134" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。相反地,基于内容的图像检索CBIR(Content-Based Image Retrieval)技术能够解决这些问题,因为图像本身能够提供更多的信息。在CBIR中,对于图像特征的提取是最为关键的一步,直接影响了整个检索系统的性能。CBIR通过提取图像特征来表达图像的内容。尺度不变特征转换SIFT(Scale-Invariant Feature Transform)<citation id="135" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、全局特征信息Gist<citation id="136" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等方法被广泛地使用在底层特征提取过程中。10年前,关于特征词袋BoF(Bag of Features)<citation id="143" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>与底层特征提取方法相结合的方法占主流,然而这些方法都没有编码全局空间信息,并且缺乏学习能力<citation id="137" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。深度卷积神经网络CNNs(Convolutional Neural Networks)应运而生,它能够挖掘到图像数据的内在隐含关系。Zeiler等人<citation id="138" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用经过卷积、池化操作得到的图像特征成功重构了原始图像,说明原始图像像素经过卷积、池化等操作后,依旧保留了大量的全局空间信息。Kan等人<citation id="139" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>发现有监督的深度学习和人工制作的判别特征相结合适用于图像检索。他们提取了一个新的嵌入人工特征的有监督深度特征模型,实验结果证明该方法有效。但是,该方法需要大量的人为工作,不具有泛化性。熊昌镇等人<citation id="140" type="reference"><link href="23" rel="bibliography" /><link href="49" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">10</a>]</sup></citation>提出了一种结合深度卷积神经网络与主体检测的图像检索方法,实验结果表明该方法能有效提升检索的准确率,但其将2个深度卷积神经网络相结合,导致检索所用时间较长。董荣胜等人<citation id="141" type="reference"><link href="25" rel="bibliography" /><link href="51" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">11</a>]</sup></citation>提出了一种基于多区域的交叉加权聚合深度卷积特征描述算法。该算法通过赋值权重实现聚合图像特征,实验结果表明该算法能有效提升检索的准确率,但并未针对商品图像进行网络设计与实验。袁晖等人<citation id="142" type="reference"><link href="27" rel="bibliography" /><link href="53" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">12</a>]</sup></citation>通过对卷积层输出特征进行重新加权及区域整合,得到一个较低维的特征用于图像检索。实验结果表明该方法能有效提升检索的准确率,但用于检索的特征维度不够低且利用余弦距离计算相似度的方法计算量很大,导致检索所用时间较长,并不适用于大规模的图像检索。近年来,CNNs被广泛地应用在图像处理,CNNs相关方法表现出的性能已经远远优于传统方法,在图像检索任务中能够达到又快又准。基于此背景,本文提出了一种通过深度卷积神经网络生成二分哈希码来描述图像信息的商品图像检索方法。</p>
                </div>
                <div class="p1">
                    <p id="57">商品图像检索CMIR的任务是寻找并返回和查询图像拥有同样商品的图像。CMIR和CBIR之间的本质区别是,CBIR只要求返回同一类别的相似图像,而CMIR要求返回拥有相同商品的图像。因此,CMIR比CBIR在细粒度任务上更严格,也更加具有挑战性<citation id="144" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。在CMIR中,通常只关注检索图像中目标商品所在的区域,其它部分都可以认为是背景。所以,图像特征提取部分是否对形变具有鲁棒性或者是否能去掉背景,对于商品图像检索方法的性能至关重要。针对CMIR只关注目标所在区域,希望去掉复杂背景的特点,一些早期学者通过人工标注的方法标注目标区域,再进行后续操作。这样的方法需要投入大量的人力。之后,一些学者通过图像分割的方法截取了商品主体所在区域,但这类方法依赖图像分割的准确性。除此之外,在商品图像中,不同种类的商品图像的数据规模存在差异。Lin等人<citation id="145" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的研究表明,样本种类间数量的差异对模型的性能影响很大,目前主流的方法都忽视了这个问题。同时,商品图像数据量增长迅速,基于大规模数据库的商品图像检索也成为迫切的需求。鉴于深度卷积神经网络在特征学习上的优越性以及哈希方法在检索中计算速度和存储空间上的优越性,近几年出现了深度卷积神经网络与哈希技术相结合的方法。Lai等人<citation id="146" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的模型除了利用深度卷积网络学习图像特征和二进制哈希码外,还把图像的三元组作为监督信息。该方法需要挑选出质量好的三元组信息,这使得人为工作量增大。Lu等人<citation id="147" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>使用随机投影法进行哈希编码,但只实现了大规模数据检索,没有考虑其它负面影响。基于以上分析,本文提出了一个基于空间金字塔池化哈希网络SHN(Spatial pyramid pooling-Hash-Net)模型的商品图像检索方法。与其他相关方法相比,本文方法的主要改进有:</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164">(1)通过空间金字塔池化SPP(Spatial Pyramid Pooling)操作,既提高了模型对输入图像形变的鲁棒性,又提升了模型对目标物品的识别精度。</h4>
                <div class="p1">
                    <p id="58">由于SPP操作能够对任意大小的特征图进行多尺度特征提取并转换成固定大小的特征向量,因此模型可以接受不同尺寸的图像输入,从而避免了因对原始图像进行裁剪或者拉伸等操作所带来的信息损失或扭曲,并进一步提升目标物品的识别精度。</p>
                </div>
                <h4 class="anchor-tag" id="165" name="165">(2)在实际应用中,商品图像的数据规模都是百万级的,因此商品图像检索方法对检索速度十分敏感。</h4>
                <div class="p1">
                    <p id="59">哈希方法在计算速度和存储空间上具有优越性。本文方法具有哈希编码功能,可以得到用于快速检索的哈希码。</p>
                </div>
                <h4 class="anchor-tag" id="166" name="166">(3)在商品图像中,不同种类间图像的数据规模往往存在明显差异。</h4>
                <div class="p1">
                    <p id="61">样本不平衡问题会严重制约模型的训练质量。本文方法通过量化损失约束哈希码的学习策略,驱使网络尽量学习不易区分的样本,从而消除样本不平衡所带来的负面影响,得到更为准确的哈希码。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag"><b>2 本文方法</b></h3>
                <h4 class="anchor-tag" id="63" name="63"><b>2.1 整体框架</b></h4>
                <div class="p1">
                    <p id="64">本文方法的流程如图1所示,提供了从待检索图像输入到相似图像输出的检索功能流程,主要分为特征提取和检索2个部分。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法流程图" src="Detail/GetImg?filename=images/JSJK201911013_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文方法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Flowchart of the proposed method</p>

                </div>
                <div class="p1">
                    <p id="65">本文设计了SHN模型来提取图像特征。图像相应的哈希码由训练好的SHN模型的哈希层输出。SHN模型的训练过程为:(1)输入图像经过卷积子网络,把图像信息映射到隐层特征空间中,特别地,SPP层可以提取到图像的深层特征表示;(2)经过全连接层6以及全连接层7,把上层得到的局部特征表示映射到样本标记空间中,其中全连接层6的输出特征矩阵为1×5376,全连接层7的输出特征矩阵为1×4096;(3)进入哈希层进行降维及哈希编码,哈希层输出<i>n</i>维的图像特征(<i>n</i>为设置的哈希码位数);(4)进入全连接层8,全连接层8的输出神经元个数为10(类别数);(5)通过FQ(Focal and Quantization)损失层计算损失,驱动网络训练。</p>
                </div>
                <div class="p1">
                    <p id="66">通过SHN模型学习到目标区域的特征表示以及相应的哈希码,得到一个哈希码库以及待检索图像的哈希码。再比较待检索图像的哈希码与特征库中图像的哈希码之间的汉明距离完成检索。将汉明距离按从小到大的顺序排列,然后返回前<i>k</i>个值对应的图像,即为该待检索图像的检索结果。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>2.2 SHN模型</b></h4>
                <div class="p1">
                    <p id="68">商品图像检索的关键在于如何提取图像特征。本文针对商品图像数据规模大、类间数据量差异大、被拍摄商品的尺度相差较大以及压缩图像会损失掉细节信息的特点,设计了SHN模型进行图像特征提取。经典的CNNs只能接受同一尺寸的输入图像,不同尺寸的图像需要经过裁剪或压缩等操作统一图像尺寸,必然会损失一些图像细节,进行图像特征提取时,不能提取到图像的潜在深层特征。SHN模型采用金字塔池化网络,可以接受不同尺寸的输入图像,完整地保留了图像信息。金字塔池化层整合多个尺度的图像特征,充分地考虑到了图像中目标区域位置多变带来的负面影响,能够挖掘到区分不同图像的潜在特征。再通过哈希层将提取到的高维图像特征降维以实现快速的商品图像检索。SHN模型包含5层卷积层、3层池化层、3层全连接层和1层哈希层,主要分为4个部分:(1)卷积子网络,用于学习图像的特征表示。(2)金字塔池化层,用于实现尺度不变性以及固定当前层输出特征的尺寸。(3)哈希层,用于得到相应的哈希码。(4)FQ损失层,用于优化网络结构。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">2.2.1 卷积子网络</h4>
                <div class="p1">
                    <p id="70">卷积子网络用于学习图像的特征表示。通过卷积、池化、激励等操作对原始图像进行逐层提取与抽象,能够挖掘到图像特征之间的内在联系。</p>
                </div>
                <div class="p1">
                    <p id="71">通过卷积运算能够提取输入的不同特征。卷积层1可能只提取了一些低级的特征,如边缘、线条和角等,经过更多卷积层能从低级特征中迭代提取更复杂的特征。卷积层由若干卷积单元组成,SHN模型在训练过程中使用了迁移学习的方法,所以每个卷积单元的权值初始值为预训练模型的值或随机值,再通过反向传播方法优化权值。本文方法中卷积子网络的配置见表1,以输入图像尺寸为3×227×227为例。</p>
                </div>
                <div class="area_img" id="72">
                                            <p class="img_tit">
                                                <b>表1 卷积子网络配置</b>
                                                    <br />
                                                <b>Table 1 Convolutional subnetwork configuration</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_07200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJK201911013_07200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_07200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 卷积子网络配置" src="Detail/GetImg?filename=images/JSJK201911013_07200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2.2 金字塔池化层</h4>
                <div class="p1">
                    <p id="74">通常图像特征在经过卷积层后的维度都很大。例如,本文卷积子网络中卷积层5输出图像特征矩阵的大小为256×13×13。通过池化层对输入的特征矩阵进行下采样操作,能够压缩图像特征数,减轻网络的计算负担。常用的池化操作有最大值池化、均值池化和P范数池化等。考虑到商品图像中商品的尺度不一以及输入图像的大小不一,本文采用金字塔池化策略,为SHN模型添加了一个SPP层。SPP层的输入是卷积层5的输出。图像特征进入金字塔池化层后,使用不同尺寸的滑窗对特征进行划分,滑窗的尺寸及步长分别为13×13/1,7×7/1,4×4/3,如图2所示。3个滑窗对图像特征分别进行池化后,再拼接输出的特征,最后得到尺寸为5 376的图像特征矩阵。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 金字塔池化层结构图" src="Detail/GetImg?filename=images/JSJK201911013_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 金字塔池化层结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Spatial pyramid pooling layer structure</p>

                </div>
                <div class="p1">
                    <p id="76">金字塔池化层整合了多个尺度的图像特征,以此模糊背景、突出目标信息,提高模型对尺度的鲁棒性。CNNs之所以要固定输入图像尺寸,是因为全连接层的需要。同时,图像特征通过SPP层后,得到的是一个固定尺寸的输出,也就是固定了下一层(全连接层6)的输入尺寸,从而使得整个模型可以接受不同大小的输入图像,避免了图像压缩或图像裁剪过程中的细节损失。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">2.2.3 哈希层</h4>
                <div class="p1">
                    <p id="78">在CNNs中,全连接层(fc6～fc8)的特征可以当作视觉特征。大量的研究直接利用全连接层输出的特征进行分类和检索,例如,Wan等人<citation id="148" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>将3个全连接层的输出都作为特征表示进行检索。然而这些特征维度都很高,计算开销很大,不适合应用在大规模的商品图像检索中。鉴于哈希方法在检索中计算速度和存储空间上的优越性,本文为SHN模型添加了一个哈希层,用于得到一个低维度的二分哈希码。在高维数据空间中的2个相邻的数据被映射到低维数据空间后,将会有很大的概率仍然是相邻的;而原本不相邻的2个数据,在低维空间中也将有很大的概率不相邻。本文根据这个基本思想,在构建哈希位时使用随机映射。哈希层的神经元个数为想要的哈希码位数。在SHN模型中,哈希层的上一层为全连接层7,则哈希层的输入为1×4096的特征矩阵,记为<i>x</i><sub><i>i</i></sub>(<i>i</i>=1,2,…,4 096)。根据式(1)进行特征映射。</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>j</mi></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中,<i>j</i>的取值范围为1,2,…,<i>n</i>;<i>n</i>为哈希码的位数。<i><b>W</b></i>为4096×<i>n</i>权重矩阵。<i><b>W</b></i><sub><i>j</i></sub>取自<i><b>W</b></i>相应的列,大小为4096×1。再通过激活函数Sigmoid,将特征矩阵<i>f</i><sub><i>j</i></sub>(<i>x</i><sub><i>i</i></sub>)的特征值近似到{0,1},如式(2)所示。</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">经过式(2)得到的图像特征矩阵还不是一个完整的哈希码。再根据式(3)才能得到一个完整二分哈希码。</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>,</mo><mi>S</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>≥</mo><mn>0</mn><mo>.</mo><mn>5</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mi>S</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0</mn><mo>.</mo><mn>5</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式(3)的转换会产生一个量化误差。在模型的训练过程中,使用一个量化误差损失函数对学习到的哈希码进行约束,以消除这种误差带来的负面影响。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">2.2.4 FQ损失层</h4>
                <div class="p1">
                    <p id="86">损失层通过比较网络的输出和目标值,最小化损失来驱动网络的训练。网络的损失通过前向操作计算,网络参数相对于损失函数的梯度则通过反向操作计算。Lin等人<citation id="149" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>发现样本数量的差异对模型的性能影响很大。在商品图像的检索中,不同类别的商品往往拥有的数据量也不同,并且在哈希编码过程中会出现量化损失。因此,本文结合Focal损失函数和量化误差损失函数,使用FQ损失函数计算损失值。</p>
                </div>
                <div class="p1">
                    <p id="87">Focal损失函数是在标准的交叉熵Softmax损失函数的基础上改进而来的。Focal损失函数针对类间数据量不均匀的问题,为不同样本添加权重,以此控制不同样本对损失值的影响。常用的Softmax Loss如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>=</mo><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">其中,<i>i</i>表示当前节点;<i>K</i>表示节点总数,一般来说等于类别数。Softmax Loss函数如下:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup><mi>y</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>ln</mi><mspace width="0.25em" /><mi>S</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mo>-</mo><mi>ln</mi><mspace width="0.25em" /><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">在单一标签分类任务中,分类的最终结果只能是一个类别。所以,在进行前向传播、计算损失值时,可以只计算真实标签对应的损失值,忽略其它标签的损失值。即当真实标签对应的节点为<i>i</i>时,式(5)中的<i>y</i><sub><i>i</i></sub>为1,其它节点的<i>y</i>值为0。与其他方法使用激活函数得到概率值<i>p</i><sub><i>i</i></sub>不同,本文使用式(4)的结果作为预测得到的概率值<i>p</i><sub><i>i</i></sub>。Focal损失函数之所以能够解决样本不均匀的问题,是因为它能够通过2个超参数的设置,使概率值成为区分样本难易程度的反馈,驱使网络更倾向于学习不易区分的样本。Focal损失函数如下所示:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>=</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mi>γ</mi></msup><mi>ln</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">其中,<i>α</i>,<i>γ</i>为超参数,<i>p</i><sub><i>i</i></sub>由式(4)可得。根据式(6)可知,<i>p</i><sub><i>i</i></sub>越大,则说明当前样本越易区分。相应地,(1-<i>p</i><sub><i>i</i></sub>)<sup><i>γ</i></sup>值就越小,即当前样本的损失值所占权重小,进行反向传播时对权值更新的影响小。从而实现了聚焦于学习不易区分样本,达到了消除样本数量不均匀带来的负面影响的目的。</p>
                </div>
                <div class="p1">
                    <p id="94">2.2.3节提到哈希码的编码过程中会产生量化误差。考虑到这一误差,本文在目标损失函数中添加量化损失函数对哈希编码进行约束,如式(7)所示。</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mtext>q</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">∥</mo><mi>Η</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>-</mo><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中,<i>f</i><sub><i>j</i></sub>(<i>x</i><sub><i>i</i></sub>)由式 (1)得出;<i>S</i>(<i>f</i><sub><i>j</i></sub>(<i>x</i><sub><i>i</i></sub>))为哈希层中激活层输出的哈希码,由式(2)可得;<i>H</i>(<i>f</i><sub><i>j</i></sub>(<i>x</i><sub><i>i</i></sub>))为哈希层输出的完整哈希码,由式(3)可得。结合Focal损失函数和量化误差损失函数,得到FQ损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>F</mtext><mtext>Q</mtext></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>+</mo><mi>λ</mi><mi>L</mi><msub><mrow></mrow><mtext>q</mtext></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中,<i>λ</i>为权重因子。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.3 相似性度量与检索策略</b></h4>
                <div class="p1">
                    <p id="100">训练图像通过SHN模型得到一个哈希码特征库,待检索图像通过SHN模型得到相应的哈希码。本文采用汉明距离来衡量待检索图像的哈希码与特征库中图像的哈希码之间的相似度。对这2个哈希码进行异或运算,统计结果为1的个数,这个数就是汉明距离<i>D</i>(<i>x</i>,<i>y</i>),如式(9)所示。汉明距离越大,则待检索图像与当前特征库图像之间的差异越大,即相似度越低。将汉明距离从小到大排序,选取前<i>k</i>个相似图像返回作为检索结果。</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mi>X</mi></mstyle><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>⊕</mo><mi>Y</mi><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">其中,<i>i</i>=0,1,…,<i>n</i>-1;<i>X</i>,<i>Y</i>是2个<i>n</i>位的哈希码。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="104" name="104"><b>3.1 实验数据与评价标准</b></h4>
                <div class="p1">
                    <p id="105">本文实验的环境配置是 Intel(R) Xeon(R) Silver 4110 CPU 2.10 GHz * 2,GPU NVIDIA TITAN XP的硬件平台以及Matlab,Python的软件平台。</p>
                </div>
                <div class="p1">
                    <p id="106">本文实验数据集来自京东、淘宝等真实购物网站。为了保障实验结果的准确性,数据集包含训练集、测试集和检索验证集,其中检索验证集与其他数据集中无重复图像。数据集中包含卖家秀与买家秀,图像尺寸大小不一,图像背景既有复杂的,也有简单的。本文数据集包含10个类别,训练集图像共有21 400幅,测试集共有6 000幅,验证集共有6 000幅。由于商品图像的特殊性,不同类别的商品数量存在很大差异,所以本文的训练集中各类别图像数量不同,最大数量差异达1 800幅,如图3所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 训练集中各类别数量分布" src="Detail/GetImg?filename=images/JSJK201911013_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 训练集中各类别数量分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Quantitative distribution of various types in training dataset</p>

                </div>
                <div class="p1">
                    <p id="108">本文采用ImageNet模型进行迁移学习。使用随机梯度下降法来训练模型,权值衰减量为0.000 5。训练过程中,基础学习率为0.001,随着训练进行以gamma为0.1进行迭代下降。式(6)和式(8)中的超参数取值为:<i>α</i>=1,<i>γ</i>=2,<i>λ</i>=0.4。衡量检索性能的指标是分类准确度<i>P</i>、平均准确率均值<i>mAP</i>以及运行时间,其定义如式(10)和式(11)所示:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Ν</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>m</mi><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mi>A</mi></mstyle><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>k</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中,<i>NP</i>为验证集中的图像总数,<i>NTP</i>为验证集中正确分类的图像数,<i>AP</i>为平均分类准确度。<i>P</i>为查询结果中正确分类的结果所占的比例。<i>mAP</i>为多个验证集的查询结果的平均准确率均值。运行时间为输入一幅查询图像到返回检索结果所用的时间。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>3.2 实验性能分析</b></h4>
                <div class="p1">
                    <p id="112">为了更好地说明本文方法的性能,将本文方法与目前检索性能较理想的方法做比较。作为对比实验的方法有:基于核函数的监督哈希方法KSH(Supervised Hashing with Kernels)<citation id="150" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、迭代量化哈希方法ITQ(ITerative Quantization)<citation id="151" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、特征学习监督哈希方法CNNH(Convolutional Neural Networks Hashing)<citation id="152" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、深度学习学习到的哈希码用于进行快速图像检索方法DCNNH(Deep Convolutional Neural Networks Hashing)<citation id="153" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、深度离散哈希方法DSDH(Deep Supervised Discrete Hashing)<citation id="154" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、具有相似自适应和离散优化的无监督哈希学习方法SADH(Similarity-Adaptive and Discrete Hashing)<citation id="155" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。对比实验结果如表2和图4所示。</p>
                </div>
                <div class="p1">
                    <p id="113">由表2可知,本文方法的<i>mAP</i>值远远大于KSH和ITQ 2种方法的。KSH和ITQ是传统方法与哈希方法的结合,而本文方法利用深度卷积神经网络学习图像特征,在图像表示能力上大大地超过传统方法,能够学习到更好的图像特征,从而得到更准确的哈希码。同时,本文方法的<i>mAP</i>值也大于其他CNNs与哈希技术相结合的方法。本文方法相对于其它方法,充分考虑了商品图像的特殊性,在商品图像检索中表现出优秀的性能,与其他方法相比得到的<i>mAP</i>值最高。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表2 各哈希检索方法的</b><i><b>mAP</b></i><b>值对比</b> (<b>返回最近邻个数为50</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Comparison of </b><i><b>mAP</b></i><b> values of different hashing retrieval methods(</b><i><b>top</b></i><b>_</b><i><b>k</b></i><b>=50</b>)</p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="4"><br />哈希码位数</td></tr><tr><td><br />48位</td><td>64位</td><td>96位</td><td>128位</td></tr><tr><td><br />SHN</td><td><b>0.913 1</b></td><td><b>0.911 9</b></td><td><b>0.919 8</b></td><td><b>0.916 7</b></td></tr><tr><td><br />SADH</td><td>0.894 8</td><td>0.907 4</td><td>0.909 2</td><td>0.909 2</td></tr><tr><td><br />DSDH</td><td>0.908 4</td><td>0.898 8</td><td>0.904 5</td><td>0.902 8</td></tr><tr><td><br />DCNNH</td><td>0.880 3</td><td>0.897 5</td><td>0.891 4</td><td>0.88 61</td></tr><tr><td><br />CNNH</td><td>0.587 4</td><td>0.588 4</td><td>0.594 7</td><td>0.607 4</td></tr><tr><td><br />KSH</td><td>0.432 4</td><td>0.434 5</td><td>0.454 7</td><td>0.440 8</td></tr><tr><td><br />ITQ</td><td>0.277 4</td><td>0.293 0</td><td>0.271 8</td><td>0.268 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="115">图4展示了本文方法与其它哈希检索方法在其他性能上的比较结果。图4a是各方法在不同哈希码位数(48,64,72,96)时的平均准确率均值对比。选取不同哈希位数,本文方法的<i>mAP</i>值均大于其它方法的。图4b是各方法在不同哈希码位数(48,64,72,96)时的运行时间对比。可以看出,本文方法在保证准确率的同时也能保持较高的运行效率。图4c是各方法返回最近邻个数(50,60,70,80,90,100)与<i>mAP</i>的关系(哈希码位数为96)。可以看出,本文方法较为稳定,随着返回最近邻个数的增加,<i>mAP</i>值变化不大。特别地,CNNH方法在返回最近邻个数为100时,<i>mAP</i>值出现明显下降。综上,本文方法在不同哈希码位数时的<i>mAP</i>值都大于其它方法的,在保证准确率的同时也能保持较高的运行效率,对返回最近邻个数具有稳定性,表现出的检索性能均优于现有的其他方法。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 各方法检索性能对比" src="Detail/GetImg?filename=images/JSJK201911013_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 各方法检索性能对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Comparison of retrieval performance among different methods</p>

                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.3 加入金字塔池化层和FQ损失层的性能对比</b></h4>
                <div class="p1">
                    <p id="118">为了进一步验证本文方法的有效性,本节对本文方法(SHN)、未加入SPP层和FQ损失层(DCNNH)的方法、在本文方法基础上未加入SPP层(SHN-SPP)以及在本文方法基础上未加入FQ损失层(SHN-FQ)的方法做了对比实验。由于本文方法对返回最近邻个数具有稳定性,为更好地验证本文方法的有效性,选择最近邻个数为100。对比实验结果如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="119">由表3可知:比较DCNNH和SHN-FQ,池化策略为金字塔池化的方法比池化策略为均值池化的方法的<i>mAP</i>值平均提升了1.8个百分点;比较DCNNH和SHN-SPP,损失层使用FQ损失函数的方法比使用Softmax损失函数的方法的<i>mAP</i>值平均提升了1.5个百分点;比较SHN与其它3种方法,SHN的<i>mAP</i>值最高。综合上述可知,本文方法能够有效地提高商品图像检索的<i>mAP</i>值,可以更好地完成商品图像检索任务。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表3 按汉明距离排序的</b><i><b>mAP</b></i><b>值(返回最近邻个数为100</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 </b><i><b>mAP</b></i><b> sorted by Hamming distance(</b><i><b>top</b></i><b>_</b><i><b>k</b></i><b>=100</b>)</p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="4"><br />哈希码位数</td></tr><tr><td><br />48位</td><td>64位</td><td>96位</td><td>128位</td></tr><tr><td><br />DCNNH</td><td>0.879 6</td><td>0.880 6</td><td>0.889 4</td><td>0.88 97</td></tr><tr><td><br />SHN-FQ</td><td>0.906 9</td><td>0.896 8</td><td>0.903 4</td><td>0.905 8</td></tr><tr><td><br />SHN-SPP</td><td>0.886 0</td><td>0.901 6</td><td>0.902 8</td><td>0.889 5</td></tr><tr><td><br />SHN</td><td><b>0.907 6</b></td><td><b>0.904 3</b></td><td><b>0.912 4</b></td><td><b>0.908 3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>3.4 检索结果</b></h4>
                <div class="p1">
                    <p id="122">图5给出了本文方法在哈希码位数不同时对应的分类准确率和运行时间(返回最近邻个数为50)。可以看出,得到的哈希码位数越多,运行所需时间越长。为了找出检索效果最好的哈希码位数,本文设计了一个性能评估函数用来评估方法的性能,如式(12)所示:</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>o</mi><mi>c</mi><mi>r</mi><mi>e</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mi>p</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>×</mo><mn>0</mn><mo>.</mo><mn>7</mn><mo>+</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mn>1</mn><mn>0</mn><mo>×</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow></mfrac><mo>×</mo><mn>0</mn><mo>.</mo><mn>3</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">其中,<i>cp</i>(<i>i</i>)是位数为<i>i</i>时的分类准确率,<i>r</i><sub><i>i</i></sub>为运行时间。由式(12)计算得到的分数充分考虑到2个重要的指标,平均准确率和运行时间。最终得分与实验性能的好坏成正相关,即分数越高,性能越好。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 本文方法不同哈希码位数时检索性能比较图" src="Detail/GetImg?filename=images/JSJK201911013_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 本文方法不同哈希码位数时检索性能比较图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Performance comparison with different hash bits</p>

                </div>
                <div class="p1">
                    <p id="126">由表4可知,哈希码位数为96位时,SHN模型的分数最高。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表4 不同位数的得分情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Scores with different hash bits</b></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />位数</td><td>16</td><td>32</td><td>48</td><td>64</td><td>96</td><td>128</td><td>256</td></tr><tr><td><br />得分</td><td>0.751</td><td>0.753</td><td>0.750</td><td>0.760</td><td>0.766</td><td>0.762</td><td>0.749</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="128">由此得到,96位的哈希码在本文的商品图像数据集中表现出的检索性能是最好的。随后的检索实验以哈希码位数为96进行。</p>
                </div>
                <div class="p1">
                    <p id="129">从检索验证集随机抽取了一些商品进行检索。图6展示了其中的部分检索结果,图中带有黑框标记的图像为检索到的同款商品。从图6中可知,本文方法能够准确地检索到同类别商品,但难以保证检索到同款商品。特别地,书本类商品图像取得了较好的检索效果,服装类、鞋类商品图像由于背景繁杂、款式众多等问题只能检索到相似商品。本文方法能够为用户检索到相似的商品,但在细粒度检索上还需要进一步学习。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911013_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 检索结果" src="Detail/GetImg?filename=images/JSJK201911013_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 检索结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911013_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Search results</p>

                </div>
                <h3 id="131" name="131" class="anchor-tag"><b>4 结束语</b></h3>
                <div class="p1">
                    <p id="132">本文提出了一个基于深度卷积神经网络的图像检索方法,适用于大规模的商品图像检索。针对商品图像的特性,结合金字塔池化策略与哈希方法设计了SHN模型,用于进行图像特征提取。SHN模型在深度卷积神经网络结构的基础上加入了金字塔池化层、哈希层和FQ损失层。本文方法能够实现快速、准确的商品图像检索。与其他现有方法相比,表现出了更好的检索性能。在实验过程中发现,本文方法能够检索出相似的商品,但难以保证检索出同款商品。本文的后续工作将着重围绕如何稳定检索出同款商品展开。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="169" type="formula" href="images/JSJK201911013_16900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">贺周雨</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="161" type="formula" href="images/JSJK201911013_16100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">冯旭鹏</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="170" type="formula" href="images/JSJK201911013_17000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘利军</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="171" type="formula" href="images/JSJK201911013_17100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">黄青松</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="5">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Where to buy it:Matching street clothing photos in online shops">

                                <b>[1]</b> Kiapour M H,Han X,Lazebnik S,et al.Where to buy it:Matching street clothing photos in online shops[C]//Proc of 2015 IEEE International Conference on Computer Vision(ICCV),2015:3343-3351.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale orderless pooling of deep convolutional activation features">

                                <b>[2]</b> Gong Y,Wang L,Guo R,et al.Multi-scale orderless pooling of deep convolutional activation features[C]//Proc of European Conference on Computer Vision,2014:392-407.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjgyMjdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZMM05KRjg9Tmo3&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830679&amp;v=MzA4NTU0SHRIT3A0eEZZdXdHWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ0RsVkwzTkpGOD1OajdCYXJP&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Oliva A,Torralba A.Modeling the shape of the scene:A holistic representation of the spatial envelope[J].International Journal of Computer Vision,2001,42(3):145-175.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fisher Kernels on Visual Vocabularies for Image Categorization">

                                <b>[5]</b> Perronnin F,Dance C.Fisher kernels on visual vocabularies for image categorization[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition,2007:1-8.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locality-constrained linear coding for image classification">

                                <b>[6]</b> Wang J,Yang J,Kai Y,et al.Locality-constrained linear coding for image classification[C]//Proc of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2010:3360-3367.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300297830&amp;v=MDk3MTRmSUoxd1JiaE09TmlmT2ZiSzdIdEROckk5Rlp1SUlCSDg1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Pramod K P,Vadakkepat P,Poh L A.Fuzzy-rough discriminative feature selection and classification algorithm,with application to microarray and image datasets[J].Applied Soft Computing,2011,11(4):3429-3440.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[8]</b> Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C]//Proc of European Conference on Computer Vision(ECCV 2014),2014:818-833.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised deep feature embedding with handcrafted feature">

                                <b>[9]</b> Kan S,Cen Y,He Z,et al.Supervised deep feature embedding with handcrafted feature[J].IEEE Transactions on Image Processing,2019,28(12):5809-5823.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Xiong Chang-zhen,Shan Yan-mei,Guo Fen-hong.Image retrival method based on image principal part detection[J].Optics and Precision Engineering,2017,25(3):792-798.(in Chinese)
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Dong Rong-sheng,Cheng De-qiang,Li Feng-ying.Aggregating deep convolutional features for image retrieval using multi-regional cross weighting[J].Journal of Computer-Aided Design &amp; Computer Graphics,2018,30(4):659-665.(in Chinese)
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 Yuan Hui,Liao Kai-yang,Zheng Yuan-lin,et al.Image retrieval based on CNN feature weighting and region integration[J].Computer Engineering &amp; Science,2019,41(1):113-121.(in Chinese)
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Lin T Y,Goyal P,Girshick R,et al.Focal loss for dense object detection[C]//Proc of 2017 IEEE International Conference on Computer Vision,2017:2999-3007.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous feature learning and hash coding with deep neural networks.">

                                <b>[14]</b> Lai H J,Pan Y,Liu Y,et al.Simultaneous feature learning and hash coding with deep neural networks[C]//Proc of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),2015:3270-3278.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep hashing for scalable image search">

                                <b>[15]</b> Lu J,Liong V E,Zhou J.Deep hashing for scalable image search[J].IEEE Transactions on Image Processing,2017,26(5):2352-2369.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for contentbased image retrieval:A comprehensive study">

                                <b>[16]</b> Wan J,Wang D,Hoi C H,et al.Deep learning for content-based image retrieval:A comprehensive study[C]//Proc of ACM International Conference on Multimedia,2014:303-313.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised hashing with kernels">

                                <b>[17]</b> Liu W,Wang J,Ji R,et al.Supervised hashing with kernels[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition,2012:2074-2081.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Iterative Quantization:A Procrustean Approach to Learning Binary Codes for Large-Scale Image Retrieval">

                                <b>[18]</b> Gong Y,Lazebnik S,Gordo A,et al.Iterative quantization:A procrustean approach to learning binary codes for large-scale image retrieval[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(12):2916-2929.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Supervised Hashing for Fast Image Retrieval&amp;quot;">

                                <b>[19]</b> Liu H,Wang R,Shan S,et al.Deep supervised hashing for fast image retrieval[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:2064-2072.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning of Binary Hash Codes for Fast Image Retrieval">

                                <b>[20]</b> Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//Proc of 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2015:27-35.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Supervised Discrete Hashing">

                                <b>[21]</b> Li Q,Sun Z,He R,et al.Deep supervised discrete hashing[C]//Proc of Advances in Neural Information Processing Systems(NIPS 2017),2017:1.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 Shen F,Xu Y,Liu L,et al.Unsupervised deep hashing with similarity-adaptive and discrete optimization[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2018,40(12):3034-3044.附中文参考文献:
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703034&amp;v=MzExMDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9uVnJ2SklqWEJZN0c0SDliTXJJOUdZSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 熊昌镇,单艳梅,郭芬红.结合主体检测的图像检索方法[J].光学精密工程,2017,25(3):792-798.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201804014&amp;v=MjcxNTZxQnRHRnJDVVJMT2VaZVJtRnkvblZydkpMejdCYUxHNEg5bk1xNDlFWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 董荣胜,程德强,李凤英.用于图像检索的多区域交叉加权聚合深度卷积特征[J].计算机辅助设计与图形学学报,2018,30(4):659-665.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201901015&amp;v=MDY3OTNvOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9uVnJ2Skx6N0JaYkc0SDlqTXI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 袁晖,廖开阳,郑元林,等.基于CNN特征加权和区域整合的图像检索[J].计算机工程与科学,2019,41(1):113-121.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201911013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911013&amp;v=MzAzMDhIOWpOcm85RVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnZKTHo3QlpiRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
