<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132348683780000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201910016%26RESULT%3d1%26SIGN%3dcFLQgrPfZ%252bKaRFIHImfMlSPE7BA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910016&amp;v=MTcyMjk5ak5yNDlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZickJMejdCWmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;2 K-medoids算法介绍&lt;/b&gt; "><b>2 K-medoids算法介绍</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;3 DWC_K-medoids算法&lt;/b&gt; "><b>3 DWC_K-medoids算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;3.1 Canopy算法介绍&lt;/b&gt;"><b>3.1 Canopy算法介绍</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;3.2 密度权重的Canopy算法&lt;/b&gt;"><b>3.2 密度权重的Canopy算法</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;3.3 DWC_K-medoids算法&lt;/b&gt;"><b>3.3 DWC_K-medoids算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="&lt;b&gt;4 仿真实验&lt;/b&gt; "><b>4 仿真实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;4.1 真实数据集&lt;/b&gt;"><b>4.1 真实数据集</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;4.2 人工数据集&lt;/b&gt;"><b>4.2 人工数据集</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 密度权重定义图解">图1 密度权重定义图解</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表1 真实数据集的描述&lt;/b&gt;"><b>表1 真实数据集的描述</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表2 3个算法在真实数据集中的聚类代价&lt;/b&gt;&lt;i&gt;&lt;b&gt;E&lt;/b&gt;&lt;/i&gt;"><b>表2 3个算法在真实数据集中的聚类代价</b><i><b>E</b></i></a></li>
                                                <li><a href="#113" data-title="图2 真实数据集的平均准确率">图2 真实数据集的平均准确率</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表3 人工数据集的描述&lt;/b&gt;"><b>表3 人工数据集的描述</b></a></li>
                                                <li><a href="#118" data-title="图3 人工数据集">图3 人工数据集</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表4 3个算法在4个人工数据集上的实验结果&lt;/b&gt;"><b>表4 3个算法在4个人工数据集上的实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Aguwa C,Olya M H,Monplaisir L.Modeling of fuzzy-based voice of customer for business decision analytics[J].Knowledge-Based Systems,2017,125(6):136-145." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES71530404562A5305124F669B76740FC3&amp;v=MDM5NzdYazZ6eE1TNkRzTFRubnIzaFV6ZnJhVU04bWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOQmh3NzI1eGFFPU5pZk9mYlM1RzlMTXE0OUJZZTBOZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Aguwa C,Olya M H,Monplaisir L.Modeling of fuzzy-based voice of customer for business decision analytics[J].Knowledge-Based Systems,2017,125(6):136-145.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Vinodhini G,Chandrasekaran R M.A sampling based sentiment mining approach for e-commerce applications[J].Information Processing &amp;amp; Management,2017,53(1):223-236." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF16F663089743BDDFD77458EF57F5766&amp;v=Mjk4NzFtYUJ1SFlmT0dRbGZDcGJRMzVOQmh3NzI1eGFFPU5pZk9mY1c1R0tmS3FZeEZiT0lJQ0g5THUySmxuamg2VEhycTJXUXdmc1NSUXJ5WkNPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Vinodhini G,Chandrasekaran R M.A sampling based sentiment mining approach for e-commerce applications[J].Information Processing &amp;amp; Management,2017,53(1):223-236.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Mcloughlin F,Duffy A,Conlon M.A clustering approach to domestic electricity load profile characterization using smart metering data[J].Applied Energy,2015,141:190-199." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600202871&amp;v=MjAzODV5am1VTGZJSjFzU2J4cz1OaWZPZmJLOUg5UE9xWTlGWnVzTkJIczRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Mcloughlin F,Duffy A,Conlon M.A clustering approach to domestic electricity load profile characterization using smart metering data[J].Applied Energy,2015,141:190-199.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Wang Y,Chen Q,Kang C,et.al.Clustering of electricity consumption behavior dynamics toward big data applications[J].IEEE Transactions on Smart Grid,2016,7(5):2437-2447." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering of Electricity Consumption Behavior Dynamics Toward Big Data Applications">
                                        <b>[4]</b>
                                         Wang Y,Chen Q,Kang C,et.al.Clustering of electricity consumption behavior dynamics toward big data applications[J].IEEE Transactions on Smart Grid,2016,7(5):2437-2447.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Sun Ji-gui,Liu Jie,Zhao Lian-yu.Clustering algorithms research[J].Journal of Software,2008,19(1):48-61.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200801009&amp;v=MTAxODhaZVJtRnkvZ1ZickJOeWZUYkxHNEh0bk1ybzlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Sun Ji-gui,Liu Jie,Zhao Lian-yu.Clustering algorithms research[J].Journal of Software,2008,19(1):48-61.(in Chinese)
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Park H S,Jun C H.A simple and fast algorithm for k-medoids clustering[J].Expert Systems with Applications,2009,36(2):3336-3341." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501642632&amp;v=MDI5OTVPZmJLN0h0RE5xbzlFWXU4TkNuODdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKMXNTYnhzPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Park H S,Jun C H.A simple and fast algorithm for k-medoids clustering[J].Expert Systems with Applications,2009,36(2):3336-3341.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     Yu Dong-hua,Guo Mao-zu,Liu Yang,et al.K-medoids clustering algorithm based on distance inequality[J].Journal of Software,2017,28(12):3115-3128.</a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Zhao Xiang-min,Chen Xi,Pan Chu.Novel K-medoids clustering algorithm based on dense region block[J].Computer Engineering and Applications,2016,52(16):85-89.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201616017&amp;v=MjU3MjNVUkxPZVplUm1GeS9nVmJyQkx6N01hYkc0SDlmTnFZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Zhao Xiang-min,Chen Xi,Pan Chu.Novel K-medoids clustering algorithm based on dense region block[J].Computer Engineering and Applications,2016,52(16):85-89.(in Chinese)
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Lai Xiang-yang,Gong Xiu-jun,Han Lai-ming.K-medoids clustering based on genetic algorithm in MapReduce architecture[J].Computer Science,2017,44(3):23-26.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=K-medoids clustering based on genetic algorithm in MapReduce architecture">
                                        <b>[9]</b>
                                         Lai Xiang-yang,Gong Xiu-jun,Han Lai-ming.K-medoids clustering based on genetic algorithm in MapReduce architecture[J].Computer Science,2017,44(3):23-26.(in Chinese)
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Xie Juan-ying,Qu Ya-nan.K-medoids clustering algorithm for optimizing initial center of density peak[J].Computer Science and Exploration,2016,10(2):230-247.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=K-medoids clustering algorithm for optimizing initial center of density peak">
                                        <b>[10]</b>
                                         Xie Juan-ying,Qu Ya-nan.K-medoids clustering algorithm for optimizing initial center of density peak[J].Computer Science and Exploration,2016,10(2):230-247.(in Chinese)
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Rodriguez A,Laio A.Clustering by fast search and find of density peaks[J].Science,2014,344(6191):1492-1496." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering by Fast Search and Find of Density Peaks">
                                        <b>[11]</b>
                                         Rodriguez A,Laio A.Clustering by fast search and find of density peaks[J].Science,2014,344(6191):1492-1496.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     He Xing-xiong,Guan Jun-yi,Ye Xuan-zuo,et al.A cluster center deterministic clustering algorithm based on density and grid[J].Control and Decision,2017,32(5):913-919.(in Chinese)</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Suo Ming-liang,Zhou Ding,An Ruo-ming,et al.Neighborhood density grid clustering algorithm and its application[J].Journal of Tsinghua University (Natural Science Edi- tion),2018,58(8):732-739.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=QHXB201808007&amp;v=MTIxOTBuTXA0OUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJyQk5DWFRiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Suo Ming-liang,Zhou Ding,An Ruo-ming,et al.Neighborhood density grid clustering algorithm and its application[J].Journal of Tsinghua University (Natural Science Edi- tion),2018,58(8):732-739.(in Chinese)
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Sun Y,Zhu Q M,Chen Z X.An iterative initial-points refinement algorithm for categorical data clustering[J].Pattern Recognition,2002,23(7):875-884." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300416027&amp;v=MjU4NDBOaWZPZmJLN0h0RE9ySTlGWU9vSkRINCtvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKMXNTYnhzPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Sun Y,Zhu Q M,Chen Z X.An iterative initial-points refinement algorithm for categorical data clustering[J].Pattern Recognition,2002,23(7):875-884.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     孙吉贵,刘杰,赵连宇.聚类算法研究[J].软件学报,2008,19(1):48-61.</a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_7" title=" 余冬华,郭茂祖,刘扬,等.基于距离不等式的 K-medoids聚类算法[J].软件学报,2017,28(12):3115-3128." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201712001&amp;v=MzE1NDNVUkxPZVplUm1GeS9nVmJyQk55ZlRiTEc0SDliTnJZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         余冬华,郭茂祖,刘扬,等.基于距离不等式的 K-medoids聚类算法[J].软件学报,2017,28(12):3115-3128.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     赵湘民,陈曦,潘楚.基于稠密区域的K-medoids聚类算法[J].计算机工程与应用,2016,52(16):85-89.</a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_9" title=" 赖向阳,宫秀军,韩来明.一种MapReduce架构下基于遗传算法的 K-medoids聚类[J].计算机科学,2017,44(3):23-26." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201703006&amp;v=MjYyMDRSTE9lWmVSbUZ5L2dWYnJCTHo3QmI3RzRIOWJNckk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         赖向阳,宫秀军,韩来明.一种MapReduce架构下基于遗传算法的 K-medoids聚类[J].计算机科学,2017,44(3):23-26.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_10" title=" 谢娟英,屈亚楠.密度峰值优化初始中心的K-medoids聚类算法[J].计算机科学与探索,2016,10(2):230-247." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201602009&amp;v=MjY4NjJDVVJMT2VaZVJtRnkvZ1ZickJMalhmZmJHNEg5Zk1yWTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         谢娟英,屈亚楠.密度峰值优化初始中心的K-medoids聚类算法[J].计算机科学与探索,2016,10(2):230-247.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_12" title=" 何熊熊,管俊轶,叶宣佐,等.一种基于密度和网格的簇心可确定聚类算法[J].控制与决策,2017,32(5):913-919." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201705020&amp;v=Mjc0OTZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJyQkxqZlNiYkc0SDliTXFvOUg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         何熊熊,管俊轶,叶宣佐,等.一种基于密度和网格的簇心可确定聚类算法[J].控制与决策,2017,32(5):913-919.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     索明亮,周鼎,安若铭,等.邻域密度网格聚类算法及应用[J].清华大学学报(自然科学版),2018,58(8):732-739.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(10),1823-1828 DOI:10.3969/j.issn.1007-130X.2019.10.015            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于密度权重Canopy的改进K-medoids算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E8%83%9C%E5%8F%91&amp;code=41915964&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈胜发</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BE%E7%91%9E%E7%8E%89&amp;code=06141553&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贾瑞玉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BE%BD%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0062694&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安徽大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了提高K-medoids算法的精度和稳定性,并解决K-medoids算法的聚类数目需要人工给定和对初始聚类中心点敏感的问题,提出了基于密度权重Canopy的改进K-medoids算法。该算法首先计算数据集中每个样本点的密度值,选择密度值最大的样本点作为第1个聚类中心,并从数据集中删除这个密度簇;然后通过计算剩下样本点的权重,选择出其他聚类中心;最后将密度权重Canopy作为K-medoids的预处理过程,其结果作为K-medoids算法的聚类数目和初始聚类中心。UCI真实数据集和人工模拟数据集上的仿真实验表明,该算法具有较高的精度和较好的稳定性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%83%E9%87%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">权重;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据挖掘;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈胜发（1994-），男，安徽无为人，硕士生，研究方向为数据挖掘。E-mail:930221458@qq.com，通信地址:230601安徽省合肥市安徽大学计算机科学与技术学院&lt;image id="139" type="formula" href="images/JSJK201910016_13900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    贾瑞玉（1965-），女，安徽合肥人，硕士，副教授，研究方向为数据挖掘。E-mail:Jiaruiyu2013@126.com，通信地址:230601安徽省合肥市安徽大学计算机科学与技术学院&lt;image id="141" type="formula" href="images/JSJK201910016_14100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家科技支撑计划项目(2015BAK24B01);</span>
                    </p>
            </div>
                    <h1><b>An improved K-medoids algorithm based on density weight Canopy</b></h1>
                    <h2>
                    <span>CHEN Sheng-fa</span>
                    <span>JIA Rui-yu</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology,Anhui University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve the accuracy and stability of the K-medoids algorithm and solve the problem that the number of clusters of K-medoids algorithm needs to be manually given and is sensitive to the initial cluster center point, we propose an improved K-medoids algorithm based on density weight Canopy. Firstly, we calculate the density value of each sample point in the data set, select the sample point with maximum density value as the first cluster center and remove the density cluster from the data set. Secondly, we select other cluster centers by calculating the weight of the remaining sample points. Finally, the density weight Canopy is used as the preprocessing procedure of the K-medoids and its result is used as the cluster number and initial clustering center of the K-medoids algorithm. The new algorithm is tested on some well-known data sets from UCI real dataset and some artificial simulated data sets. Simulation results show that the new algorithm has higher clustering accuracy and better clustering stability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=density&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">density;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weight&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weight;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20mining&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data mining;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    CHEN Sheng-fa,born in 1994,MS candidate,his research interest includes data mining.Address:School of Computer Science and Technology,Anhui University,Hefei 230601,Anhui,P.R.China;
                                </span>
                                <span>
                                    JIA Rui-yu,born in 1965,MS,associate professor,her research interest includes data mining.Address:School of Computer Science and Technology,Anhui University,Hefei 230601,Anhui,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-02-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="46">聚类算法是数据挖掘中最经典的算法之一,已经有许多学者对此进行了研究,而且聚类技术在许多领域得到了广泛的应用。在商业领域,它可以用来分析顾客的行为,为商业营销策略的制定提供重要的依据<citation id="125" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在互联网电子商务领域,它可以用来分析符合用户浏览日志的相似客户的特征,从而帮助互联网商家提供更好的客户服务<citation id="126" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。此外,聚类分析在智能电网用户端大数据挖掘中有着重要的应用<citation id="127" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。通过挖掘用户用电数据中的有效信息,分析用户用电行为,进行用电量预测,对电网公司进行电力调度具有重要意义<citation id="128" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。根据聚类方法的不同,聚类算法可分为基于划分的聚类方法、基于层次的聚类方法、基于密度的聚类方法、基于网格的聚类方法和基于模型的聚类方法<citation id="129" type="reference"><link href="11" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="47">K-medoids算法是经典的划分式聚类算法,应用广泛,近年来,越来越多的学者对K-medoids算法进行研究。文献<citation id="130" type="reference">[<a class="sup">6</a>]</citation>从初始聚类中心选择和聚类中心更迭2方面对K-medoids算法进行改进,取得了非常好的聚类效果;文献<citation id="131" type="reference">[<a class="sup">7</a>,<a class="sup">7</a>]</citation>通过引入距离加速理论提高了K-medoids聚类的效率;文献<citation id="132" type="reference">[<a class="sup">8</a>,<a class="sup">8</a>]</citation>通过最大最小法则选出<i>k</i>个稠密区域,从而得到<i>k</i>个初始聚类中心点,由此提高了K-medoids的聚类质量;文献<citation id="133" type="reference">[<a class="sup">9</a>,<a class="sup">9</a>]</citation>提出了MapReduce架构下的遗传 K-medoids算法,该算法使用遗传算法解决 K-medoids算法聚类不稳定的问题,使用MapReduce框架解决大数据聚类的问题;文献<citation id="134" type="reference">[<a class="sup">10</a>,<a class="sup">10</a>]</citation>以密度峰值点作为聚类中心,自动确定聚类数目,并分配样本到距离最近且密度较高的样本所在类簇,从而实现聚类。2014年6月,Rodriguez等<citation id="135" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>发表了快速搜索和发现密度峰值的聚类算法,该算法认为具有局部极大密度且与其他密度更高点有较远距离的样本对象就是聚类中心。围绕Rodriguez等的工作,近几年来出现了许多和密度有关的改进聚类算法<citation id="136" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="48">K-medoids算法虽然克服了K-means算法对孤立点敏感的缺陷,但依然存在着聚类结果随<i>k</i>值和初始聚类中心改变而变化的问题。本文针对K-medoids算法的聚类数目难以确定和对初始聚类中心敏感的缺陷,提出了基于密度权重Canopy的改进K-medoids算法DWC_K-medoids(improved K-medoids algorithm based on Density Weight Canopy)。DWC_K-medoids算法首先计算每个样本点的密度,选择密度最大的样本点作为第1个聚类中心,然后移除属于该聚类中心的所有样本,接着通过计算剩余的每个样本点的权重,选择最大的权重作为下一个聚类中心,直到数据集为空,这样就可以自适应地确定聚类数目和初始聚类中心。基于UCI数据集和人工模拟数据集的实验测试结果表明,该算法能够较好地确定聚类数目和选择合理的初始聚类中心,并能较好地提高聚类的精度和稳定性。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>2 K-medoids算法介绍</b></h3>
                <div class="p1">
                    <p id="50">K-medoids算法是聚类分析中常用的算法之一,该算法的流程如下:</p>
                </div>
                <div class="p1">
                    <p id="51"><b>步骤1</b> 手动输入聚类数目<i>k</i>,然后随机在数据集中选择<i>k</i>个数据对象作为初始聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="52"><b>步骤2</b> 遍历数据集中的每一个数据对象,根据相似度计算,决定每个数据对象分配到哪个聚类中心所在的类簇。相似度的计算一般都是用数据对象到聚类中心的欧氏距离。欧氏距离越小,就说明相似度越大,每个数据对象都是分配到最大相似度的聚类中心所在的类簇。</p>
                </div>
                <div class="p1">
                    <p id="53"><b>步骤3</b> 在每个类簇中按照顺序依次选取数据对象,计算该数据对象到当前类簇中所有数据对象距离之和,最后选取距离之和最小的数据对象作为新的聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="54"><b>步骤4</b> 重复步骤2和步骤3,直到各个类簇的聚类中心不再改变。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag"><b>3 DWC_K-medoids算法</b></h3>
                <h4 class="anchor-tag" id="56" name="56"><b>3.1 Canopy算法介绍</b></h4>
                <div class="p1">
                    <p id="57">Canopy算法是McCallum等提出的一种无监督预聚类算法。划分式聚类算法和层次聚类算法的预处理步骤往往采用Canopy算法,Canopy算法需要设置2个距离阈值<i>T</i><sub>1</sub>和<i>T</i><sub>2</sub>,并随机选择初始聚类中心,计算样本对象与初始聚类中心之间的欧氏距离。根据阈值将样本划分到相应的类簇。最后,将聚类数据集划分为<i>k</i>个类簇。</p>
                </div>
                <div class="p1">
                    <p id="58">Canopy的算法流程如下:</p>
                </div>
                <div class="p1">
                    <p id="59"><b>步骤1</b> 给定数据集<i>D</i>={<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>n</i></sub>},然后再设定阈值<i>T</i><sub>1</sub>和<i>T</i><sub>2</sub>(<i>T</i><sub>1</sub>&gt;<i>T</i><sub>2</sub>)。</p>
                </div>
                <div class="p1">
                    <p id="60"><b>步骤2</b> 从数据集<i>D</i>中随机取一个样本点<i>S</i>,分别计算数据集中剩余样本点与点<i>S</i>之间的欧氏距离<i>d</i>。如果<i>d</i>&lt;<i>T</i><sub>1</sub>,则样本点将被添加到当前Canopy层中。</p>
                </div>
                <div class="p1">
                    <p id="61"><b>步骤3</b> 然后比较距离<i>d</i>与<i>T</i><sub>2</sub>,如果<i>d</i>&lt;<i>T</i><sub>2</sub>,则该样本点将从数据集<i>D</i>中删除,不再被添加到其他Canopy层中。</p>
                </div>
                <div class="p1">
                    <p id="62"><b>步骤4</b> 重复步骤2和步骤3,直到数据集<i>D</i>为空。</p>
                </div>
                <div class="p1">
                    <p id="63">Canopy算法很难确定阈值<i>T</i><sub>1</sub>和<i>T</i><sub>2</sub>,阈值的取值对聚类结果有很大影响。如果阈值太大,属于不同类的数据将会分组到同一个类中;如果阈值太小,属于同一个类的数据将被分成几个类。因此,本文提出了一种密度权重Canopy算法来解决这一问题。进一步利用密度权重Canopy算法得到的聚类数目和初始聚类中心作为K-medoids算法的输入参数,完成数据集聚类。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>3.2 密度权重的Canopy算法</b></h4>
                <h4 class="anchor-tag" id="65" name="65">3.2.1 相关概念</h4>
                <div class="p1">
                    <p id="66">设数据集<i>D</i>={<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>n</i></sub>}是包含<i>n</i>个样本对象的数据集合,每个样本对象含有<i>d</i>维特征属性。其中<i>x</i><sub><i>ip</i></sub>(<i>i</i>=1,2,…,<i>n</i>;<i>p</i>=1,2,…,<i>d</i>)表示第<i>i</i>个数据对象的第<i>p</i>维属性。</p>
                </div>
                <div class="p1">
                    <p id="67"><b>定义1</b> 数据对象<i><b>x</b></i><sub><i>i</i></sub>和<i><b>x</b></i><sub><i>j</i></sub>的欧氏距离为<i>d</i><sub><i>ij</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>p</mi></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中,<i>i</i>,<i>j</i>=1,2,…,<i>n</i>。</p>
                </div>
                <div class="p1">
                    <p id="70"><b>定义2</b> 数据集<i>D</i>中所有样本元素的平均距离为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>D</mi><mi>i</mi><mi>s</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>2</mn><mrow><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72"><b>定义3</b> 数据集<i>D</i>中的样本对象<i><b>x</b></i><sub><i>i</i></sub>的密度值<i>ρ</i><sub><i>i</i></sub>是以落在以样本对象<i><b>x</b></i><sub><i>i</i></sub>为中心,<i>AverDis</i>(<i>D</i>)为半径的区域中的样本对象的数量来表示的。</p>
                </div>
                <div class="p1">
                    <p id="73"><b>定义4</b> 根据定义3,<i>ρ</i><sub><i>i</i></sub>是满足其他样品对象到样本对象<i><b>x</b></i><sub><i>i</i></sub>的距离小于<i>AverDis</i>(<i>D</i>)的样本对象的数量。满足条件的样本形成一个类簇,其中样本对象间的平均距离为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>2</mn><mrow><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75"><b>定义5</b> 类簇距离<i>s</i><sub><i>i</i></sub>表示样本对象<i><b>x</b></i><sub><i>i</i></sub>和另一个样本对象<i><b>x</b></i><sub><i>j</i></sub>之间的距离。如果样本对象<i><b>x</b></i><sub><i>i</i></sub>的密度值为最大值,<i>s</i><sub><i>i</i></sub>定义为max(<i>d</i><sub><i>ij</i></sub>);如果样本对象<i><b>x</b></i><sub><i>i</i></sub>的密度值不是最大值,<i>s</i><sub><i>i</i></sub>被定义为min(<i>d</i><sub><i>ij</i></sub>),具体定义如下:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>:</mo><mi>ρ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>&gt;</mo><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mo>∃</mo><mi>j</mi><mo>,</mo><mi>ρ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>&gt;</mo><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>D</mi></mrow></munder><mo stretchy="false">(</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>,</mo><mspace width="0.25em" /><mtable columnalign="left"><mtr><mtd></mtd></mtr><mtr><mtd></mtd></mtr><mtr><mtd><mo>/</mo><mo>∃</mo></mtd></mtr><mtr><mtd></mtd></mtr></mtable><mi>j</mi><mo>,</mo><mi>ρ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>&gt;</mo><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910016_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 密度权重定义图解" src="Detail/GetImg?filename=images/JSJK201910016_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 密度权重定义图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910016_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 1 <i>Diagram of density weight definition</i></p>

                </div>
                <div class="p1">
                    <p id="78"><b>定义6</b> 数据集D被分为k个类簇。类簇C<sub>j</sub>(j≤k)的中心是<i><b>c</b></i><sub><i>j</i></sub>,则K-medoids聚类算法的聚类效果可由聚类代价<i>E</i>来确定:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">o</mi><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">o</mi><mtext> </mtext><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中,<i><b>o</b></i>是属于簇<i>C</i><sub><i>j</i></sub>里面的数据元素,<i>d</i>为欧氏距离;<i>E</i>代表所有数据元素在中心点集合<i>C</i>下聚类产生的聚类代价,其值越小,代表簇内距离越小,簇间越紧凑独立,聚类效果越好。</p>
                </div>
                <div class="p1">
                    <p id="81"><b>定义7</b> 选取聚类中心的密度权重定义(如图1)如下:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>*</mo><mfrac><mn>1</mn><mrow><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>*</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中,<i>ρ</i><sub><i>i</i></sub>的值越大表示在点<i><b>x</b></i><sub><i>i</i></sub>周围的元素越多并且元素越集中;<i>α</i><sub><i>i</i></sub>的值越小,1/<i>α</i><sub><i>i</i></sub>的值越大,一个类簇中的元素相似度越接近;<i>s</i><sub><i>i</i></sub>的值越大,2个类簇之间的相似度差异程度越大。<i>ρ</i><sub><i>i</i></sub>,<i>α</i><sub><i>i</i></sub>和<i>s</i><sub><i>i</i></sub>这3个参数以某一种形式的乘积可以反映出类簇中心所在位置的2个特征(局部密度较大和距离值较大),也就是<i>w</i>值最大的样本对象就是下1个聚类中心。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84">3.2.2 算法流程</h4>
                <div class="p1">
                    <p id="85">传统Canopy算法随机选取阈值,对聚类结果有很大影响。本文通过式(6)计算每个样本对象的权重,选取权重最大的样本对象作为聚类中心点,它可以减少随机性带来的不稳定性,提高聚类准确率。具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="86"><b>步骤1</b> 给定数据集<i>D</i>,根据定义3计算所有样本对象的密度,选择最大密度样本对象<i><b>c</b></i><sub>1</sub>作为第1个聚类中心,将聚类中心<i><b>c</b></i><sub>1</sub>添加到中心点集合<i>C</i>中,即<i>C</i>={<i><b>c</b></i><sub>1</sub>}。同时,将所有满足剩余样本对象与第1个聚类中心之间距离小于<i>AverDis</i>(<i>D</i>)的样本对象从数据集中删除。</p>
                </div>
                <div class="p1">
                    <p id="87"><b>步骤2</b> 计算余下的数据集元素的<i>ρ</i><sub><i>i</i></sub>,<i>α</i><sub><i>i</i></sub>和<i>s</i><sub><i>i</i></sub>。第2个聚类中心<i><b>c</b></i><sub>2</sub>由最大权重(定义7)决定,并且将<i><b>c</b></i><sub>2</sub>添加到中心点集合<i>C</i>中,因此<i>C</i>={<i><b>c</b></i><sub>1</sub>,<i><b>c</b></i><sub>2</sub>}。同样,将所有满足预定条件的样本从数据集中删除。</p>
                </div>
                <div class="p1">
                    <p id="88"><b>步骤3</b> 重复步骤2,直到数据集<i>D</i>为空。</p>
                </div>
                <div class="p1">
                    <p id="89">最后,数据集被划分为几个类簇。将每个类簇中的样本对象到簇内其他样本对象距离之和最小的样本对象作为聚类中心。因此,确定了最优初始聚类中心和最合适的簇数。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>3.3 DWC_K-medoids算法</b></h4>
                <div class="p1">
                    <p id="91">本文提出了密度权重Canopy的改进K-medoids算法。首先采用密度权重Canopy算法对数据集进行预聚类,得到最优聚类数目和初始聚类中心作为K-medoids算法的输入参数,然后执行K-medoids算法的流程。</p>
                </div>
                <div class="p1">
                    <p id="92">具体算法步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="93"><b>步骤1</b> 输入待聚类的数据集<i>D</i>={<i><b>x</b></i><sub>1</sub>,<i><b>x</b></i><sub>2</sub>,…,<i><b>x</b></i><sub><i>n</i></sub>};</p>
                </div>
                <div class="p1">
                    <p id="94"><b>步骤2</b> 根据定义3计算每个数据对象的密度,选取最大密度的数据对象作为第1个聚类中心;</p>
                </div>
                <div class="p1">
                    <p id="95"><b>步骤3</b> 计算剩余数据对象的<i>ρ</i><sub><i>i</i></sub>,<i>α</i><sub><i>i</i></sub>和<i>s</i><sub><i>i</i></sub>;</p>
                </div>
                <div class="p1">
                    <p id="96"><b>步骤4</b> 根据定义7计算每个数据对象的<i>w</i><sub><i>i</i></sub>,选取具有最大<i>w</i><sub><i>i</i></sub>的数据对象作为第2个聚类中心;</p>
                </div>
                <div class="p1">
                    <p id="97"><b>步骤5</b> 重复步骤3和步骤4,直到数据集为空;</p>
                </div>
                <div class="p1">
                    <p id="98"><b>步骤6</b> 计算得到的每个类簇中的数据对象到簇内其他数据对象距离之和最小的数据对象作为聚类中心;</p>
                </div>
                <div class="p1">
                    <p id="99"><b>步骤7</b> 以步骤6得到的数据对象为初始聚类中心和聚类数目,然后对<i>D</i>中的数据进行K-medoids聚类算法操作;</p>
                </div>
                <div class="p1">
                    <p id="100"><b>步骤8</b> 输出聚类结果。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag"><b>4 仿真实验</b></h3>
                <div class="p1">
                    <p id="102">为了验证本文算法的有效性,实验将本文算法与基于传统Canopy算法的改进K-medoids算法TC_K-medoids(Traditional Canopy algorithm improves the K-medoids algorithm)和K-medoids算法进行对比。为了更好地测试本文算法的精度和稳定性,本文选择了人工数据集和UCI的真实数据集分别进行实验。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>4.1 真实数据集</b></h4>
                <div class="p1">
                    <p id="104">实验采用UCI机器学习数据库中的Iris等4个数据集,所用数据集描述如表1所示。</p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表1 真实数据集的描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Description of real datasets</b></p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br />数据名称</td><td>样本数</td><td>属性个数</td><td>类簇数</td></tr><tr><td><br />Iris</td><td>150</td><td>4</td><td>3</td></tr><tr><td><br />Wine</td><td>178</td><td>13</td><td>3</td></tr><tr><td><br />Zoo</td><td>101</td><td>16</td><td>7</td></tr><tr><td><br />Soybean-small</td><td>47</td><td>35</td><td>4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="106">为了说明本文算法的稳定性,将3个算法分别运行50次,得到的聚类代价结果如表2所示。由表2可知,相对于K-medoids算法和TC_K-medoids算法,DWC_K-medoids算法在4个数据集中的聚类代价的最大值、最小值和平均值最小,并且更为稳定。而且K-medoids算法在4个数据集中求得的聚类代价<i>E</i>(式(5))的最大值和最小值有着很大的差距,这就说明了K-medoids算法的稳定性比较差。对比TC_K-medoids算法和DWC_K-medoids算法的实验结果可以看出,由于阈值的随机选取,TC_K-medoids算法的聚类代价最大值和最小值有一定的差距。但是,DWC_K-medoids算法的聚类代价的最大值和最小值非常固定,这是因为类簇中心都是处在局部密度较大和距离值较大的位置,而且数据点的分布也是固定的,所以选取出来的聚类中心都是固定的,从而聚类代价也是固定的。</p>
                </div>
                <div class="p1">
                    <p id="107">对于聚类准确率的判定,本文算法使用Sun等<citation id="137" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出的准确率判定方法,具体判定方法如下:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>L</mi></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">其中<i>β</i><sub><i>i</i></sub>是在第<i>i</i>个类簇中的样本数目,<i>k</i>为聚类数目,<i>L</i>是样本的总数。</p>
                </div>
                <div class="p1">
                    <p id="110">3个算法求解真实数据集的平均准确率如图2所示。由图2可见,DWC_K-medoids算法在4个数据集上的平均准确率均高于K-medoids算法和TC_K-medoids算法的,从而说明DWC_K-medoids算法聚类更加准确。原因是该算法在预处理过程中通过选取周围元素最集中、类簇中相似度非常高和类簇间相似度非常小的数据对象作为聚类中心,因此选取出的初始中心点是最优的。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表2 3个算法在真实数据集中的聚类代价</b><i><b>E</b></i> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Clustering cost </b><i><b>E</b></i><b> of the three algorithms on real datasets</b></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td><br />数据集</td><td colspan="3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>Κ</mtext><mo>-</mo><mtext>m</mtext><mtext>e</mtext><mtext>d</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mtext>s</mtext></mrow><mrow><mtext>最</mtext><mtext>大</mtext><mtext>值</mtext><mspace width="0.25em" /><mtext> </mtext><mtext>最</mtext><mtext>小</mtext><mtext>值</mtext><mtext> </mtext><mspace width="0.25em" /><mtext>平</mtext><mtext>均</mtext><mtext>值</mtext></mrow></mfrac></mrow></math></td><td colspan="3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>Τ</mtext><mtext>C</mtext><mo>_</mo><mtext>Κ</mtext><mo>-</mo><mtext>m</mtext><mtext>e</mtext><mtext>d</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mtext>s</mtext></mrow><mrow><mtext>最</mtext><mtext>大</mtext><mtext>值</mtext><mspace width="0.25em" /><mtext> </mtext><mtext>最</mtext><mtext>小</mtext><mtext>值</mtext><mtext> </mtext><mspace width="0.25em" /><mtext>平</mtext><mtext>均</mtext><mtext>值</mtext></mrow></mfrac></mrow></math></td><td colspan="3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>D</mtext><mtext>W</mtext><mtext>C</mtext><mo>_</mo><mtext>Κ</mtext><mo>-</mo><mtext>m</mtext><mtext>e</mtext><mtext>d</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext><mtext>s</mtext></mrow><mrow><mtext>最</mtext><mtext>大</mtext><mtext>值</mtext><mspace width="0.25em" /><mtext> </mtext><mtext>最</mtext><mtext>小</mtext><mtext>值</mtext><mtext> </mtext><mspace width="0.25em" /><mtext>平</mtext><mtext>均</mtext><mtext>值</mtext></mrow></mfrac></mrow></math></td></tr><tr><td><br />Iris</td><td>121.16</td><td>85.95</td><td>96.55</td><td>118.74</td><td>86.83</td><td>93.76</td><td>85.12</td><td>85.12</td><td>85.12</td></tr><tr><td><br />Wine</td><td>18 551.7</td><td>14 856.0</td><td>15 766.8</td><td>16 998.3</td><td>15 039.5</td><td>15 362.7</td><td>14 981.4</td><td>14 981.4</td><td>14 981.4</td></tr><tr><td><br />Zoo</td><td>151.58</td><td>94.69</td><td>110.17</td><td>121.53</td><td>93.45</td><td>107.71</td><td>89.51</td><td>89.51</td><td>89.51</td></tr><tr><td><br />Soybean-small</td><td>135.44</td><td>92.03</td><td>106.91</td><td>133.38</td><td>91.08</td><td>103.66</td><td>85.34</td><td>85.34</td><td>85.34</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910016_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 真实数据集的平均准确率" src="Detail/GetImg?filename=images/JSJK201910016_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 真实数据集的平均准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910016_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Average accuracy on real datasets</p>

                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>4.2 人工数据集</b></h4>
                <div class="p1">
                    <p id="115">为了验证本文算法在不同的数据分布下的聚类效果,本文设计了4个人工数据集,4个人工数据集的描述如表3所示。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表3 人工数据集的描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Description of artificial datasets</b></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />数据集</td><td>样本数</td><td>属性个数</td><td>类簇数</td></tr><tr><td><br />AD1</td><td>400</td><td>2</td><td>4</td></tr><tr><td><br />AD2</td><td>300</td><td>2</td><td>3</td></tr><tr><td><br />AD3</td><td>500</td><td>2</td><td>5</td></tr><tr><td><br />AD4</td><td>1 000</td><td>2</td><td>10 </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">人工数据集AD1(Artificial Dataset 1)是类簇间距离大,没有任何重合的数据集;人工数据集AD2(Artificial Dataset 2)是类簇间间距小,并且有重合的数据集;人工数据集AD3(Artificial Dataset 3)是大类中有小类的数据集;人工数据集AD4(Artificial Dataset4)是有多个类簇的数据集。4个人工数据集的分布情况如图3所示。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910016_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 人工数据集" src="Detail/GetImg?filename=images/JSJK201910016_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 人工数据集  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910016_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Artificial datasets</p>

                </div>
                <div class="p1">
                    <p id="119">3个聚类算法对4个人工数据集的平均聚类代价<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>E</mi><mo>¯</mo></mover></math></mathml>和平均准确率<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Τ</mi><mo>¯</mo></mover></math></mathml>见表4。从表4可以看出,对于人工数据集AD1,由于类簇间距离大,3个算法的准确率都是100%,但在平均聚类代价方面,<i>DWC</i>_<i>K</i>-<i>medoids</i>算法最小。对AD2、AD3和AD4数据集,<i>DWC</i>_<i>K</i>-<i>medoids</i>算法的平均准确率和平均聚类代价均优于<i>K</i>-<i>medoids</i>算法和<i>TC</i>_<i>K</i>-<i>medoi</i>-<i>ds</i>算法的。通过4个不同分布的人工数据集可以看出,<i>DWC</i>_<i>K</i>-<i>medoids</i>算法聚类相较于<i>K</i>-<i>medoids</i>算法在稳定性和聚类准确率上有较大提高。原因是该算法在选取初始聚类中心点时,是通过密度值、距离值和元素之间的密集度来决策的,因此算法在人工数据集实验中有较好的准确率和稳定性。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表4 3个算法在4个人工数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Experimental results of the three algorithms on four artificial datasets</b></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />数据集</td><td>聚类算法</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>E</mi><mo>¯</mo></mover></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Τ</mi><mo>¯</mo></mover></math></td></tr><tr><td rowspan="3"><br /><i>AD</i>1</td><td><br /><i>K</i>-<i>medoids</i></td><td>39.33</td><td>100</td></tr><tr><td><br /><i>TC</i>_<i>K</i>-<i>medoids</i></td><td>32.54</td><td>100</td></tr><tr><td><br /><i>DWC</i>_<i>K</i>-<i>medoids</i></td><td>28.61</td><td>100</td></tr><tr><td rowspan="3"><br /><i>AD</i>2</td><td><br /><i>K</i>-<i>medoids</i></td><td>23.76</td><td>98.46</td></tr><tr><td><br /><i>TC</i>_<i>K</i>-<i>medoids</i></td><td>22.99</td><td>98.88</td></tr><tr><td><br /><i>DWC</i>_<i>K</i>-<i>medoids</i></td><td>19.35</td><td>100</td></tr><tr><td rowspan="3"><br /><i>AD</i>3</td><td><br /><i>K</i>-<i>medoids</i></td><td>31.63</td><td>94.32</td></tr><tr><td><br /><i>TC</i>_<i>K</i>-<i>medoids</i></td><td>29.04</td><td>96.55</td></tr><tr><td><br /><i>DWC</i>_<i>K</i>-<i>medoids</i></td><td>22.46</td><td>99.12</td></tr><tr><td rowspan="3"><br /><i>AD</i>4</td><td><br /><i>K</i>-<i>medoids</i></td><td>53.87</td><td>91.36</td></tr><tr><td><br /><i>TC</i>_<i>K</i>-<i>medoids</i></td><td>51.15</td><td>91.49</td></tr><tr><td><br /><i>DWC</i>_<i>K</i>-<i>medoids</i></td><td>38.46</td><td>99.29</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="122">K-medoids算法是数据挖掘中典型的方法之一。针对传统K-medoids算法中<i>K</i>值和初始聚类中心存在的缺点,本文提出了基于密度权重Canopy的改进K-medoids算法。在改进算法中,加入了密度参数,提高了聚类的准确率。UCI数据集和人工数据集上的测试结果表明了该算法具有较好的稳定性和准确率。未来的工作将侧重于高维数据集的降维和算法的并行化,数据降维对于实现更准确、更快速的数据聚类非常重要。同时,在处理大数据集时,如何结合Hadoop并行化技术来提高算法的效率值得进一步探索。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES71530404562A5305124F669B76740FC3&amp;v=MDA3Mjh3NzI1eGFFPU5pZk9mYlM1RzlMTXE0OUJZZTBOZlhrNnp4TVM2RHNMVG5ucjNoVXpmcmFVTThtY0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Aguwa C,Olya M H,Monplaisir L.Modeling of fuzzy-based voice of customer for business decision analytics[J].Knowledge-Based Systems,2017,125(6):136-145.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF16F663089743BDDFD77458EF57F5766&amp;v=MjY1MjVqaDZUSHJxMldRd2ZzU1JRcnlaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJodzcyNXhhRT1OaWZPZmNXNUdLZktxWXhGYk9JSUNIOUx1Mkpsbg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Vinodhini G,Chandrasekaran R M.A sampling based sentiment mining approach for e-commerce applications[J].Information Processing &amp; Management,2017,53(1):223-236.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122600202871&amp;v=MjQ4MzJxWTlGWnVzTkJIczRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKMXNTYnhzPU5pZk9mYks5SDlQTw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Mcloughlin F,Duffy A,Conlon M.A clustering approach to domestic electricity load profile characterization using smart metering data[J].Applied Energy,2015,141:190-199.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering of Electricity Consumption Behavior Dynamics Toward Big Data Applications">

                                <b>[4]</b> Wang Y,Chen Q,Kang C,et.al.Clustering of electricity consumption behavior dynamics toward big data applications[J].IEEE Transactions on Smart Grid,2016,7(5):2437-2447.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB200801009&amp;v=MTcxNDRSTE9lWmVSbUZ5L2dWYnJCTnlmVGJMRzRIdG5Ncm85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Sun Ji-gui,Liu Jie,Zhao Lian-yu.Clustering algorithms research[J].Journal of Software,2008,19(1):48-61.(in Chinese)
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501642632&amp;v=MDg5ODdOcW85RVl1OE5Dbjg3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSjFzU2J4cz1OaWZPZmJLN0h0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Park H S,Jun C H.A simple and fast algorithm for k-medoids clustering[J].Expert Systems with Applications,2009,36(2):3336-3341.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 Yu Dong-hua,Guo Mao-zu,Liu Yang,et al.K-medoids clustering algorithm based on distance inequality[J].Journal of Software,2017,28(12):3115-3128.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201616017&amp;v=MjQzODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJyQkx6N01hYkc0SDlmTnFZOUVZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Zhao Xiang-min,Chen Xi,Pan Chu.Novel K-medoids clustering algorithm based on dense region block[J].Computer Engineering and Applications,2016,52(16):85-89.(in Chinese)
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=K-medoids clustering based on genetic algorithm in MapReduce architecture">

                                <b>[9]</b> Lai Xiang-yang,Gong Xiu-jun,Han Lai-ming.K-medoids clustering based on genetic algorithm in MapReduce architecture[J].Computer Science,2017,44(3):23-26.(in Chinese)
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=K-medoids clustering algorithm for optimizing initial center of density peak">

                                <b>[10]</b> Xie Juan-ying,Qu Ya-nan.K-medoids clustering algorithm for optimizing initial center of density peak[J].Computer Science and Exploration,2016,10(2):230-247.(in Chinese)
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering by Fast Search and Find of Density Peaks">

                                <b>[11]</b> Rodriguez A,Laio A.Clustering by fast search and find of density peaks[J].Science,2014,344(6191):1492-1496.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 He Xing-xiong,Guan Jun-yi,Ye Xuan-zuo,et al.A cluster center deterministic clustering algorithm based on density and grid[J].Control and Decision,2017,32(5):913-919.(in Chinese)
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=QHXB201808007&amp;v=MDIzMzVnVmJyQk5DWFRiTEc0SDluTXA0OUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Suo Ming-liang,Zhou Ding,An Ruo-ming,et al.Neighborhood density grid clustering algorithm and its application[J].Journal of Tsinghua University (Natural Science Edi- tion),2018,58(8):732-739.(in Chinese)
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300416027&amp;v=MjE3MDhZT29KREg0K29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUoxc1NieHM9TmlmT2ZiSzdIdERPckk5Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Sun Y,Zhu Q M,Chen Z X.An iterative initial-points refinement algorithm for categorical data clustering[J].Pattern Recognition,2002,23(7):875-884.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 孙吉贵,刘杰,赵连宇.聚类算法研究[J].软件学报,2008,19(1):48-61.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201712001&amp;v=MTA1NzFOclk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnJCTnlmVGJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 余冬华,郭茂祖,刘扬,等.基于距离不等式的 K-medoids聚类算法[J].软件学报,2017,28(12):3115-3128.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 赵湘民,陈曦,潘楚.基于稠密区域的K-medoids聚类算法[J].计算机工程与应用,2016,52(16):85-89.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201703006&amp;v=MDI3OTdCdEdGckNVUkxPZVplUm1GeS9nVmJyQkx6N0JiN0c0SDliTXJJOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 赖向阳,宫秀军,韩来明.一种MapReduce架构下基于遗传算法的 K-medoids聚类[J].计算机科学,2017,44(3):23-26.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201602009&amp;v=MzA2MTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnJCTGpYZmZiRzRIOWZNclk5RmI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 谢娟英,屈亚楠.密度峰值优化初始中心的K-medoids聚类算法[J].计算机科学与探索,2016,10(2):230-247.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201705020&amp;v=MTIzMDBMamZTYmJHNEg5Yk1xbzlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZickI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 何熊熊,管俊轶,叶宣佐,等.一种基于密度和网格的簇心可确定聚类算法[J].控制与决策,2017,32(5):913-919.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 索明亮,周鼎,安若铭,等.邻域密度网格聚类算法及应用[J].清华大学学报(自然科学版),2018,58(8):732-739.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201910016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910016&amp;v=MTcyMjk5ak5yNDlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZickJMejdCWmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
