<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132366120655000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201908023%26RESULT%3d1%26SIGN%3d7JZlLsIEYMqhRmoU20sXGFyMPoU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908023&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908023&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908023&amp;v=MDk5ODdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bFZMdkxMejdCWmJHNEg5ak1wNDlIWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#44" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;2 基于短语的双语LDA模型&lt;/b&gt; "><b>2 基于短语的双语LDA模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="&lt;b&gt;2.1 基于短语的主题模型&lt;/b&gt;"><b>2.1 基于短语的主题模型</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;2.2 基于短语的双语LDA模型&lt;/b&gt;"><b>2.2 基于短语的双语LDA模型</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;2.3 基于吉布斯采样的参数估计&lt;/b&gt;"><b>2.3 基于吉布斯采样的参数估计</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;2.4 吉布斯采样算法&lt;/b&gt;&lt;b&gt;算法1&lt;/b&gt; 吉布斯采样算法"><b>2.4 吉布斯采样算法</b><b>算法1</b> 吉布斯采样算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#205" data-title="&lt;b&gt;3 实验&lt;/b&gt; "><b>3 实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#206" data-title="&lt;b&gt;3.1 实验语料&lt;/b&gt;"><b>3.1 实验语料</b></a></li>
                                                <li><a href="#208" data-title="&lt;b&gt;3.2 实验设计和结果分析&lt;/b&gt;"><b>3.2 实验设计和结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#221" data-title="&lt;b&gt;4 结束语&lt;/b&gt; "><b>4 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="图1 基于短语的双语LDA模型图">图1 基于短语的双语LDA模型图</a></li>
                                                <li><a href="#218" data-title="图2 中文文档的困惑度">图2 中文文档的困惑度</a></li>
                                                <li><a href="#219" data-title="图3 柬语文档的困惑度">图3 柬语文档的困惑度</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="306">


                                    <a id="bibliography_1" title=" Hofmann T.Probabilistic latent semantic indexing[C]∥Proc of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999:50-57." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic latent semantic indexing">
                                        <b>[1]</b>
                                         Hofmann T.Probabilistic latent semantic indexing[C]∥Proc of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999:50-57.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_2" title=" Blei D M, Ng A Y, Jordan M I.Latent Dirichlet allocation[J].The Journal of Machine Learning Research, 2012, 3:993-1022." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent Dirichlet allocation">
                                        <b>[2]</b>
                                         Blei D M, Ng A Y, Jordan M I.Latent Dirichlet allocation[J].The Journal of Machine Learning Research, 2012, 3:993-1022.
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_3" title=" de Smet W, Moens M F.Cross-language linking of news stories on the web using interlingual topic modelling[C]∥Proc of the 2nd ACM Workshop on Social Web Search and Mining, 2009:57-64." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-language linking of news stories on the web using interlingual topic modelling">
                                        <b>[3]</b>
                                         de Smet W, Moens M F.Cross-language linking of news stories on the web using interlingual topic modelling[C]∥Proc of the 2nd ACM Workshop on Social Web Search and Mining, 2009:57-64.
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_4" title=" de Smet W, Tang J, Moens M F.Knowledge transfer across multilingual corpora via latent topics[C]∥Proc of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2011:549-560." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Knowledge transfer across multilingual corpora via latent topics">
                                        <b>[4]</b>
                                         de Smet W, Tang J, Moens M F.Knowledge transfer across multilingual corpora via latent topics[C]∥Proc of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2011:549-560.
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_5" title=" Ni X C, Sun J T, Hu J, et al.Cross lingual text classification by mining multilingual topics from wikipedia[C]∥Proc of the4th ACM International Conference on Web Search and Web Data Mining, 2011:375-384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross lingual text classification by mining multilingual topics from wikipedia">
                                        <b>[5]</b>
                                         Ni X C, Sun J T, Hu J, et al.Cross lingual text classification by mining multilingual topics from wikipedia[C]∥Proc of the4th ACM International Conference on Web Search and Web Data Mining, 2011:375-384.
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_6" title=" Vuli&lt;image id=&quot;368&quot; type=&quot;formula&quot; href=&quot;images/JSJK201908023_36800.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt; I, de Smet W, Moens M F.Cross-language information retrieval with latent topic models trained on a comparable corpus[C]∥Proc of the 7th Asia Information Retrieval Societies Conference, 2011:37-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-language information retrieval with latent topic models trained on a comparable corpus">
                                        <b>[6]</b>
                                         Vuli&lt;image id=&quot;368&quot; type=&quot;formula&quot; href=&quot;images/JSJK201908023_36800.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt; I, de Smet W, Moens M F.Cross-language information retrieval with latent topic models trained on a comparable corpus[C]∥Proc of the 7th Asia Information Retrieval Societies Conference, 2011:37-48.
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_7" title=" Vuli&lt;image id=&quot;369&quot; type=&quot;formula&quot; href=&quot;images/JSJK201908023_36900.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt; I, Smet W, Moens M-F.Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora[J].Information Retrieval, 2013, 16 (3) :331-368." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130523453186&amp;v=MDUwODZkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2dFU3bklKMW9YTmo3QmFySzdIdFRPckl0QVorb0hDaE04enhVU21E&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Vuli&lt;image id=&quot;369&quot; type=&quot;formula&quot; href=&quot;images/JSJK201908023_36900.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt; I, Smet W, Moens M-F.Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora[J].Information Retrieval, 2013, 16 (3) :331-368.
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_8" title=" Vuli&lt;image id=&quot;370&quot; type=&quot;formula&quot; href=&quot;images/JSJK201908023_37000.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt; I, Moens M F.Detecting highly confident word translations from comparable corpora without any prior knowledge[C]∥Proc of the 13th Conference of the European Chapter of the Association for Computational Linguistics, 2012:449-459." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Detecting highly confident word translations from comparable corpora without any prior knowledge,&amp;quot;">
                                        <b>[8]</b>
                                         Vuli&lt;image id=&quot;370&quot; type=&quot;formula&quot; href=&quot;images/JSJK201908023_37000.jpg&quot; display=&quot;inline&quot; placement=&quot;inline&quot;&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt; I, Moens M F.Detecting highly confident word translations from comparable corpora without any prior knowledge[C]∥Proc of the 13th Conference of the European Chapter of the Association for Computational Linguistics, 2012:449-459.
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_9" title=" Griffiths T L, Steyvers M, Blei D M, et al.Integrating topics and syntax[C]∥Proc of the 17th International Conference on Neural Information Processing Systems, 2004:537-544." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Integrating Topics and Syntax">
                                        <b>[9]</b>
                                         Griffiths T L, Steyvers M, Blei D M, et al.Integrating topics and syntax[C]∥Proc of the 17th International Conference on Neural Information Processing Systems, 2004:537-544.
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_10" title=" Wallach H M.Topic modeling:Beyond bag-of-words[C]∥Proc of the 23rd International Conference on Machine Learning, 2006:977-984." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topic Modeling:Beyond Bag-of-Words">
                                        <b>[10]</b>
                                         Wallach H M.Topic modeling:Beyond bag-of-words[C]∥Proc of the 23rd International Conference on Machine Learning, 2006:977-984.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_11" title=" Wang X R, McCallum A.A note on topical n-grams:Technical Report UM-CS-2005-071[R].America:Department of Computer Science University of Massachusetts Amherst, 2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A note on topical N-gram">
                                        <b>[11]</b>
                                         Wang X R, McCallum A.A note on topical n-grams:Technical Report UM-CS-2005-071[R].America:Department of Computer Science University of Massachusetts Amherst, 2005.
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_12" title=" Wang X R, McCallum A, Wei X.Topical N-grams:Phrase and topic discovery, with an application to information retrieval[C]∥Proc of the 7th IEEE International Conference on Data Mining (ICDM) , 2007:697-702." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topical N-grams:Phrase andTopic Discovery,with an Application to Information Retrieval">
                                        <b>[12]</b>
                                         Wang X R, McCallum A, Wei X.Topical N-grams:Phrase and topic discovery, with an application to information retrieval[C]∥Proc of the 7th IEEE International Conference on Data Mining (ICDM) , 2007:697-702.
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_13" title=" Gruber A, Weiss Y, Rosen-Zvi M.Hidden topic Markov model[C]∥Proc of the 10th International Conference on Artificial Intelligence and Statistics, 2007:163-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hidden topic Markov Models">
                                        <b>[13]</b>
                                         Gruber A, Weiss Y, Rosen-Zvi M.Hidden topic Markov model[C]∥Proc of the 10th International Conference on Artificial Intelligence and Statistics, 2007:163-170.
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_14" title=" Boyd-Graber J L, Blei D M.Syntactic topic models[C]∥Proc of the 21st International Conference on Neural Information Processing Systems, 2008:185-192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Syntactic Topic Models">
                                        <b>[14]</b>
                                         Boyd-Graber J L, Blei D M.Syntactic topic models[C]∥Proc of the 21st International Conference on Neural Information Processing Systems, 2008:185-192.
                                    </a>
                                </li>
                                <li id="334">


                                    <a id="bibliography_15" title=" Ni X V, Sun J T, Hu J, et al.Mining multilingual topics from wikipedia[C]∥Proc of the 18th International Conference on World Wide Web, 2009:1155-1156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mining multilingual topics from wikipedia">
                                        <b>[15]</b>
                                         Ni X V, Sun J T, Hu J, et al.Mining multilingual topics from wikipedia[C]∥Proc of the 18th International Conference on World Wide Web, 2009:1155-1156.
                                    </a>
                                </li>
                                <li id="336">


                                    <a id="bibliography_16" title=" Zhu Z D, Li M, Chen L, et al.Building comparable corpora based on bilingual LDA model[C]∥Proc of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers) , 2013:278-282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Building Comparable Corpora Based on Bilingual LDA Model">
                                        <b>[16]</b>
                                         Zhu Z D, Li M, Chen L, et al.Building comparable corpora based on bilingual LDA model[C]∥Proc of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers) , 2013:278-282.
                                    </a>
                                </li>
                                <li id="338">


                                    <a id="bibliography_17" title=" Mimno D, Wallach H M, Naradowsky J, et al.Polylingual topic models[C]∥Proc of the 2009Conference on Empirical Methods in Natural Language Processing, 2009:880-889." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Polylingual topic models">
                                        <b>[17]</b>
                                         Mimno D, Wallach H M, Naradowsky J, et al.Polylingual topic models[C]∥Proc of the 2009Conference on Empirical Methods in Natural Language Processing, 2009:880-889.
                                    </a>
                                </li>
                                <li id="340">


                                    <a id="bibliography_18" title=" Boyd-Graber J, Blei D M.Multilingual topic models for unaligned text[C]∥Proc of the 25th Conference on Uncertainty in Artificial Intelligence, 2009:75-82." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multilingual topic models for unaligned text">
                                        <b>[18]</b>
                                         Boyd-Graber J, Blei D M.Multilingual topic models for unaligned text[C]∥Proc of the 25th Conference on Uncertainty in Artificial Intelligence, 2009:75-82.
                                    </a>
                                </li>
                                <li id="342">


                                    <a id="bibliography_19" title=" Jagarlamudi J, Daum&#233;H.Extracting multilingual topics from unaligned comparable corpora[C]∥Proc of the 32nd European Conference on Advances in Information Retrieval, 2010:444-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Extracting multilingual topics from unaligned comparable corpora &amp;quot;">
                                        <b>[19]</b>
                                         Jagarlamudi J, Daum&#233;H.Extracting multilingual topics from unaligned comparable corpora[C]∥Proc of the 32nd European Conference on Advances in Information Retrieval, 2010:444-456.
                                    </a>
                                </li>
                                <li id="344">


                                    <a id="bibliography_20" title=" Liu Xiao-hui.Research on the construction method of Chinese-Khmer Bilingual comparable corpus[D].Kunming:Kunming University of Science and Technology, 2016. (in Chinese) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on the construction method of Chinese-Khmer Bilingual comparable corpus">
                                        <b>[20]</b>
                                         Liu Xiao-hui.Research on the construction method of Chinese-Khmer Bilingual comparable corpus[D].Kunming:Kunming University of Science and Technology, 2016. (in Chinese) 
                                    </a>
                                </li>
                                <li id="346">


                                    <a id="bibliography_20" title="刘小惠.汉柬双语可比语料库构建方法研究[D].昆明:昆明理工大学, 2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016231300.nh&amp;v=MDk5NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWTHZMVkYyNkdMRzdIOUxNcjVFYlA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                        刘小惠.汉柬双语可比语料库构建方法研究[D].昆明:昆明理工大学, 2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(08),1497-1503 DOI:10.3969/j.issn.1007-130X.2019.08.022            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于短语的柬汉双语LDA主题模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E5%BA%86&amp;code=42704884&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢庆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E9%A6%A8&amp;code=07900010&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严馨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AF%BA%E5%AE%87&amp;code=41256666&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">诺宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%B9%BF%E4%B9%89&amp;code=13407495&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐广义</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E6%9E%AB&amp;code=08529670&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周枫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%89%91%E6%AF%85&amp;code=07895859&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭剑毅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0242668&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学信息工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BA%91%E5%8D%97%E5%8D%97%E5%A4%A9%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E4%BA%A7%E4%B8%9A%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0076304&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">云南南天电子信息产业股份有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了有效地获取双语文档的主题分布, 提出了一种基于短语的柬汉双语LDA主题模型。修改了传统LDA主题模型中的词袋模型, 融入短语 (N-gram) 的概念, 能够在主题预测过程中考虑文章的词序以及上下文, 并将之应用于可比语料的双语环境中。本模型基于一个3层贝叶斯网络模型, 在此框架下, 首先搜集中文和柬埔寨语的可比语料, 每一对双语可比语料文档共享一个相同的主题分布, 之后引入发现主题以及主题短语的主题模型:对每个单词, 首先进行主题抽样, 然后将其状态作为短语进行采样, 最后对来自特定主题短语分布的单词进行采样。通过实验结果可知, 基于短语的双语LDA主题模型比一般的双语LDA模型更能抓住文章的主题, 且有更好的主题预测能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9F%AC%E6%B1%89%E5%8F%8C%E8%AF%AD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">柬汉双语;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9F%AD%E8%AF%AD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">短语;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主题模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *严馨 (kg_yanxin@sina.com) 通信地址:650504云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                                <span>
                                    谢庆 (1993-) , 男, 江苏南京人, 硕士生, 研究方向为自然语言处理。E-mail:2682139055@qq.com通信地址:650504云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61462055, 61562049);</span>
                    </p>
            </div>
                    <h1><b>A phrase-based Khmer-Chinese bilingual LDA topic model construction method</b></h1>
                    <h2>
                    <span>XIE Qing</span>
                    <span>YAN Xin</span>
                    <span>NUO Yu</span>
                    <span>XU Guang-yi</span>
                    <span>ZHOU Feng</span>
                    <span>GUO Jian-yi</span>
            </h2>
                    <h2>
                    <span>Faculty of Information Engineering and Automation, Kunming University of Science and Technology</span>
                    <span>Yunnan Nantian Electronics Information Co.Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to obtain the topic distribution of bilingual documents effectively, we propose a phrase-based Khmer-Chinese bilingual LDA topic model. We modify the bag-of-word model in the traditional LDA topic model and incorporate the concept of phrase (N-gram) . The method considers the word order and context of the article in the topic prediction process and applies it to the bilingual environment of comparable corpus. It is based on a three-layer Bayesian network model. Under this framework, we firstly collect comparable Chinese and Khmer corpus, and each pair of bilingual comparable corpus shares a common topic distribution. And then we introduce the topic model of discovery topic and topic phrase: the topic of each word is firstly sampled; then its status is sampled as a phrase; and finally words from a particular topic phrase distribution are sampled. Experimental results show that the phrase-based bilingual LDA topic model is more capable of grasping the topic of the article than general bilingual LDA models and has better topic prediction ability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Khmer-Chinese%20bilingual&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Khmer-Chinese bilingual;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=phrase&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">phrase;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=topic%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">topic model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XIE Qing, born in 1993, MS candidate, his research interest includes natural language processing.Address:Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650504, Yunan, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-03</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="44" name="44" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="45">概率主题模型如PLSA (Probabilistic Latent Semantic Analysis) <citation id="348" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、LDA (Latent Dirichlet Allocation) <citation id="349" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>以及许多基于这些模型的变体都能学习出一个生成模型, 提供一个无监督的框架, 用来表示文档主题的潜在语义。多语概率主题模型MPTM (Multilingual Probabilistic Topic Models) 是一种独立于语言的产生式机器学习模型, 是一种高层次的文档表示方法, 可以对双语内容进行比对和处理, 主要应用在跨语言新闻聚类<citation id="350" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、跨语言文档分类<citation id="352" type="reference"><link href="312" rel="bibliography" /><link href="314" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>、跨语言信息检索<citation id="353" type="reference"><link href="316" rel="bibliography" /><link href="318" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>和跨语言的词语语义相似度<citation id="351" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等领域。</p>
                </div>
                <div class="p1">
                    <p id="46">概率主题模型基于词袋模型, 但是, 词袋模型并没有融入上下文信息, 而短语模型融入了上下文信息。短语作为一个整体, 可携带的信息比单个词信息的总和要多, 因此确定单词的集合主题要比确定单词本身的主题更为关键, 融入短语使主题模型融入了上下文信息, 可以深入挖掘文档的主题信息。引入上下文的方法有很多种, Griffiths等人<citation id="354" type="reference"><link href="322" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>通过使用HMM (Hidden Markov Models) 和LDA分别捕捉语料的句法结构信息和语义信息, 将两者结合在一起提出了HMM-LDA模型。该模型将代词主题、介词主题等归类为功能主题, 把具有具体语义的名词和动词等归类为概念主题, 不但能把主题分类, 还可以计算出主题之间的转移概率。同样很多学者研究发现短语能有效提高主题预测性能, 如Wallach<citation id="355" type="reference"><link href="324" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>认为, 一个单词除了与它的主题有关外, 还与上一个词项有关, BTM (Bigram Topic Model) 加入了单词二元组结构。Wang等人<citation id="358" type="reference"><link href="326" rel="bibliography" /><link href="328" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>认为2个相邻单词之间是否能形成组合, 不仅和上一个单词有关而且还与上一个单词的主题有关, 因此提出了有词语搭配特性的TNG (Topical N-gram Model) 模型。Gruber等人<citation id="356" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一个以句子为单位分配主题的马尔科夫模型, 当句子切换时, 重新按照二项分布选择句子。Boyd-Graber等人<citation id="357" type="reference"><link href="332" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>对每句话都进行句法分析, 提出了一种非参数的贝叶斯模型, 发现主题选择不仅与当前单词有关, 还与这个单词在句法树中的父节点有关。</p>
                </div>
                <div class="p1">
                    <p id="47">在跨语言主题模型方面, Ni等人<citation id="359" type="reference"><link href="334" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了一种跨语言的ML-LDA (MultiLingual Latent Dirichlet Allocation) 模型, 使得不同语言的文档在同一向量空间中可以采用统一的主题表示, 共享主题空间, 适用于跨语言的网络。Zhu等人<citation id="360" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了一种双语LDA主题模型, 选取的语料是描述同一个话题或者事件的可比语料, 此模型构建的依据是可比语料中的对齐文档共享同一个主题分布的特性。Mimno等人<citation id="361" type="reference"><link href="338" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出的PLTM (PolyLingual Topic Model) 模型, 用于为多元文档元组建模, 元组中的文档在主题上共享相同的特定分布。这不同于LDA, 其中每个文档都被假定具有其自己的文档特定的主题分布。以上3种方法使用的语料都是文档级对齐级别的, 而可比语料的获取比较困难。对于没有多语对齐文档的情况, Boyd-Graber等人<citation id="362" type="reference"><link href="340" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>改进了词袋模型, 使用词典生成源语言和目标语言的词语匹配组合, 为了消歧, 每个词都只有一种语义。Jagarlanmudi等人<citation id="363" type="reference"><link href="342" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>使用双语词典得到不同语言混合的主题概率分布, 构建了Joint-LDA主题模型, 并在跨语言检索上取得了不错的效果。</p>
                </div>
                <div class="p1">
                    <p id="48">短语构建方法和双语LDA主题模型结合, 能够根据上下文信息自动检测文中单词序列是否是一个短语, 并能为其分配更为准确的主题, 能够适当地形成短语是基于短语的双语LDA主题模型的独特功能, 区别于传统配置, 其在双语主题文档预测方面能够取得更好的效果。通过实验结果可知, 基于短语的双语LDA主题模型比一般的双语LDA模型更能抓住文章的主题, 且有更好的主题预测能力。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>2 基于短语的双语LDA模型</b></h3>
                <h4 class="anchor-tag" id="50" name="50"><b>2.1 基于短语的主题模型</b></h4>
                <div class="p1">
                    <p id="51">在许多文本挖掘任务中, 词序和短语对于捕捉文本的语义是至关重要的。本文引入了主题短语模型 (Topical N-gram) 的概念, 这是一个发现主题以及主题短语的主题模型。主题短语模型按照文本顺序生成单词, 对每个单词, 首先进行主题抽样, 然后对短语状态进行采样, 最后对来自特定主题短语分布的单词进行采样。因此, 本文模型可以将“白宫”作为“政治”话题中的一个特殊含义的短语并识别出来, 而不能作为“房地产”主题中的特殊含义的短语被识别出来。</p>
                </div>
                <div class="p1">
                    <p id="52">Wang等人<citation id="364" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的TNG是短语模型和LDA模型的组合, 是基于短语模型的扩展模型。其生成过程如下:</p>
                </div>
                <div class="p1">
                    <p id="53"> (1) 对每个主题<i>z</i>, 由狄利克雷先验参数<i>β</i>抽取离散分布<i>φ</i><sub><i>z</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="54"> (2) 对每个主题<i>z</i>和每个单词<i>w</i>, 由贝塔分布的先验参数<i>γ</i>抽取伯努利分布<i>ψ</i><sub><i>zw</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="55"> (3) 对每个主题<i>z</i>和每个单词<i>w</i>, 由狄利克雷先验<i>δ</i>抽取出离散分布<i>σ</i><sub><i>zw</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="56"> (4) 对每个文档<i>j</i>, 由狄利克雷先验参数<i>α</i>抽取离散分布<i>θ</i><sup><i>j</i></sup>;那么对于文档<i>j</i>中的每个单词<i>w</i><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="58">① 由伯努利分布<i>ψ</i><sub><i>z</i><sup><i>j</i></sup><sub><i>i</i>-1</sub><i>w</i><sup><i>j</i></sup><sub><i>i</i>-1</sub></sub>抽取<i>x</i><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="60">② 由离散变量<i>θ</i><sup><i>j</i></sup>抽取<i>z</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="62">③ 如果<i>x</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>=1, 由离散变量<i>σ</i><sub><i>z</i><sup><i>j</i></sup><sub><i>i</i></sub><i>w</i><sup><i>j</i></sup><sub><i>i</i>-1</sub></sub>生成单词<i>w</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>;否则由离散变量<i>φ</i><sub><i>z</i><sup><i>j</i></sup><sub><i>i</i></sub></sub>生成单词<i>w</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="66">其中, <i>x</i><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>表示文档<i>j</i>中第<i>i</i>-1个单词和第<i>i</i>个单词是否组成短语的状态。<i>z</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>表示与文档<i>j</i>中第<i>i</i>个单词相关联的主题。<i>w</i><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>表示文档<i>j</i>中第<i>i</i>个单词。对于文档<i>j</i>中的第<i>i</i>个单词相关联的主题<i>z</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>和第<i>i</i>个单词<i>w</i><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>, 如果<i>x</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>=1, 则根据离散分布<i>σ</i><sub><i>z</i><sup><i>j</i></sup><sub><i>i</i></sub><i>w</i><sup><i>j</i></sup><sub><i>i</i>-1</sub></sub>抽取<i>w</i><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>;否则根据离散分布<i>z</i><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>抽取<i>w</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908023_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于短语的双语LDA模型图" src="Detail/GetImg?filename=images/JSJK201908023_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于短语的双语LDA模型图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908023_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Diagram of the phrased-based bilingual LDA model</p>

                </div>
                <div class="p1">
                    <p id="77">这个模型是一种新型短语模型, 它根据上下文自动确定单个单词和短语, 并将主题混合分配到单个单词和短语中。此模型的短语生成方法, 相比于传统的生成方法, 不仅仅考虑了词语文法上的搭配, 而且还结合了文档的上下文来考虑。融入短语 (N-gram) 的LDA模型, 可以更深入地挖掘文档的主题信息。本文提出的模型是在此基础上融入短语模型, 并将基于短语主题的模型拓展到基于双语主题的模型。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>2.2 基于短语的双语LDA模型</b></h4>
                <div class="p1">
                    <p id="79">在传统LDA模型的基础上引入的短语的概念, 与通常所认识的短语概念不同, 这里所定义的短语为服从同一主题分布的相邻词汇集合, 且该短语的长度在长短上并没有限定。基于短语的双语LDA模型可以被看成一个3层的贝叶斯网络模型, 每一对可比语料文档共享一个相同的主题分布, 基于短语的双语LDA模型的生成过程如下:</p>
                </div>
                <div class="p1">
                    <p id="80"> (1) 对每个主题<i>z</i>, 由狄利克雷先验参数<i>β</i>选择离散分布<image href="images/JSJK201908023_081.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>z</mi><mi>s</mi></msubsup></mrow></math></mathml>和<image href="images/JSJK201908023_083.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>z</mi><mi>t</mi></msubsup></mrow></math></mathml>。<image href="images/JSJK201908023_085.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>z</mi><mi>s</mi></msubsup></mrow></math></mathml>和<image href="images/JSJK201908023_087.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>z</mi><mi>t</mi></msubsup></mrow></math></mathml>分别表示源语言<i>s</i>和目标语言<i>t</i>中, 主题<i>z</i>的词项概率分布;<i>s</i>表示源语言, <i>t</i>表示目标语言, 先验参数<i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>等取值与源语言和目标语言无关, 且以下符号中上标<i>s</i>和<i>t</i>的含义与此相同。</p>
                </div>
                <div class="p1">
                    <p id="89"> (2) 对每个主题<i>z</i>和每个单词<i>w</i>, 由贝塔分布的先验参数<i>γ</i>选择伯努利分布<i>ψ</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>和<i>ψ</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="92"> (3) 对每个主题<i>z</i>和每个单词<i>w</i>, 由狄利克雷先验<i>δ</i>选择离散分布<i>σ</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>和<i>σ</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="95"> (4) <sub></sub><i>d</i>为可比文档对, <i>ds</i>为其中的源语言文档, <i>dt</i>为与<i>ds</i>对齐的目标语言文档, 由狄利克雷先验参数<i>α</i>选择离散分布<i>θ</i><sup><i>d</i></sup>来确定文档对的主题分布。<i>θ</i><sub><i>d</i></sub>表示可比文档对<i>d</i>中文档的主题分布。</p>
                </div>
                <div class="p1">
                    <p id="96"> (5) <i>x</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>和<i>x</i><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>为源语言文档<i>ds</i>和目标语言文档<i>dt</i>中第<i>i</i>个词的标志位;由先验参数<i>γ</i>选择伯努利分布<i>ψ</i><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>和<i>ψ</i><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>, 再根据伯努利分布<i>ψ</i><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>和<i>ψ</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>抽取<i>x</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>和<i>x</i><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>, 之后判断<i>w</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>和<i>w</i><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>是否为短语。<i>w</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>和<i>w</i><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>分别为源语言文档<i>ds</i>和目标语言文档<i>dt</i>中第<i>i</i>个单词:若<i>x</i><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>=1, 则表示<i>w</i><sub><i>i</i>-1</sub>和<i>w</i><sub><i>i</i></sub>构成短语, 根据离散分布<i>σ</i><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mi>s</mi></msubsup></mrow></math></mathml>生成单词<i>w</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>, 同理<i>x</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>=1时, 情况与上文相同;若<i>x</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>=0, 则表示<i>w</i><sub><i>i</i>-1</sub>和<i>w</i><sub><i>i</i></sub>不构成短语, 根据离散分布<image href="images/JSJK201908023_114.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>s</mi></msubsup></mrow></math></mathml>生成单词<i>w</i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>, 同理<i>x</i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>=0, 与上文相同。</p>
                </div>
                <div class="p1">
                    <p id="118">本文提出的基于短语的双语LDA模型有向图, 如图1所示。</p>
                </div>
                <div class="p1">
                    <p id="119">我们的目标是当给定可比语料时, 实验结果可以得出每篇文档的主题分布, 利用吉布斯抽样 (Gibbs Sampling) 来对参数<i>θ</i><sub><i>d</i></sub>进行估计, 模型可以根据相邻的上下文决定2个连续的单词是否形成一个短语, 在使用贝叶斯法则下, 得到语言<i>l</i>文档<i>dl</i>中第<i>i</i>个词的主题和标志位的概率为:<i>P</i> (<i>z</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>, <i>x</i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>, <b><i>x</i></b><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) , 这里使用吉布斯抽样来进行估计, 其中, <i>D</i>表示相对齐的2种语言文档对的数量。<i>KW</i>表示源语言主题和单词组合的个数, <i>K</i>表示源语言主题的个数, 源语言和目标语言中主题的数目一样。<i>z</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>表示语言<i>l</i>文档<i>dl</i>中第<i>i</i>个单词的主题, <b><i>z</i></b><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>和<b><i>x</i></b><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>分别表示语言<i>l</i>文档<i>dl</i>中除单词<i>w</i><sub><i>i</i></sub>之外的所有主题和标志位, <b><i>w</i></b>为可比语料中观测到的所有单词。吉布斯采样是蒙特卡洛算法 (Markov chain Monte Carlo) 的特例, 它是从2个或多个随机变量的联合概率分布中生成样本序列的一种算法。吉布斯采样算法每次仅对1个主题及其标志位进行采样。在对每个采样的词语的主题-词分布和标志位的0-1分布进行迭代的时候, 主题<i>z</i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>和标志位<i>x</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>的条件概率分布满足如下条件:</p>
                </div>
                <div class="p1">
                    <p id="129"> (1) 若采样的文档为源语言时, 概率分布如式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>γ</mi><mo>, </mo><mi>δ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></msub><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>*</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="131">式 (1) 等价于式 (2) :</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>γ</mi><mo>, </mo><mi>δ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></msub><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>*</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">and</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>γ</mi><mo>, </mo><mi>δ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>*</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135"> (2) 若采样的文档为目标语言时, 概率分布如式 (3) 所示:</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>γ</mi><mo>, </mo><mi>δ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></msub><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>*</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>t</mi></msubsup><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>t</mi></msubsup><mo>+</mo><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">式 (3) 等价于式 (4) :</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>γ</mi><mo>, </mo><mi>δ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></msub><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>s</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>*</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>t</mi></msubsup><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>t</mi></msubsup><mo>+</mo><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139">and</p>
                </div>
                <div class="p1">
                    <p id="140" class="code-formula">
                        <mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msup><mo>, </mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>, </mo><mi mathvariant="bold-italic">w</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>γ</mi><mo>, </mo><mi>δ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>γ</mi><msub><mrow></mrow><mrow><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>*</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>t</mi></msubsup><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow><mi>t</mi></msubsup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mi>v</mi></mrow><mi>t</mi></msubsup><mo>+</mo><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup><mo>=</mo><mn>1</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="141">这样就能计算得到每一个词的主题概率分布, 其中<i>γ</i><sub><i>x</i><sup><i>ds</i></sup><sub><i>i</i></sub></sub>表示源语言文档<i>ds</i>中, 第<i>i</i>个单词的标志位的先验参数;同理, <i>γ</i><sub><i>x</i><sup><i>dt</i></sup><sub><i>i</i></sub></sub>表示目标语言文档<i>dt</i>中, 第<i>i</i>个单词的标志位的先验参数;<i>p</i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>x</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>表示源语言所有文档中, 给定前一个单词<i>w</i>和前一个单词主题<i>z</i>, 状态变量取<i>x</i> (0或1) 的次数;同理, <i>p</i><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>x</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>与<i>p</i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>x</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>含义类似;<i>α</i><sub><i>z</i><sup><i>d</i></sup><sub><i>i</i></sub></sub>表示主题先验分布的参数, <i>q</i><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>表示源语言文档<i>ds</i>中单词被分配给主题<i>z</i>的次数, <i>q</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>与<i>q</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>含义类似;<i>β</i><sub><i>w</i><sup><i>ds</i></sup><sub><i>i</i></sub></sub>表示源语言文档<i>ds</i>中单词<i>w</i><sub><i>i</i></sub>的先验参数, <i>β</i><sub><i>w</i><sup><i>dt</i></sup><sub><i>i</i></sub></sub>表示目标语言文档<i>dt</i>中单词<i>w</i><sub><i>i</i></sub>的先验参数;<i>n</i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>表示源语言所有文档中, 单词<i>w</i>作为单个单词分配给主题<i>z</i>的次数, <i>n</i><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>与<i>n</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>含义类似;<i>δ</i><sub><i>w</i><sup><i>ds</i></sup><sub><i>i</i></sub></sub>表示源语言文档<i>ds</i>中单词<i>w</i><sub><i>i</i></sub>的先验参数, <i>δ</i><sub><i>w</i><sup><i>dt</i></sup><sub><i>i</i></sub></sub>表示目标语言文档<i>dt</i>中单词<i>w</i><sub><i>i</i></sub>的先验参数;<i>m</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>v</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>表示源语言所有文档中, 给定前一个单词<i>w</i>的情况下, 将单词<i>v</i>作为短语的第2项分配给主题<i>z</i>的次数, <i>m</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>v</mi></mrow><mi>t</mi></msubsup></mrow></math></mathml>与<i>m</i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>v</mi></mrow><mi>s</mi></msubsup></mrow></math></mathml>含义类似。式 (1) ～式 (4) 中所有-<i>i</i>表示除当前词<i>w</i><sub><i>i</i></sub>之外的所有词。</p>
                </div>
                <h4 class="anchor-tag" id="154" name="154"><b>2.3 基于吉布斯采样的参数估计</b></h4>
                <div class="p1">
                    <p id="155">吉布斯采样算法是马尔可夫链蒙特卡洛算法的一个特例, 它是从2个或多个随机变量的联合概率分布中生成样本序列的一种算法。</p>
                </div>
                <div class="p1">
                    <p id="156">对于多个随机变量的联合概率分布的情况, 使用吉布斯采样可以产生比较简单的算法, 且算法容易实现, 使用吉布斯采样算法得到<i>θ</i>, <image href="images/JSJK201908023_157.jpg" type="" display="inline" placement="inline"><alt></alt></image>, <i>ψ</i>, <i>σ</i>等参数的后验估计, 公式如下:</p>
                </div>
                <div class="p1">
                    <p id="158" class="code-formula">
                        <mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>z</mi><mi>d</mi></msubsup><mo>=</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>z</mi></msub><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi></mrow><mi>s</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>z</mi></mrow><mi>t</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mrow><mo> (</mo><mrow><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>k</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>s</mi></msubsup><mo>+</mo><mi>q</mi><msubsup><mrow></mrow><mrow><mi>d</mi><mi>k</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>t</mi></msubsup></mrow><mo>) </mo></mrow></mrow></mstyle><mo>+</mo><mi>Κ</mi><mi>α</mi><msub><mrow></mrow><mrow><mi>k</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></msub></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi><mglyph src="数学符号.jpg" height="100%" width="100%" /></mi></mrow></mstyle><mrow><mo>^</mo><mspace width="0.25em" /></mrow></mover><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mfrac><mrow><mi>β</mi><msub><mrow></mrow><mi>w</mi></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi></mrow><mi>l</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>W</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mrow><mrow><mo> (</mo><mrow><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo>+</mo><mi>n</mi><msubsup><mrow></mrow><mrow><mi>z</mi><mi>v</mi></mrow><mi>l</mi></msubsup></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>ψ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>x</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mfrac><mrow><mi>γ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>x</mi></mrow><mi>l</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><mn>1</mn></munderover><mrow><mrow><mo> (</mo><mrow><mi>γ</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>x</mi></mrow><mi>l</mi></msubsup></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>σ</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>v</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mfrac><mrow><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>v</mi></mrow><mi>l</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>W</mi><msup><mrow></mrow><mi>l</mi></msup></mrow></munderover><mrow><mrow><mo> (</mo><mrow><mi>δ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>z</mi><mi>w</mi><mi>v</mi></mrow><mi>l</mi></msubsup></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="159">其中, <i>K</i>表示主题的个数, <i>W</i><sup><i>l</i></sup>表示语言<i>l</i>词汇表的大小。</p>
                </div>
                <h4 class="anchor-tag" id="160" name="160"><b>2.4 吉布斯采样算法</b><b>算法1</b> 吉布斯采样算法</h4>
                <div class="p1">
                    <p id="161"><b>Input</b>:Randomly assign <i>K</i> topic numbers to all words and 0-1 to all <i>x</i>。</p>
                </div>
                <div class="p1">
                    <p id="162"><b>Output</b>:The topic assignment of each word assigned to each word。</p>
                </div>
                <div class="p1">
                    <p id="163">foreach 1 to <i>NumIterations</i> do</p>
                </div>
                <div class="p1">
                    <p id="164">foreach Comparable document pair <i>d</i>∈<i>L</i> do</p>
                </div>
                <div class="p1">
                    <p id="165">foreach word do</p>
                </div>
                <div class="p1">
                    <p id="166">if (word is source language) {</p>
                </div>
                <div class="p1">
                    <p id="167">compute <i>P</i> (<i>z</i><mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>, <b><i>x</i></b><sup><i>ds</i></sup>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) and <i>P</i> (<i>x</i><mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><sup><i>ds</i></sup>, <b><i>x</i></b><mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) according to formula (2) ;</p>
                </div>
                <div class="p1">
                    <p id="172">sample <i>x</i><mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>from the distribution <i>P</i> (<i>x</i><mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><sup><i>ds</i></sup>, <b><i>x</i></b><mathml id="175"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="176">then sample <i>z</i><mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>from the distribution <i>P</i> (<i>z</i><mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>, <b><i>x</i></b><sup><i>ds</i></sup>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="180">Update the counts;</p>
                </div>
                <div class="p1">
                    <p id="181">}</p>
                </div>
                <div class="p1">
                    <p id="182">else{</p>
                </div>
                <div class="p1">
                    <p id="183">compute <i>P</i> (<i>z</i><mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>, <b><i>x</i></b><sup><i>dt</i></sup>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) and  <i>P</i> (<i>x</i><mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><sup><i>dt</i></sup>, <b><i>x</i></b><mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) according to  formula (4) ;</p>
                </div>
                <div class="p1">
                    <p id="188">sample <i>x</i><mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>from the distribution <i>P</i> (<i>x</i><mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><sup><i>dt</i></sup>, <b><i>x</i></b><mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="192">then sample <i>z</i><mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>from the distribution <i>P</i> (<i>z</i><mathml id="194"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>|<b><i>z</i></b><mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></msubsup></mrow></math></mathml>, <b><i>x</i></b><sup><i>dt</i></sup>, <b><i>w</i></b>, <i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="196">Update the counts;</p>
                </div>
                <div class="p1">
                    <p id="197">}</p>
                </div>
                <div class="p1">
                    <p id="198">end</p>
                </div>
                <div class="p1">
                    <p id="199">end</p>
                </div>
                <div class="p1">
                    <p id="200">end</p>
                </div>
                <div class="p1">
                    <p id="201">end</p>
                </div>
                <div class="p1">
                    <p id="202">其中, <i>L</i>为可比文档对<i>d</i>的集合, <b><i>x</i></b><mathml id="203"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mrow><mi>d</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>表示文档<i>l</i>中除第<i>i</i>个单词外其他各个单词的主题分配情况, <b><i>x</i></b><mathml id="204"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo>-</mo><mi>i</mi></mrow><mi>d</mi></msubsup></mrow></math></mathml>表示除第<i>i</i>个短语标志位外其他各个短语标志位的分配情况。每次采样过程保持其他分量值不变, 只采样一个分量, 对每个词语的主题-词分布进行迭代, 最终分布达到稳态为止。<i>NumIteration</i>指迭代总次数, 实验分别针对不同迭代次数进行比较, 从1开始计算当前词的概率。</p>
                </div>
                <h3 id="205" name="205" class="anchor-tag"><b>3 实验</b></h3>
                <h4 class="anchor-tag" id="206" name="206"><b>3.1 实验语料</b></h4>
                <div class="p1">
                    <p id="207">柬汉双语可比语料来自于以跨语言查询方式构建的双语可比语料库<citation id="365" type="reference"><link href="344" rel="bibliography" /><link href="346" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">20</a>]</sup></citation>。还有部分语料是从柬埔寨日报网站和柬埔寨中文社区爬取的相关的柬语和中文新闻语料, 这部分语料中, 中文新闻文档与相关柬文新闻文档都是对相同的新闻事件进行描述。最后一部分是请柬埔寨留学生和柬语语言专家标注翻译的中柬双语平行语料, 以及通过柬约公众号获得的平行语料, 作为双语可比语料的补充, 最终合并全部语料。这些文档集覆盖了许多领域, 如经济、政治、环境、旅游、网络等, 再通过去除广告等噪声的预处理操作, 最终获得132 964个柬语词和93 818个汉字, 共942篇短文对。</p>
                </div>
                <h4 class="anchor-tag" id="208" name="208"><b>3.2 实验设计和结果分析</b></h4>
                <div class="p1">
                    <p id="209">困惑度 (<i>Perplexity</i>) 广泛应用在语言模型中, 用来衡量语言模型对于不可见文本集合的解释能力。通过式 (6) 计算测试集上的困惑度的大小, 相对地讲, 困惑度越小, 模型越优, 其泛化能力和预测性能越强。对于本文模型, <i>p</i> (<i>w</i><mathml id="210"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></math></mathml>) =∑<sub><i>K</i></sub><i>p</i> (<i>w</i><mathml id="211"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></math></mathml>|<i>d</i>, <i>ζ</i>) =∑<sub><i>K</i></sub><i>p</i> (<i>w</i><mathml id="212"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></math></mathml>|<i>z</i>, <i>ζ</i>) <i>p</i> (<i>z</i>|<i>d</i>, <i>ζ</i>) , 其中<i>p</i> (<i>w</i><mathml id="213"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></math></mathml>) 为语言<i>l</i>的词<i>w</i>的边缘概率, <i>d</i>表示第<i>d</i>对可比文档对, <i>z</i>表示第<i>z</i>个主题词标号, <i>K</i>表示主题总数, <i>ζ</i>表示独立于模型外的参数, 如<i>ζ</i>={<i>α</i>, <i>β</i>, <i>γ</i>, <i>δ</i>}即先验分布系数和主题总数<i>K</i>。根据Wang等人<citation id="366" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的论文, 取经验值<i>α</i>= 1, <i>β</i>= 0.01, <i>γ</i>= 0.1和<i>δ</i>= 0.01。<i>p</i> (<i>w</i><mathml id="214"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></math></mathml>|<i>z</i>, <i>ζ</i>) 表示语言<i>l</i>的词<i>w</i>在主题<i>z</i>下的概率, <i>p</i> (<i>z</i>|<i>d</i>, <i>ζ</i>) 表示主题<i>z</i>在第<i>d</i>个文档下的概率, ∑<sub><i>d</i>∈<i>u</i></sub><i>N</i><mathml id="215"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></math></mathml>表示所有测试集<i>u</i>上语言<i>l</i>的总词数, 困惑度越低的模型, 越能适应新文档, 且越能代表原来那些不可见的文档。困惑度的计算方法如式 (6) 所示:</p>
                </div>
                <div class="p1">
                    <p id="216" class="code-formula">
                        <mathml id="216"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>e</mi><mi>r</mi><mi>p</mi><mo stretchy="false"> (</mo><mi>u</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mo>=</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>d</mi><mo>∈</mo><mi>u</mi></mrow></msub><mtext>l</mtext></mstyle><mtext>o</mtext><mtext>g</mtext><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>d</mi><mo>∈</mo><mi>u</mi></mrow></msub><mi>Ν</mi></mstyle><msubsup><mrow></mrow><mi>d</mi><mi>l</mi></msubsup></mrow></mfrac><mo stretchy="false">) </mo><mo>, </mo><mi>l</mi><mo>=</mo><mi>s</mi><mtext>或</mtext><mi>t</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="217">为了获得模型的最优条件, 本文分别就主题数目<i>K</i>、测试语料百分比、吉布斯抽样迭代次数以及不同数据主题领域进行了实验, 并将Zhu等人<citation id="367" type="reference"><link href="336" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出的Bi-LDA (Bilingual Latent Dirichlet Allocation) 模型作为对比实验模型, 实验使用全部语料作为训练语料, 之后又从中柬新闻上搜集了15个事件, 共50对中柬文档作为测试语料。本文模型的中文文档的困惑度如图2所示。柬文文档的困惑度如图3所示。</p>
                </div>
                <div class="area_img" id="218">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908023_218.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 中文文档的困惑度" src="Detail/GetImg?filename=images/JSJK201908023_218.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 中文文档的困惑度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908023_218.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 2 <i>Perplexity in Chinese corpus</i></p>

                </div>
                <div class="area_img" id="219">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908023_219.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 柬语文档的困惑度" src="Detail/GetImg?filename=images/JSJK201908023_219.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 柬语文档的困惑度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908023_219.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 3 <i>Perplexity in Khmer corpus</i></p>

                </div>
                <div class="p1">
                    <p id="220">图2<i>a</i>和图3<i>a</i>显示了主题数从10～100的困惑度散点图, 寻找最优的主题数。实验结果表明, 选取主题数量在80附近, 主题抽取比较理想。图2<i>b</i>和图3<i>b</i>所示为训练语料所占百分比对应的困惑度散点图, 困惑度随百分比增大而减小, 百分比为85%时困惑度趋于平衡, 因此将百分比定为85%时最佳。图2<i>c</i>和图3<i>c</i>所示的迭代次数为100～1 200的困惑度散点图, 可以看出迭代次数为1 000时困惑度趋于稳定, 效果最佳。图2的3幅图显示, 相比基准模型<i>BiLDA</i>, 本文的<i>NGB</i>-<i>LDA</i>模型拥有更好的困惑度值, 有效地利用了跨语言语料, 更好地挖掘了文档主题。因为<i>BiLDA</i>模型只适用词袋模型, 本文模型能考虑上下文信息, 能更好地提高主题预测能力, 因此考虑了上下文因素, 能有效提高模型能力。</p>
                </div>
                <h3 id="221" name="221" class="anchor-tag"><b>4 结束语</b></h3>
                <div class="p1">
                    <p id="222">本文提出了一种基于短语的双语<i>LDA</i>模型, 对中柬可比语料进行主题预测。该模型克服了传统<i>LDA</i>中词袋模型不能考虑上下文的缺点, 充分考虑一篇文档中前一个词及其主题对当前词的影响, 大大提高了模型的主题预测能力。实验结果表明, 本文提出的模型得到的实验结果比传统双语<i>LDA</i>模型的结果困惑度更低, 主题预测能力更强。在实验过程中发现, 相同主题的可比文档之间还有较大差异, 无法直接应用于机器翻译任务中。所以, 从可比语料中提取平行片段成为下一步工作的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="245" type="formula" href="images/JSJK201908023_24500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">严馨</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="306">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic latent semantic indexing">

                                <b>[1]</b> Hofmann T.Probabilistic latent semantic indexing[C]∥Proc of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999:50-57.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent Dirichlet allocation">

                                <b>[2]</b> Blei D M, Ng A Y, Jordan M I.Latent Dirichlet allocation[J].The Journal of Machine Learning Research, 2012, 3:993-1022.
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-language linking of news stories on the web using interlingual topic modelling">

                                <b>[3]</b> de Smet W, Moens M F.Cross-language linking of news stories on the web using interlingual topic modelling[C]∥Proc of the 2nd ACM Workshop on Social Web Search and Mining, 2009:57-64.
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Knowledge transfer across multilingual corpora via latent topics">

                                <b>[4]</b> de Smet W, Tang J, Moens M F.Knowledge transfer across multilingual corpora via latent topics[C]∥Proc of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2011:549-560.
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross lingual text classification by mining multilingual topics from wikipedia">

                                <b>[5]</b> Ni X C, Sun J T, Hu J, et al.Cross lingual text classification by mining multilingual topics from wikipedia[C]∥Proc of the4th ACM International Conference on Web Search and Web Data Mining, 2011:375-384.
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-language information retrieval with latent topic models trained on a comparable corpus">

                                <b>[6]</b> Vuli<image id="368" type="formula" href="images/JSJK201908023_36800.jpg" display="inline" placement="inline"><alt></alt></image> I, de Smet W, Moens M F.Cross-language information retrieval with latent topic models trained on a comparable corpus[C]∥Proc of the 7th Asia Information Retrieval Societies Conference, 2011:37-48.
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130523453186&amp;v=MjA5NTJ6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2dFU3bklKMW9YTmo3QmFySzdIdFRPckl0QVorb0hDaE04&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Vuli<image id="369" type="formula" href="images/JSJK201908023_36900.jpg" display="inline" placement="inline"><alt></alt></image> I, Smet W, Moens M-F.Cross-language information retrieval models based on latent topic models trained with document-aligned comparable corpora[J].Information Retrieval, 2013, 16 (3) :331-368.
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Detecting highly confident word translations from comparable corpora without any prior knowledge,&amp;quot;">

                                <b>[8]</b> Vuli<image id="370" type="formula" href="images/JSJK201908023_37000.jpg" display="inline" placement="inline"><alt></alt></image> I, Moens M F.Detecting highly confident word translations from comparable corpora without any prior knowledge[C]∥Proc of the 13th Conference of the European Chapter of the Association for Computational Linguistics, 2012:449-459.
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Integrating Topics and Syntax">

                                <b>[9]</b> Griffiths T L, Steyvers M, Blei D M, et al.Integrating topics and syntax[C]∥Proc of the 17th International Conference on Neural Information Processing Systems, 2004:537-544.
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topic Modeling:Beyond Bag-of-Words">

                                <b>[10]</b> Wallach H M.Topic modeling:Beyond bag-of-words[C]∥Proc of the 23rd International Conference on Machine Learning, 2006:977-984.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A note on topical N-gram">

                                <b>[11]</b> Wang X R, McCallum A.A note on topical n-grams:Technical Report UM-CS-2005-071[R].America:Department of Computer Science University of Massachusetts Amherst, 2005.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topical N-grams:Phrase andTopic Discovery,with an Application to Information Retrieval">

                                <b>[12]</b> Wang X R, McCallum A, Wei X.Topical N-grams:Phrase and topic discovery, with an application to information retrieval[C]∥Proc of the 7th IEEE International Conference on Data Mining (ICDM) , 2007:697-702.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hidden topic Markov Models">

                                <b>[13]</b> Gruber A, Weiss Y, Rosen-Zvi M.Hidden topic Markov model[C]∥Proc of the 10th International Conference on Artificial Intelligence and Statistics, 2007:163-170.
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Syntactic Topic Models">

                                <b>[14]</b> Boyd-Graber J L, Blei D M.Syntactic topic models[C]∥Proc of the 21st International Conference on Neural Information Processing Systems, 2008:185-192.
                            </a>
                        </p>
                        <p id="334">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mining multilingual topics from wikipedia">

                                <b>[15]</b> Ni X V, Sun J T, Hu J, et al.Mining multilingual topics from wikipedia[C]∥Proc of the 18th International Conference on World Wide Web, 2009:1155-1156.
                            </a>
                        </p>
                        <p id="336">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Building Comparable Corpora Based on Bilingual LDA Model">

                                <b>[16]</b> Zhu Z D, Li M, Chen L, et al.Building comparable corpora based on bilingual LDA model[C]∥Proc of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers) , 2013:278-282.
                            </a>
                        </p>
                        <p id="338">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Polylingual topic models">

                                <b>[17]</b> Mimno D, Wallach H M, Naradowsky J, et al.Polylingual topic models[C]∥Proc of the 2009Conference on Empirical Methods in Natural Language Processing, 2009:880-889.
                            </a>
                        </p>
                        <p id="340">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multilingual topic models for unaligned text">

                                <b>[18]</b> Boyd-Graber J, Blei D M.Multilingual topic models for unaligned text[C]∥Proc of the 25th Conference on Uncertainty in Artificial Intelligence, 2009:75-82.
                            </a>
                        </p>
                        <p id="342">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Extracting multilingual topics from unaligned comparable corpora &amp;quot;">

                                <b>[19]</b> Jagarlamudi J, DauméH.Extracting multilingual topics from unaligned comparable corpora[C]∥Proc of the 32nd European Conference on Advances in Information Retrieval, 2010:444-456.
                            </a>
                        </p>
                        <p id="344">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on the construction method of Chinese-Khmer Bilingual comparable corpus">

                                <b>[20]</b> Liu Xiao-hui.Research on the construction method of Chinese-Khmer Bilingual comparable corpus[D].Kunming:Kunming University of Science and Technology, 2016. (in Chinese) 
                            </a>
                        </p>
                        <p id="346">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016231300.nh&amp;v=MDQzNzh2TFZGMjZHTEc3SDlMTXI1RWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b>刘小惠.汉柬双语可比语料库构建方法研究[D].昆明:昆明理工大学, 2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201908023" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908023&amp;v=MDk5ODdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bFZMdkxMejdCWmJHNEg5ak1wNDlIWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
