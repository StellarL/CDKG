<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132373432686250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201907023%26RESULT%3d1%26SIGN%3du5PBZFjewB2smSnuxFI2BCj%252bj48%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907023&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907023&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907023&amp;v=MjI5MDdlWmVSbUZ5N21XNzdLTHo3QlpiRzRIOWpNcUk5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="&lt;b&gt;2 理论基础&lt;/b&gt; "><b>2 理论基础</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;2.1 不确定性数据对象&lt;/b&gt;"><b>2.1 不确定性数据对象</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;2.2 区间数&lt;/b&gt;"><b>2.2 区间数</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;2.3 算法相关概念&lt;/b&gt;"><b>2.3 算法相关概念</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="&lt;b&gt;3 UD-OPTICS算法&lt;/b&gt; "><b>3 UD-OPTICS算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#99" data-title="&lt;b&gt;3.1 不确定性数据的表示&lt;/b&gt;"><b>3.1 不确定性数据的表示</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;3.2 不确定性数据对象间的距离&lt;/b&gt;"><b>3.2 不确定性数据对象间的距离</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;3.3 算法描述&lt;/b&gt;"><b>3.3 算法描述</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#162" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#163" data-title="&lt;b&gt;4.1 实验数据&lt;/b&gt;"><b>4.1 实验数据</b></a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;4.2 相关参数&lt;i&gt;λ&lt;/i&gt;与包含因子&lt;i&gt;k&lt;/i&gt;的选取&lt;/b&gt;"><b>4.2 相关参数<i>λ</i>与包含因子<i>k</i>的选取</b></a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;4.3 与其他不确定性聚类算法比较&lt;/b&gt;"><b>4.3 与其他不确定性聚类算法比较</b></a></li>
                                                <li><a href="#183" data-title="&lt;b&gt;4.4 密度不均匀的人工合成数据验证&lt;/b&gt;"><b>4.4 密度不均匀的人工合成数据验证</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#189" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="&lt;b&gt;表1 概率分布&lt;i&gt;P&lt;/i&gt;与包含因子&lt;i&gt;k&lt;/i&gt;的关系&lt;/b&gt;"><b>表1 概率分布<i>P</i>与包含因子<i>k</i>的关系</b></a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;表2 UCI数据集&lt;/b&gt;"><b>表2 UCI数据集</b></a></li>
                                                <li><a href="#170" data-title="图1 &lt;i&gt;F&lt;/i&gt;-&lt;i&gt;measure&lt;/i&gt;值与&lt;i&gt;λ&lt;/i&gt;的关系曲线">图1 <i>F</i>-<i>measure</i>值与<i>λ</i>的关系曲线</a></li>
                                                <li><a href="#173" data-title="图2 &lt;i&gt;F&lt;/i&gt;-&lt;i&gt;measure&lt;/i&gt;值与&lt;i&gt;k&lt;/i&gt;的关系曲线">图2 <i>F</i>-<i>measure</i>值与<i>k</i>的关系曲线</a></li>
                                                <li><a href="#180" data-title="&lt;b&gt;表3 算法性能对比结果&lt;/b&gt;"><b>表3 算法性能对比结果</b></a></li>
                                                <li><a href="#185" data-title="图3 密度不均匀数据对象分布及其可达序列图">图3 密度不均匀数据对象分布及其可达序列图</a></li>
                                                <li><a href="#187" data-title="图4 不同聚类算法聚类&lt;i&gt;F&lt;/i&gt;-&lt;i&gt;measure&lt;/i&gt;值随&lt;i&gt;u&lt;/i&gt;的变化曲线">图4 不同聚类算法聚类<i>F</i>-<i>measure</i>值随<i>u</i>的变化曲线</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="213">


                                    <a id="bibliography_1" title=" Chi Rong-hua, Cheng Yuan, Zhu Su-xia, et al.Uncertain data analysis algorithm based on fast Gaussian transform[J].Journal of Communications, 2017, 38 (3) :101-111. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201703012&amp;v=MTAwNTZxQnRHRnJDVVJMT2VaZVJtRnk3bVc3N0tNVFhUYkxHNEg5Yk1ySTlFWm9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Chi Rong-hua, Cheng Yuan, Zhu Su-xia, et al.Uncertain data analysis algorithm based on fast Gaussian transform[J].Journal of Communications, 2017, 38 (3) :101-111. (in Chinese) 
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_2" title=" Ren Shi-jin.Research on uncertain data mining based on interval number and its application[D].Hangzhou:Zhejiang University, 2006. (in Chinese) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on uncertain data mining based on interval number and its application">
                                        <b>[2]</b>
                                         Ren Shi-jin.Research on uncertain data mining based on interval number and its application[D].Hangzhou:Zhejiang University, 2006. (in Chinese) 
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_3" title=" Qiu Zhi-ping.Interval analysis method for static response and eigenvalue problems of uncertain parameter structures[D].Jilin:Jilin University of Technology, Jilin University, 1994. (in Chinese) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interval analysis method for static response and eigenvalue problems of uncertain parameter structures">
                                        <b>[3]</b>
                                         Qiu Zhi-ping.Interval analysis method for static response and eigenvalue problems of uncertain parameter structures[D].Jilin:Jilin University of Technology, Jilin University, 1994. (in Chinese) 
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_4" title=" Li Jia-fei, Sun Xiao-yu.Clustering method for uncertain data based on spectral decomposition[J].Journal of Jilin University (Engineering and Technology Edition) , 2017, 47 (5) :1604-1611. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201705037&amp;v=MjYxMTBiTXFvOUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdtVzc3S0x5SE1kN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Li Jia-fei, Sun Xiao-yu.Clustering method for uncertain data based on spectral decomposition[J].Journal of Jilin University (Engineering and Technology Edition) , 2017, 47 (5) :1604-1611. (in Chinese) 
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_5" title=" Kriegel H P, Pfeifle M.Hierarchical density-based clustering of uncertain data[C]//Proc of IEEE International Conference on Data Mining, 2005:27-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical density based clustering of uncertain data">
                                        <b>[5]</b>
                                         Kriegel H P, Pfeifle M.Hierarchical density-based clustering of uncertain data[C]//Proc of IEEE International Conference on Data Mining, 2005:27-30.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_6" title=" Kriegel H P, Pfeifle M.Density-based clustering of uncertain data[C]//Proc of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, 2005:672-677." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Density-based clustering of uncertain data">
                                        <b>[6]</b>
                                         Kriegel H P, Pfeifle M.Density-based clustering of uncertain data[C]//Proc of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, 2005:672-677.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_7" title=" Chau M, Cheng R, Kao B, et al.Uncertain data mining:An example in clustering location data[C]//Proc of Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2006:199-204." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Uncertain data mining:An example in clustering location data">
                                        <b>[7]</b>
                                         Chau M, Cheng R, Kao B, et al.Uncertain data mining:An example in clustering location data[C]//Proc of Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2006:199-204.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_8" title=" Kao B, Lee S D, Cheung D W, et al.Clustering uncertain data using voronoi diagrams[C]//Proc of the 8th IEEE International Conference on Data Mining, 2008:333-342." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering uncertain data using voronoi diagrams">
                                        <b>[8]</b>
                                         Kao B, Lee S D, Cheung D W, et al.Clustering uncertain data using voronoi diagrams[C]//Proc of the 8th IEEE International Conference on Data Mining, 2008:333-342.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_9" title=" Cormode G, Mcgregor A.Approximation algorithms for clustering uncertain data[C]//Proc of the 27th ACM Sigmod-Sigact-Sigart Symposium on Principles of Database Systems, 2008:191-200." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Approximation Algorithms for Clustering Uncertain Data">
                                        <b>[9]</b>
                                         Cormode G, Mcgregor A.Approximation algorithms for clustering uncertain data[C]//Proc of the 27th ACM Sigmod-Sigact-Sigart Symposium on Principles of Database Systems, 2008:191-200.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_10" title=" Peng Yu, Luo Qing-hua, Peng Xi-yuan.UIDK-means:A multi-dimensional uncertain measurement data clustering algorithm[J].Chinese Journal of Scientific Instrument, 2011, 32 (6) :1201-1207. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201106000&amp;v=MTUyODZPZVplUm1GeTdtVzc3S1BEelRiTEc0SDlETXFZOUZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Peng Yu, Luo Qing-hua, Peng Xi-yuan.UIDK-means:A multi-dimensional uncertain measurement data clustering algorithm[J].Chinese Journal of Scientific Instrument, 2011, 32 (6) :1201-1207. (in Chinese) 
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_11" title=" He Yun-bin, Zhang Zhi-chao, Wan Jing, et al.Research for uncertain data clustering algorithm:U-PAM and UM-PAM algorithm[J].Computer Science, 2016, 43 (6) :263-269. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201606054&amp;v=MDk4NDNCYjdHNEg5Zk1xWTlBWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bVc3N0tMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         He Yun-bin, Zhang Zhi-chao, Wan Jing, et al.Research for uncertain data clustering algorithm:U-PAM and UM-PAM algorithm[J].Computer Science, 2016, 43 (6) :263-269. (in Chinese) 
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_12" title=" Wei Fang-yuan, Huang De-cai.UID-DBSCAN clustering algorithm of multi-dimensional uncertain data based on interval number[J].Computer Science, 2017, 44 (11A) :442-447. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S2095&amp;v=MzIzMjJDVVJMT2VaZVJtRnk3bVc3N0tMejdCYjdHNEg5YXZyWTlNWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Wei Fang-yuan, Huang De-cai.UID-DBSCAN clustering algorithm of multi-dimensional uncertain data based on interval number[J].Computer Science, 2017, 44 (11A) :442-447. (in Chinese) 
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_13" title=" Liu Xiu-mei, Zhao Ke-qin.Interval number decision set pair analysis[M].Beijing:Science Press, 2014. (in Chinese) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interval number decision set pair analysis">
                                        <b>[13]</b>
                                         Liu Xiu-mei, Zhao Ke-qin.Interval number decision set pair analysis[M].Beijing:Science Press, 2014. (in Chinese) 
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_14" title=" Moore R E.Error in digital computation, Volume 1[M].New York:John Wiley &amp;amp; Sons, 1965." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Error in digital computation">
                                        <b>[14]</b>
                                         Moore R E.Error in digital computation, Volume 1[M].New York:John Wiley &amp;amp; Sons, 1965.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_15" title=" Ankerst M.OPTICS:Ordering points to identify the clustering structure[J].ACM Sigmod Record, 1999, 28 (2) :49-60." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000068983&amp;v=MjM0NzJyNDlGWk8wSEJYUTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDBjYXhBPU5pZklZN0s3SHRqTg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Ankerst M.OPTICS:Ordering points to identify the clustering structure[J].ACM Sigmod Record, 1999, 28 (2) :49-60.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     Luo Yan-fu, Qian Xiao-dong.Uncertain data clustering algorithm based on local density[J].Data Analysis &amp;amp; Knowledge Discovery, 2017, 12:84-91. (in Chinese) </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     迟荣华, 程媛, 朱素霞, 等.基于快速高斯变换的不确定数据聚类算法[J].通信学报, 2017, 38 (3) :101-111.</a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_2" title=" 任世锦.基于区间数的不确定性数据挖掘及其应用研究[D].杭州:浙江大学, 2006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2007079077.nh&amp;v=MTk5NjllUm1GeTdtVzc3S1YxMjdHYk8vRjlITHFKRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         任世锦.基于区间数的不确定性数据挖掘及其应用研究[D].杭州:浙江大学, 2006.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     邱志平.不确定参数结构静力响应和特征值问题的区间分析方法[D].吉林:吉林工业大学, 吉林大学, 1994.</a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     李嘉菲, 孙小玉.基于谱分解的不确定数据聚类方法[J].吉林大学学报 (工学版) , 2017, 47 (5) :1604-1611.</a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     彭宇, 罗清华, 彭喜元.UIDK-means:多维不确定性测量数据聚类算法[J].仪器仪表学报, 2011, 32 (6) :1201-1207.</a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     何云斌, 张志超, 万静, 等.不确定数据聚类的U-PAM算法和UM-PAM算法的研究[J].计算机科学, 2016, 43 (6) :263-269.</a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     魏方圆, 黄德才.基于区间数的多维不确定性数据UID-DBSCAN聚类算法[J].计算机科学, 2017, 44 (11A) :442-447.</a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_13" title=" 刘秀梅, 赵克勤.区间数决策集对分析[M].北京:科学出版社, 2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787030419934000&amp;v=Mjc1NjdmWnU5dUZDdnRVN25MS0Y4V1hGcXpHYk83SHRYTnBvWkdZT3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3Jp&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         刘秀梅, 赵克勤.区间数决策集对分析[M].北京:科学出版社, 2014.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_16" title=" 罗彦福, 钱晓东.基于局部密度的不确定数据聚类算法[J].数据分析与知识发现, 2017, 12:84-91." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201712009&amp;v=MTQxOTR6cXFCdEdGckNVUkxPZVplUm1GeTdtVzc3S1BTbmZmN0c0SDliTnJZOUZiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         罗彦福, 钱晓东.基于局部密度的不确定数据聚类算法[J].数据分析与知识发现, 2017, 12:84-91.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(07),1303-1311 DOI:10.3969/j.issn.1007-130X.2019.07.023            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于区间数的不确定性数据聚类算法:UD-OPTICS</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E7%BF%A0%E5%85%88&amp;code=26488006&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴翠先</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E5%B0%91%E5%85%83&amp;code=42267114&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何少元</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E6%96%B0%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0415387&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信新技术应用研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E4%BF%A1%E7%A7%91%E8%AE%BE%E8%AE%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆信科设计有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在不确定性数据聚类算法的研究中, 普遍需要假设不确定性数据服从某种分布, 继而获得表示不确定性数据的概率密度函数或概率分布函数, 然而这种假设很难保证与实际应用系统中的不确定性数据分布一致。现有的基于密度的算法对初始参数敏感, 在对密度不均匀的不确定性数据聚类时, 无法发现任意密度的类簇。鉴于这些不足, 提出基于区间数的不确定性数据对象排序识别聚类结构算法 (UD-OPTICS) 。该算法利用区间数理论, 结合不确定性数据的相关统计信息来更加合理地表示不确定性数据, 提出了低计算复杂度的区间核心距离与区间可达距离的概念与计算方法, 将其用于度量不确定性数据间的相似度, 拓展类簇与对象排序识别聚类结构。该算法可很好地发现任意密度的类簇。实验结果表明, UD-OPTICS算法具有较高的聚类精度和较低的复杂度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不确定性数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%BA%E9%97%B4%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">区间数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密度聚类算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=OPTICS&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">OPTICS;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    吴翠先 (1969-) , 女, 四川仁寿人, 硕士, 正高级工程师, 研究方向为大数据技术研究及应用。E-mail:372181772@qq.com通信地址:400065重庆市重庆邮电大学通信新技术应用研究中心;
                                </span>
                                <span>
                                    何少元, 通信地址:400065重庆市重庆邮电大学通信新技术应用研究中心;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-24</p>

            </div>
                    <h1><b>UD-OPTICS: An uncertain data clustering algorithm based on interval number</b></h1>
                    <h2>
                    <span>WU Cui-xian</span>
                    <span>HE Shao-yuan</span>
            </h2>
                    <h2>
                    <span>School of Telecommunication and Information Engineering, Chongqing University of Posts and Telecommunications</span>
                    <span>Research Center of New Telecommunication Technology Applications, Chongqing University of Posts and Telecommunications</span>
                    <span>Chongqing Information Technology Designing Company Limited</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The research on uncertain data clustering algorithms generally assumes that uncertain data obeys a certain distribution, so we can obtain the probability density function or probability distribution function which represents the uncertain data. However, it is difficult to guarantee the consistency between the assumed distribution and the distribution of uncertain data in practical applications. Existing algorithms based on density are sensitive to initial parameters, so they cannot find class clusters of arbitrary density when clustering uncertain data with uneven density. In view of these shortcomings, we propose an algorithm based on interval number for uncertain data object sorting recognition clustering structure (UD-OPTICS) . It uses the interval number theory and the statistical information of the uncertain data to represent the uncertain data more reasonably. We propose the concept and calculation method of interval core distance and interval reachable distance with low computational complexity, which are used to measure the similarity between uncertain data and expand the cluster structure of clusters and object sorting. This algorithm can well find clusters of arbitrary density. Experimental results show that the UD-OPTICS algorithm has higher clustering accuracy and lower complexity.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=uncertain%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">uncertain data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=interval%20number&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">interval number;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=density%20clustering%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">density clustering algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=OPTICS&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">OPTICS;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WU Cui-xian, born in 1969, MS, senior engineer, her research interest includes big data technology research and application.Address:Research Center of New Telecommunication Technology Applications, Chongqing University of Posts and Telecommunications, Chongqing 400065, P.R.China;
                                </span>
                                <span>
                                    HE Shao-yuan, Address:Research Center of New Telecommunication Technology Applications, Chongqing University of Posts and Telecommunications, Chongqing 400065, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-07-24</p>
                            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="54">无线通信技术与网络信息技术的快速发展产生了极其庞大的数据量。然而, 由于原始数据不准确、采用粗粒度数据集合、出于隐私保护的特殊目的以及数据集成等原因, 经济、电信、气象等众多领域中普遍包含不确定性数据<citation id="263" type="reference"><link href="213" rel="bibliography" /><link href="245" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">1</a>]</sup></citation>。不确定性数据中的不确定性包括不定性、不可靠性、不可预知性、随机性、意义模糊性、易变性、不完整、未知然有界性和不规则性等<citation id="265" type="reference"><link href="215" rel="bibliography" /><link href="217" rel="bibliography" /><link href="247" rel="bibliography" /><link href="249" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。根据不确定性数据对象的表现形式将其分为:存在级不确定性数据对象与属性级不确定性数据对象<citation id="264" type="reference"><link href="219" rel="bibliography" /><link href="251" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">4</a>]</sup></citation>。存在级不确定性数据对象指对象存在与否是不确定的, 相关研究采用点概率模型或者可能世界模型等方法表示一个存在级的不确定性数据对象。属性级不确定性数据对象指对象属性值是不确定的, 是一个范围的概念, 相关研究采用概率密度函数或者其他统计参数来描述这种不确定性。本文主要对属性级不确定性数据聚类进行研究。</p>
                </div>
                <div class="p1">
                    <p id="55">在数据挖掘过程中, 必须考虑数据的不确定性, 不然得到的数据挖掘结果将是不准确的甚至是错误的。只有充分考虑数据的不确定性, 我们才能更加准确有效地了解数据的本质, 继而得到准确反映客观事物的潜在有用的知识。</p>
                </div>
                <div class="p1">
                    <p id="56">作为数据挖掘的重要技术之一, 不确定性数据的聚类分析得到了研究者们的广泛关注。现有的属性级不确定性数据聚类算法多是由传统的确定性数据聚类算法改进得来, 通过构建不确定性数据对象的模型, 以及改进不确定性数据间的相似性度量, 继而提出面向属性级不确定性数据的聚类算法。Kriegel等人<citation id="269" type="reference"><link href="221" rel="bibliography" /><link href="223" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>提出基于密度的模糊点排序识别聚类结构FOPTICS (Fuzzy Ordering Points To Identify the Clustering Structure ) 算法和模糊基于密度的噪声应用空间聚类FDBSCAN (Fuzzy Density-Based Spatial Clustering of Applications with Noise) 算法, 他们利用距离概率函数来度量2个模糊对象间的相似度, 这些模糊距离函数为每个可能的距离值分配一个概率值, 其计算过程是高复杂度的多重积分运算。Chau等人<citation id="266" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>运用概率密度函数表示属性级不确定性数据, 利用平均期望距离进行相似性度量, 继而将确定性数据聚类算法K-means拓展到不确定性数据聚类中, 提出了不确定的K均值算法UK-means (Uncertain K-means) 算法。但是, 只要聚类中心一更新, 就必须使用高复杂度的积分方式重新计算期望距离, 导致计算量过大。因此, Kao等人<citation id="267" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>又在UK-means算法的基础上研究各种修剪方法, 以避免昂贵的期望距离计算, 继而提出了CK-means (Classical K-means) 算法。迟荣华等人<citation id="268" type="reference"><link href="213" rel="bibliography" /><link href="245" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">1</a>]</sup></citation>提出了一种基于快速高斯变换的不确定性数据聚类算法IUK-means (Interface to Uncertain data K-means) (面向不确定性数据的K均值算法) , 该算法运用快速高斯变换的核密度估计方法, 构建不确定数据模型, 结合数据对象属性特征与不确定分布特征, 度量不确定性数据间的相似性, 继而进行聚类。</p>
                </div>
                <div class="p1">
                    <p id="57">此类算法需要事先得知数据对象的概率分布信息。由于直接获取数据的概率分布函数或概率密度函数比较困难, 其大多在假设数据对象服从某种概率分布的前提下, 来获取表示不确定性数据的概率分布函数或概率密度函数<citation id="270" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 继而用这些信息来表示不确定性数据。这种方式很难保证假设与实际情况一致, 容易造成对数据的误解。</p>
                </div>
                <div class="p1">
                    <p id="58">对此, 彭宇等人<citation id="271" type="reference"><link href="231" rel="bibliography" /><link href="253" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">10</a>]</sup></citation>利用区间数和统计信息表示不确定性数据, 提出UIDK-means (Uncertain multi-dimension Data K-means) 算法, 该算法不仅具有较低计算复杂度, 还不需要事先得知数据的概率分布或假设概率分布便可以进行聚类。但是, 该算法使用灵活性较低的随机采样的方法对簇中心点初始化, 且聚类结果对簇中心点和噪声敏感。何云斌等人<citation id="272" type="reference"><link href="233" rel="bibliography" /><link href="255" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">11</a>]</sup></citation>采用区间数结合标准差表示不确定性数据, 提出了不确定的围绕中心点的划分算法U-PAM (Uncertain Partitioning Around Medoid) 与UM-PAM (Uncertainy Measurement data Partitioning  Around Medoid) 算法, 其利用CH 指标确定最佳聚类个数, 一定程度上提高了聚类质量。该类基于划分的算法, 仍然有无法识别噪声点和不能发现任意形状簇的缺点。对此, 魏方圆等人<citation id="273" type="reference"><link href="235" rel="bibliography" /><link href="257" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">12</a>]</sup></citation>提出了UID-DBSCAN (Uncertain multi-dimension Data DBSCAN) 算法。该算法基于密度聚类, 可以发现任意形状的簇, 对噪声点也不敏感。但是, 该算法对初始参数敏感, 且对密度变化较大的不确定性数据聚类时无法发现某些簇, 无法发现任意密度的类簇。</p>
                </div>
                <div class="p1">
                    <p id="59">通过上述分析发现, 现有的不确定性数据聚类算法存在着不确定信息难以获取且表示过于理想化、对初始参数敏感、无法发现任意密度的类簇等问题。</p>
                </div>
                <div class="p1">
                    <p id="60">最初为了解决计算机计算时因舍去而产生的误差在进一步计算中会出现误差放大或传播的问题, 研究者们提出了区间数<citation id="274" type="reference"><link href="231" rel="bibliography" /><link href="253" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">10</a>]</sup></citation>。后来人们发现区间数也适用于很多其他场合, 例如近年来研究较多的利用区间数进行不确定的多属性决策;将区间数添加到数学规划之中形成不确定性优化模型等。可见区间数在不确定性度量上具有很强的优势。</p>
                </div>
                <div class="p1">
                    <p id="61">从区间数的角度考虑, 不确定性数据的区间范围是较容易获得的, 且通过统计计算可以较容易地得到数据的均值与标准差等信息, 不需要假设和过多的先验知识。可以运用区间数结合均值与标准差等信息准确地表示不确定性数据, 避免了采用假设的概率密度函数或概率分布函数表示不确定性数据时对数据的误解。因此, 本文面向属性级不确定性数据, 利用区间数结合均值与标准差表示此类不确定性数据, 同时提出区间核心距离、区间可达距离的概念与计算方法并将其应用于拓展簇与点排序识别聚类结构中, 继而提出基于区间数的不确定性数据聚类算法UD-OPTICS (Uncertain Data OPTICS) 。该算法对初始参数不敏感, 能够识别噪声点, 能很好地发现任意密度的簇与任意形状的簇。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag"><b>2 理论基础</b></h3>
                <h4 class="anchor-tag" id="63" name="63"><b>2.1 不确定性数据对象</b></h4>
                <div class="p1">
                    <p id="64">确定性数据对象可以表示为空间中的一个精确的点, 而不确定性数据对象面对由于原始数据不准确、采用粗粒度数据集合或需要满足特殊应用目的等原因引起的属性值的不确定性时, 则需要用空间中一个区域内的数据点集合来表示这个对象。</p>
                </div>
                <div class="p1">
                    <p id="65"><b>定义1</b> (不确定性数据对象<citation id="275" type="reference"><link href="235" rel="bibliography" /><link href="257" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">12</a>]</sup></citation>) 不确定性数据对象是一个由<i>s</i>个<i>m</i>维样本点数据集构成的形如<b><i>Q</i></b><sub><i>i</i></sub>={<b><i>Q</i></b><sub><i>i</i>1</sub>, <b><i>Q</i></b><sub><i>i</i>2</sub>, …, <b><i>Q</i></b><sub><i>is</i></sub>}的数据对象, 其中<mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>p</mi></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi>Q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>p</mi><mn>1</mn></mrow></msub><mo>, </mo><mi>Q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>p</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Q</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>p</mi><mi>m</mi></mrow></msub><mo stretchy="false">}</mo><mrow><mo> (</mo><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>s</mi><mo stretchy="false">}</mo></mrow><mo>) </mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="67"><b>定义2</b> (不确定数据集<citation id="276" type="reference"><link href="235" rel="bibliography" /><link href="257" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">12</a>]</sup></citation>) 一个由<i>n</i>个不确定性数据对象构成的集合称为不确定性数据集, 表示为<i>Q</i>={<b><i>Q</i></b><sub>1</sub>, <b><i>Q</i></b><sub>2</sub>, …, <b><i>Q</i></b><sub><i>n</i></sub>}。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>2.2 区间数</b></h4>
                <div class="p1">
                    <p id="69"><b>定义3</b> (区间数<citation id="277" type="reference"><link href="237" rel="bibliography" /><link href="259" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">13</a>]</sup></citation>) 给定<b><i>A</i></b><sub>L</sub>, <b><i>A</i></b><sub>R</sub>∈<b>R</b><sup><i>m</i></sup>且<b><i>A</i></b><sub>R</sub>≥<b><i>A</i></b><sub>L</sub>, 其中<b><i>A</i></b><sub>L</sub>为区间数的下界, <b><i>A</i></b><sub>R</sub>为区间数的上界。称集合<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>R</mtext></msub></mrow><mo>]</mo></mrow></mrow></math></mathml>≜{<i>μ</i>|<b><i>A</i></b><sub>L</sub>≤<i>μ</i>≤<b><i>A</i></b><sub>R</sub>} 为一个区间数, 当<b><i>A</i></b><sub>L</sub>=<b><i>A</i></b><sub>R</sub>时, 区间数为一个精确数。</p>
                </div>
                <div class="p1">
                    <p id="71"><b>定义4</b> (区间数的中点与半径<citation id="278" type="reference"><link href="237" rel="bibliography" /><link href="259" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">13</a>]</sup></citation>) 给定区间数<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>R</mtext></msub></mrow><mo>]</mo></mrow></mrow></math></mathml>, 令<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>A</mi></msub><mo>=</mo><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow><mo>) </mo></mrow><mo>/</mo><mn>2</mn><mo>, </mo><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>A</mi></msub><mo>=</mo><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>+</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mtext>L</mtext></msub></mrow><mo>) </mo></mrow><mo>/</mo><mn>2</mn></mrow></math></mathml>, 则有:<b><i>A</i></b><sub>L</sub>=<b><i>m</i></b><sub><i>A</i></sub>-<i>α</i><sub><i>A</i></sub>, <b><i>A</i></b><sub>R</sub>=<b><i>m</i></b><sub><i>A</i></sub>+<i>α</i><sub><i>A</i></sub>, 其中, 称<b><i>m</i></b><sub><i>A</i></sub>为区间数的中点, <i>α</i><sub><i>A</i></sub>为区间数的半径, 因此区间数也可表示为[<b><i>m</i></b><sub><i>A</i></sub>-<i>α</i><sub><i>A</i></sub>, <b><i>m</i></b><sub><i>A</i></sub>+<i>α</i><sub><i>A</i></sub>]。</p>
                </div>
                <div class="p1">
                    <p id="74"><b>定义5</b> (区间数的距离<citation id="279" type="reference"><link href="237" rel="bibliography" /><link href="259" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">13</a>]</sup></citation>) 对于区间数<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>X</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi>X</mi><msub><mrow></mrow><mtext>R</mtext></msub></mrow><mo>]</mo></mrow><mo>, </mo><mi>Y</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>Y</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>, </mo><mi>Y</mi><msub><mrow></mrow><mtext>R</mtext></msub></mrow><mo>]</mo></mrow></mrow></math></mathml>, 将<i>X</i>, <i>Y</i>之间的距离定义为:<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mrow><mo> (</mo><mrow><mi>X</mi><mo>, </mo><mi>Y</mi></mrow><mo>) </mo></mrow><mo>=</mo><mo stretchy="false">∥</mo><mi>X</mi><mo>-</mo><mi>Y</mi><mo stretchy="false">∥</mo><mo>=</mo><msqrt><mrow><mo stretchy="false">|</mo><mi>X</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo>-</mo><mi>Y</mi><msub><mrow></mrow><mtext>L</mtext></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">|</mo><mi>X</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo>-</mo><mi>Y</mi><msub><mrow></mrow><mtext>R</mtext></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>2.3 算法相关概念</b></h4>
                <div class="p1">
                    <p id="78">本文不确定性数据聚类算法<i>UD</i>-<i>OPTICS</i>属于基于密度的聚类算法, 这里我们将给出区间数环境下适合本文算法的密度聚类算法的相关概念。</p>
                </div>
                <div class="p1">
                    <p id="79"><b>定义6</b> (区间领域) 给定对象为中心, 以<i>ε</i>为半径的空间, 称为<i>ε</i>区间领域, 其中<i>ε</i>称为区间半径。</p>
                </div>
                <div class="p1">
                    <p id="80"><b>定义7</b> (区间阈值<i>MinPts</i>) 区间阈值<i>MinPts</i>是指在<i>ε</i>确定的情况下, 使得不确定性数据对象<b><i>Q</i></b><sub><i>i</i></sub>成为核心对象的最小邻居不确定数据对象的个数。</p>
                </div>
                <div class="p1">
                    <p id="81">UD-OPTICS算法的输出是具有区间可达距离等信息的样本点输出排序, 与其他聚类算法相比, UD-OPTICS算法对输入参数 (即<i>ε</i>和<i>MinPts</i>的值) 相当不敏感, 即<i>ε</i>和<i>MinPts</i>的值在一定范围内变化, 对排序结果的影响非常小。粗略地说, 这些值只要足够“大”就能产生良好的结果。这里采用如下方法确定<i>ε</i>和<i>MinPts</i>值。对于包含<i>m</i>个对象的不确定性数据空间UDS (Uncertain Data Space) , <i>ε</i>取UDS中<i>n</i>维超球面<i>S</i>的半径<i>r</i>, 其中<i>S</i>恰好包含<i>MinPts</i>个点。<i>n</i>维超球面<i>S</i>的体积为<i>V</i><sub><i>S</i></sub>=<i>V</i><sub>UDS</sub>/<i>m</i>×<i>MinPts</i>, 采用半径<i>r</i>表示<i>n</i>维超球面<i>S</i>的体积为<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msub><mrow></mrow><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><msqrt><mrow><mtext>π</mtext><msup><mrow></mrow><mi>n</mi></msup></mrow></msqrt><mo>/</mo><mtext>Γ</mtext><mo stretchy="false"> (</mo><mi>n</mi><mo>/</mo><mn>2</mn><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>×</mo><mi>r</mi><msup><mrow></mrow><mi>n</mi></msup></mrow></math></mathml>, 其中Γ表示Gamma函数。半径<i>r</i>可以计算为:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mo>=</mo><mroot><mrow><mo stretchy="false"> (</mo><mi>V</mi><msub><mrow></mrow><mrow><mtext>U</mtext><mtext>D</mtext><mtext>S</mtext></mrow></msub><mo>×</mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi><mo>×</mo><mtext>Γ</mtext><mrow><mo> (</mo><mrow><mi>n</mi><mo>/</mo><mn>2</mn><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mi>m</mi><mo>×</mo><msqrt><mrow><mtext>π</mtext><msup><mrow></mrow><mi>n</mi></msup></mrow></msqrt></mrow><mi>n</mi></mroot><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">这种方法可以确保2个参数是相互适应的, 即不会出现<i>MinPts</i>值一定时, <i>ε</i>值过小而导致大量对象具有Undefined可达距离的情况。</p>
                </div>
                <div class="p1">
                    <p id="85"><b>定义8</b> (核心对象) 在给定<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>ε</mi><mo>, </mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>情况下, 对<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∀</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Q</mi><mrow><mo> (</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>n</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>, 若<i>N</i><sub><i>ε</i></sub> (<b><i>Q</i></b><sub><i>i</i></sub>) ≥<i>MinPts</i> (其中<i>N</i><sub><i>ε</i></sub> (<b><i>Q</i></b><sub><i>i</i></sub>) 表示<b><i>Q</i></b><sub><i>i</i></sub>的区间领域邻居对象个数) , 则称<b><i>Q</i></b><sub><i>i</i></sub>是不确定数据集<i>Q</i>关于 (<i>ε</i>, <i>MinPts</i>) 的一个核心对象。</p>
                </div>
                <div class="p1">
                    <p id="88"><b>定义9</b> (区间直接密度可达) 对∀<b><i>Q</i></b><sub><i>i</i></sub>, <b><i>Q</i></b><sub><i>j</i></sub>∈<i>Q</i>, 如果<b><i>Q</i></b><sub><i>i</i></sub>是一个核心对象, 且<b><i>Q</i></b><sub><i>j</i></sub>∈<i>ε</i> (<b><i>Q</i></b><sub><i>i</i></sub>) , 那么称不确定性数据对象<b><i>Q</i></b><sub><i>j</i></sub>从对象<b><i>Q</i></b><sub><i>i</i></sub>出发关于<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>ε</mi><mo>, </mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>区间直接密度可达。</p>
                </div>
                <div class="p1">
                    <p id="90"><b>定义10</b> (区间密度可达) 对于不确定数据集<i>Q</i>, 给定一串样本对象<b><i>Q</i></b><sub>1</sub>, <b><i>Q</i></b><sub>2</sub>, …, <b><i>Q</i></b><sub><i>m</i></sub>, 令<b><i>W</i></b>=<b><i>Q</i></b><sub>1</sub>, <b><i>Z</i></b>=<b><i>Q</i></b><sub><i>m</i></sub>, 若对象<b><i>Q</i></b><sub><i>i</i></sub>到<b><i>Q</i></b><sub><i>i</i>-1</sub>区间直接密度可达, 那么对象<b><i>W</i></b>到对象<b><i>Z</i></b>区间密度可达。</p>
                </div>
                <div class="p1">
                    <p id="91"><b>定义11</b> (区间密度相连) 对∀<b><i>Q</i></b><sub><i>j</i></sub>, <b><i>Q</i></b><sub><i>k</i></sub>∈<i>Q</i>, 如果∃<b><i>Q</i></b><sub><i>i</i></sub>∈<i>Q</i>, 使从<b><i>Q</i></b><sub><i>i</i></sub>到<b><i>Q</i></b><sub><i>j</i></sub>, 从<b><i>Q</i></b><sub><i>i</i></sub>到<b><i>Q</i></b><sub><i>k</i></sub>都区间密度可达, 则称<b><i>Q</i></b><sub><i>j</i></sub>到<b><i>Q</i></b><sub><i>k</i></sub>区间密度相连。</p>
                </div>
                <div class="p1">
                    <p id="92"><b>定义12</b> (区间核心距离) 不确定性数据对象<b><i>Q</i></b><sub><i>i</i></sub>的区间核心距离是指<b><i>Q</i></b><sub><i>i</i></sub>成为核心对象的最小<i>ε</i>′, 如果<b><i>Q</i></b><sub><i>i</i></sub>不是核心对象, 则其区间核心距离无意义;继而<b><i>Q</i></b><sub><i>i</i></sub>的区间核心距离可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>C</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mrow><mo> (</mo><mrow><mi>ε</mi><mo>, </mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi></mrow><mo>) </mo></mrow></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mtext>U</mtext><mtext>n</mtext><mtext>d</mtext><mtext>e</mtext><mtext>f</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mtext>d</mtext><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mo stretchy="false">|</mo><mi>Ν</mi><msub><mrow></mrow><mi>ε</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>&lt;</mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi><mo>-</mo><mtext>t</mtext><mtext>h</mtext><mspace width="0.25em" /><mtext>s</mtext><mtext>m</mtext><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mtext>d</mtext><mtext>i</mtext><mtext>s</mtext><mtext>t</mtext><mtext>a</mtext><mtext>n</mtext><mtext>c</mtext><mtext>e</mtext><mspace width="0.25em" /><mtext>t</mtext><mtext>o</mtext><mspace width="0.25em" /><mi>Ν</mi><msub><mrow></mrow><mi>ε</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94"><b>定义13</b> (区间可达距离) 不确定性数据对象<b><i>Q</i></b><sub><i>i</i></sub>的区间核心距离和不确定性数据对象<b><i>Q</i></b><sub><i>j</i></sub>与<b><i>Q</i></b><sub><i>i</i></sub>距离的最大值, 称为区间可达距离, 若<b><i>Q</i></b><sub><i>i</i></sub>不是核心对象, 则其区间可达距离无意义, 其定义为:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mrow><mo> (</mo><mrow><mi>ε</mi><mo>, </mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi></mrow><mo>) </mo></mrow></mrow></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>) </mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mtext>U</mtext><mtext>n</mtext><mtext>d</mtext><mtext>e</mtext><mtext>f</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext><mtext>d</mtext><mo>, </mo><mo stretchy="false">|</mo><mi>Ν</mi><msub><mrow></mrow><mi>ε</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>&lt;</mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi></mtd></mtr><mtr><mtd><mtext>Μ</mtext><mtext>a</mtext><mtext>x</mtext><mo stretchy="false"> (</mo><mi>R</mi><mi>C</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>ε</mi><mo>, </mo><mi>Μ</mi><mi>i</mi><mi>n</mi><mi>Ρ</mi><mi>t</mi><mi>s</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mtext>e</mtext><mtext>l</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中, <i>D</i> (<b><i>Q</i></b><sub><i>j</i></sub>, <b><i>Q</i></b><sub><i>i</i></sub>) 表示<b><i>Q</i></b><sub><i>j</i></sub>和<b><i>Q</i></b><sub><i>i</i></sub>之间距离。</p>
                </div>
                <h3 id="97" name="97" class="anchor-tag"><b>3 UD-OPTICS算法</b></h3>
                <div class="p1">
                    <p id="98">本节主要介绍如何运用区间数表示不确定性数据;然后利用新的区间数间的距离度量方式实现区间核心距离、区间可达距离的计算, 继而实现不确定性数据对象排序识别聚类结构, 提出了不确定性数据聚类算法<i>UD</i>-<i>OPTICS</i>, 并对其进行描述与分析。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>3.1 不确定性数据的表示</b></h4>
                <div class="p1">
                    <p id="100">对于m个n维不确定性数据对象<b><i>Q</i></b><sub>1</sub>, <b><i>Q</i></b><sub>2</sub>, …, <b><i>Q</i></b><sub><i>m</i></sub>, 第<i>i</i>个数据对象第<i>j</i>维度的均值为<i>μ</i><sub><i>j</i></sub> (<b><i>Q</i></b><sub><i>i</i></sub>) , 如此相应地与<b><i>Q</i></b><sub><i>i</i></sub>相对应的<i>n</i>维均值向量可以表示为<i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) ;第<i>i</i>个数据对象第<i>j</i>维度的误差用<i>φ</i><sub><i>j</i></sub> (<b><i>Q</i></b><sub><i>i</i></sub>) 表示, 它的值是与不确定数据对象<b><i>Q</i></b><sub><i>i</i></sub>第<i>j</i>维度相关的标准差。如此, 相应地与<b><i>Q</i></b><sub><i>i</i></sub>相对应的<i>n</i>维误差向量可表示为<i>φ</i><sub><i>j</i></sub> (<b><i>Q</i></b><sub><i>i</i></sub>) 。依据区间数的中点和半径的表示方式, 可以将不确定性数据对象表示为[<i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) -<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) , <i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) +<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) ]。而由文献<citation id="280" type="reference">[<a class="sup">14</a>]</citation>可知, 不确定性数据<b><i>Q</i></b><sub><i>i</i></sub>分布在区间[<i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) -<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) , <i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) +<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) ] (<i>k</i>∈<b>R</b><citation id="281">|0</citation>≤<i>k</i>≤3) 上, 其概率分布<i>P</i>与包含因子<i>k</i>的关系如表1所示。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表1 概率分布<i>P</i>与包含因子<i>k</i>的关系</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Relationship between probability distribution and inclusion factor <i>k</i></b></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td><br /><i>k</i></td><td><i>P</i></td><td><i>k</i></td><td><i>P</i></td></tr><tr><td><br />0.676</td><td>0.50</td><td>2.58</td><td>0.99</td></tr><tr><td><br />1.64</td><td>0.90</td><td>3</td><td>0.997 3</td></tr><tr><td><br />1.96</td><td>0.95</td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>3.2 不确定性数据对象间的距离</b></h4>
                <div class="p1">
                    <p id="103">定义5将区间数之间的距离定义为一个精确数, 而2个区间数均表示数据的范围, 它们之间的距离也应该是一个范围的概念。若2个区间数间的距离使用单一精确数表示, 将无法很好地表示区间数间各种可能的距离值, 则会丢失其他有用的距离信息, 会严重影响数据处理结果。因此, 本文将采用一种新的区间数距离计算方法。</p>
                </div>
                <div class="p1">
                    <p id="104">已知区间数<i>X</i>=[<b><i>m</i></b><sub><i>X</i></sub>-<i>α</i><sub><i>X</i></sub>, <b><i>m</i></b><sub><i>X</i></sub>+<i>α</i><sub><i>X</i></sub>], <i>Y</i>=[<b><i>m</i></b><sub><i>Y</i></sub>-<i>α</i><sub><i>Y</i></sub>, <b><i>m</i></b><sub><i>Y</i></sub>+<i>α</i><sub><i>Y</i></sub>], 其中<b><i>m</i></b><sub><i>X</i></sub>, <b><i>m</i></b><sub><i>Y</i></sub>, <i>α</i><sub><i>X</i></sub>, <i>α</i><sub><i>Y</i></sub>∈<b>R</b><sup><i>n</i></sup>。在区间数任意维度<i>j</i> (1≤<i>j</i>≤<i>n</i>) 上, 2个区间数在数轴上的关系可归纳为以下2种:有公共区域和无公共区域。在第<i>j</i>维度上, 当2个区间数有公共区域时, 2个区间数的最小距离为0, 最大距离为|<i>m</i><sub><i>Xj</i></sub>-<i>m</i><sub><i>Yj</i></sub>|+<i>α</i><sub><i>Xj</i></sub>+<i>α</i><sub><i>Yj</i></sub>;当2个区间数无公共区域时, 2个区间数的最小距离为|<i>m</i><sub><i>Xj</i></sub>-<i>m</i><sub><i>Yj</i></sub>|-<i>α</i><sub><i>Xj</i></sub>-<i>α</i><sub><i>Yj</i></sub>, 2个区间数的最大距离为|<i>m</i><sub><i>Xj</i></sub>-<i>m</i><sub><i>Yj</i></sub>|+<i>α</i><sub><i>Xj</i></sub>+<i>α</i><sub><i>Yj</i></sub>。其中, <i>m</i><sub><i>Xj</i></sub>, <i>m</i><sub><i>Yj</i></sub>, <i>α</i><sub><i>Xj</i></sub>, <i>α</i><sub><i>Yj</i></sub>分别表示<b><i>m</i></b><sub><i>X</i></sub>, <b><i>m</i></b><sub><i>Y</i></sub>, <i>α</i><sub><i>X</i></sub>, <i>α</i><sub><i>Y</i></sub>的第<i>j</i>维分量值。</p>
                </div>
                <div class="p1">
                    <p id="105">所以在第<i>j</i>维度上, 2个区间数间的距离<i>D</i><sub><i>j</i></sub>=[<i>D</i><sub><i>j</i>min</sub>, <i>D</i><sub><i>j</i>max</sub>], 可以按式 (3) 与式 (4) 计算:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><msub><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mrow><mo>|</mo><mrow><mi>m</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mo>|</mo><mrow><mi>m</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mrow><mo>|</mo><mrow><mi>m</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>D</mi><msub><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo>=</mo><mo stretchy="false">|</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub><mo stretchy="false">|</mo><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>Y</mi><mi>j</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">继而区间数之间的距离可表示为<i>D</i>=[<i>D</i><sub>min</sub>, <i>D</i><sub>max</sub>], 其中</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt><mo>, </mo><mi>D</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">继而得到:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">[</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>, </mo><mi>D</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">]</mo><mo>=</mo><mrow><mo>[</mo><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt><mo>, </mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">由此得到的区间数之间的距离仍是一个区间数, 其比较全面地表示了区间数之间各种可能的距离值。由上述区间数间的距离计算公式可得, <i>n</i>维不确定性数据对象<b><i>M</i></b>, <b><i>N</i></b>间的距离计算方法如式 (6) 所示:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>[</mo><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>, </mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub></mrow><mo>]</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi></mrow><mo>) </mo></mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt><mo>, </mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi></mrow><mo>) </mo></mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">其中, <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Μ</mi><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>k</mi><mi mathvariant="bold-italic">φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi mathvariant="bold-italic">μ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>k</mi><mi mathvariant="bold-italic">φ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">μ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mo>) </mo></mrow><mo>-</mo><mi>k</mi><mi mathvariant="bold-italic">φ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mo>) </mo></mrow><mo>, </mo><mi mathvariant="bold-italic">μ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mo>) </mo></mrow><mo>+</mo><mi>k</mi><mi mathvariant="bold-italic">φ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Q</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mo>) </mo></mrow><mo stretchy="false">]</mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="115">为了可以有效合理地将距离度量与聚类算法结合起来, 并用于区间核心距离与区间可达距离的求取;引入相关系数<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><mrow><mo>{</mo><mrow><mi>λ</mi><mo>∈</mo><mi mathvariant="bold">R</mi><mo stretchy="false">|</mo><mn>0</mn><mo>≤</mo><mi>λ</mi><mo>≤</mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math></mathml>, 把不确定性数据之间距离的最小值与最大值结合起来。那么不确定性数据对象<b><i>M</i></b>, <b><i>N</i></b>之间的距离可表示为:</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mi>λ</mi><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">) </mo><msubsup><mrow></mrow><mrow><mi>min</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>λ</mi><mo stretchy="false">) </mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Μ</mi><mo>, </mo><mi mathvariant="bold-italic">Ν</mi><mo stretchy="false">) </mo><msubsup><mrow></mrow><mrow><mi>max</mi></mrow><mn>2</mn></msubsup></mrow></msqrt><mo>=</mo></mtd></mtr><mtr><mtd><msqrt><mrow><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow><mn>2</mn></msubsup><mo>+</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi>λ</mi></mrow><mo>) </mo></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>D</mi></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow><mn>2</mn></msubsup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">其中, 当<i>λ</i>=0时, 则对应2个不确定性数据间距离的最大值, 此时相关系数最小, 两者相距最远;当<i>λ</i>=1时, 则对应2个不确定性数据间距离的最小值, 此时相关系数最大, 两者相距最近。由此得到的不确定性数据对象间的距离满足非负性和对称性。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>3.3 算法描述</b></h4>
                <div class="p1">
                    <p id="120">在上述分析研究的基础上, 本节提出基于区间数的不确定性数据对象排序识别聚类结构算法<i>UD</i>-<i>OPTICS</i>。算法描述如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="121"><b>算法1</b><i>UD</i>-<i>OPTICS</i>算法</p>
                </div>
                <div class="p1">
                    <p id="122"><b>输入</b>:不确定数据集<i>Q</i>, 区间领域半径<i>ε</i>, 区间阈值<i>MinPts</i>。</p>
                </div>
                <div class="p1">
                    <p id="123"><b>输出</b>:结果序列 (具有区间可达距离等信息的样本点输出排序) 。</p>
                </div>
                <div class="p1">
                    <p id="124"><b>步骤1</b> 运用区间数结合统计值将不确定数据集<i>Q</i>中的每一个不确定性数据对象<b><i>Q</i></b><sub><i>i</i></sub>表示为: <b><i>Q</i></b><sub><i>i</i></sub>=[<i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) -<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) , <i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) +<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) ]。</p>
                </div>
                <div class="p1">
                    <p id="125"><b>步骤2</b> 创建2个队列:有序队列和结果队列。</p>
                </div>
                <div class="p1">
                    <p id="126"><b>步骤3</b> 如果数据集<i>Q</i>中的数据处理完毕, 转步骤8;否则从数据集<i>Q</i>中选取一个未处理且为核心对象的对象<b><i>E</i></b>=[<i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) -<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) , <i>μ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) +<i>kφ</i> (<b><i>Q</i></b><sub><i>i</i></sub>) ], 将对象<b><i>E</i></b>放入结果队列, 并将核心对象<b><i>E</i></b>的区间直接密度可达对象放入有序队列, 所有区间直接密度可达对象按区间可达距离升序排列。</p>
                </div>
                <div class="p1">
                    <p id="127"><b>步骤4</b> 如果有序队列为空, 则跳转到步骤3;否则, 从有序队列中取出第1个对象<b><i>F</i></b>=[<i>μ</i> (<b><i>Q</i></b><sub><i>j</i></sub>) -<i>kφ</i> (<b><i>Q</i></b><sub><i>j</i></sub>) , <i>μ</i> (<b><i>Q</i></b><sub><i>j</i></sub>) +<i>kφ</i> (<b><i>Q</i></b><sub><i>j</i></sub>) ]转至步骤5。</p>
                </div>
                <div class="p1">
                    <p id="128"><b>步骤5</b> 判断<b><i>F</i></b>是否是核心对象, 如果不是, 跳转至步骤4, 如果该对象不在结果队列中则将该对象存入结果队列。</p>
                </div>
                <div class="p1">
                    <p id="129"><b>步骤6</b> 如果对象<b><i>F</i></b>是核心对象, 则将其所有区间直接可达对象放入有序队列, 且将有序队列中的对象按照区间可达距离重新排序;如果即将存入的对象已在有序队列中且新的区间可达距离较小, 则更新该对象的区间可达距离。</p>
                </div>
                <div class="p1">
                    <p id="130"><b>步骤7</b> 重复步骤4, 直到有序队列为空。</p>
                </div>
                <div class="p1">
                    <p id="131"><b>步骤8</b> 输出结果队列, 算法结束。</p>
                </div>
                <div class="p1">
                    <p id="132"><b>定义14</b> (陡峭点<citation id="282" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>) 如果结果队列中的第<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>}</mo></mrow><mo stretchy="false">) </mo></mrow></math></mathml>个对象的区间可达距离小于或等于其后继对象的区间可达距离的1-<i>ξ</i>倍, (<i>ξ</i>∈ (0, 1) ) , 则称此对象为陡峭上升点:</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>U</mi><mi>p</mi><mi>Ρ</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><msub><mrow></mrow><mi>ξ</mi></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>⇔</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>≤</mo></mtd></mtr><mtr><mtd><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">如果结果队列中的第<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>}</mo></mrow><mo stretchy="false">) </mo></mrow></math></mathml>个对象的区间可达距离的1-<i>ξ</i>倍, (<i>ξ</i>∈ (0, 1) ) 大于或等于其后继对象的区间可达距离, 则称此对象为陡峭下降点:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><mi>o</mi><mi>w</mi><mi>n</mi><mi>Ρ</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><msub><mrow></mrow><mi>ξ</mi></msub><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>⇔</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">) </mo><mo>×</mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><mo stretchy="false">) </mo><mo>≥</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138"><b>定义15</b> (陡峭区域<citation id="283" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>) 区间<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>s</mi><mo>, </mo><mi>e</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>是陡峭上升区域, 其必须满足以下条件:<i>s</i>, <i>e</i>必须是陡峭上升点;<i>s</i>和<i>e</i>之间的每个点至少与其前身一样高;<i>I</i>不包含超过<i>MinPts</i>个连续点, 这些点不是陡峭上升点;区间<i>I</i>是包含陡峭上升点的极大区间。陡峭下降区域的定义与陡峭上升区域的定义类似。</p>
                </div>
                <div class="p1">
                    <p id="140"><b>定义16</b> (类簇) 区间<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>s</mi><mo>, </mo><mi>e</mi></mrow><mo>]</mo></mrow><mo>⊆</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>, </mo><mi>m</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>是一个类簇, 则<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>C</mi><mo stretchy="false">) </mo><mo>⇔</mo><mo>∃</mo><mi>D</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>D</mi></msub><mo>, </mo><mi>e</mi><msub><mrow></mrow><mi>D</mi></msub></mrow><mo>]</mo></mrow><mo>, </mo><mi>U</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>U</mi></msub><mo>, </mo><mi>e</mi><msub><mrow></mrow><mi>U</mi></msub></mrow><mo>]</mo></mrow></mrow></math></mathml>且</p>
                </div>
                <div class="p1">
                    <p id="143"> (1) <i>s</i>∈<i>D</i>, <i>e</i>∈<i>U</i>。</p>
                </div>
                <div class="p1">
                    <p id="144"> (2) a. <i>e</i>-<i>s</i>≥<i>MinPts</i>;</p>
                </div>
                <div class="p1">
                    <p id="145">b. max{<i>x</i>|<i>s</i><sub><i>D</i></sub>&lt;<i>x</i>&lt;<i>e</i><sub><i>U</i></sub>}≤<i>RR</i>-<i>D</i> (<i>s</i><sub><i>D</i></sub>) × (1-<i>ξ</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="146">c. max{<i>x</i>|<i>s</i><sub><i>D</i></sub>&lt;<i>x</i>&lt;<i>e</i><sub><i>U</i></sub>}≤<i>RR</i>-<i>D</i> (<i>e</i><sub><i>U</i></sub>) × (1-<i>ξ</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="147"> (3) </p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo> (</mo><mrow><mi>s</mi><mo>, </mo><mi>e</mi></mrow><mo>) </mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false"> (</mo><mi>max</mi><mo stretchy="false">{</mo><mi>x</mi><mo>∈</mo><mi>D</mi><mo stretchy="false">|</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&gt;</mo></mtd></mtr><mtr><mtd><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>e</mi><msub><mrow></mrow><mi>U</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>, </mo><mi>e</mi><msub><mrow></mrow><mi>U</mi></msub><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false">) </mo><mo>×</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><mo stretchy="false">) </mo><mo>≥</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>e</mi><msub><mrow></mrow><mi>U</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>D</mi></msub><mo>, </mo><mi>min</mi><mo stretchy="false">{</mo><mi>x</mi><mo>∈</mo><mi>U</mi><mo stretchy="false">|</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>e</mi><msub><mrow></mrow><mi>U</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><mo stretchy="false">) </mo><mo>≥</mo><mi>R</mi><mi>R</mi><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>D</mi></msub><mo>, </mo><mi>e</mi><msub><mrow></mrow><mi>U</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149"><b>定义17</b> (最大中间值 (<i>MIB</i>) ) 它代表陡峭下降区域末尾区间可达距离和当前序号对应区间可达距离值之间的最大值。全局<i>MIB</i> 值, 指当前序号对应区间可达距离值与陡峭队列中最后一个陡峭区域 (上升或下降) 末尾区间可达距离值的最大值。</p>
                </div>
                <div class="p1">
                    <p id="150">在定义14～定义17的基础上, 可将类簇提取算法描述如算法2所示:</p>
                </div>
                <div class="p1">
                    <p id="151"><b>算法2</b> 类簇提取算法</p>
                </div>
                <div class="p1">
                    <p id="152"><b>输入</b>:算法1的输出。</p>
                </div>
                <div class="p1">
                    <p id="153"><b>输出</b>:带有类簇标记的类簇队列。</p>
                </div>
                <div class="p1">
                    <p id="154"><b>步骤1</b> 创建2个队列:陡峭区域队列和类簇队列, 初始化最大中间值<i>MIB</i>为零。</p>
                </div>
                <div class="p1">
                    <p id="155"><b>步骤2</b> 如果未遍历完毕, 则按顺序遍历算法1的结果队列的区间可达距离;如果遍历完毕, 转至步骤6。</p>
                </div>
                <div class="p1">
                    <p id="156"><b>步骤3</b> 如果是从陡峭下降区域开始: (1) 更新<i>MIB</i>值, 并过滤掉陡峭队列中所有陡峭下降区域开始点乘以<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi>ξ</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>小于全局<i>MIB</i>值的陡峭下降区域; (2) 将该陡峭下降区域添加到陡峭队列中; (3) 将遍历序号设为该陡峭区域结束位置的后一序号, 让<i>MIB</i>值等于该序号对应的区间可达距离; (4) 转至步骤5。</p>
                </div>
                <div class="p1">
                    <p id="158"><b>步骤4</b> 否则, 如果是从陡峭上升区域开始: (1) 执行步骤3中的 (1) 、 (3) ; (2) 将陡峭队列中每一个陡峭下降区域与该陡峭上升区域组合, 判断是否满足定义16中的条件 (1) 、 (2) a、 (2) c, 如果满足, 则利用定义16中的条件 (3) 计算类簇<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi>s</mi><mo>, </mo><mi>e</mi></mrow><mo>]</mo></mrow></mrow></math></mathml>, 标记后将其添加到类簇队列, 转至步骤5;否则, 转至步骤5。</p>
                </div>
                <div class="p1">
                    <p id="160"><b>步骤5</b> 遍历序号后移一位, 转至步骤2。</p>
                </div>
                <div class="p1">
                    <p id="161"><b>步骤6</b> 输出带有类簇标记的类簇队列。</p>
                </div>
                <h3 id="162" name="162" class="anchor-tag"><b>4 实验与分析</b></h3>
                <h4 class="anchor-tag" id="163" name="163"><b>4.1 实验数据</b></h4>
                <div class="p1">
                    <p id="164">为了验证所提算法的有效性, 并在UCI的标准数据集和人工合成数据集上与其他算法进行比较。本文所有算法都是基于C语言编程实现的, 实验在因特尔酷睿i3 2.3 GHz处理器, 4 GB内存的PC上进行。几种常见的用于验证聚类结果的数据集和人工合成数据集如表2所示, 通过在这些数据集上与其他不确定性聚类算法进行比较, 来说明所提算法的有效性。</p>
                </div>
                <div class="area_img" id="165">
                    <p class="img_tit"><b>表2 UCI数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 UCI data set</b></p>
                    <p class="img_note"></p>
                    <table id="165" border="1"><tr><td><br />数据集</td><td>类别数</td><td>属性数</td><td>数据对象数</td></tr><tr><td><br />Iris</td><td>3</td><td>4</td><td>150</td></tr><tr><td><br />Glass</td><td>6</td><td>10</td><td>214</td></tr><tr><td><br />Wine</td><td>3</td><td>13</td><td>178</td></tr><tr><td><br />Letter</td><td>26</td><td>16</td><td>20 000</td></tr><tr><td><br />合成数据集</td><td>4</td><td>2</td><td>1 098 </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="166">由于算法新引入了2个参数, 包含因子<i>k</i>和相关系数<i>λ</i>, 在此, 我们将首先采用控制变量的方式分析<i>k</i>与<i>λ</i>对UD-OPTICS算法的影响。然后分析UD-OPTICS算法的聚类精度与聚类效率与其他不确定性数据聚类算法进行比较, 最后分析本文算法聚类密度分布不均匀的人工合成数据集时的性能。</p>
                </div>
                <div class="p1">
                    <p id="167">准确率与召回率均可以作为聚类结果的评价指标, 但单独使用其中一个指标无法充分反映聚类结果的质量。<i>F</i>-<i>measure</i><citation id="284" type="reference"><link href="243" rel="bibliography" /><link href="261" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">16</a>]</sup></citation>结合了2项评价指标, 可以更加全面地评价聚类结果的质量, 其计算公式如下:<i>F</i>-<i>measure</i>=2<i>PR</i>/ (<i>P</i>+<i>R</i>) , 其中<i>R</i>表示召回率, <i>P</i>表示准确率, <i>F</i>-<i>measure</i>值为[0, 1], <i>F</i>-<i>measure</i>值越大表示聚类质量越好。所以, 本文将采用具有更好评价性能的<i>F</i>-<i>measure</i>指标作为聚类结果的评价标准。</p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>4.2 相关参数<i>λ</i>与包含因子<i>k</i>的选取</b></h4>
                <div class="p1">
                    <p id="169">这里首先控制包含因子<i>k</i>=1, 即在固定的不确定数据对象误差范围的情况下采用不同的<i>λ</i> (<i>λ</i>∈[0, 1]) 值, 使用UD-OPTICS算法来聚类具有不确定性的Iris、Glass、Wine、Letter数据集, 得到<i>F</i>-<i>measure</i>与<i>λ</i>的关系曲线, 如图1所示。</p>
                </div>
                <div class="area_img" id="170">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907023_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 F-measure值与λ的关系曲线" src="Detail/GetImg?filename=images/JSJK201907023_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 <i>F</i>-<i>measure</i>值与<i>λ</i>的关系曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907023_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 <i>F</i>-<i>measure</i> value vs.<i>λ</i></p>

                </div>
                <div class="p1">
                    <p id="171">从图1可以看出, 在包含因子<i>k</i>=1时, 相关参数<i>λ</i>的取值对不同的数据集聚类时的影响是不同的。在<i>λ</i>从0变到1时, 对UD-OPTICS算法聚类具有不确定性的Iris数据集的<i>F</i>-<i>measure</i>值影响较小;在<i>λ</i>=0.5之后, 具有不确定性的Letter数据集的聚类结果的<i>F</i>-<i>measure</i>值开始明显下降, 但<i>λ</i>=0.6时, 仍然具有较高的<i>F</i>-<i>measure</i>值;在<i>λ</i>=0.6之后, 具有不确定性的Wine与Glass数据集的聚类结果的<i>F</i>-<i>measure</i>值开始明显下降;<i>λ</i>∈[0, 0.6]时, UD-OPTICS算法对4个数据集的聚类效果相对稳定, 因此, 建议<i>λ</i>在0～0.6取值。</p>
                </div>
                <div class="p1">
                    <p id="172">包含因子<i>k</i>决定区间数表示不确定性数据范围的大小, 根据误差理论, 当<i>k</i>&gt;3时, 数据是异常值, 所以<i>k</i>在0～3取值。根据上述建议, 这里控制相关参数<i>λ</i>=0.4, 改变<i>k</i>的取值, 使用UD-OPTICS算法对Iris、Glass、Wine、Letter数据集进行聚类, 评估<i>k</i>的取值对UD-OPTICS算法聚类结果的影响。图2给出了聚类结果的<i>F</i>-<i>measure</i>值随包含因子<i>k</i>变化的关系曲线。</p>
                </div>
                <div class="area_img" id="173">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907023_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 F-measure值与k的关系曲线" src="Detail/GetImg?filename=images/JSJK201907023_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>F</i>-<i>measure</i>值与<i>k</i>的关系曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907023_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 <i>F</i>-<i>measure</i> value vs.<i>k</i></p>

                </div>
                <div class="p1">
                    <p id="174">从图2可以看出, <i>k</i>对不同的数据集的影响是不同的。对具有不确定性的Wine数据集基本没有影响;对具有不确定性的Iris数据集影响较小, <i>k</i>在0.5～1.75取值时, 聚类的<i>F</i>-<i>measure</i>值相对比较稳定, 然后随着<i>k</i>的增大, 聚类结果的<i>F</i>-<i>measure</i>值有下降趋势, 但不太明显;对具有不确定性的Letter数据集, 当<i>k</i>在0.5～1.75变化时, 聚类的<i>F</i>-<i>measure</i>值相对稳定, 但是当<i>k</i>值继续增加时, 聚类结果的<i>F</i>-<i>measure</i>值开始明显下降。对具有不确定性的Glass数据集, 当<i>k</i>在0.5～1.25取值时, 聚类的<i>F</i>-<i>measure</i>值相对稳定, 但是当<i>k</i>值继续增加时, 聚类结果的<i>F</i>-<i>measure</i>值开始明显下降。由上述分析可知, 不同数据集应该采用不同的<i>k</i>值, 以使聚类结果最优, 因此在使用UD-OPTICS聚类时, 建议包含因子<i>k</i>在0.5～1.5取值。</p>
                </div>
                <h4 class="anchor-tag" id="175" name="175"><b>4.3 与其他不确定性聚类算法比较</b></h4>
                <div class="p1">
                    <p id="176">为了很好地表示数据的不确定性, 为数据集<i>Q</i>上的每一个<i>d</i>维数据对象<b><i>p</i></b>生成<i>s</i>个样本点, 来表示数据对象<b><i>p</i></b>的不确定性。方法如下:</p>
                </div>
                <div class="p1">
                    <p id="177">为数据对象每个维度选用一个统一的标准差<i>σ</i><sub><i>j</i></sub>且<i>σ</i><sub><i>j</i></sub>∈[<i>u</i>/2, <i>u</i>] (其中<i>u</i>∈[1, 8], 表示标准差上限, <i>j</i>∈[1, <i>d</i>], 以数据对象<b><i>p</i></b>每个属性维度的值为该维度的均值<i>u</i><sub><i>j</i></sub> (<i>j</i>∈[1, <i>d</i>]) ;然后针对每个属性维度, 在区间[<i>u</i><sub><i>j</i></sub>-<i>σ</i><sub><i>j</i></sub>, <i>u</i><sub><i>j</i></sub>+<i>σ</i><sub><i>j</i></sub>]内随机生成其余<i>s</i>-1个数据点, 继而得到不确定性数据对象<b><i>P</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="178">首先, 采用此方法为Iris、Wine、Glass、Letter数据集添加不确定性, 然后, 运用UD-OPTICS、FOPTICS、UID-DBSCAN、UIDK-means、IUK-means等不确定性聚类算法聚类添加有不确定性的Iris、Wine、Glass、Letter 4种数据集, 从聚类结果的<i>F</i>-<i>measure</i>值与运行时间上评估本文算法的性能。5种算法的<i>F</i>-<i>measure</i>值与运行时间如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="179">从表3的实验数据可以看出, 本文所提算法UD-OPTICS在聚类添加了不确定性的Iris、Wine、Glass、Letter数据集时的<i>F</i>-<i>measure</i>平均值为0.872 2, FOPTICS算法聚类的<i>F</i>-<i>measure</i>均值为0.711 8, UID-DBSCAN算法聚类的<i>F</i>-<i>measure</i>均值为0.769 4, UIDK-means算法聚类的<i>F</i>-<i>measure</i>均值为0.783 6, IUK-means算法聚类的<i>F</i>-<i>measure</i>均值为0.792 2。可以看出本文算法的聚类效果比其他几种算法的聚类效果更优, 这个结果得益于所提算法利用区间数更加准确有效地表示了不确定性数据, 且其对噪声和初始参数不敏感, 以这三者为基础使得算法聚类效果更优。而FOPTICS、IUK-means算法均需假设或估计不确定性数据的概率分布, 这便无法很好地描述数据的不确定性, 容易丢失有用信息, 使得聚类效果相对较差;而UID-DBSCAN、UIDK-means算法均对初始参数敏感且UIDK-means算法无法识别噪声, 使得聚类效果相对较差;而所提算法采用区间数表示不确定性数据, 不需要假设不确定性数据的分布, 能够很好地保留数据的有用信息, 继而保证所提算法的聚类质量。</p>
                </div>
                <div class="area_img" id="180">
                                            <p class="img_tit">
                                                <b>表3 算法性能对比结果</b>
                                                    <br />
                                                <b>Table 3 Performance comparison among algorithms</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907023_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJK201907023_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907023_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 算法性能对比结果" src="Detail/GetImg?filename=images/JSJK201907023_18000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="182">就运行时间而言, UD-OPTICS、FOPTICS、UID-DBSCAN 3种算法的理论时间复杂度是同阶的, 均为<i>O</i> (<i>n</i><sup>2</sup>) , UIDK-means、IUK-means算法的理论时间复杂度为<i>O</i> (<i>tKn</i>) , 其中, <i>t</i>表示迭代次数, <i>K</i>表示类簇个数, <i>n</i>表示对象数。由表3的运行时间数据可以看出, UD-OPTICS算法在聚类时, 相对于其他2种算法具有较少的处理时间。在处理不确定性Iris、Wine、Glass、Letter数据集时, 本文算法的处理时间分别为FOPTICS的27.12%、26.21%、25.28%、50.20%, 为UID-DBSCAN算法的51.33%、58.25%、57.34%、72.96%, 为UIDK-means算法的52.52%、56.51%、73.32%、71.61%, 为IUK-means算法的29.94%、29.28%、27.84%、58.53%。所提算法的运行时间上更短, 得益于利用区间数表示不确定性数据更加简单, 不确定性数据对象间的距离计算采用只需要进行加法与乘法运算的区间数间的距离计算方法, 因此有效缩短了运算时间。而FOPTICS算法采用概率密度函数表示不确定性数据, 使用多重积分的模糊距离度量不确定性数据对象间的相似度, 使得计算量较大。IUK-means算法在不确定性数据表示时需运用快速高斯变换的核密度估计方法, 构建不确定数据模型, 在相似性度量时它需要结合数据对象属性与不确定分布2方面的特征, 这都使得运算时间变长。对于UID-DBSCAN算法而言, 其对初始参数敏感, 该算法为了可以自适应地选取初始参数, 不得不在处理初始参数选取时, 承担较昂贵的计算代价。</p>
                </div>
                <h4 class="anchor-tag" id="183" name="183"><b>4.4 密度不均匀的人工合成数据验证</b></h4>
                <div class="p1">
                    <p id="184">图3显示了具有非常典型的真实世界数据集特征的可达性图的一个例子。为了更好地体现数据的密度不均匀, 继而比较实际分布与对象的集群排序, 数据集合是在2个维度上人工合成的, 并为该数据集添加了不确定性, 其中噪声标准差上限值<i>u</i>=3, 运用UD-OPTICS算法聚类该数据集, 其数据对象分布与对应的可达序列图如图3所示。由图3可知, UD-OPTICS算法没有同UID-DBSCAN算法一样的全局密度阈值 (图中是可达性图中的水平线) , UD-OPTICS算法可以更好地揭示数据集中的所有聚类结构。从图3还可以看出, 左边二维数据集中分布的所能观察到的不同密度的类簇 (在可达性图中就如箭头所指向的波谷区域) , 清晰明了。</p>
                </div>
                <div class="area_img" id="185">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907023_185.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 密度不均匀数据对象分布及其可达序列图" src="Detail/GetImg?filename=images/JSJK201907023_185.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 密度不均匀数据对象分布及其可达序列图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907023_185.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Distribution data objects of uneven density  and its reachable sequence diagram</p>

                </div>
                <div class="p1">
                    <p id="186">接下来采用不同的噪声标准差上限值<i>u</i>来为人工合成数据集添加不确定性, 比较UD-OPTICS、FOPTICS、UID-DBSCAN、UIDK-means、IUK-means算法聚类该数据集的聚类效果, 得聚类<i>F</i>-<i>measure</i>值随噪声标准差上限值<i>u</i>的变化曲线如图4所示。</p>
                </div>
                <div class="area_img" id="187">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907023_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同聚类算法聚类F-measure值随u的变化曲线" src="Detail/GetImg?filename=images/JSJK201907023_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同聚类算法聚类<i>F</i>-<i>measure</i>值随<i>u</i>的变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907023_187.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 <i>F</i>-<i>measure</i> value vs.<i>u</i> for different algorithms</p>

                </div>
                <div class="p1">
                    <p id="188">从图4可以看出, 在噪声较小时, FOPTICS算法与本文提出的UD-OPTICS算法的聚类<i>F</i>-<i>measure</i>值相近, 而UID-DBSCAN、UIDK-means、IUK-means算法的聚类<i>F</i>-<i>measure</i>值已经明显低于UD-OPTICS的, 随着噪声的加大, 所列算法的聚类<i>F</i>-<i>measure</i>也都随之下降。但是, 在同等噪声条件下, UD-OPTICS算法的聚类<i>F</i>-<i>measure</i>值均高于其他算法的聚类<i>F</i>-<i>measure</i>值。这是由于UD-OPTICS算法使用区间数结合不确定性数据的统计值来更加合理地表示了不确定性数据, 并采用区间数间的距离计算方式更好地度量了不确定数据间的距离, 且算法对初始参数也不敏感, 使用对象排序识别聚类结构, 能够保证良好的聚类效果。UD-OPTICS算法的聚类<i>F</i>-<i>measure</i>值最高高出FOPTICS算法聚类<i>F</i>-<i>measure</i>值0.234 9, 平均高出0.125 2;最高高出UID-DBSCAN算法聚类<i>F</i>-<i>measure</i>值0.330 9, 平均高出0.270 1;最高高出UIDK-means算法聚类<i>F</i>-<i>measure</i>值0.108 0, 平均高出0.107 5;最高高出IUK-means算法聚类<i>F</i>-<i>measure</i>值0.420 9, 平均高出0.322 2。当噪声达到一定程度后所列算法的聚类效果都严重下降。</p>
                </div>
                <h3 id="189" name="189" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="190">本文以属性级不确定性数据为研究对象, 针对现有算法因假设与实际情况难以获取的概率分布的方式导致聚类质量不佳且计算复杂度较高, 以及不能很好地发现任意密度类簇的缺陷, 运用区间数理论结合均值与标准差合理地表示不确定性数据, 并采用低计算复杂度的区间数距离计算方法来计算不确定性数据间的距离, 提出不确定性数据聚类算法UD-OPTICS。通过仿真实验表明, 在5种数据集上, 本文算法聚类<i>F</i>-<i>measure</i>值比FOPTICS算法聚类的<i>F</i>-<i>measure</i>值平均高出0.142 8, 比UID-DBSCAN算法的平均高出0.186 4, 比UIDK-means算法的平均高出0.098 1, 比IUK-means算法的平均高出0.201 1。由于现今数据量迅速增长, 传统的在单一计算机上运行的算法已经无法应对大规模数据聚类, 因此在未来可研究实现本文算法的并行化。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="287" type="formula" href="images/JSJK201907023_28700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">吴翠先</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="213">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201703012&amp;v=MDc2MDFyQ1VSTE9lWmVSbUZ5N21XNzdLTVRYVGJMRzRIOWJNckk5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Chi Rong-hua, Cheng Yuan, Zhu Su-xia, et al.Uncertain data analysis algorithm based on fast Gaussian transform[J].Journal of Communications, 2017, 38 (3) :101-111. (in Chinese) 
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on uncertain data mining based on interval number and its application">

                                <b>[2]</b> Ren Shi-jin.Research on uncertain data mining based on interval number and its application[D].Hangzhou:Zhejiang University, 2006. (in Chinese) 
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interval analysis method for static response and eigenvalue problems of uncertain parameter structures">

                                <b>[3]</b> Qiu Zhi-ping.Interval analysis method for static response and eigenvalue problems of uncertain parameter structures[D].Jilin:Jilin University of Technology, Jilin University, 1994. (in Chinese) 
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201705037&amp;v=MDk4NjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XNzdLTHlITWQ3RzRIOWJNcW85R1k0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Li Jia-fei, Sun Xiao-yu.Clustering method for uncertain data based on spectral decomposition[J].Journal of Jilin University (Engineering and Technology Edition) , 2017, 47 (5) :1604-1611. (in Chinese) 
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical density based clustering of uncertain data">

                                <b>[5]</b> Kriegel H P, Pfeifle M.Hierarchical density-based clustering of uncertain data[C]//Proc of IEEE International Conference on Data Mining, 2005:27-30.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Density-based clustering of uncertain data">

                                <b>[6]</b> Kriegel H P, Pfeifle M.Density-based clustering of uncertain data[C]//Proc of the 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, 2005:672-677.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Uncertain data mining:An example in clustering location data">

                                <b>[7]</b> Chau M, Cheng R, Kao B, et al.Uncertain data mining:An example in clustering location data[C]//Proc of Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2006:199-204.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering uncertain data using voronoi diagrams">

                                <b>[8]</b> Kao B, Lee S D, Cheung D W, et al.Clustering uncertain data using voronoi diagrams[C]//Proc of the 8th IEEE International Conference on Data Mining, 2008:333-342.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Approximation Algorithms for Clustering Uncertain Data">

                                <b>[9]</b> Cormode G, Mcgregor A.Approximation algorithms for clustering uncertain data[C]//Proc of the 27th ACM Sigmod-Sigact-Sigart Symposium on Principles of Database Systems, 2008:191-200.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201106000&amp;v=MjA2NDRiTEc0SDlETXFZOUZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdtVzc3S1BEelQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Peng Yu, Luo Qing-hua, Peng Xi-yuan.UIDK-means:A multi-dimensional uncertain measurement data clustering algorithm[J].Chinese Journal of Scientific Instrument, 2011, 32 (6) :1201-1207. (in Chinese) 
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201606054&amp;v=MjQwNzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdtVzc3S0x6N0JiN0c0SDlmTXFZOUFZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> He Yun-bin, Zhang Zhi-chao, Wan Jing, et al.Research for uncertain data clustering algorithm:U-PAM and UM-PAM algorithm[J].Computer Science, 2016, 43 (6) :263-269. (in Chinese) 
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S2095&amp;v=MTc4MzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XNzdLTHo3QmI3RzRIOWF2clk5TVk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Wei Fang-yuan, Huang De-cai.UID-DBSCAN clustering algorithm of multi-dimensional uncertain data based on interval number[J].Computer Science, 2017, 44 (11A) :442-447. (in Chinese) 
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interval number decision set pair analysis">

                                <b>[13]</b> Liu Xiu-mei, Zhao Ke-qin.Interval number decision set pair analysis[M].Beijing:Science Press, 2014. (in Chinese) 
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Error in digital computation">

                                <b>[14]</b> Moore R E.Error in digital computation, Volume 1[M].New York:John Wiley &amp; Sons, 1965.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000068983&amp;v=MDE2MzZZN0s3SHRqTnI0OUZaTzBIQlhRNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsMGNheEE9TmlmSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Ankerst M.OPTICS:Ordering points to identify the clustering structure[J].ACM Sigmod Record, 1999, 28 (2) :49-60.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 Luo Yan-fu, Qian Xiao-dong.Uncertain data clustering algorithm based on local density[J].Data Analysis &amp; Knowledge Discovery, 2017, 12:84-91. (in Chinese) 
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 迟荣华, 程媛, 朱素霞, 等.基于快速高斯变换的不确定数据聚类算法[J].通信学报, 2017, 38 (3) :101-111.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2007079077.nh&amp;v=MTAyMzlHRnJDVVJMT2VaZVJtRnk3bVc3N0tWMTI3R2JPL0Y5SExxSkViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 任世锦.基于区间数的不确定性数据挖掘及其应用研究[D].杭州:浙江大学, 2006.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 邱志平.不确定参数结构静力响应和特征值问题的区间分析方法[D].吉林:吉林工业大学, 吉林大学, 1994.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 李嘉菲, 孙小玉.基于谱分解的不确定数据聚类方法[J].吉林大学学报 (工学版) , 2017, 47 (5) :1604-1611.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 彭宇, 罗清华, 彭喜元.UIDK-means:多维不确定性测量数据聚类算法[J].仪器仪表学报, 2011, 32 (6) :1201-1207.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 何云斌, 张志超, 万静, 等.不确定数据聚类的U-PAM算法和UM-PAM算法的研究[J].计算机科学, 2016, 43 (6) :263-269.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 魏方圆, 黄德才.基于区间数的多维不确定性数据UID-DBSCAN聚类算法[J].计算机科学, 2017, 44 (11A) :442-447.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787030419934000&amp;v=MzA4NzdEZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdnRVN25MS0Y4V1hGcXpHYk83SHRYTnBvWkdZT3NQREJNOHp4VVNt&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 刘秀梅, 赵克勤.区间数决策集对分析[M].北京:科学出版社, 2014.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201712009&amp;v=MDExMjVMT2VaZVJtRnk3bVc3N0tQU25mZjdHNEg5Yk5yWTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 罗彦福, 钱晓东.基于局部密度的不确定数据聚类算法[J].数据分析与知识发现, 2017, 12:84-91.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201907023" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907023&amp;v=MjI5MDdlWmVSbUZ5N21XNzdLTHo3QlpiRzRIOWpNcUk5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
