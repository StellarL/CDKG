<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132364031592500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201908001%26RESULT%3d1%26SIGN%3dmDNfBc%252bMmRAOG768%252boN1pQK3ucI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908001&amp;v=MDQyMjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xVYnpLTHo3QlpiRzRIOWpNcDQ5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;2 Warp部分重组&lt;/b&gt; "><b>2 Warp部分重组</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;2.1 Warp重组开销分析&lt;/b&gt;"><b>2.1 Warp重组开销分析</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;2.2 部分Warp重组基本思想&lt;/b&gt;"><b>2.2 部分Warp重组基本思想</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;3 通用的部分重组框架&lt;/b&gt; "><b>3 通用的部分重组框架</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;4 性能测试与分析&lt;/b&gt; "><b>4 性能测试与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="&lt;b&gt;4.1 基准程序&lt;/b&gt;"><b>4.1 基准程序</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;4.2 测试结果&lt;/b&gt;"><b>4.2 测试结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="图1 Non-Aware重组与Lane-Aware重组的区别">图1 Non-Aware重组与Lane-Aware重组的区别</a></li>
                                                <li><a href="#48" data-title="图2 Warp访存示例">图2 Warp访存示例</a></li>
                                                <li><a href="#54" data-title="图3 部分重组的基本思想">图3 部分重组的基本思想</a></li>
                                                <li><a href="#57" data-title="图4 部分重组通用框架">图4 部分重组通用框架</a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;表1 实验用GPU配置参数&lt;/b&gt;"><b>表1 实验用GPU配置参数</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;表2 基准程序特征统计表&lt;/b&gt;"><b>表2 基准程序特征统计表</b></a></li>
                                                <li><a href="#83" data-title="图5 基准程序特征统计">图5 基准程序特征统计</a></li>
                                                <li><a href="#86" data-title="图6 不同重组方法下Warp数量的比较">图6 不同重组方法下Warp数量的比较</a></li>
                                                <li><a href="#88" data-title="图7 不同重组方法下寄存器体冲突的比较">图7 不同重组方法下寄存器体冲突的比较</a></li>
                                                <li><a href="#90" data-title="图8 不同重组方法下访存冲突的比较">图8 不同重组方法下访存冲突的比较</a></li>
                                                <li><a href="#92" data-title="图9 不同重组方法下加速比的比较">图9 不同重组方法下加速比的比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Jin X, Daku B, Ko S-B.Improved GPU SIMD control flow efficiency via hybrid warp size mechanism [J].Microprocessors and Microsystems, 2014, 38 (7) :717-729." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700140922&amp;v=MTA1NDNPZmJLOEg5RE1xSTlGWmU4UEJYNDdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDRXYVJBPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Jin X, Daku B, Ko S-B.Improved GPU SIMD control flow efficiency via hybrid warp size mechanism [J].Microprocessors and Microsystems, 2014, 38 (7) :717-729.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Bakhoda A, Yuan G L, Fung W W L, et al.Analyzing CUDA workloads using a detailed GPU simulator [C]//Proc of 2009 IEEE International Symposium on Performance Analysis of Systems and Software, 2009:163-174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analyzing CUDA Workloads Using a Detailed GPU Simulator">
                                        <b>[2]</b>
                                         Bakhoda A, Yuan G L, Fung W W L, et al.Analyzing CUDA workloads using a detailed GPU simulator [C]//Proc of 2009 IEEE International Symposium on Performance Analysis of Systems and Software, 2009:163-174.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Lashgar A, Khonsari A, Baniasadi A.Harp:Harnessing inactive threads in many-core processors [J].ACM Transactions on Embedded Computing Systems, 2014, 13 (3s) :1-25." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM14041100001418&amp;v=MDA0ODE3SzhIdFhOcm85RlpPc09DSDB4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0V2FSQT1OaWZJWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Lashgar A, Khonsari A, Baniasadi A.Harp:Harnessing inactive threads in many-core processors [J].ACM Transactions on Embedded Computing Systems, 2014, 13 (3s) :1-25.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Meng J, Tarjan D, Skadron K.Dynamic warp subdivision for integrated branch and memory divergence tolerance [J].ACM SIGARCH Computer Architecture News, 2010, 38 (3) :235-246." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000014243&amp;v=MzI3NThadUh5am1VTGZJSmw0V2FSQT1OaWZJWTdLN0h0ak5yNDlGWk9vTERuZzZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Meng J, Tarjan D, Skadron K.Dynamic warp subdivision for integrated branch and memory divergence tolerance [J].ACM SIGARCH Computer Architecture News, 2010, 38 (3) :235-246.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Rhu M, Erez M.Maximizing SIMD resource utilization in GPGPUs with SIMD lane permutation [J].ACM Sigarch Computer Architecture News, 2013, 41 (3) :356-367." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000011042&amp;v=MjM1MjRhUkE9TmlmSVk3SzdIdGpOcjQ5RlpPb09ESGc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0Vw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Rhu M, Erez M.Maximizing SIMD resource utilization in GPGPUs with SIMD lane permutation [J].ACM Sigarch Computer Architecture News, 2013, 41 (3) :356-367.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Yu L, Tang X, Wu M, et al.Improving branch divergence performance on GPGPU with a new PDOM stack and multi-level warp scheduling [J].Journal of Systems Architecture, 2014, 60 (5) :420-430." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600490799&amp;v=MTQ3MjZkR2VycVFUTW53WmVadUh5am1VTGZJSmw0V2FSQT1OaWZPZmJLOEh0RE1xWTlGWU9JUEMzVXdvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Yu L, Tang X, Wu M, et al.Improving branch divergence performance on GPGPU with a new PDOM stack and multi-level warp scheduling [J].Journal of Systems Architecture, 2014, 60 (5) :420-430.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" ElTantawy A, Ma J W, Connor M O, et al.A scalable multi-path microarchitecture for efficient GPU control flow [C]//Proc of 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA) , 2014:248-259." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A scalable multi-path microarchitecture for efficient GPU control flow&amp;quot;">
                                        <b>[7]</b>
                                         ElTantawy A, Ma J W, Connor M O, et al.A scalable multi-path microarchitecture for efficient GPU control flow [C]//Proc of 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA) , 2014:248-259.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Rhu M, Erez M.The dual-path execution model for efficient GPU control flow [C]//Proc of 2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA) , 2013:591-602." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;The Dual-Path Execution Model for Efficient GPU Control Flow&amp;quot;">
                                        <b>[8]</b>
                                         Rhu M, Erez M.The dual-path execution model for efficient GPU control flow [C]//Proc of 2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA) , 2013:591-602.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Vaidya A S, Shayesteh A, Dong H W, et al.SIMD divergence optimization through intra-warp compaction [M].ACM SIGARCH Computer Architecture News, 2013, 41 (3) :368-379." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000011043&amp;v=MTI0MzFtVUxmSUpsNFdhUkE9TmlmSVk3SzdIdGpOcjQ5RlpPb09ESGc2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Vaidya A S, Shayesteh A, Dong H W, et al.SIMD divergence optimization through intra-warp compaction [M].ACM SIGARCH Computer Architecture News, 2013, 41 (3) :368-379.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Fung W W L, Sham I, Yuan G, et al.Dynamic warp formation and scheduling for efficient GPU control flow [C]//Proc of the 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007) , 2007:407-420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic warp formation and scheduling for efficient GPU control flow">
                                        <b>[10]</b>
                                         Fung W W L, Sham I, Yuan G, et al.Dynamic warp formation and scheduling for efficient GPU control flow [C]//Proc of the 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007) , 2007:407-420.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Fung W W L, Aamodt T M.Thread block compaction for efficient SIMT control flow [C]//Proc of 2011 IEEE 17th International Symposium on High Performance Computer Architecture, 2011:25-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Thread block compaction for efficient SIMT control flow">
                                        <b>[11]</b>
                                         Fung W W L, Aamodt T M.Thread block compaction for efficient SIMT control flow [C]//Proc of 2011 IEEE 17th International Symposium on High Performance Computer Architecture, 2011:25-36.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Narasiman V, Shebanow M, Lee C J, et al.Improving GPU performance via large warps and two-level warp scheduling [C]//Proc of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, 2011:308-317." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving GPU Performance via Large Warps and Two-Level Warp Scheduling">
                                        <b>[12]</b>
                                         Narasiman V, Shebanow M, Lee C J, et al.Improving GPU performance via large warps and two-level warp scheduling [C]//Proc of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, 2011:308-317.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Malits R, Bolotin E, Kolodny A, et al.Exploring the limits of GPGPU scheduling in control flow bound applications [J].ACM Transactions on Architecture and Code Optimization, 2012, 8 (4) :1-22." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004202&amp;v=MTIwNTZEbnc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0V2FSQT1OaWZJWTdLN0h0ak5yNDlGWk9zTA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Malits R, Bolotin E, Kolodny A, et al.Exploring the limits of GPGPU scheduling in control flow bound applications [J].ACM Transactions on Architecture and Code Optimization, 2012, 8 (4) :1-22.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Brunie N, Collange S, Diamos G.Simultaneous branch and warp interweaving for sustained GPU performance [C]//Proc of 2013 39th Annual International Symposium on Computer Architecture, 2013:49-60." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous branch and warp interweaving for sustained GPU performance">
                                        <b>[14]</b>
                                         Brunie N, Collange S, Diamos G.Simultaneous branch and warp interweaving for sustained GPU performance [C]//Proc of 2013 39th Annual International Symposium on Computer Architecture, 2013:49-60.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(08),1335-1342 DOI:10.3969/j.issn.1007-130X.2019.08.001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>通过部分Warp重组消除GPGPU控制流的不一致性</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B2%88%E7%AB%8B&amp;code=20871192&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">沈立</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E8%80%80%E5%8D%8E&amp;code=20297514&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨耀华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BF%97%E8%8B%B1&amp;code=20250881&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王志英</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0269230&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国防科技大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>GPU已被广泛应用于当前的高性能计算系统中, 但其性能却受到程序运行时不同控制流方向的严重制约。这一问题通常通过动态Warp重组技术来解决, 即将一个或多个Warp内沿相同控制流执行的线程组合在一起, 构成一个新的Warp。但是, 这类方法普遍存在一些不必要的重组, 引入了较大的额外性能开销。分析了线程重组的性能开销, 并提出了一种称作“部分重组”的性能优化方法。这种方法在保证重组效率的前提下, 避免了对包含活跃线程数量较多的Warp的重组, 从而有效减少了线程重组引入的性能开销。测试结果表明, 部分重组能够在保证重组效率的前提下带来较为明显的性能提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=GPGPU&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">GPGPU;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8E%A7%E5%88%B6%E6%B5%81%E4%B8%8D%E4%B8%80%E8%87%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">控制流不一致;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Warp%E9%87%8D%E7%BB%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Warp重组;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A1%86%E6%9E%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">框架;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    沈立 (1976-) , 男, 四川成都人, 博士, 教授, 研究方向为多核/众核体系结构、运行时和编译优化、高性能计算。E-mail:lishen@nudt.edu.cn 通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                                <span>
                                    杨耀华 (1987-) , 男, 山东临沂人, 硕士生, 研究方向为多核/众核体系结构。E-mail:yangyaohua@nudt.edu.cn 通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                                <span>
                                    王志英 (1956-) , 男, 山西长治人, 博士, 教授, 研究方向为高性能处理器体系结构。E-mail:zywang@nudt.edu.cn 通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61472431);</span>
                    </p>
            </div>
                    <h1><b>Eliminating control divergence on GPGPU via partial warp regrouping</b></h1>
                    <h2>
                    <span>SHEN Li</span>
                    <span>YANG Yao-hua</span>
                    <span>WANG Zhi-ying</span>
            </h2>
                    <h2>
                    <span>School of Computer, National University of Defense Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>GPUs have been widely used in current high-performance computing systems. However, their performance is severely constrained by the different directions of control flow during runtime. In response to this problem, warp regrouping methods are generally applied to combine the threads that execute the same branch path within one or more warps, thus obtaining a new warp. However, some unnecessary reorganization existing in these methods introduces additional performance overheads. We analyze the sources of regrouping overhead and propose a partial warp regrouping approach. Under the premise of ensuring certain efficiency, it reduces the reorganization of warps with a large number of active threads so as to avoid performance overhead. Experimental results indicate that the proposed method can significantly reduce unnecessary overheads while ensuring regrouping efficiency.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=GPGPU&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">GPGPU;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=control%20divergence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">control divergence;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=warp%20regrouping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">warp regrouping;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=framework&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">framework;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    SHEN Li, born in 1976, PhD, professor, his research interests include multi/many-core architecture, runtime and compilation optimization, and high performance computing.Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                                <span>
                                    YANG Yao-hua, born in 1987, MS candidate, his research interest includes multi/many-core architecture.Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                                <span>
                                    WANG Zhi-ying, born in 1956, PhD, professor, his research interest includes high performance processor architecture.Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="31" name="31" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="32">图形处理器GPU (Graphical Processing Unit) 已被广泛地应用于当前的高性能计算系统中, 显著提升了系统性能。但是, 应用程序中普遍存在的控制流不一致性 (Control Divergence) 会严重降低GPU中计算资源的利用率以及GPU的性能。文献<citation id="95" type="reference">[<a class="sup">1</a>]</citation>对ispass2009-benchmarks基准程序包<citation id="96" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>进行了测试, 统计了测试程序中包含不同活跃线程数量的Warp的分布情况以及程序中分支指令所占的比重, 发现很多基准程序 (如BFS, NN, MUM, LPS和NQU) 中存在着大量的线程束 (Warp) 没有满线程执行, 同时还存在着相对较多的分支指令。这种现象在通用应用中非常普遍, 文献<citation id="97" type="reference">[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</citation>也得出了类似的结论, 这说明应用程序很容易受到Control Divergence的影响并导致计算资源空闲。因此, 消除分支指令带来的影响对提升程序的性能具有重要的意义。</p>
                </div>
                <div class="p1">
                    <p id="33">针对GPU上控制流不一致性引起的性能损失问题, 目前主要存在3种不同的优化方式, 分别是:基于栈的重汇聚机制、多路交替执行机制DWS (Dynamic Warp Subdivision) 和Warp重组技术。基于栈的重汇聚机制PDOM (Post-DOMinator) 是目前优化Control Divergence最传统、最通用的方法, 已被广泛应用在商用GPU中。它为每个Warp维护一个重汇聚栈, 能够在串行执行完每条分支路径后将线程重新汇聚到它们所属的原始Warp中, 而不会一直串行执行到线程结束。PDOM利用栈结构的特性区分不同层次的分支, 从而有效解决了嵌套分支导致SIMD利用率进一步下降的问题。虽然PDOM在一定程度上减少了GPU计算资源的浪费, 但它仍具有很大的性能提升空间, 因为当遇到Control Divergence时, Warp中仍然只有一部分线程在执行, 计算资源并没有得到充分利用。</p>
                </div>
                <div class="p1">
                    <p id="34">多路交替执行机制的基本思想是改变目前传统分支路径串行执行的方式, 它尽可能地让沿着不同控制流执行的线程交替执行, 而不像PDOM那样, 一个分支路径必须在另一个分支路径执行完成之后才开始执行。Meng等人<citation id="98" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>最先提出了多路交替执行的优化策略DWS。之后Yu等人<citation id="99" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和ElTantawy等人<citation id="100" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>也分别通过不同的方法实现了多个子Warp的交替执行, 他们分别提出了PDOM-ASI (PDOM with All Sub-warps Issuable) 栈结构和MP IPDOM (Multi-Path Immediate PDOM) 机制。Rhu等人<citation id="101" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>则提出一种双通道结构 DPE (Dual-Path Execution) , 修改了重汇聚栈的结构, 使其仅支持2条分支路径上的交替执行。不过这种方法往往只在活跃Warp数量有限的情况下才获得性能收益, 对于那些线程数量足够多的应用, 完全可以通过调度执行其它Warp来隐藏延迟。此外, 采用这些技术, 在某些时刻Warp内仍然只有部分线程在执行, GPU资源的利用率依然比较低。</p>
                </div>
                <div class="p1">
                    <p id="35">Warp重组的基本思想则是当执行分支指令产生控制流不一致时, 将执行相同分支路径的多个线程重新组合在一起, 构成一个新的Warp, 以提高某一分支路径上并发执行的线程数, 从而提高GPU的吞吐率。Vaidya等人<citation id="102" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了Warp内的线程重组机制BCC (Basic Cycle Compaction) 和SCC (Swizzled Cycle Compaction) , 利用GPU中SIMD物理通道数比Warp内线程数少的特征对Warp内的线程进行“压缩”, 减少了执行Warp所需的时钟周期数, 并且可以在一定程度上避免破坏数据的局部性。Fung等人<citation id="103" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在2007年提出的 DWF (Dynamic Warp Formation) <citation id="104" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和TBC (Thread Block Compaction) <citation id="105" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation><sup></sup>是最早的Warp间重组技术。DWF会在每个时钟周期检查不同Warp间执行相同指令且具有相同分支路径的线程, 并将其重新组合在一起以提高某一分支路径上并行执行的线程数。TBC则将重组范围缩小到一个线程块内。类似地, Narasiman等人<citation id="106" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了“大线程束 (Large Warp) ”的概念, 它是由若干个Warp组成的一个自定义的线程块, 重组时在这个自定义的线程块中选择固定数量的线程组成新Warp。Malits等人<citation id="107" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>发现一些应用由于访存特性的影响并不能有效地利用共享内存, 为了进一步提升重组的效率, 将重组的范围扩展到流多处理器SM (Stream Multiprocessor) 之间。Brunie等人<citation id="108" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>则将Warp内和Warp间2种方式结合起来, 提出了SBI (Simultaneous Branch Interweaving) 与 SWI (Simultaneous Warp Interweaving) 的方式并且将重组扩展到执行不同指令的Warp之间, 取得了不错的效果, 但却大大增加了控制逻辑的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="36">本文首先分析了Warp重组引入的开销类型并且对这些开销进行了量化评估。我们发现, 由于引入了额外的寄存器访问冲突或存储访问操作, 当对包含活跃线程数量较多的Warp进行重组时往往会产生不必要的开销, 限制重组对性能的提升。为此, 本文提出了一种轻量级的重组方法——部分重组, 通过设置阈值的方式来避免过度重组引入的开销, 测试结果表明该方法能够带来平均12%的性能提升。本文还设计了一种通用的部分Warp重组框架, 该框架能够很容易地集成到现有的GPU中。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>2 Warp部分重组</b></h3>
                <div class="p1">
                    <p id="38">传统的Warp重组策略虽然能够较好地提升线程级并行, 但并不是所有的重组都是有效的, 而且重组在硬件开销上也不容忽视。本节首先对现有的重组机制进行分析, 确定影响性能的主要开销, 然后在此基础上提出Warp部分重组策略。</p>
                </div>
                <h4 class="anchor-tag" id="39" name="39"><b>2.1 Warp重组开销分析</b></h4>
                <h4 class="anchor-tag" id="40" name="40"> (1) 寄存器访问。</h4>
                <div class="p1">
                    <p id="41">为了尽可能地减少硬件开销并保证每个线程都可以分配到一定数量的寄存器, GPU通常将寄存器组织为多个体 (bank) , 并在编译时静态地将每个线程分配到固定的流水线上执行, 这个线程所访问的寄存器体也相应地确定了下来。对于那些Warp内的线程直到执行结束都不改变执行通道的情形, 这种方案是比较合适的。但在进行线程重组的时候, 每个线程所在的SIMD通道可能改变, 造成2个或者多个线程访问同一个寄存器体的现象, 导致寄存器体冲突。这种寄存器体冲突会引起Warp执行的停顿, 从而降低性能。</p>
                </div>
                <div class="p1">
                    <p id="42">针对这一问题, 人们提出了Lane-Aware重组方式<citation id="109" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 它在重组之前对2个Warp内的线程进行检测, 如果存在寄存器冲突, 这2个线程将不再参与重组。相应地, 不进行检测的重组被称为Non-Aware重组。</p>
                </div>
                <div class="area_img" id="43">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Non-Aware重组与Lane-Aware重组的区别" src="Detail/GetImg?filename=images/JSJK201908001_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Non-Aware重组与Lane-Aware重组的区别  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Defference between  Non-Aware and Lane-Aware</p>

                </div>
                <div class="p1">
                    <p id="44">如图1所示 (其中, W0、W1为重组前的Warp, WX和WY为重组后的Warp) , 在None-Aware重组中, 只要有空闲的SIMD通道, 便可将线程映射到该通道上执行, 所以在图1b中, 线程4和线程6可以合并到WX中, 构成一个完整的Warp。相反地, 在图1c中, 在对WX和WY进行重组之前, GPU通过硬件对每个线程进行检测, 发现线程0和线程4被映射到了相同的执行通道上, 若将2个线程重组到一起会产生寄存器体冲突。为了避免这种情况的发生, GPU将不再把线程4重组进WX, 而是保持在原来的位置上。而对线程6来说, 它与线程0和线程1访问的寄存器体各不相同, 所以将它们重组在一起不会引入任何寄存器体冲突。从上述分析可以看出, Lane-Aware重组会拆分原本属于同一个Warp的线程, 因而可能破坏原有的数据局部性。同时, 这种方法还大大地增加了硬件设计的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="45">一般说来, None-Aware重组并没有减少Warp的总数, 因此线程级并行的提升很有限, 但是它会引入额外的寄存器体冲突。由于这些冲突往往是不可避免的, 因而会限制甚至降低程序的性能。与之对应的是, Lane-Aware的重组更倾向于消除寄存器体冲突, 这会在很大程度上提高重组的效率。而在另一方面, Lane-Aware重组相对于None-Aware重组增加了额外的体冲突检测机制, 所以在硬件的设计上会变得更复杂。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46"> (2) 访存请求合并。</h4>
                <div class="p1">
                    <p id="47">现今的GPU中都集成了多级存储层次, 位于片外的全局存储器容量最大, 但访存延迟也是最大的。为了确保GPU对数据的高效访问, 维持较高的吞吐率, GPU提供了访存合并机制<citation id="110" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 即如果一个Warp内的线程对内存进行连续对齐的一对一访问, 那么这些线程的访存请求就可以合并起来发送到L1 Cache, 只需要一个访存事务便可获得各个线程需要的全部数据。然而, 如果Warp内多个线程的访问地址不连续, 那么就需要多个访存事务来完成这些访存请求。对于每一个访存请求, 如果命中L1 Cache, 则被请求的数据将会被直接送回执行单元;如果没有命中, 这个请求将被送到更低一级的存储层次中。随着存储层次的降低, 访存的延迟也变得更大。对于一个Warp来说, 它只有在所有访问存储器的请求被满足之后才可以被调度, 所以当访存地址不连续时, Warp可能需要更长的等待时间。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Warp访存示例" src="Detail/GetImg?filename=images/JSJK201908001_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Warp访存示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 An example of warp memory access</p>

                </div>
                <div class="p1">
                    <p id="49">如图2所示, Warp 1中每个线程要访问的数据均位于同1个Cache块内, 因此只需要1次读取Cache便可获得所有线程需要的全部数据。相反, 对于Warp 2, 4个线程所需的数据分别位于3个Cache块中, 这就必须由3个访存请求来读取。若其中某个请求要访问的数据没有被缓存, 则该请求只能继续访问L2 Cache, 在此期间, Warp 2将不会被调度执行, 这就在一定程度上降低了GPU的吞吐率, 带来了严重的性能损失。</p>
                </div>
                <div class="p1">
                    <p id="50">在静态的线程组织方式下, 这种机制对程序的执行是完全有利的。然而在进行动态Warp重组时, Warp的拆分往往会引起访存请求数量增加, 因此可以得出结论:当Warp内各线程的访存地址连续时, 重组所导致的Warp拆分会严重影响程序的性能。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>2.2 部分Warp重组基本思想</b></h4>
                <div class="p1">
                    <p id="52">Warp重组的优势主要体现在它能够减少程序执行时Warp的数量, 提高线程级并行度。然而, 在None-Aware重组方法中, 当对包含活跃线程数量较多的Warp进行重组时, Warp数量的减少并不明显, 由此产生的性能收益也是很有限的。相反, Warp重组引入的寄存器体冲突和访存开销甚至还会降低应用的性能。因此, 重组时应减少对包含活跃线程数量较多的Warp的重组, 通过牺牲有限的收益来避免引入更多的开销, 这就是部分重组的思想。</p>
                </div>
                <div class="p1">
                    <p id="53">部分重组简单地采用了设置阈值的方法来控制重组的范围, 以减少不必要的Warp重组。其具体流程如图3所示, 当来自某一分支路径上的Warp进入重组逻辑之前, 先将Warp内活跃线程的数量与系统设置的阈值进行比较, 如果活跃线程的数量大于或者等于阈值, 则选择不对其进行重组, 而是直接加入到调度池 (Warp Pool) 中等待被调度, 同时将该Warp的状态标记为不可被重组, 以防止对后续到来的其它Warp产生影响。对于活跃线程数量小于阈值的Warp则继续进行重组操作, 以减少计算资源的浪费。在阈值的选择上, 要考虑到部分重组对SIMD流水线利用率的影响, 如果阈值太小, 可参与重组的Warp数量有限, 会导致大量计算资源空闲, 重组带来的收益降低。如果阈值太大, 则会增加对包含活跃线程数较多Warp的重组, 引入过多的额外开销。因此, 关于阈值的选择是对重组中收益和开销的权衡。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 部分重组的基本思想" src="Detail/GetImg?filename=images/JSJK201908001_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 部分重组的基本思想  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Basic idea of partial warp regrouping</p>

                </div>
                <h3 id="55" name="55" class="anchor-tag"><b>3 通用的部分重组框架</b></h3>
                <div class="p1">
                    <p id="56">根据前面的分析可以看出, 不同的测试程序在部分重组中所取的最佳阈值是不同的, 即使同一个程序在不同的执行时间段, 分支的特征也会有所变化。为了提高部分重组的自适应性, 本文提出了一种通用的部分重组框架, 如图4所示。它对现有的Warp重组机制进行简单的修改, 增加了一个性能参数的收集部件 (Collector) , 通过收集并分析相关性能计数器的计数值实现重组阈值的动态调整, 以平衡重组的性能收益与开销。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 部分重组通用框架" src="Detail/GetImg?filename=images/JSJK201908001_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 部分重组通用框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 General framework of partial warp regrouping</p>

                </div>
                <div class="p1">
                    <p id="58">从图4中可以看出, 每个Warp在取指令之前就需要对线程进行重新组织, 这实际上已改变了GPU中原来线程与执行通道的映射关系, 由原来静态的映射方式变为动态映射。性能参数收集器 (Collector) 是实现动态阈值调整的基础, 它主要有3个作用:</p>
                </div>
                <div class="p1">
                    <p id="59"> (1) 收集性能计数值。这里主要收集<i>gpu</i>_<i>reg</i>_<i>bank</i>_<i>conflict</i>_<i>stalls</i>, <i>gpu</i>_<i>stall</i>_<i>shd</i>_<i>mem</i>, <i>n</i>_<i>warp</i>_<i>issued</i>等3个计数器的计数值。其中, <i>gpu</i>_<i>stall</i>_<i>shd</i>_<i>mem</i>是指程序执行时因访存产生的停顿, 包括了共享内存的体冲突, Cache失效引起的停顿以及常量、纹理、全局的访存行为引起的长延迟操作等。它们都直接或间接地受到访存次数的影响, 所以可以简单地被视作是对访存开销的描述。<i>gpu</i>_<i>reg</i>_<i>bank</i>_<i>conflict</i>_<i>stalls</i>代表了因寄存器体冲突导致数据串行访问带来的停顿。<i>n</i>_<i>warp</i>_<i>issued</i>是指程序执行的Warp数, 实际上是Warp指令执行的周期数。</p>
                </div>
                <div class="p1">
                    <p id="60"> (2) 性能参数分析。在某个时间段内, 寄存器体冲突和访存停顿的变化可以被简单地认为是重组引入的额外开销, 程序执行Warp数的减少则可以被认为是重组带来的性能收益。将二者进行比较可以在一定程度上反映出整体性能收益 (或开销) 的变化。</p>
                </div>
                <div class="p1">
                    <p id="61"> (3) 阈值生成。阈值的生成主要是根据收益、开销的分析动态地增加或减少阈值。若分析结果表示这段时间内收益大于开销, 则可以尝试增加阈值, 进一步扩大重组范围, 提升线程级并行。若分析结果为开销大于收益, 则表示当前重组仍然存在着大量不必要的开销, 则可减少阈值, 减少对包含活跃线程数量较多的Warp的重组。</p>
                </div>
                <div class="p1">
                    <p id="62">在阈值生成后便可以与Warp包含的活跃线程数进行比较, 以达到控制重组范围的目的。图4所示的Warp部分重组框架可以很容易地集成到现有的重组硬件中, 甚至可以根据不同程序的特征在Warp部分重组、None-Aware重组和Lane-Aware重组等3种方式中进行选择。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag"><b>4 性能测试与分析</b></h3>
                <div class="p1">
                    <p id="64">本文在GPGPU-Sim (v2.1.1b) 模拟器上对所提出的部分Warp重组技术进行了性能测试和分析。GPGPU-Sim主要有2种工作模式, 分别是性能模拟和功能模拟, 其中性能模拟会收集程序执行过程中的性能参数, 如执行指令的条数、执行的时钟周期数、执行Warp的数量和各级存储访问产生的停顿、等等, 因而相比于功能模拟, 性能模拟的速度会慢很多。GPGPU-Sim可以通过修改配置参数来模拟不同型号的GPU, 具有很强的灵活性。本节的实验采用了类似 NVIDIA Quadro 5800的配置, 详细参数如表1所示。</p>
                </div>
                <div class="area_img" id="65">
                    <p class="img_tit"><b>表1 实验用GPU配置参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 GPU configuration parameters for the experiment</b></p>
                    <p class="img_note"></p>
                    <table id="65" border="1"><tr><td><br />参数</td><td>取值</td></tr><tr><td><br />Number of shader cores</td><td>30</td></tr><tr><td><br />Threads per core</td><td>1 024</td></tr><tr><td><br />Threads per warp</td><td>32</td></tr><tr><td><br />SIMD pipeline width</td><td>32</td></tr><tr><td><br />Register per core</td><td>16 384</td></tr><tr><td><br />Shared memory per core</td><td>16 KB</td></tr><tr><td><br />L1 Cache (size/assoc/line) </td><td>32 KB/8-way/64 B</td></tr><tr><td><br />L2 Cache (size/assoc/line) </td><td>128 KB/8-way/64 B</td></tr><tr><td><br />Memory controller</td><td>Out-of-order (FR-FCFS) </td></tr><tr><td><br />DRAM request queue size</td><td>32</td></tr><tr><td><br />Compute Node Clock</td><td>1 300 MHz</td></tr><tr><td><br />Interconnect Clock</td><td>650 MHz</td></tr><tr><td><br />Memory Clock</td><td>800 MHz</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>4.1 基准程序</b></h4>
                <div class="p1">
                    <p id="67">本节使用的基准程序主要来源于ispass2009-benchmarks和Rodinia这2个基准程序包, 包括 Hotspot、LPS (Laplace 3D) 、MUM (MUMmer) 、BFS (Breadth First Search) 和NN (Neural Network) , 在进行重组操作时, 它们分别表现了不同的特征。为了测试重组对访存请求次数的影响, 我们还编写了BLK程序, 其Kernel函数如下所示 (在BLK中, 每个Warp内各线程的访存地址是连续的) :</p>
                </div>
                <div class="p1">
                    <p id="68">/*Branch with both true/false path active on divergence the last 4 threads within a warp execute in the IF path and  they always access the data in d_Input1 and d_Result continuously by the value of <i>tid</i>*/</p>
                </div>
                <div class="p1">
                    <p id="69">if (<i>tid</i>%32≥28) {</p>
                </div>
                <div class="p1">
                    <p id="70">for (int <i>pos</i>=<i>tid</i>;<i>pos</i>&lt;<i>N</i>;<i>pos</i>+=<i>threadN</i>) </p>
                </div>
                <div class="p1">
                    <p id="71"><i>sum</i>1+=<i>d</i>_<i>Input</i>1[<i>pos</i>];</p>
                </div>
                <div class="p1">
                    <p id="72"><i>d</i>_<i>Result</i> [<i>tid</i>]=<i>suml</i>;</p>
                </div>
                <div class="p1">
                    <p id="73">}</p>
                </div>
                <div class="p1">
                    <p id="74">/*the first 20 threads within a warp execute in the ELSE path and have a same memory access pattern as in the IF path*/</p>
                </div>
                <div class="p1">
                    <p id="75">else if ( (<i>tid</i>%32) &lt;28) {</p>
                </div>
                <div class="p1">
                    <p id="76">int <i>n</i>=<i>tid</i>*<i>tid</i>;</p>
                </div>
                <div class="p1">
                    <p id="77">for (int <i>pos</i>=<i>tid</i>;<i>pos</i>&lt;<i>N</i>;<i>pos</i>+=<i>threadN</i>) </p>
                </div>
                <div class="p1">
                    <p id="78"><i>sum</i>2+=<i>n</i>*<i>d</i>_<i>Input</i>2[<i>pos</i>];</p>
                </div>
                <div class="p1">
                    <p id="79"><i>d</i>_<i>Result</i>[<i>tid</i>]=<i>sum</i>2;</p>
                </div>
                <div class="p1">
                    <p id="80">}</p>
                </div>
                <div class="p1">
                    <p id="81">表2和图5描述了这些测试程序的特征, 主要包括PDOM机制下SIMD的利用率 (SIMD<sub><i>util</i></sub>, 处于非空闲状态的SIMD通道所占的比例) 和程序执行过程中包含不同活跃线程数量的Warp分布 (<i>wx</i>表示该Warp中含有<i>x</i>个活跃的线程, 其中<i>w</i>32意味着该Warp所执行的指令为非分支指令或者是分支指令但未产生非一致的控制流, 因此这类Warp并不属于重组的范畴) 。可以看出BLK、Hotspot和LPS中均存在着一定数量的包含活跃线程数量较多的Warp (<i>w</i>24-<i>w</i>31) , 若直接进行重组, 效果可能并不好。相反, MUM、BFS和NN则具有较低的SIMD利用率, 并且存在着大量适合重组的Warp。</p>
                </div>
                <div class="area_img" id="82">
                    <p class="img_tit"><b>表2 基准程序特征统计表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Statistics of banchbank features</b></p>
                    <p class="img_note"></p>
                    <table id="82" border="1"><tr><td><br />测试程序</td><td>PDOM SIMD<sub>util</sub>/%</td><td>来源</td></tr><tr><td><br />BLK</td><td>75</td><td>self-development</td></tr><tr><td><br />HOTSPT</td><td>87</td><td>Rodinia</td></tr><tr><td><br />LPS</td><td>75</td><td>GPGPU-Sim</td></tr><tr><td><br />MUM</td><td>24</td><td>GPGPU-Sim</td></tr><tr><td><br />BFS</td><td>36</td><td>Rodinia</td></tr><tr><td><br />NN</td><td>7</td><td>GPGPU-Sim</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基准程序特征统计" src="Detail/GetImg?filename=images/JSJK201908001_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基准程序特征统计  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Statistics of benchmark features</p>

                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>4.2 测试结果</b></h4>
                <div class="p1">
                    <p id="85">图6描述了测试程序在不同的重组方法中的Warp数量。可以看出, 在Hotspot和LPS中, 3种重组方式均不能明显减少Warp的数量, 这主要是因为测试程序在PDOM下就已经具有了良好的SIMD利用率, 通过重组获得的收益有限。而在BLK、MUM、BFS和NN中则存在着大量包含活跃线程数量较少的Warp, 根据图5可以看出这些Warp中活跃线程数小于等于8的居多, 此时重组会将若干个独立的Warp进行压缩合并, Warp数量的减少达到了34%～66%, 充分发挥出了重组的优势。在之前的分析中, None-Aware重组侧重于减少Warp的数量, 而Lane-Aware重组则更有可能导致Warp的拆分, 使得数量相对None-Aware重组会有一定的增加。然而从图6中可以看出, 部分重组虽然避免了一些Warp参与重组, 但是Warp数量相对None-Aware重组基本持平或者只有少量增加, 而且明显低于Lane-Aware重组的。这主要是因为针对包含活跃线程数量较多的Warp, 重组与不重组在Warp数量的减少上并不会有很大的差距, 相反避免对这类Warp的重组, 反而会增加包含活跃线程数量较少的Warp结合的几率, 使得总体重组效率基本维持不变。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同重组方法下Warp数量的比较" src="Detail/GetImg?filename=images/JSJK201908001_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同重组方法下Warp数量的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Comparison of the number of  warps under different regrouping methods</p>

                </div>
                <div class="p1">
                    <p id="87">图7比较了部分重组和None-Aware重组所产生的寄存器体冲突。部分重组下的BLK, LPS 和 NN, 寄存器体冲突能够减少45%～86%, 剩余的寄存器体冲突主要是对包活跃线程数量较少的Warp进行重组时产生的。由图6可以看出, 在Hotspot中Warp内活跃线程的数量普遍大于24, 所以当阈值小于24时, 部分重组基本能够消除寄存器体冲突带来的开销。与之相反, 对MUM和BFS中的Warp来说, 活跃线程的数量普遍小于8, 此时应以减少Warp数量为目标, 所以在部分重组下寄存器体冲突并没有得到有效减少。但是, 在这类程序中, 产生寄存器体冲突的绝对数并不大, 因此程序执行仍能获得一定的收益。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同重组方法下寄存器体冲突的比较" src="Detail/GetImg?filename=images/JSJK201908001_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同重组方法下寄存器体冲突的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Comparison of register bank conflicts</p>

                </div>
                <div class="p1">
                    <p id="89">图8描述了不同重组方法中访存请求数量的变化。首先需要说明的是, 通过对测试程序源代码的分析, 存在着部分测试程序Warp内线程的访存不连续的情形, 这种情况下, 重组并不会增加访存请求的数量, 甚至可能会促进Warp内访存请求的合并, 例如NN。然而, 对于某些测试程序, 如BLK, Warp内线程的访存连续且存在着大量包含活跃线程数量较多的Warp, 此时None-Aware重组和Lane-Aware重组会导致访存请求总数的增加。从图8中可以看出, 在这2种重组方式中BLK的访存请求数量分别增加了41%和33%, 这严重地限制了性能的提升。相反, Warp部分重组由于避免了这部分Warp的拆分, 反而减少了访存的开销。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同重组方法下访存冲突的比较" src="Detail/GetImg?filename=images/JSJK201908001_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同重组方法下访存冲突的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 Comparison of memory access conflicts  under different regrouping methods</p>

                </div>
                <div class="p1">
                    <p id="91">图9给出了测试程序在4种重组方法下的总体性能, 在BLK、Hotspot和LPS中, None-Aware重组方式增加了程序的执行时间, 特别是在Hotspot的性能相对PDOM下降了22%。部分Warp重组和Lane-Aware重组则能够对性能损失进行弥补甚至提升。在BLK中, Lane-Aware重组由于Warp拆分增加了访存的开销, 影响了对性能的进一步提升, 而部分Warp重组则较好地解决了这个问题, 相对Lane-Aware重组进一步提升了6%的性能。在BFS和NN中, 3种重组方式均能获得良好的性能加速, 而部分Warp重组相对None-Aware重组分别能够进一步提升6%和3%, 相对PDOM则分别提升了21%和27%。MUM由于自身程序的特性, 导致Cache极易发生抖动, 所以性能提升幅度相对较小。总体来说, None-Aware重组相对PDOM仅能够实现平均1%的加速, 而Warp部分重组因为能够避免多余的开销, 减少了性能的损失, 相对PDOM性能平均提升了12%。Lane-Aware重组虽然同样能够实现大约11%的平均性能加速, 但它的硬件开销也是不容忽视的。同时, 对于某些访存地址连续的测试程序, Lane-Aware重组并不能达到最优的效果。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908001_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同重组方法下加速比的比较" src="Detail/GetImg?filename=images/JSJK201908001_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 不同重组方法下加速比的比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908001_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 9 Comparison of speedup</p>

                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="94">本文主要针对GPU中非一致控制流的问题, 通过分析目前已有的重组优化方法, 提出了一种轻量级的部分Warp重组优化技术, 在降低不必要开销的前提下, 提升了系统的性能。本文在GPGPU-Sim上对上述方法进行了实现, 选取一定数目的测试程序进行验证并与PDOM、None-Aware、Lane-Aware等重组方法进行了比较。实验结果表明, 部分Warp重组能够显著减少重组所引入的额外开销, 减少了性能的损失, 相对PDOM性能提升了平均12%, 最高提升27%。Lane-Aware重组虽然也能实现不错的性能提升, 但是硬件设计开销也是不容忽视的。总体来说, 部分Warp重组能够实现不错的性能加速, 它的优势主要在于相对简单的硬件设计, 而且可以很容易地集成到目前已有GPU中, 通过显式的阈值调整可以实现PDOM和完全重组之间的切换, 大大提升了其通用性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="114" type="formula" href="images/JSJK201908001_11400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">沈立</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="115" type="formula" href="images/JSJK201908001_11500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">杨耀华</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="116" type="formula" href="images/JSJK201908001_11600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王志英</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700140922&amp;v=MDM2NDRlcnFRVE1ud1plWnVIeWptVUxmSUpsNFdhUkE9TmlmT2ZiSzhIOURNcUk5RlplOFBCWDQ3b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Jin X, Daku B, Ko S-B.Improved GPU SIMD control flow efficiency via hybrid warp size mechanism [J].Microprocessors and Microsystems, 2014, 38 (7) :717-729.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analyzing CUDA Workloads Using a Detailed GPU Simulator">

                                <b>[2]</b> Bakhoda A, Yuan G L, Fung W W L, et al.Analyzing CUDA workloads using a detailed GPU simulator [C]//Proc of 2009 IEEE International Symposium on Performance Analysis of Systems and Software, 2009:163-174.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM14041100001418&amp;v=MDM5NzNUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0V2FSQT1OaWZJWTdLOEh0WE5ybzlGWk9zT0NIMHhvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Lashgar A, Khonsari A, Baniasadi A.Harp:Harnessing inactive threads in many-core processors [J].ACM Transactions on Embedded Computing Systems, 2014, 13 (3s) :1-25.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000014243&amp;v=MjUyNzl1SHlqbVVMZklKbDRXYVJBPU5pZklZN0s3SHRqTnI0OUZaT29MRG5nNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Meng J, Tarjan D, Skadron K.Dynamic warp subdivision for integrated branch and memory divergence tolerance [J].ACM SIGARCH Computer Architecture News, 2010, 38 (3) :235-246.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000011042&amp;v=MDI2MjJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0V2FSQT1OaWZJWTdLN0h0ak5yNDlGWk9vT0RIZzdvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Rhu M, Erez M.Maximizing SIMD resource utilization in GPGPUs with SIMD lane permutation [J].ACM Sigarch Computer Architecture News, 2013, 41 (3) :356-367.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600490799&amp;v=MzE5ODN3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0V2FSQT1OaWZPZmJLOEh0RE1xWTlGWU9JUEMzVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Yu L, Tang X, Wu M, et al.Improving branch divergence performance on GPGPU with a new PDOM stack and multi-level warp scheduling [J].Journal of Systems Architecture, 2014, 60 (5) :420-430.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A scalable multi-path microarchitecture for efficient GPU control flow&amp;quot;">

                                <b>[7]</b> ElTantawy A, Ma J W, Connor M O, et al.A scalable multi-path microarchitecture for efficient GPU control flow [C]//Proc of 2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA) , 2014:248-259.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;The Dual-Path Execution Model for Efficient GPU Control Flow&amp;quot;">

                                <b>[8]</b> Rhu M, Erez M.The dual-path execution model for efficient GPU control flow [C]//Proc of 2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA) , 2013:591-602.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000011043&amp;v=MTcyMTZyNDlGWk9vT0RIZzZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDRXYVJBPU5pZklZN0s3SHRqTg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Vaidya A S, Shayesteh A, Dong H W, et al.SIMD divergence optimization through intra-warp compaction [M].ACM SIGARCH Computer Architecture News, 2013, 41 (3) :368-379.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic warp formation and scheduling for efficient GPU control flow">

                                <b>[10]</b> Fung W W L, Sham I, Yuan G, et al.Dynamic warp formation and scheduling for efficient GPU control flow [C]//Proc of the 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007) , 2007:407-420.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Thread block compaction for efficient SIMT control flow">

                                <b>[11]</b> Fung W W L, Aamodt T M.Thread block compaction for efficient SIMT control flow [C]//Proc of 2011 IEEE 17th International Symposium on High Performance Computer Architecture, 2011:25-36.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving GPU Performance via Large Warps and Two-Level Warp Scheduling">

                                <b>[12]</b> Narasiman V, Shebanow M, Lee C J, et al.Improving GPU performance via large warps and two-level warp scheduling [C]//Proc of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, 2011:308-317.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004202&amp;v=MTgyNDlCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsNFdhUkE9TmlmSVk3SzdIdGpOcjQ5RlpPc0xEbnc3bw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Malits R, Bolotin E, Kolodny A, et al.Exploring the limits of GPGPU scheduling in control flow bound applications [J].ACM Transactions on Architecture and Code Optimization, 2012, 8 (4) :1-22.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous branch and warp interweaving for sustained GPU performance">

                                <b>[14]</b> Brunie N, Collange S, Diamos G.Simultaneous branch and warp interweaving for sustained GPU performance [C]//Proc of 2013 39th Annual International Symposium on Computer Architecture, 2013:49-60.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201908001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908001&amp;v=MDQyMjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xVYnpLTHo3QlpiRzRIOWpNcDQ5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
