<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358187686250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909021%26RESULT%3d1%26SIGN%3dFNs%252bA%252fuAszetLrHZukQVxIQhMmo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909021&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909021&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909021&amp;v=MDMzNDBGckNVUkxPZVplUm1GeTdrVWIzQkx6N0JaYkc0SDlqTXBvOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="&lt;b&gt;2 可变形卷积&lt;/b&gt; "><b>2 可变形卷积</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;3 图像哈希编码&lt;/b&gt; "><b>3 图像哈希编码</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="&lt;b&gt;4 相似性学习&lt;/b&gt; "><b>4 相似性学习</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="&lt;b&gt;5 实验结果与分析&lt;/b&gt; "><b>5 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="&lt;b&gt;5.1 可变形卷积实验&lt;/b&gt;"><b>5.1 可变形卷积实验</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;5.2 图像哈希编码实验&lt;/b&gt;"><b>5.2 图像哈希编码实验</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;5.3 相似性学习实验&lt;/b&gt;"><b>5.3 相似性学习实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="图1 2种卷积方式">图1 2种卷积方式</a></li>
                                                <li><a href="#80" data-title="图2 可变形卷积过程">图2 可变形卷积过程</a></li>
                                                <li><a href="#83" data-title="图3 哈希编码过程">图3 哈希编码过程</a></li>
                                                <li><a href="#91" data-title="图4 相似性学习网络结构">图4 相似性学习网络结构</a></li>
                                                <li><a href="#92" data-title="图5 DeepFashion数据集示例">图5 DeepFashion数据集示例</a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表1 相似性学习网络参数&lt;/b&gt;"><b>表1 相似性学习网络参数</b></a></li>
                                                <li><a href="#107" data-title="图6 基础网络结构">图6 基础网络结构</a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表2 基础网络参数&lt;/b&gt;"><b>表2 基础网络参数</b></a></li>
                                                <li><a href="#112" data-title="图7 可变形卷积采样位置">图7 可变形卷积采样位置</a></li>
                                                <li><a href="#116" data-title="图8 服装图像哈希编码">图8 服装图像哈希编码</a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表3 不同分类方法性能对比&lt;/b&gt;"><b>表3 不同分类方法性能对比</b></a></li>
                                                <li><a href="#125" data-title="图9 Top 5检索结果">图9 Top 5检索结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="153">


                                    <a id="bibliography_1" title=" Smeulders A W M,Worring M,Santini S,et al.Content-based image retrieval at the end of the early years[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2000,22(12):1349-1380." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Content-Based Image Retrieval at the End of the Early Years">
                                        <b>[1]</b>
                                         Smeulders A W M,Worring M,Santini S,et al.Content-based image retrieval at the end of the early years[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2000,22(12):1349-1380.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_2" title=" Hiremath P S,Pujari J.Content based image retrieval using color,texture and shape features[C]//Proc of the 15th International Conference on Advanced Computing and Communications,2007:780-784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Content Based Image Retrieval Using Color, Texture and Shape Features">
                                        <b>[2]</b>
                                         Hiremath P S,Pujari J.Content based image retrieval using color,texture and shape features[C]//Proc of the 15th International Conference on Advanced Computing and Communications,2007:780-784.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_3" title=" Liu G H,Yang J Y.Content-based image retrieval using color difference histogram[J].Pattern Recognition,2013,46(1):188-198." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600107343&amp;v=MTQwMTlNcVk5Rlplc0lEM2c2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw4V2FCcz1OaWZPZmJLOEh0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Liu G H,Yang J Y.Content-based image retrieval using color difference histogram[J].Pattern Recognition,2013,46(1):188-198.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_4" title=" Liang Y,Wang L,Zhang L.Infrared image segmentation using HOG feature and kernel extreme learning machine[C]// Proc of Applied Optics and Photonics China,2015:1376-1383." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Infrared image segmentation using HOG feature and kernel extreme learning machine">
                                        <b>[4]</b>
                                         Liang Y,Wang L,Zhang L.Infrared image segmentation using HOG feature and kernel extreme learning machine[C]// Proc of Applied Optics and Photonics China,2015:1376-1383.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_5" title=" Gopal N,Bhooshan R S.Content based image retrieval using enhanced SURF[C]//Proc of the 15th National Conference on Computer Vision,Pattern Recognition,Image Processing and Graphics,2015:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Content based image retrieval using enhanced SURF">
                                        <b>[5]</b>
                                         Gopal N,Bhooshan R S.Content based image retrieval using enhanced SURF[C]//Proc of the 15th National Conference on Computer Vision,Pattern Recognition,Image Processing and Graphics,2015:1-4.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_6" title=" Wang J Y,Zhu Z.Image retrieval system based on multi-feature fusion and relevance feedback[C]// Proc of International Conference on Machine Learning and Cybernetics,2010:2053-2058." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image retrieval system based on multi-feature fusion and relevance feedback">
                                        <b>[6]</b>
                                         Wang J Y,Zhu Z.Image retrieval system based on multi-feature fusion and relevance feedback[C]// Proc of International Conference on Machine Learning and Cybernetics,2010:2053-2058.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_7" title=" Chaudhary P,Sharma S.A color,texture and shape based hybrid approach for clothing retrieval techniques[J].International Journal of Mechanical Engineering and Computer Applications,2016,4(6):382-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A color,texture and shape based hybrid approach for clothing retrieval techniques">
                                        <b>[7]</b>
                                         Chaudhary P,Sharma S.A color,texture and shape based hybrid approach for clothing retrieval techniques[J].International Journal of Mechanical Engineering and Computer Applications,2016,4(6):382-387.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_8" title=" Huang J,Feris R S,Chen Q,et al.Cross-domain image retrieval with a dual attribute-aware ranking network[C]// Proc of International Conference on Computer Vision,2015:1062-1070." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-domain image retrieval with a dual attribute-aware ranking network">
                                        <b>[8]</b>
                                         Huang J,Feris R S,Chen Q,et al.Cross-domain image retrieval with a dual attribute-aware ranking network[C]// Proc of International Conference on Computer Vision,2015:1062-1070.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_9" title=" Huang J,Xia W,Yan S.Deep search with attribute-aware deep network[C]//Proc of International Conference on Multimedia,2014:731-732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Search with Attribute-aware Deep Network">
                                        <b>[9]</b>
                                         Huang J,Xia W,Yan S.Deep search with attribute-aware deep network[C]//Proc of International Conference on Multimedia,2014:731-732.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_10" title=" Li Z,Li Y,Tian W,et al.Cross-scenario clothing retrieval and fine-grained style recognition[C] // Proc of International Conference on Pattern Recognition (ICPR 2016),2016:2912-2917." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-scenario clothing retrieval and fine-grained style recognition">
                                        <b>[10]</b>
                                         Li Z,Li Y,Tian W,et al.Cross-scenario clothing retrieval and fine-grained style recognition[C] // Proc of International Conference on Pattern Recognition (ICPR 2016),2016:2912-2917.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Li Zhi,Sun Yu-bao,Wang Feng,et al.Clothing image classification and retrieval algorithm based on deep convolutional neural network[J].Computer Engineering,2016,42(11):309-315.(in Chinese)</a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_12" title=" Chen Ai-ai,Li Lai,Liu Guang-can,et al.Clothing retrieval based on landmarks[J].Journal of Computer Applications,2017,37(11):3249-3255.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201711037&amp;v=MzIyNzZVYjNCTHo3QmQ3RzRIOWJOcm85R1k0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2s=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Chen Ai-ai,Li Lai,Liu Guang-can,et al.Clothing retrieval based on landmarks[J].Journal of Computer Applications,2017,37(11):3249-3255.(in Chinese)
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_13" title=" Wang Z,Gu Y,Zhang Y,et al.Clothing retrieval with visual attention model[C]// Proc of Visual Communications and Image Processing (VCIP),2017:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clothing retrieval with visual attention model">
                                        <b>[13]</b>
                                         Wang Z,Gu Y,Zhang Y,et al.Clothing retrieval with visual attention model[C]// Proc of Visual Communications and Image Processing (VCIP),2017:1-4.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_14" title=" Zhang Zhen-huan,Zhou Cai-lan,Liang Yuan.An optimized clothing classification algorithm based on residual convolutional neural network[J].Computer Engineering &amp;amp; Science,2018,40(2):354-360.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201802024&amp;v=MDQyNzZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzQkx6N0JaYkc0SDluTXJZOUg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Zhang Zhen-huan,Zhou Cai-lan,Liang Yuan.An optimized clothing classification algorithm based on residual convolutional neural network[J].Computer Engineering &amp;amp; Science,2018,40(2):354-360.(in Chinese)
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_15" title=" Hadi K M,Han X,Lazebnik S,et al.Where to buy it:Matching street clothing photos in online shops[C] // Proc of International Conference on Computer Vision,2015:3343-3351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Where to Buy It:Matching Street Clothing Photos in Online Shops">
                                        <b>[15]</b>
                                         Hadi K M,Han X,Lazebnik S,et al.Where to buy it:Matching street clothing photos in online shops[C] // Proc of International Conference on Computer Vision,2015:3343-3351.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_16" title=" Bell S,Bala K.Learning visual similarity for product design with convolutional neural networks[J].ACM Transactions on Graphics (TOG),2015,34(4):98-98." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM31F4DDE5EC2A525567D3D3CB7DD8ECA9&amp;v=MTU2OTRpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOQmh3cm05d3FFPU5pZklZN0M1YU5XNDIvcEFFWmdOZlhrN3loTVY3VXQrUEh5UjNoVkJEYnJoTnN1V0NPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Bell S,Bala K.Learning visual similarity for product design with convolutional neural networks[J].ACM Transactions on Graphics (TOG),2015,34(4):98-98.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_17" title=" Wang X,Sun Z,Zhang W,et al.Matching user photos to online products with robust deep features[C]//Proc of International Conference on Multimedia Retrieval,2016:7-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Matching user photos to online products with robust deep features">
                                        <b>[17]</b>
                                         Wang X,Sun Z,Zhang W,et al.Matching user photos to online products with robust deep features[C]//Proc of International Conference on Multimedia Retrieval,2016:7-14.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_18" title=" Kalantidis Y,Kennedy L,Li L J.Getting the look:Clothing recognition and segmentation for automatic product suggestions in everyday photos[C]//Proc of International Conference on Multimedia Retrieval,2013:105-112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Getting the Look:Clothing Recognition and Segmentation for Automatic Product Suggestions in Everyday Photos">
                                        <b>[18]</b>
                                         Kalantidis Y,Kennedy L,Li L J.Getting the look:Clothing recognition and segmentation for automatic product suggestions in everyday photos[C]//Proc of International Conference on Multimedia Retrieval,2013:105-112.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_19" title=" Liu Z,Luo P,Qiu S,et al.DeepFashion:Powering robust clothes recognition and retrieval with rich annotations[C] // Proc of 2016 International Conference on Computer Vision and Pattern Recognition,2016:1096-1104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepFashion:Powering Robust Clothes Recognition and Retrieval with Rich Annotations">
                                        <b>[19]</b>
                                         Liu Z,Luo P,Qiu S,et al.DeepFashion:Powering robust clothes recognition and retrieval with rich annotations[C] // Proc of 2016 International Conference on Computer Vision and Pattern Recognition,2016:1096-1104.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_20" title=" Zhou L,Zhou Z,Zhang L.Deep part-based image feature for clothing retrieval[C]//Proc of International Conference on Neural Information Processing,2017:340-347." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep part-based image feature for clothing retrieval">
                                        <b>[20]</b>
                                         Zhou L,Zhou Z,Zhang L.Deep part-based image feature for clothing retrieval[C]//Proc of International Conference on Neural Information Processing,2017:340-347.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_21" title=" Yu X,Wu X,Luo C,et al.Deep learning in remote sensing scene classification:A data augmentation enhanced convolutional neural network framework[J].GI Science &amp;amp; Remote Sensing,2017,54(5):741-758." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD3837D9B0EA32C110282342F1CBBBAB47&amp;v=Mjg1NDVoWVI0ajErVEgyVXJXRkhDOERsTjc2WUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaHdybTl3cUU9TmpuQmFyQ3dIZGE0cHYxRkVab01EZzg0eg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Yu X,Wu X,Luo C,et al.Deep learning in remote sensing scene classification:A data augmentation enhanced convolutional neural network framework[J].GI Science &amp;amp; Remote Sensing,2017,54(5):741-758.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_22" title=" Dai J,Qi H,Xiong Y,et al.Deformable convolutional networks[C] // Proc of International Conference on Computer Vision (ICCV 2017),2017:764-773." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deformable Convolutional Networks">
                                        <b>[22]</b>
                                         Dai J,Qi H,Xiong Y,et al.Deformable convolutional networks[C] // Proc of International Conference on Computer Vision (ICCV 2017),2017:764-773.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_23" title=" Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C] // Proc of International Conference on Machine Learning,2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[23]</b>
                                         Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C] // Proc of International Conference on Machine Learning,2015:448-456.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_24" >
                                        <b>[24]</b>
                                     Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]// Proc of International Conference on Neural Information Processing Systems,2012:1106-1114.</a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_25" title=" Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,2015:27-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning of binary hash codes for fast image retrieval">
                                        <b>[25]</b>
                                         Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,2015:27-35.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_26" title=" Very deep convolutional networks for large-scale image recognition[EB/OL].[2015-04-10].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[26]</b>
                                         Very deep convolutional networks for large-scale image recognition[EB/OL].[2015-04-10].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_27" title=" He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]// Proc of 2016 Conference on Computer Vision and Pattern Recognition,2016:770-778.附中文参考文献:" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[27]</b>
                                         He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]// Proc of 2016 Conference on Computer Vision and Pattern Recognition,2016:770-778.附中文参考文献:
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_11" title=" 厉智,孙玉宝,王枫,等.基于深度卷积神经网络的服装图像分类检索算法[J].计算机工程,2016,42(11):309-315." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201611054&amp;v=MTY1MzZxQnRHRnJDVVJMT2VaZVJtRnk3a1ViM0JMejdCYmJHNEg5Zk5ybzlBWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         厉智,孙玉宝,王枫,等.基于深度卷积神经网络的服装图像分类检索算法[J].计算机工程,2016,42(11):309-315.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     陈嫒嫒,李来,刘光灿,等.基于关键点的服装检索[J].计算机应用,2017,37(11):3249-3255.</a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     张振焕,周彩兰,梁媛.基于残差的优化卷积神经网络服装分类算法[J].计算机工程与科学,2018,40(2):354-360.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1671-1678 DOI:10.3969/j.issn.1007-130X.2019.09.020            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于可变形卷积的服装检索方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%8C%AF&amp;code=33023080&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王振</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E7%BA%A2%E8%89%B3&amp;code=17347997&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全红艳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%9C%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0092795&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华东师范大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统的服装检索方法使用固定形状的感受野,当服装目标存在几何变形时无法有效地提取其特征。针对这个问题,提出基于可变形卷积和相似性学习的服装检索方法。首先,构建可变形卷积网络,自动学习服装特征的采样位置和服装图像的哈希编码;然后,级联相似性学习网络,度量哈希编码的相似性;最后,根据相似性评分产生检索结果。实验结果表明,该方法能够有效地提取存在几何变形的服装目标的特征,从而减少了图像背景特征的干扰,提高了检索模型的准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%8D%E8%A3%85%E6%A3%80%E7%B4%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">服装检索;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可变形卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%93%88%E5%B8%8C%E7%BC%96%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈希编码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相似性学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王振（1995-），男，江苏睢宁人，硕士生，CCF会员（95300G），研究方向为计算机视觉和深度学习。E-mail:1806333477@qq.com,通信地址:200062上海市中山北路3663号华东师范大学理科楼B1009&lt;image id="150" type="formula" href="images/JSJK201909021_15000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *全红艳（1968-），女，黑龙江哈尔滨人，博士，副教授，CCF会员（19765S），研究方向为计算机视觉和深度学习。E-mail:hyquan@sei.ecnu.edu.cn,通信地址:200062上海市中山北路3663号华东师范大学理科楼B1009&lt;image id="152" type="formula" href="images/JSJK201909021_15200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-27</p>

            </div>
                    <h1><b>A garment retrieval method based on deformable convolution</b></h1>
                    <h2>
                    <span>WANG Zhen</span>
                    <span>QUAN Hong-yan</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology,East China Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional garment retrieval methods use fixed-shape receptive fields, and they cannot extract features effectively when the garment target has geometric deformation. To solve this problem, we propose a garment retrieval method based on deformable convolution and similarity learning. Firstly, we build a deformable convolutional network which can automatically learn the sampling locations of garment features and the Hash code of garment images. Secondly, a similarity learning network is cascaded to measure the similarity of the Hash code. Finally, we obtain the retrieval results according to similarity scores. Experimental results show that this method can effectively extract the features of garment objects with geometric deformation, thus reducing the impact of image background features and improving the accuracy of the retrieval model.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=garment%20retrieval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">garment retrieval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deformable%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deformable convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hash%20code&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hash code;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=similarity%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">similarity learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WANG Zhen,born in 1995,MS candidate,CCF member(95300G),his research interests include computer vision,and deep learning.Address:Room B1009,Science Building,East China Normal University,3663Zhongshan North Road,Shanghai 200062,P.R.China;
                                </span>
                                <span>
                                    QUAN Hong-yan,born in 1968,PhD,associate professor,CCF member(19765S),her research interests include computer vision,and deep learning.Address:Room B1009,Science Building,East China Normal University,3663Zhongshan North Road,Shanghai 200062,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-27</p>
                            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="64">传统的服装检索技术大多是基于文本的图像检索TBIR(Text-Based Image Retrieval)。TBIR方式需要商家对每幅服装图像的特征进行手工标注。然而,简单的文本关键字并不能很好地描述图像所包含的信息。面对与日俱增的海量图像,手工标注特征不仅费时费力,而且具有很强的主观性,检索结果会产生很大偏差,从而导致用户体验不友好。</p>
                </div>
                <div class="p1">
                    <p id="65">由于TBIR方式需要人工介入且很难实现服装图像的精确检索,因此基于内容的图像检索CBIR (Content-Based Image Retrieval)<citation id="213" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>受到研究人员的广泛关注。CBIR主要利用各种视觉算法,提取服装图像的颜色、纹理、形状等低层视觉特征<citation id="214" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>;然后对这些视觉特征进行编码并建立特征索引库,通过计算查询图像的特征编码与索引库中的特征编码之间的空间距离来衡量2幅图像的相似度;最后根据相似度降序输出检索结果。CBIR较之于TBIR,能够充分描述服装图像的视觉特征,在检索速度和精度上均有提升。如Liu等<citation id="215" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>使用图像的色差直方图(Color Difference Histogram)特征进行服装图像检索;Liang等<citation id="216" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出利用图像分割与方向梯度直方图(Histogram of Oriented Gradient)特征匹配相结合的方式,消除服装图像中的复杂背景;Gopal等<citation id="217" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出使用鲁棒性好、检测速度快、具有特征不变性的SURF(Speed-Up Robust Feature)算法来减少形变、尺度、光照、背景等因素对检索精度的影响。上述检索方法均基于单一的低层视觉特征,对服装的语义信息描述不够充分。因此,Wang等<citation id="218" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出一种融合高层全局特征和中层区块特征的服装检索方法;Chaudhary等<citation id="219" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出结合了服装的颜色、纹理和形状的多特征融合图像检索算法,尽可能地描述服装图像的特征以达到更好的检索效果。然而,多特征融合仅仅是将多个手工视觉特征进行线性组合,使用的仍然是图像的低层视觉特征,依旧无法完全描述服装图像的高层语义特征。</p>
                </div>
                <div class="p1">
                    <p id="66">近年来,深度学习(Deep Learning)的出现很好地解决了图像高低层特征的提取与融合问题。在具有若干个隐藏层的卷积神经网络CNN(Convolutional Neural Network)中,低层网络学习图像的低层视觉特征,高层网络学习图像的高层语义特征,最后CNN的输出层会自动输出高维度特征。因此,现有的服装检索技术大多利用了CNN的这种特性<citation id="227" type="reference"><link href="167" rel="bibliography" /><link href="169" rel="bibliography" /><link href="171" rel="bibliography" /><link href="173" rel="bibliography" /><link href="175" rel="bibliography" /><link href="177" rel="bibliography" /><link href="179" rel="bibliography" /><link href="207" rel="bibliography" /><link href="209" rel="bibliography" /><link href="211" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">14</a>]</sup></citation>。如Hadi等<citation id="220" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>使用预训练的CNN模型提取图像的高维特征,并直接将其用于服装检索;Bell等<citation id="221" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>利用孪生网络(Siamese Network)将学习到的服装图像特征映射到隐藏空间中,然后通过比较隐藏空间中特征向量之间的距离衡量2幅图像的相似性;Wang等<citation id="222" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出利用鲁棒对比损失(Robust Contrastive Loss)监督CNN的训练以实现服装图像的跨领域匹配。此外,为了让模型专注于服装特征的提取,Kalantidis等<citation id="223" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>利用图像分割方法,将服装目标从图像中分割出来后再提取服装特征;Liu等<citation id="224" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出FashionNet模型,将服装的类别、纹理、形状、风格和关键点等属性作为监督信息,尝试利用丰富的服装属性充分学习服装目标的特征;Zhou等<citation id="225" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>将服装中多个关键部位的高维特征融合为全局特征,而后再进行服装检索。然而,上述基于CNN的服装检索方法有其内在的局限性。首先,在现有的服装检索方法中,CNN中使用的感受野(Receptive Field)的形状是固定的,在进行特征采样时,只能提取固定形状区域中固定位置的目标特征,因此在提取到的特征中会包含大量无用的背景信息;其次,图像中的服装目标会因不同大小、视角和非刚体形变而呈现出几何多样性,固定形状的感受野无法对这些未知的几何多样性进行有效建模。现有方法通过对图像进行翻转、裁剪、缩放等操作,人为地增加这种多样性以达到数据增强的目的<citation id="226" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。然而这种基于先验知识的处理方法并不具备良好的泛化能力。最后,传统服装检索技术使用Cosine距离或欧氏距离来衡量高维特征向量之间的相似性,这样不仅会导致维度灾难,而且单纯使用空间距离作为相似性的衡量标准太过笼统,无法感知图像之间潜在的异同点,很难保证图像视觉特征与语义特征相一致。</p>
                </div>
                <div class="p1">
                    <p id="67">针对上述问题,本文提出一种基于可变形卷积网络和相似性学习的服装检索方法。首先在传统的CNN中加入可变形卷积操作,自动学习服装特征的采样位置,从而减少背景信息的干扰,提高模型对服装几何多样性的适应能力;而后利用可变形卷积网络模型提取服装图像的哈希编码;最后训练相似性学习网络,度量哈希编码之间的相似性并降序输出检索结果。本文提出的方法不仅可以有效地提取存在几何变形的服装目标的特征,提高模型的泛化能力,而且还可以解决因维度灾难导致的检索效率低下问题,保证了服装图像高低层特征的视觉一致性。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag"><b>2 可变形卷积</b></h3>
                <div class="p1">
                    <p id="69">在标准卷积中,卷积核(Convolution Kernel)可以反映感受野的形状和大小。标准二维卷积计算过程可以分为2步:特征图上固定窗口内元素采样和对应窗口内元素的点乘并求和计算。假设有3×3的卷积核<i>g</i>={(-1,-1),(-1,0),…,(0,1),(1,1)},其中每个元素代表卷积核中每个位置相对于中心位置的偏移量,则标准卷积方式可定义为:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>g</mi></mrow></munder><mi>w</mi></mstyle><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中,<i>i</i>=1,…,<i>N</i>,<i>N</i>=|<i>g</i>|,<i>p</i><sub>0</sub>表示中心位置,<i>w</i>表示卷积核<i>g</i>的权重,<i>f</i>表示输入特征图,<i>y</i>表示输出特征图。从式(1)可以看出,卷积核确定后,采样尺寸是固定的,即感受野的形状和大小是固定的。因此在对服装图像进行特征采样时,如图1a所示,固定形状的感受野很难感知服装目标的几何变形,提取到的特征中包含了大量的背景特征。而在可变形卷积方式中,如图1b所示,通过给卷积核<i>g</i>中每个采样位置添加额外的参数,使得采样位置能够自动向服装目标区域偏移<citation id="228" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。可变形卷积方式定义如下:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>g</mi></mrow></munder><mi>w</mi></mstyle><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo>△</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">此时采样位置相对于中心位置的偏移量变为<i>p</i><sub><i>i</i></sub>+△<i>p</i><sub><i>i</i></sub>。由于△<i>p</i><sub><i>i</i></sub>在大多数情况下不是整数,因此式(2)通过双线性插值(Bilinear Interpolation)算法实现:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>q</mi></munder><mi>Τ</mi></mstyle><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中,<i>p</i>=<i>p</i><sub>0</sub>+<i>p</i><sub><i>i</i></sub>+△<i>p</i><sub><i>i</i></sub>,<i>q</i>代表输入特征图<i>f</i>上的任意位置,<i>T</i>(<i>q</i>,<i>p</i>)=<i>t</i>(<i>q</i><sub><i>x</i></sub>,<i>p</i><sub><i>x</i></sub>)·<i>t</i>(<i>q</i><sub><i>y</i></sub>,<i>p</i><sub><i>y</i></sub>),<i>t</i>(<i>a</i>,<i>b</i>)=max(0,1-|<i>a</i>-<i>b</i>|)。由于在<i>f</i>的绝大多数位置处<i>T</i>(<i>q</i>,<i>p</i>)为0,因此式(3)很容易计算。可变形卷积网络在进行反向传播BP(Back Propagation)时,偏移量△<i>p</i><sub><i>i</i></sub>的学习过程为:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>y</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mo>△</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>g</mi></mrow></msub><mi>w</mi></mstyle><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi>f</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo>△</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mo>△</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>g</mi></mrow></msub><mo stretchy="false">[</mo></mstyle><mi>w</mi><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mi>q</mi></msub><mrow><mfrac><mrow><mo>∂</mo><mi>Τ</mi><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo>△</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mo>△</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></mstyle><mi>f</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">之后根据式(4)的偏导数更新参数△<i>p</i><sub><i>i</i></sub>。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 2种卷积方式" src="Detail/GetImg?filename=images/JSJK201909021_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 2种卷积方式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Two convolution methods</p>

                </div>
                <div class="p1">
                    <p id="79">图2展示了可变形卷积操作的整个过程:对于输入特征图,假定原始卷积核大小为<i>M</i>×<i>M</i>(图中<i>M</i>=3),则先通过额外的标准二维卷积计算得到一个新特征图,其大小与输入特征图一致,所用卷积核的大小依然为<i>M</i>×<i>M</i>,通道数为2<i>M</i><sup>2</sup>,通道维度方向上的2<i>M</i><sup>2</sup>维向量代表<i>M</i><sup>2</sup>个二维偏移量(即<i>X</i>、<i>Y</i> 2个方向的偏移量),因此新特征图中每个位置代表的就是原始卷积核在输入特征图上对应位置的偏移量。然后在执行可变形卷积计算时,将原始卷积核的采样位置与对应位置的偏移量进行叠加,得到偏移后的采样位置,特征采样后计算过程与标准二维卷积一致,最终得到输出特征图。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 可变形卷积过程" src="Detail/GetImg?filename=images/JSJK201909021_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 可变形卷积过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Procedure of deformable convolution</p>

                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>3 图像哈希编码</b></h3>
                <div class="p1">
                    <p id="82">在对服装图像进行可变形卷积操作后,利用后接的全连接层,如图3中的<i>FC</i>6～<i>FC</i>8层,可以获得服装图像的高维特征。虽然高维特征能够充分描述服装图像信息,但同时也带来了模型训练时间长、检索性能下降等问题。因此,在<i>FC</i>7层后加入一个包含若干个特征节点的隐藏层,用来训练服装图像的中层特征,如图3中的<i>H</i>层。中层特征介于图像低层视觉特征与高层语义特征之间,可以缩小高低层特征之间的差距,保证高低层特征的视觉一致性。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 哈希编码过程" src="Detail/GetImg?filename=images/JSJK201909021_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 哈希编码过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Procedure of hash encoding</p>

                </div>
                <div class="p1">
                    <p id="84">给定一幅服装图像<i>I</i>,经过可变形卷积网络的前向传播(Forward Propagation)计算,提取隐藏层<i>H</i>的中层特征值,记为<i>Output</i>(<i>H</i>)。设定一个阈值,对中层特征值进行哈希编码。对于隐藏层<i>H</i>中的每一个节点<i>j</i>,其中<i>j</i>=1,…,<i>h</i>,<i>h</i>为隐藏层<i>H</i>的节点个数,其对应的中层特征值可表示为<i>Output</i><sup><i>j</i></sup>(<i>H</i>)。哈希编码规则如下所示:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><msup><mrow></mrow><mi>j</mi></msup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>,</mo><mi>Ο</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false">(</mo><mi>Η</mi><mo stretchy="false">)</mo><mo>≥</mo><mn>0</mn><mo>.</mo><mn>5</mn></mtd></mtr><mtr><mtd><mn>0</mn><mo>,</mo><mi>Ο</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false">(</mo><mi>Η</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mn>0</mn><mo>.</mo><mn>5</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中,<i>H</i><sup><i>j</i></sup>表示第<i>j</i>个节点的哈希编码。当<i>H</i><sup><i>j</i></sup>为1时表示第<i>j</i>个节点代表的中层特征被激活;当<i>H</i><sup><i>j</i></sup>为0时表示第<i>j</i>个节点代表的中层特征被抑制。若有服装图像数据集<i>R</i>={<i>I</i><sup>1</sup>,<i>I</i><sup>2</sup>,…,<i>I</i><sup><i>k</i></sup>},其中<i>k</i>为服装图像总数,根据式(5)的哈希编码规则,每幅图像对应的哈希编码为<i>R</i><sub>H</sub>={<i>H</i><sub>1</sub>,<i>H</i><sub>2</sub>,…,<i>H</i><sub><i>k</i></sub>}。</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag"><b>4 相似性学习</b></h3>
                <div class="p1">
                    <p id="88">本文利用相似性学习方法衡量图像哈希编码之间的相似性。图4展示了相似性学习网络的结构,其参数设置如表1所示,其中<i>FC</i>1和<i>FC</i>2为全连接层,<i>h</i>为图3中隐藏层<i>H</i>的节点个数。<i>FC</i>1和<i>FC</i>2使用ReLU (Rectified Linear Unit)激活函数,函数形式如式(6)所示:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">ReLU激活函数为相似性学习网络加入了非线性因素,能够提高网络模型的表达能力。此外,ReLU激活函数可以有效防止梯度消失,所以网络在训练阶段的损失函数值能够得到快速收敛。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 相似性学习网络结构" src="Detail/GetImg?filename=images/JSJK201909021_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 相似性学习网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Structure of similarity learning network</p>

                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 DeepFashion数据集示例" src="Detail/GetImg?filename=images/JSJK201909021_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 DeepFashion数据集示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Examples of DeepFashion dataset</p>

                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表1 相似性学习网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Parameters of similarity learning network</b></p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />Layer</td><td>Number of nodes</td><td>Activation function</td></tr><tr><td><br />Input</td><td>2<i>h</i></td><td></td></tr><tr><td><br /><i>FC</i>1</td><td>512</td><td>ReLU</td></tr><tr><td><br /><i>FC</i>2</td><td>512</td><td>ReLU</td></tr><tr><td><br />Output</td><td>2</td><td>Softmax</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="94">相似性学习网络的输出层使用Softmax激活函数:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>j</mi></msub><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中,<i>s</i><sub><i>i</i></sub>表示预测结果为第<i>i</i>个类别的概率,<i>y</i><sub><i>i</i></sub>和<i>y</i><sub><i>j</i></sub>分别表示输出层中第<i>i</i>个和第<i>j</i>个神经元的输出。通过式(7)可以直观地看出类别概率的归一化分布;在BP时,式(7)也能够方便计算。</p>
                </div>
                <div class="p1">
                    <p id="97">相似性学习网络的输入层包含一对服装图像的哈希编码,输出层包含2个节点,分别代表“相似”与“不相似”2种类别。相似性学习任务实质为二分类问题,其中“正类”代表哈希编码对相似,“负类”代表哈希编码对不相似。相似性学习网络通过最小化交叉熵损失函数(Cross-entropy Loss),并利用小批量梯度下降(Mini-batch Stochastic Gradient Descent)算法来更新网络参数。首先,定义交叉熵损失函数:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>s</mi><mo>′</mo></msup></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>ln</mi><mspace width="0.25em" /><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">其中,<i>n</i>为分类类别数,<i>s</i><sub><i>i</i></sub>为式(7)的结果,<i>s</i>′<sub><i>i</i></sub>为第<i>i</i>个类别的标签值,<i>s</i>′<sub><i>i</i></sub>=0时代表负类,<i>s</i>′<sub><i>i</i></sub>=1时代表正类。然后对式(8)求偏导:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mo>∂</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi></mrow></munder><mspace width="0.25em" /></mstyle><mfrac><mrow><mo>∂</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mo>∂</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mo>∂</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>k</mi></mrow></munder><mspace width="0.25em" /></mstyle><mfrac><mrow><mo>∂</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mo>∂</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mo>∂</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>(</mo><mrow><mo>-</mo><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mfrac><mn>1</mn><mrow><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow><mo>⋅</mo><mo stretchy="false">[</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>k</mi></mrow></munder><mrow><mrow><mo>(</mo><mrow><mo>-</mo><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mfrac><mn>1</mn><mrow><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mstyle><mo stretchy="false">(</mo><mo>-</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mo>-</mo><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><msup><mi>s</mi><mo>′</mo></msup></mstyle><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">其中,<i>s</i><sub><i>k</i></sub>依然为式(7)的结果,<i>s</i>′<sub><i>k</i></sub>代表第<i>k</i>个类别的标签值。又因<i>y</i><sub><i>i</i></sub>可形式化为<i>w</i><sub><i>i</i></sub><i>x</i><sub><i>i</i></sub>+<i>b</i><sub><i>i</i></sub>,则式(9)分别对网络权重<i>w</i><sub><i>i</i></sub>和偏置<i>b</i><sub><i>i</i></sub>求偏导:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mo>∂</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi></mrow><mrow><mo>∂</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mo stretchy="false">(</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">接下来根据式(10)和链式求导法则求出所有权重和偏置的偏导数,进而更新所有网络参数。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag"><b>5 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="105">实验部分采用的服装数据集来源于公开数据集DeepFashion<citation id="229" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,该数据集包含19种服装类别,85 127个服装项目,超过20万对相似图像。以下3部分实验均基于该数据集。图5展示了数据集中部分服装图像示例。</p>
                </div>
                <div class="p1">
                    <p id="106">实验平台硬件配置为处理器Intel<sup>®</sup> Core<sup>TM</sup> i5-7500 3.4 GHz CPU,8 GB内存,GeForce GTX 1070 8 GB显卡。软件环境为Caffe框架,Python 2.7,OpenCV 3.3.1。为了方便模型的训练,在进行实验之前,需要将服装图像数据集预处理为LevelDB格式。此外,5.1节和5.2节使用的基础网络结构如图6所示,参数设置如表2所示。基础网络中<i>Conv</i>1～<i>Conv</i>5为卷积层,<i>FC</i>6～<i>FC</i>8为全连接层;数据归一化采用BN(Batch Normalization)算法<citation id="230" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,池化层采用的策略为最大池化(Max Pooling)。最后<i>FC</i>8层经Softmax函数输出的节点数为19,对应19种服装类别。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基础网络结构" src="Detail/GetImg?filename=images/JSJK201909021_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 基础网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Structure of basic network</p>

                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>5.1 可变形卷积实验</b></h4>
                <div class="p1">
                    <p id="109">为了实现可变形卷积,根据图6和表2构建可变形卷积网络。在训练阶段,偏移量初始值设为0,基础学习率设为0.001,且随着迭代次数的增加,学习率逐渐减小。<i>batch</i>_<i>size</i>设置为32,利用式(3)和式(4)进行参数的学习和更新。</p>
                </div>
                <div class="p1">
                    <p id="110">图7展示了可变形卷积层在预测过程中采样位置的分布情况。图中的点表示3层可变形卷积层的所有采样位置,每一层的感受野大小均为3×3,总共9<sup>3</sup>=729个点。通过对比可以发现,当感受野处于背景区域时,采样位置比较稀疏,且有一部分采样点会朝着服装目标区域偏移;当感受野处于服装目标区域时,绝大部分采样点会自动地聚集在该区域中。因此,在进行服装特征提取时,可变形卷积方式能够有效地提取服装目标的特征,显著减少复杂背景特征的干扰,提高模型对服装几何多样性的适应能力。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表2 基础网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Parameters of basic network</b></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td><br />Layer</td><td>Input</td><td>Kernel <br />Size</td><td>Stride</td><td>Output</td></tr><tr><td><br /><i>Conv</i>1</td><td>3×227×227</td><td>11×11</td><td>4</td><td>96×55×55</td></tr><tr><td><br /><i>BN</i>1+<i>ReLU</i>1</td><td>96×55×55</td><td></td><td></td><td>96×55×55</td></tr><tr><td><br /><i>MaxPool</i>1</td><td>96×55×55</td><td>3×3</td><td>2</td><td>96×27×27</td></tr><tr><td><br /><i>Offset</i>1</td><td>96×27×27</td><td>3×3</td><td>1</td><td>18×27×27</td></tr><tr><td><br /><i>DeformConv</i>1</td><td>96×27×27,<br />18×27×27</td><td></td><td></td><td>512×27×27</td></tr><tr><td><br /><i>Conv</i>2</td><td>512×27×27</td><td>5×5</td><td>1</td><td>256×27×27</td></tr><tr><td><br /><i>BN</i>2+<i>ReLU</i>2</td><td>256×27×27</td><td></td><td></td><td>256×27×27</td></tr><tr><td><br /><i>MaxPool</i>2</td><td>256×27×27</td><td>3×3</td><td>2</td><td>256×13×13</td></tr><tr><td><br /><i>Offset</i>2</td><td>256×13×13</td><td>3×3</td><td>1</td><td>18×13×13</td></tr><tr><td><br /><i>DeformConv</i>2</td><td>256×13×13,<br />18×13×13</td><td></td><td></td><td>256×13×13</td></tr><tr><td><br /><i>Conv</i>3</td><td>256×13×13</td><td>3×3</td><td>1</td><td>384×13×13</td></tr><tr><td><br /><i>BN</i>3+<i>ReLU</i>3</td><td>384×13×13</td><td></td><td></td><td>384×13×13</td></tr><tr><td><br /><i>Offset</i>3</td><td>384×13×13</td><td>3×3</td><td>1</td><td>18×13×13</td></tr><tr><td><br /><i>DeformConv</i>3</td><td>384×13×13,<br />18×13×13</td><td></td><td></td><td>384×13×13</td></tr><tr><td><br /><i>Conv</i>4</td><td>384×13×13</td><td>3×3</td><td>1</td><td>384×13×13</td></tr><tr><td><br /><i>ReLU</i>4</td><td>384×13×13</td><td></td><td></td><td>384×13×13</td></tr><tr><td><br /><i>Conv</i>5</td><td>384×13×13</td><td>3×3</td><td>1</td><td>256×13×13</td></tr><tr><td><br /><i>ReLU</i>5</td><td>256×13×13</td><td></td><td></td><td>256×13×13</td></tr><tr><td><br /><i>MaxPool</i>5</td><td>256×13×13</td><td>3×3</td><td>2</td><td>256×6×6</td></tr><tr><td><br /><i>FC</i>6+<i>ReLU</i>6</td><td>9216×1</td><td></td><td></td><td>4096×1</td></tr><tr><td><br /><i>DropOut</i>6</td><td>4096×1</td><td></td><td></td><td>4096×1</td></tr><tr><td><br /><i>FC</i>7+<i>ReLU</i>7</td><td>4096×1</td><td></td><td></td><td>4096×1</td></tr><tr><td><br /><i>DropOut</i>7</td><td>4096×1</td><td></td><td></td><td>4096×1</td></tr><tr><td><br />Hashcode</td><td>4096×1</td><td></td><td></td><td>128×1</td></tr><tr><td><br /><i>FC</i>8+Softmax</td><td>128×1</td><td></td><td></td><td>19×1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 可变形卷积采样位置" src="Detail/GetImg?filename=images/JSJK201909021_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 可变形卷积采样位置  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Sampling locations of deformable convolution</p>

                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>5.2 图像哈希编码实验</b></h4>
                <div class="p1">
                    <p id="115">本节实验的训练过程与可变形卷积网络的训练过程同时进行。<i>FC</i>7层与<i>FC</i>8层之间的Hashcode层用于提取服装图像的中层特征。模型训练完成后,通过前向运算并结合编码规则可获得图像的哈希编码。图8展示了部分通过可变形卷积网络提取的服装图像的哈希编码。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 服装图像哈希编码" src="Detail/GetImg?filename=images/JSJK201909021_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 服装图像哈希编码  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 Hashcodes of garment images</p>

                </div>
                <div class="p1">
                    <p id="117">为了衡量生成哈希编码的有效性,隐藏层<i>H</i>的节点个数<i>h</i>分别取64,128,256,并和已有方法对比,衡量标准为有监督分类的准确率。首先从DeepFashion数据集中随机选取5 000幅图像作为测试数据集,然后对测试集中的图像进行分类预测,预测正确结果数与测试集中图像总数的比值即为分类的准确率。</p>
                </div>
                <div class="p1">
                    <p id="118">本文所述的图像哈希编码方法与现有方法(AlexNet+Fine-tuning<citation id="231" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、Network In Network+Dropout<citation id="232" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、VGG-16+Fine-tuning<citation id="233" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、ResNet-50+Fine-tuning<citation id="234" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、FashionNet<citation id="235" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>)的对比结果如表3所示。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表3 不同分类方法性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Performance comparison of different classification methods</b></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td colspan="2"><br />Methods</td><td>Accuracy/%</td></tr><tr><td colspan="2"><br />AlexNet+Fine-tuning</td><td>87.11</td></tr><tr><td colspan="2"><br />Network In Network+Dropout</td><td>89.59</td></tr><tr><td colspan="2"><br />VGG-16+Fine-tuning</td><td>89.35</td></tr><tr><td colspan="2"><br />ResNet-50+Fine-tuning</td><td>89.96</td></tr><tr><td colspan="2"><br />FashionNet</td><td>90.17</td></tr><tr><td colspan="2"><br />Ours without deformable convolution</td><td>89.72</td></tr><tr><td><br /></td><td><i>h</i> =64</td><td>90.45</td></tr><tr><td><br />Ours with deformable <br />convolution</td><td><i>h</i> =128</td><td>92.79</td></tr><tr><td><br /></td><td><i>h</i> =256</td><td>90.23</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">从表3中可以看出,通过向网络中添加用于提取图像中层特征的隐藏层可以显著提高服装图像分类的准确率。需要注意的是,当隐藏层的节点个数为256时,准确率下降了2.56%,这是因为随着隐藏层节点个数的增加,网络训练参数增多,导致训练模型出现过拟合,因此准确率会略微下降。</p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>5.3 相似性学习实验</b></h4>
                <div class="p1">
                    <p id="122">基于特征空间距离的相似性度量方式很难保证图像的低层视觉特征与高层语义特征相一致。为了保证图像高低层特征的一致性,本文提出使用基于相似性学习的度量方式。根据图4和表1构建相似性学习网络,网络参数初始化方式为随机初始化。在进行检索时,先利用相似性学习网络挑选出与查询图像相似的图像集合,然后再根据“相似”分类的评分对图像集合进行降序排序,排序结果即为最终的检索结果。图9展示了Top 5检索结果。从图9中可以看出,Query图像与检索结果图像在视觉与语义特征方面相一致。此外,尽管Query图像中包含了复杂背景,而且服装目标也发生了几何变形,但模型依然能够感知到这种几何变形并减少了背景信息对检索结果的影响,提高了检索结果的准确性。</p>
                </div>
                <h3 id="123" name="123" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="124">针对传统服装检索方法中因使用固定形状的感受野而无法有效提取其特征的问题,本文提出利用可变形卷积网络,自动学习图像特征的采样位置,使网络专注于服装目标特征的感知,提高模型对服装几何变形的适应能力。此外,为了保证查询图像与检索结果图像的视觉一致性,本文提出服装图像哈希编码和相似性学习方法,实现了视觉特征相似的同时,语义特征也相似。实验结果表明,本文提出的检索方法与表3中基于现有模型微调的传统方法相比,具有较高的准确率和较强的实用性。然而,本文中可变形卷积网络和相似性学习网络的训练过程是分为2个不同阶段,首先训练可变形卷积网络,以获得服装图像的哈希编码;其次训练相似性学习网络,以衡量哈希编码对代表的服装图像相对的相似性,因此模型整体训练效率较低。未来工作考虑将训练过程合二为一,构建一个完整的端到端的训练模型,从而提高检索模型的训练效率。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909021_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 Top 5检索结果" src="Detail/GetImg?filename=images/JSJK201909021_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 Top 5检索结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909021_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 9 Top 5 retrieval results</p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="153">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Content-Based Image Retrieval at the End of the Early Years">

                                <b>[1]</b> Smeulders A W M,Worring M,Santini S,et al.Content-based image retrieval at the end of the early years[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2000,22(12):1349-1380.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Content Based Image Retrieval Using Color, Texture and Shape Features">

                                <b>[2]</b> Hiremath P S,Pujari J.Content based image retrieval using color,texture and shape features[C]//Proc of the 15th International Conference on Advanced Computing and Communications,2007:780-784.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600107343&amp;v=MTAwMzZlcnFRVE1ud1plWnVIeWptVUxmSUpsOFdhQnM9TmlmT2ZiSzhIdERNcVk5Rlplc0lEM2c2b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Liu G H,Yang J Y.Content-based image retrieval using color difference histogram[J].Pattern Recognition,2013,46(1):188-198.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Infrared image segmentation using HOG feature and kernel extreme learning machine">

                                <b>[4]</b> Liang Y,Wang L,Zhang L.Infrared image segmentation using HOG feature and kernel extreme learning machine[C]// Proc of Applied Optics and Photonics China,2015:1376-1383.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Content based image retrieval using enhanced SURF">

                                <b>[5]</b> Gopal N,Bhooshan R S.Content based image retrieval using enhanced SURF[C]//Proc of the 15th National Conference on Computer Vision,Pattern Recognition,Image Processing and Graphics,2015:1-4.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image retrieval system based on multi-feature fusion and relevance feedback">

                                <b>[6]</b> Wang J Y,Zhu Z.Image retrieval system based on multi-feature fusion and relevance feedback[C]// Proc of International Conference on Machine Learning and Cybernetics,2010:2053-2058.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A color,texture and shape based hybrid approach for clothing retrieval techniques">

                                <b>[7]</b> Chaudhary P,Sharma S.A color,texture and shape based hybrid approach for clothing retrieval techniques[J].International Journal of Mechanical Engineering and Computer Applications,2016,4(6):382-387.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-domain image retrieval with a dual attribute-aware ranking network">

                                <b>[8]</b> Huang J,Feris R S,Chen Q,et al.Cross-domain image retrieval with a dual attribute-aware ranking network[C]// Proc of International Conference on Computer Vision,2015:1062-1070.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Search with Attribute-aware Deep Network">

                                <b>[9]</b> Huang J,Xia W,Yan S.Deep search with attribute-aware deep network[C]//Proc of International Conference on Multimedia,2014:731-732.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-scenario clothing retrieval and fine-grained style recognition">

                                <b>[10]</b> Li Z,Li Y,Tian W,et al.Cross-scenario clothing retrieval and fine-grained style recognition[C] // Proc of International Conference on Pattern Recognition (ICPR 2016),2016:2912-2917.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Li Zhi,Sun Yu-bao,Wang Feng,et al.Clothing image classification and retrieval algorithm based on deep convolutional neural network[J].Computer Engineering,2016,42(11):309-315.(in Chinese)
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201711037&amp;v=Mjk0MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1ViM0JMejdCZDdHNEg5Yk5ybzlHWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Chen Ai-ai,Li Lai,Liu Guang-can,et al.Clothing retrieval based on landmarks[J].Journal of Computer Applications,2017,37(11):3249-3255.(in Chinese)
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clothing retrieval with visual attention model">

                                <b>[13]</b> Wang Z,Gu Y,Zhang Y,et al.Clothing retrieval with visual attention model[C]// Proc of Visual Communications and Image Processing (VCIP),2017:1-4.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201802024&amp;v=MTA2NDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzQkx6N0JaYkc0SDluTXJZOUhZSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Zhang Zhen-huan,Zhou Cai-lan,Liang Yuan.An optimized clothing classification algorithm based on residual convolutional neural network[J].Computer Engineering &amp; Science,2018,40(2):354-360.(in Chinese)
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Where to Buy It:Matching Street Clothing Photos in Online Shops">

                                <b>[15]</b> Hadi K M,Han X,Lazebnik S,et al.Where to buy it:Matching street clothing photos in online shops[C] // Proc of International Conference on Computer Vision,2015:3343-3351.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM31F4DDE5EC2A525567D3D3CB7DD8ECA9&amp;v=MTE5OTVSM2hWQkRicmhOc3VXQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3JtOXdxRT1OaWZJWTdDNWFOVzQyL3BBRVpnTmZYazd5aE1WN1V0K1BIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Bell S,Bala K.Learning visual similarity for product design with convolutional neural networks[J].ACM Transactions on Graphics (TOG),2015,34(4):98-98.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Matching user photos to online products with robust deep features">

                                <b>[17]</b> Wang X,Sun Z,Zhang W,et al.Matching user photos to online products with robust deep features[C]//Proc of International Conference on Multimedia Retrieval,2016:7-14.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Getting the Look:Clothing Recognition and Segmentation for Automatic Product Suggestions in Everyday Photos">

                                <b>[18]</b> Kalantidis Y,Kennedy L,Li L J.Getting the look:Clothing recognition and segmentation for automatic product suggestions in everyday photos[C]//Proc of International Conference on Multimedia Retrieval,2013:105-112.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepFashion:Powering Robust Clothes Recognition and Retrieval with Rich Annotations">

                                <b>[19]</b> Liu Z,Luo P,Qiu S,et al.DeepFashion:Powering robust clothes recognition and retrieval with rich annotations[C] // Proc of 2016 International Conference on Computer Vision and Pattern Recognition,2016:1096-1104.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep part-based image feature for clothing retrieval">

                                <b>[20]</b> Zhou L,Zhou Z,Zhang L.Deep part-based image feature for clothing retrieval[C]//Proc of International Conference on Neural Information Processing,2017:340-347.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD3837D9B0EA32C110282342F1CBBBAB47&amp;v=MzI1NTFDcGJRMzVOQmh3cm05d3FFPU5qbkJhckN3SGRhNHB2MUZFWm9NRGc4NHpoWVI0ajErVEgyVXJXRkhDOERsTjc2WUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Yu X,Wu X,Luo C,et al.Deep learning in remote sensing scene classification:A data augmentation enhanced convolutional neural network framework[J].GI Science &amp; Remote Sensing,2017,54(5):741-758.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deformable Convolutional Networks">

                                <b>[22]</b> Dai J,Qi H,Xiong Y,et al.Deformable convolutional networks[C] // Proc of International Conference on Computer Vision (ICCV 2017),2017:764-773.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[23]</b> Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C] // Proc of International Conference on Machine Learning,2015:448-456.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_24" >
                                    <b>[24]</b>
                                 Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]// Proc of International Conference on Neural Information Processing Systems,2012:1106-1114.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning of binary hash codes for fast image retrieval">

                                <b>[25]</b> Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,2015:27-35.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[26]</b> Very deep convolutional networks for large-scale image recognition[EB/OL].[2015-04-10].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[27]</b> He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]// Proc of 2016 Conference on Computer Vision and Pattern Recognition,2016:770-778.附中文参考文献:
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201611054&amp;v=MDY2ODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1ViM0JMejdCYmJHNEg5Zk5ybzlBWUlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 厉智,孙玉宝,王枫,等.基于深度卷积神经网络的服装图像分类检索算法[J].计算机工程,2016,42(11):309-315.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 陈嫒嫒,李来,刘光灿,等.基于关键点的服装检索[J].计算机应用,2017,37(11):3249-3255.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 张振焕,周彩兰,梁媛.基于残差的优化卷积神经网络服装分类算法[J].计算机工程与科学,2018,40(2):354-360.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909021" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909021&amp;v=MDMzNDBGckNVUkxPZVplUm1GeTdrVWIzQkx6N0JaYkc0SDlqTXBvOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
