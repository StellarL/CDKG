<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358081280000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909018%26RESULT%3d1%26SIGN%3dY0HcR%252f2JG9Pc%252bahNdQThP1gkRBQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909018&amp;v=MTg3NTh0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnpCTHo3QlpiRzRIOWpNcG85RWJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;2 基本原理&lt;/b&gt; "><b>2 基本原理</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;2.1 光度误差&lt;/b&gt;"><b>2.1 光度误差</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;2.2 极线约束&lt;/b&gt;"><b>2.2 极线约束</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;3 多约束框架介绍&lt;/b&gt; "><b>3 多约束框架介绍</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;3.1 特征点匹配与本质矩阵提取&lt;/b&gt;"><b>3.1 特征点匹配与本质矩阵提取</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;3.2 网络结构&lt;/b&gt;"><b>3.2 网络结构</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;3.3 匹配点对位置约束&lt;/b&gt;"><b>3.3 匹配点对位置约束</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;3.4 极线约束与平滑约束&lt;/b&gt;"><b>3.4 极线约束与平滑约束</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;3.5 非相邻图像之间的损失计算&lt;/b&gt;"><b>3.5 非相邻图像之间的损失计算</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="&lt;b&gt;4 实验结果&lt;/b&gt; "><b>4 实验结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;4.1 训练细节&lt;/b&gt;"><b>4.1 训练细节</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;4.2 实验评估&lt;/b&gt;"><b>4.2 实验评估</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;4.3 实验对比&lt;/b&gt;"><b>4.3 实验对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 多约束框架结构示意图">图1 多约束框架结构示意图</a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表1 深度评估结果&lt;/b&gt;"><b>表1 深度评估结果</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表2 时间分析结果&lt;/b&gt; s"><b>表2 时间分析结果</b> s</a></li>
                                                <li><a href="#100" data-title="图2 深度预测对比">图2 深度预测对比</a></li>
                                                <li><a href="#102" data-title="图3 深度预测对比">图3 深度预测对比</a></li>
                                                <li><a href="#104" data-title="图4 模型在KITTI数据集上的表现">图4 模型在KITTI数据集上的表现</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="172">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2012:3354-3361.</a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_2" title=" Garg R,Bg V K,Cameiro G,et al.Unsupervised CNN for single view depth estimation:Geometry to the rescue[C]//Proc of European Conference on Computer Vision,2016:740-756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised CNN for Single View Depth Estimation:Geometry to the Rescue">
                                        <b>[2]</b>
                                         Garg R,Bg V K,Cameiro G,et al.Unsupervised CNN for single view depth estimation:Geometry to the rescue[C]//Proc of European Conference on Computer Vision,2016:740-756.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_3" title=" Godard C,Aodha O M,Brostow G J.Unsupervised monocular depth estimation with left-right consistency[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017:6602-6611." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised monocular depth estimation with leftright consistency">
                                        <b>[3]</b>
                                         Godard C,Aodha O M,Brostow G J.Unsupervised monocular depth estimation with left-right consistency[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017:6602-6611.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_4" title=" Zhou T,Brown M,Snavely N,et al.Unsupervised learning of depth and ego-motion from video[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2017:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Learning of Depth and Ego-Motion from Video">
                                        <b>[4]</b>
                                         Zhou T,Brown M,Snavely N,et al.Unsupervised learning of depth and ego-motion from video[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2017:1-10.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_5" title=" Vijayanarasimhan S,Ricco S,Schmid C,et al.Sfm-net:Learning of structure and motion from video[J].arXiv:1704.07804,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sfm-net:Learning of structure and motion from video">
                                        <b>[5]</b>
                                         Vijayanarasimhan S,Ricco S,Schmid C,et al.Sfm-net:Learning of structure and motion from video[J].arXiv:1704.07804,2017.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_6" title=" Zhou L,Ye J,Abello M,et al.Unsupervised learning of monocular depth estimation with bundle adjustment,super-resolution and clip loss[J].arXiv:1812.03368,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised learning of monocular depth estimation with bundle adjustment super-resolution and clip loss">
                                        <b>[6]</b>
                                         Zhou L,Ye J,Abello M,et al.Unsupervised learning of monocular depth estimation with bundle adjustment,super-resolution and clip loss[J].arXiv:1812.03368,2018.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_7" title=" Engel J,Sch&#246;ps T,Cremers D.LSD-SLAM:Large-scale direct monocular SLAM[C]//Proc of European Conference on Computer Vision,2014:834-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD-SLAM:Large-Scale Direct Monocular SLAM">
                                        <b>[7]</b>
                                         Engel J,Sch&#246;ps T,Cremers D.LSD-SLAM:Large-scale direct monocular SLAM[C]//Proc of European Conference on Computer Vision,2014:834-849.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_8" title=" Mur-Artal R,Montiel J M M,Tardos J D.ORB-SLAM:A versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics,2015,31(5):1147-1163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM: A Versatile and Accurate Monocular SLAM System">
                                        <b>[8]</b>
                                         Mur-Artal R,Montiel J M M,Tardos J D.ORB-SLAM:A versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics,2015,31(5):1147-1163.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_9" title=" Jaderberg M,Karen S,Andrew Z.Spatial transformer networks[J].arXiv:1506.02025v2,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial transformer networks">
                                        <b>[9]</b>
                                         Jaderberg M,Karen S,Andrew Z.Spatial transformer networks[J].arXiv:1506.02025v2,2015.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_10" title=" Bay H,Ess A,Tuytelaars T,et al.SRUF:Speeded up robust features[C]//Proc of European Conference on Computer Vision,2006:1-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SRUF:Speeded up robust features">
                                        <b>[10]</b>
                                         Bay H,Ess A,Tuytelaars T,et al.SRUF:Speeded up robust features[C]//Proc of European Conference on Computer Vision,2006:1-14.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_11" title=" Hartley R I.In defense of the eight-point algorithm[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,19(6):580-593." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of eight-point algorithm">
                                        <b>[11]</b>
                                         Hartley R I.In defense of the eight-point algorithm[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,19(6):580-593.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_12" title=" Abadi M,Barham P,Chen J M,et al.Tensorflow:A system for large-scale machine learning[C]//Proc of the 12th {USENIX} Symposium on Operating Systems Design and Implementation,2016:265-283." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A System for Large-Scale Machine Learning">
                                        <b>[12]</b>
                                         Abadi M,Barham P,Chen J M,et al.Tensorflow:A system for large-scale machine learning[C]//Proc of the 12th {USENIX} Symposium on Operating Systems Design and Implementation,2016:265-283.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_13" title=" Kingma D P,Ba J.Adam:A method for stochastic optimization[J].arXiv:1412.6980,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization">
                                        <b>[13]</b>
                                         Kingma D P,Ba J.Adam:A method for stochastic optimization[J].arXiv:1412.6980,2014.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_14" title=" Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of International Conference on International Conference on Machine Learning,2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">
                                        <b>[14]</b>
                                         Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of International Conference on International Conference on Machine Learning,2015:448-456.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_15" title=" Eigen D,Puhrsch C,Fergus R.Depth map prediction from a single image using a multi-scale deep network [C]//Proc of Advances in Neural Information Processing Systems,2014:2366-2374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depth map prediction from a single image using a multi-scale deep network">
                                        <b>[15]</b>
                                         Eigen D,Puhrsch C,Fergus R.Depth map prediction from a single image using a multi-scale deep network [C]//Proc of Advances in Neural Information Processing Systems,2014:2366-2374.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_16" title=" Liu F,Shen C,Lin G,et al.Learning depth from single monocular images using deep convolutional neural fields[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,30(10):2024-2039." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning depth from single monocular images using deep convolutional neural fields">
                                        <b>[16]</b>
                                         Liu F,Shen C,Lin G,et al.Learning depth from single monocular images using deep convolutional neural fields[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,30(10):2024-2039.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1650-1655 DOI:10.3969/j.issn.1007-130X.2019.09.017            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多约束优化的图像深度信息估计</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%B5%A9%E7%BF%94&amp;code=42876323&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁浩翔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%A7%9D&amp;code=27032110&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈姝</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E6%95%8F&amp;code=29385700&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林敏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%98%E6%BD%AD%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0006772&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湘潭大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出了一种基于深度学习的多约束框架,用于从单目视频中预测深度图。该框架不仅通过最小化光度误差来对网络进行优化,还引入了匹配点对约束和极线约束来弥补光度误差在无纹理区域和光照变化情况下的不稳定性。此外,该框架还加入了非连续图像之间的约束来改善模型的表现。通过与其他深度估计方法进行对比分析,结果表明:该框架可以提高深度预测的准确性,增强了模型在处理无纹理区域和光照变化时的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E7%BA%A6%E6%9D%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多约束;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    袁浩翔（1993-），男，湖南株洲人，硕士，CCF会员（A9718G），研究方向为计算机视觉。E-mail:lovpodtt@163.com,通信地址：411105湖南省湘潭市湘潭大学信息工程学院&lt;image id="169" type="formula" href="images/JSJK201909018_16900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    陈姝,通信地址：411105湖南省湘潭市湘潭大学信息工程学院;
                                </span>
                                <span>
                                    林敏,通信地址：411105湖南省湘潭市湘潭大学信息工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>湖南省自然科学基金(2017JJ2252);</span>
                                <span>湖南省教育厅青年项目(16B258);</span>
                    </p>
            </div>
                    <h1><b>Depth information estimation based on multi-constraint optimization</b></h1>
                    <h2>
                    <span>YUAN Hao-xiang</span>
                    <span>CHEN Shu</span>
                    <span>LIN Min</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering,Xiangtan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>We propose a multi-constraint framework based on deep learning to predict depth maps from monocular videos. The framework not only optimizes the network by minimizing the photometric error, but also introduces the matching point constraints and epipolar constraints to compensate for the instability of the photometric error in texture-less regions and varying illumination conditions. In addition, the framework adds the constraints between non-adjacent frames to the model to improve its performance. Compared with other methods,our method can improve the accuracy of depth prediction and enhance the robustness of the model in handling texture-less regions and varied illumination.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=computer%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">computer vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depth%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depth estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-constraint&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-constraint;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YUAN Hao-xiang,born in 1993,MS, CCF member(A9718G),his research interest includes computer vision.Address:College of Information Engineering,Xiangtan University,Xiangtan 411105,Hunan,P.R.China;
                                </span>
                                <span>
                                    CHEN Shu,Address:College of Information Engineering,Xiangtan University,Xiangtan 411105,Hunan,P.R.China;
                                </span>
                                <span>
                                    LIN Min,Address:College of Information Engineering,Xiangtan University,Xiangtan 411105,Hunan,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-03-01</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="36">从单目图像中提取三维场景的深度信息一直是计算机视觉领域中的一个难题。近年来随着深度学习的飞速发展,许多计算机视觉任务都开始借助深度学习方法来解决相关问题。其中,卷积神经网络凭借其强大的特征提取能力成为了解决视觉问题的重要工具,在图像分类、语义分割和深度估计等任务都被大量使用。根据是否存在样本的真实值来对网络输出进行约束,深度学习可以分为监督学习和无监督学习。在深度估计任务中,获取高精度的深度真实值只能依靠价格高昂的设备,这就导致了建立数据集的成本非常高。例如KITTI数据集<citation id="204" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,尽管使用了先进的3D传感器、多个校准相机和惯性处理器,但获取的深度图也非常稀疏。因此,许多学者开始研究深度估计的无监督学习方法。文献<citation id="205" type="reference">[<a class="sup">2</a>]</citation>使用双目立体图像对来进行无监督训练,该方法利用右图像和预测出的视差图来合成左图像,然后将光度误差作为约束来进行优化。文献<citation id="206" type="reference">[<a class="sup">3</a>]</citation>加入左右一致性检查,提高了预测的准确度。文献<citation id="207" type="reference">[<a class="sup">4</a>]</citation>开始使用单目图像来进行训练,该方法将前后帧图像之间的光度误差作为约束来进行无监督训练。在此基础上,文献<citation id="208" type="reference">[<a class="sup">5</a>]</citation>将图像中的动态物体分离出来,并对其运动单独进行预测,以此来解决图像中存在动态物体的问题。文献<citation id="209" type="reference">[<a class="sup">6</a>]</citation>则通过添加非连续图像之间的约束来提高预测的准确率。</p>
                </div>
                <div class="p1">
                    <p id="37">但是,之前的方法大部分以2帧图像之间的光度误差作为约束,而计算光度误差需要满足一个前提条件,那就是灰度不变假设。因此,光照变化对光度误差的计算有很大影响,并且在无纹理区域,将光度误差作为约束并不可靠。对此,本文提出一种无监督的多约束深度估计框架。该框架在将光度误差作为约束的基础上,加入2帧图像之间的匹配点对约束来提高预测深度图的准确率和模型对光照变化的适应性。同时,为了减少无纹理区域对计算光度误差的影响,我们还引入了极线约束。极线约束给出了2帧图像匹配点的空间位置关系,并且不会受到无纹理区域的影响。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag"><b>2 基本原理</b></h3>
                <h4 class="anchor-tag" id="39" name="39"><b>2.1 光度误差</b></h4>
                <div class="p1">
                    <p id="40">计算2帧图像之间的光度误差是本文框架的一个重要部分。假设<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>与<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sub><i>s</i></sub>分别为2帧图像<i>I</i><sub><i>t</i></sub>与<i>I</i><sub><i>s</i></sub>上的某个像素点的齐次坐标,并且这2个像素点是由同1个空间点投影而来的,可以得到<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sub><i>t</i></sub>与<mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>˜</mo></mover></math></mathml><sub><i>s</i></sub>之间的关系为:</p>
                </div>
                <div class="p1">
                    <p id="41" class="code-formula">
                        <mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>s</mi></msub><mo>∼</mo><mi mathvariant="bold-italic">Κ</mi><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>→</mo><mi>s</mi></mrow></msub><mi>D</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>)</mo></mrow><mi>Κ</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mover accent="true"><mi>p</mi><mo>˜</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="42">其中,<i>D</i><sub><i>t</i></sub>为第<i>t</i>帧的深度图,<i>D</i><sub><i>t</i></sub>(<i><b>p</b></i><sub><i>t</i></sub>)代表了深度图中像素点<i><b>p</b></i><sub><i>t</i></sub>处的深度值,<i><b>K</b></i>为相机的内参矩阵,<i><b>T</b></i><sub><i>t</i></sub><sub>→</sub><sub><i>s</i></sub>为2帧图像之间的变换矩阵。为了计算光度误差,需要引入灰度不变假设,即同1个空间点的像素灰度值,在各个图像中固定不变,那么应该有<i>I</i><sub><i>t</i></sub>(<i><b>p</b></i><sub><i>t</i></sub>)=<i>I</i><sub><i>s</i></sub>(<i><b>p</b></i><sub><i>s</i></sub>)。因此,单像素的光度误差可以用式(2)来表示:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>e</mi><mo>=</mo><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>)</mo></mrow><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mi>s</mi></msub><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">在训练时,相机间的位姿变换<i><b>T</b></i><sub><i>t</i></sub><sub>→</sub><sub><i>s</i></sub>和深度图<i>D</i><sub><i>i</i></sub>均由神经网络预测而来。如果相机之间的位姿变换<i><b>T</b></i><sub><i>t</i></sub><sub>→</sub><sub><i>s</i></sub>和深度图<i>D</i><sub><i>i</i></sub>越准确,匹配点对之间的对应关系也应该越准确。</p>
                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>2.2 极线约束</b></h4>
                <div class="p1">
                    <p id="46">在多视几何中,每个图像的像素点<i><b>p</b></i><sub><i>t</i></sub>都对应着三维空间中的一条射线,该射线在其他图像上投影所成的线段被称为<i><b>p</b></i><sub><i>t</i></sub>极线,记作<i><b>Fp</b></i><sub><i>t</i></sub>。因此,<i><b>p</b></i><sub><i>t</i></sub>在其他图像上所对应的匹配点<i><b>p</b></i><sub><i>s</i></sub>应该处于极线<i><b>Fp</b></i><sub><i>t</i></sub>上,满足:</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mi>s</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">F</mi><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="48">其中,<i><b>F</b></i>为基础矩阵,描述了像点到极线的映射关系。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>3 多约束框架介绍</b></h3>
                <div class="p1">
                    <p id="50">传统即时定位与地图构建SLAM(Simultaneous Localization and Mapping)的前端方法主要分为光流法<citation id="210" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>与特征点法<citation id="211" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。光流法是以2帧图像之间的光度误差作为约束,从而对相机运动进行优化求解。光流法需要满足灰度不变假设的前提,因此光流法对光照变化非常敏感。并且,当相机或图像中物体运动过快时,光流法较难收敛到正确的结果。</p>
                </div>
                <div class="p1">
                    <p id="51">文献<citation id="212" type="reference">[<a class="sup">4</a>]</citation>以光流法原理为基础,在深度估计框架中使用卷积神经网络代替了光流法中的传统优化方法。框架以一段图像序列作为输入(假定输入图像为<i>I</i><sub><i>t</i></sub>和<i>I</i><sub><i>t</i></sub><sub>+1</sub>),并包含2个卷积神经网络,这2个网络分别预测出2帧图像之间的相机位姿变换<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">Τ</mi><mo>^</mo></mover></math></mathml><sub><i>t</i></sub><sub>→</sub><sub><i>t</i></sub><sub>+1</sub>和图像<i>I</i><sub><i>t</i></sub>的深度图<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>D</mi><mo>^</mo></mover></math></mathml><sub><i>t</i></sub>。接着利用式(1)得到图像<i>I</i><sub><i>t</i></sub>上所有像素映射到图像<i>I</i><sub><i>t</i></sub><sub>+1</sub>上的坐标值。最后,利用双线性插值根据<i>I</i><sub><i>t</i></sub><sub>+1</sub>来重构图像<i>I</i><sub><i>t</i></sub>。该重构图像<i>I</i><sub><i>t</i></sub>的详细方法请参阅文献<citation id="213" type="reference">[<a class="sup">9</a>]</citation>。将重构出来的<i>I</i><sub><i>t</i></sub>记作<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ι</mi><mo>^</mo></mover></math></mathml><sub><i>t</i></sub><sub>+1</sub>,图像<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Ι</mi><mo>^</mo></mover></math></mathml><sub><i>t</i></sub><sub>+1</sub>与真实<i>I</i><sub><i>t</i></sub>之间的光度误差以<i>L</i><sub>1</sub>范数表示,则可得到文献<citation id="214" type="reference">[<a class="sup">4</a>]</citation>中的主要约束:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>v</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>p</mi></munder><mrow><mrow><mo>|</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow><mo>-</mo><mover accent="true"><mi>Ι</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">其中,<i>L</i><sub>vs</sub>表示光度误差的损失<citation id="215" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="54">然而文献<citation id="216" type="reference">[<a class="sup">4</a>]</citation>中的光度误差约束也需要遵从灰度不变假设,对光照变化的鲁棒性相当差。在实际应用中,特别是在室外场景下,光照很容易发生变化,灰度不变假设难以成立。针对光度误差在光照变化情况下失去约束作用的问题,本文使用匹配点对之间的像素坐标关系来增强模型的约束能力。相比较而言,由于匹配特征点受光照变化的影响较少,特征点法更鲁棒。并且在相机或图像中的物体运动幅度过大时,只要求得正确的匹配点对,都可以优化得到正确的目标值。</p>
                </div>
                <div class="p1">
                    <p id="55">此外,在图像低纹理区域,像素周围的灰度值都非常相近,导致训练过程中,低纹理区域的像素梯度都趋近于零,光度误差约束在这种情况下无法对网络参数的更新提供任何贡献。并且在低纹理区域中,特征点的提取会变得比较困难。对此,本文使用极线约束来弥补光度误差在低纹理区域的失效问题。极线约束可以保证图像中的像素映射到其他视图后,位置会处于其极线上。这样一来,即便在低纹理区域,也可以将像素的投影位置限制到它的极线上,而不是整个低纹理区域。</p>
                </div>
                <div class="p1">
                    <p id="56">本文提出的多约束框架包括3个部分:特征点提取与匹配、本质矩阵提取和网络部分。多约束框架是在文献<citation id="217" type="reference">[<a class="sup">4</a>]</citation>的基础上进行改进的,但相较于文献<citation id="218" type="reference">[<a class="sup">4</a>]</citation>中的框架,本文提出了匹配点对位置约束,并将极线约束应用到网络当中。此外,我们参考了文献<citation id="219" type="reference">[<a class="sup">6</a>]</citation>所提出的方法,不仅仅使用相邻图像之间的约束来优化网络,在非相邻图像之间也加入约束来进一步优化网络。但与其不同的是,文献<citation id="220" type="reference">[<a class="sup">6</a>]</citation>采用了非相邻图像之间的深度一致性进行约束,而本文采用的是非相邻图像之间的匹配点对位置约束与极线约束。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909018_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多约束框架结构示意图" src="Detail/GetImg?filename=images/JSJK201909018_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 多约束框架结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909018_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Overview of multi-constrant framework</p>

                </div>
                <div class="p1">
                    <p id="58">训练时,多约束框架会将一段连续的图像序列作为输入,并预测出图像序列中除最后一帧图像外所有图像的深度图。同时,框架还会预测出图像序列中每对相邻图像对之间的相机位姿变换。我们用光度误差、匹配点对约束、极线约束和平滑约束来对框架中的网络进行优化。框架的整个训练过程都只有彩色图像序列的参与,不需要深度图真实值来进行约束,是一个无监督学习框架。</p>
                </div>
                <div class="p1">
                    <p id="59">虽然在训练时,多约束框架需要深度预测网络与位姿预测网络共同训练完成优化,并且还需要对输入的图像序列进行特征提取与匹配,求解出本质矩阵<i><b>E</b></i>。但是,在进行测试时,只需要将单幅图像输入深度预测网络即可获得深度图。图1为整个框架的示意图。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>3.1 特征点匹配与本质矩阵提取</b></h4>
                <div class="p1">
                    <p id="61">由于加入了匹配点对之间的位置约束,框架中需要进行特征点的提取与匹配。考虑到训练时间的开销、图像中物体的尺度变化以及光照变化的影响,我们选择采用加速稳健特征算法<i>SURF</i>(<i>Speeded Up Robust Features</i>)<citation id="221" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>进行特征点提取与匹配。<i>SURF</i>算法通过计算积分图像与<i>Fast Hessian</i>矩阵提高了特征点检测的速度,并且该算法对尺度缩放、光照变化都具有良好的适应性。</p>
                </div>
                <div class="p1">
                    <p id="62">框架首先使用<i>SURF</i>算法提取输入图像序列中的每对图像的特征点(包括非连续图像对的特征点),并对特征点进行特征匹配。在网络训练时,我们需要用到匹配点对的像素坐标来对网络进行约束,将特征点对光照变化和尺度缩放的适应性应用到网络中,从而增强网络对光照变化的鲁棒性,并提高模型预测的准确性。</p>
                </div>
                <div class="p1">
                    <p id="63">同时,为了引入极线约束,需要通过提取本质矩阵来得到图像上像素点与其极线之间的映射关系。本文框架中,通过八点法<citation id="222" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>算出2帧图像之间的本质矩阵。提取出的本质矩阵将被用于3.4节中的损失计算。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>3.2 网络结构</b></h4>
                <div class="p1">
                    <p id="65">参考文献<citation id="223" type="reference">[<a class="sup">4</a>]</citation>中的网络结构,多约束框架包括2个卷积神经网络,分别为深度预测网络和位姿预测网络。深度预测网络将一段图像序列作为输入(假设输入的图像序列长度为N),并输出图像序列中除最后一帧图像外的所有图像的深度图。位姿预测网络也是以相同的图像序列作为输入,同时会输出每对相邻图像对之间的相机位姿变换。</p>
                </div>
                <div class="p1">
                    <p id="66">深度预测网络是一个全卷积网络,输入图像首先经过14层卷积层进行特征提取,其中单数层的步长为2,双数层的步长为1。接着,被下采样的特征图再通过7次步长为2的转置卷积层,被上采样成与输入图像大小相同的单通道图像,从而得到预测的深度图。</p>
                </div>
                <div class="p1">
                    <p id="67">位姿预测网络则是一个普通的卷积网络,包括7个卷积层和2个全连接层。每个卷积层的步长都为2,最终输出N-1个6维的相机位姿向量,其中旋转用三维的欧拉角表示。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>3.3 匹配点对位置约束</b></h4>
                <div class="p1">
                    <p id="69">在3.1节中,多约束框架通过<i>SURF</i>算法提取了特征点并进行了特征点匹配。假设匹配点对的数量为w。其中一对匹配点的像素坐标分别用<i><b>p</b></i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>∈<i>I</i><sub><i>t</i></sub>和<i><b>p</b></i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>∈<i>I</i><sub><i>t</i></sub><sub>+1</sub>来表示,<i>i</i>和<i>t</i>分别为像素点和图像的索引。通过式(1)可以计算出在图像<i>I</i><sub><i>t</i></sub><sub>+1</sub>上与<i><b>p</b></i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>相匹配的像素坐标<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover></math></mathml><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>。将<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover></math></mathml><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>与<i><b>p</b></i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>之间差值的绝对值作为连续图像之间匹配点对的损失,用<i>L</i><sub>ps</sub>表示:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>3.4 极线约束与平滑约束</b></h4>
                <div class="p1">
                    <p id="72">极线约束的损失定义为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mi>t</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">Κ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">E</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>→</mo><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="bold-italic">Κ</mi><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中,<i><b>K</b></i>为相机内参。<i><b>E</b></i><sub><i>t</i></sub><sub>→</sub><sub><i>t</i></sub><sub>+1</sub>是图像<i>I</i><sub><i>t</i></sub>和图像<i>I</i><sub><i>t</i></sub><sub>+1</sub>之间的本质矩阵,它与式(3)中的基本矩阵<i><b>F</b></i>的转换关系为<i><b>E</b></i>=<i><b>K</b></i><sup>T</sup><i><b>FK</b></i>。<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover></math></mathml><sub><i>t</i></sub><sub>+1</sub>代表图像<i>I</i><sub><i>t</i></sub>上的像素<i><b>p</b></i><sub><i>t</i></sub>通过式(1)映射到图像<i>I</i><sub><i>t</i></sub><sub>+1</sub>上的像素坐标。</p>
                </div>
                <div class="p1">
                    <p id="75">由于图像中的大部分深度值都具有一定的连续性,为了进一步提高预测结果的准确性和改善模型在低纹理区域的表现,参考文献<citation id="225" type="reference">[<a class="sup">4</a>,<a class="sup">6</a>]</citation>,引入了边缘敏感的平滑约束。平滑约束可以在一定程度上抑制深度图中深度值的突变,保持深度值的连续性,并增强深度图在低纹理区域的平滑程度。平滑约束的损失<citation id="224" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>定义为:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi mathvariant="bold-italic">p</mi></munder><mrow><mrow><mo>|</mo><mrow><mo>∇</mo><mi>D</mi><msub><mrow></mrow><mi>t</mi></msub><mrow><mo>(</mo><mi mathvariant="bold-italic">p</mi><mo>)</mo></mrow><mo>⋅</mo><mrow><mo>(</mo><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">|</mo><mo>∇</mo><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">p</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo></mrow></msup></mrow><mo>)</mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>3.5 非相邻图像之间的损失计算</b></h4>
                <div class="p1">
                    <p id="78">如3.3节所述,位姿预测网络会预测出每对相邻图像之间的相机位姿变换,通过这些连续图像之间的相机位姿变换可以求出非相邻图像之间的相机位姿变换:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>→</mo><mi>t</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>-</mo><mo>&gt;</mo><mi>t</mi><mo>+</mo><mi>n</mi></mrow></msub><mo>⋯</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn><mo>→</mo><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>→</mo><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">求出图像到图像之间的相机位姿变换,按照3.3节和3.4节中的方法就可以分别求得非相邻图像之间的匹配点对约束损失和极线约束损失:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>p</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>2</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mi>t</mi></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>n</mi></mrow><mi>i</mi></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>e</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>2</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mi>t</mi></mrow></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi mathvariant="bold-italic">p</mi></munder><mi mathvariant="bold-italic">p</mi></mstyle></mrow></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>t</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">Κ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">E</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>→</mo><mi>t</mi><mo>+</mo><mi>n</mi></mrow></msub><mi mathvariant="bold-italic">Κ</mi><mover accent="true"><mi mathvariant="bold-italic">p</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>n</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中,<i>n</i>代表非相邻图像之间的间隔。若要非相邻图像之间的约束损失变小,网络预测出来的深度图和相机位姿变换必须更加准确,这样一来本文就引入了非相邻图像之间的约束。再加上文献<citation id="226" type="reference">[<a class="sup">4</a>]</citation>原本的约束,损失函数最终定义为:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>v</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>v</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>s</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>p</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>p</mtext><mtext>s</mtext></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>s</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>s</mtext></mrow></msub><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>e</mtext><mtext>s</mtext></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mi>L</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>s</mtext></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中,<i>λ</i><sub>ps</sub>、<i>λ</i><sub>es</sub>、<i>λ</i><sub>ds</sub>分别代表匹配点对位置约束损失的权重、极线约束损失的权重和平滑约束损失的权重。</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag"><b>4 实验结果</b></h3>
                <h4 class="anchor-tag" id="86" name="86"><b>4.1 训练细节</b></h4>
                <div class="p1">
                    <p id="87">本文实验在Tensorflow<citation id="227" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>框架下进行,采用KITTI数据集作为训练数据集,图像的分辨率为416×128。训练前需要对图像进行简单的预处理,本文采用文献<citation id="228" type="reference">[<a class="sup">4</a>]</citation>类似的方法,先对输入图像随机进行1～1.15倍的放大,然后再对放大过的图像进行随机裁剪,重新裁剪成分辨率为416×128的图像作为网络输入。训练的批大小设置为4,图像序列的长度为3。实验使用Adam优化器<citation id="229" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>来优化网络,其中<i>β</i><sub>1</sub>=0.9,<i>β</i><sub>2</sub>=0.999。我们将学习率设置为0.000 2,并在每一个卷积层后都加入了批量归一化处理<citation id="230" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和RELU激活函数。最终损失函数式(11)中的权重分别设置为<i>λ</i><sub>ps</sub>=1,<i>λ</i><sub>es</sub>=0.5,<i>λ</i><sub>ds</sub>=0.5。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>4.2 实验评估</b></h4>
                <div class="p1">
                    <p id="89">本文实验使用文献<citation id="231" type="reference">[<a class="sup">15</a>]</citation>提出的评估方法对模型进行评估,文献<citation id="232" type="reference">[<a class="sup">15</a>]</citation>从KITTI数据集中选择了697幅图像作为测试集。将本文的框架与之前的一些单目深度估计框架进行了对比,对比结果如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="90">表1中各个误差测量方法如下所示:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>A</mi><mi>b</mi><mi>s</mi><mspace width="0.25em" /><mi>R</mi><mi>e</mi><mi>l</mi><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mo stretchy="false">|</mo></mstyle><mi>D</mi><mo>-</mo><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup><mo stretchy="false">|</mo><mo>/</mo><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup></mtd></mtr><mtr><mtd><mi>S</mi><mi>q</mi><mspace width="0.25em" /><mi>R</mi><mi>e</mi><mi>l</mi><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>D</mi><mo>-</mo><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表1 深度评估结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Results of depth estimation</b></p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td>框架来源</td><td><i>Abs Rel</i></td><td><i>Sq Rel</i></td><td><i>RMSE</i></td><td><i>RMSE log</i></td><td><i>δ</i>&lt;1.25</td><td><i>δ</i>&lt;1.25<sup>2</sup></td><td><i>δ</i>&lt;1.25<sup>3</sup></td></tr><tr><td>文献[15]</td><td>0.203</td><td>1.548</td><td>6.307</td><td>0.282</td><td>0.702</td><td>0.890</td><td>0.958</td></tr><tr><td><br />文献[16]</td><td>0.202</td><td>1.614</td><td>6.523</td><td>0.275</td><td>0.678</td><td>0.895</td><td>0.965</td></tr><tr><td><br />文献[4]</td><td>0.183</td><td>1.595</td><td>6.709</td><td>0.270</td><td>0.734</td><td>0.902</td><td>0.959</td></tr><tr><td><br />本文</td><td><b>0.166</b></td><td><b>1.221</b></td><td><b>6.233</b></td><td><b>0.250</b></td><td><b>0.751</b></td><td><b>0.915</b></td><td><b>0.967</b></td></tr><tr><td><br />文献[4]50 m</td><td>0.201</td><td>1.391</td><td>5.181</td><td>0.264</td><td>0.696</td><td>0.900</td><td>0.966</td></tr><tr><td><br />本文50 m</td><td><b>0.157</b></td><td><b>0.908</b></td><td><b>4.559</b></td><td><b>0.231</b></td><td><b>0.770</b></td><td><b>0.929</b></td><td><b>0.975</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>D</mi><mo>-</mo><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mtd></mtr><mtr><mtd><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mspace width="0.25em" /><mi>L</mi><mi>o</mi><mi>g</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>log</mi><mspace width="0.25em" /><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>-</mo><mi>log</mi><mspace width="0.25em" /><mo stretchy="false">(</mo><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mtd></mtr><mtr><mtd><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mi>%</mi><mi>D</mi><mo>:</mo><mi>max</mi><mrow><mo>(</mo><mrow><mfrac><mi>D</mi><mrow><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup></mrow></mfrac><mo>,</mo><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msup></mrow><mi>D</mi></mfrac></mrow><mo>)</mo></mrow><mo>=</mo><mi>δ</mi><mo>&lt;</mo><mi>t</mi><mi>h</mi><mi>r</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">其中,<i>D</i>代表预测值;<i>D</i><sup>gt</sup>代表真实值;<i>S</i>为图像中有效深度真实值的数量;<i>thr</i>代表一个阈值,描述了对预测值和真实值之间误差的容忍度。</p>
                </div>
                <div class="p1">
                    <p id="95">从表1中可以看到,多约束模型的表现比文献<citation id="237" type="reference">[<a class="sup">4</a>,<a class="sup">15</a>,<a class="sup">16</a>]</citation>的模型都要好。其中文献<citation id="238" type="reference">[<a class="sup">13</a>,<a class="sup">14</a>]</citation>的模型是监督学习方法,文献<citation id="233" type="reference">[<a class="sup">4</a>]</citation>的模型为无监督方法。本文框架是在文献<citation id="234" type="reference">[<a class="sup">4</a>]</citation>的基础上改进而来的,文献<citation id="235" type="reference">[<a class="sup">4</a>]</citation>只采用了光度误差作为约束。通过评估结果可以看出,在进一步引入了匹配点约束、极线约束和非连续图像之间的约束后,本文框架预测出的深度值要比文献<citation id="236" type="reference">[<a class="sup">4</a>]</citation>中预测出的深度值更加准确。表1最后2行是将有效深度值设置在50 m内的评估结果。</p>
                </div>
                <div class="p1">
                    <p id="96">由于在原有基础上加入了更多约束,本文框架训练所需的时间比文献<citation id="239" type="reference">[<a class="sup">4</a>]</citation>的框架更多,但在测试时,本文框架的时间复杂度与文献<citation id="240" type="reference">[<a class="sup">4</a>]</citation>的一致,并没有引入额外计算。本文对比实验的设备配置为:GPU为NVIDIA厂商的GeForce GTX 1080Ti,显存容量为1 GB,主频为1 582 MHz;CPU为Intel i7 7700k,主频为4.2 GHz;内存为24 GB。表2给出了训练每次迭代的时间开销和测试的时间开销。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表2 时间分析结果</b> s <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Results of time evaluation</b></p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td><br />模型来源</td><td>训练</td><td>测试</td></tr><tr><td><br />文献[4]</td><td>0.090～0.130</td><td>0.012～0.013</td></tr><tr><td><br />本文</td><td>0.240～0.260</td><td>0.012～0.013</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="98" name="98"><b>4.3 实验对比</b></h4>
                <div class="p1">
                    <p id="99">这一节,用本文模型预测出的深度图与其他方法预测出的深度图进行对比。与文献<citation id="241" type="reference">[<a class="sup">15</a>]</citation>的对比结果如图2所示,第1列的原始图像都是KITTI数据集中的图像。预测得到的深度图中深度值越大的像素灰度值越高。从图2可以看出,无论是前后景的分离,还是对较小物体的检测和深度估计,本文模型的效果都要更好。在第1行,文献<citation id="242" type="reference">[<a class="sup">15</a>]</citation>的方法没有为图像右侧的路灯杆单独分配深度值,而是将其与后面的房屋视为同一场景。第2行图像右下角的摩托车也存在类似的问题。本文模型则为路灯杆与摩托车分配了不同的深度值。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909018_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 深度预测对比" src="Detail/GetImg?filename=images/JSJK201909018_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 深度预测对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909018_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Comparison of depth prediction</p>

                </div>
                <div class="p1">
                    <p id="101">与文献<citation id="243" type="reference">[<a class="sup">4</a>]</citation>模型的对比结果如图3所示。从图3中可以看出,本文模型预测出的深度图比文献<citation id="244" type="reference">[<a class="sup">4</a>]</citation>模型预测出的深度图细节更加完善,特别是在颜色高度相似的区域。在第1行对比中,由于受到阴影的影响,文献<citation id="245" type="reference">[<a class="sup">4</a>]</citation>的方法在预测图像左上角房屋的深度值时,错误地将房屋未被阴影遮挡的部分预测为远景,而本文的模型则没有受到阴影的影响,并且对较远的树木与车辆也进行了较好的预测。在第2行～第4行的对比中,预测出的深度图不仅很好地将前景的树木、路灯和绿化带与背景分离出来,而且完整地反映了道路由近至远的深度变化。而文献<citation id="246" type="reference">[<a class="sup">4</a>]</citation>模型预测出来的深度图对前后景的分离效果则要模糊许多。图3中的最后1行展示了在开阔地区,在文献<citation id="247" type="reference">[<a class="sup">4</a>]</citation>模型预测失败的情况下,本文模型仍然可以稳定地预测出深度图。这些对比表明本文模型具有更强的稳定性,并且能够预测出更加准确的深度图。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909018_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度预测对比" src="Detail/GetImg?filename=images/JSJK201909018_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 深度预测对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909018_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Comparison of depth predictions</p>

                </div>
                <div class="p1">
                    <p id="103">最后,图4给出了本文模型在非KITTI数据集图像上的表现,这些图像均是使用手机在校园内拍摄所得的。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909018_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 模型在KITTI数据集上的表现" src="Detail/GetImg?filename=images/JSJK201909018_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 模型在KITTI数据集上的表现  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909018_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Performance of our model 
 on the images of KITTI dataset</p>

                </div>
                <h3 id="105" name="105" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="106">本文提出了一种基于深度学习的多约束无监督框架来进行深度估计。通过引入匹配点对之间的约束和极线约束来增强模型在无纹理和光照变化情况下预测深度的稳定性;同时,通过添加非连续图像之间的约束来进一步提高模型预测的准确率。但是,本文的框架还没有解决图像中存在动态物体和遮挡物的问题。下一步,我们将继续改善模型预测的准确度并尝试解决图像中存在动态物体和遮挡物的问题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="172">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 Geiger A,Lenz P,Urtasun R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2012:3354-3361.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised CNN for Single View Depth Estimation:Geometry to the Rescue">

                                <b>[2]</b> Garg R,Bg V K,Cameiro G,et al.Unsupervised CNN for single view depth estimation:Geometry to the rescue[C]//Proc of European Conference on Computer Vision,2016:740-756.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised monocular depth estimation with leftright consistency">

                                <b>[3]</b> Godard C,Aodha O M,Brostow G J.Unsupervised monocular depth estimation with left-right consistency[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017:6602-6611.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Learning of Depth and Ego-Motion from Video">

                                <b>[4]</b> Zhou T,Brown M,Snavely N,et al.Unsupervised learning of depth and ego-motion from video[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2017:1-10.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sfm-net:Learning of structure and motion from video">

                                <b>[5]</b> Vijayanarasimhan S,Ricco S,Schmid C,et al.Sfm-net:Learning of structure and motion from video[J].arXiv:1704.07804,2017.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised learning of monocular depth estimation with bundle adjustment super-resolution and clip loss">

                                <b>[6]</b> Zhou L,Ye J,Abello M,et al.Unsupervised learning of monocular depth estimation with bundle adjustment,super-resolution and clip loss[J].arXiv:1812.03368,2018.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD-SLAM:Large-Scale Direct Monocular SLAM">

                                <b>[7]</b> Engel J,Schöps T,Cremers D.LSD-SLAM:Large-scale direct monocular SLAM[C]//Proc of European Conference on Computer Vision,2014:834-849.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB-SLAM: A Versatile and Accurate Monocular SLAM System">

                                <b>[8]</b> Mur-Artal R,Montiel J M M,Tardos J D.ORB-SLAM:A versatile and accurate monocular SLAM system[J].IEEE Transactions on Robotics,2015,31(5):1147-1163.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial transformer networks">

                                <b>[9]</b> Jaderberg M,Karen S,Andrew Z.Spatial transformer networks[J].arXiv:1506.02025v2,2015.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SRUF:Speeded up robust features">

                                <b>[10]</b> Bay H,Ess A,Tuytelaars T,et al.SRUF:Speeded up robust features[C]//Proc of European Conference on Computer Vision,2006:1-14.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of eight-point algorithm">

                                <b>[11]</b> Hartley R I.In defense of the eight-point algorithm[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,19(6):580-593.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:A System for Large-Scale Machine Learning">

                                <b>[12]</b> Abadi M,Barham P,Chen J M,et al.Tensorflow:A system for large-scale machine learning[C]//Proc of the 12th {USENIX} Symposium on Operating Systems Design and Implementation,2016:265-283.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization">

                                <b>[13]</b> Kingma D P,Ba J.Adam:A method for stochastic optimization[J].arXiv:1412.6980,2014.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">

                                <b>[14]</b> Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of International Conference on International Conference on Machine Learning,2015:448-456.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depth map prediction from a single image using a multi-scale deep network">

                                <b>[15]</b> Eigen D,Puhrsch C,Fergus R.Depth map prediction from a single image using a multi-scale deep network [C]//Proc of Advances in Neural Information Processing Systems,2014:2366-2374.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning depth from single monocular images using deep convolutional neural fields">

                                <b>[16]</b> Liu F,Shen C,Lin G,et al.Learning depth from single monocular images using deep convolutional neural fields[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,30(10):2024-2039.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909018&amp;v=MTg3NTh0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnpCTHo3QlpiRzRIOWpNcG85RWJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
