<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132344462217500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJK201911024%26RESULT%3d1%26SIGN%3d1NJhXByKdDMYiT556yOg2zx1b8Y%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911024&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911024&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911024&amp;v=MzIwMTE0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVXJyUEx6N0JaYkc0SDlqTnJvOUhZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;2 TNG模型&lt;/b&gt; "><b>2 TNG模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;第1阶段&lt;/b&gt;:构建特征扩展库。"><b>第1阶段</b>:构建特征扩展库。</a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;第2阶段&lt;/b&gt;:原始短文本做特征扩展。"><b>第2阶段</b>:原始短文本做特征扩展。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="&lt;b&gt;3 模糊最小-最大神经网络&lt;/b&gt; "><b>3 模糊最小-最大神经网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#186" data-title="(1)扩展:">(1)扩展:</a></li>
                                                <li><a href="#187" data-title="(2)重叠测试:">(2)重叠测试:</a></li>
                                                <li><a href="#188" data-title="(3)收缩:">(3)收缩:</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="&lt;b&gt;4 MLFM-MN短文本分类算法&lt;/b&gt; "><b>4 MLFM-MN短文本分类算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#120" data-title="&lt;b&gt;4.1 模型表示-向量空间模型&lt;/b&gt;"><b>4.1 模型表示-向量空间模型</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;4.2 MLFM-MN短文本分类算法&lt;/b&gt;"><b>4.2 MLFM-MN短文本分类算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="&lt;b&gt;5 实验结果分析&lt;/b&gt; "><b>5 实验结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#138" data-title="&lt;b&gt;5.1 实验环境与评估指标&lt;/b&gt;"><b>5.1 实验环境与评估指标</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;5.2 实验结果分析&lt;/b&gt;"><b>5.2 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#152" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="图1 TNG模型">图1 TNG模型</a></li>
                                                <li><a href="#102" data-title="图2 经典的FM-MN结构">图2 经典的FM-MN结构</a></li>
                                                <li><a href="#115" data-title="图3 MLFM-MN不同级别的分类">图3 MLFM-MN不同级别的分类</a></li>
                                                <li><a href="#127" data-title="图4 MLFM-MN文本分类流程">图4 MLFM-MN文本分类流程</a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表1 3种算法的消耗时间对比&lt;/b&gt;"><b>表1 3种算法的消耗时间对比</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表2 实验结果对比&lt;/b&gt;"><b>表2 实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="4">


                                    <a id="bibliography_1" title=" Wang B K,Huang Y F,Yang W X,et al.Short text classification based on strong feature thesaurus[J].Journal of Zhejiang University Science(Computer &amp;amp; Electronics),2012,13(9):649-659." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD120911003542&amp;v=Mjc2MjhLcmlmWnU5dUZDdnRVN2pOSVZzVE5qN0Jhcks2SHRqTnJvOUZaKzRMRGhNOHp4VVNtRGQ5U0g3bjN4RTlmYnZu&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Wang B K,Huang Y F,Yang W X,et al.Short text classification based on strong feature thesaurus[J].Journal of Zhejiang University Science(Computer &amp;amp; Electronics),2012,13(9):649-659.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_2" title=" Wang M,Lin L F,Wang J,et al.Improving short text classification using public search engines[C]//Proc of the 2013 International Conference on Integrated Uncertainty in Knowledge Modelling and Decision Making,2013:157-166." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving short text classification using public search engines">
                                        <b>[2]</b>
                                         Wang M,Lin L F,Wang J,et al.Improving short text classification using public search engines[C]//Proc of the 2013 International Conference on Integrated Uncertainty in Knowledge Modelling and Decision Making,2013:157-166.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_3" title=" Sun A.Short text classification using very few words[C]//Proc of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval,2012:1145-1146." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Short text classification using very few words">
                                        <b>[3]</b>
                                         Sun A.Short text classification using very few words[C]//Proc of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval,2012:1145-1146.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_4" title=" Zhang X,Wu B.Short text classification based on feature extension using the n-gram model[C]//Proc of International Conference on Fuzzy Systems and Knowledge Discovery,2015:15-17." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Short Text Classification based on feature extension using The N-Gram model">
                                        <b>[4]</b>
                                         Zhang X,Wu B.Short text classification based on feature extension using the n-gram model[C]//Proc of International Conference on Fuzzy Systems and Knowledge Discovery,2015:15-17.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_5" title=" Zhang Y,Jin R,Zhou Z H.Understanding bag-of-words model:A statistical framework[J].International Journal of Machine Learning and Cybernetics,2010,1(1-4):43-52." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding bag-of-words model: A statistical framework">
                                        <b>[5]</b>
                                         Zhang Y,Jin R,Zhou Z H.Understanding bag-of-words model:A statistical framework[J].International Journal of Machine Learning and Cybernetics,2010,1(1-4):43-52.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     Zhang Lei.The research summary of text categorization and classification algorithms[J].Computer Knowledge and Technology,2016,12(34):225-226.(in Chinese)</a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_7" title=" Liu Jing,Jiang Wen-bo,Shao Ye.Research progress of text classification technology based on machine learning[J].Computer Fan,2018(6):26.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research progress of text classification technology based on machine learning">
                                        <b>[7]</b>
                                         Liu Jing,Jiang Wen-bo,Shao Ye.Research progress of text classification technology based on machine learning[J].Computer Fan,2018(6):26.(in Chinese)
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_8" title=" Miao Y Q,Kamel M.Pairwise optimized rocchio algorithm for text categorization[J].Pattern Recognition Letters,2011,32(2):375-382." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300413654&amp;v=MTEwMDJqbVVMZklKMXNWYnhVPU5pZk9mYks3SHRET3JJOUZZT29NQ25rOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Miao Y Q,Kamel M.Pairwise optimized rocchio algorithm for text categorization[J].Pattern Recognition Letters,2011,32(2):375-382.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_9" title=" Patil T R,Sherekar S S.Performance analysis of naive bayes and J48 classification algorithm for data classification[J].International Journal of Computer Science and Applications,2013,6(2):256-261." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance analysis of Naive Bayes and J48 classification algorithm for data classification">
                                        <b>[9]</b>
                                         Patil T R,Sherekar S S.Performance analysis of naive bayes and J48 classification algorithm for data classification[J].International Journal of Computer Science and Applications,2013,6(2):256-261.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_10" title=" Tong S,Koller D.Support vector machine active learning with applications to text classification[J].Journal of Machine Learning Research,2002,2:45-66." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector machine active learning with applications to text classification">
                                        <b>[10]</b>
                                         Tong S,Koller D.Support vector machine active learning with applications to text classification[J].Journal of Machine Learning Research,2002,2:45-66.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_11" title=" Deng Z,Zhu X,Cheng D,et al.Efficient knn classification algorithm for big data[J].Neurocomputing,2016,195:143-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES285124BCF33E7EA6AA78894DA5F5C4DB&amp;v=MjA4NDRGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaHc3Mit4YTg9TmlmT2ZiR3dHOURPcS8wMkV1Z01lWHRNdmhCaW16aDFRSGJtMkdNd0Q3Zm5RYzd0Q09Odg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Deng Z,Zhu X,Cheng D,et al.Efficient knn classification algorithm for big data[J].Neurocomputing,2016,195:143-148.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_12" title=" Taravat A,Frate F D,Cornaro C,et al.Neural networks and support vector machine algorithms for automatic cloud classification of whole-sky ground-based images[J].IEEE Geoscience and Remote Sensing Letters,2015,12(3):666-670." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural networks and support vector machine algorithms for automatic cloud classification of whole-sky ground-based images">
                                        <b>[12]</b>
                                         Taravat A,Frate F D,Cornaro C,et al.Neural networks and support vector machine algorithms for automatic cloud classification of whole-sky ground-based images[J].IEEE Geoscience and Remote Sensing Letters,2015,12(3):666-670.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_13" title=" Ghareb A S,Bakar A A,Hamdan A R.Hybrid feature selection based on enhanced genetic algorithm for text categorization[J].Expert Systems with Applications,2016,49:31-47." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3D70D76EE98E8BA4046AA99FEA7B4C38&amp;v=MjQzNzQyK3hhOD1OaWZPZmJETUdkRzRxSWt3RWVJSGVYUkx2aElUN2prTU9YYnIybWRFZnNDUU5ybVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOQmh3Nw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Ghareb A S,Bakar A A,Hamdan A R.Hybrid feature selection based on enhanced genetic algorithm for text categorization[J].Expert Systems with Applications,2016,49:31-47.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_14" title=" Sun B,Zhao P.Feature extension for Chinese short text classification based on topical N-Grams[C]//Proc of 2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),2017:477-482." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature extension for Chinese short text classification based on topical N-Grams">
                                        <b>[14]</b>
                                         Sun B,Zhao P.Feature extension for Chinese short text classification based on topical N-Grams[C]//Proc of 2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),2017:477-482.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_15" title=" Wang X R,Mccallum A,Wei X.Topical n-grams:Phrase and topic discovery,with an application to information retrieval[C]//Proc of the 2007 7th IEEE International Conference on Data Mining,2007:697-702." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topical n-grams:phrase and topic discovery,with and application to information retrieval">
                                        <b>[15]</b>
                                         Wang X R,Mccallum A,Wei X.Topical n-grams:Phrase and topic discovery,with an application to information retrieval[C]//Proc of the 2007 7th IEEE International Conference on Data Mining,2007:697-702.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_16" title=" Fan Yun-jie,Liu Huai-liang.Research on Chinese short text classification based on wikipedia[J].Data Analysis and Knowledge Discovery,2012,28(3):47-52.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on Chinese short text classification based on wikipedia">
                                        <b>[16]</b>
                                         Fan Yun-jie,Liu Huai-liang.Research on Chinese short text classification based on wikipedia[J].Data Analysis and Knowledge Discovery,2012,28(3):47-52.(in Chinese)
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_17" title=" Shirakawa M,Nakayama K,Hara T,et al.Wikipedia-based semantic similarity measurements for noisy short texts using extended naive bayes[J].IEEE Transactions on Emerging Topics in Computing,2015,3(2):205-219." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wikipedia-based semantic similarity measurements for noisy short texts using extended naive Bayes">
                                        <b>[17]</b>
                                         Shirakawa M,Nakayama K,Hara T,et al.Wikipedia-based semantic similarity measurements for noisy short texts using extended naive bayes[J].IEEE Transactions on Emerging Topics in Computing,2015,3(2):205-219.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_18" title=" Simpson P K.Fuzzy min-max neural networks.I.Classification[J].IEEE Transactions on Neural Networks,1992,3(5):776-786." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy min-max neural networks. I. Classification">
                                        <b>[18]</b>
                                         Simpson P K.Fuzzy min-max neural networks.I.Classification[J].IEEE Transactions on Neural Networks,1992,3(5):776-786.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_19" title=" Davtalab R,Dezfoulian M H,Mansoorizadeh M.Multi-level fuzzy min-max neural network classifier[J].IEEE Transactions on Neural Networks and Learning Systems,2014,25(3):470-482." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-level fuzzy min-max neural network classifier">
                                        <b>[19]</b>
                                         Davtalab R,Dezfoulian M H,Mansoorizadeh M.Multi-level fuzzy min-max neural network classifier[J].IEEE Transactions on Neural Networks and Learning Systems,2014,25(3):470-482.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                     Xue Tao,Wang Ya-ling,Mu Nan.Convolutional neural network based on word sense disambiguation for text classification[J].Journal of Computer Applications,2018,35(10):2898-2903.(in Chinese)</a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_21" title=" Lilleberg J,Zhu Yun,Zhang Yan-qing.Support vector machines and word2vec for text classification with semantic features[C]//Proc of 2015 IEEE 14th International Conference on Cognitive Informatics &amp;amp; Cognitive Computing (ICCI* CC),2015:136-140." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support Vector Machines and Word2vec for Text Classification with Semantic Features">
                                        <b>[21]</b>
                                         Lilleberg J,Zhu Yun,Zhang Yan-qing.Support vector machines and word2vec for text classification with semantic features[C]//Proc of 2015 IEEE 14th International Conference on Cognitive Informatics &amp;amp; Cognitive Computing (ICCI* CC),2015:136-140.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     Chen K,Zhang Z,Long J,et al.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Applications,2016,66:245-260.附中文参考文献:</a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_6" title=" 张磊.文本分类及分类算法研究综述[J].电脑知识与技术,2016,12(34):225-226." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DNZS201634094&amp;v=MjgzMjRmYkc0SDlmUHE0OU1ZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVXJyUElTUFI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         张磊.文本分类及分类算法研究综述[J].电脑知识与技术,2016,12(34):225-226.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_7" title=" 刘婧,姜文波,邵野.基于机器学习的文本分类技术研究进展[J].电脑迷,2018(6):26." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DNMI201806026&amp;v=MDg1NTE0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVXJyUElTUEdaN0c0SDluTXFZOUhZb1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         刘婧,姜文波,邵野.基于机器学习的文本分类技术研究进展[J].电脑迷,2018(6):26.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_16" title=" 范云杰,刘怀亮.基于维基百科的中文短文本分类研究[J].数据分析与知识发现,2012,28(3):47-52." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201203009&amp;v=MDA5MzlHRnJDVVJMT2VaZVJtRnkvZ1VyclBQU25mZjdHNEg5UE1ySTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         范云杰,刘怀亮.基于维基百科的中文短文本分类研究[J].数据分析与知识发现,2012,28(3):47-52.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_20" title=" 薛涛,王雅玲,穆楠.基于词义消歧的卷积神经网络文本分类模型[J].计算机应用研究,2018,35(10):2898-2903." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201810005&amp;v=MjE1NDNTWkxHNEg5bk5yNDlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1VyclBMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         薛涛,王雅玲,穆楠.基于词义消歧的卷积神经网络文本分类模型[J].计算机应用研究,2018,35(10):2898-2903.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(11),2071-2078 DOI:10.3969/j.issn.1007-130X.2019.11.023            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于TNG特征扩展的MLFM-MN短文本分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%96%87%E6%AD%A6&amp;code=10681662&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文武</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%9F%B9%E5%BC%BA&amp;code=40293741&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李培强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E6%9C%89%E5%BA%86&amp;code=42156382&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭有庆</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E6%96%B0%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0415387&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信新技术应用研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E4%BF%A1%E7%A7%91%E8%AE%BE%E8%AE%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆信科设计有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在海量短文本中由于特征稀疏、数据维度高这一问题,传统的文本分类方法在分类速度和准确率上达不到理想的效果。针对这一问题提出了一种基于Topic N-Gram(TNG)特征扩展的多级模糊最小-最大神经网络(MLFM-MN)短文本分类算法。首先通过使用改进的TNG模型构建一个特征扩展库并对特征进行扩展,该扩展库不仅可以推断单词分布,还可以推断每个主题文本的短语分布;然后根据短文本中的原始特征,计算这些文本的主题倾向,根据主题倾向,从特征扩展库中选择适当的候选词和短语,并将这些候选词和短语放入原始文本中;最后运用MLFM-MN算法对这些扩展的原始文本对象进行分类,并使用精确率、召回率和<i>F</i>1分数来评估分类效果。实验结果表明,本文提出的新型分类算法能够显著提高文本的分类性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%A8%80%E7%96%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征稀疏;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=TNG%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">TNG模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E7%B3%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模糊神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%89%A9%E5%B1%95%E5%BA%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">扩展库;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E9%A2%98%E5%80%BE%E5%90%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主题倾向;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    文武(1970-),男,重庆人,博士,高级工程师,研究方向为大数据和数据分析。E-mail:2307239396@qq.com,通信地址:400065重庆市崇文路2号重庆邮电大学通信与信息工程学院;
                                </span>
                                <span>
                                    李培强(1991-),男,河南信阳人,硕士生,研究方向为自然语言处理。E-mail:2531646835@qq.com,通信地址:400065重庆市崇文路2号重庆邮电大学通信与信息工程学院;
                                </span>
                                <span>
                                    郭有庆(1992-),男,江西吉安人,硕士生,研究方向为大数据和数据分析。E-mail:1073079162@qq.com,通信地址:400065重庆市崇文路2号重庆邮电大学通信与信息工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-11</p>

            </div>
                    <h1><b>An MLFM-MN short text classification algorithm based on TNG feature extension</b></h1>
                    <h2>
                    <span>WEN Wu</span>
                    <span>LI Pei-qiang</span>
                    <span>GUO You-qing</span>
            </h2>
                    <h2>
                    <span>School of Communication and Information Engineering,Chongqing University of Posts and Telecommunications</span>
                    <span>Research Center of New Communication Technology Applications,Chongqing University of Posts and Telecommunications</span>
                    <span>Chongqing Xinke Design Co.Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to the problems of sparse features and high data dimension in short text, traditional text classification methods cannot achieve the desired classification rate and accuracy. Aiming at this problem, we propose a multi-level fuzzy minimum and maximum neural network(MLFM-MN) short text classification algorithm based on topic N-Gram(TNG) feature extension. The algorithm first constructs a feature extension library and extends the features by using the improved TNG model. The extension library can not only infer the word distribution, but also infer the phrase distribution of each topic text, and then calculate these based on the original features in the short text. Appropriate candidate words and phrases are selected from the feature extension library according to topic tendencies, and put into the original text. Finally, the extended text objects are classified by the MLFM-MN algorithm. We use accuracy rate, recall rate and F1 score to evaluate the classification effect. The results show that the proposed algorithm can significantly improve text classification performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparse%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparse feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=TNG%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">TNG model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fuzzy%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fuzzy neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=extension%20library&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">extension library;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=topic%20tendency&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">topic tendency;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WEN Wu,born in 1970,PhD,senior engineer,his research interests include big data,and data analysis.Address:School of Communication and Information Engineering,Chongqing University of Posts and Telecommunications,2 Chongwen Road,Chongqing 400065,P.R.China;
                                </span>
                                <span>
                                    LI Pei-qiang,born in 1991,MS candidate,his research interest includes natural language processing.Address:School of Communication and Information Engineering,Chongqing University of Posts and Telecommunications,2 Chongwen Road,Chongqing 400065,P.R.China;
                                </span>
                                <span>
                                    GUO You-qing,born in 1992,MS candidate,his research interests include big data,and data analysis.Address:School of Communication and Information Engineering,Chongqing University of Posts and Telecommunications,2 Chongwen Road,Chongqing 400065,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-11</p>
                            </div>


        <!--brief start-->
                        <h3 id="56" name="56" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="57">近年来随着社会媒体的快速发展,出现了大量短文,如微信、微博、QQ、邮件、短信等短文信息。短文长度都非常短,大多不超过100个字,导致特征稀疏,几乎不能获得分类的关键特征。同时,文本数据是一种非结构化数据,具有模糊性和高维度等特征,这使得经典算法很难达到高精度和高速度的分类效果<citation id="158" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在过去几年中,研究人员提出了多种方法来提高使用特征扩展的短文本分类的性能。这些方法通常基于外部知识库或同时发生的词典,这些词典有效提升了短文本分类的效果。然而,外部知识库并不总是可用,同步词典可能会将噪音带入原始的短文本。Wang等<citation id="159" type="reference"><link href="6" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation> 通过从搜索引擎中提取查询关键字,从短文本中提取关键特征,获得相关材料来扩展短文本。Sun<citation id="160" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation> 从简短的文本中选出最具代表性的单词作为查询词,然后使用这些单词来搜索一组标签文本,进而区分出短文本的类别。Zhang等<citation id="161" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提取的N-Gram在训练集中以更高的频率出现,作为其特征扩展模式库来扩展原始短文本。通常一些已知的具有强类型表示的短语总是由一些子词组成,但是传统的词袋子模型BOW(Bag Of Word)<citation id="162" type="reference"><link href="12" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>已经独立处理了每一个单词,这意味着BOW模型忽略了文本中的短语。考虑到这个问题,本文提出了修复特性的思路,使用大型文本集合作为通用数据来训练局部N-Gram模型,并构建包含单字单词的功能扩展库。使用这个库,我们可以将属于某个主题的单词和短语添加到原始短文本中。通过这种方法,我们可以尽可能地修复一些破坏的特性。</p>
                </div>
                <div class="p1">
                    <p id="58">文本分类是指在给定的算法模型下,让其根据文本自己的内容信息,自动确定文本所属类别<citation id="163" type="reference"><link href="14" rel="bibliography" /><link href="48" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">6</a>]</sup></citation>。目前文本分类的方法大多是基于统计学习的,运用机器学习领域的分类模型进行自动分类<citation id="164" type="reference"><link href="16" rel="bibliography" /><link href="50" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">7</a>]</sup></citation>,常用分类模型有:Rocchio 算法<citation id="165" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、朴素贝叶斯<citation id="166" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、支持向量机SVM(Support Vector Machines)<citation id="167" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、K近邻KNN(K-Nearest Neighbor)<citation id="168" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等。这些分类器在商业智能领域很有用。支持向量机是基于将数据空间分离为最优边界的概念,用于区分2个类的典型分类器,它可以与多类分类的二元决策树组合。朴素贝叶斯分类器使用联合概率计算每个样本的隶属度与预定义类别,每个样本被分配到具有最高后验概率的特定类别,在这种方法中,属性独立性可能会影响分类精度。文献<citation id="169" type="reference">[<a class="sup">12</a>,<a class="sup">13</a>]</citation>也使用了其他分类器,如神经网络和基于遗传算法的分类器。在本文中,采用新型的具有高性能和高速度的多级模糊最小-最大神经网络分类器MLFM-MN(Multi-Level Fuzzy Minimum-Maximum neural Network classifier)用于文本分类。</p>
                </div>
                <div class="p1">
                    <p id="59">本文的主要贡献如下:</p>
                </div>
                <div class="p1">
                    <p id="60">(1)特征扩展库由TNG(Topic N-Gram)模型构建,可以为每个短文本提供主题特征词和主题短语分发。</p>
                </div>
                <div class="p1">
                    <p id="61">(2)通过构建的特征扩展库可以将单词和短语放在原来的短文本中,改善了特征稀疏性问题,提高了分类的精确率。</p>
                </div>
                <div class="p1">
                    <p id="62">(3)利用多级模糊最小-最大神经网络分类器能够对海量短文本数据进行高速分类,显著提高了文本的分类速度。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag"><b>2 TNG模型</b></h3>
                <div class="p1">
                    <p id="64">TNG模型基于这样一种假设,第<i>N</i>个词的出现只与前面<i>N</i>-1个词相关,而与其它任何词都不相关,整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计<i>N</i>个词同时出现的次数得到。TNG模型不仅关联了短语同时也引入了单词,不同于传统主题模型,TNG模型根据其周围特征数据自动形成一个N-Gram<citation id="170" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。TNG模型如图1所示。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911024_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 TNG模型" src="Detail/GetImg?filename=images/JSJK201911024_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 TNG模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911024_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 TNG model</p>

                </div>
                <div class="p1">
                    <p id="66">由TNG模型可以推断出联合概率分布<i>p</i>(<i><b>w</b></i>,<i><b>z</b></i>,<i><b>x</b></i>|<i>α</i>,<i>β</i>,<i>γ</i>,<i>δ</i>)。其中<i><b>w</b></i>是标记向量,<i><b>z</b></i>表示所有单词标记的主题分配向量,<i><b>x</b></i>表示所有标记的二元状态向量,<i>α</i>,<i>β</i>,<i>γ</i>,<i>δ</i>是文档-主题分布<i>θ</i>的超参数。<i>Φ</i>是主题词分布,<i>Ψ</i>是状态向量<i><b>x</b></i>的伯努利分布,<i>σ</i>是主题短语分布,由文献<citation id="171" type="reference">[<a class="sup">15</a>]</citation>可知,TNG的联合概率分布如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>¯</mo></mover><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>¯</mo></mover><mo stretchy="false">|</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><mrow><mo>∭</mo><mspace width="0.25em" /></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>z</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mspace width="0.25em" /></mstyle><mi>p</mi><mo stretchy="false">(</mo><mi>σ</mi><msub><mrow></mrow><mrow><mi>z</mi><mi>v</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">|</mo><mi>δ</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>Ψ</mi><msub><mrow></mrow><mrow><mi>z</mi><mi>v</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">|</mo><mi>γ</mi><mo stretchy="false">)</mo><mtext>d</mtext><mi>ψ</mi><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>z</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mspace width="0.25em" /></mstyle><mi>p</mi><mo stretchy="false">(</mo><mi>Φ</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">|</mo><mi>β</mi><mo stretchy="false">)</mo><mtext>d</mtext><mi>Φ</mi><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>d</mi><mspace width="0.25em" /></mrow></msub></mrow></munderover><mspace width="0.25em" /></mstyle><mi>p</mi><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mo stretchy="false">|</mo><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mo>,</mo><mi>Φ</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mspace width="0.25em" /></mrow></msub><mo>,</mo><mi>σ</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mspace width="0.25em" /></mrow></msub><mo>,</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mo stretchy="false">|</mo><mi>ψ</mi><msub><mrow></mrow><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mo>,</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mspace width="0.25em" /></mrow></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><mrow><mo>∫</mo><mspace width="0.25em" /></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mspace width="0.25em" /></mstyle><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>d</mi><mspace width="0.25em" /></mrow></msub></mrow></munderover><mspace width="0.25em" /></mstyle><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>d</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mrow><mi>d</mi><mspace width="0.25em" /></mrow></msub><mo stretchy="false">|</mo><mi>α</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext>d</mtext><mi>θ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中,<i>σ</i><sub><i><b>z</b></i></sub>表示所有单词的主题向量所满足的短语分布,<i>ψ</i><sub><i><b>z</b></i></sub>表示所有单词的主题向量所满足的伯努利分布,<i>Φ</i><sub><i><b>z</b></i></sub>表示所有单词的主题向量所满足的主题分布,<i>Φ</i><sub><i>z</i></sub><sub><sup><i>d</i></sup></sub><sub><i>i</i></sub>表示在<i>N</i>个特征词中被标记的<i>d</i>个特征词在满足主题向量<i><b>z</b></i>下的主题分布,<i>σ</i><sub><i>z</i></sub><sub><sup><i>d</i></sup></sub><sub><i>i</i></sub>表示在<i>N</i>个特征词中被标记的<i>d</i>个特征词在满足主题向量<i><b>z</b></i>下的短语分布,<i>ψ</i><sub><i>z</i></sub><sub><sup><i>d</i></sup></sub><sub><i>i</i></sub><sub>-1 ,</sub><sub><i>w</i></sub><sub><sup><i>d</i></sup></sub><sub><i>i</i></sub><sub>-1  </sub>表示在<i>N</i>个特征词中被标记的<i>d</i>个特征词在满足主题向量<i><b>z</b></i>和标记向量<i><b>w</b></i>下的的伯努利分布。<i>T</i>表示主题数量,<i>D</i>表示文本数量,<i>TW</i>表示被标记向量的主题数量,<i>w</i>表示被标记特征数量,<i>N</i><sub><i>d</i></sub>表示文档<i>d</i>中标记的数量,<i>z</i><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup></mrow></math></mathml>表示与标记的文档<i>d</i>相关的主题,<i>w</i><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup></mrow></math></mathml>表示文档<i>d</i>中第<i>i</i>个标记,<i>x</i><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mspace width="0.25em" /></mrow><mi>d</mi></msubsup></mrow></math></mathml>表示在文档<i>d</i>中标记的第<i>i</i>个主题和标记的第(<i>i</i>-1)个主题之前的状态关系。则结合贝叶斯公式和式(1)可得到TNG的条件概率,如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo>,</mo><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo stretchy="false">|</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>¯</mo></mover><mo>,</mo><mover accent="true"><mi>z</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo>,</mo><mover accent="true"><mi>x</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo>,</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>¯</mo></mover><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo>¯</mo></mover><mo>,</mo><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>¯</mo></mover><mo stretchy="false">|</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mover accent="true"><mi mathvariant="bold-italic">w</mi><mo>¯</mo></mover><mo>,</mo><mover accent="true"><mi>z</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo>,</mo><mover accent="true"><mi>x</mi><mo>¯</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>d</mi></msubsup><mo stretchy="false">|</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo>,</mo><mi>γ</mi><mo>,</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">TNG模型的不足之处有:</p>
                </div>
                <div class="p1">
                    <p id="71">(1)参数空间过大,外部资源随时可能无法使用,搜索结果可能会令人困惑,这会给原始文本引入噪声。</p>
                </div>
                <div class="p1">
                    <p id="72">(2)不同类别的词可能有不同的含义,一般来说,这些单词可能与不同类别中的其他单词具有很高的同现频率,不具有泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="73">(3)某些类别的特征对象通常由多个单词构成,但TNG模型忽略了这些特点,数据稀疏问题依然严重。</p>
                </div>
                <div class="p1">
                    <p id="74">短文本长度一般来说不会超过100个字,对短文本扩展时,搜索引擎如百度所接受的最大长度是38个字,提取短文本的关键字是短文本分类的关键步骤之一。一些研究人员试图用Google和维基百科等外部资源来扩展短文,文献<citation id="172" type="reference">[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">16</a>]</citation>在互联网上搜索维基百科或获得Wikipedia的副本,构建索引数据库,提供相关的背景知识,帮助处理短文本,包括短文本分类和语义相似性测量。以上方法是使用扩展资源库的代表,然而这些方法存在不足之处,因为这些知识库依靠的更多是人工参与编辑,对于短文本数据,尤其是即时数据信息相对滞后,扩展特征不够全面,还存在较大的特征稀疏性问题。本文提出的基于特征扩展库的TNG模型能很好地解决这一问题。</p>
                </div>
                <div class="p1">
                    <p id="75">特征扩展过程中的一个重要问题是如何避免引入噪声特征。为了解决这个问题,本文提出了主题权重向量。一个单词的权重向量可以表示每个主题文本的倾向。我们创建的扩展库保留了每个文本上的单词分布,可以帮助我们轻松获得每个主题文本中出现的单词的概率。一个文本中出现的所有单词的概率之和是1,而每个主题生成的单词的概率之和不是1。我们将主题中出现的一个单词的概率作为这个单词在每个主题中的主题权重。主题权重的计算方法如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>w</mtext><mtext>o</mtext><mtext>r</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mn>1</mn><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mn>2</mn><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mn>3</mn><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mi>n</mi><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">预处理的原始短文本,由词组〈<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,<i>w</i><sub>3</sub>,…,<i>w</i><sub><i>n</i></sub>〉组成,然后,我们可以通过式(4)计算短文本的主题权重向量:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>e</mtext><mtext>x</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mn>1</mn><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mn>2</mn><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mn>3</mn><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>p</mi><mo stretchy="false">(</mo><mtext>主</mtext><mtext>题</mtext><mi>n</mi><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable><mo>}</mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">在获得短文本的权重向量之后,选择在该向量中权重值最大的主题特征作为该短文本的候选特征。选择适当的候选主题特征后,就可以开始扩展短文本。该方法由2个阶段组成,扩展特征和重构特征。首先,按照这个主题词在主题词分布中出现概率从大到小的顺序从单词分布中提取前100个单词,然后将这些单词放入原文中。同理,对于简短的文本,在单词扩展之后,从主题短语分布中找到前50个短语,按此主题的概率降序排列。最后,将这些短语放入原始短文本中。</p>
                </div>
                <div class="p1">
                    <p id="80">选择前50个短语是一个经验实践,因为类别具有强大代表性的短语通常在主题-短语分布中具有较高的排名,相反,如果在原文中添加了太多的短语,可能会带来嘈杂的特征文本。MLFM-MN分类算法具体步骤如下所示:</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>第1阶段</b>:构建特征扩展库。</h4>
                <div class="p1">
                    <p id="82"><b>输入</b>:大量的短文本数据集。实验数据来自于搜狗实验室,选取其新闻语料库的标准数据集做为训练数据。</p>
                </div>
                <div class="p1">
                    <p id="83"><b>输出</b>:特征扩展库。</p>
                </div>
                <div class="p1">
                    <p id="84"><b>步骤1</b> 从语料库中提取新闻内容,并写入文本文件。</p>
                </div>
                <div class="p1">
                    <p id="85"><b>步骤2</b> 对训练数据集进行分词、去停用词。</p>
                </div>
                <div class="p1">
                    <p id="86"><b>步骤3</b> 对中文单词做词性标注,去除无用单词。</p>
                </div>
                <div class="p1">
                    <p id="87"><b>步骤4</b> 利用处理过的数据集训练TNG模型,构建特征扩展库。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>第2阶段</b>:原始短文本做特征扩展。</h4>
                <div class="p1">
                    <p id="89"><b>输入</b>:一个简短的文本数据集,我们从搜狗新闻选择了12 000个新闻标题。</p>
                </div>
                <div class="p1">
                    <p id="90"><b>输出</b>:扩展的短文本集。</p>
                </div>
                <div class="p1">
                    <p id="91"><b>步骤1</b> 执行与第1阶段中步骤1～步骤3相同的文本集的预处理操作。</p>
                </div>
                <div class="p1">
                    <p id="92"><b>步骤2</b> 计算每个短文本的主题权重向量并选择候选主题词和候选主题短语。</p>
                </div>
                <div class="p1">
                    <p id="93"><b>步骤3</b> 根据主题词分布,将我们在步骤2中确定的候选主题词中的前100个单词附加到原始短文本中。</p>
                </div>
                <div class="p1">
                    <p id="94"><b>步骤4</b> 根据主题短语的分布,将我们在步骤2中确定的主题中的前50个短语放在原始短文本中。</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag"><b>3 模糊最小-最大神经网络</b></h3>
                <div class="p1">
                    <p id="96"><citation id="174" type="reference"><link href="38" rel="bibliography" /><i>Simpson</i>在1992</citation>年提出了模糊最小-最大神经网络<i>FM</i>-<i>MN</i>(<i>Fuzzy Minimum and Maximum neural Network</i>)<citation id="173" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,它是一个有监督分类,由训练和测试阶段组成,该算法使用大小不同的超集合,分散在模式空间中。每个超集合由该超集合的最小点<i>H</i><sub>min</sub>和最大点<i>H</i><sub>max</sub>确定,超集合<i>B</i><sub><i>j</i></sub>定义如式(5)所示。</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mi>X</mi><mo>,</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>min</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>,</mo><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo>,</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>min</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>,</mo><mi>Η</mi><msub><mrow></mrow><mrow><mi>max</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">)</mo></mrow><mo>}</mo></mrow><mo>,</mo><mo>∀</mo><mi>X</mi><mo>∈</mo><mi>Ι</mi><msup><mrow></mrow><mi>n</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中,<i>X</i>代表样本的输入,<i>n</i>代表样本的数目,每个类别可以有一个超集合,但一个超集合可以包括几个不同类别的样本。不同类别的超集合不能重叠,因为这样会使分类准确率降低。在测试阶段,通过使用这些超集合及其成员函数,确定输入样本的类别。每个超集合的大小由参数<i>ζ</i>(0&lt;<i>ζ</i>&lt;1)控制。</p>
                </div>
                <div class="p1">
                    <p id="99">FM-MN包含3层,如图2所示,其中第1层是输入层<i>F</i><sub><i>A</i></sub>,与输入样本有关,第2层与超集合有关,是模糊层<i>F</i><sub><i>B</i></sub>,第3层是输出层<i>F</i><sub><i>C</i></sub>,并且第3层中的每个节点代表一个类。输入层的每个节点通过<i>H</i><sub>min</sub>和<i>H</i><sub>max</sub>的链路连接到第2层中的所有节点。同样,第2层的每个节点都连接到第3层的所有节点,这些链路的权重通过式(6)计算,样本点的类别<i>C</i><sub><i>k</i></sub>通过式(7)得出:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>u</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>,</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∉</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mn>1</mn><mo>,</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∪</mo><mrow><mi>k</mi><mo>∈</mo><mi>j</mi></mrow></munder><mrow></mrow></mstyle><mi>B</mi><msub><mrow></mrow><mi>j</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">其中,<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∪</mo><mrow><mi>k</mi><mo>∈</mo><mi>j</mi></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml>指的是输入向量与权向量内积后与传递函数处理后输出的结果。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911024_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 经典的FM-MN结构" src="Detail/GetImg?filename=images/JSJK201911024_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 经典的FM-MN结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911024_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Structure of classic FM-MN</p>

                </div>
                <div class="p1">
                    <p id="103">图2中,<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>指的是输入的各个分量,<i>b</i><sub>1</sub>,<i>b</i><sub>2</sub>,…,<i>b</i><sub><i>n</i></sub>是超集合<i>B</i><sub><i>j</i></sub>的各个传递函数,<i>c</i><sub>1</sub>,<i>c</i><sub>2</sub>,…,<i>c</i><sub>3</sub>是神经元的输出结果。</p>
                </div>
                <div class="p1">
                    <p id="104">在训练阶段,在样本空间内对每个数据样本<i><b>X</b></i><sub><i>h</i></sub>搜索,找出具有相同样本的超集合,如果在这些条件下找到这样的一个超集合,则不需要对该集合内样本进行处理,直接对下一个样本集合进行训练;如果没有找到这样的超集合,则进入训练阶段。主要包括3个部分:</p>
                </div>
                <h4 class="anchor-tag" id="186" name="186">(1)扩展:</h4>
                <div class="p1">
                    <p id="105">在此步骤中,找到与样本具有相同类别的超集合,然后扩展超集合以覆盖样本。如果没有找到这样的超集合,则创建一个新的超集合。</p>
                </div>
                <h4 class="anchor-tag" id="187" name="187">(2)重叠测试:</h4>
                <div class="p1">
                    <p id="106">在此步骤中,检查扩展超集合的重叠区域与其他超集合,这些其他超集合与扩展的超集合不属于一个类别。</p>
                </div>
                <h4 class="anchor-tag" id="188" name="188">(3)收缩:</h4>
                <div class="p1">
                    <p id="107">如果没有重叠,则此步骤将被忽略,不执行;否则,根据重叠的类型,在前一步骤中检测到重叠的类型,将重叠的样本直接删除。</p>
                </div>
                <div class="p1">
                    <p id="108">对于每个训练样本执行这3个步骤以创建所需的超集合。在测试阶段,为了确定每个样本的确切类别,根据文献<citation id="175" type="reference">[<a class="sup">18</a>]</citation>中的成员函数公式确定样本的类别,如式(8)所示。</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mi>z</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy="false">[</mo></mstyle><mi>max</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>-</mo><mi>max</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>ρ</mi><mi>min</mi><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>X</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>max</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>ρ</mi><mi>min</mi><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>v</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>X</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中,<i><b>X</b></i><sub><i>h</i></sub>=(<i>X</i><sub><i>h</i></sub><sub>1</sub>,<i>X</i><sub><i>h</i></sub><sub>2</sub>,<i>X</i><sub><i>h</i></sub><sub>3</sub>,…,<i>X</i><sub><i>hn</i></sub>)是第<i>h</i>个样本,<i>ρ</i>是在成员函数中调整<i><b>X</b></i><sub><i>h</i></sub>和<i>b</i><sub><i>j</i></sub>的系数(0&lt;<i>ρ</i>&lt;1)。</p>
                </div>
                <div class="p1">
                    <p id="111">FM-MN存在的不足之处主要有以下2点:</p>
                </div>
                <div class="p1">
                    <p id="112">(1)运用超集合来存放不同类型的文本样本,在此过程中允许集合中存放的不同类别的短文本对象,会造成一定比例的超集合重叠,进而增加了文本处理的空间和时间复杂度。</p>
                </div>
                <div class="p1">
                    <p id="113">(2)在检测出重叠的超集合时直接删除该重叠的超集合,这样会导致重叠的超集合中的文本信息丢失,进而使分类准确率降低。</p>
                </div>
                <div class="p1">
                    <p id="114">多级模糊最小-最大神经网络作为基于FM-MN的方法之一,是一个均匀的分类器,并且使用多级树结构<citation id="176" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。在该分类器中,使用不同级别的网络中的各种大小的超盒子来实现高精度分类。在这种方法中,不采用收缩步骤,应用另一种方法来处理重叠。在MLFM-MN中,网络的每个节点都是一个子网络<citation id="177" type="reference"><link href="42" rel="bibliography" /><link href="54" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">20</a>]</sup></citation>,是一个独立的分类器,用于对属于其区域的模式进行分类。该分类器被称为树形网络。第1级(根节点)对不在边缘的样本进行分类,第2级负责对属于根子网络重叠区域的样本进行分类。通常,网络第<i>i</i>级的每个节点对属于第(<i>i</i>-1)级网络中重叠区域的模式进行分类,各级分类器的关系如图3所示。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911024_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MLFM-MN不同级别的分类" src="Detail/GetImg?filename=images/JSJK201911024_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 MLFM-MN不同级别的分类  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911024_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Classification of different levels in MLFM-MN</p>

                </div>
                <div class="p1">
                    <p id="116">MLFM-MN的优点主要有以下2点:</p>
                </div>
                <div class="p1">
                    <p id="117">(1)在这种算法中,需要的所有超集合都是在训练阶段创建的,用于在测试阶段对样本进行分类。这种方法减少了文本在分类模型中的空间复杂度和时间复杂度,因MLFM-MN中,位于重叠区域的文本对象在下一级分类器被处理,提高了文本分类速度。</p>
                </div>
                <div class="p1">
                    <p id="118">(2)MLFM-MN采用的是多级分类结构,对重叠区域的样本没有直接丢弃,而是进入下一级分类器再一次判断其所属类别,避免了FM-MN模型中的短文本信息的丢失,提高了分类的准确率。</p>
                </div>
                <h3 id="119" name="119" class="anchor-tag"><b>4 MLFM-MN短文本分类算法</b></h3>
                <h4 class="anchor-tag" id="120" name="120"><b>4.1 模型表示-向量空间模型</b></h4>
                <div class="p1">
                    <p id="121">向量空间模型VSM(Vector Space Model)是一种简便、高效的文本表示模型<citation id="178" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。VSM的关键在于怎样选取较大相似度的特征词和有效地计算权重值。在向量空间模型中,一篇文档由提取出的特征词和该词权重以向量的形式表示出来,权重的大小反映了特征词在一篇文档中的地位;而在分类阶段之前,一个文档数据集的类别都是由带有权重的特征词所表示的。在文本分类中TF-IDF(Term Frequency-Inverse Document Frequency)<citation id="179" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>是一种经典的特征词权重值计算方法。</p>
                </div>
                <div class="p1">
                    <p id="122">根据TF-IDF的基本思想,在待测文本<i>S</i>中,特征词<i>t</i><sub><i>i</i></sub>的权重计算公式如式(9)所示。</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>S</mi><mo>¯</mo></mover><mo>=</mo><mo>〈</mo><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>;</mo><mi>t</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>;</mo><mo>⋯</mo><mo>,</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>t</mi><msub><mrow></mrow><mi>n</mi></msub><mo>,</mo><mi>w</mi><msub><mrow></mrow><mi>n</mi></msub><mo>〉</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>W</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>t</mi><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mi>lg</mi><mo stretchy="false">(</mo><mfrac><mi>Ν</mi><mrow><mi>d</mi><mi>f</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></mfrac><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">∥</mo><mover accent="true"><mi>S</mi><mo>¯</mo></mover><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">其中,<i>tf</i><sub><i>i</i></sub>表示特征词<i>t</i><sub><i>i</i></sub>出现的频率,<i>N</i>表示训练集中所有文档的数目,<i>df</i><sub><i>ti</i></sub>表示包含特征词<i>t</i><sub><i>i</i></sub>的文档出现的频率。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>4.2 MLFM-MN短文本分类算法</b></h4>
                <div class="p1">
                    <p id="126">MLFM-MN算法的主要思想是:将得到的短文本数据集分为测试集和训练集,首先通过TNG模型构建特征扩展库,并利用扩展库对原始短文本进行特征扩展;然后对于扩展后的短文本对象,运用TF-IDF公式对扩展后的短文本特征构造VSM模型并将特征向量化;最后运用MLFM-MN算法得到新的短文本对象判断其所属类别,并使用召回率、准确率、<i>F</i>1值对分类效果进行衡量。MLFM-MN算法流程图如图4所示。</p>
                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911024_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MLFM-MN文本分类流程" src="Detail/GetImg?filename=images/JSJK201911024_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 MLFM-MN文本分类流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911024_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Text classification process in MLFM-MN</p>

                </div>
                <div class="p1">
                    <p id="128">MLFM-MN算法的分类流程描述如下:</p>
                </div>
                <div class="p1">
                    <p id="129">(1)首先将数据集分为测试集和训练集,并采用中国科学院分词系统ICTCLAS对中文文档进行分词、去停用词。</p>
                </div>
                <div class="p1">
                    <p id="130">(2)运用TNG模型构建特征扩展库。</p>
                </div>
                <div class="p1">
                    <p id="131">(3)采用式(1)和式(2)计算短文本每个主题词和主题短语的概率。</p>
                </div>
                <div class="p1">
                    <p id="132">(4)利用式(3)和式(4)在扩展库中计算主题词和短语的主题权重值,并由权重值大小选出候选词和候选短语。</p>
                </div>
                <div class="p1">
                    <p id="133">(5)将选出的候选词和候选短语加入到原始短文本中。</p>
                </div>
                <div class="p1">
                    <p id="134">(6)TF-IDF对扩展后的短文本特征配置权重进而构建VSM模型,并将短文本特征向量化。</p>
                </div>
                <div class="p1">
                    <p id="135">(7)运用MLFM-MN算法对(6)中向量化的短文本对象进行分类。</p>
                </div>
                <div class="p1">
                    <p id="136">(8)利用召回率、准确率、<i>F</i>1值评价分类结果。</p>
                </div>
                <h3 id="137" name="137" class="anchor-tag"><b>5 实验结果分析</b></h3>
                <h4 class="anchor-tag" id="138" name="138"><b>5.1 实验环境与评估指标</b></h4>
                <div class="p1">
                    <p id="139">针对提出的MLFM-MN短文本分类算法进行实验验证,本实验平台采用Windows 7 64位操作系统、CPU为Intel(R)-i5-4210M-2.6 GHz、物理内存为4 GB和Eclipse集成开发环境,实验数据来自于搜狗实验室,选取其分类语料库的标准数据集-SogouC做为样本数据,其中包括财经、互联网、健康、教育、军事、旅游、体育、文化、招聘9个类别,各1 990篇共17 910篇文档。对分类效果的评价采用精确率(<i>precision</i>)、召回率(<i>recall</i>)和综合评价指标(<i>F</i>1-measure)。定义如下:</p>
                </div>
                <div class="p1">
                    <p id="140">精确率:</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">召回率:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144"><i>F</i>1-<i>measure</i>:</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><mo>×</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">其中,<i>TP</i>表示分类时将正类预测为正类的数目,<i>FN</i>表示将正类预测为负类的数目,<i>FP</i>将负类预测为正类的数目,<i>TN</i>表示将负类预测为负类的数目。精确率衡量的是类别的查准率;召回率衡量的是类别的查全率;<i>F</i>1-measure对查准率、召回率进行综合考察,以及对它们的偏向程度,且<i>F</i>1综合了<i>P</i>和<i>R</i>的结果,所以<i>F</i>1值越高则越能说明方法有效,分类器具有较强的分类能力。</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147"><b>5.2 实验结果分析</b></h4>
                <div class="p1">
                    <p id="148">为验证本文提出的<i>MLFM</i>-<i>MN</i>算法的有效性,在保证同样数据集的情况下进行了10次实验,与<i>SVM</i>分类算法和朴素贝叶斯文本分类算法进行了比较,表1给出了在同一数据集上3种不同算法分类耗时。利用式(12)～式(14)分别计算出精确率、召回率和F1值,如表2所示。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表1 3种算法的消耗时间对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison of consumption time among the three algorithms</b></p>
                    <p class="img_note">s</p>
                    <table id="149" border="1"><tr><td><br />实验次数</td><td>SVM<br />算法</td><td>MLFM-MN<br />算法</td><td>朴素贝叶斯<br />算法</td></tr><tr><td><br />1</td><td>605</td><td>301</td><td>452</td></tr><tr><td><br />2</td><td>623</td><td>332</td><td>478</td></tr><tr><td><br />3</td><td>634</td><td>307</td><td>501</td></tr><tr><td><br />4</td><td>598</td><td>345</td><td>412</td></tr><tr><td><br />5</td><td>643</td><td>380</td><td>479</td></tr><tr><td><br />6</td><td>606</td><td>381</td><td>506</td></tr><tr><td><br />7</td><td>590</td><td>334</td><td>456</td></tr><tr><td><br />8</td><td>687</td><td>363</td><td>482</td></tr><tr><td><br />9</td><td>652</td><td>371</td><td>411</td></tr><tr><td><br />10</td><td>608</td><td>389</td><td>488</td></tr><tr><td><br />平均</td><td>624</td><td>350</td><td>466</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">通过表1可以看出,在同一数据集上,本文提出的MLFM-MN算法的时间消耗比SVM算法和朴素贝叶斯算法要少得多,与SVM算法相比平均缩短了274 s,与朴素贝叶斯算法相比,耗时均值减少了116 s。同时,从表1还可以看出,本文提出的MLFM-MN算法耗时降低的幅度没有SVM算法与朴素贝叶斯算法的大, 这是因为MLFM-MN在聚类完成后为了保证分类的准确性,在对短文本特征扩展时还需要对样本特征计算概率和排序,然后选出候选特征,而朴素贝叶斯分类时由贝叶斯公式和先验概率直接计算后验概率,而不需要对短文本特征进行扩展。但是,从总体上来说分类速度得到了明显的提升,说明了本文算法的有效性。</p>
                </div>
                <div class="area_img" id="151">
                                            <p class="img_tit">
                                                <b>表2 实验结果对比</b>
                                                    <br />
                                                <b>Table 2 Comparison of experimental results</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911024_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJK201911024_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911024_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 实验结果对比" src="Detail/GetImg?filename=images/JSJK201911024_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="152" name="152" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="153">本文针对海量的短文本特征稀疏、高维度而引起的分类速度和准确率低下的问题,提出了基于TNG特征扩展的MLFM-MN短文本分类算法。该算法首先通过TNG模型对短文本构建特征库,并由特征库以选出的候选特征词和短语对原始短文本进行特征扩展,将扩展后的短文本对象利用VSM模型向量化,最后通过MLFM-MN算法对处理后的样本数据做最后的类别判断。通过对比实验结果可知,本文的MLFM-MN算法在保证较高的准确率前提下,能高速地对文本数据集进行有效分类,在精度和运行时间方面比其他监督分类器(如SVM和朴素贝叶斯)具有更高的分类效率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="189" type="formula" href="images/JSJK201911024_18900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">文武</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="190" type="formula" href="images/JSJK201911024_19000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李培强</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="191" type="formula" href="images/JSJK201911024_19100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">郭有庆</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="4">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD120911003542&amp;v=MDIwNTZIdGpOcm85RlorNExEaE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdnRVN2pOSVZzVE5qN0Jhcks2&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Wang B K,Huang Y F,Yang W X,et al.Short text classification based on strong feature thesaurus[J].Journal of Zhejiang University Science(Computer &amp; Electronics),2012,13(9):649-659.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving short text classification using public search engines">

                                <b>[2]</b> Wang M,Lin L F,Wang J,et al.Improving short text classification using public search engines[C]//Proc of the 2013 International Conference on Integrated Uncertainty in Knowledge Modelling and Decision Making,2013:157-166.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Short text classification using very few words">

                                <b>[3]</b> Sun A.Short text classification using very few words[C]//Proc of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval,2012:1145-1146.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Short Text Classification based on feature extension using The N-Gram model">

                                <b>[4]</b> Zhang X,Wu B.Short text classification based on feature extension using the n-gram model[C]//Proc of International Conference on Fuzzy Systems and Knowledge Discovery,2015:15-17.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding bag-of-words model: A statistical framework">

                                <b>[5]</b> Zhang Y,Jin R,Zhou Z H.Understanding bag-of-words model:A statistical framework[J].International Journal of Machine Learning and Cybernetics,2010,1(1-4):43-52.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 Zhang Lei.The research summary of text categorization and classification algorithms[J].Computer Knowledge and Technology,2016,12(34):225-226.(in Chinese)
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research progress of text classification technology based on machine learning">

                                <b>[7]</b> Liu Jing,Jiang Wen-bo,Shao Ye.Research progress of text classification technology based on machine learning[J].Computer Fan,2018(6):26.(in Chinese)
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300413654&amp;v=MDY4NzJmYks3SHRET3JJOUZZT29NQ25rOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUoxc1ZieFU9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Miao Y Q,Kamel M.Pairwise optimized rocchio algorithm for text categorization[J].Pattern Recognition Letters,2011,32(2):375-382.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance analysis of Naive Bayes and J48 classification algorithm for data classification">

                                <b>[9]</b> Patil T R,Sherekar S S.Performance analysis of naive bayes and J48 classification algorithm for data classification[J].International Journal of Computer Science and Applications,2013,6(2):256-261.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector machine active learning with applications to text classification">

                                <b>[10]</b> Tong S,Koller D.Support vector machine active learning with applications to text classification[J].Journal of Machine Learning Research,2002,2:45-66.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES285124BCF33E7EA6AA78894DA5F5C4DB&amp;v=MTUyOThCaW16aDFRSGJtMkdNd0Q3Zm5RYzd0Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJodzcyK3hhOD1OaWZPZmJHd0c5RE9xLzAyRXVnTWVYdE12aA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Deng Z,Zhu X,Cheng D,et al.Efficient knn classification algorithm for big data[J].Neurocomputing,2016,195:143-148.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural networks and support vector machine algorithms for automatic cloud classification of whole-sky ground-based images">

                                <b>[12]</b> Taravat A,Frate F D,Cornaro C,et al.Neural networks and support vector machine algorithms for automatic cloud classification of whole-sky ground-based images[J].IEEE Geoscience and Remote Sensing Letters,2015,12(3):666-670.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3D70D76EE98E8BA4046AA99FEA7B4C38&amp;v=MjQyOTBmT2ZiRE1HZEc0cUlrd0VlSUhlWFJMdmhJVDdqa01PWGJyMm1kRWZzQ1FOcm1YQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJodzcyK3hhOD1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Ghareb A S,Bakar A A,Hamdan A R.Hybrid feature selection based on enhanced genetic algorithm for text categorization[J].Expert Systems with Applications,2016,49:31-47.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature extension for Chinese short text classification based on topical N-Grams">

                                <b>[14]</b> Sun B,Zhao P.Feature extension for Chinese short text classification based on topical N-Grams[C]//Proc of 2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),2017:477-482.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topical n-grams:phrase and topic discovery,with and application to information retrieval">

                                <b>[15]</b> Wang X R,Mccallum A,Wei X.Topical n-grams:Phrase and topic discovery,with an application to information retrieval[C]//Proc of the 2007 7th IEEE International Conference on Data Mining,2007:697-702.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on Chinese short text classification based on wikipedia">

                                <b>[16]</b> Fan Yun-jie,Liu Huai-liang.Research on Chinese short text classification based on wikipedia[J].Data Analysis and Knowledge Discovery,2012,28(3):47-52.(in Chinese)
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wikipedia-based semantic similarity measurements for noisy short texts using extended naive Bayes">

                                <b>[17]</b> Shirakawa M,Nakayama K,Hara T,et al.Wikipedia-based semantic similarity measurements for noisy short texts using extended naive bayes[J].IEEE Transactions on Emerging Topics in Computing,2015,3(2):205-219.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy min-max neural networks. I. Classification">

                                <b>[18]</b> Simpson P K.Fuzzy min-max neural networks.I.Classification[J].IEEE Transactions on Neural Networks,1992,3(5):776-786.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-level fuzzy min-max neural network classifier">

                                <b>[19]</b> Davtalab R,Dezfoulian M H,Mansoorizadeh M.Multi-level fuzzy min-max neural network classifier[J].IEEE Transactions on Neural Networks and Learning Systems,2014,25(3):470-482.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                 Xue Tao,Wang Ya-ling,Mu Nan.Convolutional neural network based on word sense disambiguation for text classification[J].Journal of Computer Applications,2018,35(10):2898-2903.(in Chinese)
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support Vector Machines and Word2vec for Text Classification with Semantic Features">

                                <b>[21]</b> Lilleberg J,Zhu Yun,Zhang Yan-qing.Support vector machines and word2vec for text classification with semantic features[C]//Proc of 2015 IEEE 14th International Conference on Cognitive Informatics &amp; Cognitive Computing (ICCI* CC),2015:136-140.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 Chen K,Zhang Z,Long J,et al.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Applications,2016,66:245-260.附中文参考文献:
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DNZS201634094&amp;v=MjgxODZPZVplUm1GeS9nVXJyUElTUFJmYkc0SDlmUHE0OU1ZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 张磊.文本分类及分类算法研究综述[J].电脑知识与技术,2016,12(34):225-226.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DNMI201806026&amp;v=MzA3NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dVcnJQSVNQR1o3RzRIOW5NcVk5SFlvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 刘婧,姜文波,邵野.基于机器学习的文本分类技术研究进展[J].电脑迷,2018(6):26.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDTQ201203009&amp;v=MDM4MTR6cXFCdEdGckNVUkxPZVplUm1GeS9nVXJyUFBTbmZmN0c0SDlQTXJJOUZiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 范云杰,刘怀亮.基于维基百科的中文短文本分类研究[J].数据分析与知识发现,2012,28(3):47-52.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201810005&amp;v=MjI4MDVMRzRIOW5OcjQ5RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dVcnJQTHo3U1o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 薛涛,王雅玲,穆楠.基于词义消歧的卷积神经网络文本分类模型[J].计算机应用研究,2018,35(10):2898-2903.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201911024" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911024&amp;v=MzIwMTE0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVXJyUEx6N0JaYkc0SDlqTnJvOUhZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
