<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132348630655000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201910013%26RESULT%3d1%26SIGN%3dNwp2N4hgDTfEn9zIx73tNWEo1qU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910013&amp;v=MDYzNDBGckNVUkxPZVplUm1GeS9nVmJyS0x6N0JaYkc0SDlqTnI0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;2 SIFT特征提取算法&lt;/b&gt; "><b>2 SIFT特征提取算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="(1)构建尺度空间。">(1)构建尺度空间。</a></li>
                                                <li><a href="#50" data-title="(2)在DOG尺度空间定位关键点。">(2)在DOG尺度空间定位关键点。</a></li>
                                                <li><a href="#54" data-title="(3)确定关键点的方向。">(3)确定关键点的方向。</a></li>
                                                <li><a href="#56" data-title="(4)生成特征点描述子。">(4)生成特征点描述子。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="&lt;b&gt;3 纹理特征的生成及阈值模型的建立&lt;/b&gt; "><b>3 纹理特征的生成及阈值模型的建立</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;3.1 纹理特征提取&lt;/b&gt;"><b>3.1 纹理特征提取</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;3.2 最优阈值模型的建立&lt;/b&gt;"><b>3.2 最优阈值模型的建立</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="&lt;b&gt;4 引入纹理特征向量约束的SIFT匹配准则&lt;/b&gt; "><b>4 引入纹理特征向量约束的SIFT匹配准则</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="&lt;b&gt;5 实验结果与分析&lt;/b&gt; "><b>5 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="&lt;b&gt;5.1 模糊及光照变化条件下特征点的提取&lt;/b&gt;"><b>5.1 模糊及光照变化条件下特征点的提取</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;5.2 不同环境条件下图像的匹配&lt;/b&gt;"><b>5.2 不同环境条件下图像的匹配</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="图1 二阶矩的均值与特征点数目的关系">图1 二阶矩的均值与特征点数目的关系</a></li>
                                                <li><a href="#87" data-title="图2 示例图像">图2 示例图像</a></li>
                                                <li><a href="#89" data-title="图3 对比度阈值与特征点数目对比">图3 对比度阈值与特征点数目对比</a></li>
                                                <li><a href="#93" data-title="图4 光照图像匹配结果对比">图4 光照图像匹配结果对比</a></li>
                                                <li><a href="#94" data-title="图5 噪声图像匹配结果对比">图5 噪声图像匹配结果对比</a></li>
                                                <li><a href="#95" data-title="图6 旋转图像匹配结果对比">图6 旋转图像匹配结果对比</a></li>
                                                <li><a href="#96" data-title="图7 模糊图像匹配结果对比">图7 模糊图像匹配结果对比</a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表1 对各测试组图像经典算法和本文算法的比较&lt;/b&gt;"><b>表1 对各测试组图像经典算法和本文算法的比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDA4ODN0RkNEbFZMck9KVnc9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVk&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Wang Si-zheng.Research on scale-invariance based methods for image feature extraction [D].Xi’an:Xidian University,2014.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on scale-invariance based methods for image feature extraction">
                                        <b>[2]</b>
                                         Wang Si-zheng.Research on scale-invariance based methods for image feature extraction [D].Xi’an:Xidian University,2014.(in Chinese)
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" He L Y,Zhou X Y.An auto-adaptive threshold pre-detection SUSAN corner detection algorithm[C]//Proc of the 2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics,2013:511-514." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Auto-adaptive Threshold Pre-detection SUSAN Corner Detection Algorithm">
                                        <b>[3]</b>
                                         He L Y,Zhou X Y.An auto-adaptive threshold pre-detection SUSAN corner detection algorithm[C]//Proc of the 2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics,2013:511-514.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Shen Shi-zhe,Zhang Xiao-long,Heng Wei.Improved Harris corner detection algorithm based on auto-adaptive threshold and pre-selection[J].Journal of Data Acquisition &amp;amp; Processing,2011,26(2):207-213.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJCJ201102015&amp;v=MTUwMjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnJLTmlmSVpMRzRIOURNclk5RVlZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Shen Shi-zhe,Zhang Xiao-long,Heng Wei.Improved Harris corner detection algorithm based on auto-adaptive threshold and pre-selection[J].Journal of Data Acquisition &amp;amp; Processing,2011,26(2):207-213.(in Chinese)
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     Li Jian-lei,Wang Guang-hui,Gao Ning,et al.A SIFT matching algorithm based on global context DAISY descriptor [J].Science of Surveying and Mapping,2016,41(7):33-36.(in Chinese)</a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Ji Hua,Wu Yuan-hao,Sun Hong-hai,et al.SIFT feature matching algorithm with global information [J].Optics and Precision Engineering,2009,17(2):439-444.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM200902034&amp;v=MjM2MzFtRnkvZ1ZicktJalhCWTdHNEh0ak1yWTlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Ji Hua,Wu Yuan-hao,Sun Hong-hai,et al.SIFT feature matching algorithm with global information [J].Optics and Precision Engineering,2009,17(2):439-444.(in Chinese)
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Zhai Y,Zeng L.A SIFT matching algorithm based on adaptive contrast threshold[C]//Proc of 2011 International Conference on Consumer Electronics,Communications and Networks (CECNet),2011:1934-1937." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A SIFT matching algorithm based on adaptive contrast threshold">
                                        <b>[7]</b>
                                         Zhai Y,Zeng L.A SIFT matching algorithm based on adaptive contrast threshold[C]//Proc of 2011 International Conference on Consumer Electronics,Communications and Networks (CECNet),2011:1934-1937.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Bay H,Ess A,Tuyelaars T,et al.Speeded-up robust features(SURF)[J].Computer Vision and Image Understanding,2008,110(3):346-359." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MjMwMTI2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUoxc1NieEE9TmlmT2ZiSzdIdEROcW85RVpPTU1CSFF4b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Bay H,Ess A,Tuyelaars T,et al.Speeded-up robust features(SURF)[J].Computer Vision and Image Understanding,2008,110(3):346-359.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Conner R W,Harlow C A.A theoretical comparison of texture algorithms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1980,2(3):204-222." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Theoretical Comparison of Texture Algorithms">
                                        <b>[9]</b>
                                         Conner R W,Harlow C A.A theoretical comparison of texture algorithms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1980,2(3):204-222.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Bai Ya-xi,Liu Zhu-ping,Ling Jian-guo.Improvd SIFT algorithm based on texture features [J].Infrared Technology,2016,38(8):705-708.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWJS201608014&amp;v=MjA1OTZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJyS0xUckJmYkc0SDlmTXA0OUU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Bai Ya-xi,Liu Zhu-ping,Ling Jian-guo.Improvd SIFT algorithm based on texture features [J].Infrared Technology,2016,38(8):705-708.(in Chinese)
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Mei Lang-qi,Guo Jian-ming,Liu Qing.Research and application of texture feature extraction based on multi-features [J].Journal of Transport Information and Safety,2015,33(2):31-38.(in Chinese)</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_2" title=" 王斯正.基于尺度不变性的图像特征提取方法研究[D].西安:西安电子科技大学,2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015433661.nh&amp;v=MDY4NTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnJLVkYyNkc3ZTdIZGZLcnBFYlA=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         王斯正.基于尺度不变性的图像特征提取方法研究[D].西安:西安电子科技大学,2014.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     沈士喆,张小龙,衡伟.一种自适应阈值的预筛选Harris角点检测方法[J].数据采集与处理,2011,26(2):207-213.</a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_5" title=" 李建磊,王光辉,高宁,等.结合全局信息描述子的局部特征匹配算法[J].测绘科学,2016,41(7):33-36." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHKD201607007&amp;v=MzIyMzZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJyS0ppWEFhckc0SDlmTXFJOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         李建磊,王光辉,高宁,等.结合全局信息描述子的局部特征匹配算法[J].测绘科学,2016,41(7):33-36.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     纪华,吴元昊,孙宏海,等.结合全局信息的SIFT特征匹配算法[J].光学精密工程,2009,17(2):439-444.</a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     白亚茜,刘著平,凌建国.基于纹理特征的SIFT算法改进[J].红外技术,2016,38(8):705-708.</a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_11" title=" 梅浪奇,郭建明,刘清.基于多特征的纹理特征提取方法研究与应用[J].交通信息与安全,2015,33(2):31-38." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JTJS201502006&amp;v=MjcwMjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZicktMem5CZmJHNEg5VE1yWTlGWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         梅浪奇,郭建明,刘清.基于多特征的纹理特征提取方法研究与应用[J].交通信息与安全,2015,33(2):31-38.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(10),1803-1808 DOI:10.3969/j.issn.1007-130X.2019.10.012            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>混合特征下最优阈值预测的图像匹配</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E6%98%A5%E6%BB%A1&amp;code=10215380&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严春满</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%9D%E6%9C%89%E8%8F%B2&amp;code=42557061&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郝有菲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E8%BF%AA&amp;code=11399597&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张迪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E4%BD%B3%E8%BE%89&amp;code=42380957&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈佳辉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8C%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86%E4%B8%8E%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0012645&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西北师范大学物理与电子工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对单一特征条件下图像匹配率较低,以及SIFT算法由于固定对比度阈值造成特征点数目提取不均的问题,提出一种混合特征下最优阈值预测的图像匹配算法。该算法首先采用SIFT算法提取图像特征点,然后利用纹理参数二阶矩自适应法得到最优阈值,并用描述性较强的纹理特征向量对SIFT匹配过程进行约束实现图像的匹配。实验结果表明,提出的算法根据图像灰度分布自适应选取对比度阈值,能够增强图像细节信息且使提取的特征点数量稳定,在匹配过程中引入纹理向量作为约束准则,避免了相似区域的误匹配,对光照和模糊图像有较好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SIFT&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SIFT;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%B9%E7%90%86%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">纹理特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%B9%E6%AF%94%E5%BA%A6%E9%98%88%E5%80%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">对比度阈值;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    严春满（1970-），男，甘肃华亭人，博士，副教授，研究方向为信号处理。E-mail:Yancha02@163.com，通信地址:730070甘肃省兰州市西北师范大学物理与电子信息工程学院&lt;image id="128" type="formula" href="images/JSJK201910013_12800.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    郝有菲（1995-），女，甘肃白银人，硕士生，研究方向为图像配准和电子稳像。E-mail:1436613290@qq.com，通信地址:730070甘肃省兰州市西北师范大学物理与电子信息工程学院&lt;image id="130" type="formula" href="images/JSJK201910013_13000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    张迪（1994-），女，河北石家庄人，硕士生，研究方向为人脸识别。E-mail:2812434011@qq.com，通信地址:730070甘肃省兰州市西北师范大学物理与电子信息工程学院&lt;image id="132" type="formula" href="images/JSJK201910013_13200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    陈佳辉，通信地址:730070甘肃省兰州市西北师范大学物理与电子信息工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61741119);</span>
                                <span>甘肃省自然科学基金(17JR5RA074,17JR5RA078);</span>
                    </p>
            </div>
                    <h1><b>An image matching method based on optimal threshold prediction under hybrid features</b></h1>
                    <h2>
                    <span>YAN Chun-man</span>
                    <span>HAO You-fei</span>
                    <span>ZHANG Di</span>
                    <span>CHEN Jia-hui</span>
            </h2>
                    <h2>
                    <span>College of Physics and Electronic Engineering,Northwest Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of low image matching rate under single feature condition, and the uneven extraction of feature points of the scale-invariant feature transform(SIFT) algorithm due to fixed contrast threshold, we propose a novel image matching method based on adaptive threshold prediction under hybrid features. Firstly, the algorithm uses the SIFT to extract image feature points. Then, we employ the texture parameter second moment method to adaptively calculate the optimal threshold, and the descriptive texture feature vector to constrain the SIFT matching process. Experimental results demonstrate that the proposed method can adaptively select the contrast threshold according to the gray level distribution of the image, enhance image detail information and stabilize the number of extracted feature points. The texture vectors constrain the matching process to avoid the mismatch of similar regions. The method is robust to illumination and blurred images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SIFT&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SIFT;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=texture%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">texture feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=contrast%20threshold&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">contrast threshold;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YAN Chun-man,born in 1970,PhD, associate professor,his research interest includes signal processing.Address:College of Physics and Electronic Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    HAO You-fei,born in 1995,MS candidate,her research interests include image registration,and electronic image stabilization.Address:College of Physics and Electronic Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    ZHANG Di,born in 1994,MS candidate,her research interest includes face recognition.Address:College of Physics and Electronic Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    CHEN Jia-hui,Address:College of Physics and Electronic Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="38">图像匹配技术主要应用于图像融合、目标识别、医学图像分析、机器人视觉等领域。基于特征的图像匹配算法在图像配准领域尤为重要,其关键步骤涉及特征点的检测与提取,并直接影响后续的匹配精度。图像局部特征由于其良好的不变性及鲁棒性,一直以来是图像匹配算法的研究热点。在众多特征提取算法中,Lowe等人<citation id="107" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的SIFT(Scale-Invariant Feature Transform)算法获得了广泛的应用。SIFT算法通过构造尺度空间提取具有稳健性的特征描述子,该算子能较好地解决影像旋转、缩放、视点变化等问题。但是,SIFT算法采用固定对比度阈值,缺乏对不同灰度图像的适应性,在特定条件下提取的特征点数量较少且分布不均匀<citation id="108" type="reference"><link href="5" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">2</a>]</sup></citation>。文献<citation id="113" type="reference">[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">4</a>]</citation>采用迭代算法获取自适应阈值,提高了算法的适应性,但计算量较大。另外,在影像相似或场景重复条件下,局部特征描述子由于缺乏全局信息从而导致产生大量的误匹配点对<citation id="109" type="reference"><link href="11" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">5</a>]</sup></citation>。针对以上问题,文献<citation id="110" type="reference">[<a class="sup">6</a>,<a class="sup">6</a>]</citation>根据图像归一化熵,提出一种自动计算SIFT对比度阈值的方法,相对计算量较小且能够提高不同对比度图像匹配的效率,但对自相似性较强的图像进行匹配时,效果不佳。文献<citation id="111" type="reference">[<a class="sup">7</a>]</citation>通过加入全局纹理信息对SIFT算法做改进,降低了因影像相似造成的误匹配概率,但特征维数的提高会使算法效率下降。为解决SIFT算法的实时性问题,Bay等人<citation id="112" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>在SIFT算法基础之上提出了加速稳健特征SURF(Speeded Up Robust Features)提取算法,特征维度较低,算法速度比SIFT算法提高了3～5倍。低维度描述有可能造成细节纹理和空间结构信息丢失。</p>
                </div>
                <div class="p1">
                    <p id="39">针对以上存在的问题,本文提出一种混合特征下最优阈值预测的图像匹配算法。根据纹理特征参数二阶矩及先验知识建立最优阈值预测模型,随图像灰度的变化自适应地改变阈值使得特征点数目足量且分布均匀,并利用描述性较强的纹理特征向量约束SIFT匹配过程,有效避免相似区域的误匹配。实验结果表明,在实时性要求不高的情况下,本文算法可有效地提高图像特征点检测质量,降低误匹配率。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>2 SIFT特征提取算法</b></h3>
                <div class="p1">
                    <p id="41">SIFT算法主要步骤如下所示:</p>
                </div>
                <h4 class="anchor-tag" id="42" name="42">(1)构建尺度空间。</h4>
                <div class="p1">
                    <p id="43">构造尺度空间可获取图像数据的多尺度特征。实践中多采用高斯卷积核实现尺度变换。二维图像的尺度空间定义为:</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>σ</mi><mo stretchy="false">)</mo><mo>=</mo><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>σ</mi><mo stretchy="false">)</mo><mo>*</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">其中,<i>G</i>(<i>x</i>,<i>y</i>,<i>σ</i>)为高斯核函数。</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>σ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mtext>π</mtext><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mo stretchy="false">(</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">其中,<i>I</i>(<i>x</i>,<i>y</i>)为图像数据,(<i>x</i>,<i>y</i>)代表图像像素点的坐标;<i>σ</i>为尺度因子,反映图像被平滑的程度;<i>L</i>(<i>x</i>,<i>y</i>,<i>σ</i>)代表图像尺度空间。</p>
                </div>
                <div class="p1">
                    <p id="48">为了能在尺度空间检测出有效的极值点,使用高斯差分DOG(Difference of Gaussians)算子作为判据。设<i>k</i>为2个相邻尺度间的比例因子,DOG算子定义如下:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>σ</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">[</mo><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>k</mi><mi>σ</mi><mo stretchy="false">)</mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>G</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>σ</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>×</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>k</mi><mi>σ</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>σ</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="50" name="50">(2)在DOG尺度空间定位关键点。</h4>
                <div class="p1">
                    <p id="51">通过DOG函数的二阶泰勒展开式精确定位关键点的位置和尺度。</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">D</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">X</mi></mrow></mfrac><mi mathvariant="bold-italic">X</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi mathvariant="bold-italic">D</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mi mathvariant="bold-italic">X</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">其中,<i><b>X</b></i>=(<i>x</i>,<i>y</i>,<i>σ</i>)<sup>T</sup>。低对比度以及处于边缘的点都属于不稳定的关键点,极易受到噪声的影响,需要剔除。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">(3)确定关键点的方向。</h4>
                <div class="p1">
                    <p id="55">为确定每幅图中的特征点,计算每一个关键点<i>L</i>(<i>x</i>,<i>y</i>)的梯度模值<i>m</i>和方向<i>θ</i>,利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数,使算子具备旋转不变性。<i>L</i>(<i>x</i>,<i>y</i>)所用尺度为每个关键点所在尺度。至此,图像关键点检测完成,每个关键点有3个信息:位置、所处尺度和方向。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56">(4)生成特征点描述子。</h4>
                <div class="p1">
                    <p id="57">取特征点周围16×16像素的窗口,将此窗口分为4×4像素的子区域。在每个子区域计算8个方向的梯度,形成128维的特征向量,再对特征向量进行归一化去除光照变化的影响。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag"><b>3 纹理特征的生成及阈值模型的建立</b></h3>
                <div class="p1">
                    <p id="59">SIFT中离关键点越近的梯度值贡献越大,用高斯加权函数进行累加。实际计算过程中,取高斯加权函数的半径为关键点尺度大小,使半径以外的高斯加权系数为0。关键点的尺度较小时,其特征向量有效邻域范围就比较小,如果图像中存在多个相似区域,那么SIFT特征向量就有很大的相似性,从而造成误匹配<citation id="114" type="reference"><link href="11" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">5</a>]</sup></citation>。针对此,本文还提取出具有区分局部相似性的纹理特征参数矩阵<i><b>T</b></i>,表示如下:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mn>1</mn><mtext>Τ</mtext></msubsup><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mn>3</mn><mtext>Τ</mtext></msubsup><mspace width="0.25em" /><mo>⋯</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mspace width="0.25em" /><mo>⋯</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mn>1</mn><mn>6</mn></mrow><mtext>Τ</mtext></msubsup></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中,<i><b>T</b></i><sub><i>i</i></sub>=(<i>t</i><sub>1</sub>,<i>t</i><sub>2</sub>,<i>t</i><sub>3</sub>,<i>t</i><sub>4</sub>,<i>t</i><sub>5</sub>,<i>t</i><sub>6</sub>)。</p>
                </div>
                <div class="p1">
                    <p id="62">在SIFT匹配过程中,引入<i><b>T</b></i>作为一种新的约束准则,避免匹配过程中的相似区域误匹配,由于纹理特征参数矩阵<i><b>T</b></i>不用进行描述子表示,只参与后续的匹配过程,因而没有描述子表达的耗时问题。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63"><b>3.1 纹理特征提取</b></h4>
                <div class="p1">
                    <p id="64">纹理是图像的重要特征,不同物体的纹理特征区分度较明显,它是图像分析的一种常用特征。角点特征受像素点灰度对比和相关分布影响,因此提取图像灰度共生矩阵的若干参数作为图像特征点提取的补充依据<citation id="115" type="reference"><link href="11" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">5</a>]</sup></citation>。为提高运行效率,将图像的灰度级量化成16级。</p>
                </div>
                <div class="p1">
                    <p id="65">已知<i>SIFT</i>在每个子区域中计算8个方向0°,45°,90°,135°,180°(-0°),225°(-45°),270°(-90°),315°(-135°)的梯度累加值,得到特征描述子。本文选取距离d=1;<i>θ</i>为0°,45°,90°及135° 4个方向的灰度共生矩阵,在保证SIFT特征优点的前提下又可以区分相似局部特征,相比于其他算法,该纹理特征的划分更加简化而又不失效果。取如下4种描述能力较强的纹理特征向量<citation id="116" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="66">(1)二阶矩<i><b>f</b></i><b>1</b>:反映图像灰度分布的均匀程度和纹理粗细度;</p>
                </div>
                <div class="p1">
                    <p id="67">(2)对比度(惯性矩)<i><b>f</b></i><b>2</b>:反映图像的清晰度;</p>
                </div>
                <div class="p1">
                    <p id="68">(3)相关<i><b>f</b></i><b>3</b>:衡量灰度共生矩阵元素在行或列方向上的相似性;</p>
                </div>
                <div class="p1">
                    <p id="69">(4)熵<i><b>f</b></i><b>4</b>:反映图像中纹理的复杂程度或非均匀度。</p>
                </div>
                <div class="p1">
                    <p id="70">分别计算对应于SIFT每个4×4子区域0°,45°,90°,135° 4个方向的灰度共生矩阵的上述4种特征,然后对4个方向的同一特征求平均值和均方差,从而得到具有旋转不变性的8位灰度共生矩阵纹理特征。此处纹理特征主要用来区分相似度,因而只取关联性较高的对比度、相关及熵这3个特征的平均值和均方差构成6维的特征参数向量(<i>t</i><sub>1</sub>,<i>t</i><sub>2</sub>,<i>t</i><sub>3</sub>,<i>t</i><sub>4</sub>,<i>t</i><sub>5</sub>,<i>t</i><sub>6</sub>)。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>3.2 最优阈值模型的建立</b></h4>
                <div class="p1">
                    <p id="72">在光照较暗的条件下,固定的对比度阈值会把很多对比度值较小的特征点忽略,因此有必要根据不同的光照强度选取合适对比度阈值,以提高特征点提取的质量。</p>
                </div>
                <div class="p1">
                    <p id="73">另外,研究结果表明纹理特征参数与特征点关系密切<citation id="117" type="reference"><link href="21" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">10</a>]</sup></citation>。本文对Oxford数据集图像纹理特征参数二阶矩的均值和特征点数目进行统计,结果如图1所示。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 二阶矩的均值与特征点数目的关系" src="Detail/GetImg?filename=images/JSJK201910013_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 二阶矩的均值与特征点数目的关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Relationship between the mean of the 
 secondary moment and the number of feature points</p>

                </div>
                <div class="p1">
                    <p id="75">由图1可见,在图像模糊(Blur)和光照(Lighting)条件下,随着二阶矩均值的增大,提取的特征点数目减少。依据文献<citation id="118" type="reference">[<a class="sup">10</a>,<a class="sup">10</a>]</citation>的研究结果,当二阶矩值小于0.015时,固定阈值提取的特征点能够获得良好的匹配效果。分析图1可知,当二阶矩值增大时,提取的特征点数目呈下降趋势,影响图像的匹配效果。合适的特征点数是保证图像匹配的前提,为保证提取到适量的匹配特征点,当二阶矩值增大时,应相应降低对比度阈值。基于以上分析,本文建立适当的自适应阈值模型,根据图像纹理特征二阶矩值预测适当的对比度阈值,使得提取的特征点数较为稳定,以获得良好的匹配效果。</p>
                </div>
                <div class="p1">
                    <p id="76">为建立一个对比度阈值随图像二阶矩增大而相对降低且变化平稳的阈值模型,本文选用对数函数来实现。一般SIFT对比度阈值大约取[0.01,0.03],为能使二阶矩值映射到相应的对比度阈值区间,式(6)中引入参数<i>μ</i>调整对比度阈值的数量级;引入<i>λ</i>对二阶矩均值适当地放大,并在多次实验的基础上选取<i>λ</i>的值为1.5;为避免二阶矩值出现0而使得对数函数自变量为0的情况,加入参数<i>ε</i>,其大小可在0.001～0.01任取,多次实验结果显示该参数对阈值预测模型无大的影响。对比度阈值的自适应选择可以增强各种场景图像匹配的鲁棒性。因此,根据不同图像的特征参数建立最优阈值模型,如下所示:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>.</mo><mn>0</mn><mn>3</mn><mo>,</mo><mi>A</mi><mi>S</mi><mi>Μ</mi><mo>&lt;</mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>1</mn><mn>5</mn></mtd></mtr><mtr><mtd><mi>μ</mi><mspace width="0.25em" /><mi>ln</mi><mo stretchy="false">(</mo><mi>λ</mi><mo>⋅</mo><mi>A</mi><mi>S</mi><mi>Μ</mi><mo>+</mo><mi>ε</mi><mo stretchy="false">)</mo><mo>,</mo><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>s</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中,<i>ASM</i>为待配准图像纹理特征二阶矩的均值,<i>μ</i>、<i>λ</i>、<i>ε</i>为阈值系数。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag"><b>4 引入纹理特征向量约束的SIFT匹配准则</b></h3>
                <div class="p1">
                    <p id="80">图像的匹配通常是通过特征的相似性度量来实现的。相似性度量就是用某种相似性准则计算2幅图像特征之间的距离,然后根据距离的大小判断2图像的相似度。由于欧氏距离计算简单,本文选择欧氏距离作为相似性度量准则<citation id="119" type="reference"><link href="23" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="81">基于<i>SIFT</i>描述子的匹配过程一般由于细节信息的丢失会造成相似区域的误匹配现象,因此本文在图像匹配过程中添加主要纹理信息,以区别相似区域。匹配准则定义如下:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>ω</mi><msub><mrow></mrow><mn>1</mn></msub><mi>D</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mn>2</mn></msub><mi>D</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中,<i><b>I</b></i><sub>1</sub>、<i><b>I</b></i><sub>2</sub>分别对应基准图像和待匹配图像的特征矩阵,<i>D</i><sub>S</sub>、<i>D</i><sub>T</sub>分别对应<i>SIFT</i>特征与纹理特征参数向量的欧氏距离,<i>ω</i><sub>1</sub>、<i>ω</i><sub>2</sub>为对应的权重参数,满足<i>ω</i><sub>1</sub>+<i>ω</i><sub>2</sub>=1。本文以<i>SIFT</i>匹配为主,引用纹理特征参数向量作为约束<i>SIFT</i>匹配过程的新准则,避免相似区域误匹配。</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag"><b>5 实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="85" name="85"><b>5.1 模糊及光照变化条件下特征点的提取</b></h4>
                <div class="p1">
                    <p id="86">为了进一步验证自适应阈值模型在特征点提取方面的良好性能,从<i>Oxford</i>数据集选取图像模糊变化组和光照变化组完成图像的特征点提取实验,示例图像如图2所示。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 示例图像" src="Detail/GetImg?filename=images/JSJK201910013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 示例图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 2 <i>Sample images</i></p>

                </div>
                <div class="p1">
                    <p id="88">首先将本文算法应用于各变化组图像进行最优阈值预测,然后用固定对比度0.03的<i>SIFT</i>算法和自适应最优阈值模型对图像组依次进行特征提取。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 对比度阈值与特征点数目对比" src="Detail/GetImg?filename=images/JSJK201910013_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 对比度阈值与特征点数目对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 3 <i>Comparison of the number of</i><i>feature points and contrast threshold</i></p>

                </div>
                <div class="p1">
                    <p id="90">图3为<i>SIFT</i>算法、文献<citation id="120" type="reference">[<a class="sup">10</a>,<a class="sup">10</a>]</citation>算法和本文算法对光照组和模糊组图像的阈值预测和特征点数目对比。分析图3<i>a</i>、图3<i>c</i>可知,<i>SIFT</i>算法使用固定阈值,未关注图像组的场景变化情况,而文献<citation id="121" type="reference">[<a class="sup">10</a>,<a class="sup">10</a>]</citation>和本文算法均能够根据场景的具体变化自适应调整阈值,使得图像的特征点数目更充足。但是,文献<citation id="122" type="reference">[<a class="sup">10</a>,<a class="sup">10</a>]</citation>算法阈值预测波动幅度较大,甚至超出<i>SIFT</i>对比度阈值的一般取值[0.01,0.03],不能较好地适应复杂场景的图像匹配。分析图3 <i>b</i> 、图3<i>d</i>结果可知,本文算法提取的特征点数量相对固定阈值的<i>SIFT</i>算法明显增多。在模糊或是光照度很暗的极限条件下,提出的算法可根据图像二阶矩参数自适应调整对比度阈值,提高特征点数量,以避免匹配失败。进一步分析图3<i>b</i>的实验结果可知,本文算法对光照变化较大的图像提取特征点时,其数目相应比较稳定。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>5.2 不同环境条件下图像的匹配</b></h4>
                <div class="p1">
                    <p id="92">为进一步测试算法的适应性,对几组不同条件下的图像进行匹配实验,依次完成光照、噪声、旋转及模糊4种环境条件下图像的匹配,并完成经典算法及同类改进算法(文献<citation id="123" type="reference">[<a class="sup">6</a>,<a class="sup">6</a>]</citation>)与本文算法的对比测试。对比测试结果如图4～图7所示,图4<i>a</i>、图5<i>a</i>、图6<i>a</i>、图7<i>a</i>为<i>SIFT</i>算法的匹配结果,图4<i>b</i>、图5<i>b</i>、图6<i>b</i>、图7<i>b</i>为本文算法的匹配结果。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 光照图像匹配结果对比" src="Detail/GetImg?filename=images/JSJK201910013_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 光照图像匹配结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 4 <i>Matching results comparison of illuminated images</i></p>

                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 噪声图像匹配结果对比" src="Detail/GetImg?filename=images/JSJK201910013_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 噪声图像匹配结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 5 <i>Matching results comparison of noisy images</i></p>

                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 旋转图像匹配结果对比" src="Detail/GetImg?filename=images/JSJK201910013_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 旋转图像匹配结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 6 <i>Matching results comparison of rotated images</i></p>

                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910013_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 模糊图像匹配结果对比" src="Detail/GetImg?filename=images/JSJK201910013_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 模糊图像匹配结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910013_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 7 <i>Matching results comparison of blurred images</i></p>

                </div>
                <div class="p1">
                    <p id="97">由于在<i>SIFT</i>每个子区域中添加纹理特征参数做匹配约束准则,从测试结果可见匹配过程中避开了相似区域的匹配,误匹配对减少。一般噪声干扰下会出现许多相似的伪特征点,<i>SIFT</i>描述子没有其邻域内的相似性判断,图像噪声的增加对图像的弱纹理细节影响较大,导致无法较好地提取特征点,而固定的对比度阈值进一步限制了特征点的提取。本文算法根据图像特征自适应选取对比度阈值且引入纹理向量欧氏距离,可有效增强相似区域的判定。</p>
                </div>
                <div class="p1">
                    <p id="98">为进一步验证算法的有效性,选取正确匹配率<i>R</i>作为客观指标,完成4组图像的对比测试。<i>R</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mtext>正</mtext><mtext>确</mtext><mtext>匹</mtext><mtext>配</mtext><mtext>点</mtext><mtext>对</mtext><mtext>数</mtext></mrow><mrow><mtext>总</mtext><mtext>匹</mtext><mtext>配</mtext><mtext>点</mtext><mtext>对</mtext><mtext>数</mtext></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">为了进行详细对比,我们统计了对比测试实验中的对比度阈值、特征点总数目、总匹配点对数、正确匹配点对数和正确匹配率,测试结果如表1所示。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表1 对各测试组图像经典算法和本文算法的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison between the classical algorithms and the proposed algorithm for each test group images</b></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td>测试<br />图像组</td><td>比较<br />算法</td><td>对比<br />度阈值</td><td>特征点<br />总数目</td><td>总匹配<br />点对数</td><td>正确匹配<br />点对数</td><td>正确匹<br />配率/%</td></tr><tr><td></td><td>SURF算法</td><td>\</td><td>408、416</td><td>318</td><td>237</td><td>74.30</td></tr><tr><td><br /></td><td>原SIFT<br />算法</td><td>0.03</td><td>376、417</td><td>221</td><td>161</td><td>72.80</td></tr><tr><td><br />光照<br />图像组<br /></td><td>文献[6]<br />算法</td><td>0.024</td><td>449、427</td><td>234</td><td>171</td><td>73.10</td></tr><tr><td><br /></td><td>本文算法</td><td>0.023</td><td>444、499</td><td>258</td><td>198</td><td>76.70</td></tr><tr><td><br /></td><td>SURF算法</td><td>\</td><td>327、389</td><td>134</td><td>103</td><td>76.80</td></tr><tr><td><br /></td><td>原SIFT<br />算法</td><td>0.03</td><td>223、238</td><td>88</td><td>64</td><td>72.70</td></tr><tr><td><br />噪声<br />图像组<br /></td><td>文献[6]<br />算法</td><td>0.019 3</td><td>321、366</td><td>102</td><td>84</td><td>82.35</td></tr><tr><td><br /></td><td>本文算法</td><td>0.010 3</td><td>404、467</td><td>124</td><td>100</td><td>80.50</td></tr><tr><td><br /></td><td>SURF算法</td><td>\</td><td>571、695</td><td>287</td><td>237</td><td>82.50</td></tr><tr><td><br /></td><td>原SIFT<br />算法</td><td>0.03</td><td>554、632</td><td>220</td><td>177</td><td>80.50</td></tr><tr><td><br />旋转<br />图像组<br /></td><td>文献[6]<br />算法</td><td>0.027 5</td><td>664、884</td><td>223</td><td>179</td><td>80.30</td></tr><tr><td><br /></td><td>本文算法</td><td>0.029 8</td><td>558、636</td><td>221</td><td>186</td><td>84.50</td></tr><tr><td><br /></td><td>SURF算法</td><td>\</td><td>183、286</td><td>136</td><td>65</td><td>47.50</td></tr><tr><td><br /></td><td>原SIFT<br />算法</td><td>0.03</td><td>223、238</td><td>30</td><td>11</td><td>36.70</td></tr><tr><td><br />模糊<br />图像组<br /></td><td>文献[6]<br />算法</td><td>0.021 2</td><td>104、295</td><td>67</td><td>33</td><td>49.30</td></tr><tr><td><br /></td><td>本文算法</td><td>0.021 4</td><td>100、292</td><td>54</td><td>32</td><td>59.30</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="102">分析表1可见,本文算法在各测试组图像中表现较优,客观指标高于SIFT算法和SURF算法的。对于光照和旋转图像组,本文算法相对SIFT算法在匹配精度上有少许提高。光照组图像匹配精度主要受最优阈值模型的影响,根据特定情况设置阈值,提取足量特征点数以提高匹配精度。旋转图像组中,旋转并不改变图像的细节信息,因而检测到的特征点数目相近,但由于在SIFT匹配过程中引入了纹理特征参数向量做约束匹配准则,匹配精度得以提高。</p>
                </div>
                <div class="p1">
                    <p id="103">从表1还可以看出,本文算法相比于文献<citation id="124" type="reference">[<a class="sup">6</a>,<a class="sup">6</a>]</citation> 算法,对除噪声图像组外的其他场景图像的匹配均有较好的表现。2种算法均通过特定参数自适应调整对比度阈值来增加匹配点数,从而获得较好的匹配效果。文献<citation id="125" type="reference">[<a class="sup">6</a>,<a class="sup">6</a>]</citation>算法采用熵值方法,当图像对比度变化较显著时匹配率较高,但对灰度分布均匀且相似性强的图像匹配效果与SIFT相当,而本文采用的二阶矩值更能反映图像的灰度分布状况,又通过引入匹配约束条件,避免了相似区域的误匹配,使得匹配效果增强。</p>
                </div>
                <div class="p1">
                    <p id="104">综合表1及图4～图7的实验结果可见,相对于经典SIFT、SURF和文献<citation id="126" type="reference">[<a class="sup">6</a>,<a class="sup">6</a>]</citation>算法,本文算法由于最优阈值模型的提出增加了特征点数,并在匹配过程中引入了纹理特征参数向量做匹配约束准则,正确匹配点对显著增多,匹配精度有所提高。</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="106">在图像特征提取时,SIFT由于固定的对比度阈值忽略了对图像的适应性,从而影响了算法的鲁棒性。本文算法引入图像的纹理特征参数,根据二阶矩自适应预测对比度阈值,在图像灰度变化较大时,能够提取出足够充分且分布均匀的特征点。并且利用描述性较强的纹理特征向量做SIFT特征匹配过程的约束准则,有效避免相似区域的误匹配,使得匹配性能更加优异。在实验中,与经典的SIFT、SURF及同类的新算法相比较,结果表明本文提出的算法性能最优,对光照和模糊图像的匹配有较好的鲁棒性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDU0MDY5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZMck9KVnc9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on scale-invariance based methods for image feature extraction">

                                <b>[2]</b> Wang Si-zheng.Research on scale-invariance based methods for image feature extraction [D].Xi’an:Xidian University,2014.(in Chinese)
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Auto-adaptive Threshold Pre-detection SUSAN Corner Detection Algorithm">

                                <b>[3]</b> He L Y,Zhou X Y.An auto-adaptive threshold pre-detection SUSAN corner detection algorithm[C]//Proc of the 2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics,2013:511-514.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJCJ201102015&amp;v=MzIyMzQ5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnJLTmlmSVpMRzRIOURNclk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Shen Shi-zhe,Zhang Xiao-long,Heng Wei.Improved Harris corner detection algorithm based on auto-adaptive threshold and pre-selection[J].Journal of Data Acquisition &amp; Processing,2011,26(2):207-213.(in Chinese)
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 Li Jian-lei,Wang Guang-hui,Gao Ning,et al.A SIFT matching algorithm based on global context DAISY descriptor [J].Science of Surveying and Mapping,2016,41(7):33-36.(in Chinese)
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM200902034&amp;v=MDcyNzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJyS0lqWEJZN0c0SHRqTXJZOUdZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Ji Hua,Wu Yuan-hao,Sun Hong-hai,et al.SIFT feature matching algorithm with global information [J].Optics and Precision Engineering,2009,17(2):439-444.(in Chinese)
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A SIFT matching algorithm based on adaptive contrast threshold">

                                <b>[7]</b> Zhai Y,Zeng L.A SIFT matching algorithm based on adaptive contrast threshold[C]//Proc of 2011 International Conference on Consumer Electronics,Communications and Networks (CECNet),2011:1934-1937.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083888&amp;v=MDkzNjZadUh5am1VTGZJSjFzU2J4QT1OaWZPZmJLN0h0RE5xbzlFWk9NTUJIUXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Bay H,Ess A,Tuyelaars T,et al.Speeded-up robust features(SURF)[J].Computer Vision and Image Understanding,2008,110(3):346-359.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Theoretical Comparison of Texture Algorithms">

                                <b>[9]</b> Conner R W,Harlow C A.A theoretical comparison of texture algorithms[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1980,2(3):204-222.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWJS201608014&amp;v=MjE5MzZxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZicktMVHJCZmJHNEg5Zk1wNDlFWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Bai Ya-xi,Liu Zhu-ping,Ling Jian-guo.Improvd SIFT algorithm based on texture features [J].Infrared Technology,2016,38(8):705-708.(in Chinese)
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Mei Lang-qi,Guo Jian-ming,Liu Qing.Research and application of texture feature extraction based on multi-features [J].Journal of Transport Information and Safety,2015,33(2):31-38.(in Chinese)
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015433661.nh&amp;v=MTgzMzlHRnJDVVJMT2VaZVJtRnkvZ1ZicktWRjI2RzdlN0hkZktycEViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 王斯正.基于尺度不变性的图像特征提取方法研究[D].西安:西安电子科技大学,2014.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 沈士喆,张小龙,衡伟.一种自适应阈值的预筛选Harris角点检测方法[J].数据采集与处理,2011,26(2):207-213.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHKD201607007&amp;v=MzE3OTlLSmlYQWFyRzRIOWZNcUk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 李建磊,王光辉,高宁,等.结合全局信息描述子的局部特征匹配算法[J].测绘科学,2016,41(7):33-36.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 纪华,吴元昊,孙宏海,等.结合全局信息的SIFT特征匹配算法[J].光学精密工程,2009,17(2):439-444.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 白亚茜,刘著平,凌建国.基于纹理特征的SIFT算法改进[J].红外技术,2016,38(8):705-708.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JTJS201502006&amp;v=MTI0NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZicktMem5CZmJHNEg5VE1yWTlGWW9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 梅浪奇,郭建明,刘清.基于多特征的纹理特征提取方法研究与应用[J].交通信息与安全,2015,33(2):31-38.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201910013" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910013&amp;v=MDYzNDBGckNVUkxPZVplUm1GeS9nVmJyS0x6N0JaYkc0SDlqTnI0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
