<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358336123750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909025%26RESULT%3d1%26SIGN%3dT8k0dvv%252bD0fYJFX8sWlbeDm9%252bI8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909025&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909025&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909025&amp;v=MDc5NTlHRnJDVVJMT2VaZVJtRnk3a1VidktMejdCWmJHNEg5ak1wbzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#89" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;2 分类器多样性&lt;/b&gt; "><b>2 分类器多样性</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#95" data-title="&lt;b&gt;2.1 多样性的概念&lt;/b&gt;"><b>2.1 多样性的概念</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;2.2 多样性与集成预测性能&lt;/b&gt;"><b>2.2 多样性与集成预测性能</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="&lt;b&gt;3 相关工作&lt;/b&gt; "><b>3 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;3.1 多样性度量通用方法&lt;/b&gt;"><b>3.1 多样性度量通用方法</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.2 基于信息熵的多样性度量方法&lt;/b&gt;"><b>3.2 基于信息熵的多样性度量方法</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;3.3 当前多样性度量方法的局限性&lt;/b&gt;"><b>3.3 当前多样性度量方法的局限性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="&lt;b&gt;4 基于信息熵的过程多样性度量&lt;/b&gt; "><b>4 基于信息熵的过程多样性度量</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#133" data-title="&lt;b&gt;4.1 C4.5决策树分类器&lt;/b&gt;"><b>4.1 C4.5决策树分类器</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;4.2 基于信息熵的过程多样性度量&lt;/b&gt;"><b>4.2 基于信息熵的过程多样性度量</b></a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;4.3 动态选择集成构建分类树&lt;/b&gt;"><b>4.3 动态选择集成构建分类树</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#168" data-title="&lt;b&gt;5 实验分析&lt;/b&gt; "><b>5 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#170" data-title="&lt;b&gt;5.1 GGR-IDS在实验数据集上的应用&lt;/b&gt;"><b>5.1 GGR-IDS在实验数据集上的应用</b></a></li>
                                                <li><a href="#192" data-title="&lt;b&gt;5.2 GGR-IDS多样性度量性能评估&lt;/b&gt;"><b>5.2 GGR-IDS多样性度量性能评估</b></a></li>
                                                <li><a href="#203" data-title="&lt;b&gt;5.3 GGR-IDS在经典数据集上集成学习效果评估&lt;/b&gt;"><b>5.3 GGR-IDS在经典数据集上集成学习效果评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#210" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#153" data-title="图1 一个典型C4.5决策树">图1 一个典型C4.5决策树</a></li>
                                                <li><a href="#178" data-title="图2 &lt;i&gt;C&lt;/i&gt;1决策树根节点划分">图2 <i>C</i>1决策树根节点划分</a></li>
                                                <li><a href="#200" data-title="&lt;b&gt;表1 多样性度量方法比较&lt;/b&gt;"><b>表1 多样性度量方法比较</b></a></li>
                                                <li><a href="#207" data-title="&lt;b&gt;表2 典型分类数据集的不同算法效能比较&lt;/b&gt;"><b>表2 典型分类数据集的不同算法效能比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="248">


                                    <a id="bibliography_1" title=" David O,Maclin R.Popular ensemble methods:An empirical study[J].Journal of Artificial Intelligence Research,1999,11(1):169-198." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Popular ensemble methods: an empirical study">
                                        <b>[1]</b>
                                         David O,Maclin R.Popular ensemble methods:An empirical study[J].Journal of Artificial Intelligence Research,1999,11(1):169-198.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_2" title=" Dietterich T G.Machine learning research:Four current directions [J].AI Magazine,1997,18(4):97-136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine Learning Research: Four Current Directions">
                                        <b>[2]</b>
                                         Dietterich T G.Machine learning research:Four current directions [J].AI Magazine,1997,18(4):97-136.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_3" title=" Xie Yuan-cheng.Research on classifier integration [D].Nanjing:Nanjing University of Science &amp;amp; Technology,2009.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on classifier integration">
                                        <b>[3]</b>
                                         Xie Yuan-cheng.Research on classifier integration [D].Nanjing:Nanjing University of Science &amp;amp; Technology,2009.(in Chinese)
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_4" title=" Dietterich T G.Ensemble methods in machine learning [C]//Proc of the 1st International Workshop on Multiple Classifier Systems,2000:1-15." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble Methods in Machine Learning">
                                        <b>[4]</b>
                                         Dietterich T G.Ensemble methods in machine learning [C]//Proc of the 1st International Workshop on Multiple Classifier Systems,2000:1-15.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_5" title=" Jing Xiao-yuan,Yang Jing-yu.Combining classifiers based on analysis of correlation and effective supplement [J].Acta Automatica Sinica,2000,26(6):741-747.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining classifiers based on analysis of correlation and effective supplement">
                                        <b>[5]</b>
                                         Jing Xiao-yuan,Yang Jing-yu.Combining classifiers based on analysis of correlation and effective supplement [J].Acta Automatica Sinica,2000,26(6):741-747.(in Chinese)
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_6" title=" Brown G,Wyatt J,Harris R,et al.Diversity creation methods:A survey and categorisation[J].Information Fusion,2005,6(1):5-20." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329294&amp;v=MDAzODRUTW53WmVadUh5am1VTGZJSmw4V2JoYz1OaWZPZmJLN0h0RE5ySTlGWitrR0RuVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Brown G,Wyatt J,Harris R,et al.Diversity creation methods:A survey and categorisation[J].Information Fusion,2005,6(1):5-20.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_7" title=" Breiman L.Bagging predictors [J].Machine Learning,1996,24(2):123-140." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MjA5MzhITnJJeE1ZT01OWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ0RsVmI3S0pGcz1OajdCYXJPNEh0&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Breiman L.Bagging predictors [J].Machine Learning,1996,24(2):123-140.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_8" title=" Qin Li-long,Yu Qi,Wang Zhen-yu.Modified algorithm of neural network ensemble based on generalization theory[J].Computer Simulation,2013,30(11):361-364.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201311082&amp;v=MjIwODk5TE5ybzlOWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1Vidk5MejdCZExHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Qin Li-long,Yu Qi,Wang Zhen-yu.Modified algorithm of neural network ensemble based on generalization theory[J].Computer Simulation,2013,30(11):361-364.(in Chinese)
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_9" title=" Kantardzic M.Data minings:Concepts,models,methods,and algorithms[M].2 Edition.New Jersey:Wiley,2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data minings:Concepts,models,methods,and algorithms">
                                        <b>[9]</b>
                                         Kantardzic M.Data minings:Concepts,models,methods,and algorithms[M].2 Edition.New Jersey:Wiley,2013.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_10" title=" Geman S,Bienenstock E,Doursat R.Neural networks and the bias/variance dilemma[J].Neural Computation,1992,4(1):1-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500013717&amp;v=MDUzNDlNbndaZVp1SHlqbVVMZklKbDhXYmhjPU5pZkpaYks5SHRqTXFvOUZaT29NQzMwK29CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Geman S,Bienenstock E,Doursat R.Neural networks and the bias/variance dilemma[J].Neural Computation,1992,4(1):1-58.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_11" title=" Krogh A,Vedelsby J.Neural network ensembles,cross validation and active learning[C]//Proc of the 7th International Conference on Neural Information Processing Systems(NIPS),1994:231-238." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural network ensembles,cross validation,and active learning">
                                        <b>[11]</b>
                                         Krogh A,Vedelsby J.Neural network ensembles,cross validation and active learning[C]//Proc of the 7th International Conference on Neural Information Processing Systems(NIPS),1994:231-238.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_12" title=" Hastie T,Tibshirani R,Friedman J.The elements of statistical learning-Data minging,inference and prediction [M].2nd Edition.Berlin:Springer,2001." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Elements of Statistical Learning: Data Mining, Inference and Prediction">
                                        <b>[12]</b>
                                         Hastie T,Tibshirani R,Friedman J.The elements of statistical learning-Data minging,inference and prediction [M].2nd Edition.Berlin:Springer,2001.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Zhang Hong-da,Wang Xiao-dan,Han Jun,et al.Survey of diversity researches on classifier ensembles [J].Systems Engineering and Electronics ,2009,31(12):3007-3012.(in Chinese)</a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_14" title=" Cunningham P,Carney J.Diversity versus quality in classification ensembles based on feature selection[C]//Proc of the 11th European Conference on Machine Learning,2000:109-116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diversity versus quality in classification ensembles based on feature selection">
                                        <b>[14]</b>
                                         Cunningham P,Carney J.Diversity versus quality in classification ensembles based on feature selection[C]//Proc of the 11th European Conference on Machine Learning,2000:109-116.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_15" title=" Hu Q H,Yu D R.Diversity measure for multiple classifier systems[C]//Proc of the 2nd International Conference on Fuzzy Systems and Knowledge Discovery,2005:1261-1265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diversity measure for multiple classifier systems">
                                        <b>[15]</b>
                                         Hu Q H,Yu D R.Diversity measure for multiple classifier systems[C]//Proc of the 2nd International Conference on Fuzzy Systems and Knowledge Discovery,2005:1261-1265.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_16" title=" Shipp C A,Kuncheva L I.Relationships between combination methods and measures of diversity in combining classifiers [J].Information Fusion,2002,3(2):135-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329412&amp;v=MTIzNDRyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDhXYmhjPU5pZk9mYks3SHRETnJJOUZaK2tHQ0gwN29CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Shipp C A,Kuncheva L I.Relationships between combination methods and measures of diversity in combining classifiers [J].Information Fusion,2002,3(2):135-148.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                     Sun Bo,Wang Jian-dong,Chen Hai-yan,et al.Diversity measures in ensemble learning [J].Control and Decision ,2014,29(3):385-395.(in Chinese)</a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_18" title=" Liu W Y,Wu Z H,Pan G.An entropy-based diversity measure for classifier combining and its application to face classifier ensemble thinning[C]//Proc of the 5th Chinese Conference on Biometric Person Authentication,2004:118-124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An entropy-based diversity measure for classifier combining and its application to face classifier ensemble thinning">
                                        <b>[18]</b>
                                         Liu W Y,Wu Z H,Pan G.An entropy-based diversity measure for classifier combining and its application to face classifier ensemble thinning[C]//Proc of the 5th Chinese Conference on Biometric Person Authentication,2004:118-124.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_19" title=" Brown G.An information theoretic perspective on multiple classifier systems[C]//Proc of the 8th International Workshop on Multiple Classifier Systems,2009:344-353." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An information theoretic perspective on multiple classifier systems">
                                        <b>[19]</b>
                                         Brown G.An information theoretic perspective on multiple classifier systems[C]//Proc of the 8th International Workshop on Multiple Classifier Systems,2009:344-353.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_20" title=" Zhou Z H,Li N.Multi-information ensemble diversity[C]//Proc of the 9th International Workshop on Multiple Classifier Systems,2010:134-144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Information Ensemble Diversity">
                                        <b>[20]</b>
                                         Zhou Z H,Li N.Multi-information ensemble diversity[C]//Proc of the 9th International Workshop on Multiple Classifier Systems,2010:134-144.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     Xing Hong-jie,Wei Yong-le.Selective ensemble of SVDDs based on correntropy and distance variance[J].Computer Science,2016,43(5):252-256.(in Chinese)</a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_22" title=" Li Li.Optimization method research and application of multiple classifiers ensemble based on diversity measure[D].Dalian:Dalian Maritime University,2017.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimization method research and application of multiple classifiers ensemble based on diversity measure">
                                        <b>[22]</b>
                                         Li Li.Optimization method research and application of multiple classifiers ensemble based on diversity measure[D].Dalian:Dalian Maritime University,2017.(in Chinese)
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                     Zhao Jun-yang,Han Chong-zhao,Han De-qiang,et al.A novel measure method of classifier integrations using complement information entropy[J].Journal of Xi’an Jiaotong University,2016,50(2):13-19.(in Chinese)</a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_24" title=" Quinlan J R.C4.5:Programs for machine learning [M].San Francisco:Morgan Kaufmann Publishers Inc,1993:17-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=C4.5: Programs for Machine Learning">
                                        <b>[24]</b>
                                         Quinlan J R.C4.5:Programs for machine learning [M].San Francisco:Morgan Kaufmann Publishers Inc,1993:17-42.
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_25" title=" Wu X D,Kumar V,Quinlan J R,et al.Top 10 algorithms in data mining[J].Knowledge &amp;amp; Information Systems,2007,14(1):1-37." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001682629&amp;v=MjYyMzQ3S0pGcz1OajdCYXJPNEh0SE5xWWRIWXVrR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZi&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         Wu X D,Kumar V,Quinlan J R,et al.Top 10 algorithms in data mining[J].Knowledge &amp;amp; Information Systems,2007,14(1):1-37.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_26" title=" Liu Rui-yuan.Euclidean distance with weighted and its application [J].Application of Statistics and Management,2002,21 (5):17-19.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SLTJ200205004&amp;v=MTY4ODlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnZOTmlIZlpMRzRIdFBNcW85RllJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         Liu Rui-yuan.Euclidean distance with weighted and its application [J].Application of Statistics and Management,2002,21 (5):17-19.(in Chinese)
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_27" >
                                        <b>[27]</b>
                                     Guan Qing-yun,Chen Xue-long,Wang Yan-zhang.Distance entropy based decision-making information fusion method [J].Systems Engineering-Theory &amp;amp; Practice,2015,35(1):216-227.(in Chinese)</a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_28" title=" Amudha S,B.SC.,M.SC.,M.PHIL.An overview of clustering algorithms in data mining[J].International Research Journal of Engineering and Technology (IRJET),2016,12(3):1359-1403." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An overview of clustering algorithms in data mining">
                                        <b>[28]</b>
                                         Amudha S,B.SC.,M.SC.,M.PHIL.An overview of clustering algorithms in data mining[J].International Research Journal of Engineering and Technology (IRJET),2016,12(3):1359-1403.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_29" title=" Zhou Zhi-hua.Machine learning [M].Beijing:Tsinghua University Press,2016.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning">
                                        <b>[29]</b>
                                         Zhou Zhi-hua.Machine learning [M].Beijing:Tsinghua University Press,2016.(in Chinese)
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_30" title=" Kuncheva L I,Whitaker C J.Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy[J].Machine Learning,2003,51(2):181-207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340615&amp;v=MTIyNTFoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZiN0tKRnM9Tmo3QmFyTzRIdEhOckl0Rll1b0tZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                         Kuncheva L I,Whitaker C J.Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy[J].Machine Learning,2003,51(2):181-207.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_31" >
                                        <b>[31]</b>
                                     Cai Tie,Wu Xing,Li Ye.Research on construction of base classifier based on discretization method for ensemble learning [J].Journal of Computer Applications,2008,28(8):2091-2093.(in Chinese)附中文参考文献:</a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_3" title=" 谢元澄.分类器集成研究[D].南京:南京理工大学,2009." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2010096743.nh&amp;v=MTMxODZPZVplUm1GeTdrVWJ2TlYxMjZIck94R05iSXJKRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         谢元澄.分类器集成研究[D].南京:南京理工大学,2009.
                                    </a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_5" title=" 荆晓远,杨静宇.基于相关性和有效互补性分析的多分类器组合方法[J].自动化学报,2000,26(6):741-747." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200006003&amp;v=MDU0NjJMZlliRzRIdEhNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnZOS0M=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         荆晓远,杨静宇.基于相关性和有效互补性分析的多分类器组合方法[J].自动化学报,2000,26(6):741-747.
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     秦立龙,余奇,王振宇.基于泛化理论的集成神经网络优化算法[J].计算机仿真,2013,30(11):361-364.</a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_13" title=" 张宏达,王晓丹,韩钧,等.分类器集成差异性研究[J].系统工程与电子技术,2009,31(12):3007-3012." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD200912048&amp;v=Mjc1MDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWJ2TlBUblNhckc0SHRqTnJZOUJiSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         张宏达,王晓丹,韩钧,等.分类器集成差异性研究[J].系统工程与电子技术,2009,31(12):3007-3012.
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_17" title=" 孙博,王建东,陈海燕,等.集成学习中的多样性度量[J].控制与决策,2014,29(3):385-395." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201403001&amp;v=Mjg3OTlOTGpmU2JiRzRIOVhNckk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         孙博,王建东,陈海燕,等.集成学习中的多样性度量[J].控制与决策,2014,29(3):385-395.
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_21" title=" 邢红杰,魏勇乐.基于相关熵和距离方差的支持向量数据描述选择性集成[J].计算机科学,2016,43(5):252-256." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201605049&amp;v=MDczMjRiN0c0SDlmTXFvOUJiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWJ2Tkx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         邢红杰,魏勇乐.基于相关熵和距离方差的支持向量数据描述选择性集成[J].计算机科学,2016,43(5):252-256.
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_22" title=" 李莉.基于差异性度量的分类器集成优化方法研究与应用[D].大连:大连海事大学,2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017054461.nh&amp;v=MTI4NTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnZOVkYyNkdiTzlHdFhLcnBFYlA=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         李莉.基于差异性度量的分类器集成优化方法研究与应用[D].大连:大连海事大学,2017.
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_23" title=" 赵军阳,韩崇昭,韩德强,等.采用互补信息熵的分类器集成差异性度量方法[J].西安交通大学学报,2016,50(2):13-19." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAJT201602003&amp;v=MDAxMjJ6QmVyRzRIOWZNclk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnZOUFM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         赵军阳,韩崇昭,韩德强,等.采用互补信息熵的分类器集成差异性度量方法[J].西安交通大学学报,2016,50(2):13-19.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                     刘瑞元.加权欧氏距离及其应用[J].数理统计与管理,2002,21(5):17-19.</a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_27" title=" 管清云,陈雪龙,王延章.基于距离熵的应急决策层信息融合方法[J].系统工程理论与实践,2015,35(1):216-227." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTLL201501024&amp;v=MDQ1MDhIOVRNcm85SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnZOUFRuSFlyRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         管清云,陈雪龙,王延章.基于距离熵的应急决策层信息融合方法[J].系统工程理论与实践,2015,35(1):216-227.
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_29" title=" 周志华.机器学习[M].北京:清华大学出版社,2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MTIwMjFDdnRVN25KSWxvUlhGcXpHYkM0SE5YT3JJMU5ZK3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVG&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                         周志华.机器学习[M].北京:清华大学出版社,2016.
                                    </a>
                                </li>
                                <li id="332">


                                    <a id="bibliography_31" title=" 蔡铁,伍星,李烨.集成学习中基于离散化方法的基分类器构造研究[J].计算机应用,2008,28(8):2091-2093." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200808056&amp;v=MjIzNzFtRnk3a1Vidk5MejdCZDdHNEh0bk1wNDlBWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[31]</b>
                                         蔡铁,伍星,李烨.集成学习中基于离散化方法的基分类器构造研究[J].计算机应用,2008,28(8):2091-2093.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1700-1707 DOI:10.3969/j.issn.1007-130X.2019.09.024            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于信息熵的集成学习过程多样性度量研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E9%92%A2&amp;code=26191247&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周钢</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E7%A6%8F%E4%BA%AE&amp;code=20277237&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭福亮</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6&amp;code=0069518&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军工程大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基分类器的多样性是提升集成学习的精度和泛化能力的重要因素,大数据环境下的传统后验证多样性度量方法计算效率较低,提出一种基于信息熵的过程多样性度量方法。通过使用分类器各属性的增益及其所在树层次得到属性集的联合增益,并计算分类器间的熵距离评估其多样性,利用熵距离按照K-means方法即可动态购置集成学习分类器。在西瓜数据集和典型分类数据集上进行比较研究,发现与传统集成学习方法相比,该方法具有相近的准确性和更高的计算效率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%87%E7%A8%8B%E5%A4%9A%E6%A0%B7%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">过程多样性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%81%94%E5%90%88%E5%A2%9E%E7%9B%8A&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">联合增益;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-means&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-means;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%B7%E6%80%A7%E5%BA%A6%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多样性度量;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    周钢（1984-），男，湖北武汉人，博士生，讲师，研究方向为数据挖掘。E-mail:18672923066@163.com,通信地址：430033湖北省武汉市硚口区解放大道717号海军工程大学149信箱&lt;image id="245" type="formula" href="images/JSJK201909025_24500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    郭福亮（1963-），男，河北沧州人，博士，教授，研究方向为数据挖掘。E-mail:313919864@163.com,通信地址：430033湖北省武汉市硚口区解放大道717号海军工程大学149信箱&lt;image id="247" type="formula" href="images/JSJK201909025_24700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-29</p>

            </div>
                    <h1><b>Process diversity measurement of ensemble learning based on information entropy</b></h1>
                    <h2>
                    <span>ZHOU Gang</span>
                    <span>GUO Fu-liang</span>
            </h2>
                    <h2>
                    <span>Naval University of Engineering</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The diversity of base classifiers is an important factor to improve the accuracy and generalization ability of ensemble learning. The traditional post verification diversity measurement method is inefficient in big data environment. We therefore propose a process diversity measurement method based on information entropy. The method uses the gain of each attribute of the base classifier and its tree level to obtain the joint gain of the attribute set, and calculates the entropy distance between the classifiers to evaluate their diversity. It dynamically integrates the learning classifier with the entropy distance in accordance with the K-means method. Compared with traditional methods on watermelon dataset and other classification datasets, it is found that the proposed method has similar accuracy and higher computational efficiency.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=process%20diversity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">process diversity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=joint%20gain&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">joint gain;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-means&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-means;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=diversity%20measurement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">diversity measurement;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHOU Gang,born in 1984,PhD candidate,lecturer,his research interest includes data mining.Address:Mail Box 149,Naval University of Engineering,717Jiefang Avenue,Qiaokou District,Wuhan 430033,Hubei,P.R.China;
                                </span>
                                <span>
                                    GUO Fu-liang,born in 1963,PhD,professor,his research interest includes data mining.Address:Mail Box 149,Naval University of Engineering,717Jiefang Avenue,Qiaokou District,Wuhan 430033,Hubei,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-29</p>
                            </div>


        <!--brief start-->
                        <h3 id="89" name="89" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="90">集成学习(Ensemble Learning)是一种重要的数据挖掘方法,主要是利用多个学习器的集成来解决问题,从而可显著提高学习系统的预测精度和泛化能力<citation id="334" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,因而被认为是机器学习未来发展的主要方向之一<citation id="335" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。以数据挖掘中的分类预测为例,集成学习模型的构建一般分为生成基分类器、分类结果整合和测试数据验证3个步骤<citation id="336" type="reference"><link href="252" rel="bibliography" /><link href="310" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="91">对于生成的基分类器,如果分类器相同或差异不大,那么对于相同数据的分类效果相似,集成后整体分类效果不佳。相反,通过多样性度量选取差异性大的分类器进行集成,分类器多样性越大进行整合后的集成分类效果越好,同时删去相关度大的分类器,减少了分类器数量,从而控制了集成学习模型复杂度,其泛化能力也会增强<citation id="337" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="92">围绕分类器多样性的主题,在分析分类器多样性的概念、作用的基础上,以决策树分类方法为例,从信息熵角度提出了一种过程性多样性度量方法,并以此为据动态构建选择性集成模型,通过实例验证发现新方法在分类集成方面具有相当的预测精度和更高的执行效率。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>2 分类器多样性</b></h3>
                <div class="p1">
                    <p id="94">在分析分类器多样性概念的基础上,研究分类器多样性对集成精度和泛化能力的作用。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>2.1 多样性的概念</b></h4>
                <div class="p1">
                    <p id="96">分类器的多样性是融合了分类器的差异性、独立性和互补性的泛化概念<citation id="338" type="reference"><link href="256" rel="bibliography" /><link href="312" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">5</a>]</sup></citation>,其中差异性主要体现在分类器的相互区别,独立性主要体现在分类器间的相互不关联程度,互补性则体现在集成分类器对全集的覆盖程度。</p>
                </div>
                <div class="p1">
                    <p id="97">分类器的差异性主要可以从数据、参数和算法3个方面实现差异分类器的构建<citation id="339" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,具体包括数据样本的差异性、模型参数的差异性和分类算法的差异性,其中,数据样本的差异性是指对同质分类器抽取训练集中的不同数据样本来构建基分类器,从而形成差异化的分类器。如经典集成方法Bagging通过引导程序使用一个训练集的多个版本,即放回抽样,对每一个数据集都训练一个不同的模型,再对训练模型通过整合输出形成一个最终的预测结果<citation id="340" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,这也是本文研究的重点。</p>
                </div>
                <div class="p1">
                    <p id="98">分类器的独立性用于描述对于同一测试样本数据一组(两个或多个)分类器的不关联程度,一般用分类错误概率的相关程度来描述。假设对于数据样本<i>D</i>,存在2个分类器<i>A</i>和<i>B</i>,那么在<i>D</i>上<i>A</i>分类错误概率为<i>P</i>(<i>A</i>),<i>B</i>分类错误概率为<i>P</i>(<i>B</i>),如果<i>A</i>和<i>B</i>同时分类错误概率<i>P</i>(<i>AB</i>)=0,则认为分类器<i>A</i>、<i>B</i>相互独立。</p>
                </div>
                <div class="p1">
                    <p id="99">分类器的互补性是指在全体测试数据集上任一样本数据均存在一个分类器能够正确分类。互补性体现在2个方面,一是能够涵盖整个测试数据样本集,二是对任一样本均能正确分类。</p>
                </div>
                <div class="p1">
                    <p id="100">分类器的差异性源自于集成学习中分类器构建中数据样本、参数设定和分类算法的不同,而独立性和互补性则是后验性能,是在集成学习的测试数据验证阶段计算得到的,虽然可以在训练数据集中通过交叉验证进行测算,但集成分类的泛化能力及过拟化问题不易控制<citation id="341" type="reference"><link href="262" rel="bibliography" /><link href="314" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">8</a>]</sup></citation>。因此,分类器差异性度量对构建集成优化具有指导意义,而分类器独立性和互补性度量则对集成预测性能具有验证作用。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>2.2 多样性与集成预测性能</b></h4>
                <div class="p1">
                    <p id="102">一个集成模型的预测性能与模型复杂度、数据量、测试数据集泛化误差等均有关联,一般预测性能使用预测误差率及其衍生的精度、<i>F</i>度量等进行衡量<citation id="342" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,这个误差率包含2个方面:一是在训练数据集上的训练误差,二是在测试数据集上的泛化误差。</p>
                </div>
                <div class="p1">
                    <p id="103">在不考虑集成预测具体业务背景的基础上,集成分类的预测性能主要从预测误差率和泛化能力2个方面衡量,可从Bias-Variance分解(偏差-方差分解) <citation id="343" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和Error-Ambiguity分解(误差-分歧分解)<citation id="344" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>角度进行分析。</p>
                </div>
                <div class="p1">
                    <p id="104">假设个体分类器<i>h</i><sub>1</sub>,<i>h</i><sub>2</sub>,…,<i>h</i><sub><i>n</i></sub>通过集成形成新的分类器<i>H</i>,对于某一数据样本<i>x</i><sub>0</sub>,集成分类器<i>H</i>在该处的集成预测值为<i>H</i>(<i>x</i><sub>0</sub>),那么集成损失函数(平方误差)的期望按Bias-Variance分解为:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mi>B</mi><mi>i</mi><mi>a</mi><mi>s</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>Η</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mi>Η</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo stretchy="false">)</mo><mi>C</mi><mi>o</mi><mi>v</mi><mo stretchy="false">(</mo><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>s</mi><msup><mrow></mrow><mn>2</mn></msup></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中,<i>Bias</i><sup>2</sup>(<i>H</i>(<i>x</i><sub>0</sub>))为集成平方偏差,表征各个体分类器预测均值与实际值的偏离情况;<i>Var</i>(<i>H</i>(<i>x</i><sub>0</sub>))为集成平方方差,表征各个体分类器预测与实际值的偏离波动情况;<i>Cov</i>(<i>h</i><sub><i>i</i></sub>(<i>x</i><sub>0</sub>))为个体分类器相关性,表征各个体分类器间的相关程度,当<i>Cov</i>(<i>h</i><sub><i>i</i></sub>(<i>x</i><sub>0</sub>))=0时,认为个体分类器相互独立;<i>s</i>为高阶差值变量。对于一个集成分类器,偏差和方差发挥相反作用,当偏差减小时,方差往往会增大,反之亦然,因此,一般用集成模型复杂度控制偏差和方差的折衷,获取较小的集成误差<citation id="345" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。<i>Cov</i>(<i>h</i><sub><i>i</i></sub>(<i>x</i><sub>0</sub>))越小,即分类器多样性越大则集成误差越小。</p>
                </div>
                <div class="p1">
                    <p id="107">引入<i>EH</i>(<i>x</i>)为加权集成分类器,可取权重<i>w</i><sub><i>i</i></sub>为1/<i>n</i>,即各基分类器权重相同,那么集成误差期望<i>E</i>按照Error-Ambiguity分解为:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo>=</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mo stretchy="false">(</mo></mrow></mstyle><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>-</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext>d</mtext><mi>x</mi><mo>-</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mo stretchy="false">(</mo></mrow></mstyle><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>-</mo><mi>E</mi><mi>Η</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext>d</mtext><mi>x</mi><mo>=</mo></mtd></mtr><mtr><mtd><mi>E</mi><mi>r</mi><mi>r</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>-</mo><mi>A</mi><mi>m</mi><mi>b</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">其中,<i>Err</i>(<i>h</i>(<i>x</i>))为个体分类器泛化误差的加权均值;<i>Amb</i>(<i>h</i>(<i>x</i>))为个体分类器的加权分歧值,表征个体分类器对于样本<i>x</i>上的分类不一致性。那么,为了减少集成泛化误差<i>E</i>,应降低个体分类器误差,增加个体分类器间的分类不一致性。</p>
                </div>
                <div class="p1">
                    <p id="110">因此,从Bias-Variance分解和Error-Ambiguity分解2个角度可以发现,增加分类器多样性能够提高集成预测精度,增强集成泛化能力。由此可见,分类器多样性是集成预测性能的重要影响因素之一。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag"><b>3 相关工作</b></h3>
                <div class="p1">
                    <p id="112">运用分类器之间的多样性构建集成学习模型,主要包括多样性度量、多样性组合和多样性构建3个方面的工作,其中多样性度量是用于度量集成分类器中个体分类器的多样性,即估算个体学习器的多样化程度。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>3.1 多样性度量通用方法</b></h4>
                <div class="p1">
                    <p id="114">根据度量考查对象,多样性度量通用方法可以分为局部多样性度量和全局多样性度量,局部多样性度量主要是对个体分类器两两之间的成对度量,全局多样性度量主要对所有个体分类器在数据样本分类上展示出的多样性度量;根据度量的侧重方向,可以分为差异度量、相似度量和独立度量;根据度量的依据来源,可以分为统计学度量、信息学度量和软件工程学度量等<citation id="346" type="reference"><link href="272" rel="bibliography" /><link href="316" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="115">目前多样性度量方法主要是依据对样本数据集预测结果进行统计分析,局部多样性度量主要包括成对度量(即不合度量、相关系数、Q-统计量、k-统计量等)和非成对度量(即ch度量、M度量等),全局多样性度量包括k-w度量、熵度量、难度度量、广义差异性度量GD和一致错误差异度CFD等。</p>
                </div>
                <div class="p1">
                    <p id="116">上述的多样性度量主要是验证性度量,通过个体分类器对测试数据集或训练数据集上的验证样本的分类预测结果进行计算度量,根据度量结果计算多样性。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.2 基于信息熵的多样性度量方法</b></h4>
                <div class="p1">
                    <p id="118">信息熵用于描述信息的不确定性,度量某特定信息的出现概率或离散随机事件的发生概率<citation id="347" type="reference"><link href="274" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。采用信息熵对集成学习分类器多样性进行度量是多样性评价的重要手段<citation id="348" type="reference"><link href="276" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="119">文献<citation id="349" type="reference">[<a class="sup">16</a>]</citation>介绍了一种运用信息熵对<i>L</i>个分类器进行集成时的多样性度量:</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mrow><mo>(</mo><mrow><mi>L</mi><mo>-</mo><mi>L</mi><mo>/</mo><mn>2</mn></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></mstyle><mi>min</mi><mrow><mo>{</mo><mrow><mi>l</mi><mrow><mo>(</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>L</mi><mo>-</mo><mi>l</mi><mrow><mo>(</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">对于任意样本<i>z</i><sub><i>j</i></sub>,<i>l</i>(<i>z</i><sub><i>j</i></sub>)为分类正确的分类器数目,那么如果分类器给出的分类结果相同则熵的多样性度量为0;当[<i>L</i>/2]个分类器分类正确时,熵度量为1,此时基分类器的多样性最高。</p>
                </div>
                <div class="p1">
                    <p id="122">在此基础上,还可以分类输出矩阵<i><b>U</b></i>为研究对象,采用互补信息熵建立分类器集成多样性度量<citation id="350" type="reference"><link href="280" rel="bibliography" /><link href="318" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">17</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo>=</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mrow><mo>[</mo><mrow><mi>Ο</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>]</mo></mrow><msub><mrow></mrow><mi>R</mi></msub></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">U</mi><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mrow><mo>[</mo><mrow><mi>Ο</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>]</mo></mrow><msub><mrow></mrow><mi>R</mi></msub></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">U</mi><mo>|</mo></mrow></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">其中,|[<i>O</i><sub><i>i</i></sub>]<sub><i>R</i></sub>|表示第<i>i</i>个分类器输出的预测值在样本空间上的模糊关系<i>R</i>下的势。</p>
                </div>
                <div class="p1">
                    <p id="125">除此之外,亦可从其他角度运用熵对集成学习的多样性进行描述,可从2个分类器的分类结果按照Q-统计的基本原理构建多样性的熵度量<citation id="351" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>;也可从信息论角度分析分类器间的互信息与集成误差间的关系,进而使用集成互信息扩展到集成多样性进行分析研究<citation id="352" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,并在此基础上分析集成中的多阶相关性,给出简化的多样性描述方法<citation id="353" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="126">近年来,国内关于信息熵的多样性描述研究,重点在于将信息熵作为多样性度量并运用于集成方法,提出基于信息熵的多种多样性描述形式。例如文献<citation id="354" type="reference">[<a class="sup">21</a>,<a class="sup">21</a>]</citation>结合相关熵和距离方差描述支持向量数据,来实现选择性集成;文献<citation id="355" type="reference">[<a class="sup">22</a>,<a class="sup">22</a>]</citation>提出了一种距离信息熵的多样性描述方法,实现了基分类器集成;文献<citation id="356" type="reference">[<a class="sup">23</a>,<a class="sup">23</a>]</citation>提出了一种互补信息熵概念,用于描述基分类器多样性并用于集成。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>3.3 当前多样性度量方法的局限性</b></h4>
                <div class="p1">
                    <p id="128">当前多样性度量方法一般都是采用验证性度量,即对分类结果进行分析计算其多样性。根据多样性度量结果科学选取合适的基分类器来整合完成集成学习模型。</p>
                </div>
                <div class="p1">
                    <p id="129">从集成算法效率角度来看,以Bagging集成学习算法为例,在大数据背景下,当训练样本数据集极大时,构建全部基分类器并进行分类预测后再进行多样性度量,选取部分多样性大的分类器结果进行整合。可以发现集成学习模型的主要工作集中在基分类器的构建和分类算法的执行上,通过多样性度量,相当一部分基分类器结果将不参与整合,从而造成计算机资源浪费。</p>
                </div>
                <div class="p1">
                    <p id="130">因此,为了提高集成学习模型的工作效率,考虑一种在分类器构建过程中即能发现分类器相关性,并及时停止相关度大的分类器参与构建的多样性度量方法。分类器构建过程中能够预判其多样性的过程性度量方法,区别于传统的验证型多样性度量方法,同时兼顾集成预测精度动态地选择部分个体分类器进行集成,对提高集成学习的效率,提升其预测精度和泛化能力具有重要意义。</p>
                </div>
                <h3 id="131" name="131" class="anchor-tag"><b>4 基于信息熵的过程多样性度量</b></h3>
                <div class="p1">
                    <p id="132">使用C4.5决策树作为基分类器的Bagging集成学习算法采用信息熵进行过程多样性度量,及时发现相关性强的基分类器并中断其决策树的继续生成,从而提高集成学习效率。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>4.1 C4.5决策树分类器</b></h4>
                <div class="p1">
                    <p id="134">1993年,Quinlan<citation id="357" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>在继承决策树基本构建思想基础上提出C4.5算法,该算法用信息增益率代替信息熵作为选择分裂属性的标准,对连续数值进行离散化,并递归生成一个判定树。</p>
                </div>
                <div class="p1">
                    <p id="135">在决策树的每个节点使用信息增益率的度量选择测试属性,选择具有最高信息增益的属性作为当前节点的划分属性。假设<i>S</i>是<i>s</i>个数据样本的集合,类标号属性具有<i>m</i>个不同值(或连续属性分割为<i>m</i>个数值区间),定义<i>m</i>个不同的类<i>C</i><sub><i>i</i></sub>(<i>i</i>为正整数),<i>s</i><sub><i>i</i></sub>是类<i>C</i><sub><i>i</i></sub>中的样本数。那么给定样本的期望信息为:</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>*</mo><mi>log</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">其中,<i>p</i><sub><i>i</i></sub>为任意样本属于<i>C</i><sub><i>i</i></sub>的概率,假设<i>p</i><sub><i>i</i></sub>估计为<i>s</i><sub><i>i</i></sub>/<i>s</i>。</p>
                </div>
                <div class="p1">
                    <p id="138">对于分类属性<i>A</i>,假设具有<i>v</i>个不同的值(<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>,…,<i>a</i><sub><i>v</i></sub>),那么数据样本总集合<i>S</i>可以划分为<i>v</i>个子集(<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>,…,<i>S</i><sub><i>v</i></sub>),其中<i>S</i><sub><i>j</i></sub>是<i>S</i>中具有<i>a</i><sub><i>j</i></sub>的样本,<i>s</i><sub><i>ij</i></sub>是子集<i>S</i><sub><i>j</i></sub>中<i>C</i><sub><i>i</i></sub>的样本数。那么,在属性<i>A</i>上划分子集的熵为:</p>
                </div>
                <div class="p1">
                    <p id="139" class="code-formula">
                        <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>v</mi></munderover><mo stretchy="false">(</mo></mstyle><mrow><mo>|</mo><mrow><mi>S</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>S</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow><mo>+</mo><mo>⋯</mo><mo>+</mo><mrow><mo>|</mo><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>j</mi></mrow></msub></mrow><mo>|</mo></mrow><mo stretchy="false">)</mo><mo>*</mo></mtd></mtr><mtr><mtd><mi>E</mi><mo stretchy="false">(</mo><mi>S</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msub><mo>,</mo><mi>S</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo><mo>/</mo><mrow><mo>|</mo><mi>S</mi><mo>|</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="140">那么整个分类目标集合<i>C</i>,针对属性<i>A</i>划分的信息增益为:</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><mo stretchy="false">(</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>E</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142">属性<i>A</i>在整个分类目标集合<i>C</i>的信息增益率为:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>/</mo><mi>E</mi><mo stretchy="false">(</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>4.2 基于信息熵的过程多样性度量</b></h4>
                <div class="p1">
                    <p id="145">C4.5分类决策树以信息增益率<i>GainRatio</i>(<i>C</i>,<i>A</i>)作为分类标准,其中<i>Gain</i>(<i>C</i>,<i>A</i>)是属性<i>A</i>的信息增益,与<i>A</i>的类别密切相关,<i>E</i>(<i>C</i><sub>1</sub>,<i>C</i><sub>2</sub>,…,<i>C</i><sub><i>m</i></sub>)是属性<i>A</i>的取值分布概率,与类别无关<citation id="358" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="146">由于C4.5算法是典型的敏感型分类决策树,对于样本的选取不同导致决策树结构上的差异,信息增益率越大的属性<i>A</i>更倾向于用作更高层的分类节点,对属性<i>A</i>上的取值进行划分,<i>GainRatio</i>(<i>C</i>,<i>A</i>)值体现了在分类目标集合<i>C</i>上属性<i>A</i>对数据的分类程度。根据不同样本数据按照敏感的C4.5算法形成的不同决策树依据各属性信息增益率构建决策树各节点,越是顶层的节点对属性及其划分效果越明显,对于根据相同C4.5算法形成的不同决策树,如果上层节点及其划分相同,而底层节点及叶子节点不同,那么对训练集的最终划分必定是相似的,也就是具有强相关性。因此,通过各属性节点的信息增益及其所在决策树层次来分析2个决策树的相关度具有可行性,在决策树没有构建完全时,即在决策树构建过程中即可得到2个决策树的相关程度估计值,从而及时终止强相关度(弱多样性)的决策树的继续构建,避免集成学习的分类树构建消耗更多的计算资源。</p>
                </div>
                <div class="p1">
                    <p id="147">依据信息熵设计决策树多样性的基本思路如下:</p>
                </div>
                <div class="p1">
                    <p id="148">(1)假设根据训练数据集构建<i>M</i>层决策树,在构建到第<i>N</i>层(<i>N</i>&lt;<i>M</i>)时,对第1层到第<i>N</i>层节点计算各节点的信息增益率。</p>
                </div>
                <div class="p1">
                    <p id="149">(2)假设第<i>i</i>层某节点<i>A</i><sub><i>i</i></sub>对应的子树为<i>T</i><sub><i>i</i></sub>,节点<i>A</i><sub><i>i</i></sub>的信息增益为<i>GR</i>(<i>T</i><sub><i>i</i></sub>,<i>A</i><sub><i>i</i></sub>),<i>A</i><sub><i>i</i></sub>对应的取值划分区间集合为(<i>S</i><sub><i>i</i></sub><sub>1</sub>,<i>S</i><sub><i>i</i></sub><sub>2</sub>,…,<i>S</i><sub><i>im</i></sub>),子树第<i>j</i>个分支取值概率为<i>p</i><sub><i>ij</i></sub>,对应的属性为<i>A</i><sub><i>ij</i></sub>,该属性的信息增益为<i>GR</i>(<i>T</i><sub><i>ij</i></sub>,<i>A</i><sub><i>ij</i></sub>)。</p>
                </div>
                <div class="p1">
                    <p id="150">(3)对于属性集合<i>AT</i>中任意一个属性<i>A</i><sub><i>k</i></sub>(<i>k</i>∈{1,2,…,<i>n</i>}),由于C4.5决策树的特点,从根节点到叶子节点同一属性不会出现2次,因此根据已经构建完成的<i>N</i>层决策树,可以得到从根节点到各子树的属性<i>A</i><sub><i>k</i></sub>的联合信息增益。假设属性出现了<i>x</i>次,联合信息增益为<mathml id="212"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>x</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mn>1</mn><mi>i</mi><mspace width="0.25em" /></mrow></msub><mi>p</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>i</mi><mspace width="0.25em" /></mrow></msub><mo>⋯</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>i</mi><mspace width="0.25em" /></mrow></msub><mspace width="0.25em" /><mo>⋅</mo><mspace width="0.25em" /><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="151">一个典型的C4.5算法决策树,如图1所示,对于属性<i>A</i><sub>11</sub>的联合信息增益率为:</p>
                </div>
                <div class="p1">
                    <p id="152" class="code-formula">
                        <mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mi>p</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>1</mn></mrow></msub><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mrow><mn>0</mn><mn>1</mn></mrow></msub><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mi>p</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>0</mn></mrow></msub><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>0</mn></mrow></msub><mo>,</mo><mi>A</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909025_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 一个典型C4.5决策树" src="Detail/GetImg?filename=images/JSJK201909025_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 一个典型C4.5决策树  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909025_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 A typical C4.5 decision tree</p>

                </div>
                <div class="p1">
                    <p id="154">(4)按照步骤(3)得到决策树<i>T</i><sub><i>i</i></sub>的所有属性集合<i>AT</i>的联合增益率集合<i>GGR</i>(<i>T</i><sub><i>i</i></sub>,<i>AT</i>),那么可以根据放回样本得到所有构建的决策树属性联合增益率集合。</p>
                </div>
                <div class="p1">
                    <p id="155">(5)计算任意决策树<i>T</i><sub><i>i</i></sub>与<i>T</i><sub><i>j</i></sub>的成对多样性,利用它们的联合增益集合<i>GGR</i>(<i>T</i><sub><i>i</i></sub>,<i>AT</i>)与<i>GGR</i>(<i>T</i><sub><i>j</i></sub>,<i>AT</i>)的熵距离作为多样性的度量。</p>
                </div>
                <div class="p1">
                    <p id="156">计算属性集合<i>AT</i>中的某一属性<i>A</i><sub><i>k</i></sub>在决策树<i>T</i><sub><i>i</i></sub>和<i>T</i><sub><i>j</i></sub>上的欧氏距离<citation id="359" type="reference"><link href="298" rel="bibliography" /><link href="326" rel="bibliography" /><sup>[<a class="sup">26</a>,<a class="sup">26</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="157" class="code-formula">
                        <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><msqrt><mrow><mo stretchy="false">(</mo><mi>G</mi><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>G</mi><mi>G</mi><mi>R</mi><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>A</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="158">决策树<i>T</i><sub><i>i</i></sub>与<i>T</i><sub><i>j</i></sub>的熵距离<i>H</i><mathml id="213"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false">[</mo><mn>2</mn><mn>7</mn><mo stretchy="false">]</mo></mrow></msubsup></mrow></math></mathml>可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="159" class="code-formula">
                        <mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub></mrow></mfrac></mrow></mstyle><mo>⋅</mo><mi>ln</mi><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>d</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub></mrow></mfrac><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>n</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="160">根据信息熵的特点,熵距离<i>H</i><sub><i>ij</i></sub>越大,表明决策树<i>T</i><sub><i>i</i></sub>与<i>T</i><sub><i>j</i></sub>的多样性越小。</p>
                </div>
                <div class="p1">
                    <p id="161">(6)计算全局多样性,计算某一决策树<i>T</i><sub><i>i</i></sub>与已经选取的决策树集合<i>T</i>的熵距离,对于决策树集合<i>T</i>计算在属性集合<i>AT</i>上所有属性的联合增益集合均值集合:</p>
                </div>
                <div class="p1">
                    <p id="162" class="code-formula">
                        <mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>G</mi><mi>G</mi><mi>R</mi><mrow><mo>(</mo><mrow><mi>Τ</mi><mo>,</mo><mi>A</mi><mi>Τ</mi></mrow><mo>)</mo></mrow><mo>=</mo><mi>a</mi><mi>v</mi><mi>g</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Τ</mi></mrow></munder><mi>G</mi></mstyle><mi>G</mi><mi>R</mi><mrow><mo>(</mo><mrow><mi>Τ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>A</mi><mi>Τ</mi></mrow><mo>)</mo></mrow><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="163"><i>T</i><sub><i>i</i></sub>在属性集合<i>AT</i>上的联合增益集合为<i>GGR</i>(<i>T</i><sub><i>i</i></sub>,<i>AT</i>),根据步骤(5)可以计算决策树<i>T</i><sub><i>i</i></sub>与决策树组<i>T</i>的熵距离。</p>
                </div>
                <div class="p1">
                    <p id="164">从统计学角度分析运用熵及增益作为划分属性节点标准,增益值体现了数据集在该属性上的区分程度。如果2个分类器在同一属性上具有相同或类似的区分程度,即认为它们之间具有更高的相关性。通过各属性的联合增益体现的是该属性在决策树不同层对整体决策树的影响,即越下层的决策树节点对数据集中数据划分影响越小,因此联合增益考虑了数据集上的划分及层次,从统计学上讲具有一定科学性和合理性。</p>
                </div>
                <h4 class="anchor-tag" id="165" name="165"><b>4.3 动态选择集成构建分类树</b></h4>
                <div class="p1">
                    <p id="166">根据4.2节的方法,通过联合增益率得到2棵决策树的成对多样性,单一决策树和已形成决策树组之间的多样性,根据决策树的属性集合<i>AT</i>可以得到<i>n</i>维空间的向量点。利用K-means聚类算法<citation id="360" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>来计算聚簇的均值以及所有离散点,以聚簇的向量点数目作为聚簇均值点的权重,那么可以根据离散点和聚簇均值点代表的决策树构建集成分类模型。其中,权重可以作为集成学习的基分类器结果集成阶段的结果权重。</p>
                </div>
                <div class="p1">
                    <p id="167">随着决策树构建层数进一步增加,使用已经选取的决策树组构建多维空间向量点,继续使用K-means聚类算法优化选取决策树组,进一步减少参与集成学习模型构建的决策树,节约计算资源,提升学习效率。</p>
                </div>
                <h3 id="168" name="168" class="anchor-tag"><b>5 实验分析</b></h3>
                <div class="p1">
                    <p id="169">使用本文介绍的联合信息增益的过程多样性估算方法GGR-IDS(Gather Gam Rate-Intermediate Diversity Statistics)在经典的分类数据集上估算基分类器的多样性,并同现有典型的后验证多样性计算方法比较,验证新方法多样性评估的正确性。最后,在通用机器学习数据集上,使用GGR-IDS方法进行Bagging集成,并同典型的Bagging集成方法比较集成学习精度和效率。</p>
                </div>
                <h4 class="anchor-tag" id="170" name="170"><b>5.1 GGR-IDS在实验数据集上的应用</b></h4>
                <div class="p1">
                    <p id="171">实验采用的数据集为西瓜数据集2.0<citation id="361" type="reference"><link href="304" rel="bibliography" /><link href="330" rel="bibliography" /><sup>[<a class="sup">29</a>,<a class="sup">29</a>]</sup></citation>,该数据集包含6个标称属性:色泽、根蒂、敲声、纹理、脐部、触感,用于进行二分类,判决目标属性是否为好瓜。该数据集包含17个数据样本,选择其中集合编号为{1,2,3,6,7,10,14,15,16,17}的10个样本为训练集,其余7个为测试集,在10个训练样本集中任意选取其中6个样本作为基础数据,采用C4.5算法构建基分类器,运用GGR-IDS估算各基分类器的多样性并动态构建集成分类器。</p>
                </div>
                <div class="p1">
                    <p id="172">选取训练集中的6个样本,采用C4.5分类算法构建3个基分类器,3个基分类器选用样本集分别为<i>C</i>1={3,6,7,15,16,17},<i>C</i>2={1,2,3,10,14,15},<i>C</i>3={3,6,7,10,14,15}。</p>
                </div>
                <div class="p1">
                    <p id="173">对于基分类器<i>C</i>1,<i>C</i>2,<i>C</i>3采用GGR-IDS计算2层决策树所包含的节点属性的联合增益GGR,然后计算其分类器<i>C</i>1,<i>C</i>2,<i>C</i>3两两之间的相互多样性。</p>
                </div>
                <div class="p1">
                    <p id="174">以基分类器<i>C</i>1为例,计算联合增益GGR的方法为:</p>
                </div>
                <div class="p1">
                    <p id="175">对于数据样本<i>C</i>1={3,6,7,15,16,17},信息熵<i>Ent</i>(<i>C</i>1)= 0.6931,各属性分支节点信息增益为:</p>
                </div>
                <div class="p1">
                    <p id="176"><i>Gain</i>(<i>C</i>1,色泽)= 0.1438,<i>Gain</i>(<i>C</i>1,根蒂)= 0.0565,<i>Gain</i>(<i>C</i>1,敲声)= 0.1322,<i>Gain</i>(<i>C</i>1,纹理)= 0.1438,<i>Gain</i>(<i>C</i>1,脐部)= 0.0566,<i>Gain</i>(<i>C</i>1,触感)= 0.0566。</p>
                </div>
                <div class="p1">
                    <p id="177">因此,选取“色泽”作为第一分支节点构建1层C4.5决策树,如图2所示。</p>
                </div>
                <div class="area_img" id="178">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909025_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 C1决策树根节点划分" src="Detail/GetImg?filename=images/JSJK201909025_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>C</i>1决策树根节点划分  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909025_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Root node partition of <i>C</i>1 decision tree</p>

                </div>
                <div class="p1">
                    <p id="179">因此,可以得到属性集<i>U</i>={色泽,根蒂,敲声,纹理,脐部,触感}上的<i>C</i>1的联合增益为:</p>
                </div>
                <div class="p1">
                    <p id="180"><i>GGR</i>(<i>C</i>1,<i>U</i>)={0.1438,0.63651*0.5+0.6931*0.33,0,0,0,0}={0.1438,0.5470,0,0,0,0}</p>
                </div>
                <div class="p1">
                    <p id="181">同理,可以得到<i>C</i>2,<i>C</i>3的属性集上的联合增益为:</p>
                </div>
                <div class="p1">
                    <p id="182"><i>GGR</i>(<i>C</i>2,<i>U</i>)={0,0.58093,0,0,0,0}</p>
                </div>
                <div class="p1">
                    <p id="183"><i>GGR</i>(<i>C</i>3,<i>U</i>)={0.5623*2/3,0,0.5013,0,0,0}={0.3749,0,0.5013,0,0,0}</p>
                </div>
                <div class="p1">
                    <p id="184">利用信息熵计算基分类器<i>C</i>1,<i>C</i>2,<i>C</i>3的两两之间的相似距离。</p>
                </div>
                <div class="p1">
                    <p id="185">计算基分类器在属性集<i>U</i>上的欧氏距离:</p>
                </div>
                <div class="p1">
                    <p id="186"><i>d</i>12(<i>U</i>)={0.1438,0.0340,0,0,0,0}</p>
                </div>
                <div class="p1">
                    <p id="187"><i>d</i>23(<i>U</i>)={0.3749,0.58093,0.5013,0,0,0}</p>
                </div>
                <div class="p1">
                    <p id="188"><i>d</i>13(<i>U</i>)={0.2311,0.5470,0.5013,0,0,0}</p>
                </div>
                <div class="p1">
                    <p id="189">那么,基分类器<i>C</i>1和<i>C</i>2的熵距离<i>E</i>12为:<i>E</i>12= 0.488。</p>
                </div>
                <div class="p1">
                    <p id="190">同理,可得到<i>C</i>1,<i>C</i>3的熵距离<i>E</i>13=1.083,<i>C</i>2,<i>C</i>3的熵距离<i>E</i>23=1.0395。</p>
                </div>
                <div class="p1">
                    <p id="191">因此,认为<i>C</i>1和<i>C</i>2之间具有最大的多样性,<i>C</i>1,<i>C</i>3之间次之,<i>C</i>1,<i>C</i>3之间多样性最差。</p>
                </div>
                <h4 class="anchor-tag" id="192" name="192"><b>5.2 GGR-IDS多样性度量性能评估</b></h4>
                <div class="p1">
                    <p id="193">将使用联合增益率方法进行基分类器多样性估算评估的方法同典型的相关系数<citation id="362" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>多样性评估方法进行比较,研究GGR-IDS多样性评估的准确率和计算效率这2个方面的主要性能。</p>
                </div>
                <div class="p1">
                    <p id="194">相关系数<i>ρ</i>是一种成对多样性度量方法,通过对2个基分类器<i>Cm</i>,<i>Cn</i>在测试数据集对样本的预测分类正确情况进行分析统计,得到相关性度量。分类器<i>Cm</i>,<i>Cn</i>的相关程度用相关系数<i>ρ</i><sub><i>mn</i></sub>表示为:</p>
                </div>
                <div class="p1">
                    <p id="195" class="code-formula">
                        <mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>Ν</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msup><mi>Ν</mi><msup><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msup><mo>-</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>0</mn><mn>1</mn></mrow></msup><mi>Ν</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>0</mn></mrow></msup></mrow><mrow><msqrt><mrow><mo stretchy="false">(</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msup><mo>+</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>0</mn><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msup><mo>+</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>0</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msup><mo>+</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>0</mn><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>0</mn><mn>0</mn></mrow></msup><mo>+</mo><mi>Ν</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>0</mn></mrow></msup><mo stretchy="false">)</mo></mrow></msqrt></mrow></mfrac></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="196">其中,<i>N</i><sup>10</sup>是指在测试数据集上基分类器<i>Cm</i>正确分类而<i>Cn</i>错误分类的样本数,其余类似。</p>
                </div>
                <div class="p1">
                    <p id="197"><i>ρ</i><sub><i>mn</i></sub>越小,表明2个分类器多样性越大,当<i>ρ</i><sub><i>mn</i></sub>为负数时,表明2个分类器负相关。如果基分类器之间是负相关的,则集成可以获得更好的泛化性能<citation id="363" type="reference"><link href="308" rel="bibliography" /><link href="332" rel="bibliography" /><sup>[<a class="sup">31</a>,<a class="sup">31</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="198">西瓜数据集2.0的测试数据集<i>D</i><sub>test</sub>={4,5,8,9,11,12,13}包含7个样本数据,目标属性的二分类比例为3∶4。在<i>D</i><sub>test</sub>上按照构建完整的基分类器<i>C</i>1,<i>C</i>2,<i>C</i>3进行分类预测,并统计基分类器测试数据集上的分类正误情况,据此计算相关系数。</p>
                </div>
                <div class="p1">
                    <p id="199">基分类器<i>C</i>1,<i>C</i>2,<i>C</i>3之间的相关程度,使用GGR-IDS多样性评估方法和相关系数<i>ρ</i>计算方法比较,如表1所示。</p>
                </div>
                <div class="area_img" id="200">
                    <p class="img_tit"><b>表1 多样性度量方法比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison of diversity measurement methods</b></p>
                    <p class="img_note"></p>
                    <table id="200" border="1"><tr><td><br /></td><td>GGR-IDS熵距离<i>E</i></td><td>相关系数<i>ρ</i></td></tr><tr><td><br /><i>C</i>1,<i>C</i>2</td><td>0.488 0</td><td>-0.353 5</td></tr><tr><td><br /><i>C</i>1,<i>C</i>3</td><td>1.083 0</td><td>-0.730 3</td></tr><tr><td><br /><i>C</i>2,<i>C</i>3</td><td>1.039 5</td><td>0.353 5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="201">从表1可以发现,2种多样性度量方法得到的结果比较接近,均认为<i>C</i>1,<i>C</i>2具有最大的多样性,<i>C</i>1,<i>C</i>3则最差。因此,从定性角度可以认为GGR-IDS方法具有与传统后验证多样性方法相近的准确性。</p>
                </div>
                <div class="p1">
                    <p id="202">由于GGR-IDS方法构建完成2层决策树即进行分类器多样性估算,而传统后验证多样性方法需要完成完整分类器的构建并在测试数据集完成所有验证后才能得到多样性评估值,因此在实验数据集上,GGR-IDS方法较传统多样性度量方法具有更高的计算效率。</p>
                </div>
                <h4 class="anchor-tag" id="203" name="203"><b>5.3 GGR-IDS在经典数据集上集成学习效果评估</b></h4>
                <div class="p1">
                    <p id="204">使用基于GGR-IDS的Bagging集成学习方法同传统的Bagging集成学习在UCI的经典分类数据集上进行比较,主要比较执行效率(模型构建时间)和预测分类精度。预测精度分为在训练数据集和测试数据集2个部分,其中测试数据集上的分类预测精度体现了分类模型的泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="205">测试平台使用Intel Core i7的2.4 GB处理器微机,利用Python的sklearn模块实现传统Bagging集成学习方法,使用多线程实现并行决策树构建。那么,在UCI的典型分类数据集上,选取50%样本,按照10%交叉验证,基分类器数目取值为属性数的一半或小于10时取值为10,分类器决策树深度取5。传统的Bagging集成学习和相同条件下基于GGR-IDS的Bagging集成学习的平均正确率和平均模型构建时间比较如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="206">从表2可以发现:(1)基于GGR-IDS的多样性评估方法的Bagging集成学习方法,通过多线程并行和过程性的多样性评估组合基分类器,可以在不明显降低分类正确率的基础上有效缩短模型构建的时间。(2)当数据集的属性数过大时,由于基于GGR-IDS的多样性评估方法的基分类器提前完成决策树构建,属性的联合熵取值为零,导致多样性判决不准确,从而导致基分类器组合的多样性未能明显提升,而提前完成决策树构建又降低了基分类器的精确度,从而导致整体集成分类正确率有所降低。(3)对于属性数和分类数都较小的数据集,基于GGR-IDS的多样性评估方法的Bagging算法在正确率和时间效率上均有明显优势。</p>
                </div>
                <div class="area_img" id="207">
                    <p class="img_tit"><b>表2 典型分类数据集的不同算法效能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Efficiency comparison of different algorithms on typical classification datasets</b></p>
                    <p class="img_note"></p>
                    <table id="207" border="1"><tr><td><br />Dataset</td><td colspan="3"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>规</mtext><mtext>模</mtext></mrow><mrow><mtext>样</mtext><mtext>本</mtext><mtext>数</mtext><mtext> </mtext><mtext> </mtext><mtext>属</mtext><mtext>性</mtext><mtext>数</mtext><mtext> </mtext><mtext> </mtext><mtext>分</mtext><mtext>类</mtext><mtext>数</mtext></mrow></mfrac></mrow></math></td><td colspan="2"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>传</mtext><mtext>统</mtext><mtext>B</mtext><mtext>a</mtext><mtext>g</mtext><mtext>g</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext></mrow><mrow><mtext>正</mtext><mtext>确</mtext><mtext>率</mtext><mo>/</mo><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext>构</mtext><mtext>建</mtext><mtext>时</mtext><mtext>间</mtext><mo>/</mo><mtext>s</mtext></mrow></mfrac></mrow></math></td><td colspan="2"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mtext>G</mtext><mtext>G</mtext><mtext>R</mtext><mo>-</mo><mtext>Ι</mtext><mtext>D</mtext><mtext>S</mtext><mtext>的</mtext><mtext>B</mtext><mtext>a</mtext><mtext>g</mtext><mtext>g</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext></mrow><mrow><mtext>正</mtext><mtext>确</mtext><mtext>率</mtext><mo>/</mo><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext>构</mtext><mtext>建</mtext><mtext>时</mtext><mtext>间</mtext><mo>/</mo><mtext>s</mtext></mrow></mfrac></mrow></math></td></tr><tr><td><br />Chess</td><td>3 196</td><td>36</td><td>2</td><td>98.32</td><td>2.88</td><td>92.35</td><td>2.25</td></tr><tr><td><br />Heart</td><td>270</td><td>13</td><td>2</td><td>83.20</td><td>1.05</td><td>84.95</td><td>0.85</td></tr><tr><td><br />Glass</td><td>214</td><td>9</td><td>6</td><td>72.96</td><td>1.65</td><td>74.46</td><td>1.35</td></tr><tr><td><br />Iris</td><td>150</td><td>4</td><td>3</td><td>96.63</td><td>0.58</td><td>97.60</td><td>0.55</td></tr><tr><td><br />Monks</td><td>432</td><td>6</td><td>2</td><td>96.55</td><td>0.98</td><td>97.25</td><td>0.87</td></tr><tr><td><br />Seeds</td><td>210</td><td>7</td><td>3</td><td>93.33</td><td>0.85</td><td>94.94</td><td>0.78</td></tr><tr><td><br />Parkinson</td><td>1 040</td><td>28</td><td>2</td><td>98.07</td><td>1.95</td><td>95.27</td><td>1.64</td></tr><tr><td><br />Vehicle</td><td>846</td><td>18</td><td>4</td><td>86.65</td><td>2.45</td><td>84.35</td><td>2.06</td></tr><tr><td><br />Wine</td><td>178</td><td>13</td><td>3</td><td>95.94</td><td>0.65</td><td>96.25</td><td>0.58</td></tr><tr><td colspan="4"><br />平均值</td><td>91.29</td><td>1.45</td><td>90.82</td><td>1.21</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="209">因此,对于属性数和分类数较少的大样本数据集,采用基于GGR-IDS的多样性评估方法的Bagging算法能够有效提升正确率和模型构建效率。</p>
                </div>
                <h3 id="210" name="210" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="211">基分类器的多样性是提升集成学习的精度和泛化能力的重要因素,传统的多样性评估方法利用基分类器在测试数据集的分类结果进行后验证性评估,在大数据环境下构建完整基分类器需要大量计算资源,据此,本文提出一种基于联合增益的过程多样性计算方法。该方法通过使用分类器各属性的增益及其所在树层次得到属性集的联合增益,并计算分类器间的熵距离评估其多样性,利用熵距离按照K-means方法即可动态构建集成学习分类器。在西瓜数据集上,使用联合信息增益的过程性多样性估算方法(GGR-IDS)同现有的相关系数后验证多样性计算方法比较,发现该方法具有相近的准确性和更高的计算效率,特别是在属性数和分类数较少的数据集上有明显的正确率和构建效率的提升。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="248">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Popular ensemble methods: an empirical study">

                                <b>[1]</b> David O,Maclin R.Popular ensemble methods:An empirical study[J].Journal of Artificial Intelligence Research,1999,11(1):169-198.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine Learning Research: Four Current Directions">

                                <b>[2]</b> Dietterich T G.Machine learning research:Four current directions [J].AI Magazine,1997,18(4):97-136.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on classifier integration">

                                <b>[3]</b> Xie Yuan-cheng.Research on classifier integration [D].Nanjing:Nanjing University of Science &amp; Technology,2009.(in Chinese)
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble Methods in Machine Learning">

                                <b>[4]</b> Dietterich T G.Ensemble methods in machine learning [C]//Proc of the 1st International Workshop on Multiple Classifier Systems,2000:1-15.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining classifiers based on analysis of correlation and effective supplement">

                                <b>[5]</b> Jing Xiao-yuan,Yang Jing-yu.Combining classifiers based on analysis of correlation and effective supplement [J].Acta Automatica Sinica,2000,26(6):741-747.(in Chinese)
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329294&amp;v=MTc1NjREblU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw4V2JoYz1OaWZPZmJLN0h0RE5ySTlGWitrRw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Brown G,Wyatt J,Harris R,et al.Diversity creation methods:A survey and categorisation[J].Information Fusion,2005,6(1):5-20.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MjA4ODRZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDRGxWYjdLSkZzPU5qN0Jhck80SHRITnJJeE1ZT01O&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Breiman L.Bagging predictors [J].Machine Learning,1996,24(2):123-140.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201311082&amp;v=MjE1NTZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWJ2Tkx6N0JkTEc0SDlMTnJvOU4=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Qin Li-long,Yu Qi,Wang Zhen-yu.Modified algorithm of neural network ensemble based on generalization theory[J].Computer Simulation,2013,30(11):361-364.(in Chinese)
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data minings:Concepts,models,methods,and algorithms">

                                <b>[9]</b> Kantardzic M.Data minings:Concepts,models,methods,and algorithms[M].2 Edition.New Jersey:Wiley,2013.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500013717&amp;v=MDY1MTF1SHlqbVVMZklKbDhXYmhjPU5pZkpaYks5SHRqTXFvOUZaT29NQzMwK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Geman S,Bienenstock E,Doursat R.Neural networks and the bias/variance dilemma[J].Neural Computation,1992,4(1):1-58.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural network ensembles,cross validation,and active learning">

                                <b>[11]</b> Krogh A,Vedelsby J.Neural network ensembles,cross validation and active learning[C]//Proc of the 7th International Conference on Neural Information Processing Systems(NIPS),1994:231-238.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Elements of Statistical Learning: Data Mining, Inference and Prediction">

                                <b>[12]</b> Hastie T,Tibshirani R,Friedman J.The elements of statistical learning-Data minging,inference and prediction [M].2nd Edition.Berlin:Springer,2001.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Zhang Hong-da,Wang Xiao-dan,Han Jun,et al.Survey of diversity researches on classifier ensembles [J].Systems Engineering and Electronics ,2009,31(12):3007-3012.(in Chinese)
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diversity versus quality in classification ensembles based on feature selection">

                                <b>[14]</b> Cunningham P,Carney J.Diversity versus quality in classification ensembles based on feature selection[C]//Proc of the 11th European Conference on Machine Learning,2000:109-116.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diversity measure for multiple classifier systems">

                                <b>[15]</b> Hu Q H,Yu D R.Diversity measure for multiple classifier systems[C]//Proc of the 2nd International Conference on Fuzzy Systems and Knowledge Discovery,2005:1261-1265.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329412&amp;v=MTc3NTNiSzdIdEROckk5Rlora0dDSDA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw4V2JoYz1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Shipp C A,Kuncheva L I.Relationships between combination methods and measures of diversity in combining classifiers [J].Information Fusion,2002,3(2):135-148.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                 Sun Bo,Wang Jian-dong,Chen Hai-yan,et al.Diversity measures in ensemble learning [J].Control and Decision ,2014,29(3):385-395.(in Chinese)
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An entropy-based diversity measure for classifier combining and its application to face classifier ensemble thinning">

                                <b>[18]</b> Liu W Y,Wu Z H,Pan G.An entropy-based diversity measure for classifier combining and its application to face classifier ensemble thinning[C]//Proc of the 5th Chinese Conference on Biometric Person Authentication,2004:118-124.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An information theoretic perspective on multiple classifier systems">

                                <b>[19]</b> Brown G.An information theoretic perspective on multiple classifier systems[C]//Proc of the 8th International Workshop on Multiple Classifier Systems,2009:344-353.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Information Ensemble Diversity">

                                <b>[20]</b> Zhou Z H,Li N.Multi-information ensemble diversity[C]//Proc of the 9th International Workshop on Multiple Classifier Systems,2010:134-144.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 Xing Hong-jie,Wei Yong-le.Selective ensemble of SVDDs based on correntropy and distance variance[J].Computer Science,2016,43(5):252-256.(in Chinese)
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimization method research and application of multiple classifiers ensemble based on diversity measure">

                                <b>[22]</b> Li Li.Optimization method research and application of multiple classifiers ensemble based on diversity measure[D].Dalian:Dalian Maritime University,2017.(in Chinese)
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                 Zhao Jun-yang,Han Chong-zhao,Han De-qiang,et al.A novel measure method of classifier integrations using complement information entropy[J].Journal of Xi’an Jiaotong University,2016,50(2):13-19.(in Chinese)
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=C4.5: Programs for Machine Learning">

                                <b>[24]</b> Quinlan J R.C4.5:Programs for machine learning [M].San Francisco:Morgan Kaufmann Publishers Inc,1993:17-42.
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001682629&amp;v=MjYxMDJEbFZiN0tKRnM9Tmo3QmFyTzRIdEhOcVlkSFl1a0dZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZD&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> Wu X D,Kumar V,Quinlan J R,et al.Top 10 algorithms in data mining[J].Knowledge &amp; Information Systems,2007,14(1):1-37.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SLTJ200205004&amp;v=MDQ0NzZxQnRHRnJDVVJMT2VaZVJtRnk3a1Vidk5OaUhmWkxHNEh0UE1xbzlGWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> Liu Rui-yuan.Euclidean distance with weighted and its application [J].Application of Statistics and Management,2002,21 (5):17-19.(in Chinese)
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_27" >
                                    <b>[27]</b>
                                 Guan Qing-yun,Chen Xue-long,Wang Yan-zhang.Distance entropy based decision-making information fusion method [J].Systems Engineering-Theory &amp; Practice,2015,35(1):216-227.(in Chinese)
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An overview of clustering algorithms in data mining">

                                <b>[28]</b> Amudha S,B.SC.,M.SC.,M.PHIL.An overview of clustering algorithms in data mining[J].International Research Journal of Engineering and Technology (IRJET),2016,12(3):1359-1403.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning">

                                <b>[29]</b> Zhou Zhi-hua.Machine learning [M].Beijing:Tsinghua University Press,2016.(in Chinese)
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340615&amp;v=MTQyMTRPNEh0SE5ySXRGWXVvS1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZiN0tKRnM9Tmo3QmFy&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b> Kuncheva L I,Whitaker C J.Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy[J].Machine Learning,2003,51(2):181-207.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_31" >
                                    <b>[31]</b>
                                 Cai Tie,Wu Xing,Li Ye.Research on construction of base classifier based on discretization method for ensemble learning [J].Journal of Computer Applications,2008,28(8):2091-2093.(in Chinese)附中文参考文献:
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2010096743.nh&amp;v=MDY1MzZVYnZOVjEyNkhyT3hHTmJJckpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2s=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 谢元澄.分类器集成研究[D].南京:南京理工大学,2009.
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200006003&amp;v=MTk3OTVrVWJ2TktDTGZZYkc0SHRITXFZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 荆晓远,杨静宇.基于相关性和有效互补性分析的多分类器组合方法[J].自动化学报,2000,26(6):741-747.
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 秦立龙,余奇,王振宇.基于泛化理论的集成神经网络优化算法[J].计算机仿真,2013,30(11):361-364.
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD200912048&amp;v=MjUwNzBSbUZ5N2tVYnZOUFRuU2FyRzRIdGpOclk5QmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 张宏达,王晓丹,韩钧,等.分类器集成差异性研究[J].系统工程与电子技术,2009,31(12):3007-3012.
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201403001&amp;v=MDcxMjNVUkxPZVplUm1GeTdrVWJ2TkxqZlNiYkc0SDlYTXJJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 孙博,王建东,陈海燕,等.集成学习中的多样性度量[J].控制与决策,2014,29(3):385-395.
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201605049&amp;v=MTIyNzh0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnZOTHo3QmI3RzRIOWZNcW85QmJZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 邢红杰,魏勇乐.基于相关熵和距离方差的支持向量数据描述选择性集成[J].计算机科学,2016,43(5):252-256.
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017054461.nh&amp;v=Mjk1MTh2TlZGMjZHYk85R3RYS3JwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> 李莉.基于差异性度量的分类器集成优化方法研究与应用[D].大连:大连海事大学,2017.
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAJT201602003&amp;v=MDY3NjhaZVJtRnk3a1Vidk5QU3pCZXJHNEg5Zk1yWTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 赵军阳,韩崇昭,韩德强,等.采用互补信息熵的分类器集成差异性度量方法[J].西安交通大学学报,2016,50(2):13-19.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                 刘瑞元.加权欧氏距离及其应用[J].数理统计与管理,2002,21(5):17-19.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTLL201501024&amp;v=MTY0Mzh2TlBUbkhZckc0SDlUTXJvOUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 管清云,陈雪龙,王延章.基于距离熵的应急决策层信息融合方法[J].系统工程理论与实践,2015,35(1):216-227.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjkwMDdOWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2dFU3bkpJbG9SWEZxekdiQzRITlhPckkx&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b> 周志华.机器学习[M].北京:清华大学出版社,2016.
                            </a>
                        </p>
                        <p id="332">
                            <a id="bibliography_31" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200808056&amp;v=MjMyODNCZDdHNEh0bk1wNDlBWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1Vidk5Mejc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[31]</b> 蔡铁,伍星,李烨.集成学习中基于离散化方法的基分类器构造研究[J].计算机应用,2008,28(8):2091-2093.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909025" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909025&amp;v=MDc5NTlHRnJDVVJMT2VaZVJtRnk3a1VidktMejdCWmJHNEg5ak1wbzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
