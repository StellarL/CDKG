<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132366206748750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201908026%26RESULT%3d1%26SIGN%3dn1wvHZvwc4vuhSlJUvzIKil2Pwc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908026&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908026&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908026&amp;v=MTUzOTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWYjdKTHo3QlpiRzRIOWpNcDQ5SFlvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="&lt;b&gt;2 集成式数据流分类研究背景&lt;/b&gt; "><b>2 集成式数据流分类研究背景</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;2.1 相关工作&lt;/b&gt;"><b>2.1 相关工作</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;2.2 相关概念&lt;/b&gt;"><b>2.2 相关概念</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;2.3 基于数据块的数据流集成分类算法&lt;/b&gt;"><b>2.3 基于数据块的数据流集成分类算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="&lt;b&gt;3 基于集成的不均衡数据流分类算法&lt;/b&gt; "><b>3 基于集成的不均衡数据流分类算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="&lt;b&gt;3.1 基本思想&lt;/b&gt;"><b>3.1 基本思想</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;3.2 混合采样过程描述&lt;/b&gt;"><b>3.2 混合采样过程描述</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;3.3 基于集成的不均衡数据流分类算法&lt;/b&gt;"><b>3.3 基于集成的不均衡数据流分类算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="&lt;b&gt;4 实验结果与分析&lt;/b&gt; "><b>4 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#149" data-title="&lt;b&gt;4.1 数据集描述&lt;/b&gt;"><b>4.1 数据集描述</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;4.2 实验参数分析&lt;/b&gt;"><b>4.2 实验参数分析</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;4.3 不同数据块大小对算法性能影响&lt;/b&gt;"><b>4.3 不同数据块大小对算法性能影响</b></a></li>
                                                <li><a href="#161" data-title="&lt;b&gt;4.4 不同算法性能对比&lt;/b&gt;"><b>4.4 不同算法性能对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#170" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="图1 基于数据块的算法流程图">图1 基于数据块的算法流程图</a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表1 混淆矩阵&lt;/b&gt;"><b>表1 混淆矩阵</b></a></li>
                                                <li><a href="#159" data-title="图2 数据块不同时平均&lt;i&gt;G&lt;/i&gt;-&lt;i&gt;means&lt;/i&gt;值">图2 数据块不同时平均<i>G</i>-<i>means</i>值</a></li>
                                                <li><a href="#163" data-title="图3 SEA数据集上的&lt;i&gt;G&lt;/i&gt;-&lt;i&gt;means&lt;/i&gt;值对比">图3 SEA数据集上的<i>G</i>-<i>means</i>值对比</a></li>
                                                <li><a href="#165" data-title="图4 Rotating Spiral数据集上的&lt;i&gt;G&lt;/i&gt;-&lt;i&gt;means&lt;/i&gt;值对比">图4 Rotating Spiral数据集上的<i>G</i>-<i>means</i>值对比</a></li>
                                                <li><a href="#168" data-title="图5 Spam数据集上的&lt;i&gt;G&lt;/i&gt;-&lt;i&gt;means&lt;/i&gt;值对比">图5 Spam数据集上的<i>G</i>-<i>means</i>值对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="196">


                                    <a id="bibliography_1" title=" Gama J, Ganguly A, Omitaomu O, et al.Knowledge discovery from data streams[J].Intelligent Data Analysis-Knowledge Discovery from Data Streams, 2009, 13 (3) :403-404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Knowledge discovery from data streams">
                                        <b>[1]</b>
                                         Gama J, Ganguly A, Omitaomu O, et al.Knowledge discovery from data streams[J].Intelligent Data Analysis-Knowledge Discovery from Data Streams, 2009, 13 (3) :403-404.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_2" title=" Wang Hai-xun, Fan Wei, Philip S Y, et al.Mining concept-drifting data streams using ensembles classifiers[C]//Proc of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2003) , 2003:226-235." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mining concept-drifting data streams using ensemble classifiers">
                                        <b>[2]</b>
                                         Wang Hai-xun, Fan Wei, Philip S Y, et al.Mining concept-drifting data streams using ensembles classifiers[C]//Proc of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2003) , 2003:226-235.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_3" title=" Cohen L, Avrahami-Bakish G, Last M, et al.Real-time data mining of non-stationary data streams from sensor networks[J].Information Fusion, 2008, 9 (3) :344-353." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329121&amp;v=MjExODRyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDRTYXhNPU5pZk9mYks3SHRETnJJOUZaK2tHRFg0NG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Cohen L, Avrahami-Bakish G, Last M, et al.Real-time data mining of non-stationary data streams from sensor networks[J].Information Fusion, 2008, 9 (3) :344-353.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_4" title=" Widmer G, Kubat M.Learning in the presence of concept drift and hidden contexts[J].Machine Learning, 1996, 23 (1) :69-101." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339404&amp;v=MDQzNTk0SHRITnJJeE1ZT3NMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ0RsVmIvT0lWOD1OajdCYXJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Widmer G, Kubat M.Learning in the presence of concept drift and hidden contexts[J].Machine Learning, 1996, 23 (1) :69-101.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_5" title=" Tsymbal A.The problem of concept drift:Definitions and related work:Technical Report TCD-CS-2004-15[R].Dublin:Trinity College Dublin, 2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The problem of concept drift:definitionsand related work">
                                        <b>[5]</b>
                                         Tsymbal A.The problem of concept drift:Definitions and related work:Technical Report TCD-CS-2004-15[R].Dublin:Trinity College Dublin, 2004.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_6" title=" Gama J, Žliobaite I, Bifet A, et al.A survey on concept drift adaptation[J].ACM Computing Surveys (CSUR) , 2014, 46 (4) :Article No.44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM14061800000195&amp;v=MTUyMTNlWnVIeWptVUxmSUpsNFNheE09TmlmSVk3SzhIdGZOcDQ5RlpPc1BEWFU4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Gama J, Žliobaite I, Bifet A, et al.A survey on concept drift adaptation[J].ACM Computing Surveys (CSUR) , 2014, 46 (4) :Article No.44.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_7" title=" Charles X L, Victor S S, Yang Qiang.Test strategies for cost-sensitive decision trees[J].IEEE Transactions on Knowledge and Data Engineering, 2006, 18 (8) :1055-1067." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Test Strategies for Cost-Sensitive Decision Trees">
                                        <b>[7]</b>
                                         Charles X L, Victor S S, Yang Qiang.Test strategies for cost-sensitive decision trees[J].IEEE Transactions on Knowledge and Data Engineering, 2006, 18 (8) :1055-1067.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_8" title=" Sun Y, Kanmel M S, Wong A K C, et al.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MDI0Mjd3WmVadUh5am1VTGZJSmw0U2F4TT1OaWZPZmJLN0h0RE5xWTlGWStnR0QzczZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Sun Y, Kanmel M S, Wong A K C, et al.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_9" title=" He H B, Garcia E A.Learning from imbalanced data[J].IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (9) :1263-1284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning from Imbalanced Data">
                                        <b>[9]</b>
                                         He H B, Garcia E A.Learning from imbalanced data[J].IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (9) :1263-1284.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_10" title=" Chawla N V, Bowyer K W, Hall L O, et al.SMOTE:Synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">
                                        <b>[10]</b>
                                         Chawla N V, Bowyer K W, Hall L O, et al.SMOTE:Synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_11" title=" Wang Shuo, Yao Xin.Diversity analysis on imbalanced data sets by using ensemble models[C]//Proc of the IEEE Symposium Series on Computational Intelligence and Data Mining (CIDM 2009) , 2009:324-331." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diversity analysis on imbalanced data sets by using ensemble models">
                                        <b>[11]</b>
                                         Wang Shuo, Yao Xin.Diversity analysis on imbalanced data sets by using ensemble models[C]//Proc of the IEEE Symposium Series on Computational Intelligence and Data Mining (CIDM 2009) , 2009:324-331.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     Hu Xue-gang, He Jun-hong, Li Pei-pei.Data streams classification approach based on distance and sampling [J].Application Research of Computers, 2018, 35 (4) :992-995. (in Chinese) </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_13" title=" Zhang Pan-pan, Yin Shao-hong.An ensemble classification algorithm for uncertain data stream containing concept drift[J].Computer Engineering &amp;amp; Science, 2016, 38 (7) :1510-1516. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201607032&amp;v=MDAxOTNJOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVmI3Skx6N0JaYkc0SDlmTXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Zhang Pan-pan, Yin Shao-hong.An ensemble classification algorithm for uncertain data stream containing concept drift[J].Computer Engineering &amp;amp; Science, 2016, 38 (7) :1510-1516. (in Chinese) 
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_14" title=" Gao Jing, Fan Wei, Han Jia-wei, et al.A general framework for mining concept-drifting data streams with skewed distributions[C]//Proc of the 7th SIAM International Conference on Data Mining, 2007:3-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A general framework for mining concept-drifting data streams with skewed distributions">
                                        <b>[14]</b>
                                         Gao Jing, Fan Wei, Han Jia-wei, et al.A general framework for mining concept-drifting data streams with skewed distributions[C]//Proc of the 7th SIAM International Conference on Data Mining, 2007:3-14.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_15" title=" Chen Sheng, He Hai-bo.Towards incremental learning of nonstationary imbalanced data stream:A multiple selectively recursive approach[J].Evolving Systems, 2011, 2 (1) :35-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards incremental learning of nonstationary imbalanced data stream: A multiple selectively recursive approach">
                                        <b>[15]</b>
                                         Chen Sheng, He Hai-bo.Towards incremental learning of nonstationary imbalanced data stream:A multiple selectively recursive approach[J].Evolving Systems, 2011, 2 (1) :35-50.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     Ouyang Zhen-zheng, Luo Jian-shu, Hu Dong-min, et al.An ensemble classifier framework for mining imbalanced data streams[J].Acta Electronica Sinica, 2010, 38 (1) :184-189. (in Chinese) </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_17" title=" Ditzler G, Polikar R.Incremental learning of concept drift from streaming imbalanced data[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (10) :2283-2301." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incremental Learning of Concept Drift fromStreaming Imbalanced Data">
                                        <b>[17]</b>
                                         Ditzler G, Polikar R.Incremental learning of concept drift from streaming imbalanced data[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (10) :2283-2301.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_18" title=" Mirza B, Lin Z P, Toh K A.Weighted online sequential extreme learning machine for class imbalance learning[J].Neural Processing Letters, 2013, 38 (3) :465-486." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13120400000960&amp;v=MjMzMTVJSmw0U2F4TT1OajdCYXJLN0g5UE1xNDlGWk9zUEJYbzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Mirza B, Lin Z P, Toh K A.Weighted online sequential extreme learning machine for class imbalance learning[J].Neural Processing Letters, 2013, 38 (3) :465-486.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_19" title=" Gomes H M, Barddal J P, Enembreck F, et al.A survey on ensemble learning for data stream classification[J].ACM Computing Surveys (CSUR) , 2017, 50 (2) :Article No.23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM239273940810D7C32E2AC724EFBD687A&amp;v=MDc3NzJUYjN1Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3JpNXdhaz1OaWZJWTdHN0Y5UExySVpCWk9NT0RBZyt2QlVSbnowTU8zamdxR2REQzhhUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Gomes H M, Barddal J P, Enembreck F, et al.A survey on ensemble learning for data stream classification[J].ACM Computing Surveys (CSUR) , 2017, 50 (2) :Article No.23.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_20" title=" Street W N, Kim Y S.A streaming ensemble algorithm (SEA) for large-scale classification[C]//Proc of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2001:377-382." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A streaming ensemble algorithm (SEA) for large-scale classification">
                                        <b>[20]</b>
                                         Street W N, Kim Y S.A streaming ensemble algorithm (SEA) for large-scale classification[C]//Proc of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2001:377-382.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_21" title=" Bifet A, Holmes G, Kirkby R, et al.MOA:Massive online analysis[J].The Journal of Machine Learning Research, 2010, 11 (5) :1601-1604." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MOA: Massive Online Analysis">
                                        <b>[21]</b>
                                         Bifet A, Holmes G, Kirkby R, et al.MOA:Massive online analysis[J].The Journal of Machine Learning Research, 2010, 11 (5) :1601-1604.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_22" title=" Domingos P, Hulten G.Mining high-speed data streams[C]//Proc of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2007:71-80." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mining high-speed data streams">
                                        <b>[22]</b>
                                         Domingos P, Hulten G.Mining high-speed data streams[C]//Proc of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2007:71-80.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_12" title=" 胡学钢, 何俊宏, 李培培.一种基于距离和采样机制的数据流分类方法[J].计算机应用研究, 2018, 35 (4) :992-995." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201804008&amp;v=MjYwMzBSbUZ5N2xWYjdKTHo3U1pMRzRIOW5NcTQ5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         胡学钢, 何俊宏, 李培培.一种基于距离和采样机制的数据流分类方法[J].计算机应用研究, 2018, 35 (4) :992-995.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     张盼盼, 尹绍宏.隐含概念漂移的不确定数据流集成分类算法[J].计算机工程与科学, 2016, 38 (7) :1510-1516.</a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_16" title=" 欧阳震诤, 罗建书, 胡东敏, 等.一种不平衡数据的集成分类模型[J].电子学报, 2010, 38 (1) :184-189." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201001033&amp;v=MDExMjRSTE9lWmVSbUZ5N2xWYjdKSVRmVGU3RzRIOUhNcm85R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         欧阳震诤, 罗建书, 胡东敏, 等.一种不平衡数据的集成分类模型[J].电子学报, 2010, 38 (1) :184-189.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(08),1519-1524 DOI:10.3969/j.issn.1007-130X.2019.08.025            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于集成的不均衡数据流分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%B3%89&amp;code=11522161&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁泉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E6%B1%9F%E5%B8%86&amp;code=39421492&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭江帆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%AD%A6%E5%8D%8E&amp;code=42704889&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵学华</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E9%80%9A%E4%BF%A1%E6%96%B0%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学通信与信息工程学院通信新技术应用研究中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E4%BF%A1%E7%A7%91%E8%AE%BE%E8%AE%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0415387&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆信科设计有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目前数据流分类算法大多是基于类分布这一理想状态, 然而在真实数据流环境中数据分布往往是不均衡的, 并且数据流中往往伴随着概念漂移。针对数据流中的不均衡问题和概念漂移问题, 提出了一种新的基于集成学习的不均衡数据流分类算法。首先为了解决数据流的不均衡问题, 在训练模型前加入混合采样方法平衡数据集, 然后采用基分类器加权和淘汰策略处理概念漂移问题, 从而提高分类器的分类性能。最后与经典数据流分类算法在人工数据集和真实数据集上进行对比实验, 实验结果表明, 本文提出的算法在含有概念漂移和不均衡的数据流环境中, 其整体分类性能优于其他算法的。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E6%B5%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据流;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A6%82%E5%BF%B5%E6%BC%82%E7%A7%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">概念漂移;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%8D%E5%9D%87%E8%A1%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">不均衡;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    袁泉 (1976-) , 男, 湖南绥宁人, 硕士, 高级工程师, 研究方向为数字图像处理和通信新技术应用。E-mail:80250938@qq.com通信地址:400065重庆市重庆邮电大学通信新技术应用研究中心;
                                </span>
                                <span>
                                    郭江帆 (1991-) , 男, 河南舞阳人, 硕士, 研究方向为大数据技术研究及应用。E-mail:502420381@qq.com通信地址:400065重庆市重庆邮电大学通信新技术应用研究中心;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-12</p>

            </div>
                    <h1><b>An imbalanced data stream classification algorithm based on ensemble learning</b></h1>
                    <h2>
                    <span>YUAN Quan</span>
                    <span>GUO Jiang-fan</span>
                    <span>ZHAO Xue-hua</span>
            </h2>
                    <h2>
                    <span>Research Center of New Telecommunication Technology Applications, School of Telecommunications and Information Engineering, Chongqing University of Posts and Telecommunications</span>
                    <span>Chongqing Information Technology Designing Company Limited</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>At present, most data stream classification algorithms assume that the class distribution is basically balanced. However, the data distribution is often unbalanced and accompanied by conceptual drift in real data stream environments. Aiming at the problem of unbalanced data distribution and concept drift, we propose an unbalanced data stream classification algorithm based on ensemble learning. Firstly, in order to solve the problem of unbalanced data flows, a mixed sampling method is added to balance the data set before model training. And then the concept drift problem is solved with base classifier weight and elimination strategy. Finally, comparison experiments among data stream classification algorithms are carried out on artificial and real data sets. Experimental results show that the proposed algorithm has better overall classification performance than other algorithms in data stream environments with concept drift and imbalance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20stream&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data stream;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=concept%20drift&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">concept drift;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=unbalance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">unbalance;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    YUAN Quan, born in 1976, MS, senior engineer, his research interests include digital image processing, and new telecommunication technology applications.Address:Research Center of New Telecommunication Technology Applications, Chongqing University of Posts and Telecommunications, Chongqing 400065, P.R.China;
                                </span>
                                <span>
                                    GUO Jiang-fan, born in 1991, MS, his research interests include big data technology research and application.Address:Research Center of New Telecommunication Technology Applications, Chongqing University of Posts and Telecommunications, Chongqing 400065, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-12</p>
                            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="54">近年来, 作为统计学、人工智能、机器学习等学科的交叉学科, 数据挖掘技术已成为研究热点并广泛运用。随着信息技术的迅速发展, 数据生成速率越来越快, 产生了高速、动态、连续且规模巨大的数据流DS (Data Streams) <citation id="246" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 如网络流量监控、实时交通系统和电信业务记录等。</p>
                </div>
                <div class="p1">
                    <p id="55">然而与静态数据不同的是, 数据流中呈现出高速、连续、多变、无限、单遍等特点。正是因为这些特点, 数据流中的数据分布在真实环境中时刻发生着变化<citation id="248" type="reference"><link href="198" rel="bibliography" /><link href="200" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>, 因此数据中的目标概念也会随时间而变, 即发生概念漂移<citation id="247" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="56">文献<citation id="249" type="reference">[<a class="sup">5</a>]</citation>中将传统处理概念漂移的算法分成3种, 即基于示例选择、基于示例加权与基于集成学习。在实际应用中, 多数算法并不是单纯地运用单一的概念漂移检测方法, 基于集成学习的方法处理概念漂移问题<citation id="250" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>最为广泛。在现实环境的数据流中往往存在着大量的类分布不均衡问题, 而我们更关注的是少数类的分类情况, 例如在线医疗诊断、网络入侵检测、信用卡欺诈等。</p>
                </div>
                <div class="p1">
                    <p id="57">在不平衡数据流分类的研究中, 学者主要在以下3个方面进行研究:一是对传统机器学习算法的改进, 设计代价敏感函数<citation id="251" type="reference"><link href="208" rel="bibliography" /><link href="210" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>;二是调整不同类别样本的权值重新构造训练集<citation id="252" type="reference"><link href="212" rel="bibliography" /><link href="214" rel="bibliography" /><link href="216" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>;三是采用集成学习的方式<citation id="253" type="reference"><link href="218" rel="bibliography" /><link href="220" rel="bibliography" /><link href="240" rel="bibliography" /><link href="242" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。数据流的特点决定了经典数据流分类算法不适用于数据流挖掘。</p>
                </div>
                <div class="p1">
                    <p id="58">基于此, 本文提出一种新的不均衡数据流集成分类算法。该算法首先处理数据流中类不均衡问题, 采用上采样技术增加正类样本, 再采用下采样技术删除负类样本, 减少过拟合均衡数据。其次采用集成方法周期更新分类器权值, 以应对概念漂移。在动态更新权值过程中, 综合考虑分类器在最新数据块的分类精度和误分类总代价。在分类器的淘汰策略中, 计算分类器在集成分类器中的贡献值, 根据贡献值替换分类器。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag"><b>2 集成式数据流分类研究背景</b></h3>
                <h4 class="anchor-tag" id="60" name="60"><b>2.1 相关工作</b></h4>
                <div class="p1">
                    <p id="61">近年来, 越来越多的学者投入到不均衡数据流的研究中。Gao等人<citation id="254" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了一种集成分类的算法SE (Sample and Ensemble) , 该算法综合重采样和集成的思想, 将数据集正负类样本进行重组。文献<citation id="255" type="reference">[<a class="sup">15</a>]</citation>提出了一种新的定义正类和负类的边界的方法, 以此来处理不均衡数据, 提高分类器的集成效果。欧阳震诤等人<citation id="256" type="reference"><link href="226" rel="bibliography" /><link href="244" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">16</a>]</sup></citation>提出了基于加权集成的不平衡数据流分类算法IMDWE (IMbalanced Data using Weighted Ensemble) , 该算法将数据块中负类实例划分为<i>n</i>块, 然后和正类实例进行组合, 在组合数目上比SE算法多很多。Ditzler等人<citation id="257" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了Learn<sup>++</sup>.NSE-SMOTE和Learn<sup>++</sup>.NIE, 该算法是基于SMOTE方法的在线分类, 利用分类器的时间因素以及与当前数据的不平衡调节正确率。Mirza等人<citation id="258" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了一种基于极限学习机ELM (Extreme Learning Machine) 的集成算法, 该算法未考虑数据流中的概念漂移问题, 只是将数据流划分为数据块来处理。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62"><b>2.2 相关概念</b></h4>
                <div class="p1">
                    <p id="63">数据流指的是大量连续、快速到达、潜在无限且随时间变化的数据序列 (由到达的时间隐含地表示或显式地以时间戳指定) , 可形式化地表示为:<i>DS</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>t</i></sub>, …}, 其中<i>s</i><sub><i>t</i></sub>= (<b><i>x</i></b><sub><i>t</i></sub>, <i>y</i><sub><i>t</i></sub>) 是<i>t</i>时刻的实例, <b><i>x</i></b><sub><i>t</i></sub>表示<i>d</i>维特征向量, <i>y</i><sub><i>t</i></sub>表示类值。</p>
                </div>
                <div class="p1">
                    <p id="64">经典概念漂移定义指的是一个随时间变化的目标概念。假设目标概念表示为联合概率分布, 则概念漂移指的是输入变量和目标变量之间的联合概率分布发生变化的现象, 用公式可表示为:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∃</mo><mi mathvariant="bold-italic">x</mi><mo>:</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>0</mn></msub></mrow></msub><mo stretchy="false">) </mo><mo>≠</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中, <b><i>x</i></b><sub><i>t</i><sub>0</sub></sub>和<i>y</i><sub><i>t</i><sub>0</sub></sub>表示<i>t</i><sub>0</sub>时刻实例的特征向量和类值, <b><i>x</i></b><sub><i>t</i><sub>1</sub></sub>和<i>y</i><sub><i>t</i><sub>1</sub></sub>表示<i>t</i><sub>1</sub>时刻实例的特征向量和类值, <i>P</i><sub><i>t</i><sub>0</sub></sub>和<i>P</i><sub><i>t</i><sub>1</sub></sub>表示<i>t</i><sub>0</sub>到<i>t</i><sub>1</sub>时刻之间的概念漂移。</p>
                </div>
                <div class="p1">
                    <p id="67">不同学者对概念漂移的定义也不尽相同。文献<citation id="259" type="reference">[<a class="sup">12</a>,<a class="sup">12</a>]</citation>将概念漂移分为突变式概念漂移 (Abrupt Concept Drift) 和渐变式概念漂移 (Gradual Concept Drift) , 分类依据为新旧概念的差异。文献<citation id="260" type="reference">[<a class="sup">13</a>,<a class="sup">13</a>]</citation>以漂移的速率作为分类标准将渐变式概念漂移进行细化, 分为中等漂移 (Moderate Drift) 和缓慢漂移 (Slow Drift) 。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>2.3 基于数据块的数据流集成分类算法</b></h4>
                <div class="p1">
                    <p id="69">如图1所示是基于数据块的数据流集成分类算法的流程图。在该算法中将数据流<i>DS</i>均分为数据大小为<i>d</i>的数据块<i>b</i><sub>1</sub>, <i>b</i><sub>2</sub>, …, <i>b</i><sub><i>n</i></sub>。当数据块完成分块时, 在每个数据块<i>b</i><sub><i>i</i></sub>, <i>i</i>∈[1, <i>n</i>]上训练基分类器<i>C</i><sub><i>k</i></sub>, <i>k</i>∈[1, <i>K</i>], 然后添加到集成分类器中。当基分类器数量达到预设值<i>K</i>时将分类性能最差的基分类器从集成分类器中删掉, 以此来应对数据流的概念漂移, 最后采用加权投票等策略进行预测分析。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908026_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于数据块的算法流程图" src="Detail/GetImg?filename=images/JSJK201908026_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于数据块的算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908026_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Flow chart of the algoritm based on data block</p>

                </div>
                <div class="p1">
                    <p id="71">基于数据块的集成分类算法的关键是如何动态地更新分类器的权值<citation id="261" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。Street等人<citation id="262" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>提出了数据流集成算法SEA (Streaming Ensemble Algorithm) , 该算法可以适应概念漂移, 以基分类器淘汰策略为启发式。Wang等人<citation id="263" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出了基于准确率加权的集成AWE (Accuracy Weighted Ensemble) 算法, 其性能受数据块大小的影响较为严重, 且不能应对突变式概念漂移。该算法的权值计算如下所示:</p>
                </div>
                <div class="p1">
                    <p id="72">首先, 计算基分类器在最新数据块上的误差率:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo></mstyle><mn>1</mn><mo>-</mo><mi>f</mi><msubsup><mrow></mrow><mi>y</mi><mi>k</mi></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中<i>MSE</i><sub><i>ik</i></sub>表示在数据块<i>b</i><sub><i>i</i></sub>上分类器<i>C</i><sub><i>k</i></sub>的预测均方误差, <i>f</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>k</mi></msubsup></mrow></math></mathml> (<b><i>x</i></b>) 表示分类器<i>C</i><sub><i>k</i></sub>属性值为<b><i>x</i></b>, 类值为<i>y</i>的预测概率。然后, 计算随机预测分类器的均方误差<i>MSE</i><sub>r</sub>∑<sub><i>y</i></sub><i>p</i> (<i>y</i>) - (1-<i>p</i> (<i>y</i>) ) <sup>2</sup>, <i>p</i> (<i>y</i>) 表示<i>y</i>的后验概率。最后, 计算权值<i>ω</i> (<i>C</i><sub><i>k</i></sub>) =<i>MSE</i><sub>r</sub>-<i>MSE</i><sub><i>ik</i></sub>。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag"><b>3 基于集成的不均衡数据流分类算法</b></h3>
                <h4 class="anchor-tag" id="77" name="77"><b>3.1 基本思想</b></h4>
                <div class="p1">
                    <p id="78">在处理概念漂移问题上, 多分类器要比单分类器的效果好得多。为了应对和处理数据流中不均衡问题和数据流分类中的概念漂移问题, 本文提出了一种新的不均衡数据流集成分类算法ECIDS (the Ensemble Classifiers for Imbalanced Data Streams) 。</p>
                </div>
                <div class="p1">
                    <p id="79">简单起见, 本文只考虑2类问题, 可以扩展到多类问题, 其主要包括2个步骤:</p>
                </div>
                <div class="p1">
                    <p id="80"><b>Step 1</b> 采用SMOTE和Tomek Links混合采样方式, 消除过拟合均衡数据流。</p>
                </div>
                <div class="p1">
                    <p id="81"><b>Step 2</b> 基于集成方式对数据流分类, 在动态更新权值时综合考虑分类错误情况和误分类总代价, 并用最新的基分类器模型替换掉会降低分类性能的分类器。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>3.2 混合采样过程描述</b></h4>
                <div class="p1">
                    <p id="83">本文采用的混合采样方法是先采用上采样SMOTE方法进行差值计算得到样本, 增加正类实例数量以降低不均衡度, 这种采样方法容易导致过拟合;然后采用下采样Tomek Links方法, 删除负类样本中的噪音数据和边界数据, 消除过拟合。</p>
                </div>
                <div class="p1">
                    <p id="84">混合采样方法描述如下:</p>
                </div>
                <div class="p1">
                    <p id="85"><b>Input</b>:正类样本数<i>n</i>, 过采样百分比<i>N</i>%, 最近邻数量<i>q</i>。</p>
                </div>
                <div class="p1">
                    <p id="86"><b>Output</b>:采样后样本集。</p>
                </div>
                <div class="p1">
                    <p id="87">Begin</p>
                </div>
                <div class="p1">
                    <p id="88">While (new data stream) :</p>
                </div>
                <div class="p1">
                    <p id="89"><i>Attrs</i>:属性数</p>
                </div>
                <div class="p1">
                    <p id="90"><i>SampleArray</i>[][]:原正类样本数组</p>
                </div>
                <div class="p1">
                    <p id="91"><i>SynNum</i>:合成样本数量</p>
                </div>
                <div class="p1">
                    <p id="92"><i>SynSam</i>[][]:合成样本数组</p>
                </div>
                <div class="p1">
                    <p id="93">For (<i>i</i>≤<i>n</i>) {</p>
                </div>
                <div class="p1">
                    <p id="94">上采样方法, 计算最近邻值<i>q</i>, 保存到数组<i>Narray</i>中, 生成合成样本 (<i>N</i>, <i>i</i>, <i>Narray</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="95">}</p>
                </div>
                <div class="p1">
                    <p id="96">下采样方法, 找出合成样本中Tomek Link对, 删除对中负类样本;</p>
                </div>
                <div class="p1">
                    <p id="97">End</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98"><b>3.3 基于集成的不均衡数据流分类算法</b></h4>
                <div class="p1">
                    <p id="99">将处理后的数据流<i>DS</i>均分为若干个数据大小为<i>d</i>的数据块<i>b</i><sub>1</sub>, <i>b</i><sub>2</sub>, …, <i>b</i><sub><i>n</i></sub>。当数据流分割为数据块后, 在最新数据块<i>b</i><sub><i>i</i></sub>, <i>i</i>∈[1, <i>n</i>]上训练基分类器<i>C</i>, 并赋予其一个权值<i>ω</i><sub>C</sub>。在集成分类器<image id="100" type="" href="images/JSJK201908026_10000.jpg" display="inline" placement="inline"><alt></alt></image>={<i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>, …, <i>C</i><sub><i>K</i></sub>}中每一个基分类器的权值都是动态更新的, 首先依据基分类器在最新数据块上的分类性能, 然后考虑其误分类总代价, 将新建的基分类器加入到集成分类器中。当集分类器数量达到预设值<i>K</i>时, 将分类性能最差的基分类器从集成分类器中删掉, 以此来应对数据流的概念漂移。最后采用加权投票等策略进行预测分析。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">3.3.1 加权策略</h4>
                <div class="p1">
                    <p id="102">为了计算误分类总代价, 本文引入一个混淆矩阵, 如表1所示。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表1 混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Confusion matrix</b></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br /></td><td>预测正类</td><td>预测负类</td></tr><tr><td><br />实际正类</td><td><i>TP</i></td><td><i>FN</i></td></tr><tr><td><br />实际负类</td><td><i>FP</i></td><td><i>TN</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="104">基分类器加权过程中考虑了分类错误的代价, 即只需要考虑<i>FP</i> (错误正类的代价) 和<i>FN</i> (错误负类的代价) 。一般情况下<i>FP</i>&lt;<i>FN</i>, 用户可以根据具体情况来定。</p>
                </div>
                <div class="p1">
                    <p id="105">设将实例 (<b><i>x</i></b>, <i>y</i>) 的类别<i>y</i>误分为<i>y</i>′的代价为<i>J</i><sub><i>y</i>, <i>y</i>′</sub> (<b><i>x</i></b>) , 则基分类器<i>C</i><sub><i>k</i></sub>在数据块<i>b</i><sub><i>i</i></sub>上的误分类总代价为:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>t</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">}</mo><mo>∈</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><msup><mi>y</mi><mo>′</mo></msup></msub><mi>J</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>y</mi><mo>, </mo><msup><mi>y</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>.</mo><mi>f</mi><msubsup><mrow></mrow><msup><mi>y</mi><mo>′</mo></msup><mi>k</mi></msubsup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">基分类器<i>C</i><sub><i>k</i></sub>在数据块<i>b</i><sub><i>i</i></sub>上的权值<i>ω</i><sub><i>ki</i></sub>为:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>t</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>+</mo><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>i</mi></mrow></msub><mo>+</mo><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mtext>r</mtext></msub><mo>+</mo><mi>α</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">其中, <i>α</i>→0, 作用是避免分母为零。最后对权值做归一化处理。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">3.3.2 淘汰策略</h4>
                <div class="p1">
                    <p id="111">如何淘汰一个原有的基分类器是一个值得关注的问题, 需要一个合适的淘汰策略来对分类器进行评估, 并删除性能差的分类器。设数据块<i>b</i><sub><i>i</i></sub>的大小为<i>d</i>, 则集成分类器<image id="112" type="" href="images/JSJK201908026_11200.jpg" display="inline" placement="inline"><alt></alt></image>在数据块<i>b</i><sub><i>i</i></sub>上的整体正确率如式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi></mrow></msub><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>Ι</mi></mstyle><msub><mrow></mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi></msub><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow><mi>Ν</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">其中, <i>s</i><sub><i>t</i></sub>是数据块<i>b</i><sub><i>i</i></sub>中的一个实例, <image id="115" type="" href="images/JSJK201908026_11500.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>s</i><sub><i>t</i></sub>) 表示指示函数, 若分类器对<i>s</i><sub><i>t</i></sub>分类正确, 则<image id="116" type="" href="images/JSJK201908026_11600.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>s</i><sub><i>t</i></sub>) =1;否则<image id="117" type="" href="images/JSJK201908026_11700.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>s</i><sub><i>t</i></sub>) =0。</p>
                </div>
                <div class="p1">
                    <p id="118">若设<i>C</i><sub><i>k</i></sub>是性能最差的一个基分类器, 将其从集成分类器中移除后的整体正确率如式 (5) 所示:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi><mo>-</mo><mo stretchy="false">{</mo><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">}</mo></mrow></msub><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>Ι</mi></mstyle><msub><mrow></mrow><mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi><mo>-</mo><mo stretchy="false">{</mo><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">}</mo></mrow></msub><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo></mrow><mi>Ν</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">则<i>C</i><sub><i>k</i></sub>在数据块<i>b</i><sub><i>i</i></sub>上对集成分类器的贡献值<i>Con</i><image id="121" type="" href="images/JSJK201908026_12100.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>C</i><sub><i>k</i></sub>) 如式 (6) 所示:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>n</mi><msub><mrow></mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi></msub><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><msub><mrow></mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi></msub><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi><mglyph src="e难字.jpg" height="60%" width="60%" /></mi><mo>-</mo><mo stretchy="false">{</mo><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">}</mo></mrow></msub><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">当<i>Con</i><image id="124" type="" href="images/JSJK201908026_12400.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>C</i><sub><i>k</i></sub>) 值为负时, 说明<i>C</i><sub><i>k</i></sub>使集成分类器的整体正确率降低了;当<i>Con</i><image id="125" type="" href="images/JSJK201908026_12500.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>C</i><sub><i>k</i></sub>) 值为正时, 说明<i>C</i><sub><i>k</i></sub>使集成分类器<image id="126" type="" href="images/JSJK201908026_12600.jpg" display="inline" placement="inline"><alt></alt></image>的整体正确率提高了。当下一个数据块训练出新基分类器后就替换掉贡献值最小的分类器。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.3.3 ECIDS算法描述</h4>
                <div class="p1">
                    <p id="128">ECIDS算法步骤描述如下:</p>
                </div>
                <div class="p1">
                    <p id="129"><b>Input</b>:数据流<i>DS</i>, 集成分类器个数<i>K</i>。</p>
                </div>
                <div class="p1">
                    <p id="130"><b>Output</b>:<i>K</i>个带权值的集成分类器<image id="131" type="" href="images/JSJK201908026_13100.jpg" display="inline" placement="inline"><alt></alt></image>。</p>
                </div>
                <div class="p1">
                    <p id="132">Begin</p>
                </div>
                <div class="p1">
                    <p id="133"><b>Step 1</b> 将均衡后的数据流<i>DS</i>分成大小相等的数据块<i>b</i><sub><i>i</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="134"><b>Step 2</b> 在<i>b</i><sub><i>i</i></sub>上训练新分类器<i>C</i>, 计算分类器<i>C</i>的随机均方误差<i>MSE</i><sub>r</sub>, 并计算其权值<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><msub><mrow></mrow><mi>C</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><msub><mrow></mrow><mtext>r</mtext></msub><mo>+</mo><mi>α</mi></mrow></mfrac></mrow></math></mathml>, 执行下一步。</p>
                </div>
                <div class="p1">
                    <p id="136"><b>Step 3</b> 当<i>C</i><sub><i>k</i></sub>∈<image id="137" type="" href="images/JSJK201908026_13700.jpg" display="inline" placement="inline"><alt></alt></image>时, 在数据块<i>b</i><sub><i>i</i></sub>上计算均方误差<i>MSE</i><sub><i>ik</i></sub>、误分类总代价<i>Cost</i><sub><i>ki</i></sub>, 最后计算其权值<i>ω</i><sub><i>ki</i></sub>, 执行Step 4。</p>
                </div>
                <div class="p1">
                    <p id="138"><b>Step 4</b> 计算基分类器<i>C</i><sub><i>k</i></sub>对集成分类器<image id="139" type="" href="images/JSJK201908026_13900.jpg" display="inline" placement="inline"><alt></alt></image>的贡献值<i>Con</i><image id="140" type="" href="images/JSJK201908026_14000.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>C</i><sub><i>k</i></sub>) , 执行Step 5。</p>
                </div>
                <div class="p1">
                    <p id="141"><b>Step 5</b> 当集成分类器<image id="142" type="" href="images/JSJK201908026_14200.jpg" display="inline" placement="inline"><alt></alt></image>中基分类器个数小于预设值<i>K</i>时, 将分类器<i>C</i>加入其中;否则用<i>C</i>替换掉<i>Con</i><image id="143" type="" href="images/JSJK201908026_14300.jpg" display="inline" placement="inline"><alt></alt></image><sub> (</sub><i>C</i><sub><i>k</i></sub>) 最低的分类器。</p>
                </div>
                <div class="p1">
                    <p id="144"><b>Step 6</b> 当<i>C</i><sub><i>k</i></sub>∈<image id="145" type="" href="images/JSJK201908026_14500.jpg" display="inline" placement="inline"><alt></alt></image>/{<i>C</i>}时, 在数据块<i>b</i><sub><i>i</i></sub>上增量式地训练分类器<i>C</i><sub><i>k</i></sub>, 采用加权投票策略进行数据流分析预测。</p>
                </div>
                <div class="p1">
                    <p id="146">End</p>
                </div>
                <h3 id="147" name="147" class="anchor-tag"><b>4 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="148">本文实验硬件环境是2.7 GHz处理器, 8 GB内存的PC机, 操作系统是Windows 10, 软件环境是大规模数据在线分析开源平台MOA (Massive Online Analysis) <citation id="264" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。实验数据集选取2个人工合成数据集 (SEA数据集, Rotating Spiral数据集) 和1个真实数据集 (Spam数据集) , 将ECIDS算法与3种经典数据流分类算法VFDT (Very Fast Decision Tree) <citation id="265" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、AUE2 (Accuracy Updated Ensemble 2) 和Learn<sup>++</sup>.NIE进行大量的实验对比分析。</p>
                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>4.1 数据集描述</b></h4>
                <div class="p1">
                    <p id="150"> (1) SEA数据集是经典突变式概念漂移数据集, 其中包含2次突变, 本文选取的SEA数据集是由MOA中的SEAGenerator数据生成器生成的1 000 000个实例。</p>
                </div>
                <div class="p1">
                    <p id="151"> (2) Rotating Spiral数据集是渐变式概念漂移的不均衡数据集, 本文选取的数据集中含5%正类样本的1 000 000个实例。</p>
                </div>
                <div class="p1">
                    <p id="152"> (3) Spam数据集是渐变型不均衡数据流, 包含9 324个实例, 每个实例包含500个属性, 2个类型类别。其中垃圾邮件占20%, 合法邮件占80%。用MOA中的ArffFileStream产生器将静态数据模拟成数据流。</p>
                </div>
                <h4 class="anchor-tag" id="153" name="153"><b>4.2 实验参数分析</b></h4>
                <div class="p1">
                    <p id="154">为了便于对比分析, 集成分类器中基分类器数目<i>K</i>设置为10。本文所提算法参数设置为:数据块大小<i>d</i>=1000, 过采样数量<i>N</i>=300, 最近邻数量<i>q</i>=6。其他算法均采用MOA中的默认设置。本文采用常用于衡量不均衡数据流的性能指标几何平均<i>G</i>-<i>means</i>, 如式 (7) 所示:</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>s</mi><mo>=</mo><msqrt><mrow><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156"><i>G</i>-<i>means</i>值是查全率<i>R</i> (Recall) 和查准率<i>P</i> (Precision) 的几何平均。只有正类和负类样本的分类精度都较高时, <i>G</i>-<i>means</i>值才比较高, 可以用来反映集成分类器的总体性能。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>4.3 不同数据块大小对算法性能影响</b></h4>
                <div class="p1">
                    <p id="158">为了验证数据块大小对算法性能的影响, 将数据块的大小取值设置在[500, 2000], 并在SEA数据集上进行测试, 结果如图2所示。</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908026_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数据块不同时平均G-means值" src="Detail/GetImg?filename=images/JSJK201908026_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 数据块不同时平均<i>G</i>-<i>means</i>值  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908026_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Average <i>G</i>-<i>means</i> of blocks different sizes</p>

                </div>
                <div class="p1">
                    <p id="160">从图2中可知, 随着数据块的增大, 算法构建分类器的数量增加, 平均<i>G</i>-<i>means</i>值也随之增加。但是, 随着数据块的持续增加, 平均<i>G</i>-<i>means</i>值又逐渐降低, 在数据块大小为1 000时, 平均<i>G</i>-<i>means</i>值达到峰值。</p>
                </div>
                <h4 class="anchor-tag" id="161" name="161"><b>4.4 不同算法性能对比</b></h4>
                <div class="p1">
                    <p id="162">首先在2个人工合成数据集上进行算法验证。图3和图4分别是在SEA数据集和Rotating Spiral数据集上的平均<i>G</i>-<i>means</i>值对比。</p>
                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908026_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 SEA数据集上的G-means值对比" src="Detail/GetImg?filename=images/JSJK201908026_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 SEA数据集上的<i>G</i>-<i>means</i>值对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908026_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 <i>G</i>-<i>means</i> comparison on SEA dataset</p>

                </div>
                <div class="p1">
                    <p id="164">从图3中可以分析出, 本文提出的ECIDS算法的<i>G</i>-<i>means</i>值最高, 性能最好, 和AUE2算法性能相差不大, 而VFDT算法的<i>G</i>-<i>means</i>值最低, 性能最差;在5×10<sup>5</sup>个实例左右, 经典算法的<i>G</i>-<i>means</i>值都有所降低, 而本文算法虽有波动, 但依然有较好的效果, 因为ECIDS算法比经典算法能更好地应对概念漂移, 建立基分类器应对变化。</p>
                </div>
                <div class="area_img" id="165">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908026_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Rotating Spiral数据集上的G-means值对比" src="Detail/GetImg?filename=images/JSJK201908026_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Rotating Spiral数据集上的<i>G</i>-<i>means</i>值对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908026_165.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 <i>G</i>-<i>means</i> comparison on Rotating Spiral dataset</p>

                </div>
                <div class="p1">
                    <p id="166">从图4中可以看出, 本文算法和经典算法的变化趋势基本一致, 其中VFDT算法波动最大, 本文算法能保持较高的分类精度。因为Rotating Spiral数据集是不均衡的渐变型概念漂移数据集, VFDT算法并不能很好地处理不均衡数据流, 而本文算法能够对数据流环境下的数据不均衡问题维持正类实例和负类实例在数据集中的均衡性, 保证算法能够及时有效地适应数据流, 因此可以保持较好的性能。</p>
                </div>
                <div class="p1">
                    <p id="167">图5是在真实数据集Spam数据集上算法的性能对比。</p>
                </div>
                <div class="area_img" id="168">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908026_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Spam数据集上的G-means值对比" src="Detail/GetImg?filename=images/JSJK201908026_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Spam数据集上的<i>G</i>-<i>means</i>值对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908026_168.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 <i>G</i>-<i>means</i> comparison on Spam dataset</p>

                </div>
                <div class="p1">
                    <p id="169">从图5中可以看出, 所有算法的波动都较大, 其中AUE2算法和VFDT算法的性能最差, 本文算法和Learn<sup>++</sup>.NIE算法的曲线较为平缓。这说明本文算法受概念漂移的影响较小。</p>
                </div>
                <h3 id="170" name="170" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="171">本文设计了一种基于集成学习的不均衡数据流分类算法。为了能够实现对不均衡数据流的处理和概念漂移的检测, 该算法考虑了不同特征的概念漂移 (突变型和渐变型) , 在不均衡数据集的基础上设计出一种有效的解决方案。通过实验表明, 该算法对含有概念漂移的不均衡数据流有较好的分类效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="194" type="formula" href="images/JSJK201908026_19400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">袁泉</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="195" type="formula" href="images/JSJK201908026_19500.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">郭江帆</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="196">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Knowledge discovery from data streams">

                                <b>[1]</b> Gama J, Ganguly A, Omitaomu O, et al.Knowledge discovery from data streams[J].Intelligent Data Analysis-Knowledge Discovery from Data Streams, 2009, 13 (3) :403-404.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mining concept-drifting data streams using ensemble classifiers">

                                <b>[2]</b> Wang Hai-xun, Fan Wei, Philip S Y, et al.Mining concept-drifting data streams using ensembles classifiers[C]//Proc of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2003) , 2003:226-235.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329121&amp;v=MDQ1MjhlcnFRVE1ud1plWnVIeWptVUxmSUpsNFNheE09TmlmT2ZiSzdIdEROckk5Rlora0dEWDQ0b0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Cohen L, Avrahami-Bakish G, Last M, et al.Real-time data mining of non-stationary data streams from sensor networks[J].Information Fusion, 2008, 9 (3) :344-353.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339404&amp;v=MDU5Mzd1ZHRGQ0RsVmIvT0lWOD1OajdCYXJPNEh0SE5ySXhNWU9zTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVi&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Widmer G, Kubat M.Learning in the presence of concept drift and hidden contexts[J].Machine Learning, 1996, 23 (1) :69-101.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The problem of concept drift:definitionsand related work">

                                <b>[5]</b> Tsymbal A.The problem of concept drift:Definitions and related work:Technical Report TCD-CS-2004-15[R].Dublin:Trinity College Dublin, 2004.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM14061800000195&amp;v=MDAzNzA0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDRTYXhNPU5pZklZN0s4SHRmTnA0OUZaT3NQRFhVOG9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Gama J, Žliobaite I, Bifet A, et al.A survey on concept drift adaptation[J].ACM Computing Surveys (CSUR) , 2014, 46 (4) :Article No.44.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Test Strategies for Cost-Sensitive Decision Trees">

                                <b>[7]</b> Charles X L, Victor S S, Yang Qiang.Test strategies for cost-sensitive decision trees[J].IEEE Transactions on Knowledge and Data Engineering, 2006, 18 (8) :1055-1067.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MDI0NTBmT2ZiSzdIdEROcVk5RlkrZ0dEM3M2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0U2F4TT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Sun Y, Kanmel M S, Wong A K C, et al.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning from Imbalanced Data">

                                <b>[9]</b> He H B, Garcia E A.Learning from imbalanced data[J].IEEE Transactions on Knowledge and Data Engineering, 2009, 21 (9) :1263-1284.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">

                                <b>[10]</b> Chawla N V, Bowyer K W, Hall L O, et al.SMOTE:Synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diversity analysis on imbalanced data sets by using ensemble models">

                                <b>[11]</b> Wang Shuo, Yao Xin.Diversity analysis on imbalanced data sets by using ensemble models[C]//Proc of the IEEE Symposium Series on Computational Intelligence and Data Mining (CIDM 2009) , 2009:324-331.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 Hu Xue-gang, He Jun-hong, Li Pei-pei.Data streams classification approach based on distance and sampling [J].Application Research of Computers, 2018, 35 (4) :992-995. (in Chinese) 
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201607032&amp;v=MTQwNzE0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVmI3Skx6N0JaYkc0SDlmTXFJOUdab1FLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Zhang Pan-pan, Yin Shao-hong.An ensemble classification algorithm for uncertain data stream containing concept drift[J].Computer Engineering &amp; Science, 2016, 38 (7) :1510-1516. (in Chinese) 
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A general framework for mining concept-drifting data streams with skewed distributions">

                                <b>[14]</b> Gao Jing, Fan Wei, Han Jia-wei, et al.A general framework for mining concept-drifting data streams with skewed distributions[C]//Proc of the 7th SIAM International Conference on Data Mining, 2007:3-14.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards incremental learning of nonstationary imbalanced data stream: A multiple selectively recursive approach">

                                <b>[15]</b> Chen Sheng, He Hai-bo.Towards incremental learning of nonstationary imbalanced data stream:A multiple selectively recursive approach[J].Evolving Systems, 2011, 2 (1) :35-50.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 Ouyang Zhen-zheng, Luo Jian-shu, Hu Dong-min, et al.An ensemble classifier framework for mining imbalanced data streams[J].Acta Electronica Sinica, 2010, 38 (1) :184-189. (in Chinese) 
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incremental Learning of Concept Drift fromStreaming Imbalanced Data">

                                <b>[17]</b> Ditzler G, Polikar R.Incremental learning of concept drift from streaming imbalanced data[J].IEEE Transactions on Knowledge and Data Engineering, 2013, 25 (10) :2283-2301.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13120400000960&amp;v=MjAwMTJCWG81b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0U2F4TT1OajdCYXJLN0g5UE1xNDlGWk9zUA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Mirza B, Lin Z P, Toh K A.Weighted online sequential extreme learning machine for class imbalance learning[J].Neural Processing Letters, 2013, 38 (3) :465-486.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM239273940810D7C32E2AC724EFBD687A&amp;v=MDYyMjlCdUhZZk9HUWxmQ3BiUTM1TkJod3JpNXdhaz1OaWZJWTdHN0Y5UExySVpCWk9NT0RBZyt2QlVSbnowTU8zamdxR2REQzhhU1RiM3VDT052RlNpV1dyN0pJRnBtYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Gomes H M, Barddal J P, Enembreck F, et al.A survey on ensemble learning for data stream classification[J].ACM Computing Surveys (CSUR) , 2017, 50 (2) :Article No.23.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A streaming ensemble algorithm (SEA) for large-scale classification">

                                <b>[20]</b> Street W N, Kim Y S.A streaming ensemble algorithm (SEA) for large-scale classification[C]//Proc of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2001:377-382.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MOA: Massive Online Analysis">

                                <b>[21]</b> Bifet A, Holmes G, Kirkby R, et al.MOA:Massive online analysis[J].The Journal of Machine Learning Research, 2010, 11 (5) :1601-1604.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mining high-speed data streams">

                                <b>[22]</b> Domingos P, Hulten G.Mining high-speed data streams[C]//Proc of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2007:71-80.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201804008&amp;v=MDAxNzg3Skx6N1NaTEc0SDluTXE0OUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 胡学钢, 何俊宏, 李培培.一种基于距离和采样机制的数据流分类方法[J].计算机应用研究, 2018, 35 (4) :992-995.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 张盼盼, 尹绍宏.隐含概念漂移的不确定数据流集成分类算法[J].计算机工程与科学, 2016, 38 (7) :1510-1516.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201001033&amp;v=MjA2MDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWYjdKSVRmVGU3RzRIOUhNcm85R1o0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 欧阳震诤, 罗建书, 胡东敏, 等.一种不平衡数据的集成分类模型[J].电子学报, 2010, 38 (1) :184-189.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201908026" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908026&amp;v=MTUzOTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWYjdKTHo3QlpiRzRIOWpNcDQ5SFlvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
