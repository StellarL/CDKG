<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132340925967500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJK201911015%26RESULT%3d1%26SIGN%3divn68BDu%252fDZ%252fsdRW3Q3RL3zSmUY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911015&amp;v=MTk4OTVxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnZMTHo3QlpiRzRIOWpOcm85RVlZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;2 卷积神经网络&lt;/b&gt; "><b>2 卷积神经网络</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="&lt;b&gt;3 激活区域处理算法AR&lt;/b&gt; "><b>3 激活区域处理算法AR</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="&lt;b&gt;4 实验结果分析&lt;/b&gt; "><b>4 实验结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;4.1 对比实验的设置&lt;/b&gt;"><b>4.1 对比实验的设置</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;4.2 参数选取&lt;/b&gt;"><b>4.2 参数选取</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;4.3 实验结果对比&lt;/b&gt;"><b>4.3 实验结果对比</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;4.4 不使用数据增强下的性能&lt;/b&gt;"><b>4.4 不使用数据增强下的性能</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;4.5 对卷积神经网络的影响&lt;/b&gt;"><b>4.5 对卷积神经网络的影响</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;4.6 对随机遮挡的鲁棒性&lt;/b&gt;"><b>4.6 对随机遮挡的鲁棒性</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;4.7 算法复杂性分析&lt;/b&gt;"><b>4.7 算法复杂性分析</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;4.8 与预训练模型相结合&lt;/b&gt;"><b>4.8 与预训练模型相结合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#149" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="图1 卷积神经网络">图1 卷积神经网络</a></li>
                                                <li><a href="#89" data-title="图2 在训练ResNet-18时使用激活区域处理算法对Cifar 10图像进行遮挡的可视化">图2 在训练ResNet-18时使用激活区域处理算法对Cifar 10图像进行遮挡的可视化</a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表1 ResNet-18的网络结构以及选取不同卷积层 输出特征图制作图像掩膜的实验结果&lt;/b&gt;"><b>表1 ResNet-18的网络结构以及选取不同卷积层 输出特征图制作图像掩膜的实验结果</b></a></li>
                                                <li><a href="#124" data-title="图3 在ResNet-18中不同卷积层对不同类别的Cifar10图像提取特征的可视化">图3 在ResNet-18中不同卷积层对不同类别的Cifar10图像提取特征的可视化</a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表2 AR算法使用不同填充方式进行遮挡的错误率&lt;/b&gt;"><b>表2 AR算法使用不同填充方式进行遮挡的错误率</b></a></li>
                                                <li><a href="#128" data-title="图4 不同超参数对算法性能的影响">图4 不同超参数对算法性能的影响</a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表3 与当前一些新方法的错误率对比&lt;/b&gt;"><b>表3 与当前一些新方法的错误率对比</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表4 在不使用数据增强的情况下&lt;/b&gt;, &lt;b&gt;本文算法与不同基线的错误率对比&lt;/b&gt;"><b>表4 在不使用数据增强的情况下</b>, <b>本文算法与不同基线的错误率对比</b></a></li>
                                                <li><a href="#139" data-title="图5 比较使用不同方式训练的模型的不同卷积层的激活情况">图5 比较使用不同方式训练的模型的不同卷积层的激活情况</a></li>
                                                <li><a href="#140" data-title="图6 对随机遮挡图像的鲁棒性">图6 对随机遮挡图像的鲁棒性</a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表5 模型的物理内存&lt;/b&gt;、 &lt;b&gt;GPU内用和每周期的训练时间比较&lt;/b&gt;"><b>表5 模型的物理内存</b>、 <b>GPU内用和每周期的训练时间比较</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表6 使用预先训练的Xception模型 对胎盘组织细胞图像的实验分类结果&lt;/b&gt;"><b>表6 使用预先训练的Xception模型 对胎盘组织细胞图像的实验分类结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Krizhevsky A,Sutskever I,Hinton G.ImageNet classification with deep convolutional neural networks[C]//Proc of the International Conference on Neural Information Processing Systems,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[1]</b>
                                         Krizhevsky A,Sutskever I,Hinton G.ImageNet classification with deep convolutional neural networks[C]//Proc of the International Conference on Neural Information Processing Systems,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[2]</b>
                                         Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,39(4):640-651.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Vinyals O,Toshev A,Bengio S,et al.Show and tell:A neural image caption generator[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:3156-3164." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show and tell:A neural image caption generator">
                                        <b>[3]</b>
                                         Vinyals O,Toshev A,Bengio S,et al.Show and tell:A neural image caption generator[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:3156-3164.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Toshev A,Szegedy C.DeepPose:Human pose estimation via deep neural networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2014:1653-1660." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepPose:Human pose estimationvia deep neural netw orks">
                                        <b>[4]</b>
                                         Toshev A,Szegedy C.DeepPose:Human pose estimation via deep neural networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2014:1653-1660.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     Lecun Y,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Krizhevsky A,Hinton G.Learning multiple layers of features from tiny images[R].Toronto:University of Toronto,2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multiple layers of features from tiny images">
                                        <b>[6]</b>
                                         Krizhevsky A,Hinton G.Learning multiple layers of features from tiny images[R].Toronto:University of Toronto,2009.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     Srivastava N,Hinton G,Krizhevsky A,et al.Dropout:A simple way to prevent neural networks from over-fitting[J].Journal of Machine Learning Research,2014,15(1):1929-1958.</a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.</a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Wan L,Zeiler M,Zhang S,et al.Regularization of neural networks using dropconnect[C]//Proc of Machine Learning Research,2013:1058-1066." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Regularization of neural networks using dropconnect">
                                        <b>[9]</b>
                                         Wan L,Zeiler M,Zhang S,et al.Regularization of neural networks using dropconnect[C]//Proc of Machine Learning Research,2013:1058-1066.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of the International Conference on Machine Learning,2015:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[10]</b>
                                         Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of the International Conference on Machine Learning,2015:1-11.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Tompson J,Goroshin R,Jain A,et al.Efficient object localization using convolutional networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:648-656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient object localization using Convolutional Networks">
                                        <b>[11]</b>
                                         Tompson J,Goroshin R,Jain A,et al.Efficient object localization using convolutional networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:648-656.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Park S,Kwak N.Analysis on the dropout effect in convolutional neural networks[C]//Proc of Asian Conference on Computer Vision,2016:189-204." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analysis on the dropout effect in convolutional neural networks">
                                        <b>[12]</b>
                                         Park S,Kwak N.Analysis on the dropout effect in convolutional neural networks[C]//Proc of Asian Conference on Computer Vision,2016:189-204.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Zhong Z,Zheng L,Kang G,et al.Random erasing data augmentation[J].arXiv:1708.04896,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Random erasing data augmentation">
                                        <b>[13]</b>
                                         Zhong Z,Zheng L,Kang G,et al.Random erasing data augmentation[J].arXiv:1708.04896,2017.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Li Xiao-xin,Liang Rong-hua.A review for face recognition with occlusion:From subspace regression to deep learning[J].Chinese Journal of Computers,2018,41(1):177-207.(in Chinese)</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Liu Wan-jun,Dong Shuai-han,Qu Hai-cheng.Anti-occlusion visual tracking algorithm based on spatio-temporal context learning[J].Journal of Image and Graphics,2016,21(8):1057-1067.(in Chinese)</a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     Chu Jun,Zhu Tao,Miao Jun,et al.Target tracking based on occlusion detection and spatio-temporal context information[J].Pattern Recognition and Artificial Intelligence,2017,30(8):718-727.(in Chinese)</a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Vincent P,Larochelle H,Lajoie I,et al.Stacked denoising autoencoders:Learning useful representations in a deep network with a local denoising criterion[J].Journal of Machine Learning Research,2010,11(12):3371-3408." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion">
                                        <b>[17]</b>
                                         Vincent P,Larochelle H,Lajoie I,et al.Stacked denoising autoencoders:Learning useful representations in a deep network with a local denoising criterion[J].Journal of Machine Learning Research,2010,11(12):3371-3408.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                     Pathak D,Krahenbuhl P,Donahue J,et al.Context encoders:Feature learning by inpainting[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:2536-2544.</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" He K,Zhang X,Ren S,et al.Identity mappings in deep residual networks[C]//Proc of the European Conference on Computer Vision,2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[19]</b>
                                         He K,Zhang X,Ren S,et al.Identity mappings in deep residual networks[C]//Proc of the European Conference on Computer Vision,2016:630-645.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Zagoruyko S,Komodakis N.Wide residual networks[J].arXiv:1605.07146,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wide residual networks">
                                        <b>[20]</b>
                                         Zagoruyko S,Komodakis N.Wide residual networks[J].arXiv:1605.07146,2016.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Xie S,Girshick R,Doll&#225;r P,et al.Aggregated residual transformations for deep neural networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2017:5987-5995." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">
                                        <b>[21]</b>
                                         Xie S,Girshick R,Doll&#225;r P,et al.Aggregated residual transformations for deep neural networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2017:5987-5995.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Chollet F.Xception:Deep learning with depthwise separable convolutions[J].arXiv preprint arXiv:1610-02357,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">
                                        <b>[22]</b>
                                         Chollet F.Xception:Deep learning with depthwise separable convolutions[J].arXiv preprint arXiv:1610-02357,2016.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Xiao H,Rasul K,Vollgraf R.Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms[J].arXiv:1708.07747,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms">
                                        <b>[23]</b>
                                         Xiao H,Rasul K,Vollgraf R.Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms[J].arXiv:1708.07747,2017.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" Ferlaino M,Glastonbury C A,Motta-Mejia C,et al.Towards deep cellular phenotyping in placental histology[J].arXiv:1804.03270,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards deep cellular phenotyping in placental histology">
                                        <b>[24]</b>
                                         Ferlaino M,Glastonbury C A,Motta-Mejia C,et al.Towards deep cellular phenotyping in placental histology[J].arXiv:1804.03270,2018.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" Lin M,Chen Q,Yan S.Network in network[J].arXiv:1312.4400,2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network in network">
                                        <b>[25]</b>
                                         Lin M,Chen Q,Yan S.Network in network[J].arXiv:1312.4400,2013.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" Feng J,Ni B,Tian Q,et al.Geometric-norm feature pooling for image classification[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2011:2609-2704." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Geometric lp-norm feature pooling for image classifica-tion">
                                        <b>[26]</b>
                                         Feng J,Ni B,Tian Q,et al.Geometric-norm feature pooling for image classification[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2011:2609-2704.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" Bengio Y.Learning deep architectures for AI[J].Foundations and Trendsin Machine Learning,2009,2(1):1-127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for ai">
                                        <b>[27]</b>
                                         Bengio Y.Learning deep architectures for AI[J].Foundations and Trendsin Machine Learning,2009,2(1):1-127.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" Lu B,Hu Q,Hui Y,et al.Feature reinforcement network for image classification[C]//Proc of the IEEE International Conference on Multimedia and Expo,2018:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature reinforcement network for image classification">
                                        <b>[28]</b>
                                         Lu B,Hu Q,Hui Y,et al.Feature reinforcement network for image classification[C]//Proc of the IEEE International Conference on Multimedia and Expo,2018:1-6.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" >
                                        <b>[29]</b>
                                     Zhang K,Guo L,Gao C.Optimization method of residual networks of residual networks for image classification[C]//Proc of the Big Data and Smart Computing,2018:321-325.附中文参考文献:</a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_14" title=" 李小薪,梁荣华.有遮挡人脸识别综述:从子空间回归到深度学习[J].计算机学报,2018,41(1):177-207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801011&amp;v=MDU0NDdlWmVSbUZ5L25WcnZMTHo3QmRyRzRIOW5Ncm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         李小薪,梁荣华.有遮挡人脸识别综述:从子空间回归到深度学习[J].计算机学报,2018,41(1):177-207.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_15" title=" 刘万军,董帅含,曲海成.时空上下文抗遮挡视觉跟踪[J].中国图象图形学报,2016,21(8):1057-1067." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201608010&amp;v=MjE3MjZHNEg5Zk1wNDlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvblZydkxQeXJmYkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         刘万军,董帅含,曲海成.时空上下文抗遮挡视觉跟踪[J].中国图象图形学报,2016,21(8):1057-1067.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_16" title=" 储珺,朱陶,缪君,等.基于遮挡检测和时空上下文信息的目标跟踪算法[J].模式识别与人工智能,2017,30(8):718-727." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201708007&amp;v=MTQ3NzFNcDQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnZMS0Q3WWJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         储珺,朱陶,缪君,等.基于遮挡检测和时空上下文信息的目标跟踪算法[J].模式识别与人工智能,2017,30(8):718-727.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(11),2007-2016 DOI:10.3969/j.issn.1007-130X.2019.11.015            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络的图像数据增强算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E8%8A%B8&amp;code=09140808&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋芸</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B5%B7&amp;code=15344374&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E8%8E%89&amp;code=11390184&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈莉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%B6%E7%94%9F%E9%91%AB&amp;code=43338816&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陶生鑫</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8C%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0012645&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西北师范大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提升卷积神经网络的泛化能力和降低过拟合的风险是深度卷积神经网络的研究重点。遮挡是影响卷积神经网络泛化能力的关键因素之一,通常希望经过复杂训练得到的模型能够对遮挡图像有良好的泛化性。为了降低模型过拟合的风险和提升模型对随机遮挡图像识别的鲁棒性,提出了激活区域处理算法,在训练过程中对某一卷积层的最大激活特征图进行处理后对输入图像进行遮挡,然后将被遮挡的新图像作为网络的新输入并继续训练模型。实验结果表明,提出的算法能够提高多种卷积神经网络模型在不同数据集上的分类性能,并且训练好的模型对随机遮挡图像的识别具有非常好的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据增强;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    蒋芸(1970-),女,浙江绍兴人,博士,教授,研究方向为数据挖掘,粗糙集理论及应用。E-mail:jiangyun@nwnu.edu.cn,通信地址:730070甘肃省兰州市安宁区西北师范大学计算机科学与工程学院;
                                </span>
                                <span>
                                    张海(1995-),男,江西赣州人,硕士,CCF会员(91466G),研究方向为数据挖掘。E-mail:haicheung1995@gmail.com,通信地址:730070甘肃省兰州市安宁区西北师范大学计算机科学与工程学院;
                                </span>
                                <span>
                                    陈莉,通信地址:730070甘肃省兰州市安宁区西北师范大学计算机科学与工程学院;
                                </span>
                                <span>
                                    陶生鑫,通信地址:730070甘肃省兰州市安宁区西北师范大学计算机科学与工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61962054);</span>
                                <span>2016年甘肃省科技计划资助自然科学基金(1606RJZA047);</span>
                                <span>2012年度甘肃省高校基本科研业务费专项资金;</span>
                                <span>甘肃省高校研究生导师项目(1201-16);</span>
                                <span>西北师范大学第三期知识与创新工程科研骨干项目(nwnu-kjcxgc-03-67);</span>
                    </p>
            </div>
                    <h1><b>An image data augmentation algorithm based on convolutional neural networks</b></h1>
                    <h2>
                    <span>JIANG Yun</span>
                    <span>ZHANG Hai</span>
                    <span>CHEN Li</span>
                    <span>TAO Sheng-xin</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering,Northwest Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Improving the generalization ability and reducing the over-fitting risk is the research focus of deep convolutional neural networks. Occlusion is one of the critical factors affecting the generalization ability of convolutional neural networks. It is usually hoped that the models after complex training can have a good generalization for occlusion images.In order to reduce the over-fitting risk and improve the robustness of the model to random occlusion image recognition, this paper proposes an activation feature processing algorithm. During the training process, the input image is occluded by processing the maximum activation feature map of a convolutional layer, then the occluded new image is used as a new input to the network to go on training the model. The experimental results show that the proposed algorithm can improve the classification performance of multiple convolutional neural network models on different datasets and the trained models have excellent robustness to the identification of random occlusion images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20augmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data augmentation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JIANG Yun,born in 1970,PhD,professor,her research interests include data mining,and rough set theory &amp;amp; applications.Address:College of Computer Science and Engineering,Northwest Normal University,Anning District,Lanzhou 730070,Gansu,P.R. China;
                                </span>
                                <span>
                                    ZHANG Hai,born in 1995,MS,CCF member(91466G),his research interest includes data mining.Address:College of Computer Science and Engineering,Northwest Normal University,Anning District,Lanzhou 730070,Gansu,P.R. China;
                                </span>
                                <span>
                                    CHEN Li,Address:College of Computer Science and Engineering,Northwest Normal University,Anning District,Lanzhou 730070,Gansu,P.R. China;
                                </span>
                                <span>
                                    TAO Sheng-xin,Address:College of Computer Science and Engineering,Northwest Normal University,Anning District,Lanzhou 730070,Gansu,P.R. China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="67" name="67" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="68">近年来,深度学习在计算机视觉领域取得了巨大进步,在许多具有挑战性的视觉任务中取得了最好的性能,如图像分类<citation id="156" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、语义分割<citation id="157" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、目标检测<citation id="158" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和人体姿势识别<citation id="159" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。这些性能的大幅提升大部分可以归功于卷积神经网络CNN(Convolutional Neural Network)<sup></sup><citation id="160" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,它能够学习图像的复杂层次特征。深度卷积神经网络通常包含几千万到几亿个学习参数,这些参数为复杂的图像识别任务提供必要的表示能力,但是随着网络越来越复杂以及参数的增加,其在训练过程中过拟合的风险随之增加,泛化能力也随之变差。在深度学习中,研究者提出了很多用于解决卷积神经网络模型过拟合问题的方法,包括正则化方法(Regularization)<citation id="161" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Dropout算法<citation id="162" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、数据增强(Data Augmentation)<citation id="164" type="reference"><link href="3" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>、批归一化(Batch Normalization)算法<citation id="163" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="69">正则化方法<citation id="165" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>是常用于解决神经网络过拟合的方法,包括<i>L</i><sub>1</sub>正则化、<i>L</i><sub>2</sub>正则化等,这些正则化方法不仅可以控制模型的复杂度,提高模型的泛化能力,而且还可以约束模型的特性,例如稀疏、平滑等特性。在数学公式上体现为在最优化损失函数后面加上正则化项,也称为惩罚项,用于限制模型权重参数。批归一化算法<citation id="166" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>通过确保在深度神经网络训练过程中每一层神经网络的输入保持同分布来使得激活输入值落在非线性函数对输入比较敏感的区域,加速模型的收敛速度,也可以正则化模型,有效地防止模型过拟合。</p>
                </div>
                <div class="p1">
                    <p id="70">在图像识别领域中,图像包含着各种巨大变化因素的高维数据,对训练集图像进行平移、旋转几个像素的数据增强<citation id="167" type="reference"><link href="3" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>操作通常可以大大改善模型的泛化能力,降低过拟合风险,提高模型的鲁棒性。由于数据增强的有效性、可扩展性且易于实施,因此其广泛用于计算机视觉领域中。常用的数据增强方式有:旋转、翻转、裁剪、添加噪声、平移、错切变换等,通过这些方式对输入图像进行扩充,旨在通过扩充训练集图像来防止过拟合。虽然数据增强方法简单有效,但是对于不同数据集,通常需要人工设计不同的数据增强策略,因此需要丰富的实验经验来寻找一个最佳的数据增强策略。</p>
                </div>
                <div class="p1">
                    <p id="71">由Srivastava等<citation id="168" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的Dropout算法在模型训练期间以一定的丢失概率将隐藏层单元激活设置为零,在评估网络性能时保留所有激活,并根据丢失概率缩放得到输出。在全连接神经网络中使用Dropout算法能够得到非常好的正则化效果,有效减轻了过拟合的问题,提高了网络的鲁棒性与泛化能力,阻碍了相邻特征检测器相关性。但是,Dropout算法对卷积神经网络的改善效果并不那么好,很大程度上归因于2个因素:首先,卷积层已经具有比全连接层更少的参数;其次,图像中的相邻像素共享大部分相同的信息,如果它们中的任何一个像素被丢弃,那么它们包含的信息可能仍然会从仍处于激活状态的相邻像素传递。由于这些原因,Dropout算法虽然能够增加卷积神经网络对输入噪声的鲁棒性,但是不能对卷积神经网络起到模型平均效应。为了提高卷积层中Dropout算法的有效性,已经有很多研究者对标准Dropout算法进行改进。Tompson等<citation id="169" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出的SpatialDropout通过随机丢弃整个特征图而不是单个像素,有效地绕过相邻像素传递类似信息的问题。在更有针对性的方法中,Park等<citation id="170" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了max-drop,它以一定的概率降低了特征映射或通道的最大激活,虽然这种方法在某些情况下比卷积层上的标准Dropout算法表现更好,但是在使用批归一化的卷积神经网络中使用时,max-drop和SpatialDropout都比标准Dropout算法的性能更差。</p>
                </div>
                <div class="p1">
                    <p id="72">提升模型的泛化能力一直是深度学习领域中的研究重点。通常我们希望训练好的模型可以自动处理随机遮挡,并且在预测新数据时依旧表现良好,而不仅仅是对基础数据集数据分布的表示。遮挡是影响卷积神经网络泛化能力的关键因素之一<citation id="171" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。李小薪等<citation id="172" type="reference"><link href="29" rel="bibliography" /><link href="61" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">14</a>]</sup></citation>从鲁棒分类器的设计和鲁棒特征提取2方面回顾了现有的遮挡人脸识别方法,并指出识别遮挡人脸的困难性主要体现在由遮挡所引发的特征损失、对准误差和局部混叠等方面。刘万军等<citation id="173" type="reference"><link href="31" rel="bibliography" /><link href="63" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">15</a>]</sup></citation>提出一种时空上下文抗遮挡视觉跟踪算法,能够用于光照变化、目标旋转、遮挡等复杂情况下的视觉目标跟踪,具有一定的实时性和高效性,尤其是在目标发生遮挡情况下具有很好的抗遮挡能力和较快的运行速度。储珺等<citation id="174" type="reference"><link href="33" rel="bibliography" /><link href="65" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">16</a>]</sup></citation>提出基于遮挡检测和时空上下文信息的目标跟踪算法,较好地解决了复杂场景下较严重的静态遮挡和动态遮挡问题。当图像的某些部分被遮挡时,强分类模型能够从整个图像结构中提取出全局特征并正确分类。然而,采用所收集的样本训练的网络模型通常在样本被遮挡方面表现出的泛化效果有限。在所有训练对象都清晰可见没有发生遮挡时,训练出来的卷积神经网络能在未被遮挡的测试图像中取得非常好的效果,但是由于卷积神经网络模型的泛化能力有限,可能无法识别部分被遮挡的对象。</p>
                </div>
                <div class="p1">
                    <p id="73">降噪自动编码器<citation id="175" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>和上下文自动编码器<citation id="176" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>通过破坏输入图像并要求网络使用上下文像素来重构图像,以确定如何最好地降噪和填充空白使得模型更好地工作。这2种方法依赖无监督方式来学习如何从图像中更好地学习全局特征,而不仅仅是简单地学习标识特征。我们认为,模型能否利用好图像的上下文信息(即能否提取出全局特征)和能否降低遮挡区域噪声带来的影响,是解决图像遮挡识别问题的一个方向。</p>
                </div>
                <div class="p1">
                    <p id="74">受现有研究的启发,我们考虑根据上一个训练周期中网络的激活情况来自动对输入图像进行有针对的遮挡,以迫使网络使用更多的全局特征进行决策,而不是根据少量局部特征进行决策。这种方式与Dropout算法类似,但有3个重要的区别:(1)此方式仅对输入图像进行有针对的丢弃,而不是对中间层的特征像素进行随机丢弃;(2)此方式对输入图像部分连续区域进行遮挡,而不是随机对单个像素进行丢弃,有效地降低了相邻特征检测器之间的相关性;(3)我们希望能提升模型对遮挡识别的鲁棒性,而Dropout算法不能有效地应对遮挡问题。</p>
                </div>
                <div class="p1">
                    <p id="75">本文提出了一种在训练过程中处理卷积层输出的激活特征图,然后根据特征图的激活区域来实现有针对地对输入图像进行遮挡的算法。为了评估算法的性能,将其与ResNet(Residual  Network)<citation id="177" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、WRN(Wide Residual Networks)<citation id="178" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、ResNeXt<citation id="179" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和Xception<citation id="180" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>等网络结合,在Cifar<citation id="181" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Fashion-MNIST<citation id="182" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和胎盘组织细胞图像<citation id="183" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>等数据集上进行了实验,取得了非常有竞争力的实验结果。</p>
                </div>
                <div class="p1">
                    <p id="76">本文的主要贡献如下:(1)提出了激活区域处理算法AR(Activation Region processing algorithm),这是一种轻量计算的算法,以非常低的内存消耗和训练时间为代价,在不增加额外训练参数和不影响测试时间的情况下,很好地提升了多种卷积神经网络模型在多个数据集上的性能;(2)结合激活区域处理算法训练出来的模型,在随机遮挡图像识别方面具有很好的鲁棒性;(3)激活区域处理算法是现有数据增强和正则化方法的补充,在与现有数据增强算法、批归一化等方法结合时,本文提出的算法能进一步提高模型的性能;(4)激活区域处理算法能够提高模型深层卷积层的激活强度,这表明算法能够鼓励模型更好地利用图像的全局特征进行决策,而不是依赖少数局部区域的激活特征来决策。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>2 卷积神经网络</b></h3>
                <div class="p1">
                    <p id="78">卷积神经网络主要由输入层、卷积层、池化层、全连接层和输出层构成,如图1所示。卷积层主要使用指定数量和指定感受野大小的卷积核对输入图像或上一层的输出特征进行卷积操作,计算整个卷积核和输入图像或特征图的相应位置的内积,并加上一个偏置项来提取相关图像特征图,再将提取的特征图输入至非线性激活函数上得到激活后的特征图并作为卷积层的输出。设第<i>i</i>层卷积层的输入记为<i><b>x</b></i><sup><i>i</i></sup><sup>-1</sup>,输出为<i><b>x</b></i><sup><i>i</i></sup>,⨂代表卷积,卷积核的参数权值为<i><b>w</b></i><sup><i>i</i></sup>,偏置项为<i>b</i><sup><i>i</i></sup>,激活函数为<i>σ</i><sup><i>i</i></sup>(·),则对应输出的激活特征<i><b>x</b></i><sup><i>i</i></sup>为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi>σ</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>⊗</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mi>i</mi></msup><mo>+</mo><mi>b</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911015_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 卷积神经网络" src="Detail/GetImg?filename=images/JSJK201911015_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 卷积神经网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911015_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 1 <i>Convolutional neural network</i></p>

                </div>
                <div class="p1">
                    <p id="81">通常在卷积神经网络中的连续卷积层之间周期性地插入池化层。池化层的作用主要有:(1)对激活特征图进行降维,减少网络中的参数数量和计算量;(2)保持特征尺度不变特性;(3)在一定程度上降低过拟合。池化层在输入的每个通道上独立操作,并在空间上调整其大小。池化方法主要有最大池化<citation id="184" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、平均池化<citation id="185" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>、L<sub>p</sub>范数池化<citation id="186" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>等。记池化层的输入为<i><b>x</b></i><sup><i>i</i></sup>,则对应池化层的输出为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mtext>p</mtext><mtext>o</mtext><mtext>o</mtext><mtext>l</mtext></mrow><mi>i</mi></msubsup><mo>=</mo><mi>s</mi><mi>u</mi><mi>b</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">在对图像进行多次卷积池化操作后,卷积神经网络通过将三维激活特征图展开后得到的一维激活特征向量作为全连接层的输入,通过全连接层对特征进行分类,得到基于输入图像的概率分布。卷积神经网络的实质是通过多次数据变换或降维,将输入图像映射到一个新的数学模型。记卷积神经网络的输入样本为<i><b>X</b></i>,权值参数为<i><b>W</b></i>,偏置参数为<i>b</i>,输出为<i><b>Y</b></i>,第<i>i</i>个类别标签记为<i>y</i><sub><i>i</i></sub>,则将模型样本<i><b>x</b></i>预测为<i>y</i><sub><i>i</i></sub>的概率<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>¯</mo></mover></math></mathml><sub><i>i</i></sub>为:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>¯</mo></mover><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo>;</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">本文使用图像分类领域中常用的交叉熵函数作为损失函数。设损失函数为<i>L</i>(<i><b>W</b></i>,<i>b</i>),样本类别数为<i>N</i>,则损失值的计算如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>y</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /><mi>log</mi><mspace width="0.25em" /><mover accent="true"><mi>y</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">通过式(1)计算得到损失值后,卷积神经网络通过梯度下降法<citation id="187" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>对损失值进行反向传播,从输出层开始向输入层逐层更新卷积神经网络的可训练参数<i><b>W</b></i>和<i>b</i>。设<i><b>x</b></i><sup><i>i</i></sup>是第<i>i</i>层输出,也是第<i>i</i>+1层输入,学习率参数为<i>η</i>,则由式(1)和求偏导链式法则可以得到反向传播过程中第<i>i</i>层求偏导的梯度计算公式和参数调整公式分别如式(2)～式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>n</mi></msup></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac><mo>⋯</mo><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>i</mi></msup><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>b</mi><msup><mrow></mrow><mi>i</mi></msup><mo>=</mo><mi>b</mi><msup><mrow></mrow><mi>i</mi></msup><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>b</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h3 id="90" name="90" class="anchor-tag"><b>3 激活区域处理算法AR</b></h3>
                <div class="p1">
                    <p id="91">设网络的输入图像为<i><b>I</b></i>,维度为<i>C</i>×<i>H</i>×<i>W</i>。选取第<i>i</i>层卷积层的输出<i><b>x</b></i><sup><i>i</i></sup>,维度为<i>C</i><sup><i>i</i></sup>×<i>H</i><sup><i>i</i></sup>×<i>W</i><sup><i>i</i></sup>。使用双线性插值(Bilinear Interpolation)方法对<i><b>x</b></i><sup><i>i</i></sup>上采样得到与输入图像相同尺寸的特征图<i><b>x</b></i>=<i>upsampling</i>(<i><b>x</b></i><sup><i>i</i></sup>),维度为<i>C</i><sup><i>i</i></sup>×<i>H</i>×<i>W</i>,即得到<i>C</i><sup><i>i</i></sup>幅<i>H</i>×<i>W</i>大小的特征图,然后比较特征图的最大像素值,从中取像素值最大的特征图作为最大激活特征图<i><b>C</b></i><sub>max</sub>。计算<i><b>C</b></i><sub>max</sub>的均值<i>mean</i>(<i><b>C</b></i><sub>max</sub>)与标准差<i>std</i>(<i><b>C</b></i><sub>max</sub>),设用来调整阈值大小的参数为<i>λ</i>∈[0,1],则阈值选取公式如式(5)所示:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>=</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><mo>×</mo><mi>s</mi><mi>t</mi><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">设图像掩膜为<i><b>M</b></i>,维度为<i>H</i>×<i>W</i>,取最大激活特征图<i><b>C</b></i><sub>max</sub>中的每一点<i>C</i><sub>(</sub><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub><sub>)</sub>与阈值<i>T</i>进行比较,其中,<i>m</i>∈[0,<i>H</i>),<i>n</i>∈[0,<i>W</i>),如果特征图中某一点的值大于阈值<i>T</i>,则图像掩膜中对应点的值设为0,如果特征图中某点的值小于阈值<i>T</i>,则图像掩膜中对应点设为1,即对图像掩膜<i><b>M</b></i>中的任意一点<i>M</i><sub>(</sub><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub><sub>)</sub>的计算公式如式(6)所示:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>,</mo><mi>C</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>≥</mo><mi>Τ</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>,</mo><mi>C</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>&lt;</mo><mi>Τ</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中,图像掩膜中值为0的点组成的区域即为遮挡区域。由于<i><b>C</b></i><sub>max</sub>中大于阈值<i>T</i>的点并不一定相邻,因此遮挡区域不一定是连续的,并且是不规则的。遮挡区域<i><b>O</b></i>可以表示如下:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ο</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi>C</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>≥</mo><mi>Τ</mi><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">算法1详细描述了在卷积神经网络中使用激活区域处理算法的过程。图2展示了在不同数据集和不同网络结构上基于激活区域处理算法对输入图像进行遮挡的效果。受降噪自动编码器研究的启发,考虑使用不同噪声值对被遮挡区域进行填充:(1)使用0对遮挡区域填充,记为Fill-0;(2)使用1对遮挡区域填充,记为Fill-1;(3)对于RGB彩色图像,使用ImageNet数据集的RGB各通道的平均像素值[0.4902,0.4784,0.4471]对遮挡区域填充;对于灰度图像,使用0.478 4对遮挡区域填充,记为Fill-I;(4)使用[0,1]中的随机噪声对遮挡区域填充,记为Fill-R。</p>
                </div>
                <div class="p1">
                    <p id="98"><b>算法1</b> 激活区域处理算法</p>
                </div>
                <div class="p1">
                    <p id="99"><b>输入</b>:输入图像<i><b>I</b></i>,图像的宽<i>W</i>和高<i>H</i>,图像随机遮挡的概率<i>P</i>,参数<i>λ</i>,前一个训练周期中以图像<i><b>I</b></i>为输入的第<i>i</i>层卷积层的输出特征图<i><b>x</b></i><sub><i>i</i></sub>,当前训练周期数<i>e</i>。</p>
                </div>
                <div class="p1">
                    <p id="100"><b>输出</b>:被遮挡的图像<i><b>I</b></i>′。</p>
                </div>
                <div class="p1">
                    <p id="101">1.<i>p</i>=<i>rand</i>(0,1);</p>
                </div>
                <div class="p1">
                    <p id="102">2. if <i>p</i>≥<i>P</i> or <i>e</i>==0 then</p>
                </div>
                <div class="p1">
                    <p id="103">3.  <i><b>I</b></i>′=<i><b>I</b></i></p>
                </div>
                <div class="p1">
                    <p id="104">4. else</p>
                </div>
                <div class="p1">
                    <p id="105">5.  <i><b>x</b></i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>u</mtext><mtext>p</mtext></mrow><mi>i</mi></msubsup></mrow></math></mathml>=<i>upsampling</i>(<i><b>x</b></i><sup><i>i</i></sup>);</p>
                </div>
                <div class="p1">
                    <p id="106">6.  <i><b>C</b></i><sub>max</sub>=max(<i><b>x</b></i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>u</mtext><mtext>p</mtext></mrow><mi>i</mi></msubsup></mrow></math></mathml>);</p>
                </div>
                <div class="p1">
                    <p id="107">7.  <i>T</i>=<i>mean</i>(<i><b>C</b></i><sub>max</sub>)+<i>λ</i>×<i>std</i>(<i><b>C</b></i><sub>max</sub>);</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>8</mn><mo>.</mo><mtext> </mtext><mtext> </mtext><mi>Μ</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>,</mo><mi>C</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>≥</mo><mi>Τ</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>,</mo><mi>C</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msub><mo>&lt;</mo><mi>Τ</mi></mtd></mtr></mtable></mrow><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">9.  <i><b>I</b></i>′=<i><b>I</b></i>×<i><b>M</b></i>;</p>
                </div>
                <div class="p1">
                    <p id="110">10. end if</p>
                </div>
                <div class="p1">
                    <p id="111">11.return <i><b>I</b></i>′</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911015_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 在训练ResNet-18时使用激活区域处理算法对Cifar 10图像进行遮挡的可视化" src="Detail/GetImg?filename=images/JSJK201911015_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 在训练ResNet-18时使用激活区域处理算法对Cifar 10图像进行遮挡的可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911015_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Visualization of occlusion of Cifar 10 using the activation region processing algorithm while training ResNet-18</p>

                </div>
                <h3 id="112" name="112" class="anchor-tag"><b>4 实验结果分析</b></h3>
                <h4 class="anchor-tag" id="113" name="113"><b>4.1 对比实验的设置</b></h4>
                <div class="p1">
                    <p id="114">我们在不同数据集上比较了使用和不使用激活区域处理算法训练的卷积神经网络模型。为确保实验结果的有效性,各数据集的训练集和测试集完全分开,首先使用训练集对网络进行训练,然后使用测试集进行测试。对于相同的神经网络结构和数据集,我们使用相同的参数设置、初始化方法和训练步骤进行训练,以在同等训练情形下对比加入本文算法前后的实验结果。我们通过计算模型的分类错误率来衡量模型性能,理想结果是错误率等于0。为了减少随机因素的影响和保证实验结果的准确性,本文所展示的实验结果是5次实验结果的平均错误率和标准差。实验平台、数据集、网络结构、参数设置详细介绍如下:</p>
                </div>
                <h4 class="anchor-tag" id="194" name="194">(1)实验平台:</h4>
                <div class="p1">
                    <p id="115">本文实验使用的硬件设备为Intel Xeon(R) CPU E5-2620 v3 2.40 GHz,NVIDIA Tesla K80 (12G)。本文实验所用操作系统为Ubuntu 16.04,采用Python 3.6作为编程语言,使用Facebook开源的Pytorch 1.0.0深度学习框架进行算法编码。</p>
                </div>
                <h4 class="anchor-tag" id="195" name="195">(2)数据集:</h4>
                <div class="p1">
                    <p id="116">Cifar10和Cifar100是深度学习领域常用的自然图像分类数据集之一,它们都包含50  000幅图像的训练集和10 000幅图像的测试集,每幅图像为32×32 pixel的RGB彩色图像,分别包含10和100个类别。Fashion-MNIST是一个包含10种类别的服装图像数据集,其中训练集包含60 000幅图像,测试集包含10 000幅图像,每幅图像均是28×28 pixel的灰度图像。</p>
                </div>
                <h4 class="anchor-tag" id="196" name="196">(3)网络结构:</h4>
                <div class="p1">
                    <p id="117">ResNet-18、WRN-28-10和ResNeXt-8-64。</p>
                </div>
                <h4 class="anchor-tag" id="197" name="197">(4)参数设置:</h4>
                <div class="p1">
                    <p id="118">使用Nesterov动量的随机梯度下降法(Stochastic Gradient Descent)对各网络进行端到端的训练,动量参数设置为0.9,权值衰减系数为0.000 5,小批量大小为128,使用交叉熵函数计算损失值,通过反向传播算法逐层传播损失和更新网络参数。对于相同的数据集,所有模型均采用相同的数据归一化和数据增强方法进行预处理。通过计算数据集各通道的平均值、标准差,然后将数据集图像各通道减去平均值再除以标准差得到归一化后的图像。训练200个周期,初始学习率设置为0.1,并在80,120,160个周期后依次将学习率减少5倍。在训练过程中对这3个数据集执行相同的数据增强操作,首先对图像每侧进行4个像素的0填充,然后随机裁剪出与输入图像大小相同的图像,最后进行随机水平翻转。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>4.2 参数选取</b></h4>
                <div class="p1">
                    <p id="120">由于CNN包含多个卷积块,因此我们在相同超参数设置下基于ResNet-18的不同卷积块在Cifar10上提取的特征图进行了实验。表1展现了ResNet-18的详细结构。分别选取block1、block2、block3、block4的输出作为激活区域处理算法的输入来提取最大特征图和制作图像掩膜。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表1 ResNet-18的网络结构以及选取不同卷积层 输出特征图制作图像掩膜的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 ResNet-18 network structure and experimental results of using different convolutional layer output feature maps to create image mask</b></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />ResNet-18</td><td>Layer</td><td>Output size</td><td>Error rate/%</td></tr><tr><td><br />conv1</td><td>3×3,64,Stride 1</td><td>32×32×64</td><td>-</td></tr><tr><td><br />block1</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>6</mn><mn>4</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn></mrow></math></td><td>32×32×64</td><td>4.19±0.06</td></tr><tr><td><br />block2</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn></mrow></math></td><td>16×16×128</td><td>3.86±0.04</td></tr><tr><td><br />block3</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn></mrow></math></td><td>8×8×256</td><td><b>3.63±0.07</b></td></tr><tr><td><br />block4</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mtable columnalign="left"><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr><mtr><mtd><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr></mtable><mo>]</mo></mrow><mo>×</mo><mn>2</mn></mrow></math></td><td>4×4×512</td><td>3.96±0.05</td></tr><tr><td><br />fc</td><td>global average pool,<br />10 classes,softmax</td><td>1×1</td><td>-</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">从表1可以看出,激活区域处理算法在以block3的特征图作为输入的情况下在Cifar10上得到了更好的结果。将block2、block3、block4输出的最大特征图上采样至32×32后进行可视化对比如图3所示。从图3中可以看出,较浅层的block2提取出的特征更多的是零散的边缘特征,较深层次的block3提取出来的是较为关键的局部特征,而更深层次的block4更多地利用了全局特征信息。在图像分类过程中,卷积神经网络中的浅层学习到的是图像中的部分边缘特征信息,而更深层根据浅层得到的边缘特征信息进一步学习,得到全局特征信息,最后进行分类决策。</p>
                </div>
                <div class="p1">
                    <p id="123">对于本文提出的算法而言,block2提取的零散边缘特征信息和block4提取的全局特征信息并不利于制作图像掩膜。因为零散边缘特征信息会导致制作出来的掩膜的遮挡区域相对零散,达不到很好的遮挡效果,全局特征信息会导致制作出来的掩膜的遮挡区域过大,使得原始图像信息损失过多。我们希望遮挡图像的一部分关键的局部特征信息,以迫使网络学习更多的特征并提高网络的泛化能力,因此我们在ResNet-18网络中选取block3的输出作为算法的输入。WRN-28-10和ResNeXt-8-64均没有block4卷积模块,为了避免使用最后一个卷积层的输出来制作图像掩膜,我们选取WRN-28-10和ResNeXt-8-64的block2的输出来作为算法的输入。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911015_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 在ResNet-18中不同卷积层对不同类别的Cifar10图像提取特征的可视化" src="Detail/GetImg?filename=images/JSJK201911015_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 在ResNet-18中不同卷积层对不同类别的Cifar10图像提取特征的可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911015_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Visualization of different classes of Cifar10 extraction features in different convolutional layers in ResNet-18</p>

                </div>
                <div class="p1">
                    <p id="126">激活区域处理算法有3个超参数需要选取,分别是随机遮挡概率、阈值、填充方式。为了评估超参数对算法性能的影响,在不同的超参数设置下基于ResNet-18对Cifar10进行了实验。在评估其中一个参数时,其他参数保持不变。实验结果如表2和图4所示,当<i>P</i>=0.4,<i>λ</i>=0.3,方式填充为Fill-R时,算法的性能最好。值得注意的是,算法在不同参数设置下的实验结果均优于ResNet-18在Cifar10上的基线结果。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表2 AR算法使用不同填充方式进行遮挡的错误率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 AR error rate using different filling methods for occlusion</b></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td rowspan="2"><br />Method</td><td colspan="4"><br />填充方式</td></tr><tr><td><br />Fill-0</td><td>Fill-1</td><td>Fill-I</td><td>Fill-R</td></tr><tr><td><br />AR Error <br />Rate/%</td><td>3.95±0.09</td><td>3.91±0.07</td><td>3.88±0.10</td><td><b>3.63±0.07</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911015_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同超参数对算法性能的影响" src="Detail/GetImg?filename=images/JSJK201911015_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同超参数对算法性能的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911015_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Effect of different hyperparameters on algorithm performance</p>

                </div>
                <h4 class="anchor-tag" id="129" name="129"><b>4.3 实验结果对比</b></h4>
                <div class="p1">
                    <p id="130">表3中显示了在使用数据增强的情况下,本文算法与不同网络结合后在不同数据集上的实验结果。与现有的一些研究结果相比,本文算法在Cifar10、Cifar100和Fashion-MNIST数据集上分别得到了3.11%,17.44%和3.97%的更低的错误率。对于ResNet-18、WRN-28-10、ResNeXt-8-64这3个基础网络而言,未加任何遮挡的基线模型和添加随机遮挡的模型的错误率均高于添加本文算法后的模型的错误率。同一网络模型在Cifar10、Cifar100和Fashion-MNIST上比基线最多降低了约0.71%,1.04%和1.77%的错误率。值得注意的是,本文提出的算法不仅适用于彩色图像数据集Cifar,而且还适用于灰度图像Fashion-MNIST。这说明本文算法能够有效地提升不同网络结构在不同数据集上的分类性能。</p>
                </div>
                <div class="p1">
                    <p id="131">为了对比采用AR算法训练的模型和采用随机遮挡图像方法训练的模型的性能,本文进行了以下实验:在训练过程中对图像随机添加不同大小的遮挡,随机遮挡区域面积与原始图像面积(<i>H</i>×<i>W</i>)比值为<i>s</i>∈[0,0.5],从[0.5,1.5]范围中随机选取遮挡区域面积的宽高比<i>R</i>,则遮挡区域的宽高为:<mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>Η</mi><mo>′</mo></msup><mo>=</mo><msqrt><mrow><mi>s</mi><mo>×</mo><mi>Η</mi><mo>×</mo><mi>W</mi><mo>÷</mo><mi>R</mi></mrow></msqrt><mo>,</mo><msup><mi>W</mi><mo>′</mo></msup><mo>=</mo><msqrt><mrow><mi>s</mi><mo>×</mo><mi>Η</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>R</mi></mrow></msqrt></mrow></math></mathml>,并用[0,1]内的随机值填充遮挡区域。实验结果如表3所示。</p>
                </div>
                <div class="p1">
                    <p id="132">从表3可以看出,采用随机遮挡图像方法训练的模型也比基线模型的性能更好。与AR算法相比,AR算法对模型的性能提升更加显著。由于随机遮挡方法中遮挡区域位置是随机不定的,其功能更多的是增加了训练图像的多样性,并不一定能遮挡关键区域来降低模型对局部区域的依赖。AR算法对关键区域的遮挡可能是带来更好性能的关键。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表3 与当前一些新方法的错误率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Error rate comparison with different baselines and current new methods</b></p>
                    <p class="img_note">%</p>
                    <table id="133" border="1"><tr><td rowspan="2"><br />Method</td><td rowspan="2">Year</td><td colspan="3"><br />Datasets</td></tr><tr><td><br />Cifar10</td><td>Cifar100</td><td>Fashion-MNIST</td></tr><tr><td><br />ResNet-1001<sup>[19]</sup></td><td>2016</td><td>4.69±0.20</td><td>22.68±0.22</td><td>-</td></tr><tr><td><br />WRN-40-10<sup>[20]</sup></td><td>2016</td><td>3.8</td><td>18.3</td><td>-</td></tr><tr><td><br />ResNeXt-8-64<sup>[21]</sup></td><td>2017</td><td>3.65</td><td>17.77</td><td>-</td></tr><tr><td><br />Random Erasing<sup>[13]</sup></td><td>2017</td><td>3.08±0.05</td><td>17.73±0.15</td><td>4.2±0.03</td></tr><tr><td><br />DenseNet-40-FRN<sup>[28]</sup></td><td>2018</td><td>4.95</td><td>23.36</td><td>-</td></tr><tr><td><br />RoR optimization method<sup>[29]</sup></td><td>2018</td><td>3.52</td><td>19.07</td><td>-</td></tr><tr><td><br />ResNet-18(基线)</td><td>2019</td><td>4.34±0.08</td><td>21.96±0.13</td><td>5.86±0.05</td></tr><tr><td><br />ResNet-18+随机遮挡</td><td>2019</td><td>3.85±0.06(↓0.49)</td><td>21.33±0.10(↓0.63)</td><td>4.45±0.07(↓1.41)</td></tr><tr><td><br />ResNet-18+AR</td><td>2019</td><td>3.63±0.07(↓0.71)</td><td>21.16±0.12(↓0.80)</td><td>4.09±0.09(↓1.77)</td></tr><tr><td><br />WRN-28-10(基线)</td><td>2019</td><td>3.70±0.11</td><td>18.48±0.21</td><td>5.50±0.08</td></tr><tr><td><br />WRN-28-10+随机遮挡</td><td>2019</td><td>3.38±0.10(↓0.32)</td><td>17.72±0.14(↓0.76)</td><td>4.23±0.09(↓1.27)</td></tr><tr><td><br />WRN-28-10+AR</td><td>2019</td><td><b>3.11±0.07(↓0.59)</b></td><td><b>17.44±0.17(↓1.04)</b></td><td><b>3.97±0.07(↓1.53</b>)</td></tr><tr><td><br />ResNeXt-8-64(基线)</td><td>2019</td><td>3.68±0.07</td><td>18.72±0.23</td><td>5.43±0.05</td></tr><tr><td><br />ResNeXt-8-64+随机遮挡</td><td>2019</td><td>3.35±0.08(↓0.33)</td><td>18.49±0.18(↓0.23)</td><td>4.36±0.05(↓0.71)</td></tr><tr><td><br />ResNeXt-8-64+AR</td><td>2019</td><td>3.12±0.05(↓0.46)</td><td>18.33±0.17(↓0.39)</td><td>4.07±0.10(↓1.36)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="134" name="134"><b>4.4 不使用数据增强下的性能</b></h4>
                <div class="p1">
                    <p id="135">由表4可知,在不使用数据增强的情况下,本文算法均降低了ResNet-18、WRN-28-10网络在Cifar10、Cifar100数据集上的分类错误率。结合表3和表4可知,本文算法与数据增强算法结合后的效果更好,并且可以将算法看做是一种新型的数据增强算法。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表4 在不使用数据增强的情况下</b>, <b>本文算法与不同基线的错误率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Error rate comparison with the proposed algorithm and different baselines without using data augmentation</b>.</p>
                    <p class="img_note">%</p>
                    <table id="136" border="1"><tr><td rowspan="2"><br />Mehtod</td><td colspan="2"><br />Datasets</td></tr><tr><td><br />Cifar10</td><td>Cifar100</td></tr><tr><td><br />ResNet-18</td><td>10.17±0.04</td><td>35.92±0.16</td></tr><tr><td><br />ResNet-18+AR</td><td>9.47±0.07±<br />(↓0.70)</td><td>35.23±0.11±<br />(↓0.69)</td></tr><tr><td><br />WRN-28-10</td><td>6.67±0.11</td><td>25.19±0.14</td></tr><tr><td><br />WRN-28-10+AR</td><td>6.16±0.14<br />(↓0.51)</td><td>24.81±0.11<br />(↓0.38)</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="137" name="137"><b>4.5 对卷积神经网络的影响</b></h4>
                <div class="p1">
                    <p id="138">为了更好地了解算法对卷积神经网络产生的影响,进行了以下对比实验:随机地从测试集中采样出128幅图像,作为训练好的模型的输入,从中提取出某一卷积块的输出特征图;然后计算每幅特征图的平均值作为激活值,并对于同一幅图像的激活值进行降序排序;为了减少随机采样带来的影响,最后计算这128幅图像对应特征激活值的平均值。对于ResNet-18,在Cifar10测试集上提取block2、block3和block4这3个模块的输出特征图。对于WRN-28-10,在Fashion-MNIST测试集上提取block1、block2和block3这3个模块的输出特征图。最后对比结果如图5所示。从图5可以观察到,使用AR算法训练的模型在不同卷积层的激活值均强于基线模型的。值得注意的是,AR算法训练的模型的激活值与基线模型的激活值的比也随着卷积层的加深而提高。这表明AR算法确实鼓励模型学习更多的特征信息,并且利用更多的激活特征进行决策,而不仅仅是依赖于少量激活特征进行决策。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911015_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 比较使用不同方式训练的模型的不同卷积层的激活情况" src="Detail/GetImg?filename=images/JSJK201911015_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 比较使用不同方式训练的模型的不同卷积层的激活情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911015_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Compare different convolutional layers activation of models trained using different methods</p>

                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>4.6 对随机遮挡的鲁棒性</b></h4>
                <div class="p1">
                    <p id="142">为了验证AR算法训练的模型对随机遮挡图像识别的鲁棒性,本文进行了以下实验:对Cifar10和Fashion-MNIST的测试集图像随机添加不同大小的遮挡,随机遮挡区域面积与原始图像面积(<i>H</i>×<i>W</i>)比为<i>s</i>∈[0,0.5],从[0.5,1.5]范围中随机选取遮挡区域面积的宽高比<i>R</i>,则遮挡区域的宽高为:<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msup><mi>Η</mi><mo>′</mo></msup><mo>=</mo></mrow><msqrt><mrow><mi>s</mi><mo>×</mo><mi>Η</mi><mo>×</mo><mi>W</mi><mo>÷</mo><mi>R</mi></mrow></msqrt><mo>,</mo><msup><mi>W</mi><mo>′</mo></msup><mo>=</mo><msqrt><mrow><mi>s</mi><mo>×</mo><mi>Η</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>R</mi></mrow></msqrt></mrow></math></mathml>,并用[0,1]随机值填充遮挡区域,然后使用训练好的ResNet-18和WRN-28-10对遮挡测试集进行实验。对遮挡测试集进行分类后的错误率如图6所示。从图6可以看出,使用AR算法训练的ResNet-18和WRN-28-10模型在不同遮挡情况下的性能均优于基线模型的。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911015_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 对随机遮挡图像的鲁棒性" src="Detail/GetImg?filename=images/JSJK201911015_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 对随机遮挡图像的鲁棒性  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911015_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Robustness of random occlusion images</p>

                </div>
                <h4 class="anchor-tag" id="143" name="143"><b>4.7 算法复杂性分析</b></h4>
                <div class="p1">
                    <p id="144">为了提高在实际应用中的可行性和可扩展性,必须权衡算法复杂性和性能。为了说明AR算法的成本,以ResNet-18和WRN-28-10为例,在Cifar10上进行稳定训练时,比较模型的物理内存占用、GPU内存占用和每周期的训练时间,比较结果如表5所示。由表5可知,结合AR算法后的模型与基线模型占用的物理内存大致相当,这是由于本文算法的运算全部在GPU上。比较GPU内存的占用情况可知,结合AR算法后的模型比基线模型占用的GPU内存多了不到100 Mb,并且不会随着模型的变动有太大的变化。比较每周期的训练时间,结合AR算法后的模型比基线模型每周期的训练时间多了不到10%。根据算法的设计,AR算法自身并不会参与网络内部的传播运算,不涉及梯度等复杂的计算,算法额外需要的内存和计算时间取决于数据集的大小和输入特征图的尺寸大小。由于AR算法并不会改变模型的结构和带来额外的训练参数,因此训练好的模型与基线模型在文件大小和对测试集进行测试的时间相当。通过以上分析可知,AR算法在空间上带来的代价是非常小的。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表5 模型的物理内存</b>、 <b>GPU内用和每周期的训练时间比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Comparison of physical memory</b>, <b>GPU memory,and training time per epoch of the model</b></p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />Method</td><td>Memory/Mb</td><td>GPU memory/Mb</td><td>Time/s</td></tr><tr><td><br />ResNet-18</td><td>2 464</td><td>1 543</td><td>79</td></tr><tr><td><br />ResNet-18+AR</td><td>2 477</td><td>1 615</td><td>86</td></tr><tr><td><br />WRN-28-10</td><td>2 575</td><td>4 721</td><td>412</td></tr><tr><td><br />WRN-28-10+AR</td><td>2 584</td><td>4 807</td><td>431</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="146" name="146"><b>4.8 与预训练模型相结合</b></h4>
                <div class="p1">
                    <p id="147">为了进一步证明本文算法具有良好的泛化性,使用预训练模型Xception<citation id="188" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>在一个全新的胎盘组织细胞图像数据集上进行微调分类实验。使用Ferlaino等<citation id="189" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提供的包含5个类别的胎盘组织细胞图像数据集,其中训练集有7 529幅图像,测试集有1 000幅图像(每个类别各有200幅),验证集有1 000幅图像(每个类别各有200幅),所有图像均为200×200 pixel的RGB彩色图像。使用训练集进行微调,验证集和测试集均不参与微调训练模型。移除原Xception网络的全连接层,使用全局平均池化对卷积层输出进行池化,然后构建新的全连接层:2048<i>fc</i>→128<i>fc</i>→5<i>fc</i>,层与层之间使用丢失率为0.5的Dropout层。使用随机梯度下降法对Xception进行端到端的微调,学习率为0.01,小批量大小为16,损失函数为交叉熵函数。将图像上采样至299×299 pixel以保持与原Xception网络的输入一致,并使用Xception的block3模块的输出(37×37×256)作为本文算法的输入。由表6的结果可知,使用AR算法能够显著改进微调Xception模型的性能,在验证集上获得了1%的性能增益,在测试集上获得了1.5%的性能增益。</p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表6 使用预先训练的Xception模型 对胎盘组织细胞图像的实验分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 Experimental classification results of placental tissue cell images using pre-trained Xception model</b></p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td><br />Method</td><td>Validation <br />accuracy/%</td><td>Test <br />accuracy/%</td></tr><tr><td><br />InceptionV3<sup>[24]</sup></td><td>90</td><td>88</td></tr><tr><td><br />InceptionResNetV2<sup>[24]</sup></td><td>91</td><td>87</td></tr><tr><td><br />Xception<sup>[24]</sup></td><td>91</td><td>87</td></tr><tr><td><br />Ensemble (Max)<sup>[24]</sup></td><td>91</td><td>89</td></tr><tr><td><br />Xception+AR</td><td>92.0±0.2</td><td>90.5±0.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="149" name="149" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="150">降低网络模型过拟合的风险和提升模型在遮挡图像识别方面的泛化能力是深度学习方向卷积神经网络的研究重点。本文提出了激活区域处理算法,用于处理卷积神经网络某一层卷积层的最大激活特征图,以实现对输入图像进行遮挡,并将遮挡后的图像作为网络的新输入来继续训练网络。这种算法可以看成是数据增强算法的新形式,不仅可以降低网络模型过拟合的风险,而且还能够提升网络模型的性能。此外,使用这种算法训练的网络模型在识别随机遮挡图像方面也具有很好的鲁棒性。未来的工作中,我们会将这些方法应用于其他的计算机视觉领域中,例如图像分割、目标检测等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="201" type="formula" href="images/JSJK201911015_20100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">蒋芸</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="200" type="formula" href="images/JSJK201911015_20000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张海</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[1]</b> Krizhevsky A,Sutskever I,Hinton G.ImageNet classification with deep convolutional neural networks[C]//Proc of the International Conference on Neural Information Processing Systems,2012:1097-1105.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[2]</b> Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2014,39(4):640-651.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show and tell:A neural image caption generator">

                                <b>[3]</b> Vinyals O,Toshev A,Bengio S,et al.Show and tell:A neural image caption generator[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:3156-3164.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepPose:Human pose estimationvia deep neural netw orks">

                                <b>[4]</b> Toshev A,Szegedy C.DeepPose:Human pose estimation via deep neural networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2014:1653-1660.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 Lecun Y,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multiple layers of features from tiny images">

                                <b>[6]</b> Krizhevsky A,Hinton G.Learning multiple layers of features from tiny images[R].Toronto:University of Toronto,2009.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 Srivastava N,Hinton G,Krizhevsky A,et al.Dropout:A simple way to prevent neural networks from over-fitting[J].Journal of Machine Learning Research,2014,15(1):1929-1958.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Regularization of neural networks using dropconnect">

                                <b>[9]</b> Wan L,Zeiler M,Zhang S,et al.Regularization of neural networks using dropconnect[C]//Proc of Machine Learning Research,2013:1058-1066.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[10]</b> Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of the International Conference on Machine Learning,2015:1-11.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient object localization using Convolutional Networks">

                                <b>[11]</b> Tompson J,Goroshin R,Jain A,et al.Efficient object localization using convolutional networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:648-656.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analysis on the dropout effect in convolutional neural networks">

                                <b>[12]</b> Park S,Kwak N.Analysis on the dropout effect in convolutional neural networks[C]//Proc of Asian Conference on Computer Vision,2016:189-204.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Random erasing data augmentation">

                                <b>[13]</b> Zhong Z,Zheng L,Kang G,et al.Random erasing data augmentation[J].arXiv:1708.04896,2017.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Li Xiao-xin,Liang Rong-hua.A review for face recognition with occlusion:From subspace regression to deep learning[J].Chinese Journal of Computers,2018,41(1):177-207.(in Chinese)
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Liu Wan-jun,Dong Shuai-han,Qu Hai-cheng.Anti-occlusion visual tracking algorithm based on spatio-temporal context learning[J].Journal of Image and Graphics,2016,21(8):1057-1067.(in Chinese)
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 Chu Jun,Zhu Tao,Miao Jun,et al.Target tracking based on occlusion detection and spatio-temporal context information[J].Pattern Recognition and Artificial Intelligence,2017,30(8):718-727.(in Chinese)
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion">

                                <b>[17]</b> Vincent P,Larochelle H,Lajoie I,et al.Stacked denoising autoencoders:Learning useful representations in a deep network with a local denoising criterion[J].Journal of Machine Learning Research,2010,11(12):3371-3408.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                 Pathak D,Krahenbuhl P,Donahue J,et al.Context encoders:Feature learning by inpainting[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:2536-2544.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[19]</b> He K,Zhang X,Ren S,et al.Identity mappings in deep residual networks[C]//Proc of the European Conference on Computer Vision,2016:630-645.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wide residual networks">

                                <b>[20]</b> Zagoruyko S,Komodakis N.Wide residual networks[J].arXiv:1605.07146,2016.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aggregated Residual Transformations for Deep Neural Networks">

                                <b>[21]</b> Xie S,Girshick R,Dollár P,et al.Aggregated residual transformations for deep neural networks[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2017:5987-5995.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">

                                <b>[22]</b> Chollet F.Xception:Deep learning with depthwise separable convolutions[J].arXiv preprint arXiv:1610-02357,2016.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms">

                                <b>[23]</b> Xiao H,Rasul K,Vollgraf R.Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms[J].arXiv:1708.07747,2017.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards deep cellular phenotyping in placental histology">

                                <b>[24]</b> Ferlaino M,Glastonbury C A,Motta-Mejia C,et al.Towards deep cellular phenotyping in placental histology[J].arXiv:1804.03270,2018.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network in network">

                                <b>[25]</b> Lin M,Chen Q,Yan S.Network in network[J].arXiv:1312.4400,2013.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Geometric lp-norm feature pooling for image classifica-tion">

                                <b>[26]</b> Feng J,Ni B,Tian Q,et al.Geometric-norm feature pooling for image classification[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2011:2609-2704.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for ai">

                                <b>[27]</b> Bengio Y.Learning deep architectures for AI[J].Foundations and Trendsin Machine Learning,2009,2(1):1-127.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature reinforcement network for image classification">

                                <b>[28]</b> Lu B,Hu Q,Hui Y,et al.Feature reinforcement network for image classification[C]//Proc of the IEEE International Conference on Multimedia and Expo,2018:1-6.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" >
                                    <b>[29]</b>
                                 Zhang K,Guo L,Gao C.Optimization method of residual networks of residual networks for image classification[C]//Proc of the Big Data and Smart Computing,2018:321-325.附中文参考文献:
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801011&amp;v=MTE1MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvblZydkxMejdCZHJHNEg5bk1ybzlFWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 李小薪,梁荣华.有遮挡人脸识别综述:从子空间回归到深度学习[J].计算机学报,2018,41(1):177-207.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201608010&amp;v=MDE4Njk5Zk1wNDlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvblZydkxQeXJmYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 刘万军,董帅含,曲海成.时空上下文抗遮挡视觉跟踪[J].中国图象图形学报,2016,21(8):1057-1067.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201708007&amp;v=MTk5MTJGeS9uVnJ2TEtEN1liTEc0SDliTXA0OUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm0=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 储珺,朱陶,缪君,等.基于遮挡检测和时空上下文信息的目标跟踪算法[J].模式识别与人工智能,2017,30(8):718-727.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201911015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911015&amp;v=MTk4OTVxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L25WcnZMTHo3QlpiRzRIOWpOcm85RVlZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
