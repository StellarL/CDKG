<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358019092500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909015%26RESULT%3d1%26SIGN%3dOHjWyB41VAogNk98Ss%252fWHVPuMu0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909015&amp;v=MjcwODlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnpMTHo3QlpiRzRIOWpNcG85RVlZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;2 概述&lt;/b&gt; "><b>2 概述</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;3 改进的SSD网络&lt;/b&gt; "><b>3 改进的SSD网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="&lt;b&gt;3.1 改进的网络结构&lt;/b&gt;"><b>3.1 改进的网络结构</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;3.2 多尺度特征预测&lt;/b&gt;"><b>3.2 多尺度特征预测</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;3.3 改进的损失函数&lt;/b&gt;"><b>3.3 改进的损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="&lt;b&gt;4 反卷积层与特征融合&lt;/b&gt; "><b>4 反卷积层与特征融合</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;4.1 反卷积(Deconvolution)结构&lt;/b&gt;"><b>4.1 反卷积(Deconvolution)结构</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;4.2 特征融合(Feature Fusion)结构&lt;/b&gt;"><b>4.2 特征融合(Feature Fusion)结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;5 训练与实验分析&lt;/b&gt; "><b>5 训练与实验分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 SSD300网络结构">图1 SSD300网络结构</a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;表1 SSD300卷积感受野特征映射图&lt;/b&gt;"><b>表1 SSD300卷积感受野特征映射图</b></a></li>
                                                <li><a href="#66" data-title="图2 &lt;i&gt;hgSSD&lt;/i&gt;网络结构">图2 <i>hgSSD</i>网络结构</a></li>
                                                <li><a href="#70" data-title="图3 反卷积与特征融合结构">图3 反卷积与特征融合结构</a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表2 反卷积结构参数&lt;/b&gt;"><b>表2 反卷积结构参数</b></a></li>
                                                <li><a href="#91" data-title="图4 特征融合原理(引用图)">图4 特征融合原理(引用图)</a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表3 DSSD特征层参数&lt;/b&gt;"><b>表3 DSSD特征层参数</b></a></li>
                                                <li><a href="#101" data-title="图5 SSD300实验结果图">图5 SSD300实验结果图</a></li>
                                                <li><a href="#102" data-title="图6 hgSSD实验效果图">图6 hgSSD实验效果图</a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表4 各参数训练结果比较&lt;/b&gt;"><b>表4 各参数训练结果比较</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表5 各模型训练效果&lt;/b&gt;"><b>表5 各模型训练效果</b></a></li>
                                                <li><a href="#110" data-title="图7 准确率/迭代次数折线图">图7 准确率/迭代次数折线图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="150">


                                    <a id="bibliography_1" title=" Erhan D,Szegedy C,Toshev A,et al.Scalable object detection using deep neural networks [C]//Proc of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,2014:2155-2162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">
                                        <b>[1]</b>
                                         Erhan D,Szegedy C,Toshev A,et al.Scalable object detection using deep neural networks [C]//Proc of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,2014:2155-2162.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_2" title=" Panning A,Alhamadi A K,Niese R,et al.Facial expression recognition based on Haar-like feature detection[J].Pattern Recognition &amp;amp; Image Analysis,2008,18(3):447-452." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15103100173670&amp;v=MTgzNDlMZklKbDhXYVJFPU5qN0Jhcks5SDlIUHJvOUZaZXdNQ25zNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Panning A,Alhamadi A K,Niese R,et al.Facial expression recognition based on Haar-like feature detection[J].Pattern Recognition &amp;amp; Image Analysis,2008,18(3):447-452.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_3" title=" Dalal N,Triggs B.Histograms of oriented gradients for human detection [C]//Proc of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">
                                        <b>[3]</b>
                                         Dalal N,Triggs B.Histograms of oriented gradients for human detection [C]//Proc of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2005:886-893.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_4" title=" Ojala T,Pietik&#228;inen M,Topi M.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,24(7):971-987." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns">
                                        <b>[4]</b>
                                         Ojala T,Pietik&#228;inen M,Topi M.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,24(7):971-987.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_5" title=" Burges C J C.A tutorial on support vector machines for pattern recognition [J].Data Mining and Knowledge Discovery,1998,2(2):121-167." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002157166&amp;v=MDk3Mjc9Tmo3QmFyTzRIdEhPcm9wQ1plMEpZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDRGxWYjdLSTEw&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Burges C J C.A tutorial on support vector machines for pattern recognition [J].Data Mining and Knowledge Discovery,1998,2(2):121-167.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_6" title=" Zhu J,Zou H,Rosset S,et al.Multi-class adaboost [J].Statistics and Its Interface,2009,2(3):349-360." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJQI&amp;filename=SJQIF3BBEE8CB3D8F8F091DF44E2969FFE8E&amp;v=MjI1MzhzPU5pZmFaOFc3YktPNTJvYzJGdWg3QkFveHVSWWE2MHNMVEh1WHJoc3pjTVRpTUxMcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaHdybTl3Ng==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Zhu J,Zou H,Rosset S,et al.Multi-class adaboost [J].Statistics and Its Interface,2009,2(3):349-360.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_7" title=" Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 25th International Conference on Neural Information Processing Systems,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[7]</b>
                                         Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 25th International Conference on Neural Information Processing Systems,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_8" title=" Girshick R,Donahue J,Darrell T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation [C]//Proc of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[8]</b>
                                         Girshick R,Donahue J,Darrell T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation [C]//Proc of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,2014:580-587.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_9" title=" Girshick R.Fast R-CNN [C]//Proc of the IEEE International Conference on Computer Vision,2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[9]</b>
                                         Girshick R.Fast R-CNN [C]//Proc of the IEEE International Conference on Computer Vision,2015:1440-1448.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Ren S Q,HE K M,Girshick R,et al.Faster R-CNN:Towards realtime object detection with region proposal networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.</a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_11" title=" He Kai-ming,Zhang Xiang-yu,Ren Shao-qing,et al.Deep residual learning for image recognition [J].arXiv:1512.03385v1,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[11]</b>
                                         He Kai-ming,Zhang Xiang-yu,Ren Shao-qing,et al.Deep residual learning for image recognition [J].arXiv:1512.03385v1,2015.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_12" title=" Dai Ji-feng,Li Yi,He Kai-ming,el al.R-FCN:Object detection via region-based fully convolutional networks [C]//Proc of the 30th International Conference on Neural Information Processing Systems,2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:object detection via regionbased fully convolutional networks">
                                        <b>[12]</b>
                                         Dai Ji-feng,Li Yi,He Kai-ming,el al.R-FCN:Object detection via region-based fully convolutional networks [C]//Proc of the 30th International Conference on Neural Information Processing Systems,2016:379-387.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proc of the 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:779-788.</a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_14" title=" Liu Wei,Anguelov D,Erhan D,et al.SSD:Single shot multibox detector[C]//Proc of the 14th European Conference on Computer Vision (ECCV 2016),2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ssd:Single shot multibox detector">
                                        <b>[14]</b>
                                         Liu Wei,Anguelov D,Erhan D,et al.SSD:Single shot multibox detector[C]//Proc of the 14th European Conference on Computer Vision (ECCV 2016),2016:21-37.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Redmom J,Farhadi A.YOLO9000:Better,faster,stronger[C]//Proc of the 2017 IEEE Conference on Computer Vision and Pattern Recognition,2017:6517-6525.</a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_16" title=" Li Zuo-xin,Zhou Fu-qiang.FSSD:Feature fusion single shot multibox detector [J].arXiv:1712.00960v1,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FSSD:Feature fusion single shot multibox detector">
                                        <b>[16]</b>
                                         Li Zuo-xin,Zhou Fu-qiang.FSSD:Feature fusion single shot multibox detector [J].arXiv:1712.00960v1,2017.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_17" title=" Fu Cheng-yang,Liu Wei,Ranga A,et al.DSSD :Deconvolutional single shot detector [J].arXiv:1701.06659v1,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DSSD :Deconvolutional single shot detector">
                                        <b>[17]</b>
                                         Fu Cheng-yang,Liu Wei,Ranga A,et al.DSSD :Deconvolutional single shot detector [J].arXiv:1701.06659v1,2017.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_18" title=" Lin Tsung-yi,Doll&#225;r P,Girshick R,et al.Feature pyramid networks for object detection [J].arXiv:1612.03144v2,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">
                                        <b>[18]</b>
                                         Lin Tsung-yi,Doll&#225;r P,Girshick R,et al.Feature pyramid networks for object detection [J].arXiv:1612.03144v2,2017.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_19" title=" Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition [J/OL].arXiv:1409.1556v6,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[19]</b>
                                         Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition [J/OL].arXiv:1409.1556v6,2015.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_20" title=" Luo Wen-jie,Li Yu-jia,Urtasun R,et al.Understanding the effective receptive field in deep convolutional neural networks [C]//Proc of the 29th International Conference on Neural Information Processing Systems,2016:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding the effective receptive field in deep convolutional neural networks">
                                        <b>[20]</b>
                                         Luo Wen-jie,Li Yu-jia,Urtasun R,et al.Understanding the effective receptive field in deep convolutional neural networks [C]//Proc of the 29th International Conference on Neural Information Processing Systems,2016:1-9.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_21" title=" Wu Peng-ying,Zhang Jian-ming,Peng Jian,et al.Research on pedestrian detection based on multi-layer convolution feature in real scene [J].CAAI Transactions on Intelligent Systems,2019,14(2):306-315.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201902016&amp;v=MzE5NjBGckNVUkxPZVplUm1GeTdrVWJ6TFB5UFRlckc0SDlqTXJZOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Wu Peng-ying,Zhang Jian-ming,Peng Jian,et al.Research on pedestrian detection based on multi-layer convolution feature in real scene [J].CAAI Transactions on Intelligent Systems,2019,14(2):306-315.(in Chinese)
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     伍鹏瑛,张建明,彭建,等.多层卷积特征的真实场景下行人检测研究[J].智能系统学报,2019,14(2):306-315.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1627-1634 DOI:10.3969/j.issn.1007-130X.2019.09.014            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多尺度特征融合的小目标行人检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%80%9D%E5%AE%87&amp;code=21904872&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张思宇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E8%BD%B6&amp;code=09804428&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张轶</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0054367&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学计算机学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E8%A7%86%E8%A7%89%E5%90%88%E6%88%90%E5%9B%BE%E5%BD%A2%E5%9B%BE%E5%83%8F%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AD%A6%E7%A7%91%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学视觉合成图形图像技术国家重点学科实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对SSD当前存在的小目标漏检以及误检问题,结合反卷积与特征融合思想,提出hgSSD模型。将原SSD特征层反卷积后与较浅层特征结合,实现复杂场景下小目标行人检测。为了保留浅层网络特征,提高算法实时性,节省计算资源,hgSSD模型基础网络使用VGG16,而非更深层的ResNet101。为了加强对小目标的检测,将VGG16中的Conv3<sub>3</sub>改进为特征层加入训练。融合后的网络相对于SSD较为复杂,但基本保证实时性,且成功检测到大部分SSD网络漏检的小目标,检测精度相比于SSD模型也有提升。在选择框置信度得分阈值为0.3的情况下,基本检测到SSD漏检小目标。在VOC2007+2012中相对于SSD行人检测的Average Precision值从0.765提升为0.83。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E7%9B%AE%E6%A0%87%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小目标行人检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8D%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">反卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张思宇（1995-），男，山东威海人，硕士生，研究方向为计算机视觉和深度学习。E-mail:249356520@qq.com,通信地址：610065四川省成都市武侯区四川川大智胜软件股份有限公司&lt;image id="147" type="formula" href="images/JSJK201909015_14700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    张轶（1981-），男，四川成都人，博士后，副教授，研究方向为计算机视觉和机器学习。E-mail:12738078@qq.com,通信地址：610065四川省成都市武侯区四川川大智胜软件股份有限公司&lt;image id="149" type="formula" href="images/JSJK201909015_14900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-25</p>

            </div>
                    <h1><b>Small target pedestrian detection based on multi-scale feature fusion</b></h1>
                    <h2>
                    <span>ZHANG Si-yu</span>
                    <span>ZHANG Yi</span>
            </h2>
                    <h2>
                    <span>College of Computer Science,Sichuan University</span>
                    <span>National Key Laboratory of Fundamental Science on Synthetic Vision,Sichuan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Given the problems of missing detection and detection failure for small targets in the single shot multibox detector(SSD), we propose an hourglass SSD model based on the idea of deconvolution and feature fusion, called hgSSD model. It deconvolutes the conventional SSD feature, which is then combined with shallower features to detect small target pedestrians in complex scenes. In order to preserve shallow network characteristics, ensure real-time detection and save computing resources, we use the VGG-16 instead of the deeper RestNet-101 as the basic network. In order to enhance the detection of small targets, Conv3<sub>3</sub> in VGG16 is improved as the feature layer added into the training. The fused network is more complex than the conventional SSD, but the real-time performance is basically guaranteed. It can successfully detect most of the small targets that are missed by the conventional SSD network, and the network has a higher accuracy than the conventional SSD model. In the case where the default box confidence threshold of 0.3, it basically detects the small targets undetected by the conventional SSD. In VOC 2007+2012, the pedestrian average precision value is increased from 0.765 to 0.83 in comparison with the conventional SSD.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=small%20target%20pedestrian%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">small target pedestrian detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deconvolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deconvolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Si-yu,born in 1995,MS candidate,his research interests include computer vision,and deep learning.Address:Sichuan Chuandazhisheng Software Co.Ltd.,Wuhou District,Chengdu 610065,Sichuan,P.R.China;
                                </span>
                                <span>
                                    ZHANG Yi,born in 1981,post doctor, associate professor,his research interests include computer vision,and machine learning.Address:Sichuan Chuandazhisheng Software Co.Ltd.,Wuhou District,Chengdu 610065,Sichuan,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-25</p>
                            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="46">远距离目标检测在无人驾驶、摄像监控、智能安防、预测跟踪等领域均为重要研究课题。从某种角度来说,远距离目标检测也可以理解为小目标检测。小目标行人检测是当今计算机视觉研究中的热点和难点<citation id="194" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在复杂场景(姿态、遮挡、穿着、视角等影响)下,实现高精度小目标行人检测还有待于进一步研究。随着深度学习研究的深入以及硬件水平的提高,各种条件下的实时行人检测也取得了一定效果。但是,深度学习技术往往会保留深层特征,抛弃浅层特征,这对于小目标行人的检测有很大影响。本文着重于将深度学习中深层特征与浅层特征结合使用,以实现小目标检测。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>2 概述</b></h3>
                <div class="p1">
                    <p id="48">早期的行人检测<citation id="195" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>依赖于传统方法,包括Haar特征<citation id="196" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、方向梯度直方图HOG(Histogram of Oriented Gradient)<citation id="197" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、局部二值模式LBP(Local Binary Pattern)<citation id="198" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和分类器(SVM(Support Vector Machine)<citation id="199" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、Adaboost<citation id="200" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>)。但是,行人姿态千变万化,行人之间相互遮挡以及各种光照问题,使得传统方法的鲁棒性和实时性都差强人意。随着2012年AlexNet<citation id="201" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>以压倒性的优势在ILSVRC-2012取得胜利后,深度学习在目标检测方面逐渐显露出其出众的能力。相比于传统方法,基于深度学习的行人检测方法学习到的特征更丰富,表达能力更强。</p>
                </div>
                <div class="p1">
                    <p id="49">如今,基于深度学习的行人检测方法主要分为2类:基于区域候选框ROI(Region Of Interest)的方法和基于回归(Regression)的方法。前一方法的代表作是由Girshick等<citation id="202" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的R-CNN(Region-Convolutional Netual Network),其主要贡献是将卷积神经网络用于分类。随后又针对R-CNN模型的重复计算等问题提出了Fast R-CNN<citation id="203" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、Faster R-CNN<citation id="204" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。但是,由于候选框ROI和区域建议网络RPN(Region Proposal Network)的使用,Faster R-CNN也很难做到实时性检测。2015年微软研发的ResNet(Residual Network)<citation id="205" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>残差结构的出现,使得训练时间变短。随后出现的R-FCN(Region-based Fully Convolutional Network)<citation id="206" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>结合RPN与ResNet,实现了高实时性高精度检测。基于回归的方法主要依赖于默认框的选取以及损失函数的设计。主要代表模型有YOLO(You Only Look Once)<citation id="207" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和SSD(Single Shot multibox Detector)<citation id="208" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。YOLO由于其检测精度过低问题被SSD(Single Shot multibox Detctor)超越。随后YOLO V2<citation id="209" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>问世,既保持了YOLO的实时性,也弥补了YOLO检测精度低的缺陷。而SSD的出现,更是进一步证明了回归方法的优势,但因其对于小目标出现误检漏检等问题,又出现了FSSD(Feature fusion Single Shot multibox Detector)<citation id="210" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和DSSD(Deconvolutional Single Shot multibox Detector)<citation id="211" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>等方法。</p>
                </div>
                <div class="p1">
                    <p id="50">本文基于SSD网络,提出hgSSD(hourglass Single Shot multibox Detector)模型,改进SSD的小目标漏检问题并应用于行人检测。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag"><b>3 改进的SSD网络</b></h3>
                <div class="p1">
                    <p id="52">SSD是一种端到端单步目标检测模型,其主要结构为将全连接层(FC6、FC7)替换为卷积层后的VGG16<citation id="212" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。FC7层后添加4个卷积层作为特征预测层。Conv4_3与新添加的卷积层组成多尺度特征层,实现多尺度检测,称之为SSD Layer。本文使用300*300图像作为模型输入,故下文称SSD300,如图1所示。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SSD300网络结构" src="Detail/GetImg?filename=images/JSJK201909015_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SSD300网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 SSD300 network structure</p>

                </div>
                <div class="p1">
                    <p id="54">图1中方块代表网络结构,包括输入图像(300*300)、VGG16、Conv8、Conv9、Conv10和Conv11。图1中特征层(箭头起点)分别为Conv4_3(38*38*512)、Conv7_2(19*19*1024)、Conv8_2(10*10*512)、Conv9_2(5*5*256)、Conv10_2(3*3*256)和Conv11_2(1*1*256)。箭头终点为预测层,在预测前需对所有候选框(defaultbox)进行非极大值抑制NMS(Non Maximum Suppression),剔除重复样本。将特征(featuremap)按像素切分成8*8或4*4的格子,并在每一个格子上标定一系列固定大小的box,称为defaultbox。与FasterR-CNN不同,SSD直接预测目标类别和定位框,该算法不使用RPN,极大地提高了检测速度。在FasterR-CNN中,特征向量由网络最后一层卷积而来,这种单一的特征层所获信息十分有限,没有充分利用浅层网络。SSD针对不同的特征层,设置不同尺寸(scale)、不同纵横比(aspect ratio)的defaultbox来更好地匹配目标。SSD300在VOC2007+2012数据集上的<i>MAP</i>(Mean Average-Precision)为0.721<citation id="213" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,行人的<i>AP</i>为0.745<citation id="214" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>(本文训练后为0.765,因为改变aspect ratio)。帧率为59 fps<citation id="215" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="55">SSD运行速度和检测精度都基本令人满意,但其缺点也显而易见,对于小目标的检测能力较弱。首先,SSD针对人工标定groundtruthbox进行训练,以此驱动feature检测。小目标在训练集中所对应的groundtruthbox数量较少,难以得到充分训练。其次也是关键原因,小目标大多由感受野较小的网络浅层部分检测。浅层网络卷积次数少,对目标的提取能力较弱。换句话说,浅层特征层(featuremap)虽然能检测到小目标,但由于其置信度较弱(无法确定正确分类)在训练过程被舍弃,有时还会出现部分误检。而深层虽然对特征提取能力强,但却大量舍弃了小目标。</p>
                </div>
                <div class="p1">
                    <p id="56">为什么浅层网络表征能力不强?由于不同层特征感受野<citation id="216" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>大小不同,深层特征卷积次数多,感受野大,提取的目标更加抽象完整,但忽略了小目标。浅层特征卷积次数少,感受野小,虽然提取到小目标,但对目标的敏感度不如深层特征。感受野随层数计算公式如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>F</mtext><mspace width="0.25em" /></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>S</mi><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>F</mtext></mrow></msub><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mi>Ν</mi><msub><mrow></mrow><mtext>S</mtext></msub><mo>+</mo><mi>S</mi><msub><mrow></mrow><mtext>f</mtext></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中,<i>S</i><sub>RF</sub>(<i>t</i>)为第<i>t</i>层卷积层感受野,<i>N</i><sub>S</sub>为步长;<i>S</i><sub>f</sub>为滤波器尺寸。通过迭代可知,每一层感受野均通过上一层感受野求得。在SSD300中,各特征层感受野如表1所示。</p>
                </div>
                <div class="area_img" id="59">
                    <p class="img_tit"><b>表1 SSD300卷积感受野特征映射图</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 SSD300 convolution receptive field feature mapping</b></p>
                    <p class="img_note"></p>
                    <table id="59" border="1"><tr><td><br />Layer</td><td>Receptive Field</td><td>Output Scale</td></tr><tr><td><br />Conv4_3</td><td>92 * 92</td><td>38 * 38</td></tr><tr><td><br />FC7(Conv)</td><td>260 * 260</td><td>19 * 19</td></tr><tr><td><br />Conv8_2</td><td>292 * 292</td><td>10 * 10</td></tr><tr><td><br />Conv9_2</td><td>356 * 356</td><td>5 * 5</td></tr><tr><td><br />Conv10_2</td><td>485 * 485</td><td>3 * 3</td></tr><tr><td><br />Conv11_2</td><td>612 * 612</td><td>1 * 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="60">表1数据显示,在Conv8时,感受野就基本覆盖了整幅图像。因此,网络关注的都是大且明显的目标,小目标可能被当作背景而舍弃。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>3.1 改进的网络结构</b></h4>
                <div class="p1">
                    <p id="62">为了解决SSD浅层网络目标提取能力差的问题,本文在VGG的基础上将Conv3_3归一化后加入特征层。Conv3_3输出尺寸为75*75(根据VGG网络卷积得到),感受野较小,特性信息保存更完整,对于小目标的敏感度更高。为了匹配大小不一的行人目标,本文的defaultbox尺寸计算策略如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>+</mo><mfrac><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mrow><mo>(</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中,<i>s</i><sub>min</sub>、<i>s</i><sub>max</sub>分别设置为0.15、0.9,在SSD中分别被设置为0.2<citation id="217" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、0.9<citation id="218" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,表示最底层scale为0.2。此处因开放Conv3_3,故将最小值改为0.15。<i>m</i>为特征层数,因新添加卷积层数为4层,此处设置为4。本文针对行人目标,设置纵横比大小<i>ar</i>={1,1/2,1/3}。当<i>ar</i>为1时,增加一种尺寸计算<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo></mrow><msqrt><mrow><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mi>s</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msqrt></mrow></math></mathml>。Conv3_3与Conv4_3保留VGG结构尺寸。每个box的宽和高计算公式分别为<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>w</mi><msubsup><mrow></mrow><mi>k</mi><mi>a</mi></msubsup><mo>=</mo></mrow><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><msqrt><mrow><mi>a</mi><mi>r</mi></mrow></msqrt></mrow></math></mathml>和<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msubsup><mrow></mrow><mi>k</mi><mi>a</mi></msubsup><mo>=</mo><mi>s</mi><msub><mrow></mrow><mi>k</mi></msub><mo>/</mo><msqrt><mrow><mi>a</mi><mi>r</mi></mrow></msqrt></mrow></math></mathml>。中心坐标为<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo stretchy="false">)</mo><mo>/</mo><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mo>,</mo><mo stretchy="false">(</mo><mi>j</mi><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo stretchy="false">)</mo><mo>/</mo><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mo stretchy="false">)</mo></mrow></math></mathml>,其中<i>f</i><sub><i>k</i></sub>为第<i>k</i>个特征层的大小。将Conv3_3加入特征层后参数增加,可以适当在Conv3和Conv4后加入Batch Normalization,增加网络鲁棒性,防止梯度消失或爆炸。</p>
                </div>
                <div class="p1">
                    <p id="65">本文使用融合(Fusion)的方式将深层SSD Layer反卷积后与浅层特征(低分辨率高语义)结合,形成一种高分辨率高语义特征层。将上文中提出的defaultbox用于融合后的反卷积特征层。本文新提出的网络称之为hgSSD,其结构如图2所示。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 hgSSD网络结构" src="Detail/GetImg?filename=images/JSJK201909015_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>hgSSD</i>网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 2 <i>hgSSD network structure</i></p>

                </div>
                <div class="p1">
                    <p id="67">图2中横向箭头代表反卷积过程,实线<i>Fusion</i>部分代表特征融合,竖直箭头代表多尺度预测。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>3.2 多尺度特征预测</b></h4>
                <div class="p1">
                    <p id="69">所谓多尺度预测,即<i>Conv</i>3_3(75*75)、<i>Conv</i>4_3(38*38)、<i>Conv</i>7_2(19*19)、<i>Conv</i>11_2(1*1)、<i>Dconv</i>12(3*3)、<i>Dconv</i>13(5*5)和<i>Dconv</i>14(10*10)分别提取不同分辨率的特征用来预测。本文将所有<i>defaultbox</i>与<i>groundtruthbox</i>按照交并比<i>IOU</i>(<i>Intersection Over Union</i>)进行匹配,匹配成功则为正样本,否则为负样本。为了防止负样本的数量过大,导致训练期间正负样本严重不平衡,<i>SSD</i>中使用<i>Hard Negative Mining</i>,对正负<i>defaultbox</i>按照最高置信度进行排序,将正负样本控制在1∶3左右,多余负样本不加入预测。实验中发现,当正负样本按1∶3控制时模型优化得更快,网络更加稳定。随后使用<i>NMS</i>进行样本筛选,得到最终行人目标。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 反卷积与特征融合结构" src="Detail/GetImg?filename=images/JSJK201909015_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 反卷积与特征融合结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 3 <i>Deconvolution and feature fusion structure</i></p>

                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>3.3 改进的损失函数</b></h4>
                <div class="p1">
                    <p id="72"><i>SSD</i>中损失函数是定位与分类的联合损失,这也是<i>SSD</i>实现端到端训练不可缺少的一部分。其计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>g</mi></mrow><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mo>(</mo><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>f</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>c</mi></mrow><mo>)</mo></mrow><mo>+</mo><mi>α</mi><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>g</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中,<i>N</i>为匹配到的<i>box</i>的数量;<i>c</i>为置信度;<i>L</i><sub>conf</sub>为分类损失函数,采用<i>Softmax Loss</i>。由于本文只做行人检测,故可换为二分分类损失函数;<i>L</i><sub>loc</sub>为定位损失,采用<i>Smooth</i> L<sub>1</sub><i>Loss</i>。<i>α</i>为定位权重项,用来惩罚定位损失。定位损失函数的详细公式如下:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>g</mi></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ρ</mi><mi>o</mi><mi>s</mi></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>c</mi><mi>x</mi><mo>,</mo><mi>c</mi><mi>y</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>h</mi></mrow><mo>}</mo></mrow></mrow></munder><mi>x</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>k</mi></msubsup><mi>s</mi><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mrow><mo>(</mo><mrow><mi>l</mi><msubsup><mrow></mrow><mi>i</mi><mi>m</mi></msubsup><mo>-</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>c</mi><mi>x</mi></mrow></msubsup><mo>=</mo><mrow><mo>(</mo><mrow><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>c</mi><mi>x</mi></mrow></msubsup><mo>-</mo><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>c</mi><mi>x</mi></mrow></msubsup></mrow><mo>)</mo></mrow><mo>/</mo><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mi>w</mi></msubsup><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mrow><mi>c</mi><mi>y</mi></mrow></msubsup><mo>=</mo><mrow><mo>(</mo><mrow><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>c</mi><mi>y</mi></mrow></msubsup><mo>-</mo><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>c</mi><mi>y</mi></mrow></msubsup></mrow><mo>)</mo></mrow><mo>/</mo><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mi>h</mi></msubsup><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>w</mi></msubsup><mo>=</mo><mi>log</mi><mrow><mo>(</mo><mrow><mfrac><mrow><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mi>w</mi></msubsup></mrow><mrow><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mi>w</mi></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>h</mi></msubsup><mo>=</mo><mi>log</mi><mrow><mo>(</mo><mrow><mfrac><mrow><mi>g</mi><msubsup><mrow></mrow><mi>j</mi><mi>h</mi></msubsup></mrow><mrow><mi>d</mi><msubsup><mrow></mrow><mi>i</mi><mi>h</mi></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">其中,<i>x</i>∈{1,0}表示<i>default box</i>与<i>groundtruth box</i>匹配是否成功。<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mrow><mo>(</mo><mrow><mi>l</mi><msubsup><mrow></mrow><mi>i</mi><mi>m</mi></msubsup><mo>-</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup></mrow><mo>)</mo></mrow></mrow></math></mathml>表示平滑<i>L</i><sub>1</sub>范数,<i>l</i>表示预测目标(<i>prediction box</i>),<i>g</i>表示真实框(<i>groundtruth box</i>),<i>d</i>表示先验框(<i>default box</i>)。分类损失详细公式如下:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>f</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ρ</mi><mi>o</mi><mi>s</mi></mrow><mi>Ν</mi></munderover><mi>x</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mi>log</mi><mo stretchy="false">(</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">)</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Ν</mi><mi>e</mi><mi>g</mi></mrow></munder><mspace width="0.25em" /></mstyle><mi>log</mi><mo stretchy="false">(</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext>w</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>e</mtext><mspace width="0.25em" /><mover accent="true"><mi>c</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>p</mi></msub><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中,<i>i</i>∈<i>Pos</i>和<i>i</i>∈<i>Neg</i>分别表示第<i>i</i>个正样本预测框和第<i>i</i>个负样本预测框。</p>
                </div>
                <div class="p1">
                    <p id="79">为了防止正负样本比例过大,不易收敛,本文借鉴<i>SSD</i>提出新的<i>hgSSD</i>损失,将<i>L</i><sub>conf</sub>与<i>L</i><sub>loc</sub>按照1∶1的比例进行计算。同时,为了防止增加网络层数后造成的过拟合以及收敛较慢问题,本文将L<sub>2</sub>正则化项(即L<sub>2</sub>范数)加入损失函数。<i>hgSSD</i>损失函数如式(7)所示。</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>g</mi></mrow><mo>)</mo></mrow><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>o</mtext><mtext>n</mtext><mtext>f</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>c</mi></mrow><mo>)</mo></mrow><mo>+</mo><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>g</mi></mrow><mo>)</mo></mrow><mo>+</mo><mi>β</mi><mi>L</mi><msub><mrow></mrow><mn>2</mn></msub><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中,<i>β</i>为L<sub>2</sub>正则化因子,此处取0.1,保证惩罚值与原损失值相当。</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag"><b>4 反卷积层与特征融合</b></h3>
                <div class="p1">
                    <p id="83"><i>hgSSD</i>最关键的部分是反卷积与特征融合(如图2所示),二者须同步进行。本文对每一层特征使用图3所示网络模块进行反卷积与特征融合操作。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>4.1 反卷积(Deconvolution)结构</b></h4>
                <div class="p1">
                    <p id="85">图3中,步骤A为反卷积部分,输入特征首先进行反卷积将特征放大,随后进行1次卷积核3*3、步长为1的卷积操作,目的是消除反卷积操作中填充的冗余数据,最后再进行正则化处理。反卷积虽为卷积的逆过程,但却不是将卷积前的值还原,而是使用类似<i>Dilation</i>的方法对结果进行填充,填充后反卷积值与卷积前并不完全相同。</p>
                </div>
                <div class="p1">
                    <p id="86">反卷积操作将由<i>Conv</i>11开始(图3中横向箭头),只对新添加的<i>SSD Layer</i>进行反卷积操作,保留<i>VGG</i> 16结构。原因有2:首先,由<i>Conv</i>11开始,反卷积得到的输出特征分别为1*1,3*3,5*5,10*10,20*20,而<i>SSD</i>中<i>Conv</i>7输出特征结构为19*19,需要浪费资源进行调整;其次,<i>Conv</i>3_3、<i>Conv</i>4_3本就属于浅层特征层,具有小目标提取能力,可以保留。反卷积过程中所使用的卷积核(<i>Kernel</i>)、步长(<i>Stride</i>)和填充(<i>Pad</i>)以及其他参数如表2所示。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表2 反卷积结构参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Parameters of Deconvolution structure</b></p>
                    <p class="img_note"></p>
                    <table id="87" border="1"><tr><td><br />Layer</td><td>Dconv(Y/N)</td><td>Output Scale</td><td>Filter/Kernel/Stride/Pad</td></tr><tr><td><br />Conv11</td><td>Y</td><td>1 * 1</td><td>512 / 3 / 1 / valid</td></tr><tr><td><br />pDconv12</td><td>Y</td><td>3 * 3</td><td>512 / 3 / 1 / valid</td></tr><tr><td><br />pDconv13</td><td>Y</td><td>5 * 5</td><td>512 / 2 / 2 / same</td></tr><tr><td><br />pDconv14</td><td>N</td><td>10 * 10</td><td>/</td></tr><tr><td><br />FC7</td><td>N</td><td>19 * 19</td><td>/</td></tr><tr><td><br />Conv4_3</td><td>N</td><td>38 * 38</td><td>/</td></tr><tr><td><br />Conv3_3</td><td>N</td><td>75 * 75</td><td>/</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="88">表2中,<i>Dconv</i>代表是否进行反卷积,Output Scale代表当前层输出尺寸的大小,Filter/Kernel/Stride/Pad则是反卷积操作时使用的参数。反卷积后特征与原SSD特征融合时特征语义更加明显。反卷积和融合操作中所有批归一化BN(Batch Normalization)层的作用都是增强网络的鲁棒性,防止网络中权重偏置溢出。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>4.2 特征融合(Feature Fusion)结构</b></h4>
                <div class="p1">
                    <p id="90">浅层网络可用来检测小目标,但浅层网络语义性不强,若只进行反卷积运算而不进行特征融合,通过一次次的卷积、反卷积之后又会丢失部分信息,对小目标的检测更加不利。本文采用融合的方式,与反卷积过程同步进行,将高层特征依次向下与浅层特征融合,最终使得特征层语义信息和分辨率都得以保留。特征融合思想来源于FPN网络。FPN给出2种特征融合预测,如图4所示。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 特征融合原理(引用图)[18]" src="Detail/GetImg?filename=images/JSJK201909015_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 特征融合原理(引用图)<citation id="219" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Principle of feature fusion (reference)<citation id="220" type="reference"><link href="184" rel="bibliography" /><sup>[18]</sup></citation></p>

                </div>
                <div class="p1">
                    <p id="92">图4中Predict即特征预测。图4a表示特征融合后再预测,即将所有特征都融合到最后的高分辨率特征图中进行预测。图4b为融合后的多尺度预测,边融合边预测。本文的特征融合部分借鉴图4b的原理。在进行特征融合前需要对待融合SSD Layer进行处理(图3中步骤<i>B</i>),即对特征进行2次卷积并激活,其卷积各项参数如表3所示。</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表3 DSSD特征层参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Parameters of DSSD feature layers</b></p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />Layer</td><td>Original</td><td>Scale</td><td>AddWith</td><td>Filter/Kernel<br />/Stride</td></tr><tr><td><br />Feat10</td><td>Conv10_2</td><td>3 * 3</td><td>pDconv11</td><td>512 / 3 / 1</td></tr><tr><td><br />Feat9</td><td>Conv9_2</td><td>5 * 5</td><td>pDconv12</td><td>512 / 3 / 1</td></tr><tr><td><br />Feat8</td><td>Conv8_2</td><td>10 * 10</td><td>pDconv13</td><td>512 / 3 / 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="94">表3中,Feat8～Feat10为新特征层,Original为原SSD中特征,Scale为输出大小,AddWith为与之融合的反卷积层,Filter/Kernel/Stride为卷积操作各参数,此处Pad全部选择same,不改变特征层大小,否则无法进行融合。</p>
                </div>
                <div class="p1">
                    <p id="95">预处理后便可进行特征融合。融合可以采用2种方式:特征值各元素求和(element-wisesum)或各元素求积(element-wiseproduct)。实验中发现各元素求积方式可提供更高的精度。进行各元素求积融合后对所有参数进行ReLU激活便可形成新的hgSSD反卷积特征层。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>5 训练与实验分析</b></h3>
                <div class="p1">
                    <p id="97">本文实验基于Ubantu16.04,GPU使用GTX 1063。使用VGG16初始化SSD模型进行80 000次迭代。对于hgSSD的训练采用如下2种方法:方法1:使用训练好的SSD模型初始化hgSSD模型,此时关闭已初始化部分的微调,训练剩余结构80 000次,最后开放原SSD结构进行微调80 000次,两阶段迭代次数为1∶1。方法2:直接使用VGG16预训练模型对hgSSD开放训练160 000次。训练优化算法使用动量梯度下降法(Momentum)与随机梯度下降法SGD(Stochastic Gradient Descent)相结合。实验发现,在此GPU下,当学习率设置为0.001,<i>Batch</i>_<i>size</i>大小为8(网络加深后参数变多,可适当改变)时,采用方法2网络会更加稳定,收敛速度更快。训练集:VOC2007(2 095)+VOC2012(9 583)行人数据集(11 678)。测试集:VOC2007TEST(2 097)。</p>
                </div>
                <div class="p1">
                    <p id="98">本文使用准确率和检测时间来衡量hgSSD模型的性能。使用IOU值来作为是否标定图中目标的标准。计算检测框(<i>B</i><sub>df</sub>)与真实框(<i>B</i><sub>gt</sub>)的IOU值,计算公式如式(8)所示:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mo>=</mo><mfrac><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>B</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>f</mtext></mrow></msub><mstyle displaystyle="true"><mo>∩</mo><mi>B</mi></mstyle><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>B</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>f</mtext></mrow></msub><mstyle displaystyle="true"><mo>∪</mo><mi>B</mi></mstyle><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">其中,<i>S</i>为区域面积。<i>a</i>即计算后阈值,文中取阈值为0.3。实验得到的SSD模型检测结果如图5所示,hgSSD模型检测结果如图6所示。图5中,SDD模型对于小目标(图5a、图5e、图5f)、复杂场景(图5b、图5d)、行人遮挡(图5c、图5e)等情况都有漏检。hgSSD模型检测结果如图6所示,对小目标的检测能力增强。对图5中漏检的小目标和被遮挡的行人等的检测率都有一定提升,最为明显的效果如图6a所示。虽然hgSSD模型能检测到很多密集度较为复杂的行人,但由于特征融合缘故,可能会出现小部分误检或者多检,如图6b所示。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SSD300实验结果图" src="Detail/GetImg?filename=images/JSJK201909015_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 SSD300实验结果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Experimental results of SSD300</p>

                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 hgSSD实验效果图" src="Detail/GetImg?filename=images/JSJK201909015_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 hgSSD实验效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Experimental effect of hgSSD</p>

                </div>
                <div class="p1">
                    <p id="103">本文对于检测精度的定义如下:在<i>B</i><sub>df</sub>与<i>B</i><sub>gt</sub>匹配的过程中,检测成功的目标数为<i>TP</i>(True Positive),误检的目标数为<i>TN</i>(True Negative),未匹配成功的目标数为<i>FN</i>(False Negative)。本文准确率计算公式如式(9)所示:</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">按照上述不同训练超参数(Hyper-parameters),分别使用原SSD模型与本文模型迭代得到的结果如表4所示。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表4 各参数训练结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Training results of hyper-parameters</b></p>
                    <p class="img_note"></p>
                    <table id="106" border="1"><tr><td><br />模型</td><td>预训练模型</td><td>学习率</td><td>迭代次数<br />(×10<sup>3</sup>)</td><td>行人检测<br />精度</td></tr><tr><td><br />SSD300</td><td>VGG 16</td><td>0.000 4</td><td>80</td><td>0.793</td></tr><tr><td><br />SSD300</td><td>VGG 16</td><td>0.000 4</td><td>160</td><td>0.801</td></tr><tr><td><br />hgSSD</td><td>VGG 16</td><td>0.000 4</td><td>80</td><td>0.788</td></tr><tr><td><br />hgSSD</td><td>SSD300_8W</td><td>0.000 4</td><td>160</td><td>0.820</td></tr><tr><td><br />hgSSD</td><td>SSD300_16W</td><td>0.000 4</td><td>160</td><td>0.816</td></tr><tr><td><br />hgSSD</td><td>SSD300_16W</td><td>0.001 0</td><td>160</td><td>0.832</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="107">从表4可以看出:若使用VGG 16作为预训练模型,SSD300比hgSSD的精度略高,但使用SSD300训练hgSSD,精度明显提升。当学习率分别为0.001和0.000 4时,训练hgSSD学习率大者训练精度略高。当使用迭代160 000次的SSD300作为预训练模型,学习率为0.001时,达到了实验中的最高精度0.832。借鉴文献<citation id="221" type="reference">[<a class="sup">10</a>,<a class="sup">12</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">21</a>,<a class="sup">21</a>]</citation>的实验结果,在其他模型上使用本文训练集做类似训练,并计算每幅图像的处理时间,结果如表5所示。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表5 各模型训练效果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Accuracy of each model</b></p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td><br />模型</td><td>准确率/%</td><td>处理时间/s</td></tr><tr><td><br />SSD300</td><td>80.1</td><td>0.021</td></tr><tr><td><br />hgSSD300</td><td>83.2</td><td>0.029</td></tr><tr><td><br />FSSD300</td><td>84.1</td><td>0.025</td></tr><tr><td><br />DSSD321</td><td>83.4</td><td>0.067</td></tr><tr><td><br />YOLO</td><td>72.0</td><td>0.014</td></tr><tr><td><br />R-FCN</td><td>82.9</td><td>0.103</td></tr><tr><td><br />FasterR-CNN</td><td>80.7</td><td>0.132</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="109">表5表明,hgSSD模型相比于YOLO、R-FCN、FasterR-CNN模型,检测精度都有提高,达到了与DSSD321模型相近的准确率,但检测速度却远大于同使用反卷积方法的DSSD模型的。检测精度和检测速度相比于state-of-the-art网络FSSD还有待提高。准确率随迭代次数变化折线图如图7所示。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909015_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 准确率/迭代次数折线图" src="Detail/GetImg?filename=images/JSJK201909015_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 准确率/迭代次数折线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909015_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Line chart of accuracy / iteration numbers</p>

                </div>
                <h3 id="111" name="111" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="112">本文将SSD网络与反卷积、特征融合相结合实现hgSSD模型。本文模型可应用于检测远距离行人目标以及多场景下的小目标行人、类似遮挡行人(如图6所示)。模型在场景复杂度较高、行人密集以及遮挡情况严重时仍会漏检(如图6e所示),检测效果还有提升余地。若采用更深层的ResNet代替VGG网络或许会取得更高的精度,但更换深层网络会使得实时性下降。如何进一步精简网络结构,实现嵌入式使用或进一步提高检测精度和检测实时性,降低误检率是下一步努力的方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="150">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable Object Detection Using Deep Neural Networks">

                                <b>[1]</b> Erhan D,Szegedy C,Toshev A,et al.Scalable object detection using deep neural networks [C]//Proc of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,2014:2155-2162.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15103100173670&amp;v=MTQyODNCYXJLOUg5SFBybzlGWmV3TUNuczVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDhXYVJFPU5qNw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Panning A,Alhamadi A K,Niese R,et al.Facial expression recognition based on Haar-like feature detection[J].Pattern Recognition &amp; Image Analysis,2008,18(3):447-452.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for human detection">

                                <b>[3]</b> Dalal N,Triggs B.Histograms of oriented gradients for human detection [C]//Proc of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2005:886-893.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns">

                                <b>[4]</b> Ojala T,Pietikäinen M,Topi M.Multiresolution gray-scale and rotation invariant texture classification with local binary patterns [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2002,24(7):971-987.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002157166&amp;v=MTU0MzN0SE9yb3BDWmUwSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZiN0tJMTA9Tmo3QmFyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Burges C J C.A tutorial on support vector machines for pattern recognition [J].Data Mining and Knowledge Discovery,1998,2(2):121-167.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJQI&amp;filename=SJQIF3BBEE8CB3D8F8F091DF44E2969FFE8E&amp;v=MTQ5NzFIWWZPR1FsZkNwYlEzNU5CaHdybTl3NnM9TmlmYVo4VzdiS081Mm9jMkZ1aDdCQW94dVJZYTYwc0xUSHVYcmhzemNNVGlNTExxQ09OdkZTaVdXcjdKSUZwbWFCdQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Zhu J,Zou H,Rosset S,et al.Multi-class adaboost [J].Statistics and Its Interface,2009,2(3):349-360.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[7]</b> Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 25th International Conference on Neural Information Processing Systems,2012:1097-1105.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[8]</b> Girshick R,Donahue J,Darrell T,et al.Rich feature hierarchies for accurate object detection and semantic segmentation [C]//Proc of the 2014 IEEE Conference on Computer Vision and Pattern Recognition,2014:580-587.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[9]</b> Girshick R.Fast R-CNN [C]//Proc of the IEEE International Conference on Computer Vision,2015:1440-1448.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Ren S Q,HE K M,Girshick R,et al.Faster R-CNN:Towards realtime object detection with region proposal networks [J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[11]</b> He Kai-ming,Zhang Xiang-yu,Ren Shao-qing,et al.Deep residual learning for image recognition [J].arXiv:1512.03385v1,2015.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:object detection via regionbased fully convolutional networks">

                                <b>[12]</b> Dai Ji-feng,Li Yi,He Kai-ming,el al.R-FCN:Object detection via region-based fully convolutional networks [C]//Proc of the 30th International Conference on Neural Information Processing Systems,2016:379-387.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proc of the 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:779-788.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ssd:Single shot multibox detector">

                                <b>[14]</b> Liu Wei,Anguelov D,Erhan D,et al.SSD:Single shot multibox detector[C]//Proc of the 14th European Conference on Computer Vision (ECCV 2016),2016:21-37.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Redmom J,Farhadi A.YOLO9000:Better,faster,stronger[C]//Proc of the 2017 IEEE Conference on Computer Vision and Pattern Recognition,2017:6517-6525.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FSSD:Feature fusion single shot multibox detector">

                                <b>[16]</b> Li Zuo-xin,Zhou Fu-qiang.FSSD:Feature fusion single shot multibox detector [J].arXiv:1712.00960v1,2017.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DSSD :Deconvolutional single shot detector">

                                <b>[17]</b> Fu Cheng-yang,Liu Wei,Ranga A,et al.DSSD :Deconvolutional single shot detector [J].arXiv:1701.06659v1,2017.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">

                                <b>[18]</b> Lin Tsung-yi,Dollár P,Girshick R,et al.Feature pyramid networks for object detection [J].arXiv:1612.03144v2,2017.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[19]</b> Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition [J/OL].arXiv:1409.1556v6,2015.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding the effective receptive field in deep convolutional neural networks">

                                <b>[20]</b> Luo Wen-jie,Li Yu-jia,Urtasun R,et al.Understanding the effective receptive field in deep convolutional neural networks [C]//Proc of the 29th International Conference on Neural Information Processing Systems,2016:1-9.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201902016&amp;v=MjI5OThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1ViekxQeVBUZXJHNEg5ak1yWTlFWW8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Wu Peng-ying,Zhang Jian-ming,Peng Jian,et al.Research on pedestrian detection based on multi-layer convolution feature in real scene [J].CAAI Transactions on Intelligent Systems,2019,14(2):306-315.(in Chinese)
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 伍鹏瑛,张建明,彭建,等.多层卷积特征的真实场景下行人检测研究[J].智能系统学报,2019,14(2):306-315.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909015&amp;v=MjcwODlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYnpMTHo3QlpiRzRIOWpNcG85RVlZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
