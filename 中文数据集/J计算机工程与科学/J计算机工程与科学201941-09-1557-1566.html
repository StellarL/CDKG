<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132356241280000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909005%26RESULT%3d1%26SIGN%3dDbH74kzKWQsLSPVPNMCw7PuhBBQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909005&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909005&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909005&amp;v=MTEyNzN5N2tVcnpOTHo3QlpiRzRIOWpNcG85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;2 背景知识&lt;/b&gt; "><b>2 背景知识</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;2.1 卷积神经网络&lt;/b&gt;"><b>2.1 卷积神经网络</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;2.2 Winograd卷积算法&lt;/b&gt;"><b>2.2 Winograd卷积算法</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.3 Winograd稀疏算法&lt;/b&gt;"><b>2.3 Winograd稀疏算法</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;2.4 相关工作&lt;/b&gt;"><b>2.4 相关工作</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="&lt;b&gt;3 加速器设计&lt;/b&gt; "><b>3 加速器设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="&lt;b&gt;3.1 加速器概览&lt;/b&gt;"><b>3.1 加速器概览</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;3.2 处理单元&lt;/b&gt;"><b>3.2 处理单元</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;3.3 压缩编码&lt;/b&gt;"><b>3.3 压缩编码</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;3.4 PE的设计&lt;/b&gt;"><b>3.4 PE的设计</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;3.5 BUFFER模块的设计&lt;/b&gt;"><b>3.5 BUFFER模块的设计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="&lt;b&gt;4 实验准备及分析&lt;/b&gt; "><b>4 实验准备及分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#119" data-title="&lt;b&gt;4.1 实验准备&lt;/b&gt;"><b>4.1 实验准备</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;4.2 实验分析&lt;/b&gt;"><b>4.2 实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="&lt;b&gt;5 实验结果&lt;/b&gt; "><b>5 实验结果</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#154" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 传统&lt;i&gt;Winograd&lt;/i&gt;算法应用于稀疏网络">图1 传统<i>Winograd</i>算法应用于稀疏网络</a></li>
                                                <li><a href="#80" data-title="图2 Winograd稀疏算法,通过调整ReLU层在网络中的位置,来保持网络稀疏度">图2 Winograd稀疏算法,通过调整ReLU层在网络中的位置,来保持网络稀疏度</a></li>
                                                <li><a href="#91" data-title="图3 加速器设计概览">图3 加速器设计概览</a></li>
                                                <li><a href="#95" data-title="图4 PU模块结构示意图和运算过程伪代码">图4 PU模块结构示意图和运算过程伪代码</a></li>
                                                <li><a href="#99" data-title="图5 压缩编码过程">图5 压缩编码过程</a></li>
                                                <li><a href="#102" data-title="图6 压缩编码在BUFFER中的存储">图6 压缩编码在BUFFER中的存储</a></li>
                                                <li><a href="#106" data-title="图7 PE的结构图和工作流程图">图7 PE的结构图和工作流程图</a></li>
                                                <li><a href="#108" data-title="图9 buffer的结构图">图9 buffer的结构图</a></li>
                                                <li><a href="#109" data-title="图8 输入变换模块结构及工作流程">图8 输入变换模块结构及工作流程</a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表1 VGG-16网络参数&lt;/b&gt;"><b>表1 VGG-16网络参数</b></a></li>
                                                <li><a href="#125" data-title="图10 VGG-16网络权重稀疏分布情况">图10 VGG-16网络权重稀疏分布情况</a></li>
                                                <li><a href="#139" data-title="图11 相同乘法器数目下几种算法的VGG-16执行时间对比">图11 相同乘法器数目下几种算法的VGG-16执行时间对比</a></li>
                                                <li><a href="#143" data-title="图12 理论执行时间与实验中测出的实际执行时间的对比">图12 理论执行时间与实验中测出的实际执行时间的对比</a></li>
                                                <li><a href="#145" data-title="图13 加速器吞吐率曲线图">图13 加速器吞吐率曲线图</a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表2 与几种热门设计方案的性能对比&lt;/b&gt;"><b>表2 与几种热门设计方案的性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="196">


                                    <a id="bibliography_1" title=" Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[1]</b>
                                         Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-444.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_2" title=" Chetlur S,Woolley C,Vandermersch P,et al.cuDNN:Efficient primitives for deep learning[J].arXiv:1410.0759,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=cuDNN:Efficient primitives for deep learning">
                                        <b>[2]</b>
                                         Chetlur S,Woolley C,Vandermersch P,et al.cuDNN:Efficient primitives for deep learning[J].arXiv:1410.0759,2014.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_3" title=" Aydonat U,O’Connell S,Capalija D,et al.An OpenCLTM deep learning accelerator on Arria 10[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2017:55-64." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An OpenCLTM deep learning accelerator on arria 10">
                                        <b>[3]</b>
                                         Aydonat U,O’Connell S,Capalija D,et al.An OpenCLTM deep learning accelerator on Arria 10[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2017:55-64.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_4" title=" Alwani M,Chen H,Ferdman M,et al.Fused-layer CNN accelerators[C]//Proc of IEEE/ACM International Symposium on Microarchitecture,2016:1-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fused-layer CNN accelerators">
                                        <b>[4]</b>
                                         Alwani M,Chen H,Ferdman M,et al.Fused-layer CNN accelerators[C]//Proc of IEEE/ACM International Symposium on Microarchitecture,2016:1-12.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_5" title=" Sze V,Chen Y-H,Yang T-J,et al.Efficient processing of deep neural networks:A tutorial and survey[J].Proceedings of the IEEE,2017,105(12):2295-2329." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient processing of deep neural networks:a tutorial and survey">
                                        <b>[5]</b>
                                         Sze V,Chen Y-H,Yang T-J,et al.Efficient processing of deep neural networks:A tutorial and survey[J].Proceedings of the IEEE,2017,105(12):2295-2329.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_6" title=" Winograd S.Arithmetic complexity of computations[J].Philadelphia Society for Industrial &amp;amp; Applied Mathematics,1980,43(2):625-633." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Arithmetic complexity of computations">
                                        <b>[6]</b>
                                         Winograd S.Arithmetic complexity of computations[J].Philadelphia Society for Industrial &amp;amp; Applied Mathematics,1980,43(2):625-633.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_7" title=" Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[7]</b>
                                         Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556,2014.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_8" title=" Lavin A,Gray S.Fast algorithms for convolutional neural networks[C]//Proc of Computer Vision and Pattern Recognition,2016:4013-4021." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast algorithms for convolution neural networks">
                                        <b>[8]</b>
                                         Lavin A,Gray S.Fast algorithms for convolutional neural networks[C]//Proc of Computer Vision and Pattern Recognition,2016:4013-4021.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_9" title=" Han S,Mao H,Dally W J.Deep compression:Compressing deep neural networks with pruning,trained quantization and huffman coding[J].Fiber,2015,56(4):3-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Compression:Compressing Deep Neural Networks with Pruning,Trained Quantization and Huffman Coding">
                                        <b>[9]</b>
                                         Han S,Mao H,Dally W J.Deep compression:Compressing deep neural networks with pruning,trained quantization and huffman coding[J].Fiber,2015,56(4):3-7.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_10" title=" Han S,Liu X,Mao H,et al.EIE:Efficient inference engine on compressed deep neural network[J].ACM SIGARCH Computer Architecture News,2016,44(3):243-254." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EIE: Efficient Inference Engine on Compressed Deep Neural Network">
                                        <b>[10]</b>
                                         Han S,Liu X,Mao H,et al.EIE:Efficient inference engine on compressed deep neural network[J].ACM SIGARCH Computer Architecture News,2016,44(3):243-254.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_11" title=" Liu Xing-yu,Pool J,Han S,et al.Efficient sparse-Winograd convolutional neural networks[C]//Proc of the 6th International Conference on Learning Representations,2018:1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient sparse-Winograd convolutional neural networks">
                                        <b>[11]</b>
                                         Liu Xing-yu,Pool J,Han S,et al.Efficient sparse-Winograd convolutional neural networks[C]//Proc of the 6th International Conference on Learning Representations,2018:1.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_12" title=" Chen T,Du Z,Sun N,et al.DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning[J].ACM SIGPLAN Notices,2014,49(4):269-284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diannao:a small-footprint highthroughput accelerator for ubiquitous machine-learning">
                                        <b>[12]</b>
                                         Chen T,Du Z,Sun N,et al.DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning[J].ACM SIGPLAN Notices,2014,49(4):269-284.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_13" title=" Chen Y,Luo T,Liu S,et al.DaDianNao:A machine-learning supercomputer[C]//Proc of IEEE/ACM International Symposium on Microarchitecture,2014:609-622." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DaDianNao:A Machine-Learning Supercomputer">
                                        <b>[13]</b>
                                         Chen Y,Luo T,Liu S,et al.DaDianNao:A machine-learning supercomputer[C]//Proc of IEEE/ACM International Symposium on Microarchitecture,2014:609-622.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_14" title=" Chen Y,Krishna T,Emer J S,et al.Eyeriss:An energy-efficient reconfigurable accelerator for deep convolutional neural networks[J].IEEE Solid-State Circuits,2017,52(1):127-138." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Eyeriss:An EnergyEfficient Reconfigurable Accelerator for Deep Convolu-tional Neural Networks,&amp;quot;">
                                        <b>[14]</b>
                                         Chen Y,Krishna T,Emer J S,et al.Eyeriss:An energy-efficient reconfigurable accelerator for deep convolutional neural networks[J].IEEE Solid-State Circuits,2017,52(1):127-138.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_15" title=" Parashar A,Rhu M,Mukkara A,et al.SCNN:An accelerator for compressed-sparse convolutional neural networks[C]//Proc of ACM/IEEE International Symposium on Computer Architecture,2017:27-40." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SCNN:an accelerator for compressed-sparse convolutional neural networks">
                                        <b>[15]</b>
                                         Parashar A,Rhu M,Mukkara A,et al.SCNN:An accelerator for compressed-sparse convolutional neural networks[C]//Proc of ACM/IEEE International Symposium on Computer Architecture,2017:27-40.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_16" title=" Zhang C,Prasanna V.Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2017:35-44." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Frequency Domain Acceleration of Convolutional Neural Networks on CPU-FPGA Shared Memory System">
                                        <b>[16]</b>
                                         Zhang C,Prasanna V.Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2017:35-44.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_17" title=" Shen J Z,Huang Y,Wang Z,et al.Towards a uniform template-based architecture for accelerating 2D and 3D CNNs on FPGA[C]//Proc of 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2018:97-106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards a uniform template-based architecture for accelerating 2D and 3D CNNs on FPGA">
                                        <b>[17]</b>
                                         Shen J Z,Huang Y,Wang Z,et al.Towards a uniform template-based architecture for accelerating 2D and 3D CNNs on FPGA[C]//Proc of 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2018:97-106.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_18" title=" Chen Z,Li P,Sun G,et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2015:161-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks">
                                        <b>[18]</b>
                                         Chen Z,Li P,Sun G,et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2015:161-170.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_19" title=" Zhang C,Fang Z,Zhou P,et al.Caffeine:Towards uniformed representation and acceleration for deep convolutional neural networks[C]//Proc of 2016 IEEE/ACM Interna- tional Conference on Computer-Aided Design,2016:Article No.12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffeine:Towards uniformed representation and acceleration for deep convolutional neural networks">
                                        <b>[19]</b>
                                         Zhang C,Fang Z,Zhou P,et al.Caffeine:Towards uniformed representation and acceleration for deep convolutional neural networks[C]//Proc of 2016 IEEE/ACM Interna- tional Conference on Computer-Aided Design,2016:Article No.12.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_20" title=" Qiu J,Wang J,Yao S,et al.Going deeper with embedded FPGA platform for convolutional neural network[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2016:26-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with embedded fpga platform for convolutional neural network">
                                        <b>[20]</b>
                                         Qiu J,Wang J,Yao S,et al.Going deeper with embedded FPGA platform for convolutional neural network[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2016:26-35.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_21" title=" Gr&#246;tker T,Liao S,Martin G E,et al.System design with SystemC[M].Norwell:Kluwer Academic Publishers,2002." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=System design with SystemC">
                                        <b>[21]</b>
                                         Gr&#246;tker T,Liao S,Martin G E,et al.System design with SystemC[M].Norwell:Kluwer Academic Publishers,2002.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1557-1566 DOI:10.3969/j.issn.1007-130X.2019.09.005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Winograd稀疏算法的卷积神经网络加速器设计与研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E7%9D%BF&amp;code=41902420&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐睿</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E8%83%9C&amp;code=23817039&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E9%98%B3&amp;code=20282380&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭阳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E5%8F%8B&amp;code=39243223&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄友</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%89%BA%E7%85%8C&amp;code=41902419&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李艺煌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0269230&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国防科技大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着卷积神经网络得到愈加广泛的应用,针对其复杂运算的定制硬件加速器得到越来越多的重视与研究。但是,目前定制硬件加速器多采用传统的卷积算法,并且缺乏对神经网络稀疏性的支持,从而丧失了进一步改进硬件,提升硬件性能的空间。重新设计一款卷积神经网络加速器,该加速器基于Winograd稀疏算法,该算法被证明有效降低了卷积神经网络的计算复杂性,并可以很好地适应稀疏神经网络。通过硬件实现该算法,本文的设计可以在减少硬件资源的同时,获得相当大的计算效率。实验表明,相比于传统算法,该加速器设计方案将运算速度提升了近4.15倍;从乘法器利用率的角度出发,相比现有的其他方案,该方案将利用率最多提高了近9倍。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E9%80%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加速器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Winograd%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Winograd算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    徐睿（1994-），男，江苏沭阳人，硕士生，CCF会员（79592G），研究方向为加速器体系结构。E-mail:nudtxurui@gmail.com,通信地址：410073湖南省长沙市国防科技大学计算机学院&lt;image id="187" type="formula" href="images/JSJK201909005_18700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *马胜（1986-），男，湖南永州人，博士，副研究员，研究方向为计算机体系结构。E-mail:masheng@nudt.edu.cn,通信地址：410073湖南省长沙市国防科技大学计算机学院&lt;image id="189" type="formula" href="images/JSJK201909005_18900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    郭阳（1971-），男，浙江东阳人，博士，研究员，CCF会员（06794S），研究方向为微电子学和固体电子学。E-mail:guoyang@nudt.edu.cn,通信地址：410073湖南省长沙市国防科技大学计算机学院&lt;image id="191" type="formula" href="images/JSJK201909005_19100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    黄友（1994-），男，湖南长沙人，硕士生，研究方向为计算机体系结构。E-mail:hy690212@163.com,通信地址：410073湖南省长沙市国防科技大学计算机学院&lt;image id="193" type="formula" href="images/JSJK201909005_19300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    李艺煌（1995-），男，四川广安人，硕士生，研究方向为加速器体系结构。E-mail:liyihuang@qq.com,通信地址：410073湖南省长沙市国防科技大学计算机学院&lt;image id="195" type="formula" href="images/JSJK201909005_19500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61672526);</span>
                                <span>国防科技大学科研项目(ZK17-03-06);</span>
                    </p>
            </div>
                    <h1><b>A convolutional neural network accelerator based on Winograd-Sparse algorithm</b></h1>
                    <h2>
                    <span>XU Rui</span>
                    <span>MA Sheng</span>
                    <span>GUO Yang</span>
                    <span>HUANG You</span>
                    <span>LI Yi-huang</span>
            </h2>
                    <h2>
                    <span>School of Computer,National University of Defense Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>As convolutional neural networks are widely used, more researchers pay attention to customized hardware accelerators to solve the problem of complex computation. However, current hardware accelerators mostly use the traditional convolution algorithm. There is lack of support for sparse neural networks, and there is little improvement space for these accelerators from an algorithmic perspective. We redesign a convolutional neural network accelerator based on the Winograd-Sparse algorithm, and we prove that it can effectively reduce the computational complexity of convolutional neural networks and also well adapts to sparse neural networks. A combination of hardware and the algorithm can achieve considerable computational efficiency while reducing hardware resources. Experiments show that compared with the traditional algorithm, our accelerator can improve the computation speed by nearly 4.15 times. From the perspective of multiplier utilization, we have increase the utilization rate by up to 9 times in comparison with other existing designs</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=accelerator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">accelerator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Winograd%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Winograd algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparse%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparse network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Rui,born in 1994,MS candidate,CCF member(79592G),his research interest includes hardware accelerator architecture.Address:School of Computer,National University of Defense Technology,Changsha 410073,Hunan,P.R.China;
                                </span>
                                <span>
                                    MA Sheng,born in 1986,PhD,associate research fellow,his research interest includes computer architecture.Address:School of Computer,National University of Defense Technology,Changsha 410073,Hunan,P.R.China;
                                </span>
                                <span>
                                    GUO Yang,born in 1971,PhD,research fellow,CCF member(06794S),his research interests include microelectronics,and solid state electronics.Address:School of Computer,National University of Defense Technology,Changsha 410073,Hunan,P.R.China;
                                </span>
                                <span>
                                    HUANG You,born in 1994,MS candidate,his research interest includes computer architecture.Address:School of Computer,National University of Defense Technology,Changsha 410073,Hunan,P.R.China;
                                </span>
                                <span>
                                    LI Yi-huang,born in 1995,MS candi-date,his research interest includes hardware accelerator architecture.Address:School of Computer,National University of Defense Technology,Changsha 410073,Hunan,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-11</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="46">卷积神经网络CNNs(Convolutional Neural Networks)目前广泛地应用于各项计算机领域,如图像识别、推荐系统和语言处理。然而,训练和推导CNNs的时间是难以忍受的<citation id="238" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。原因便是CNNs引入了卷积层,增加了网络中的计算复杂性,带来了巨大的工作负载。这是目前CPU或是嵌入式端处理器所难以解决的。</p>
                </div>
                <div class="p1">
                    <p id="47">为了解决这个问题,许多方案被提出,如运算过程中采用GPU加速<citation id="239" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,或是使用FPGA、定制ASIC等硬件来完成卷积运算<citation id="240" type="reference"><link href="200" rel="bibliography" /><link href="202" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。这些方案大多是利用了卷积神经网络算法的并行性从而提高CNNs的效率。</p>
                </div>
                <div class="p1">
                    <p id="48">但是,GPU面积、功耗和使用的平台限制,使其很难在热门的移动端或是嵌入式端中得到广泛的应用。因而,采用FPGA或是ASIC进行定制化的硬件设计,在控制功耗和面积的情况下达到加速的目的,是一种十分高效的解决手段<citation id="241" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="49">然而,目前FPGA、ASIC方案在处理卷积层运算时更多的是采用直接卷积运算,鲜有使用其他算法的方案。这实际上只是通过硬件来优化运算效率,而忽视了软件或是算法层面的优化空间。从目前趋势来看,CNNs网络拓扑不断加深从而带来了更高的运算复杂度<citation id="242" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,因而选择其他的加速方案从而在有限的硬件资源下获得更多效益很有必要。</p>
                </div>
                <div class="p1">
                    <p id="50">本文将采用Winograd稀疏算法<citation id="243" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>作为基础来进行硬件方案设计。该算法被证实可以有效降低卷积的运算复杂度;同时,该算法还采用了稀疏网络的设计,从而减少工作负载和存储空间的需求。加之通过设计卷积神经网络硬件加速器,定制化地完成对该算法的支持,从而获得该算法给卷积运算带来的收益(目前主流运算平台并不支持该算法的直接实现,所以无法从该算法中获得提升空间)。</p>
                </div>
                <div class="p1">
                    <p id="51">与目前设计的传统硬件加速器不同,为了适应该算法,文中设计了全新的加速器结构。依照模块划分,首先设计了转化模块,通过最简单的加法操作,模块可以很迅速地将需要处理的数据转化到Winograd域中。其次为了利用网络的稀疏性,在处理引擎PE(Processing Engine)前端设计了0跳过模块,从而减少在计算过程中使用乘法器的压力。该设计还为已经离线处理好的稀疏权重数据设计了一种压缩编码方式,既可以减少存储需求,也能降低加速器的工作负载。最后,设计了线性BUFFER的缓存方式,从而尽可能地重用数据,减少对于DRAM的读取次数。</p>
                </div>
                <div class="p1">
                    <p id="52">在本文的最后,对方案进行了实验评估,对吞吐量、硬件资源及功耗进行全面分析。还与目前已有的其他方案进行了比较,包括采用传统卷积算法和Winograd加速算法的加速器方案。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag"><b>2 背景知识</b></h3>
                <h4 class="anchor-tag" id="54" name="54"><b>2.1 卷积神经网络</b></h4>
                <div class="p1">
                    <p id="55">卷积神经网络CNNs由一系列层组成,包括卷积层、线性整流函数ReLU(Rectified Linear Unit)<citation id="244" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>层、池化层和全连接层。其中,卷积层对CNNs做出了主要贡献,同时也占据了大部分计算时间。因此,许多卷积神经网络硬件加速器设计方案都侧重于加速卷积层。</p>
                </div>
                <div class="p1">
                    <p id="56">典型的卷积层包含输入特征图、卷积核(滤波器)和输出特征图(激活值)<citation id="245" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。在运算过程中,每一层都会通过一组<i>N</i>通道的输入,其大小为<i>H</i>×<i>W</i>(也就是<i>N</i>×<i>H</i>×<i>W</i>),每一个通道都对应有<i>M</i>组大小为<i>K</i>×<i>K</i>的卷积核(一共有<i>N</i>×<i>M</i>×<i>K</i>×<i>K</i>)。对应于同一输入通道,每一个卷积核都以相同的步幅<i>S</i>在对应的输入图上滑动。滑动所到位置,卷积核与输入图的重叠元素进行乘积并累加以生成一个输出值(对应坐标为(<i>r</i>,<i>c</i>)),最终会生成具有<i>M</i>通道的输出特征图。而对应于输入的不同通道,每个卷积操作产生的输出的同一位置将会被累加。每个输出特征图的大小为<i>R</i>×<i>C</i>,其中,<i>R</i>=(<i>H</i>-<i>K</i>)/<i>S</i>+1,<i>C</i>=(<i>W</i>-<i>K</i>)/<i>S</i>+1。卷积层的计算公式<citation id="246" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ο</mi><mi>u</mi><mi>t</mi><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>r</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo><mo>=</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi>Ν</mi></msubsup><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mn>0</mn></mrow><msup><mi>Κ</mi><mo>′</mo></msup></msubsup><mrow></mrow></mstyle><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>Κ</mi></msubsup><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mi>l</mi><mi>n</mi><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>S</mi><mo>×</mo><mi>r</mi><mo>+</mo><msup><mi>k</mi><mo>′</mo></msup><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>S</mi><mo>×</mo><mi>c</mi><mo>+</mo><mi>k</mi><mo stretchy="false">]</mo><mo>×</mo><mi>W</mi><mo stretchy="false">[</mo><mi>m</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>n</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mo stretchy="false">[</mo><mi>k</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中,<i>In</i>和<i>Out</i>表示三维阵列的输入和输出特征图,<i><b>W</b></i>表示卷积核。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>2.2 Winograd卷积算法</b></h4>
                <div class="p1">
                    <p id="60">Winograd卷积是一种新的CNNs快速算法,它源自Winograd的最小滤波算法<citation id="247" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。Winograd算法通过输入图和卷积核上的一系列变换来减少乘法运算次数。为了简单说明,先讨论一维的情况。首先定义,如果一次卷积的输出尺寸为<i>r</i>,卷积核长为<i>k</i>,则将这次卷积描述为<i>F</i>(<i>r</i>,<i>k</i>)。那么对于<i>F</i>(2,3),假设输入向量为<i><b>x</b></i>=(<i>x</i><sub>0</sub>,<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,<i>x</i><sub>3</sub>),卷积核为<i><b>w</b></i>=(<i>w</i><sub>0</sub>,<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>),输出向量为<i><b>y</b></i>=(<i>y</i><sub>0</sub>,<i>y</i><sub>1</sub>)。</p>
                </div>
                <div class="p1">
                    <p id="61">对于Winograd算法,首先要对输入向量和卷积核进行适当的变换(后文中称为Winograd变换):</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><msup><mi mathvariant="bold-italic">x</mi><mo>′</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold-italic">w</mi><mo>′</mo></msup><mo>=</mo><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mo>,</mo><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mo>,</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 传统Winograd算法应用于稀疏网络" src="Detail/GetImg?filename=images/JSJK201909005_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 传统<i>Winograd</i>算法应用于稀疏网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 1 <i>The traditional Winograd algorithm is applied to sparse networks</i></p>

                </div>
                <div class="p1">
                    <p id="64">其次,通过计算<i><b>x</b></i>′和<i><b>w</b></i>′的点积,可以得到以下结果:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mo>,</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">)</mo><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>-</mo><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mo>,</mo></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>4</mn></msub><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">最后,Winograd算法给出以下等式:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">y</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mn>3</mn></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>w</mi><msub><mrow></mrow><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi>f</mi><msub><mrow></mrow><mn>3</mn></msub></mtd></mtr><mtr><mtd><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>f</mi><msub><mrow></mrow><mn>3</mn></msub><mo>-</mo><mi>f</mi><msub><mrow></mrow><mn>4</mn></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">将结果整理成矩阵乘法后得到:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>⊙</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中,⊙表示对应元素相乘。<i><b>A</b></i>、<i><b>G</b></i>和<i><b>B</b></i>都是恒定的变换系数矩阵,由所处理的数据尺寸决定。对于<i>F</i>(2,3),有以下系数矩阵:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext> </mtext><mn>1</mn></mtd><mtd><mtext> </mtext><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mtext> </mtext><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext> </mtext><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>,</mo><mi mathvariant="bold-italic">G</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mtext> </mtext><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mtext> </mtext><mn>1</mn></mtd><mtd><mtext> </mtext><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd><mtd><mo>-</mo><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">可以看出,通过Winograd算法,一维卷积的乘法操作数由原先的<i>m</i>×<i>r</i>降低至(<i>m</i>+<i>r</i>-1)。</p>
                </div>
                <div class="p1">
                    <p id="73">相似地,二维Winograd算法<i>F</i>(<i>m</i>×<i>m</i>,<i>r</i>×<i>r</i>)可以描述为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">G</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">)</mo><mo>⊙</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">B</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="bold-italic">A</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">从上面的公式中可以看出,Winograd算法需要(<i>m</i>+<i>r</i>-1)×(<i>m</i>+<i>r</i>-1)乘法操作,而传统的卷积运算需要<i>m</i>×<i>m</i>×<i>r</i>×<i>r</i>乘法操作。如果在CNNs中采用Winograd算法来加速卷积计算,对于大小为3×3的卷积核,至少可以将运算复杂度降低2.25倍<citation id="248" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>(相当于<i>m</i>=2,<i>r</i>=3)。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.3 Winograd稀疏算法</b></h4>
                <div class="p1">
                    <p id="77">除了使用加速算法来提高卷积计算速度,Han等<citation id="249" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了一种新方案,即通过削减模型权重数量,减少神经网络连接数来提高CNNs的执行效率。同时,网络中为引入非线性因素而采用的激活函数——线性整流函数ReLU,也给网络中的激活值带来了动态的稀疏度,这对于减少工作负载,提高卷积执行速度都是有利的。</p>
                </div>
                <div class="p1">
                    <p id="78">然而,直接采用上述办法来对Winograd运算过程进行优化是没有效果的。如图1所示,由于Winograd算法存在变换过程,这一过程将会分别通过削减ReLU的权重和激活值重新填满0值,从而终结从稀疏度中获取收益的可能<citation id="250" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。因此,如果直接使用Winograd算法加速稀疏网络运算,可能使网络中乘法操作数增多;这一性能上的损失,将远远大于使用Winograd加速算法所带来的收益。</p>
                </div>
                <div class="p1">
                    <p id="79">Winograd加速算法所带来的收益无法弥补网络稀疏性消失所引起的性能损失。为此,Liu等<citation id="251" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了一种全新的算法——Winograd稀疏算法(Winograd-Sparse)。如图2所示,该算法主要对原Winogard算法作出2点改进。首先,算法将ReLU层放置到Winograd变换之后,从而使得参与元素相乘的输入依然保持稀疏性;另外,对已经完成转换的权重进行削减,使得其在参与乘法运算时依然保持稀疏性,从而减少了乘法操作数目。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Winograd稀疏算法,通过调整ReLU层在网络中的位置,来保持网络稀疏度" src="Detail/GetImg?filename=images/JSJK201909005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Winograd稀疏算法,通过调整ReLU层在网络中的位置,来保持网络稀疏度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Winograd-Sparse algorithm maintains the sparsity by adjusting the position of ReLU in the network</p>

                </div>
                <div class="p1">
                    <p id="81">进一步地,可以将其归结为式(8):</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo>=</mo><mi mathvariant="bold-italic">A</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">{</mo><mo stretchy="false">[</mo><mi>Ρ</mi><mi>r</mi><mi>u</mi><mi>n</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">w</mi><mi mathvariant="bold-italic">G</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>⊙</mo><mo stretchy="false">[</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">B</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mi mathvariant="bold-italic">B</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mi mathvariant="bold-italic">A</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">通过将网络中的ReLU和PRUNE移至Winograd域中,就可以在利用Winograd算法进行加速的同时利用网络的稀疏性减少乘法操作数目,进一步提高效率。这种改进后的CNNs网络,也称其为Winograd-ReLU网络。需要说明的是,这种网络将摒弃原先空间域中的卷积核,而采用Winograd域中的新权重,二者并不是完全等价的,需要重新训练。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>2.4 相关工作</b></h4>
                <div class="p1">
                    <p id="85">神经网络卷积层硬件加速器的研究早在几年前就已经开展了,如DianNao<citation id="252" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、DaDianNao<citation id="253" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和Eyeriss<citation id="254" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等,研究重点也涉及硬件并行开发到运算阵列改进、数据流优化、数据重用等。此外,也出现一批开始考虑改进算法来优化设计的方案,如EIE(Efficient Inference Engine)<citation id="255" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>引入稀疏网络来加速全连接层的运算速度,SCNN(Sparse CNN)<citation id="256" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>利用编码和硬件结构来加速稀疏网络下卷积计算的速度。同时,FPGA作为一个方便快捷的开发平台,也被许多开发者所采用,并试验新思路。文献<citation id="257" type="reference">[<a class="sup">4</a>]</citation>使用了神经网络层融合算法,从而提高了存储效率;文献<citation id="258" type="reference">[<a class="sup">16</a>]</citation>采用快速傅里叶变换来加速运算;文献<citation id="259" type="reference">[<a class="sup">17</a>]</citation>使用Winograd算法来加速卷积运算。</p>
                </div>
                <div class="p1">
                    <p id="86">在本文中,为了尽可能加速运算,节省硬件资源,提高计算效率,选择使用Winograd稀疏算法作为基础设计硬件,同时将基于SystemC平台进行仿真,并对硬件进行资源评估。</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag"><b>3 加速器设计</b></h3>
                <div class="p1">
                    <p id="88">在本节中,将对加速器方案进行全面介绍。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>3.1 加速器概览</b></h4>
                <div class="p1">
                    <p id="90">图3显示了加速器结构的概览。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 加速器设计概览" src="Detail/GetImg?filename=images/JSJK201909005_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 加速器设计概览  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Overview of the accelerator</p>

                </div>
                <div class="p1">
                    <p id="92">该加速器可以视作由以下几个部分组成,顶层控制(TOP  CONTROL)模块:主要负责数据的搬移;缓冲(BUFFER)模块:负载数据的暂存,又可以细分为输入缓存、权重数据缓存、权重索引缓存和输出缓存;处理单元PUs(Processing Units):负责完成Winograd稀疏算法的运算。其工作方式分为3个大步骤,读取阶段:由顶层控制发送地址,输入缓存和权重缓存读取外部DRAM中的数据;数据运算:PUs从BUFFER中读取输入数据、权重数据和权重索引,完成卷积运算;发送阶段:当输出完成最终的累加操作,再经由输出缓存发送至外部DRAM,最终完成计算。下面将对各个模块进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.2 处理单元</b></h4>
                <div class="p1">
                    <p id="94">处理单元PUs是构成计算模块的重要单元,专门负责处理输入的特征数据与权重数据。图4a显示的是PU的一个结构图,可以看出PU并不是计算模块的基本单元,它还由子模块处理引擎PEs(Processing Engines)和累加器构成。PU一次可以处理4组数据,分别对应着4组输入通道(图4a中的<i>n</i><sub>1</sub>～<i>n</i><sub>4</sub>)的数据和4组权重数据,经过计算产生1组输出通道(图4中的<i>m</i><sub>0</sub>)的数据,对应于同一个通道下的输出特征图。为了减少读取权重数据的次数,数据流采用权重固定的方式,只有当一个权重被完全使用完毕,才更换下一组数据。为了缓解存储压力,数据将分块传输,所以传入缓存的数据大小为<i>Th</i>*<i>Tw</i>,而输出数据尺寸为<i>Tr</i>*<i>Tc</i>。4组输入最终会分配给PU中对应的4个PE,由4个PE并行完成计算,并累加完成结果输出。数据具体计算流程可参照伪代码。需要说明的是,这里所提的权重是经过压缩编码后的数据,包含权重数据和权重索引。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 PU模块结构示意图和运算过程伪代码" src="Detail/GetImg?filename=images/JSJK201909005_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 PU模块结构示意图和运算过程伪代码  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Structure of PU module and pseudo 
 code of operation process</p>

                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>3.3 压缩编码</b></h4>
                <div class="p1">
                    <p id="97">由于使用的是Winograd域的稀疏网络结构,正如前文所介绍的,会大大削减权重数据的数据规模,从而使网络变得稀疏。但是,如果对稀疏数据不加处理直接存储,依然会在稀疏处做填0的处理,所以反而丧失了稀疏的效果。本文工作参照目前主流的稀疏矩阵编码的格式,并结合自身硬件特点与计算需求,提出了自己的压缩编码方案。该编码方案如下所示:</p>
                </div>
                <div class="p1">
                    <p id="98">考虑一个密度约为0.4的4×4大小的稀疏矩阵,如图5所示。首先,在数据结构中,数据不再以原先的矩阵形式存储,而是以线性形式将二维矩阵以一维形式存储。然后,将非0元素的数据存储至向量<i><b>Data</b></i>中,将非0元素的位置信息存储到向量<i><b>Index</b></i>中,位置信息由(<i>l</i>×4+<i>v</i>)来表示,其中<i>l</i>表示数据在矩阵中的行,<i>v</i>表示数据在矩阵中的列。另外,在<i><b>Index</b></i>第1位中存储矩阵中所有非0元素的个数。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 压缩编码过程" src="Detail/GetImg?filename=images/JSJK201909005_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 压缩编码过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Compression coding</p>

                </div>
                <div class="p1">
                    <p id="100">考虑到<i><b>Index</b></i>中存储数据的最大值是16,所以使用5位的无符号整型数据来存储。而对于<i><b>Data</b></i>数据使用定点16位来保存,以在保证正确率的同时,减少存储空间。</p>
                </div>
                <div class="p1">
                    <p id="101">由于权重数据是已经转换到Winograd域中的,所以对于原先使用的3×3卷积核,将通过变换转为4×4大小的矩阵,并在训练过程中,添加稀疏度。之后,对于权重进行编码并存储。需要注意的是,由于二者存储所使用的数据位数不同,所以在设计硬件上的BUFFER时,会对数据与索引进行分开存储。数据与索引的联系依靠自身在BUFFER的位置与<i><b>Index</b></i>的第1位。由于索引的第1位存储的是非零元素的个数,这也是同一组下权重数据向量的长度和剩余索引向量的长度。如图6所示,在读取第1组权重时,先读取BUFFER中的第1位,获知该组数据长度为6,则读取指针将向后移动6位并依次读取(图6中深色部分),之后,再读取1位,获得下一组数据长度。重复上述过程(读取图6中浅色部分)。所以,通过索引第1位,可以很轻易地定位到每一组权重在BUFFER中的位置,并将权重数据与索引联系起来。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 压缩编码在BUFFER中的存储" src="Detail/GetImg?filename=images/JSJK201909005_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 压缩编码在BUFFER中的存储  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Storage of compressed code in BUFFER</p>

                </div>
                <div class="p1">
                    <p id="103">通过使用上述压缩编码方案,既可以起到存储稀疏格式的目的,也能减少权重数据所需的存储空间。同时,为了适应压缩编码,我们也设计了新的PE方案。这些将在后面进行介绍。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>3.4 PE的设计</b></h4>
                <div class="p1">
                    <p id="105">为了适应新的编码方式,同时也为了进一步缩减乘法器规模,本文设计了一个全新的PE方案。该PE模块的工作过程主要分为3个步骤,第1步,完成对输入特征图的处理;第2步,由索引引导完成稀疏数据结构下的计算;第3步,将输出结果逆转换,由Winograd域转回空间域中,如图7所示。下面对这几个步骤进行详细介绍。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 PE的结构图和工作流程图" src="Detail/GetImg?filename=images/JSJK201909005_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 PE的结构图和工作流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Structure and work flow of PE</p>

                </div>
                <div class="p1">
                    <p id="107">工作的第1阶段必须将输入转到Winograd域中,完成这一操作的是转换模块,从前面的章节可以看出,Winograd的变化过程中参与的参数矩阵十分简单,只涉及到对数字符号位的改变和加减运算<citation id="260" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,因此不需要太复杂的计算模块。模块设计如图8所示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 buffer的结构图" src="Detail/GetImg?filename=images/JSJK201909005_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 buffer的结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 9 Structure of buffer</p>

                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 输入变换模块结构及工作流程" src="Detail/GetImg?filename=images/JSJK201909005_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 输入变换模块结构及工作流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 Structure and workflow of 
 inputting transformation module</p>

                </div>
                <div class="p1">
                    <p id="110">图8也显示了变换模块数据处理的过程,运算步骤按照矩阵乘法的步骤来实现。</p>
                </div>
                <div class="p1">
                    <p id="111">ReLU模块的目的是实现ReLU函数功能。ReLU函数功能是保留所有正数,而将所有非正数变为0,该功能采用检测符号位来实现。</p>
                </div>
                <div class="p1">
                    <p id="112">工作的第2阶段将完成矩阵之间对应元素相乘的操作。由于已经将权重数据压缩并编码,所以输入输出数据在整个计算流程中都以一维向量的形式表示。为了利用压缩编码减少工作负载,将索引作为控制信号,选通所有要参与运算的输入输出寄存器,同时跳过所有输入为0值的寄存器,之后将输入传入并行的乘法器阵列中,完成计算并输出。由于采用压缩编码的方式,大多数的乘法操作会被跳过,所以在本文的设计中,处理16位输入单个PE中的乘法器数目只用了8个。后面的实验会证明只设计8个乘法器是足够的。</p>
                </div>
                <div class="p1">
                    <p id="113">最后1个阶段,由于Winograd算法计算得到的结果依然是Winograd域表达形式,所以需要再次通过转换模块,将乘法器输出的结果由Winograd域转换到空间域中,完成正常结果的输出,转换模块的设计类似于第1阶段的模块,这里不再赘述。</p>
                </div>
                <div class="p1">
                    <p id="114">上述工作流程由于互不干扰,可以流水进行,所以可以达到很高的数据吞吐率。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>3.5 BUFFER模块的设计</b></h4>
                <div class="p1">
                    <p id="116">权重模块的BUFFER在压缩编码一节已经做了相当详细的阐述。这一节将对输入特征的BUFFER模块做出介绍。</p>
                </div>
                <div class="p1">
                    <p id="117">由于PE模块在处理输入数据时,选择将二维矩阵处理成为一维向量,所以在选择BUFFER存储时,本文设计使用线性结构的BUFFER,这样可以直接把数据按照需要的一维形式传送到PU模块进行运算。图9为输入特征BUFFER模块,当前需要计算的数据块存储在其中。如果以横向看,存储方式为线性方式,会将特征图中同一个通道下的不同数据块读入线性BUFFER里,对应着图9中多个小方块;而纵向,存储对应于不同通道(<i>TN</i>)的数据。由于在设计中Winograd算法1次处理1个4×4(对应图9中<i>Th</i>×<i>Tw</i>)大小的矩阵块,所以在从DRAM读取到BUFFER过程中,同一通道下的数据,会以4×4大小的窗口在数据上以步长为2进行纵向滑动选取。当然这一过程存在数据重用,在第1个数据块读取完毕后,就只读取之后的2行数据(图9中深色部分为重用部分)。重用的数据对应于线性BUFFER的虚线部分,读取指针会在区域反复读取以重用数据,可重用的数据规模大约是<i>H</i>×<i>Tw</i>。</p>
                </div>
                <h3 id="118" name="118" class="anchor-tag"><b>4 实验准备及分析</b></h3>
                <h4 class="anchor-tag" id="119" name="119"><b>4.1 实验准备</b></h4>
                <div class="p1">
                    <p id="120">SystemC是一种基于C++语言的用于系统设计的计算机语言,是用C++编写的一组库和宏。它是为了提高电子系统设计效率而逐渐发展起来的产物。本文使用SystemC精确模拟各个模块和线程,以统计设计在特定工作负载下的周期数。</p>
                </div>
                <div class="p1">
                    <p id="121">工作负载设定为VGG-16<citation id="261" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>网络,网络架构参数如表1所示,其卷积层运算都为3×3的卷积运算,十分适合使用Winograd运算进行加速。另外,已经事先对网络进行了压缩,在保证正确率的同时,我们将权重统一压缩至原先密度的40%,并事先完成了编码存储。需要说明的是,第1层网络作为原始输入未经过ReLU层处理,其输入特征图的密度是100%。</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表1 VGG-16网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 VGG-16 network parameters</b></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br /></td><td>Input <br />Channel <i>N</i></td><td>Output <br />Channel <i>M</i></td><td>Input Size <br /><i>H</i>(<i>W</i>)</td><td>Kernel <br />size <i>K</i></td><td>Weight <br />Density <i>ρ</i>/%</td></tr><tr><td>conv1_1</td><td>3</td><td>64</td><td>224</td><td>3</td><td>40</td></tr><tr><td><br />conv1_2</td><td>64</td><td>64</td><td>224</td><td>3</td><td>40</td></tr><tr><td><br />conv2_1</td><td>64</td><td>128</td><td>112</td><td>3</td><td>40</td></tr><tr><td><br />conv2_2</td><td>128</td><td>128</td><td>112</td><td>3</td><td>40</td></tr><tr><td><br />conv3_1</td><td>128</td><td>256</td><td>56</td><td>3</td><td>40</td></tr><tr><td><br />conv3_2</td><td>256</td><td>256</td><td>56</td><td>3</td><td>40</td></tr><tr><td><br />conv3_3</td><td>256</td><td>256</td><td>56</td><td>3</td><td>40</td></tr><tr><td><br />conv4_1</td><td>256</td><td>512</td><td>28</td><td>3</td><td>40</td></tr><tr><td><br />conv4_2</td><td>512</td><td>512</td><td>28</td><td>3</td><td>40</td></tr><tr><td><br />conv4_3</td><td>512</td><td>512</td><td>28</td><td>3</td><td>40</td></tr><tr><td><br />conv5_1</td><td>512</td><td>512</td><td>14</td><td>3</td><td>40</td></tr><tr><td><br />conv5_2</td><td>512</td><td>512</td><td>14</td><td>3</td><td>40</td></tr><tr><td><br />conv5_3</td><td>512</td><td>512</td><td>14</td><td>3</td><td>40</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">关于加速器的设计,设定加速器工作频率为200 MHz,由4个并行的PU构成,每个PU由4个PE组成,这样处理的输出通道数(<i>TM</i>)是4,输入通道数(<i>TN</i>)是4,对应16个不同的权重。</p>
                </div>
                <div class="p1">
                    <p id="124">PE中的乘法器设计为8个,也就是整个加速器会携带128个乘法器以完成操作,之所以这样选择是有原因的。图10是对VGG-16网络进行压缩后的第2层网络权重分布示意图,该稀疏网络将权重总体规模压缩至原先密度的40%(在保证正确率基本不变的情况下)。由于事先将权重转为Winograd域,此时权重正常大小应该是4×4,而经过压缩后,单个权重元素数目将不再确定,范围是0～16(对应于图10中的横坐标),但依然可以看出大多数权重元素个数都是少于8的,并且当元素数目大于8后,元素越多的权重,数量越少,这也是其他层的分布趋势。同时,考虑到输入会经过ReLU层,根据统计,工作负载会进一步缩减40%～50%,所以选择8个乘法器是合理的,完全可以在1个周期下满足绝大多数的乘法需求。当然也会存在极少数乘法操作需求大于8的情况,本文的策略是将暂停流水,8个乘法器将花费2个周期来完成乘法操作。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 VGG-16网络权重稀疏分布情况" src="Detail/GetImg?filename=images/JSJK201909005_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 VGG-16网络权重稀疏分布情况  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 10 Sparsity distribution of VGG-16 network weights</p>

                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>4.2 实验分析</b></h4>
                <div class="p1">
                    <p id="127">在本节将建立一个分析模型,从理论上来分析加速器的性能。处理的Winograd卷积过程为<i>F</i>(<i>m</i>×<i>m</i>,<i>r</i>×<i>r</i>)=<i>F</i>(2×2,3×3),假设CNNs网络中某1层的参数为〈<i>N</i>,<i>M</i>,<i>H</i>,<i>W</i>,<i>R</i>,<i>C</i>,<i>K</i>,<i>S</i>〉,而加速器1次可以处理的数据规模是〈<i>TN</i>,<i>TM</i>,<i>TR</i>,<i>TC</i>〉,其中<i>N</i>代表输入特征图的通道数目,<i>M</i>代表输出特征图的通道数,<i>H</i>和<i>W</i>代表输入特征图的高和宽,<i>R</i>和<i>C</i>代表输出特征图的高和宽,<i>K</i>代表卷积核的尺寸,<i>S</i>则代表卷积的步长。<i>TN</i>和<i>TM</i>代表对特征图分块后的部分通道的数目,<i>TR</i>和<i>TC</i>则代表加速器1次输出下特征图的尺寸。</p>
                </div>
                <div class="p1">
                    <p id="128">之后,就可以模拟该层在加速器中最理想的执行时间。由于1次计算就可以输出<i>TR</i>×<i>TC</i>大小的特征图,所以可以得知单个通道下共需要多少乘法运算:</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext><mtext>l</mtext><mtext>e</mtext></mrow></msub><mo>=</mo><mfrac><mi>R</mi><mrow><mi>Τ</mi><mi>R</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>C</mi><mrow><mi>Τ</mi><mi>C</mi></mrow></mfrac><mo>⋅</mo><mo stretchy="false">(</mo><mi>Τ</mi><mi>R</mi><mo>+</mo><mi>Κ</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">其中,<i>Count</i><sub>single</sub>表示乘法计数,再加上多通道的情况:</p>
                </div>
                <div class="p1">
                    <p id="131" class="code-formula">
                        <mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>=</mo><mfrac><mi>Ν</mi><mrow><mi>Τ</mi><mi>Ν</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>Μ</mi><mrow><mi>Τ</mi><mi>Μ</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>R</mi><mrow><mi>Τ</mi><mi>R</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>C</mi><mrow><mi>Τ</mi><mi>C</mi></mrow></mfrac><mo>⋅</mo><mo stretchy="false">(</mo><mi>Τ</mi><mi>R</mi><mo>+</mo><mi>Κ</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="132">假设PE可以在单周期内吞吐(<i>TR</i>+<i>K</i>-1)<sup>2</sup>个数据,则所需时钟周期<i>Cycle</i>可表示为:</p>
                </div>
                <div class="p1">
                    <p id="133" class="code-formula">
                        <mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>y</mi><mi>c</mi><mi>l</mi><mi>e</mi><mo>=</mo><mfrac><mi>Ν</mi><mrow><mi>Τ</mi><mi>Ν</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>Μ</mi><mrow><mi>Τ</mi><mi>Μ</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>R</mi><mrow><mi>Τ</mi><mi>R</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>C</mi><mrow><mi>Τ</mi><mi>C</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="134">若已知器件运行频率<i>Freq</i>,则可计算得到运算所需的理想总时间<i>Time</i><sub>total</sub>为:</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mfrac><mi>Ν</mi><mrow><mi>Τ</mi><mi>Ν</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>Μ</mi><mrow><mi>Τ</mi><mi>Μ</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>R</mi><mrow><mi>Τ</mi><mi>R</mi></mrow></mfrac><mo>⋅</mo><mfrac><mi>C</mi><mrow><mi>Τ</mi><mi>C</mi></mrow></mfrac><mo>⋅</mo><mfrac><mn>1</mn><mrow><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">但并不是所有情况下PE都可以在单个周期下完成计算,所以实际运算中<i>Time</i>′<sub>total</sub>往往都大于<i>Time</i><sub>total</sub>。</p>
                </div>
                <h3 id="137" name="137" class="anchor-tag"><b>5 实验结果</b></h3>
                <div class="p1">
                    <p id="138">首先针对加速器方案中所选取的算法进行对比测试。假设加速器设计方案中只包含128个乘法器,选取3种不同的算法——传统直接卷积法、Winograd加速算法和Winograd稀疏算法分别进行硬件实现,并进行对比。实验结果如图11所示。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 相同乘法器数目下几种算法的VGG-16执行时间对比" src="Detail/GetImg?filename=images/JSJK201909005_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 相同乘法器数目下几种算法的VGG-16执行时间对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 11 Comparison of VGG-16 execution time among
 several algorithms with the same number of multipliers</p>

                </div>
                <div class="p1">
                    <p id="140">从图11可以看出,在绝大多数的情况下,Winograd稀疏算法都是执行速度最快的算法,而传统的直接卷积算法始终是最慢的。由于Winograd算法将卷积过程中的部分乘法运算替代为加法运算,所以降低了整体的计算复杂度(从原先<i>O</i>(<i>K</i><sup>2</sup>×<i>R</i><sup>2</sup>)降低到<i>O</i>((<i>K</i>+<i>R</i>-1)<sup>2</sup>))。从实验结果看,平均提升了2.25倍的运算速度。而Winograd稀疏算法是在Winograd算法的基础上设计的,即可以获得加速算法带来的优势,同时该算法利用网络稀疏性,进一步降低了工作负载,提高了乘法器利用率。从实验数据上来看,相比传统算法,Winograd稀疏算法将速度平均提高了4.15倍,相比Winograd也提高了近1.8倍。</p>
                </div>
                <div class="p1">
                    <p id="141">有趣的是,观察到在VGG-16网络的第1层,使用Winograd稀疏算法相比普通Winograd算法反而慢了近10%。这是由于第1层输入稀疏度不足的原因,导致流水中断,空等周期增加,下面的实验会给出进一步分析说明。</p>
                </div>
                <div class="p1">
                    <p id="142">图12显示的是通过SystemC精确模拟后得到的加速器执行每一层所花费的时间。其中理想时间表示通过上一节计算得到的理想条件下的执行时间,而实际时间表示通过精确模拟硬件得到的执行时间。可以看出,实际执行时间总是高于理想时间,其主要原因在于PE中设计的乘法器阵列1个周期可以进行8个乘法运算,但依然存在少数压缩后网络节点所需乘法数目大于8的情况(即压缩后的特征图与权重一一对应的非零元素超过8个),引起流水中断,造成延时。其中conv1_1层实际时间相比理想时间差距最大,相比之下增加了60%的执行时间,原因是输入为原始数据,未经过压缩。而其他层理想执行时间与实际执行时间差距在6%～13%浮动,原因是由于其他层输入和权重都经过了压缩处理,使得绝大多数的Winograd乘法操作需求都小于8,可以满足PE 1周期完成乘法计算的条件。</p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 理论执行时间与实验中测出的实际执行时间的对比" src="Detail/GetImg?filename=images/JSJK201909005_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图12 理论执行时间与实验中测出的实际执行时间的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 12 Comparison between ideal execution 
 time and actual execution time</p>

                </div>
                <div class="p1">
                    <p id="144">为了进一步观察加速器性能,我们还测定了加速器的吞吐率,如图13所示。图13中,带点的线表示在VGG-16网络某一层下工作一段时间测定的平均吞吐率,而直线是峰值吞吐率,是理论上可以达到的最大吞吐率。首先,可以观察到一个有趣的现象,网络第1层的计算性能是最差的,无论是峰值吞吐率还是平均吞吐率。造成这一现象的主要原因是,第1层原始输入的特征图通道数(<i>N</i>)是3,而设计的PE通道数(<i>TN</i>)是4,这就造成了处理能力的浪费,理论上下降了约25%。而在实际中表现还要差,原因就是第1层输入特征值不具备稀疏的特点。而其余层吞吐率趋势总体平稳,不管是峰值吞吐率还是平均吞吐率。原因有2,第1 VGG-16网络形状规整,由于加速器设计了4个PU(<i>TM</i>=4),每个PU又包含4个PE(<i>TN</i>=4),所以网络的输入或输出通道数最好可以是16的倍数,而这一点VGG-16网络可以满足;第2,加速器是针对稀疏网络设计的,为达到更高的利用率,就要求网络总体稀疏度较高,而这一点后面几层也都满足。由于上述原因,除conv1_1层以外,其他层都可以达到较高的利用率,平均利用率在92%左右。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909005_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 加速器吞吐率曲线图" src="Detail/GetImg?filename=images/JSJK201909005_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图13 加速器吞吐率曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909005_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 13 Graph of throughput rates</p>

                </div>
                <div class="p1">
                    <p id="146">有了每一层的执行时间和平均吞吐率,我们可以得到VGG-16网络卷积层的平均吞吐率是210.34 GOPS。</p>
                </div>
                <div class="p1">
                    <p id="147">由于我们设计加速器的目的是尽可能节约硬件资源,通过算法优化达到更理想的效率,与目前几种主流的加速器方案进行对比的结果如表2所示。</p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表2 与几种热门设计方案的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Performance comparison among several design schemes</b></p>
                    <p class="img_note"></p>
                    <table id="148" border="1"><tr><td>方案<br />来源</td><td>CNN</td><td>工作频率<br />/(MHz)</td><td>使用<br />精度</td><td>吞吐率<br />/(GOPS)</td><td>乘法器<br />数目</td><td>乘法器<br />利用率</td></tr><tr><td>文献[3]</td><td>AlexNet</td><td>303</td><td>16-bit <br />float</td><td>1 382</td><td>1 476</td><td>0.93</td></tr><tr><td><br />文献[18]</td><td>VGG</td><td>150</td><td>16-bit <br />fixed</td><td>137</td><td>780</td><td>0.18</td></tr><tr><td><br />文献[19]</td><td>VGG</td><td>200</td><td>16-bit <br />fixed</td><td>266</td><td>1 058</td><td>0.25</td></tr><tr><td><br />文献[17]</td><td>VGG</td><td>200</td><td>16-bit <br />fixed</td><td>943</td><td>756</td><td>1.20</td></tr><tr><td><br />本文</td><td>VGG</td><td>200</td><td>16-bit <br />fixed</td><td>210</td><td>128</td><td>1.60</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="149">表2中乘法器利用率公式为:</p>
                </div>
                <div class="p1">
                    <p id="150" class="code-formula">
                        <mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>乘</mtext><mtext>法</mtext><mtext>器</mtext><mtext>利</mtext><mtext>用</mtext><mtext>率</mtext><mo>=</mo><mfrac><mrow><mtext>吞</mtext><mtext>吐</mtext><mtext>率</mtext></mrow><mrow><mtext>乘</mtext><mtext>法</mtext><mtext>器</mtext><mtext>数</mtext><mtext>目</mtext></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="151">由于乘法器是ASIC中负载的设计模块,同时也属于FPGA中稀少的运算资源,所以如何尽可能地提高加速器设计中乘法器的利用率,是我们必须考虑的要素之一。</p>
                </div>
                <div class="p1">
                    <p id="152">表2中文献<citation id="263" type="reference">[<a class="sup">3</a>,<a class="sup">18</a>,<a class="sup">19</a>]</citation>使用的是传统卷积算法,可以看出其中乘法器利用率最高的是文献<citation id="262" type="reference">[<a class="sup">3</a>]</citation>方案,得益于高主频和使用Intel FPGA SDK 优化计算,其利用率达到了0.93,但该方案使用了大量乘法器,并不具有极高的参考价值。文献<citation id="264" type="reference">[<a class="sup">18</a>,<a class="sup">19</a>]</citation>方案利用率太低,只有0.18和0.25,无法与本文方案相比。</p>
                </div>
                <div class="p1">
                    <p id="153">文献<citation id="265" type="reference">[<a class="sup">17</a>]</citation>方案使用Winograd算法进行加速,其混合使用了<i>F</i>(4×4,3×3)和<i>F</i>(2×2,3×3)的卷积过程,使得其乘法器的利用率最高达到了1.2。而本文方案使用稀疏Winograd算法,使得利用率进一步提高到1.6,相比文献<citation id="266" type="reference">[<a class="sup">17</a>]</citation>方案,提高了1.5倍,相比文献<citation id="267" type="reference">[<a class="sup">18</a>]</citation>方案提高了近9倍。</p>
                </div>
                <h3 id="154" name="154" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="155">本文设计了一种新的卷积神经网络加速器结构,没用选择采用传统卷积算法作为设计基础,而是使用了稀疏结构下的Winograd算法来提高加速器的计算效率,同时也提高了加速器中乘法器的利用效率。同时,设计了一种简易的编码方式,并通过新的PE模块,尽可能减少运输模块的工作负载。本文还设计了线性BUFFER结构来提高数据重用的效率。通过实验可以看出,相比传统算法,在相同硬件资源下,该设计方案可以进一步提高4.15倍的计算速率。而在资源利用率上,与其他设计方案相对比,本文方案将乘法器的利用率提高了最多9倍。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="196">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[1]</b> Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=cuDNN:Efficient primitives for deep learning">

                                <b>[2]</b> Chetlur S,Woolley C,Vandermersch P,et al.cuDNN:Efficient primitives for deep learning[J].arXiv:1410.0759,2014.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An OpenCLTM deep learning accelerator on arria 10">

                                <b>[3]</b> Aydonat U,O’Connell S,Capalija D,et al.An OpenCLTM deep learning accelerator on Arria 10[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2017:55-64.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fused-layer CNN accelerators">

                                <b>[4]</b> Alwani M,Chen H,Ferdman M,et al.Fused-layer CNN accelerators[C]//Proc of IEEE/ACM International Symposium on Microarchitecture,2016:1-12.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient processing of deep neural networks:a tutorial and survey">

                                <b>[5]</b> Sze V,Chen Y-H,Yang T-J,et al.Efficient processing of deep neural networks:A tutorial and survey[J].Proceedings of the IEEE,2017,105(12):2295-2329.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Arithmetic complexity of computations">

                                <b>[6]</b> Winograd S.Arithmetic complexity of computations[J].Philadelphia Society for Industrial &amp; Applied Mathematics,1980,43(2):625-633.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[7]</b> Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556,2014.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast algorithms for convolution neural networks">

                                <b>[8]</b> Lavin A,Gray S.Fast algorithms for convolutional neural networks[C]//Proc of Computer Vision and Pattern Recognition,2016:4013-4021.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Compression:Compressing Deep Neural Networks with Pruning,Trained Quantization and Huffman Coding">

                                <b>[9]</b> Han S,Mao H,Dally W J.Deep compression:Compressing deep neural networks with pruning,trained quantization and huffman coding[J].Fiber,2015,56(4):3-7.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EIE: Efficient Inference Engine on Compressed Deep Neural Network">

                                <b>[10]</b> Han S,Liu X,Mao H,et al.EIE:Efficient inference engine on compressed deep neural network[J].ACM SIGARCH Computer Architecture News,2016,44(3):243-254.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient sparse-Winograd convolutional neural networks">

                                <b>[11]</b> Liu Xing-yu,Pool J,Han S,et al.Efficient sparse-Winograd convolutional neural networks[C]//Proc of the 6th International Conference on Learning Representations,2018:1.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diannao:a small-footprint highthroughput accelerator for ubiquitous machine-learning">

                                <b>[12]</b> Chen T,Du Z,Sun N,et al.DianNao:A small-footprint high-throughput accelerator for ubiquitous machine-learning[J].ACM SIGPLAN Notices,2014,49(4):269-284.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DaDianNao:A Machine-Learning Supercomputer">

                                <b>[13]</b> Chen Y,Luo T,Liu S,et al.DaDianNao:A machine-learning supercomputer[C]//Proc of IEEE/ACM International Symposium on Microarchitecture,2014:609-622.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Eyeriss:An EnergyEfficient Reconfigurable Accelerator for Deep Convolu-tional Neural Networks,&amp;quot;">

                                <b>[14]</b> Chen Y,Krishna T,Emer J S,et al.Eyeriss:An energy-efficient reconfigurable accelerator for deep convolutional neural networks[J].IEEE Solid-State Circuits,2017,52(1):127-138.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SCNN:an accelerator for compressed-sparse convolutional neural networks">

                                <b>[15]</b> Parashar A,Rhu M,Mukkara A,et al.SCNN:An accelerator for compressed-sparse convolutional neural networks[C]//Proc of ACM/IEEE International Symposium on Computer Architecture,2017:27-40.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Frequency Domain Acceleration of Convolutional Neural Networks on CPU-FPGA Shared Memory System">

                                <b>[16]</b> Zhang C,Prasanna V.Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2017:35-44.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards a uniform template-based architecture for accelerating 2D and 3D CNNs on FPGA">

                                <b>[17]</b> Shen J Z,Huang Y,Wang Z,et al.Towards a uniform template-based architecture for accelerating 2D and 3D CNNs on FPGA[C]//Proc of 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2018:97-106.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks">

                                <b>[18]</b> Chen Z,Li P,Sun G,et al.Optimizing FPGA-based accelerator design for deep convolutional neural networks[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2015:161-170.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffeine:Towards uniformed representation and acceleration for deep convolutional neural networks">

                                <b>[19]</b> Zhang C,Fang Z,Zhou P,et al.Caffeine:Towards uniformed representation and acceleration for deep convolutional neural networks[C]//Proc of 2016 IEEE/ACM Interna- tional Conference on Computer-Aided Design,2016:Article No.12.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with embedded fpga platform for convolutional neural network">

                                <b>[20]</b> Qiu J,Wang J,Yao S,et al.Going deeper with embedded FPGA platform for convolutional neural network[C]//Proc of ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,2016:26-35.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=System design with SystemC">

                                <b>[21]</b> Grötker T,Liao S,Martin G E,et al.System design with SystemC[M].Norwell:Kluwer Academic Publishers,2002.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909005" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909005&amp;v=MTEyNzN5N2tVcnpOTHo3QlpiRzRIOWpNcG85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
