<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132373203780000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201907017%26RESULT%3d1%26SIGN%3dGc0cA0f6bJ8iTjbaaYTr1kN6IBs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907017&amp;v=MjIxNDNCWmJHNEg5ak1xSTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bVdyckpMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#59" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#223" data-title=" (1) 点的空间距离。"> (1) 点的空间距离。</a></li>
                                                <li><a href="#224" data-title=" (2) 周期性。"> (2) 周期性。</a></li>
                                                <li><a href="#225" data-title=" (3) 连续或平滑性。"> (3) 连续或平滑性。</a></li>
                                                <li><a href="#226" data-title=" (4) 开始/结束的同步性。"> (4) 开始/结束的同步性。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;2 基音周期的准确提取&lt;/b&gt; "><b>2 基音周期的准确提取</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="&lt;b&gt;2.1 倒谱分析&lt;/b&gt;"><b>2.1 倒谱分析</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;2.2 基音周期谱图的绘制&lt;/b&gt;"><b>2.2 基音周期谱图的绘制</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;2.3 基音周期轨迹自动提取&lt;/b&gt;"><b>2.3 基音周期轨迹自动提取</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="&lt;b&gt;3 浊音分离&lt;/b&gt; "><b>3 浊音分离</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;3.1 浊音各次谐波的提取&lt;/b&gt;"><b>3.1 浊音各次谐波的提取</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;3.2 浊音的重构&lt;/b&gt;"><b>3.2 浊音的重构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="&lt;b&gt;4 实验及结果分析&lt;/b&gt; "><b>4 实验及结果分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="图1 语音段的同时组合和时序组合">图1 语音段的同时组合和时序组合</a></li>
                                                <li><a href="#80" data-title="图2 谐波结构示意图和波形图">图2 谐波结构示意图和波形图</a></li>
                                                <li><a href="#94" data-title="图3 某男声“那年正月新春”的语音波形图和基音谱图">图3 某男声“那年正月新春”的语音波形图和基音谱图</a></li>
                                                <li><a href="#112" data-title="图4 语音重构过程">图4 语音重构过程</a></li>
                                                <li><a href="#118" data-title="图5 原始语音“我不满六周岁”波形">图5 原始语音“我不满六周岁”波形</a></li>
                                                <li><a href="#119" data-title="图6 信噪比为20时的混合语音基音谱图和分离结果">图6 信噪比为20时的混合语音基音谱图和分离结果</a></li>
                                                <li><a href="#120" data-title="图7 信噪比为10时的混合语音基音谱图和分离结果">图7 信噪比为10时的混合语音基音谱图和分离结果</a></li>
                                                <li><a href="#121" data-title="图8 信噪比为0时的混合语音基音谱图和分离结果">图8 信噪比为0时的混合语音基音谱图和分离结果</a></li>
                                                <li><a href="#122" data-title="图9 信噪比为-5时的混合语音基音谱图和分离结果">图9 信噪比为-5时的混合语音基音谱图和分离结果</a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表1 &lt;i&gt;MOS&lt;/i&gt;评分标准&lt;/b&gt;"><b>表1 <i>MOS</i>评分标准</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表2 单人语音混合白噪声分离结果&lt;i&gt;MOS&lt;/i&gt;评分&lt;/b&gt;"><b>表2 单人语音混合白噪声分离结果<i>MOS</i>评分</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="146">


                                    <a id="bibliography_1" title=" Weng C, Yu D, Seltzer M L, et al.Deep neural networks for single-channel multi-talker speech recognition[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (10) :1670-1679." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMDB67B3D7EE868CC6897219A16855CFAE&amp;v=MTAzODBwYlEzNU5CaHdydTJ4YWs9TmlmSVk4ZktHTmErclB0Q0VaNEhDblJLdkJBYjR6aC9TWGFUclJROWZMZm5NOHZxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Weng C, Yu D, Seltzer M L, et al.Deep neural networks for single-channel multi-talker speech recognition[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (10) :1670-1679.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_2" title=" Loizou P C.Speech enhancement:Theory and practice[M].Boca Raton, FL:CRC Press, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech enhancement:Theory and practice">
                                        <b>[2]</b>
                                         Loizou P C.Speech enhancement:Theory and practice[M].Boca Raton, FL:CRC Press, 2013.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     Chen Xue-qin, Zhao He-ming, Chen Xiao-ping.A new method for pitch detection in strong noise based on CASA[D].Journal of Circuts and Systems, 2003, 8 (3) :128-131. (in Chinese) </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_4" title=" Cherry E C.Some experiments on the recognition of speech, with one and with two ears[J].The Journal of the Acoustical Society of America, 1953, 25 (5) :975-979." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Some experiments on the recognition of speech with one and two ears">
                                        <b>[4]</b>
                                         Cherry E C.Some experiments on the recognition of speech, with one and with two ears[J].The Journal of the Acoustical Society of America, 1953, 25 (5) :975-979.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_5" title=" Shamma S A, Elhilali M, Micheyl C.Temporal coherence and attention in auditory scene analysis[J].Trends in Neurosciences, 2011, 34 (3) :114-123." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300570495&amp;v=MTcyNzhLN0h0RE9ySTlGWWV3UENIVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDBkYnhNPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Shamma S A, Elhilali M, Micheyl C.Temporal coherence and attention in auditory scene analysis[J].Trends in Neurosciences, 2011, 34 (3) :114-123.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_6" title=" Masutomi K, Barascud N, Kashino M, et al.Sound segregation via embedded repetition is robust to inattention[J].Journal of Experimental Psychology:Human Perception and Performance, 2016, 42 (3) :386." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sound segregation via embedded repetition is robust to inattention">
                                        <b>[6]</b>
                                         Masutomi K, Barascud N, Kashino M, et al.Sound segregation via embedded repetition is robust to inattention[J].Journal of Experimental Psychology:Human Perception and Performance, 2016, 42 (3) :386.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_7" title=" Zhao Li-heng.Monaural speech segregation based on computational auditory scene analysis[D].Hefei:University of Science and Technology of China, 2012. (in Chinese) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monaural speech segregation based on computational auditory scene analysis">
                                        <b>[7]</b>
                                         Zhao Li-heng.Monaural speech segregation based on computational auditory scene analysis[D].Hefei:University of Science and Technology of China, 2012. (in Chinese) 
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_8" title=" Hu G, Wang D L.An auditory scene analysis approach to monaural speech segregation[M]//Topics in Acoustic Echo and Noise Control.New York:Springer, 2006:485-515." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An auditory scene analysis approach to monau-ral speech segregation">
                                        <b>[8]</b>
                                         Hu G, Wang D L.An auditory scene analysis approach to monaural speech segregation[M]//Topics in Acoustic Echo and Noise Control.New York:Springer, 2006:485-515.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_9" title=" Hu G, Wang D L.A tandem algorithm for pitch estimation and voiced speech segregation[J].IEEE Transactions on Audio, Speech, and Language Processing, 2010, 18 (8) :2067-2079." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A tandem algorithm for pitch estimation and voiced speech segregation">
                                        <b>[9]</b>
                                         Hu G, Wang D L.A tandem algorithm for pitch estimation and voiced speech segregation[J].IEEE Transactions on Audio, Speech, and Language Processing, 2010, 18 (8) :2067-2079.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_10" title=" Hu K, Wang D L.An unsupervised approach to cochannel speech separation[J].IEEE Transactions on Audio, Speech, and Language Processing, 2013, 21 (1) :122-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Unsupervised Approach to Cochannel Speech Separation">
                                        <b>[10]</b>
                                         Hu K, Wang D L.An unsupervised approach to cochannel speech separation[J].IEEE Transactions on Audio, Speech, and Language Processing, 2013, 21 (1) :122-131.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Liu Wen-ju, Nie Shuai, Liang Shan, et al.Deep learning based speech separation technology and its developments [J].Acta Automatica Sinica, 2016, 42 (6) :819-833. (in Chinese) </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_12" title=" Chen J, Wang D L.Long short-term memory for speaker generalization in supervised speech separation[J].The Journal of the Acoustical Society of America, 2017, 141 (6) :4705-4714." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory for speaker generalization in supervised speech separation">
                                        <b>[12]</b>
                                         Chen J, Wang D L.Long short-term memory for speaker generalization in supervised speech separation[J].The Journal of the Acoustical Society of America, 2017, 141 (6) :4705-4714.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_13" title=" Chen J, Wang D L.DNN based mask estimation for supervised speech separation[M]//Audio Source Separation.Cham:Springer, 2018:207-235." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DNN-based mask estimation for supervised speech separation">
                                        <b>[13]</b>
                                         Chen J, Wang D L.DNN based mask estimation for supervised speech separation[M]//Audio Source Separation.Cham:Springer, 2018:207-235.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_14" title=" Du J, Tu Y, Xu Y, et al.Speech separation of a target speaker based on deep neural networks[C]//Proc of 2014 12th International Conference on Signal Processing (ICSP) , 2014:473-477." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech separation of a target speaker based on deep neural networks">
                                        <b>[14]</b>
                                         Du J, Tu Y, Xu Y, et al.Speech separation of a target speaker based on deep neural networks[C]//Proc of 2014 12th International Conference on Signal Processing (ICSP) , 2014:473-477.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_15" title=" Erdogan H, Hershey J R, Watanabe S, et al.Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks[C]//Proc of 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2015:708-712." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks">
                                        <b>[15]</b>
                                         Erdogan H, Hershey J R, Watanabe S, et al.Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks[C]//Proc of 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2015:708-712.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_16" title=" Xia S, Li H, Zhang X.Using optimal ratio mask as training target for supervised speech separation[J].arXiv preprint arXiv:1709.00917, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using optimal ratio mask as training target for supervised speech separation">
                                        <b>[16]</b>
                                         Xia S, Li H, Zhang X.Using optimal ratio mask as training target for supervised speech separation[J].arXiv preprint arXiv:1709.00917, 2017.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_17" title=" David M, Lavandier M, Grimault N, et al.Sequential stream segregation of voiced and unvoiced speech sounds based on fundamental frequency[J].Hearing Research, 2017, 344:235-243." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES19B8744E92E8FEB1CB8F0A399D6C6B58&amp;v=MjQ3NzdEY0xTQTdocFJ0QmY4R1NONytYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3J1Mnhhaz1OaWZPZmJLeGJObkxxNHN3YmVsNkJBcE12UmRnbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         David M, Lavandier M, Grimault N, et al.Sequential stream segregation of voiced and unvoiced speech sounds based on fundamental frequency[J].Hearing Research, 2017, 344:235-243.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_18" title=" Huang Xiu-xuan.Research on computational auditory scene analysis for concurrent speech signals[D].Guangzhou:South China University of Technology, 2004. (in Chinese) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on computational auditory scene analysis for concurrent speech signals">
                                        <b>[18]</b>
                                         Huang Xiu-xuan.Research on computational auditory scene analysis for concurrent speech signals[D].Guangzhou:South China University of Technology, 2004. (in Chinese) 
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     Zhao He-ming, Zhu Mei-hong, Yu Yi-biao, et al.A multi-pitch detecting method suitable for CASA[D].Acta Electronica Sinica, 2003, 31 (1) :123-126. (in Chinese) </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_20" title=" Jin Z, Wang D L.HMM-based multipitch tracking for noisy and reverberant speech[J].IEEE Transactions on Audio, Speech, and Language Processing, 2011, 19 (5) :1091-1102." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HMM-Based Multipitch Tracking for Noisy and Reverberant Speech">
                                        <b>[20]</b>
                                         Jin Z, Wang D L.HMM-based multipitch tracking for noisy and reverberant speech[J].IEEE Transactions on Audio, Speech, and Language Processing, 2011, 19 (5) :1091-1102.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_21" >
                                        <b>[21]</b>
                                     Yang Zhi-hua, Qi Dong-xu, Yang Li-hua.Detecting pitch period based on Hilbert-Huang transform[J].Chinese Journal of Computers, 2006, 29 (1) :106-115. (in Chinese) </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_22" title=" Williamson D S, Wang Y, Wang D L.Complex ratio masking for monaural speech separation[J].IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) , 2016, 24 (3) :483-492." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM46BF9A7B83C04FFD857A9D9C5AEDE6F6&amp;v=MDY3MDQ1TkJod3J1Mnhhaz1OaWZJWTdlK2JLZkYzb2czYk9oOERIaFB1V0liN3pnTVFRdnIzeGRFRE1iaFE4eVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         Williamson D S, Wang Y, Wang D L.Complex ratio masking for monaural speech separation[J].IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) , 2016, 24 (3) :483-492.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_3" title=" 陈雪勤, 赵鹤鸣, 陈小平.基于计算听觉场景分析的强噪声背景下基音检测方法[D].电路与系统学报, 2003, 8 (3) :128-131." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DLYX200303027&amp;v=MjYzOTBSbUZ5N21XcnJKSVNIU2RyRzRIdExNckk5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         陈雪勤, 赵鹤鸣, 陈小平.基于计算听觉场景分析的强噪声背景下基音检测方法[D].电路与系统学报, 2003, 8 (3) :128-131.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_7" title=" 赵立恒.基于计算听觉场景分析的单声道语音分离研究[D].合肥:中国科学技术大学, 2012." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1012503441.nh&amp;v=MjAwMzJGeTdtV3JySlZGMjZITGE0SGRYSXJwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         赵立恒.基于计算听觉场景分析的单声道语音分离研究[D].合肥:中国科学技术大学, 2012.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_11" title=" 刘文举, 聂帅, 梁山, 等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报, 2016, 42 (6) :819-833." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606003&amp;v=MTczMTZxQnRHRnJDVVJMT2VaZVJtRnk3bVdyckpLQ0xmWWJHNEg5Zk1xWTlGWjRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         刘文举, 聂帅, 梁山, 等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报, 2016, 42 (6) :819-833.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_18" title=" 黄秀轩.混叠语音的计算听觉场景分析研究[D].广州:华南理工大学, 2004." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2006129701.nh&amp;v=MjU5ODhGOWJNcnBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XcnJKVjEyN0dMSzY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         黄秀轩.混叠语音的计算听觉场景分析研究[D].广州:华南理工大学, 2004.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_19" title=" 赵鹤鸣, 朱美虹, 俞一彪, 等.一种适于计算声场景分析的混叠语音基音检测方法[D].电子学报, 2003, 31 (1) :123-126." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU200301029&amp;v=MTI3MTVtV3JySklUZlRlN0c0SHRMTXJvOUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         赵鹤鸣, 朱美虹, 俞一彪, 等.一种适于计算声场景分析的混叠语音基音检测方法[D].电子学报, 2003, 31 (1) :123-126.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_21" title=" 杨志华, 齐东旭, 杨力华.一种基于 Hilbert-Huang 变换的基音周期检测新方法[J].计算机学报, 2006, 29 (1) :106-115." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX200601010&amp;v=MjA0OTMzenFxQnRHRnJDVVJMT2VaZVJtRnk3bVdyckpMejdCZHJHNEh0Zk1ybzlFWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         杨志华, 齐东旭, 杨力华.一种基于 Hilbert-Huang 变换的基音周期检测新方法[J].计算机学报, 2006, 29 (1) :106-115.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(07),1266-1272 DOI:10.3969/j.issn.1007-130X.2019.07.017            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于计算听觉场景分析的单声道浊音分离</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%B8%BD%E5%A8%9C&amp;code=08740581&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张丽娜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BA%8C%E5%8D%8E&amp;code=08096686&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张二华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E5%86%9B%E4%BA%AE&amp;code=42267110&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江军亮</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0077991&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京理工大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对单声道语音分离中浊音分离的问题, 提出了一种准确估计基音周期的方法。首先, 以语音的短时平稳性和基音周期的连续性等为线索, 利用语音信号的倒谱峰值构成基音周期谱图, 并自动提取基音周期轨迹。然后, 利用谐波频率为基音频率整数倍的性质来拾取各次谐波的频谱。最后, 通过傅里叶逆变换对浊音进行重构。实验结果表明, 该方法能准确提取基音周期轨迹, 有效分离浊音信号。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%A1%E7%AE%97%E5%90%AC%E8%A7%89%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算听觉场景分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%88%86%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音分离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9F%BA%E9%9F%B3%E5%91%A8%E6%9C%9F%E8%BD%A8%E8%BF%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">基音周期轨迹;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%8A%E9%9F%B3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浊音;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张丽娜 (1992-) , 女, 河北无极人, 硕士生, 研究方向为多说话人语音分离。E-mail:1446602770@qq.com通信地址:210094江苏省南京市南京理工大学计算机科学与工程学院;
                                </span>
                                <span>
                                    张二华 (1967-) , 男, 湖北汉川人, 博士, 副教授, CCF会员 (15741M) , 研究方向为语音信号处理。E-mail:zherhua@163.com通信地址:210094江苏省南京市南京理工大学计算机科学与工程学院;
                                </span>
                                <span>
                                    江军亮 (1993-) , 男, 江西上饶人, 硕士生, 研究方向为多维语音特征可视化。E-mail:1319791967@qq.com通信地址:210094江苏省南京市南京理工大学计算机科学与工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>军委装备发展部“十三五”装备预研领域基金 (61403120102);</span>
                    </p>
            </div>
                    <h1><b>Monaural voiced speech separation based on computational auditory scene analysis</b></h1>
                    <h2>
                    <span>ZHANG Li-na</span>
                    <span>ZHANG Er-hua</span>
                    <span>JIANG Jun-liang</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering, Nanjing University of Science & Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of voiced speech separation in monaural speech separation, we propose an accurate pitch period estimation method. Firstly, using the short-term stability of speech and the continuity of the pitch period as clues, we use the cepstrum peak of speech signals to form the pitch spectrum, and the pitch period track is automatically extracted. Then, the spectrum of each harmonic is picked up by using the property that the harmonic frequency is an integer multiple of the fundamental frequency. Finally, the voiced speech is reconstructed by the inverse Fourier transform. Experimental results show that this method can accurately extract the pitch period track and effectively separate voiced signals.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=computational%20auditory%20scene%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">computational auditory scene analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20separation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech separation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pitch%20periodic%20track&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pitch periodic track;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=voiced%20speech&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">voiced speech;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHANG Li-na, born in 1992, MS candidate, her research interest includes multi-speaker speech separation.Address:School of Computer Science and Engineering, Nanjing University of Science &amp; Technology, Nanjing 210094, Jiangsu, P.R. China;
                                </span>
                                <span>
                                    ZHANG Er-hua, born in 1967, PhD, associate professor, CCF member (15741M) , his research interest includes speech signal processing.Address:School of Computer Science and Engineering, Nanjing University of Science &amp; Technology, Nanjing 210094, Jiangsu, P.R. China;
                                </span>
                                <span>
                                    JIANG Jun-liang, born in 1993, MS candidate, his research interest includes multidimensional speech feature visualization.Address:School of Computer Science and Engineering, Nanjing University of Science &amp; Technology, Nanjing 210094, Jiangsu, P.R. China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-05-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="59" name="59" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="60">噪声干扰下的语音分离是一项重要的研究课题, 在语音识别、信息处理和情报侦听等领域有广泛的应用<citation id="202" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。许多实际场景中只有一个声道信号, 因此如何在单一信道内将目标语音从混合信号中提取并分离出来, 是一个典型的单声道语音分离问题<citation id="203" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 受到研究人员的普遍重视。</p>
                </div>
                <div class="p1">
                    <p id="61">单声道语音分离虽然难度很大, 但在现实生活中却是有据可依的。人类的听觉系统具有很强的语音分辨能力, 在嘈杂的环境中, 人能够有效地倾听感兴趣的声音, 甚至只用一只耳朵也能很好地辨别并专注于某个人的声音, 排除噪声的干扰<citation id="204" type="reference"><link href="150" rel="bibliography" /><link href="190" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">3</a>]</sup></citation>。1953年, 英国科学家Cherry<citation id="205" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>将该现象称为“鸡尾酒会”问题。尽管目前人们对鸡尾酒会问题的机理还没有完全了解清楚, 但可以通过听觉实验揭示听觉感知的一些线索。1990年, 加拿大麦吉尔大学的Bregman提出了听觉场景分析ASA (Auditory Scene Analysis) 理论, 该理论沿用了视觉场景分析的概念, 并对听觉场景分析的一系列准则进行了归纳。此后, 人们根据这一理论, 对语音分离做了大量研究工作<citation id="207" type="reference"><link href="154" rel="bibliography" /><link href="156" rel="bibliography" /><link href="158" rel="bibliography" /><link href="192" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">7</a>]</sup></citation>。2006年美国俄亥俄州立大学的Hu等人<sup></sup><citation id="206" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了计算听觉场景分析 CASA (Computational Auditory Scene Analysis) 。该方法以人耳的听觉感知现象为线索, 通过计算机模拟人耳对目标语音的感知过程, 实现单声道语音的分离。目前基于CASA的语音分离已成为该领域的主流方法。</p>
                </div>
                <div class="p1">
                    <p id="62">2010年, Hu等人<citation id="208" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出一种用于浊音分离的tandem算法。该算法以理想二值掩模IBM (Ideal Binary Mask) 为计算目标, 通过基音跟踪来提取目标语音的基音轨迹。每一帧所有信道的自相关函数叠加之后在语音频率范围内形成最大峰值, 将最大峰值所对应的时延作为该帧提取的目标语音的基音频率。</p>
                </div>
                <div class="p1">
                    <p id="63">Hu等人<citation id="209" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>于2013年提出一种无监督的算法来分离两个说话人的混合语音。对于浊音分离, 使用无监督聚类进行同时组合, 以最大化类间和类内距离的比值完成时序组合, 形成对应目标语音信号的听觉流。</p>
                </div>
                <div class="p1">
                    <p id="64">国内外学者已经提出了许多监督分离算法<citation id="210" type="reference"><link href="166" rel="bibliography" /><link href="168" rel="bibliography" /><link href="194" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">11</a>]</sup></citation>, 近年来, 深度学习的引入使监督语音分离取得了很大的进展并提高了分离性能<citation id="211" type="reference"><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><link href="176" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="65">ASA理论将听觉感知声音的过程分为2个阶段。第1阶段是分段, 将混合语音分解为一系列的听觉片段, 每个听觉片段来自于同一个声源, 并分别代表一个重要的听觉事件。第2阶段是组合, 把来自同一声源的各个听觉片段组合在一起, 形成听觉感知流。</p>
                </div>
                <div class="p1">
                    <p id="66">人类对声音的感知过程是对声音分量的组合过程, 正是这种组合可以将混合声音中同一个人发出的声音分量组合到同一个声音流中。组合有2种原则:同时组合和时序组合。</p>
                </div>
                <div class="p1">
                    <p id="67">同时组合是将同一个声源在同一时间产生的不同频率的声音分量组合在一起。如图1所示, 声音分量<i>B</i>和<i>C</i>具有相同的开始时间和结束时间, 但分布于不同的频率范围内。根据ASA的同时组合原则, 人类的听觉系统会将2个同时开始或同时结束的声音分量组合在一起。如果2个声音分量的起止时间有差别, 听觉系统就认为这2个声音分量来自不同的声源。</p>
                </div>
                <div class="p1">
                    <p id="68">时序组合是指将同一个声源在不同时间产生的听觉片段按时间的先后顺序组合到一个声音流中。如图1所示, 听觉片段<i>A</i>和<i>B</i>具有类似的频率, 但分布于不同的时间段, 根据ASA的时序组合原则, 听觉片段<i>A</i>和<i>B</i>将组合到一起。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 语音段的同时组合和时序组合" src="Detail/GetImg?filename=images/JSJK201907017_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 语音段的同时组合和时序组合  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_069.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Simultaneous organization and sequential  organization of speech segments</p>

                </div>
                <div class="p1">
                    <p id="70">将声音信号看作一个时-频场景, 则在组合过程中用到的主要线索有<citation id="212" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>:</p>
                </div>
                <h4 class="anchor-tag" id="223" name="223"> (1) 点的空间距离。</h4>
                <div class="p1">
                    <p id="71">在时频平面上, 距离越近的听觉片段越可能被组合为同一声源。</p>
                </div>
                <h4 class="anchor-tag" id="224" name="224"> (2) 周期性。</h4>
                <div class="p1">
                    <p id="72">一组具有谐波关系的声音成分倾向于被组合在一起。</p>
                </div>
                <h4 class="anchor-tag" id="225" name="225"> (3) 连续或平滑性。</h4>
                <div class="p1">
                    <p id="73">能够形成连续或平滑时间轨迹的成分倾向于组合成同一声源。</p>
                </div>
                <h4 class="anchor-tag" id="226" name="226"> (4) 开始/结束的同步性。</h4>
                <div class="p1">
                    <p id="74">“同步开始”的频率成分倾向于组合在一起, “同步结束”的频率成分也被看作来自同一声源。</p>
                </div>
                <div class="p1">
                    <p id="75">语音一般由浊音和清音构成, 其中, 浊音是不可缺少的部分。基音频率是渐变的, 而且人类的听觉系统能够测量、跟踪基音轨迹并利用它来对频率成分分类<citation id="213" type="reference"><link href="180" rel="bibliography" /><link href="196" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">18</a>]</sup></citation>。根据这2个听觉感知线索, 把相邻时刻的听觉片段连接起来形成基音轨迹, 完成声音的分类。因此, 基音估计是浊音分离的关键步骤。</p>
                </div>
                <div class="p1">
                    <p id="76">浊音具有准周期性特征, 其频谱分布具有规律性, 有明显的谐波结构 (基音和多次谐波) , 在倒谱域有明显的峰值, 倒谱的峰值对应基音周期。清音类似于随机噪声, 能量主要集中于高频带, 没有谐波特性, 没有基音周期, 在倒谱域也没有峰值。和浊音相比, 噪声的频谱也没有谐波结构, 呈随机分布, 在倒谱域也没有峰值。倒谱能反映谐波结构特征, 可以通过倒谱来区分噪声、清音和浊音, 只有浊音存在倒谱峰值, 噪声和清音都没有倒谱峰值。图2a给出了谐波结构示意图, 图2b为波形图, 帧长为512, 各个谐波表现为横向的波纹。观察谐波结构图可以发现, 浊音具有明显的谐波结构, 而清音则没有。可以利用倒谱的峰值求取浊音的基音周期, 由基音周期得到基音频率, 采用梳状滤波提取同一轨迹段各次谐波的频谱, 从而实现浊音的分离。</p>
                </div>
                <div class="p1">
                    <p id="77">浊音能量较强, 是主音段, 浊音在语音的感知中起主要作用, 浊音的分离是语音分离的主要问题, 可以利用谐波结构特征对浊音进行分离。相比浊音, 清音能量较弱, 且没有明显的特征, 清音的分离一直是语音分离的难点。</p>
                </div>
                <div class="p1">
                    <p id="78">准确提取基音周期在浊音分离中起着至关重要的作用, 根据基音频率指示的整数倍位置拾取谐波的频谱进行同时组合, 从而实现浊音的分离。低信噪比情况下具有鲁棒性的基音估计算法仍是CASA语音分离研究的重点与难点<citation id="214" type="reference"><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="198" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="79">与Hu等人检测基音的方法不同, 本文用倒谱估计基音周期, 并根据基音周期轨迹排除偏离轨迹的虚假峰值, 提高基音周期的可靠性和鲁棒性。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 谐波结构示意图和波形图" src="Detail/GetImg?filename=images/JSJK201907017_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 谐波结构示意图和波形图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Diagram of harmonic structure and waveform</p>

                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>2 基音周期的准确提取</b></h3>
                <h4 class="anchor-tag" id="82" name="82"><b>2.1 倒谱分析</b></h4>
                <div class="p1">
                    <p id="83">根据语音信号的短时平稳性, 可将语音信号划分为一系列的时窗进行分析, 在一个短时窗内语音信号是平稳的, 一个时窗称为一帧。将语音信号采样序列划分为一系列长度为10～32 ms的帧, 分帧提取语音信号的短时特征。为了使帧与帧之间平滑过渡, 保持其连续性, 相邻帧之间重合半帧。</p>
                </div>
                <div class="p1">
                    <p id="84">倒谱法是频域中提取基音周期的有效方法<citation id="215" type="reference"><link href="186" rel="bibliography" /><link href="200" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">21</a>]</sup></citation>, 由于语音信号具有短时平稳性, 用短时傅里叶变换对语音信号的频谱进行分析<citation id="216" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, 相应的频谱称为“短时谱”。</p>
                </div>
                <div class="p1">
                    <p id="85">对第<i>m</i>帧语音信号<i>x</i> (<i>n</i>) 进行短时傅里叶变换, 得到短时谱:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>x</mi></mstyle><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mo>-</mo><mi>j</mi><mn>2</mn><mtext>π</mtext><mi>k</mi><mi>n</mi></mrow><mi>Ν</mi></mfrac></mrow></msup><mo>, </mo><mn>0</mn><mo>≤</mo><mi>k</mi><mo>≤</mo><mi>Ν</mi><mo>-</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中, <i>N</i>为帧长。频谱取对数后再做傅里叶逆变换可得到倒谱, 计算方法如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo>|</mo><mrow><mtext>l</mtext><mtext>n</mtext><mi>X</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mi>j</mi><mn>2</mn><mtext>π</mtext><mi>k</mi><mi>n</mi></mrow><mi>Ν</mi></mfrac></mrow></msup><mo>, </mo><mn>0</mn><mo>≤</mo><mi>n</mi><mo>≤</mo><mi>Ν</mi><mo>-</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">谐波的幅度受声道滤波特性影响, 而声道的滤波特征表现为频谱的包络, 一般变化较平缓, 表现为低频;谐波本身的变化频率为基音频率, 是高频特征。求倒谱的过程中将频谱取对数后, 频谱包络起伏变化的幅度降低, 使包络的频谱向低频方向迁移, 而谐波的变化频率基本不变, 使倒谱的峰值更尖锐, 有效提高了基音频率的精度。</p>
                </div>
                <div class="p1">
                    <p id="90">倒谱的本质是频谱的频谱, 由于浊音段具有谐波结构特征, 谐波的频率是基音频率的整数倍, 因此浊音段的频谱呈周期性变化, 其周期以基音频率为周期, 再经过一次傅里叶变换即可求出其倒谱频率 (倒谱的量纲是时间) 。</p>
                </div>
                <div class="p1">
                    <p id="91">只有具有谐波结构特征的浊音段才能在倒谱上有尖锐的基音周期峰值, 谐波越多, 倒谱峰值越尖锐, 基音周期的精度越高。静音段、噪声段和清音段无谐波结构特征, 其倒谱无峰值。在清音到浊音的过渡段, 谐波成分较少, 倒谱的峰值较低, 基音周期精度较低, 只有在谐波成分较多的稳定浊音段才能提取可靠的基音周期。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>2.2 基音周期谱图的绘制</b></h4>
                <div class="p1">
                    <p id="93">语音信号具有短时平稳性, 在时-频平面中, 具有相同或相近基音周期的谐波倾向于组合为同一个声源。因此, 基音周期轨迹的连续性表明该段浊音来自同一个声源。语音是有规律的、渐变的信号, 并且单个音节的浊音的基音周期轨迹是连续的, 无声段和清音段没有基音周期轨迹。因此, 利用连续的基音周期轨迹来提取基音周期的方法是可靠的、准确的。本文用“基音周期谱图” (简称基音谱图) 来提取基音周期轨迹。基音谱图由相邻帧的倒谱数据转换为像素点绘制而成, 像素的灰度 (0～255) 与倒谱值成反比, 能清楚地显示基音周期轨迹曲线。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 某男声“那年正月新春”的语音波形图和基音谱图" src="Detail/GetImg?filename=images/JSJK201907017_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 某男声“那年正月新春”的语音波形图和基音谱图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Speech waveform and pitch spectrum  of a male voice "New Year's spring"</p>

                </div>
                <div class="p1">
                    <p id="95">图3a为某男声“那年正月新春”和基音谱图, 图3b为该语音的波形图, 2图中分别标记每个字对应的位置。观察基音谱图可以发现, 基音周期轨迹曲线与浊音段相对应, 若浊音之间有清音段或无声段相间隔, 则基音周期轨迹会中断。</p>
                </div>
                <div class="p1">
                    <p id="96">清音和浊音之间的过渡段也有对应的基音周期轨迹, 但是过渡段的谐波成分不丰富, 倒谱的峰值不突出, 如图3a“那”和“年”字之间的基音周期轨迹曲线显示不清晰。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>2.3 基音周期轨迹自动提取</b></h4>
                <div class="p1">
                    <p id="98">观察图3发现, 基音谱图能清楚显示基音周期轨迹, 但是倒谱峰值可能较宽, 呈现一条较粗的曲线, 覆盖多个像素点, 同一帧只能提取一个基音周期。本文根据基音周期的短时连续性提出了一种基音周期轨迹的自动检测算法, 该算法包括以下主要步骤:</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"> (1) 8连通区域标记。</h4>
                <div class="p1">
                    <p id="100">通过8连通区域追踪检测基音周期轨迹的连通域, 以倒谱最大值的20%为阈值, 将倒谱值大于阈值的时频单元进行标记并选出倒谱值的局部最大值点。以局部最大值点为“种子”, 采取从上到下、从左往右的顺序对种子点的8邻域进行追踪, 并标记出倒谱值超过阈值的时频单元。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"> (2) 细化基音周期轨迹。</h4>
                <div class="p1">
                    <p id="102">在经过8连通区域标记之后, 提取出每帧中倒谱值最大的数据点来产生基音周期轨迹。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"> (3) 拟合与中值平滑处理。</h4>
                <div class="p1">
                    <p id="104">对提取的基音周期数据点通过三次多项式拟合和七点中值滤波进行平滑处理, 消除不合理的局部波动, 从而提高提取的基音周期的精度。</p>
                </div>
                <div class="p1">
                    <p id="105">将基音周期转换为基音频率后, 就可以利用梳状滤波器提取浊音的各次谐波。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag"><b>3 浊音分离</b></h3>
                <h4 class="anchor-tag" id="107" name="107"><b>3.1 浊音各次谐波的提取</b></h4>
                <div class="p1">
                    <p id="108">各次谐波信号的频率为基音频率的整数倍, 在精确提取基音频率的前提下, 可以较准确地提取出各次谐波的频谱。</p>
                </div>
                <div class="p1">
                    <p id="109">以浊音的谐波结构特性为线索, 将一组具有谐波关系的声音成分组合在一起。按连续的基音周期轨迹进行分段, 对段内每一帧, 根据基音频率的整数倍位置提取谐波的频谱进行同时组合。在一段连续的基音周期轨迹内, 每一帧的基音周期指示的各次谐波来自同一个声源。因此, 将不同时间产生的听觉片段进行时序组合, 按时间先后顺序组合到一个声音流中, 从而组成同一个说话人的整段语音。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>3.2 浊音的重构</b></h4>
                <div class="p1">
                    <p id="111">对每一帧, 根据各次谐波的频谱, 进行傅里叶逆变换, 得到分离后的单帧语音。对于按帧分离的语音, 单帧的频谱与原始单人的频谱是相近的, 仅在谐波频率点有能量, 其他频率点的能量接近于0。因此, 得到分离后的单帧语音后, 将同一基音周期轨迹段内的所有帧按语音帧的起始位置对齐后进行叠加, 并按参与叠加的语音帧个数取均值, 重构该浊语音段, 合成输出的分离语音, 重构过程如图4所示。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 语音重构过程" src="Detail/GetImg?filename=images/JSJK201907017_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 语音重构过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Process of speech reconstruction</p>

                </div>
                <h3 id="113" name="113" class="anchor-tag"><b>4 实验及结果分析</b></h3>
                <div class="p1">
                    <p id="114">实验对含噪语音信号进行分离, 原始语音为某女声“我不满六周岁”和某男声“那年正月新春”, 采样频率为16 kHz, 采样精度为16 bit, 与冲激噪声、白噪声以不同的信噪比<i>SNR</i> (Signal-Noise Ratio) 进行混合, 作为待分离的含噪语音信号。信噪比计算公式如式 (3) 所示:</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mi>log</mi><mrow><mo>[</mo><mrow><mfrac><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo>|</mo><mrow><mi>s</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo>|</mo><mrow><mi>v</mi><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">其中, <i>SNR</i>单位为dB, <i>s</i> (<i>n</i>) 为目标语音信号, <i>v</i> (<i>n</i>) 为噪声信号。</p>
                </div>
                <div class="p1">
                    <p id="117">本文以某女声“我不满六周岁”与白噪声分别按照信噪比为20, 10, 0, -5混合之后的语音信号进行分析说明。图5为原始语音信号“我不满六周岁”的语音波形, 图6a和图6b表示信噪比为20时的混合语音信号基音谱图和波形图, 图6c为分离语音信号。可以看出, 在信噪比为20时, 基音谱图上显示出明显、准确的基音周期轨迹, 分离语音信号波形十分接近原始语音信号波形。图7～图9分别为信噪比为10, 0, -5时的混合语音基音谱图和语音分离结果, 从图中可以看出, 随着噪声强度在一定范围内增加, 基音周期轨迹受到了一定影响, 但依然能看到清晰、准确的轨迹。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 原始语音“我不满六周岁”波形" src="Detail/GetImg?filename=images/JSJK201907017_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 原始语音“我不满六周岁”波形  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Waveform of the original speech  "I'm less than six years old"</p>

                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 信噪比为20时的混合语音基音谱图和分离结果" src="Detail/GetImg?filename=images/JSJK201907017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 信噪比为20时的混合语音基音谱图和分离结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Pitch spectrum and separation result of  mixed speech signals with a signal-to-noise ratio of 20</p>

                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 信噪比为10时的混合语音基音谱图和分离结果" src="Detail/GetImg?filename=images/JSJK201907017_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 信噪比为10时的混合语音基音谱图和分离结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Pitch spectrum and separation result of  mixed speech signals with a signal-to-noise ratio of 10</p>

                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 信噪比为0时的混合语音基音谱图和分离结果" src="Detail/GetImg?filename=images/JSJK201907017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 信噪比为0时的混合语音基音谱图和分离结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 Pitch spectrum and separation result of  mixed speech signals with a signal-to-noise ratio of 0</p>

                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907017_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 信噪比为-5时的混合语音基音谱图和分离结果" src="Detail/GetImg?filename=images/JSJK201907017_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 信噪比为-5时的混合语音基音谱图和分离结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907017_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 9 Pitch spectrum and separation result of  mixed speech signals with a signal-to-noise ratio of-5</p>

                </div>
                <div class="p1">
                    <p id="123">从图8a可以看出, 信号和噪声等强度混合时, 有一部分浊音被噪声影响, 但是基音周期轨迹仍然能清晰显示。信噪比为-5时, 仍然有较明显的基音周期轨迹, 并且能够分离出有一定可懂度的语音, 如图9所示。</p>
                </div>
                <div class="p1">
                    <p id="124">准确提取基音周期轨迹有助于改善语音质量, 平均意见分<i>MOS</i> (Mean Opinion Scores) 方法是最常用的语音质量评估法, 这是一种主观测试方法, 以人的实际听觉感知来评价语音的质量。<i>MOS</i>评分标准如表1所示。</p>
                </div>
                <div class="p1">
                    <p id="125">本文采用<i>MOS</i>评分和信噪比等客观指标对分离结果进行评估, 同时验证基音周期轨迹的鲁棒性和正确性。实验参评人数为50人且测评环境完全相同, 实验中白噪声下不同信噪比混合语音分离结果<i>MOS</i>评分如表2所示。在信噪比为-5的噪声环境下, 基音轨迹比较清晰, 证明了基音周期轨迹的鲁棒性, 因此以基音周期轨迹作为基音估计的依据也是较为可靠的。而且听觉实验结果理想, 噪声基本被滤除, 保证了语音的可懂度和语义。</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表1 <i>MOS</i>评分标准</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 <i>MOS</i> scoring standards</b></p>
                    <p class="img_note"></p>
                    <table id="126" border="1"><tr><td><br /><i>MOS</i><br />评分</td><td>质量级别</td><td>失真级别</td><td>收听注意力等级</td></tr><tr><td>5</td><td>优 (Excellent) </td><td>不察觉</td><td>可完全放松, 不需要注意力集中</td></tr><tr><td><br />4</td><td>良 (Good) </td><td>刚有察觉</td><td>需要注意, 但不需要明显集中</td></tr><tr><td><br />3</td><td>可 (Fair) </td><td>有察觉且感觉可恶</td><td>中等程度的注意力</td></tr><tr><td><br />2</td><td>差 (Poor) </td><td>明显察觉且可厌但可接受</td><td>需要集中注意力</td></tr><tr><td><br />1</td><td>不可接受 (Unacceptable) </td><td>不可忍受</td><td>即使努力去听, 也很难听懂</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit"><b>表2 单人语音混合白噪声分离结果<i>MOS</i>评分</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 <i>MOS</i> scores separation results of a single speech mixed white noise</b></p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td rowspan="2"><br /><i>MOS</i>评分</td><td colspan="5"><br /><i>SNR</i></td></tr><tr><td><br />20</td><td>10</td><td>5</td><td>0</td><td>-5</td></tr><tr><td><br />含噪语音</td><td>4.4</td><td>4</td><td>3.6</td><td>3</td><td>2.6</td></tr><tr><td><br />分离结果</td><td>4.9</td><td>4.6</td><td>4.4</td><td>3.9</td><td>3.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="129" name="129" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="130">CASA研究在单声道语音分离方面已经取得了较好的成果, 但仍然在基音的准确估计上面临一些挑战。基音估计的准确性对分离效果起决定性作用, 然而噪声的干扰容易对基音的估计产生影响。本文针对背景干扰下语音信号的语音分离, 提出了一种浊音分离方法。利用基音周期谱图, 以基音周期轨迹、短时平稳性和基音的连续性等为线索, 通过连通域追踪、曲线拟合和中值平滑的方法来提取精确的基音周期。在低信噪比的条件下, 也能较好地分离浊音。但是, 在极低的信噪比情况下, 基音周期轨迹会受到严重干扰, 需进一步研究更有效的基音周期轨迹估计方法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="218" type="formula" href="images/JSJK201907017_21800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张丽娜</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="220" type="formula" href="images/JSJK201907017_22000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">张二华</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="222" type="formula" href="images/JSJK201907017_22200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">江军亮</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="146">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMDB67B3D7EE868CC6897219A16855CFAE&amp;v=MjU1NTVHUWxmQ3BiUTM1TkJod3J1Mnhhaz1OaWZJWThmS0dOYStyUHRDRVo0SENuUkt2QkFiNHpoL1NYYVRyUlE5Zkxmbk04dnFDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Weng C, Yu D, Seltzer M L, et al.Deep neural networks for single-channel multi-talker speech recognition[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (10) :1670-1679.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech enhancement:Theory and practice">

                                <b>[2]</b> Loizou P C.Speech enhancement:Theory and practice[M].Boca Raton, FL:CRC Press, 2013.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 Chen Xue-qin, Zhao He-ming, Chen Xiao-ping.A new method for pitch detection in strong noise based on CASA[D].Journal of Circuts and Systems, 2003, 8 (3) :128-131. (in Chinese) 
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Some experiments on the recognition of speech with one and two ears">

                                <b>[4]</b> Cherry E C.Some experiments on the recognition of speech, with one and with two ears[J].The Journal of the Acoustical Society of America, 1953, 25 (5) :975-979.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300570495&amp;v=MTM2ODFJOUZZZXdQQ0hVOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsMGRieE09TmlmT2ZiSzdIdERPcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Shamma S A, Elhilali M, Micheyl C.Temporal coherence and attention in auditory scene analysis[J].Trends in Neurosciences, 2011, 34 (3) :114-123.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sound segregation via embedded repetition is robust to inattention">

                                <b>[6]</b> Masutomi K, Barascud N, Kashino M, et al.Sound segregation via embedded repetition is robust to inattention[J].Journal of Experimental Psychology:Human Perception and Performance, 2016, 42 (3) :386.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monaural speech segregation based on computational auditory scene analysis">

                                <b>[7]</b> Zhao Li-heng.Monaural speech segregation based on computational auditory scene analysis[D].Hefei:University of Science and Technology of China, 2012. (in Chinese) 
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An auditory scene analysis approach to monau-ral speech segregation">

                                <b>[8]</b> Hu G, Wang D L.An auditory scene analysis approach to monaural speech segregation[M]//Topics in Acoustic Echo and Noise Control.New York:Springer, 2006:485-515.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A tandem algorithm for pitch estimation and voiced speech segregation">

                                <b>[9]</b> Hu G, Wang D L.A tandem algorithm for pitch estimation and voiced speech segregation[J].IEEE Transactions on Audio, Speech, and Language Processing, 2010, 18 (8) :2067-2079.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Unsupervised Approach to Cochannel Speech Separation">

                                <b>[10]</b> Hu K, Wang D L.An unsupervised approach to cochannel speech separation[J].IEEE Transactions on Audio, Speech, and Language Processing, 2013, 21 (1) :122-131.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Liu Wen-ju, Nie Shuai, Liang Shan, et al.Deep learning based speech separation technology and its developments [J].Acta Automatica Sinica, 2016, 42 (6) :819-833. (in Chinese) 
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory for speaker generalization in supervised speech separation">

                                <b>[12]</b> Chen J, Wang D L.Long short-term memory for speaker generalization in supervised speech separation[J].The Journal of the Acoustical Society of America, 2017, 141 (6) :4705-4714.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DNN-based mask estimation for supervised speech separation">

                                <b>[13]</b> Chen J, Wang D L.DNN based mask estimation for supervised speech separation[M]//Audio Source Separation.Cham:Springer, 2018:207-235.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech separation of a target speaker based on deep neural networks">

                                <b>[14]</b> Du J, Tu Y, Xu Y, et al.Speech separation of a target speaker based on deep neural networks[C]//Proc of 2014 12th International Conference on Signal Processing (ICSP) , 2014:473-477.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks">

                                <b>[15]</b> Erdogan H, Hershey J R, Watanabe S, et al.Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks[C]//Proc of 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2015:708-712.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using optimal ratio mask as training target for supervised speech separation">

                                <b>[16]</b> Xia S, Li H, Zhang X.Using optimal ratio mask as training target for supervised speech separation[J].arXiv preprint arXiv:1709.00917, 2017.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES19B8744E92E8FEB1CB8F0A399D6C6B58&amp;v=MTk2NDBONytYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3J1Mnhhaz1OaWZPZmJLeGJObkxxNHN3YmVsNkJBcE12UmRnbURjTFNBN2hwUnRCZjhHUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> David M, Lavandier M, Grimault N, et al.Sequential stream segregation of voiced and unvoiced speech sounds based on fundamental frequency[J].Hearing Research, 2017, 344:235-243.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on computational auditory scene analysis for concurrent speech signals">

                                <b>[18]</b> Huang Xiu-xuan.Research on computational auditory scene analysis for concurrent speech signals[D].Guangzhou:South China University of Technology, 2004. (in Chinese) 
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 Zhao He-ming, Zhu Mei-hong, Yu Yi-biao, et al.A multi-pitch detecting method suitable for CASA[D].Acta Electronica Sinica, 2003, 31 (1) :123-126. (in Chinese) 
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HMM-Based Multipitch Tracking for Noisy and Reverberant Speech">

                                <b>[20]</b> Jin Z, Wang D L.HMM-based multipitch tracking for noisy and reverberant speech[J].IEEE Transactions on Audio, Speech, and Language Processing, 2011, 19 (5) :1091-1102.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_21" >
                                    <b>[21]</b>
                                 Yang Zhi-hua, Qi Dong-xu, Yang Li-hua.Detecting pitch period based on Hilbert-Huang transform[J].Chinese Journal of Computers, 2006, 29 (1) :106-115. (in Chinese) 
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM46BF9A7B83C04FFD857A9D9C5AEDE6F6&amp;v=MDI0ODJrPU5pZklZN2UrYktmRjNvZzNiT2g4REhoUHVXSWI3emdNUVF2cjN4ZEVETWJoUTh5WkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaHdydTJ4YQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> Williamson D S, Wang Y, Wang D L.Complex ratio masking for monaural speech separation[J].IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) , 2016, 24 (3) :483-492.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DLYX200303027&amp;v=MDUzMTdCdEdGckNVUkxPZVplUm1GeTdtV3JySklTSFNkckc0SHRMTXJJOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 陈雪勤, 赵鹤鸣, 陈小平.基于计算听觉场景分析的强噪声背景下基音检测方法[D].电路与系统学报, 2003, 8 (3) :128-131.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1012503441.nh&amp;v=MDM2NDJDVVJMT2VaZVJtRnk3bVdyckpWRjI2SExhNEhkWElycEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 赵立恒.基于计算听觉场景分析的单声道语音分离研究[D].合肥:中国科学技术大学, 2012.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606003&amp;v=Mjg2MDVMT2VaZVJtRnk3bVdyckpLQ0xmWWJHNEg5Zk1xWTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 刘文举, 聂帅, 梁山, 等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报, 2016, 42 (6) :819-833.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2006129701.nh&amp;v=MTgzNjIyN0dMSzZGOWJNcnBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XcnJKVjE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 黄秀轩.混叠语音的计算听觉场景分析研究[D].广州:华南理工大学, 2004.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU200301029&amp;v=MjI0MzlKSVRmVGU3RzRIdExNcm85SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XcnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 赵鹤鸣, 朱美虹, 俞一彪, 等.一种适于计算声场景分析的混叠语音基音检测方法[D].电子学报, 2003, 31 (1) :123-126.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX200601010&amp;v=MzA2OTdCdEdGckNVUkxPZVplUm1GeTdtV3JySkx6N0Jkckc0SHRmTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 杨志华, 齐东旭, 杨力华.一种基于 Hilbert-Huang 变换的基音周期检测新方法[J].计算机学报, 2006, 29 (1) :106-115.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201907017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907017&amp;v=MjIxNDNCWmJHNEg5ak1xSTlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bVdyckpMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
