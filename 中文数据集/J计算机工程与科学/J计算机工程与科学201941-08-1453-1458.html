<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132365972998750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201908017%26RESULT%3d1%26SIGN%3dqsWRLAKrftnqme%252blGhtVpATt9iM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908017&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908017&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908017&amp;v=MTYzMjZHNEg5ak1wNDlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bFZMM09MejdCWmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="&lt;b&gt;2 相关工作&lt;/b&gt; "><b>2 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;3 监督学习的混合自动编码器&lt;/b&gt; "><b>3 监督学习的混合自动编码器</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="&lt;b&gt;4.1 实验数据&lt;/b&gt;"><b>4.1 实验数据</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;4.2 实验方法及结果分析&lt;/b&gt;"><b>4.2 实验方法及结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 监督学习混合自动编码模型逐组训练分解">图1 监督学习混合自动编码模型逐组训练分解</a></li>
                                                <li><a href="#64" data-title="图2 2种方案分割效果图对比">图2 2种方案分割效果图对比</a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;表1 2种方案图像分割性能指标比对&lt;/b&gt;"><b>表1 2种方案图像分割性能指标比对</b></a></li>
                                                <li><a href="#73" data-title="图3 测试样本分割效果">图3 测试样本分割效果</a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表2 各种模型的分割性能比对&lt;/b&gt;"><b>表2 各种模型的分割性能比对</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="98">


                                    <a id="bibliography_1" title=" Geiger A.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012:3354-3361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Are we ready for autonomous driving?The KITTI vision benchmark suite">
                                        <b>[1]</b>
                                         Geiger A.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012:3354-3361.
                                    </a>
                                </li>
                                <li id="100">


                                    <a id="bibliography_2" title=" Ess A, Mueller T, Grabner H, et al.Segmentation-based urban traffic scene understanding[C]//Proc of the 20th British Machine Vision Conference, 2009:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentation-based urban traffic scene understanding">
                                        <b>[2]</b>
                                         Ess A, Mueller T, Grabner H, et al.Segmentation-based urban traffic scene understanding[C]//Proc of the 20th British Machine Vision Conference, 2009:1-11.
                                    </a>
                                </li>
                                <li id="102">


                                    <a id="bibliography_3" title=" Hinton G E, Zemel R S.Autoencoders, minimum description length and Helmholtz free energy[C]//Proc of the 6th International Conference on Neural Information Processing Systems, 1993:3-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Autoencoders,Minimum Description Length and Helmholtz Free Energy">
                                        <b>[3]</b>
                                         Hinton G E, Zemel R S.Autoencoders, minimum description length and Helmholtz free energy[C]//Proc of the 6th International Conference on Neural Information Processing Systems, 1993:3-10.
                                    </a>
                                </li>
                                <li id="104">


                                    <a id="bibliography_4" title=" Bengio Y, Lamblin P, Popovici D, et al.Greedy layer-wise training of deep networks[C]//Proc of the 19th International Conference on Neural Information Processing Systems, 2006:153-160." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Greedy layer-wise training of deep networks">
                                        <b>[4]</b>
                                         Bengio Y, Lamblin P, Popovici D, et al.Greedy layer-wise training of deep networks[C]//Proc of the 19th International Conference on Neural Information Processing Systems, 2006:153-160.
                                    </a>
                                </li>
                                <li id="106">


                                    <a id="bibliography_5" title=" Vincent P, Larochelle H, Bengio Y, et al.Extracting and composing robust features with denoising autoencoders[C]//Proc of the 25th International Conference on Machine Learning, 2008:1096-1103." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting and composing robust features with denoising autoencoders">
                                        <b>[5]</b>
                                         Vincent P, Larochelle H, Bengio Y, et al.Extracting and composing robust features with denoising autoencoders[C]//Proc of the 25th International Conference on Machine Learning, 2008:1096-1103.
                                    </a>
                                </li>
                                <li id="108">


                                    <a id="bibliography_6" title=" Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014:1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[6]</b>
                                         Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014:1.
                                    </a>
                                </li>
                                <li id="110">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     He K M, Zhang X Y, Ren S Q, et al.Deep residual learning for image recognition[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016:770-778.</a>
                                </li>
                                <li id="112">


                                    <a id="bibliography_8" title=" Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 25th International Conference on Neural Information Processing Systems, 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[8]</b>
                                         Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 25th International Conference on Neural Information Processing Systems, 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="114">


                                    <a id="bibliography_9" title=" Szegedy C, Liu W, Jia Y Q, et al.Going deeper with convolutions[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[9]</b>
                                         Szegedy C, Liu W, Jia Y Q, et al.Going deeper with convolutions[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2015:1-9.
                                    </a>
                                </li>
                                <li id="116">


                                    <a id="bibliography_10" title=" Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[10]</b>
                                         Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                                    </a>
                                </li>
                                <li id="118">


                                    <a id="bibliography_11" title=" Mostajabi M, Yadollahpour P, Shakhnarovich G.Feedforward semantic segmentation with zoom-out features[C]//Proc of 2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:3376-3385." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feedforward semantic segmentation with zoom-out features">
                                        <b>[11]</b>
                                         Mostajabi M, Yadollahpour P, Shakhnarovich G.Feedforward semantic segmentation with zoom-out features[C]//Proc of 2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:3376-3385.
                                    </a>
                                </li>
                                <li id="120">


                                    <a id="bibliography_12" title=" Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation//Proc of the 2015 ICCV, 2015:1520-1528." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Deconvolution Network for Semantic Segmentation">
                                        <b>[12]</b>
                                         Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation//Proc of the 2015 ICCV, 2015:1520-1528.
                                    </a>
                                </li>
                                <li id="122">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Badrinarayanan V, Kendall A, Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (2) :2481-2495.</a>
                                </li>
                                <li id="124">


                                    <a id="bibliography_14" title=" Yu F, Koltun V.Multi-scale context aggregation by dilated convolutions[C]//Proc of the 4th International Conference on Learning Representations, 2016:1-13." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">
                                        <b>[14]</b>
                                         Yu F, Koltun V.Multi-scale context aggregation by dilated convolutions[C]//Proc of the 4th International Conference on Learning Representations, 2016:1-13.
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_15" title=" Fu J, Liu J, Wang Y, et al.Stacked deconvolutional network for semantic segmentation[J].arXiv:170804943, 2017:1-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked deconvolutional network for semantic segmentation">
                                        <b>[15]</b>
                                         Fu J, Liu J, Wang Y, et al.Stacked deconvolutional network for semantic segmentation[J].arXiv:170804943, 2017:1-12.
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_16" title=" Brostow G J, Fauqueur J, Cipolla R.Semantic object classes in video:A high-definition ground truth database[J].Pattern Recognition Letters, 2009, 30 (2) :88-97." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300414175&amp;v=MTk5NjZxUVRNbndaZVp1SHlqbVVMZklKbDRUYUJRPU5pZk9mYks3SHRET3JJOUZZT29MRFhzOG9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Brostow G J, Fauqueur J, Cipolla R.Semantic object classes in video:A high-definition ground truth database[J].Pattern Recognition Letters, 2009, 30 (2) :88-97.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_17" title=" Tighe J, Lazebnik S.Superparsing:Scalable nonparametric image parsing with superpixels[C]//Proc of the 11th European Conference on Computer Vision, 2010:352-365." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Superparsing:scalable nonparametric image parsing with superpixels">
                                        <b>[17]</b>
                                         Tighe J, Lazebnik S.Superparsing:Scalable nonparametric image parsing with superpixels[C]//Proc of the 11th European Conference on Computer Vision, 2010:352-365.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(08),1453-1458 DOI:10.3969/j.issn.1007-130X.2019.08.016            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于混合自动编码器道路语义分割方法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E9%A3%9E&amp;code=40224753&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E5%BB%BA&amp;code=38662925&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐建</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%88%90%E6%9D%BE&amp;code=38740064&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨成松</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8A%AE%E6%8C%BA&amp;code=40224748&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">芮挺</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%99%86%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%94%9F%E5%AD%A6%E9%99%A2&amp;code=1701801&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陆军工程大学研究生学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>道路检测是无人驾驶汽车环境感知的重要环节, 利用计算机视觉技术实现对环境场景的语义分割是确保无人驾驶汽车安全行驶的关键技术之一。提出一种稀疏自动编码器和去噪自动编码器相结合的混合自动编码器语义分割模型, 利用稀疏自动编码器的稀疏性语义编码和去噪自动编码器鲁棒的语义编码, 使混合模型学习的特征更有利于图像的语义分割。通过建立一种合理的模型排列顺序与堆叠形式, 实现对图像语义的优化选择, 从而建立一个具有深度的“富结构”语义分割模型, 进一步提高语义分割性能。实验表明, 本文所提模型更为简单、训练周期短, 具有较好的综合图像分割性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%93%E8%B7%AF%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">道路检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B7%E5%90%88%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">混合自动编码器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%8C%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">富结构;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    周飞 (1991-) , 男, 安徽蚌埠人, 硕士, 研究方向为计算机视觉与深度学习。E-mail:342404730@qq.com通信地址:210007江苏省南京市陆军工程大学研究生学院;
                                </span>
                                <span>
                                    唐建 (1977-) , 女, 江苏南京人, 博士, 副教授, CCF会员 (62439M) , 研究方向为人工智能算法及其在机械故障诊断中的应用。E-mail:lgdx_tj@163.com通信地址:210007江苏省南京市陆军工程大学研究生学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>青年科学基金 (E050302);</span>
                    </p>
            </div>
                    <h1><b>Road semantic segmentation based on hybrid auto-encoder</b></h1>
                    <h2>
                    <span>ZHOU Fei</span>
                    <span>TANG Jian</span>
                    <span>YANG Cheng-song</span>
                    <span>RUI Ting</span>
            </h2>
                    <h2>
                    <span>Department of Graduate, PLA Army Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Road detection is an important part of the environment perception technology of unmanned vehicles. Using computer vision technology to achieve the semantic segmentation of environmental scenes is one of the key technologies to ensure the safe driving of unmanned vehicles. We propose a hybrid auto-encoder semantic segmentation model combining sparse auto-encoder and denoising auto-encoder. Using the sparse semantic encoding of sparse auto-encoder and the robust semantic encoding of denoising auto-encoder makes the features learned by the model more conducive for semantic segmentation. By establishing a reasonable arrangement order and stacking form of the model, an optimal selection of image semantics can be achieved, thereby creating a semantic segmentation model with deep “rich structure”, which can further improve the semantic segmentation performance. Experiments show that this model is simpler with shorter training time and better comprehensive segmentation performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=road%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">road detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hybrid%20auto-encoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hybrid auto-encoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=rich%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">rich structure;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    ZHOU Fei, born in 1991, MS, his research interests include computer vision, and deep learning.Address:Department of Graduate, PLA Army Engineering University, Nanjing 210007, Jiangsu, P.R.China;
                                </span>
                                <span>
                                    TANG Jian, born in 1977, PhD, associate professor, CCF member (62439M) , her research interest includes artificial intelligence algorithm and its application in mechanical fault diagnosis.Address:Department of Graduate, PLA Army Engineering University, Nanjing 210007, Jiangsu, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-24</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="40">道路检测是无人驾驶汽车<citation id="132" type="reference"><link href="98" rel="bibliography" /><link href="100" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>环境感知的重要环节, 利用计算机视觉技术实现对环境场景的语义分割, 识别出场景中每个区域的语义类别, 并确定场景中的道路区域, 这对无人驾驶汽车导航具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="41">无人驾驶车辆环境感知中的道路分割是图像语义分割的具体应用。目前语义分割模型结构复杂, 训练困难, 实时性难以保证, 针对车载设备计算、存储能力有限的特点, 设计一种新语义分割方法是非常必要的。由于深度学习在图像领域的巨大成功, 目前语义分割方法大多基于深度学习框架, 由分类模型转化而来, 输出空间映射而不是分类数, 这些映射由小步幅卷积上采样 (又称反卷积) 得到, 产生密集的像素级别的预测输出, 因此深度学习语义分割模型一般都具有较大的规模和较复杂的结构。自动编码器AE (Auto-Encoder) <citation id="133" type="reference"><link href="102" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>作为深度学习的经典模型之一, 可以通过无监督的自学习方式, 从样本数据中学习编码特征, 并将学习到的特征通过精简的表达重构出数据信息。自动编码器的一个特点是它拥有丰富的结构形式, 例如稀疏自动编码器SAE (Sparse AutoEncoder) <citation id="134" type="reference"><link href="104" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、去噪自动编码器DAE (Denoising AutoEncoder) <citation id="135" type="reference"><link href="106" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等, 这些不同的结构形式使AE模型分别具有稀疏性、鲁棒性等不同的特点。本文提出一种稀疏自动编码器和去噪自动编码器相结合的深度语义分割模型。以道路环境标注图像作为监督信息, 利用稀疏自动编码器和去噪自动编码器提取的图像具有稀疏性和鲁棒性的特征, 使混合模型学习到的特征更有利于图像的语义分割。</p>
                </div>
                <div class="p1">
                    <p id="42">本文的主要工作包括以下几个方面:</p>
                </div>
                <div class="p1">
                    <p id="43"> (1) 针对无人驾驶车辆环境感知图像语义分割的应用特点, 提出了一种新的语义分割方法。通过构造稀疏自动编码器和去噪自动编码器的混合模型丰富语义特征, 使模型获得更有利于道路分割的特征。</p>
                </div>
                <div class="p1">
                    <p id="44"> (2) 提出了混合自动编码器的语义分割模型的堆叠策略与训练方法。在稀疏自动编码器和去噪自动编码器模型中加入了监督层, 将稀疏自动编码器和去噪自动编码器以混合堆叠的形式构成深度自动编码器, 构造了新的深度模型结构。</p>
                </div>
                <div class="p1">
                    <p id="45"> (3) 对提出的道路图像语义分割方法进行了系统的测试。通过对不同自动编码器组合方案的定量分析和对比, 得出一种更利于道路分割的模型排列顺序与堆叠形式。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag"><b>2 相关工作</b></h3>
                <div class="p1">
                    <p id="47">目前道路图像的语义分割方法主要是构建端到端的深层网络模型, 如通过将分类网络<citation id="142" type="reference"><link href="108" rel="bibliography" /><link href="110" rel="bibliography" /><link href="112" rel="bibliography" /><link href="114" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>转化为适合分割的形式。2015年, Shelhamer等人<citation id="136" type="reference"><link href="116" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了全卷积网络FCN (Fully Convolutional Netowrk) 方法, 设计了一种针对任意大小的输入图像, 训练端到端的全卷积网络的框架, 实现逐像素分类, 奠定了使用深度网络解决图像语义分割问题的基础框架。由于FCN得到的结果还不够精细, 目标物体的细节结构可能会丢失或边界模糊, 而且只能检测单一尺度语义, Mostajabi等人<citation id="137" type="reference"><link href="118" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出 Zoom-out 方法, 克服了单一尺度感受野无法检测不同尺度目标的不足, 通过融合图像多个级别特征提升分割效果;Noh等人<citation id="138" type="reference"><link href="120" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在 FCN 网络架构的基础上, 提出学习一个多层反卷积网络 (Deconvolution Network) 代替简单的双线性插值;在此基础上, Badrinarayana等人<citation id="139" type="reference"><link href="122" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了SegNet, 一种用于图像语义分割的深度卷积编码器-解码器架构, SegNet的网络结构类似于反卷积网络, 但去除了网络中间的2个全连接层, 并且在网络中采用了Batch Normalization方法和Softmax分类器。SegNet的效率更高并且占用更少的内存, 但划分精度不高。不论是FCN还是SegNet, 网络中池化层的存在, 虽可在降低图像尺寸的同时增大感受野, 但同时也会丢失部分位置信息。Yu等人<citation id="140" type="reference"><link href="124" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>设计了一种专用于图像像素预测的网络, 该网络不包含池化层, 并且卷积层采用扩张卷积 (Dilated Convolutions) 。扩张卷积可以指数级地扩大感受野, 从而捕获图像中多尺度上下文信息并加以聚合, 以提升像素预测的准确率。最近, Fu等人<citation id="141" type="reference"><link href="126" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了一种用于语义分割的堆叠反卷积网络SDN (Stacked Deconvolutional Network) , 通过将多个SDN堆叠加深网络, 同时采用密集的连接和分层监督来优化网络。</p>
                </div>
                <div class="p1">
                    <p id="48">目前深度学习框架下的语义分割模型在网络结构和训练策略上虽有所差别, 但基本思想都是通过具有一定深度的卷积神经网络实现对图像语义的编码, 然后通过反卷积 (Deconvolution) 过程完成语义分割。本文将稀疏自动编码器和去噪自动编码器相结合提出有监督的深度自动编码器混合模型, 并将其用于道路环境语义分割, 实验证明了该方法的简洁性和有效性。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>3 监督学习的混合自动编码器</b></h3>
                <div class="p1">
                    <p id="50">经典AE的深度结构是通过无监督贪婪算法逐层训练, 以堆叠方式实现的。在每一层训练过程中都产生一种特征表达, 其堆叠层越深特征越抽象, 从而实现了对原始样本的特征表示。对于混合自编码模型, SAE和DAE的多层堆叠组合, 也是为了获得更加抽象、更加多元化的特征表达。</p>
                </div>
                <div class="p1">
                    <p id="51">本文设计的基于监督学习的混合自动编码器模型的训练分为3组, 如图1所示。第1组和第2组训练采用的是无监督学习浅层SAE模型, 第3组训练采用的是有监督学习浅层DAE模型。每一组训练模型都是独立的, 但整个训练过程是相互关联的。在训练过程中, <b><i>H</i></b><sub><i>i</i></sub>表示模型隐含层的结果, 称为特征编码, <b><i>W</i></b><sup><i>i</i></sup><sub>e</sub>为从输入层到隐含层的编码权值, <b><i>W</i></b><sup><i>i</i></sup><sub>d</sub>为从隐含层到输出层的解码权值, 其中<i>i</i>表示第<i>i</i>组训练。</p>
                </div>
                <div class="p1">
                    <p id="52">第1组和第2组采用浅层无监督学习SAE模型进行训练, 分别以原始道路环境图像<b><i>X</i></b>作为第1组训练输入<b><i>X</i></b><sub>1</sub>, 道路分割标签图<b><i>X</i></b><sub>label</sub>作为第2组训练输入<b><i>X</i></b><sub>2</sub>。在训练过程中, 通过浅层无监督学习的SAE模型提取原始道路环境图像和道路分割标签图的特征。利用SAE从无标注数据中学习到的特征比从原始数据中学到的特征有更好的特征描述能力, 将第1组和第2组SAE模型提取的隐含层输出用于第3组模型的训练。网络结构如图1中第1组和第2组所示。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908017_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 监督学习混合自动编码模型逐组训练分解" src="Detail/GetImg?filename=images/JSJK201908017_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 监督学习混合自动编码模型逐组训练分解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908017_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Group-wise training of the hybrid supervisecd learning auto-encoder model</p>

                </div>
                <div class="p1">
                    <p id="54">第3组监督学习使用浅层监督学习DAE模型, 以第1组模型中学习的特征编码<b><i>H</i></b><sub>1</sub>作为输入<b><i>X</i></b><sub>3</sub>, 以第2组模型中学习的特征编码<b><i>H</i></b><sub>2</sub>作为标签, 进行监督学习, 得到重构数据<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml><sub>3</sub>, 网络结构如图1中第3组训练过程所示。3组模型虽然单独训练, 但各模型相互关联, 权值<b><i>W</i></b><sup>1</sup><sub>e</sub>和权值<i>W</i><sup>3</sup><sub>e</sub>为混合自动编码的编码权值, 以第3组模型的解码权值和第2组的解码权值为混合自动编码的解码权值<b><i>W</i></b><sup>2</sup><sub>d</sub>, 最终得到道路的分割图像<mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="57">整个模型分为编码层和解码层, 先通过前3组逐层训练, 然后通过栈堆叠的方式组合在一起, 构成混合自动编码模型。通过第1组和第2组的无监督SAE, 可以自动从道路环境图像和道路环境分割图像中学习特征, 给出比原始数据更好的特征描述。第3组DAE则是利用SAE中学习得到的特征进行有监督的训练。DAE模型本身可以擦除掉受损的数据中的噪声, 从而可以缩小训练数据和测试数据之间的鸿沟。通过有监督的训练, DAE模型可以提高语义分割图像的鲁棒性, 提高测试集的分割精度。监督学习的混合自动编码的编码权值对输入数据进行深层特征编码, 再根据获取的解码权值反向解码, 重构出特征编码所描述的更有利于道路环境分割的数据, 实现对道路环境图像的语义分割。对于训练好的模型, 以前馈方式完成对输入样本的重构, 其前馈重构过程如下所示:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mtext>e</mtext><mn>1</mn></msubsup><mo>*</mo><mi mathvariant="bold-italic">X</mi><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mtext>e</mtext><mn>1</mn></msubsup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mtext>e</mtext><mn>3</mn></msubsup><mo>*</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mtext>e</mtext><mn>3</mn></msubsup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mtext>d</mtext><mn>3</mn></msubsup><mo>*</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mtext>d</mtext><mn>3</mn></msubsup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mtext>d</mtext><mn>2</mn></msubsup><mo>*</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mn>3</mn></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mtext>d</mtext><mn>2</mn></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中, <i>f</i> () 表示sigmod激活函数, <b><i>b</i></b>为偏置。</p>
                </div>
                <div class="p1">
                    <p id="60">通过有监督的混合自动编码模型进行抽象特征编码, 即通过SAE提取了原始道路图像更好的特征描述, 又利用有监督的DAE在SAE提取特征之后进行像素点的语义分类, DAE对于不同的光照等噪声干扰, 鲁棒性更强, 并且逐层训练方式比直接构建深层大型网络更易于训练。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>4 实验与分析</b></h3>
                <h4 class="anchor-tag" id="62" name="62"><b>4.1 实验数据</b></h4>
                <div class="p1">
                    <p id="63">为验证本文提出的混合自动编码模型对道路语义分割的性能, 本文选择<i>Cambridge</i>-<i>driving Labeled Video Database</i> (<i>CamVid</i>) 数据集<citation id="143" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>对算法进行验证。<i>CamVid</i>数据集是一个道路、驾驶场景理解数据集, 数据集由一个安装在汽车仪表盘上的960*720分辨率的摄相机采集得到。本文对图像进行降采样, 将图像大小由原来的960*720降采样为80*60。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908017_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 2种方案分割效果图对比" src="Detail/GetImg?filename=images/JSJK201908017_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 2种方案分割效果图对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908017_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 2 <i>Comparison of segmentation results of the two schemes</i></p>

                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>4.2 实验方法及结果分析</b></h4>
                <h4 class="anchor-tag" id="66" name="66">4.2.1 混合自动编码模型的实验方案分析</h4>
                <div class="p1">
                    <p id="67">在实验的过程中, 为找出最佳的组合方案, 找出最优的分割模型组合, 设计了2种实验方案。第1种方案将第1组和第2组训练采用无监督的<i>DAE</i>模型进行特征提取, 第3组训练为采用有监督的<i>SAE</i>, 然后抽取各个模型相应的编码和解码权值组成混合自动编码模型;第2种方案是将第1组训练和第2组训练采用无监督的<i>SAE</i>, 第3组训练采用有监督的<i>DAE</i>, 然后利用各模型相应的编码权值和解码权值组成最后的混合自动编码模型。为初步验证模型的分割性能, 先将训练样本和训练样本分割标签图转化成灰度图进行初步实验。按照3.3节所述的混合自动编码模型的训练过程, 在相同的实验条件下, 2种方案实验结果对比如图2所示。</p>
                </div>
                <div class="p1">
                    <p id="68">评价指标<i>TPR</i>=<i>TP</i>/ (<i>TP</i>+<i>FN</i>) , <i>FPR</i>=<i>FP</i>/ (<i>FP</i>+<i>TN</i>) 可以定量评价2种方案的道路图像语义分割性能。TP表示样本是道路且识别为道路的像素点个数, FN表示样本是道路但识别为非道路的像素点个数, FP表示样本是非道路但识别为道路的像素点个数, TN表示样本是非道路且被识别为非道路的像素点个数。从表1可以看出, 方案2相比方案1, 指标<i>TPR</i>高出1.8%, <i>FPR</i>低了1.5%, 因此方案2的堆叠排序更利于道路分割。</p>
                </div>
                <div class="area_img" id="69">
                    <p class="img_tit"><b>表1 2种方案图像分割性能指标比对</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison two image segmentation performance indicators of the two schemes</b></p>
                    <p class="img_note">%</p>
                    <table id="69" border="1"><tr><td><br />方案</td><td><i>TPR</i></td><td><i>FPR</i></td></tr><tr><td><br />方案1</td><td>93.4</td><td>7.8</td></tr><tr><td><br />方案2</td><td>95.2</td><td>6.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="70">人眼视觉系统在初级处理图片时, 其机理类似稀疏编码, 即单个神经元仅对某一信息呈现较强的反应。方案2利用稀疏自动编码器提取道路样本和标签特征时, 隐含层中大部分神经元处于抑制状态, 通过具有稀疏性的分量来表示输入数据, 所以对边缘、条纹等图像特征提取优于去噪自动编码器的。去噪自动编码器提取特征时有更好的鲁棒性, 故噪声点更少, 精度较高。道路图像的语义分割对于道路边界划分要求较高, 以确保无人驾驶汽车安全行驶区域。基于上述<i>TPR</i>和<i>FPR</i>指标的定量分析, 本文选取方案2作为道路语义分割的混合自动编码模型。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71">4.2.2 实验结果及分析</h4>
                <div class="p1">
                    <p id="72">通过4.2.1节中2种方案的定量分析之后, 可以确定方案2要优于方案1。为了进一步验证方案的可行性, 本文对原始的道路图像降采样, 即图像由960*720降采样为80*60之后, 将样本和相对应的语义标签分3个通道各自动训练混合自动编码模型。最后, 测试集也分相应的通道分别输入训练好的模型进行编码和解码, 得到的3个通道的结果再组合成分割图, 实验结果如图3所示。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908017_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 测试样本分割效果" src="Detail/GetImg?filename=images/JSJK201908017_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 测试样本分割效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908017_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Comparison of test sample segmentation effect</p>

                </div>
                <div class="p1">
                    <p id="74">由图3f～图3o, 对比混合自动编码模型重构的图像与测试样本标签图, 可以发现混合自动编码模型对道路的分割较好。说明该模型可以获取更有利于图像分割的特征, 它可以很完整地保留图像中各区域的边界, 而忽略与分割无关的图像细节。为了定量评价本文所提出的道路图像语义分割模型, 通过道路像素精度<i>PA</i> (Pixel Accuracy) 、平均交并比<i>MIoU</i> (Mean region Intersection over Union) 2个指标与其他模型进行比较, 来衡量分割图像的质量。<i>PA</i>为正确分类的像素数量与所有像素数量的比值。<i>MIoU</i>为语义分割的标准度量, 为2个集合的交集和并集之比, 在语义分割问题中, 这2个集合为真实值 (Ground Truth) 和预测值 (Predicted Segmentation) , 在每个类上计算<i>IoU</i>之后取平均值。具体计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>A</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Μ</mi><mi>Ι</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">其中, 数据集中有<i>k</i>+1个类 (其中包含一个空类或背景) , <i>p</i><sub><i>ij</i></sub>表示属于类<i>i</i>但被预测为类<i>j</i>的像素数量。与其它模型在CamVid数据集上测试结果比对, 结果如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="77">由表2可见, 本文将所提出的混合自动编码器模型相比于其他语义分割模型, <i>MIoU</i>大约高出15%, 其中对于道路的识别率提高了, 充分说明了混合自动编码器方法优异的图像分割能力。传统端到端的语义分割模型对于图像特征的提取采用的是卷积和池化方式, 在提取特征的同时, 也在一定程度上丢失了空间位置信息。本文采用SAE和DAE组合的混合自动编码器, 不仅提取的特征形式更多样, 即特征具有稀疏性和鲁棒性, 而且避免了池化层降采样时丢失空间位置信息, 所以对于道路边界分割更精细。</p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表2 各种模型的分割性能比对</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Comparison of segmentation performance among the models</b></p>
                    <p class="img_note">%</p>
                    <table id="78" border="1"><tr><td><br />模型</td><td>道路像素准确率</td><td><i>MIoU</i></td></tr><tr><td><br />SuperParsing<sup>[17]</sup></td><td>83.4</td><td>42.00</td></tr><tr><td><br />Segnet<sup>[13]</sup></td><td>86.2</td><td>61.13</td></tr><tr><td><br />Dilation<sup>[14]</sup></td><td>92.2</td><td>65.30</td></tr><tr><td><br />混合自动编码器模型</td><td>93.1</td><td>80.40</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="81">基于机器视觉的道路环境感知是无人驾驶汽车的关键技术之一, 图像语义分割是机算机视觉中图像理解的重要一环。本文将SAE和DAE进行有效组合堆叠构成深度有监督的语义分割混合自动编码器模型。在CamVid数据集上验证了模型的有效性和简洁性, 实验的定量分析说明了混合自动编码器对道路图像的分割精度较高。自动编码器的形式多样, 未来可以考虑使用卷积自动编码器对整幅图像进行特征提取, 以获得更多有利于分割的空间特征信息。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="96" type="formula" href="images/JSJK201908017_09600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">周飞</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="97" type="formula" href="images/JSJK201908017_09700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">唐建</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="98">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Are we ready for autonomous driving?The KITTI vision benchmark suite">

                                <b>[1]</b> Geiger A.Are we ready for autonomous driving?The KITTI vision benchmark suite[C]//Proc of 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012:3354-3361.
                            </a>
                        </p>
                        <p id="100">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentation-based urban traffic scene understanding">

                                <b>[2]</b> Ess A, Mueller T, Grabner H, et al.Segmentation-based urban traffic scene understanding[C]//Proc of the 20th British Machine Vision Conference, 2009:1-11.
                            </a>
                        </p>
                        <p id="102">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Autoencoders,Minimum Description Length and Helmholtz Free Energy">

                                <b>[3]</b> Hinton G E, Zemel R S.Autoencoders, minimum description length and Helmholtz free energy[C]//Proc of the 6th International Conference on Neural Information Processing Systems, 1993:3-10.
                            </a>
                        </p>
                        <p id="104">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Greedy layer-wise training of deep networks">

                                <b>[4]</b> Bengio Y, Lamblin P, Popovici D, et al.Greedy layer-wise training of deep networks[C]//Proc of the 19th International Conference on Neural Information Processing Systems, 2006:153-160.
                            </a>
                        </p>
                        <p id="106">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting and composing robust features with denoising autoencoders">

                                <b>[5]</b> Vincent P, Larochelle H, Bengio Y, et al.Extracting and composing robust features with denoising autoencoders[C]//Proc of the 25th International Conference on Machine Learning, 2008:1096-1103.
                            </a>
                        </p>
                        <p id="108">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[6]</b> Simonyan K, Zisserman A.Very deep convolutional networks for large-scale image recognition[J].arXiv:1409.1556, 2014:1.
                            </a>
                        </p>
                        <p id="110">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 He K M, Zhang X Y, Ren S Q, et al.Deep residual learning for image recognition[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016:770-778.
                            </a>
                        </p>
                        <p id="112">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[8]</b> Krizhevsky A, Sutskever I, Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of the 25th International Conference on Neural Information Processing Systems, 2012:1097-1105.
                            </a>
                        </p>
                        <p id="114">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[9]</b> Szegedy C, Liu W, Jia Y Q, et al.Going deeper with convolutions[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2015:1-9.
                            </a>
                        </p>
                        <p id="116">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[10]</b> Shelhamer E, Long J, Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (4) :640-651.
                            </a>
                        </p>
                        <p id="118">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feedforward semantic segmentation with zoom-out features">

                                <b>[11]</b> Mostajabi M, Yadollahpour P, Shakhnarovich G.Feedforward semantic segmentation with zoom-out features[C]//Proc of 2015 IEEE Conference on Computer Vision and Pattern Recognition, 2015:3376-3385.
                            </a>
                        </p>
                        <p id="120">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Deconvolution Network for Semantic Segmentation">

                                <b>[12]</b> Noh H, Hong S, Han B.Learning deconvolution network for semantic segmentation//Proc of the 2015 ICCV, 2015:1520-1528.
                            </a>
                        </p>
                        <p id="122">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Badrinarayanan V, Kendall A, Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (2) :2481-2495.
                            </a>
                        </p>
                        <p id="124">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-scale context aggregation by dilated convolutions">

                                <b>[14]</b> Yu F, Koltun V.Multi-scale context aggregation by dilated convolutions[C]//Proc of the 4th International Conference on Learning Representations, 2016:1-13.
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked deconvolutional network for semantic segmentation">

                                <b>[15]</b> Fu J, Liu J, Wang Y, et al.Stacked deconvolutional network for semantic segmentation[J].arXiv:170804943, 2017:1-12.
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300414175&amp;v=MTM2MTQ5RllPb0xEWHM4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw0VGFCUT1OaWZPZmJLN0h0RE9ySQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Brostow G J, Fauqueur J, Cipolla R.Semantic object classes in video:A high-definition ground truth database[J].Pattern Recognition Letters, 2009, 30 (2) :88-97.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Superparsing:scalable nonparametric image parsing with superpixels">

                                <b>[17]</b> Tighe J, Lazebnik S.Superparsing:Scalable nonparametric image parsing with superpixels[C]//Proc of the 11th European Conference on Computer Vision, 2010:352-365.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201908017" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908017&amp;v=MTYzMjZHNEg5ak1wNDlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bFZMM09MejdCWmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
