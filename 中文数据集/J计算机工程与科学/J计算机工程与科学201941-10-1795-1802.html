<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132348529561250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201910012%26RESULT%3d1%26SIGN%3dN2QcmfadCbEwGArd10F3UszHL1Y%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910012&amp;v=Mjk2NzR6cXFCdEdGckNVUkxPZVplUm1GeS9nVmIzS0x6N0JaYkc0SDlqTnI0OUVab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="&lt;b&gt;2 时空上下文目标跟踪&lt;/b&gt; "><b>2 时空上下文目标跟踪</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="&lt;b&gt;3 STC-SURF算法&lt;/b&gt; "><b>3 STC-SURF算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="&lt;b&gt;3.1 SURF特征匹配与RANSAC误匹配消除&lt;/b&gt;"><b>3.1 SURF特征匹配与RANSAC误匹配消除</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;3.2 尺度自适应调整&lt;/b&gt;"><b>3.2 尺度自适应调整</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;3.3 置信图计算方式更新&lt;/b&gt;"><b>3.3 置信图计算方式更新</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;3.4 STC-SURF算法流程&lt;/b&gt;"><b>3.4 STC-SURF算法流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="&lt;b&gt;4 实验结果及分析&lt;/b&gt; "><b>4 实验结果及分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#144" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="图1 &lt;i&gt;SURF&lt;/i&gt;算法计算特征点描述子">图1 <i>SURF</i>算法计算特征点描述子</a></li>
                                                <li><a href="#102" data-title="图2 &lt;i&gt;SURF&lt;/i&gt;特征匹配与误匹配消除">图2 <i>SURF</i>特征匹配与误匹配消除</a></li>
                                                <li><a href="#123" data-title="图3 &lt;i&gt;STC&lt;/i&gt;-&lt;i&gt;SURF&lt;/i&gt;算法流程图">图3 <i>STC</i>-<i>SURF</i>算法流程图</a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表1 测试数据集视频主要信息&lt;/b&gt;"><b>表1 测试数据集视频主要信息</b></a></li>
                                                <li><a href="#133" data-title="图4 david视频中不同算法跟踪结果">图4 david视频中不同算法跟踪结果</a></li>
                                                <li><a href="#135" data-title="图5 boy视频中不同算法跟踪结果">图5 boy视频中不同算法跟踪结果</a></li>
                                                <li><a href="#137" data-title="图6 girle2视频中不同算法跟踪结果">图6 girle2视频中不同算法跟踪结果</a></li>
                                                <li><a href="#139" data-title="图7 dog视频中不同算法跟踪结果">图7 dog视频中不同算法跟踪结果</a></li>
                                                <li><a href="#141" data-title="图8 FaceOcc1视频中不同算法跟踪结果">图8 FaceOcc1视频中不同算法跟踪结果</a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;表2 不同算法跟踪平均成功率&lt;/b&gt;"><b>表2 不同算法跟踪平均成功率</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Babenko B,Yang M H,Belongie S.Visual tracking with online multiple instance learning[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2010,33(8):1619-1632." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Object Tracking with Online Multiple Instance Learning">
                                        <b>[1]</b>
                                         Babenko B,Yang M H,Belongie S.Visual tracking with online multiple instance learning[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2010,33(8):1619-1632.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Wang Li-jia,Jia Song-min,Li Xiu-zhi,et al.Person following for mobile robot using improved multiple instance learning [J].Acta Automatica Sinica,2014,40(12):2916-2925.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201412024&amp;v=MDc4MjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYjNLS0NMZlliRzRIOVhOclk5SFlJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Wang Li-jia,Jia Song-min,Li Xiu-zhi,et al.Person following for mobile robot using improved multiple instance learning [J].Acta Automatica Sinica,2014,40(12):2916-2925.(in Chinese)
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Ross D A,Lim J,Lin R-S,et al.Incremental learning for robust visual tracking[J].International Journal of Computer Vision,2008,77(1-3):125-141." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831201&amp;v=MTQ1MDhSN3FlYnVkdEZDRGxWTHJPSWx3PU5qN0Jhck80SHRIT3A0eEVadXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Ross D A,Lim J,Lin R-S,et al.Incremental learning for robust visual tracking[J].International Journal of Computer Vision,2008,77(1-3):125-141.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Zhang K H,Zhang L,Yang M H.Fast compressive tracking[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,36(10):2002-2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast compressive tracking">
                                        <b>[4]</b>
                                         Zhang K H,Zhang L,Yang M H.Fast compressive tracking[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,36(10):2002-2015.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Kwon J,Lee K M.Visual tracking decomposition[C]//Proc of the 23rd IEEE Conference on Computer Vision and Pattern Recognition,2010:1269-1276." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual tracking decomposition">
                                        <b>[5]</b>
                                         Kwon J,Lee K M.Visual tracking decomposition[C]//Proc of the 23rd IEEE Conference on Computer Vision and Pattern Recognition,2010:1269-1276.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Zhou X Z,Lu Y,Lu J W,et al.Abrupt motion tracking via intensively adaptive Markov-Chain Monte Carlo sampling[J].IEEE Transactions on Image Processing,2012,21(2):789-801." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Abrupt Motion Tracking Via Intensively Adaptive Markov-Chain Monte Carlo Sampling">
                                        <b>[6]</b>
                                         Zhou X Z,Lu Y,Lu J W,et al.Abrupt motion tracking via intensively adaptive Markov-Chain Monte Carlo sampling[J].IEEE Transactions on Image Processing,2012,21(2):789-801.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Zhou T F,Lu Y,Di H J.Nearest neighbor field driven stochastic sampling for abrupt motion tracking[C]//Proc of IEEE International Conference on Multimedia and Expo,2014:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nearest neighbor field driven stochastic sampling for abrupt motion tracking">
                                        <b>[7]</b>
                                         Zhou T F,Lu Y,Di H J.Nearest neighbor field driven stochastic sampling for abrupt motion tracking[C]//Proc of IEEE International Conference on Multimedia and Expo,2014:1-6.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Kalal Z,Mikolajczyk K,Matas J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2012,34(7):1409-1422." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">
                                        <b>[8]</b>
                                         Kalal Z,Mikolajczyk K,Matas J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2012,34(7):1409-1422.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Bay H,Tuytelaars T,Gool L V.SURF:Speeded up robust features[C]//Proc of the 9th European Conference on Computer Vision,2006:404-417." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Surf:Speeded up robust features">
                                        <b>[9]</b>
                                         Bay H,Tuytelaars T,Gool L V.SURF:Speeded up robust features[C]//Proc of the 9th European Conference on Computer Vision,2006:404-417.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Ge Bao-yi,Zuo Xian-zhang,Hu Yong-jiang.Long-term target tracking based on feature fusion[J].Acta Optica Sinica,2018,38(11):211-223.(in Chinese)</a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Chen Zhi,Liu Pei-zhong,Luo Yan-min,et al.Multi-scale correlation filter tracking algorithm based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2018,30(11):2063-2073.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201811010&amp;v=MTM1OTVxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYjNLTHo3QmFMRzRIOW5Ocm85RVpJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Chen Zhi,Liu Pei-zhong,Luo Yan-min,et al.Multi-scale correlation filter tracking algorithm based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp;amp; Computer Graphics,2018,30(11):2063-2073.(in Chinese)
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Zhang Kai-hua,Zhang Lei,Liu Qing-shan,et al.Fast visual tracking via dense spatio-temporal context learning[C]//Proc of the 13th European Conference on Computer Vision,2014:127-141." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast visual tracking via dense spatio-temporal context learning">
                                        <b>[12]</b>
                                         Zhang Kai-hua,Zhang Lei,Liu Qing-shan,et al.Fast visual tracking via dense spatio-temporal context learning[C]//Proc of the 13th European Conference on Computer Vision,2014:127-141.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Zhang Xin-kun,Huang Shan.STC target tracking algorithm based on Kalman filtering[J].Electronics Opticals and Control,2018,25(11):102-105.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGKQ201811021&amp;v=MjE3MDdlWmVSbUZ5L2dWYjNLSVNyQWY3RzRIOW5Ocm85SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Zhang Xin-kun,Huang Shan.STC target tracking algorithm based on Kalman filtering[J].Electronics Opticals and Control,2018,25(11):102-105.(in Chinese)
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Zhang Jing,Wang Xu,Fan Hong-bo.Spatio temporal context target tracking algorithm of self-adaptive learning[J].Computer Engineering,2018,44(6):294-299.(in Chinese)</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     王丽佳,贾松敏,李秀智,等.基于改进在线多示例学习算法的机器人目标跟踪[J].自动化学报,2014,40(12):2916-2925.</a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_10" title=" 葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[J].光学学报,2018,38(11):211-223." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MDU5MzgzS0lqWFRiTEc0SDluTnJvOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[J].光学学报,2018,38(11):211-223.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     陈智,柳培忠,骆炎民,等.自适应特征融合的多尺度相关滤波目标跟踪算法[J].计算机辅助设计与图形学学报,2018,30(11):2063-2073.</a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     张新堃,黄山.结合Kalman滤波的时空上下文目标跟踪算法[J].电光与控制,2018,25(11):102-105.</a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_14" title=" 张晶,王旭,范洪博.自适应学习的时空上下文目标跟踪算法[J].计算机工程,2018,44(6):294-299." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201806050&amp;v=MDI2MjllUm1GeS9nVmIzS0x6N0JiYkc0SDluTXFZOUFaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         张晶,王旭,范洪博.自适应学习的时空上下文目标跟踪算法[J].计算机工程,2018,44(6):294-299.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(10),1795-1802 DOI:10.3969/j.issn.1007-130X.2019.10.011            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>改进STC和SURF特征联合优化的目标跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E4%BA%91%E6%98%8E&amp;code=39443838&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄云明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%99%B6&amp;code=08529593&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张晶</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%96%BB%E5%B0%8F%E6%83%A0&amp;code=39443834&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">喻小惠</a>
                                <a href="javascript:;">陶涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BE%9A%E5%8A%9B%E6%B3%A2&amp;code=15085473&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">龚力波</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0242668&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学信息工程与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BA%91%E5%8D%97%E6%9E%AD%E6%B6%A6%E7%A7%91%E6%8A%80%E6%9C%8D%E5%8A%A1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0105115&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">云南枭润科技服务有限公司</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BA%91%E5%8D%97%E7%9C%81%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">云南省信息技术发展中心</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BA%91%E5%8D%97%E7%9C%81%E5%86%9C%E6%9D%91%E7%A7%91%E6%8A%80%E6%9C%8D%E5%8A%A1%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">云南省农村科技服务中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统时空上下文目标跟踪(STC)算法中目标窗口不能适应目标尺度变化,导致对目标针对性不强等问题,提出改进STC和SURF特征联合优化的目标跟踪算法(STC-SURF)。首先利用加速稳健(SURF)特征算法对相邻的2帧图像提取特征点并进行匹配,再通过随机抽样一致(RANSAC)算法消除误匹配,提高匹配精度。进而根据2帧图像中匹配特征点的变化对目标窗口进行调整。最终对STC算法中模型的更新方式进行优化以提高跟踪结果的准确性。实验结果表明,STC-SURF算法能够适应目标尺度变化,并且其目标跟踪成功率优于TLD算法和传统STC算法的。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%BA%E5%BA%A6%E5%8F%98%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尺度变化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SURF%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SURF特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E7%A9%BA%E4%B8%8A%E4%B8%8B%E6%96%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时空上下文;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    黄云明（1995-），男，广东南雄人，硕士生，研究方向为计算机图形学和计算机仿真。E-mail:409562143@qq.com，通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院&lt;image id="175" type="formula" href="images/JSJK201910012_17500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    *张晶（1974-），男，云南昆明人，博士，教授，博士生导师，CCF会员（27448M），研究方向为信息物理融合系统。E-mail:1735335400@qq.com，通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院&lt;image id="177" type="formula" href="images/JSJK201910012_17700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    喻小惠（1994-），女，四川资中人，硕士生，研究方向为信息物理融合系统。E-mail:1078233872@qq.com，通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院&lt;image id="179" type="formula" href="images/JSJK201910012_17900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    陶涛（1982-），男，云南个旧人，硕士，高级工程师，研究方向为工业互联网。E-mail:150019273@qq.com&lt;image id="181" type="formula" href="images/JSJK201910012_18100.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    龚力波（1965-），男，云南楚雄人，高级工程师，研究方向为物联网。E-mail:1970980023@qq.com&lt;image id="183" type="formula" href="images/JSJK201910012_18300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61562051);</span>
                                <span>云南省技术创新人才项目(2019HB113);</span>
                    </p>
            </div>
                    <h1><b>A target tracking algorithm based on joint optimization of improved STC and SURF features</b></h1>
                    <h2>
                    <span>HUANG Yun-ming</span>
                    <span>ZHANG Jing</span>
                    <span>YU Xiao-hui</span>
                    <span>TAO Tao</span>
                    <span>GONG Li-bo</span>
            </h2>
                    <h2>
                    <span>Faculty of Information Engineering and Automation,Kunming University of Science and Technology</span>
                    <span>Yunnan Xiaorun Technology Service Co. Ltd.</span>
                    <span>Yunnan Information Technology Development Center</span>
                    <span>Yunnan Rural Science and Technology Service Center</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem that the target window cannot adapt to target scale change in the traditional spatio-temporal context tracking(STC) algorithm, which leads to inaccurate targeting, we propose a target tracking algorithm based on joint optimization of improved STC and SURF features(STC-SURF). Firstly, the feature points of two adjacent frames are extracted and matched by the speeded up robust feature(SURF) algorithm, and the random sample consensus(RANSAC) matching algorithm is used to eliminate the mismatch and increase the matching precision. Furthermore, the target window is adjusted according to the change of the matching feature points in the two frames of the image, and then outputted. Finally, the update method of the model of the STC algorithm is optimized to increase the accuracy of the tracking result. Experimental results show that the STC-SURF algorithm can adapt to the target scale change, and the target tracking success rate is better than the target-learning detection(TLD) algorithm and the traditional STC algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adaptive&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adaptive;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scale%20change&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scale change;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SURF%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SURF feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spatio-temporal%20context&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spatio-temporal context;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    HUANG Yun-ming,born in 1995,MS candidate,his research interests include computer graphics,and computer simulation.Address:Faculty of Information Engineering and Automation,Kunming University of Science and Technology,Kunming 650500, Yunnan,P.R.China;
                                </span>
                                <span>
                                    ZHANG Jing,born in 1974,PhD,professor,PhD supervisor,CCF member(27448M),his research interest includes cyber-physical systems.Address:Faculty of Information Engineering and Automation,Kunming University of Science and Technology,Kunming 650500, Yunnan,P.R.China;
                                </span>
                                <span>
                                    YU Xiao-hui,born in 1994,MS candidate,her research interest includes cyber-physical systems.Address:Faculty of Information Engineering and Automation,Kunming University of Science and Technology,Kunming 650500, Yunnan,P.R.China;
                                </span>
                                <span>
                                    TAO Tao,born in 1982,MS,senior engineer,his research interest includes industrial internet.;
                                </span>
                                <span>
                                    GONG Li-bo,born in 1965,senior engineer,his research interest includes Internet of Things.;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="42">目标跟踪是计算机视觉研究中一个重要的研究点,并且在信息物理融合系统(Cyber-Physical Systems)中也有着非常广泛的应用。近年来,目标跟踪技术得到了飞速的发展,但是在许多的场景下目标跟踪效果并不太好,例如在目标旋转、尺度缩放、光照变化、视角变换、目标被遮挡、模糊等场景下,大部分的目标跟踪算法效果都不太好。</p>
                </div>
                <div class="p1">
                    <p id="43">学者们就该问题提出了许多算法,如多实例学习MIL(visual tracking with online Multiple-Instance Learning)算法<citation id="161" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、改进多实例学习IMIL(robot target tracking based on Improved online Multi-Instance Learning)算法<citation id="162" type="reference"><link href="5" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">2</a>]</sup></citation>、增强学习IVT(Incremental learning for robust Visual Tracking)算法<citation id="163" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、快速压缩FCT(Fast Compressive Tracking)算法<citation id="164" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、视觉跟踪分解VTD(Visual Tracking Decomposition)算法<citation id="165" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>及集中自适应采样跟踪IA-MCMC(abrupt motion tracking via Intensively Adaptive Markov-Chain Monte Carlo sampling)算法<citation id="166" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、最近邻驱动随机采样NNF-SSAMC(Nearest Neighbor Field driven Stochastic Sampling for Abrupt Motion Tracking)算法<citation id="167" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等。为了提高目标跟踪应对复杂环境的鲁棒性,Kalal等人<citation id="168" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了TLD(Tracking-Learning-Detection)算法,该算法将传统跟踪算法的跟踪与检测相结合并且加入了学习过程,对解决目标形变以及部分被遮挡有一定的效果。TLD算法跟踪模块采用光流法,所以当目标所在场景发生光照变化时该算法容易失去对目标的跟踪,导致跟踪成功率不高。Bay等人<citation id="169" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出加速稳健性特征SURF(Speeded Up Robust Features),通过Hessian矩阵选取特征点,并计算每个特征点的Haar小波响应值,进而获得特征点的特征描述子。SURF特征不仅对目标旋转、尺度缩放、光照变化、视角变换、目标被遮挡、模糊等场景有很好的鲁棒性,而且计算非常快速。文献<citation id="171" type="reference">[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">10</a>,<a class="sup">11</a>]</citation>提出融合多特征的目标跟踪算法,融合了目标方向梯度直方图特征、自适应颜色属性特征CN(Adaptive Color Attributes for Real-Time Visual Tracking)等,但在复杂场景下跟踪成功率不高。Zhang等人<citation id="170" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出了STC(Spatio-Temporal Context learning)算法。该算法根据贝叶斯框架,将目标与其上下文结合起来,在对目标进行检测时计算其空间上下文以及上下文先验概率模型,得出置信图,根据置信图的值找到似然概率最大特征点作为目标点。该算法对复杂场景具有一定的鲁棒性,并且跟踪速度也很快。</p>
                </div>
                <div class="p1">
                    <p id="44">STC算法通过计算置信图获取目标位置,而计算置信图需要先计算上下文先验概率。由传统STC算法上下文先验概率的计算方法可知,上下文中像素点和目标点的距离越近,则该像素点的灰度贡献值就越大,导致在传统STC算法中,在上下文区域中的所有像素点对于计算上下文先验概率都有同等的贡献度,因此当跟踪目标发生了尺度变化时,可能会对目标的上下文进行不必要的计算甚至失去对目标的跟踪。文献<citation id="172" type="reference">[<a class="sup">13</a>,<a class="sup">13</a>]</citation>提出将卡尔曼滤波和STC融合,利用卡尔曼滤波和STC共同预测目标位置,在目标发生尺度缩放、模糊时成功率不高;文献<citation id="173" type="reference">[<a class="sup">14</a>,<a class="sup">14</a>]</citation>在STC算法中加入了尺度自适应,对目标尺度变化有较好的鲁棒性,但是在目标被遮挡、模糊等场景下跟踪成功率不高。</p>
                </div>
                <div class="p1">
                    <p id="45">为了适应目标尺度变化,减少对上下文区域不必要的计算以及保持对目标的追踪,使STC算法在进行目标跟踪时有更好的针对性,本文提出一种改进STC和SURF特征联合优化的目标跟踪算法。在相邻的两帧视频图像中,目标变化的尺度不会太大,是连续的,所以本文在对相邻2帧图像进行目标跟踪时,先对相邻2帧图像提取其加速稳健特征点SURF,并对相邻2帧图像中的特征点进行匹配;再通过RANSAC(RANdom SAmple Consensus)算法消除误匹配点,根据2帧图像的特征点尺度变化大小对目标窗口进行分析调整,将新的窗口应用于当前帧作为目标窗口,随后对STC算法中的部分模型进行改进。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag"><b>2 时空上下文目标跟踪</b></h3>
                <div class="p1">
                    <p id="47">STC算法根据运动的、在采样区域中的点与其上下文之间的时空关系,利用时空上下文模型计算出置信图,在置信图中似然概率最大的位置就是目标在下一帧中的位置。计算置信图的公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中,<i><b>X</b></i>∈<b>R</b><sup>2</sup>是上下文区域里的一个像素点,<i><b>o</b></i>是目标点。</p>
                </div>
                <div class="p1">
                    <p id="50">在当前帧中,目标的位置用<i><b>x</b></i><sup>*</sup>表示,那么在当前帧中特征集为:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><msup><mrow></mrow><mtext>c</mtext></msup><mo>=</mo><mo stretchy="false">{</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">z</mi><mo>∈</mo><mi>Ω</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中,<i><b>z</b></i>是当前帧图像中的像素点,<i>I</i>(<i><b>z</b></i>)是上下文区域中像素点<i><b>z</b></i>的灰度值,<i>Ω</i><sub>c</sub>(<i><b>x</b></i><sup>*</sup>)是目标<i><b>x</b></i><sup>*</sup>的局部上下文区域。根据概率公式<i>P</i>(<i><b>X</b></i>,<i>c</i>(<i><b>z</b></i>)|<i><b>o</b></i>)可以将计算置信图的式(1)展开如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mtext>c</mtext></msup></mrow></msub><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mtext>c</mtext></msup></mrow></msub><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中,条件概率<i>P</i>(<i><b>X</b></i>|<i>c</i>(<i><b>z</b></i>),<i><b>o</b></i>)表示的是局部目标<i><b>o</b></i>与上下文区域的空间关系,在STC算法中作为空间上下文模型。在STC算法中对空间上下文建模时,将空间上下文模型<i>P</i>(<i><b>X</b></i>|<i>c</i>(<i><b>z</b></i>),<i><b>o</b></i>)表示为:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mi>h</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中,<i>h</i><sup>SC</sup>(<i><b>x</b></i>-<i><b>z</b></i>)表示的是目标位置的上下文区域中的像素点<i><b>z</b></i>与目标位置之间的相对位置。由于<i>h</i><sup>SC</sup>(<i><b>x</b></i>-<i><b>z</b></i>)是非径向函数,所以在进行目标识别时能够很好地分辨距离和特征相似但方向不同的点。</p>
                </div>
                <div class="p1">
                    <p id="57">概率<i>P</i>(<i>c</i>(<i><b>z</b></i>)|<i><b>o</b></i>)表示的是上下文区域的外观特征,在STC算法中作为上下文先验概率;在STC算法中对上下文先验模型建模时,将上下文先验模型<i>P</i>(<i>c</i>(<i><b>z</b></i>)|<i><b>o</b></i>)表示为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中,<i>I</i>(<i><b>z</b></i>)表示的是在目标点的上下文区域中像素点<i><b>z</b></i>的灰度值,<i>ω</i><sub><i>σ</i></sub>表示一个区域中心加权的高斯函数,该函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mi>a</mi><mo>×</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mfrac><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中,<i>a</i>代表的是一个规则化的参数,其作用是将上下文先验概率的输出限定在0～1;<i>σ</i><sup>2</sup>是正态分布函数的方差。</p>
                </div>
                <div class="p1">
                    <p id="62">将式(2)～式(5)代入到置信图计算的式(1)中可得:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>h</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>⨂</mo><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>×</mo><mi>a</mi><mo>×</mo><mi>exp</mi><mo stretchy="false">(</mo><mo>-</mo><mfrac><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中<image id="184" type="formula" href="images/JSJK201910012_18400.jpg" display="inline" placement="inline"><alt></alt></image>代表卷积运算。</p>
                </div>
                <div class="p1">
                    <p id="65">由于通常卷积运算较慢,并且在时域中进行卷积运算的运算结果和其在频域的乘积运算结果是一样的,所以采用频域中的乘积代替卷积的值。将式(6)的两边进行快速傅里叶变换可得:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>h</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>⊙</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中,<i>F</i>代表的是傅里叶变换,⊙代表的是频域元素相乘运算。对式(7)进行傅里叶逆变换可得空间上下文模型<i>h</i><sup>SC</sup>(<i><b>X</b></i>)计算公式:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo>(</mo><mrow><mfrac><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>a</mi><mo>×</mo><mi>exp</mi><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>σ</mi></mfrac></mrow><mo>|</mo></mrow><msup><mrow></mrow><mi>β</mi></msup></mrow><mo>)</mo></mrow></mrow><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中,<i>F</i><sup>-1</sup>代表的是傅里叶逆变换。</p>
                </div>
                <div class="p1">
                    <p id="70">根据式(8)可计算出第<i>t</i>帧的空间上下文模型为<i>h</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msubsup></mrow></math></mathml>,则根据式(9)计算与更新时空上下文模型:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Η</mi><msubsup><mrow></mrow><mn>2</mn><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>C</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>h</mi><msubsup><mrow></mrow><mn>1</mn><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Η</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>C</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><mo stretchy="false">)</mo><mi>Η</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>C</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>+</mo><mi>ρ</mi><mi>h</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中,<i>H</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>C</mtext></mrow></msubsup></mrow></math></mathml>(<i><b>X</b></i>)是第<i>t</i>帧的目标时空上下文模型,<i>ρ</i>是模型更新学习率。</p>
                </div>
                <div class="p1">
                    <p id="73">根据上述的公式可以计算出第<i>t</i>帧的上下文先验模型及空间上下文模型,则可根据式(11)计算第<i>t</i>帧的置信图<i>C</i><sub><i>t</i></sub>(<i><b>X</b></i>):</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Η</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mtext>S</mtext><mtext>Τ</mtext><mtext>C</mtext></mrow></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>⊙</mo><mi>F</mi><mo stretchy="false">(</mo><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">进而计算出第<i>t</i>帧中置信图概率最大位置也就是目标位置<i><b>x</b></i><sup>*</sup><sub><i>t</i></sub>。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag"><b>3 STC-SURF算法</b></h3>
                <div class="p1">
                    <p id="77">为了提高时空上下文目标跟踪算法对运动目标进行跟踪的实时性,本文提出一种基于<i>SURF</i>特征的<i>STC</i>自适应尺度跟踪(<i>STC</i>-<i>SURF</i>)算法。首先提取视频中相邻2帧的输出窗口的<i>SURF</i>特征点,并对2帧中的<i>SURF</i>特征点进行匹配,再通过<i>RANSAC</i>算法消除误匹配,根据前后2帧的目标尺度变化,在当前目标的窗口尺度下根据尺度变化的大小,计算得出后一帧中的窗口尺度。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>3.1 SURF特征匹配与RANSAC误匹配消除</b></h4>
                <div class="p1">
                    <p id="79"><i>Hessian</i>矩阵是<i>SURF</i>特征计算的关键,算法运行时需使用<i>Hessian</i>矩阵的判别式的值来判定<i>SURF</i>特征点所在的位置。在<i>SURF</i>特征点计算时<i>Hessian</i>矩阵<i><b>H</b></i>(<i><b>X</b></i>,<i>η</i>)表示为以下公式:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>η</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>μ</mi><mo stretchy="false">)</mo></mtd><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>μ</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>x</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>μ</mi><mo stretchy="false">)</mo></mtd><mtd><mi>L</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>y</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>μ</mi><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中,<i><b>H</b></i>(<i><b>X</b></i>,<i>η</i>)表示点<i><b>X</b></i>=(<i>x</i>,<i>y</i>)在尺度为<i>μ</i>时的Hessian矩阵,<i>L</i><sub><i>xx</i></sub>(<i><b>X</b></i>,<i>μ</i>)表示的是高斯核<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>g</mi><mo stretchy="false">(</mo><mi>η</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></math></mathml>和点<i><b>X</b></i>=(<i>x</i>,<i>y</i>)在当前图像帧中的卷积,同理<i>L</i><sub><i>xy</i></sub>(<i><b>X</b></i>,<i>μ</i>)、<i>L</i><sub><i>yy</i></sub>(<i><b>X</b></i>,<i>μ</i>)表示点<i><b>X</b></i>和高斯核<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>g</mi><mo stretchy="false">(</mo><mi>η</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>x</mi><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>g</mi><mo stretchy="false">(</mo><mi>η</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></math></mathml>的卷积。</p>
                </div>
                <div class="p1">
                    <p id="82">根据Hessian矩阵给出的判别式如下:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">s</mi><mi mathvariant="bold-italic">s</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">a</mi><mi mathvariant="bold-italic">n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>x</mi></mrow></msub><mo>*</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>y</mi></mrow></msub><mo>-</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>*</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">由于利用高斯核计算判别式的计算量过大,所以改用计算简单的盒式滤波器计算出的近似值<i>Dxx</i>、<i>Dxy</i>、<i>Dyy</i>来代替高斯二阶微分算子<i>L</i><sub><i>xx</i></sub>、<i>L</i><sub><i>xy</i></sub>、<i>L</i><sub><i>yy</i></sub>。并且为了减少由近似值替代引起的误差,通常在盒式滤波器的近似值<i>Dxy</i>前加上权值0.9,最终得出Hessian矩阵的近似判别式为:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext><mtext>p</mtext><mtext>r</mtext><mtext>o</mtext><mtext>x</mtext></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi>D</mi><mi>x</mi><mi>x</mi><mo>*</mo><mi>D</mi><mi>y</mi><mi>y</mi><mo>-</mo><mo stretchy="false">(</mo><mn>0</mn><mo>.</mo><mn>9</mn><mo>*</mo><mi>D</mi><mi>x</mi><mi>y</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">根据式(14)对不同的尺度空间的每个像素点计算Hessian矩阵判别式,将得出的值与自身邻域内的26个像素点判别式的值进行比较,选取出初始关键点,随后将不适合的部分关键点滤除掉,剩下的关键点作为该区域的特征点。</p>
                </div>
                <div class="p1">
                    <p id="87">找出特征点后,通过计算这些特征点一定范围内的Haar小波响应值,将响应值最大的方向作为该特征点主方向,从而确定出图像内所有特征点的方向,进而计算出所有特征点的描述子。</p>
                </div>
                <div class="p1">
                    <p id="88">提取SURF特征过程如图1所示。首先将Haar小波响应值最大的方向作为坐标轴主方向,在特征点周围取4*4的区块,每个区块包含5*5共25个像素点,统计每个区块的25个像素点的水平方向和垂直方向共4个方向的Haar小波特征,随后根据每个像素点4个方向的响应值计算出这个区块的水平方向、垂直方向共4个方向的向量作为这个特征点在区块的特征描述子,即每个特征点有一个16*4共64维向量作为该特征点的描述子。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SURF算法计算特征点描述子" src="Detail/GetImg?filename=images/JSJK201910012_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 <i>SURF</i>算法计算特征点描述子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 1 <i>SURF algorithm calculates feature point descriptor</i></p>

                </div>
                <div class="p1">
                    <p id="90">在提取<i>SURF</i>特征点后,在相邻的图像帧中寻找与该特征点欧氏距离最近的2个点,并且最近距离和第2近的距离的比值小于预设的阈值,则该特征点作为当前帧特征点的匹配点。</p>
                </div>
                <div class="p1">
                    <p id="91">通过<i>RANSAC</i>算法来消除在匹配过程中产生的误匹配点。<i>RANSAC</i>算法过程如下所示:</p>
                </div>
                <div class="area_img" id="185">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201910012_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="185">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201910012_18501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">模型内点越多则该模型越好。最后选择该模型内的点作为符合要求的点。</p>
                </div>
                <div class="p1">
                    <p id="101">图2是利用<i>SURF</i>特征匹配2幅图像的结果,图2<i>a</i>是原始匹配结果,图2<i>b</i>是根据原始匹配结果利用<i>RANSAC</i>算法消除误匹配后的结果。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SURF特征匹配与误匹配消除" src="Detail/GetImg?filename=images/JSJK201910012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 <i>SURF</i>特征匹配与误匹配消除  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 2 <i>SURF feature matching and mismatch elimination</i></p>

                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>3.2 尺度自适应调整</b></h4>
                <div class="p1">
                    <p id="104">由于在同一视频的相邻2帧中目标变化的尺度通常不会太大,因此在对视频中的第<i>t</i>帧目标窗口进行调整时,可将第<i>t</i>-1帧的目标窗口作为调整的基准。在第<i>t</i>-1帧和第<i>t</i>帧图像的上下文区域中计算并匹配<i>SURF</i>特征点,根据特征点的变化参数,在第<i>t</i>帧图像中对目标窗口进行尺度与位置调整,从而得到第<i>t</i>帧图像的实际目标窗口。</p>
                </div>
                <div class="p1">
                    <p id="105">首先计算第<i>t</i>-1帧和第<i>t</i>帧中上下文区域的<i>SURF</i>特征点,并将这2帧中的<i>SURF</i>特征点进行匹配,再通过<i>RANSAC</i>算法消除误匹配点。得到特征点集合<i>P</i><sub><i>t</i></sub><sub>-1</sub>={<i>p</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mn>1</mn></msubsup></mrow></math></mathml>,<i>p</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></math></mathml>,…,<i>p</i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>},<i>P</i><sub><i>t</i></sub>={<i>p</i><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>1</mn></msubsup></mrow></math></mathml>,<i>p</i><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>2</mn></msubsup></mrow></math></mathml>,…,<i>p</i><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>n</mi></msubsup></mrow></math></mathml>}。根据相邻2帧特征点变化尺度计算出集合中的点在前后2帧中横坐标偏移量集合<i>dx</i>={<i>dx</i><sub>1</sub>,<i>dx</i><sub>2</sub>,…,<i>dx</i><sub><i>n</i></sub>},以及纵坐标偏移量集合<i>dy</i>={<i>dy</i><sub>1</sub>,<i>dy</i><sub>2</sub>,…,<i>dy</i><sub><i>n</i></sub>}。相邻2帧的尺度伸缩值计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false">(</mo><mi>p</mi><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup><mo>,</mo><mi>p</mi><msubsup><mrow></mrow><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false">(</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>,</mo><mi>p</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">其中<i>dis</i>(<i>p</i><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>,<i>p</i><mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>)是第<i>t</i>帧中点<i>p</i><mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>和第<i>t</i>+1帧中点<i>p</i><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>的欧氏距离。通过计算得相邻帧的特征点尺度伸缩值集合<i>S</i>={<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>,…,<i>S</i><sub><i>n</i></sub>}。在横坐标偏移量集合<i>dx</i>={<i>dx</i><sub>1</sub>,<i>dx</i><sub>2</sub>,…,<i>dx</i><sub><i>n</i></sub>}、纵坐标偏移量集合<i>dy</i>={<i>dy</i><sub>1</sub>,<i>dy</i><sub>2</sub>,…,<i>dy</i><sub><i>n</i></sub>}、相邻两帧的特征点尺度伸缩值集合<i>S</i>={<i>S</i><sub>1</sub>,<i>S</i><sub>2</sub>,…,<i>S</i><sub><i>n</i></sub>}中,通过排列选出相对应的中值<i>m</i><sub><i>dx</i></sub>、<i>m</i><sub><i>dy</i></sub>、<i>m</i><sub><i>S</i></sub>。并且根据以下规则对当前帧的目标窗口进行调整:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mi>λ</mi><mo>×</mo><mo stretchy="false">(</mo><mi>m</mi><msub><mrow></mrow><mi>S</mi></msub><mo>-</mo><mi>μ</mi><mo stretchy="false">)</mo><mo>×</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>λ</mi><mo>×</mo><mo stretchy="false">(</mo><mi>m</mi><msub><mrow></mrow><mi>S</mi></msub><mo>-</mo><mi>μ</mi><mo stretchy="false">)</mo><mo>×</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>×</mo><mi>m</mi><msub><mrow></mrow><mi>S</mi></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>×</mo><mi>m</mi><msub><mrow></mrow><mi>S</mi></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>x</mi></mrow></msub><mo>-</mo><mi>s</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>y</mi></mrow></msub><mo>-</mo><mi>s</mi><msub><mrow></mrow><mn>2</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">其中,<i>s</i><sub>1</sub>、<i>s</i><sub>2</sub>分别是当前帧相比于前一帧图像中,目标点位置需要移动的横坐标的偏移量和纵坐标的偏移量,<i>λ</i>、<i>μ</i>是尺度参数,<i>w</i><sub><i>t</i></sub><sub>-1</sub>、<i>w</i><sub><i>t</i></sub>、<i>h</i><sub><i>t</i></sub><sub>-1</sub>、<i>h</i><sub><i>t</i></sub>分别是第<i>t</i>-1帧、第<i>t</i>帧目标窗口的宽度以及第<i>t</i>-1帧、第<i>t</i>帧目标窗口的高度,<i>x</i><sub><i>t</i></sub><sub>-1</sub>、<i>x</i><sub><i>t</i></sub>、<i>y</i><sub><i>t</i></sub><sub>-1</sub>、<i>y</i><sub><i>t</i></sub>分别是第<i>t</i>-1帧、第<i>t</i>帧目标窗口的左上角坐标。根据式(15)和式(16)可以确定当前帧目标窗口的位置以及大小,从而实现对目标窗口的调整。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>3.3 置信图计算方式更新</b></h4>
                <div class="p1">
                    <p id="111"><i>STC</i>算法根据上下文先验模型及空间上下文模型计算置信图。在计算上下文先验模型时仅以灰度值<i>I</i>(<i><b>z</b></i>)为依据,但灰度值在目标旋转、尺度缩放、光照变化、视角变换、目标被遮挡、模糊的场景下并不能很好地作为计算目标点特征的参数,因此本文提出使用目标点的<i>Hessian</i>矩阵判别式的值和灰度值共同作为计算上下文先验模型的参数。上下文先验模型<i>P</i>(<i><b>X</b></i>,<i>c</i>(<i><b>z</b></i>)|<i><b>o</b></i>)更新方法如下所示:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>λ</mi><mo>*</mo><mi>d</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext><mtext>p</mtext><mtext>r</mtext><mtext>o</mtext><mtext>x</mtext></mrow></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>*</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">其中,<i>λ</i>用来表示判别式<i>det</i>(<i><b>H</b></i><sub>approx</sub>)的重要程度,判别式<i>det</i>(<i><b>H</b></i><sub>approx</sub>)根据式(13)得到;<i>ω</i><sub><i>σ</i></sub>(<i><b>z</b></i>-<i><b>x</b></i><sup>*</sup>)是权值函数,根据式(5)得到。</p>
                </div>
                <div class="p1">
                    <p id="114">在当前帧已经完成目标窗口调整后,当前帧的空间上下文模型也应该进行改变。本文根据以下规则对空间上下文模型进行调整:若目标的尺度未发生变化则使用传统<i>STC</i>算法中的空间上下文更新方式;当通过计算<i>SURF</i>特征点匹配参数发现相邻2帧图像目标窗口尺度发生变化时,即当前通过计算得到2个目标窗口,传统<i>STC</i>算法计算出的目标窗口和自适应调整计算出的目标窗口,此时将<i>STC</i>算法中的空间上下文模型更新为自适应调整目标窗口得出的空间上下文模型。因此,在本文算法中空间上下文模型计算方法更新为:当目标尺度未发生变化时,根据传统<i>STC</i>算法计算空间上下文模型;当目标尺度发生了变化时,根据式(18)进行更新:</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mi>h</mi><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext><mo>-</mo><mtext>S</mtext><mtext>U</mtext><mtext>R</mtext><mtext>F</mtext></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">其中<i>h</i><sup>SC-SURF</sup>表示进行自适应窗口调整后得出的空间上下文模型。</p>
                </div>
                <div class="p1">
                    <p id="117">因此,为了提升目标跟踪的准确率,本文根据上述规则和式(17)、式(18)将置信图计算方式更新为以下规则。当目标尺度未发生变化时,将置信图计算方式更新为:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mi>c</mi></msup></mrow></msub><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mi>c</mi></msup></mrow></msub><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mi>c</mi></msup></mrow></msub><mi>h</mi></mstyle><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext></mrow></msup><mo>⨂</mo><mo stretchy="false">(</mo><mi>λ</mi><mo>*</mo><mi>d</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext><mtext>p</mtext><mtext>r</mtext><mtext>o</mtext><mtext>x</mtext></mrow></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>*</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">当目标尺度发生变化后,将置信图计算方式更新为:</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mi>c</mi></msup></mrow></msub><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo>,</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mi>c</mi></msup></mrow></msub><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">|</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>c</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>X</mi><msup><mrow></mrow><mi>c</mi></msup></mrow></msub><mi>h</mi></mstyle><msup><mrow></mrow><mrow><mtext>S</mtext><mtext>C</mtext><mo>-</mo><mtext>S</mtext><mtext>U</mtext><mtext>R</mtext><mtext>F</mtext></mrow></msup><mo>⨂</mo><mo stretchy="false">(</mo><mi>λ</mi><mo>*</mo><mi>d</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>p</mtext><mtext>p</mtext><mtext>r</mtext><mtext>o</mtext><mtext>x</mtext></mrow></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>Ι</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>*</mo><mi>ω</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><mo>-</mo><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>3.4 STC-SURF算法流程</b></h4>
                <div class="p1">
                    <p id="122"><i>STC</i>-<i>SURF</i>算法流程图如图3所示。</p>
                </div>
                <div class="area_img" id="123">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 STC-SURF算法流程图" src="Detail/GetImg?filename=images/JSJK201910012_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>STC</i>-<i>SURF</i>算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_123.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 3 <i>Flow chart of the STC</i>-<i>SURF algorithm</i></p>

                </div>
                <h3 id="124" name="124" class="anchor-tag"><b>4 实验结果及分析</b></h3>
                <div class="p1">
                    <p id="125">实验硬件环境为:<i>Intel</i>(<i>R</i>) <i>Core</i>(<i>TM</i>)<i>i</i>5-4200<i>M CPU</i>@2.50 2.50 <i>GHz CPU</i>、8 <i>GB</i>内存,软件环境为<i>Windows</i> 10 <i>x</i>64操作系统、<i>VS</i>2015、<i>OPENCV</i>2.4.13。将本文算法和<i>TLD</i>算法、<i>STC</i>算法一起进行了测试。在本次测试中共使用了6个公开的计算机视觉测试视频来对算法进行验证。使用的公开视频主要信息如表1所示。</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表1 测试数据集视频主要信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Main information of test dataset videos</b></p>
                    <p class="img_note"></p>
                    <table id="126" border="1"><tr><td><br />视频</td><td>跟踪目标</td><td>视频信息</td><td>总帧数</td></tr><tr><td><br />david</td><td>男人</td><td>光照,尺度变化</td><td>770</td></tr><tr><td><br />boy</td><td>男人</td><td>模糊,尺度变化</td><td>602</td></tr><tr><td><br />girle2</td><td>女孩</td><td>模糊,遮挡</td><td>312</td></tr><tr><td><br />dog</td><td>公仔</td><td>角度变化,旋转</td><td>1 345</td></tr><tr><td><br />FaceOcc1</td><td>女人</td><td>遮挡</td><td>892</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="127">为了验证STC-SURF算法调整窗口的有效性,本文以跟踪成功率作为评判的标准。跟踪成功率根据式(16)进行计算:</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mtext>t</mtext></msub><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false">)</mo></mrow><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mtext>t</mtext></msub><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">其中,<i>area</i>(·)是计算面积的函数,<i>R</i><sub>t</sub>是在算法跟踪时生成框的实际面积,<i>R</i><sub>g</sub>是真实的目标框面积。</p>
                </div>
                <div class="p1">
                    <p id="130">实验将5组测试视频david、boy、girle2、dog、FaceOcc1分次使用TLD算法、STC算法、STC-SURF算法进行测试。部分实验结果如图4～图8所示,其中白色框是STC-SURF算法跟踪结果,黑色框是TLD算法跟踪结果,灰色框是STC算法跟踪结果。</p>
                </div>
                <div class="p1">
                    <p id="131">3种不同的跟踪算法对david视频的跟踪结果如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="132">从图4可以看出,在第416帧中,TLD算法跟踪失败了,而STC算法和本文提出的STC-SURF算法都还有较好的跟踪效果,可见STC算法和STC-SURF算法对于光照变化剧烈的目标都有较好的跟踪效果。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 david视频中不同算法跟踪结果" src="Detail/GetImg?filename=images/JSJK201910012_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 david视频中不同算法跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Tracking results of david by different algorithms</p>

                </div>
                <div class="p1">
                    <p id="134">3种不同的跟踪算法对boy视频的跟踪结果如图5所示。可以看出,在第269帧中,TLD算法和STC算法都失去了对目标的跟踪,而STC-SURF算法还有较好的跟踪效果,可见对比其他2种算法,STC-SURF算法在目标快速运动产生模糊时对目标的跟踪效果要更好。</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 boy视频中不同算法跟踪结果" src="Detail/GetImg?filename=images/JSJK201910012_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 boy视频中不同算法跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Tracking results of boy by different algorithms</p>

                </div>
                <div class="p1">
                    <p id="136">3种不同的跟踪算法对girle2视频的跟踪结果如图6所示。可见在第55帧中,由于目标变模糊导致TLD算法失去了对目标的跟踪,而STC算法在第242帧中不能适应目标的尺度变化,相比较之下,STC-SURF算法有更好的跟踪效果,也能适应目标模糊和尺度变化。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 girle2视频中不同算法跟踪结果" src="Detail/GetImg?filename=images/JSJK201910012_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 girle2视频中不同算法跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Tracking results of girle2 by different algorithms</p>

                </div>
                <div class="p1">
                    <p id="138">3种不同的跟踪算法对dog视频的跟踪结果如图7所示。在第442帧中,由于目标和背景难以分辨,TLD算法失去了对目标的跟踪,而在第1 119帧中,STC算法由于不能适应目标尺度的变化,对目标的跟踪效果并不好,相比之下,STC-SURF算法有较好的跟踪效果。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 dog视频中不同算法跟踪结果" src="Detail/GetImg?filename=images/JSJK201910012_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 dog视频中不同算法跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Tracking results of dog by different algorithms</p>

                </div>
                <div class="p1">
                    <p id="140">3种不同的跟踪算法对FaceOcc1视频的跟踪结果如图8所示。在第660帧中,目标被遮挡,TLD算法开始发生漂移,相比之下,STC算法和STC-SURF算法的跟踪效果较好。</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910012_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 FaceOcc1视频中不同算法跟踪结果" src="Detail/GetImg?filename=images/JSJK201910012_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 FaceOcc1视频中不同算法跟踪结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910012_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 Tracking results of 
 FaceOcc1 by different algorithms</p>

                </div>
                <div class="p1">
                    <p id="142">表2是不同算法在各个视频中的平均跟踪成功率。由表2可看出,STC-SURF算法在不同视频中跟踪成功的综合表现都优于TLD算法和STC算法的。</p>
                </div>
                <div class="area_img" id="143">
                    <p class="img_tit"><b>表2 不同算法跟踪平均成功率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Different algorithms track average success rate</b></p>
                    <p class="img_note"></p>
                    <table id="143" border="1"><tr><td><br />算法</td><td>david</td><td>boy</td><td>girle2</td><td>dog</td><td>FaceOcc1</td></tr><tr><td><br />TLD</td><td>0.543</td><td>0.712</td><td>0.343</td><td>0.387</td><td>0.798</td></tr><tr><td><br />STC</td><td>0.785</td><td>0.551</td><td>0.538</td><td>0.425</td><td>0.568</td></tr><tr><td><br />STC-SURF</td><td>0.771</td><td>0.779</td><td>0.661</td><td>0.741</td><td>0.805</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="144" name="144" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="145">在对目标进行跟踪时,由于跟踪窗口不会在目标尺度发生变化时进行相应的变化,从而发生模型偏移,导致在对目标进行跟踪时可能会失去对目标的跟踪。本文提出改进STC和SURF特征联合优化的目标跟踪算法,将SURF特征与STC算法结合,提取SURF特征并将相邻2帧图像进行特征点匹配,再通过RANSAC算法进行误匹配消除,进而分析相邻2帧的特征点尺度变化,得到相应参数,通过参数对当前帧的目标窗口进行调整。通过将SURF特征和STC算法进行结合,本文算法在目标旋转、尺度缩放、光照变化、视角变换、目标被遮挡、模糊的场景下有更好的鲁棒性,并且目标窗口能够适应目标尺度的变化,在对目标进行追踪时有良好的追踪效果。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Object Tracking with Online Multiple Instance Learning">

                                <b>[1]</b> Babenko B,Yang M H,Belongie S.Visual tracking with online multiple instance learning[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2010,33(8):1619-1632.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201412024&amp;v=MjYxNDhaZVJtRnkvZ1ZiM0tLQ0xmWWJHNEg5WE5yWTlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Wang Li-jia,Jia Song-min,Li Xiu-zhi,et al.Person following for mobile robot using improved multiple instance learning [J].Acta Automatica Sinica,2014,40(12):2916-2925.(in Chinese)
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831201&amp;v=MTgyOTBkdEZDRGxWTHJPSWx3PU5qN0Jhck80SHRIT3A0eEVadXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Ross D A,Lim J,Lin R-S,et al.Incremental learning for robust visual tracking[J].International Journal of Computer Vision,2008,77(1-3):125-141.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast compressive tracking">

                                <b>[4]</b> Zhang K H,Zhang L,Yang M H.Fast compressive tracking[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2014,36(10):2002-2015.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual tracking decomposition">

                                <b>[5]</b> Kwon J,Lee K M.Visual tracking decomposition[C]//Proc of the 23rd IEEE Conference on Computer Vision and Pattern Recognition,2010:1269-1276.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Abrupt Motion Tracking Via Intensively Adaptive Markov-Chain Monte Carlo Sampling">

                                <b>[6]</b> Zhou X Z,Lu Y,Lu J W,et al.Abrupt motion tracking via intensively adaptive Markov-Chain Monte Carlo sampling[J].IEEE Transactions on Image Processing,2012,21(2):789-801.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nearest neighbor field driven stochastic sampling for abrupt motion tracking">

                                <b>[7]</b> Zhou T F,Lu Y,Di H J.Nearest neighbor field driven stochastic sampling for abrupt motion tracking[C]//Proc of IEEE International Conference on Multimedia and Expo,2014:1-6.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">

                                <b>[8]</b> Kalal Z,Mikolajczyk K,Matas J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2012,34(7):1409-1422.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Surf:Speeded up robust features">

                                <b>[9]</b> Bay H,Tuytelaars T,Gool L V.SURF:Speeded up robust features[C]//Proc of the 9th European Conference on Computer Vision,2006:404-417.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Ge Bao-yi,Zuo Xian-zhang,Hu Yong-jiang.Long-term target tracking based on feature fusion[J].Acta Optica Sinica,2018,38(11):211-223.(in Chinese)
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201811010&amp;v=MTg0ODk5bk5ybzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZiM0tMejdCYUxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Chen Zhi,Liu Pei-zhong,Luo Yan-min,et al.Multi-scale correlation filter tracking algorithm based on adaptive feature fusion[J].Journal of Computer-Aided Design &amp; Computer Graphics,2018,30(11):2063-2073.(in Chinese)
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast visual tracking via dense spatio-temporal context learning">

                                <b>[12]</b> Zhang Kai-hua,Zhang Lei,Liu Qing-shan,et al.Fast visual tracking via dense spatio-temporal context learning[C]//Proc of the 13th European Conference on Computer Vision,2014:127-141.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGKQ201811021&amp;v=MDM0NTVxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYjNLSVNyQWY3RzRIOW5Ocm85SFpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Zhang Xin-kun,Huang Shan.STC target tracking algorithm based on Kalman filtering[J].Electronics Opticals and Control,2018,25(11):102-105.(in Chinese)
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Zhang Jing,Wang Xu,Fan Hong-bo.Spatio temporal context target tracking algorithm of self-adaptive learning[J].Computer Engineering,2018,44(6):294-299.(in Chinese)
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 王丽佳,贾松敏,李秀智,等.基于改进在线多示例学习算法的机器人目标跟踪[J].自动化学报,2014,40(12):2916-2925.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201811025&amp;v=MDE0ODNVUkxPZVplUm1GeS9nVmIzS0lqWFRiTEc0SDluTnJvOUhZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 葛宝义,左宪章,胡永江.基于特征融合的长时目标跟踪算法[J].光学学报,2018,38(11):211-223.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 陈智,柳培忠,骆炎民,等.自适应特征融合的多尺度相关滤波目标跟踪算法[J].计算机辅助设计与图形学学报,2018,30(11):2063-2073.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 张新堃,黄山.结合Kalman滤波的时空上下文目标跟踪算法[J].电光与控制,2018,25(11):102-105.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201806050&amp;v=MTAxNzE0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmIzS0x6N0JiYkc0SDluTXFZOUFaSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 张晶,王旭,范洪博.自适应学习的时空上下文目标跟踪算法[J].计算机工程,2018,44(6):294-299.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201910012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910012&amp;v=Mjk2NzR6cXFCdEdGckNVUkxPZVplUm1GeS9nVmIzS0x6N0JaYkc0SDlqTnI0OUVab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
