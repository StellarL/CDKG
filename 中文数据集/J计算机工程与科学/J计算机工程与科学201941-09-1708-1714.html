<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358352061250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909026%26RESULT%3d1%26SIGN%3dNO%252b3NEm94nrppZurj73h6feCjE4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909026&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909026&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909026&amp;v=MDMxNzVrVWJ2TUx6N0JaYkc0SDlqTXBvOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="&lt;b&gt;2 方法框架&lt;/b&gt; "><b>2 方法框架</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;2.1 基于传感器数据的活动识别&lt;/b&gt;"><b>2.1 基于传感器数据的活动识别</b></a></li>
                                                <li><a href="#53" data-title="&lt;b&gt;2.2 基于CNN-LSTM的活动识别&lt;/b&gt;"><b>2.2 基于CNN-LSTM的活动识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="&lt;b&gt;3 实验及分析&lt;/b&gt; "><b>3 实验及分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;3.1 实验数据与实验环境&lt;/b&gt;"><b>3.1 实验数据与实验环境</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;3.2 实验及结果分析&lt;/b&gt;"><b>3.2 实验及结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="&lt;b&gt;4 结束语&lt;/b&gt; "><b>4 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="图1 CLAR方法总体框架">图1 CLAR方法总体框架</a></li>
                                                <li><a href="#55" data-title="图2 CNN用于提取活动特征">图2 CNN用于提取活动特征</a></li>
                                                <li><a href="#58" data-title="图3 单个LSTM神经元处理数据依赖">图3 单个LSTM神经元处理数据依赖</a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表1 不同活动在识别时的概率分布&lt;/b&gt;"><b>表1 不同活动在识别时的概率分布</b></a></li>
                                                <li><a href="#90" data-title="图4 CLAR识别方法分别与其他3种方法的准确率比较">图4 CLAR识别方法分别与其他3种方法的准确率比较</a></li>
                                                <li><a href="#93" data-title="图5 CLAR识别方法分别与其他3种方法的平均绝对误差比较">图5 CLAR识别方法分别与其他3种方法的平均绝对误差比较</a></li>
                                                <li><a href="#96" data-title="图6 4种方法的时间比较">图6 4种方法的时间比较</a></li>
                                                <li><a href="#99" data-title="图7 &lt;i&gt;L&lt;/i&gt;&lt;sub&gt;d&lt;/sub&gt;取不同值时的平均绝对误差和准确率">图7 <i>L</i><sub>d</sub>取不同值时的平均绝对误差和准确率</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="137">


                                    <a id="bibliography_1" title=" Chen Z,Zhu Q,Soh Y C,et al.Robust human activity recognition using smartphone sensors via CT-PCA and online SVM[J].IEEE Transactions on Industrial Informatics,2017,13(6):3070-3080." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust human activity recognition using smartphone sensors via CT-PCA and online SVM">
                                        <b>[1]</b>
                                         Chen Z,Zhu Q,Soh Y C,et al.Robust human activity recognition using smartphone sensors via CT-PCA and online SVM[J].IEEE Transactions on Industrial Informatics,2017,13(6):3070-3080.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_2" title=" Kalischewski K,Wagner D,Velten J,et al.Activity recognition for indoor movement and estimation of travelled path[C]//Proc of 2017 10th International Workshop on Multidimensional (nD) Systems (nDS),2017:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Activity recognition for indoor movement and estimation of travelled path">
                                        <b>[2]</b>
                                         Kalischewski K,Wagner D,Velten J,et al.Activity recognition for indoor movement and estimation of travelled path[C]//Proc of 2017 10th International Workshop on Multidimensional (nD) Systems (nDS),2017:1-5.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_3" title=" Sanal Kumar K P,Bhavani R.Human activity recognition in egocentric video using PNN,SVM,kNN and SVM+kNN classifiers[J].Cluster Computing,2017(3-4):1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human activity recognition in egocentric video using PNN SVM kNN and SVM+kNN classifiers">
                                        <b>[3]</b>
                                         Sanal Kumar K P,Bhavani R.Human activity recognition in egocentric video using PNN,SVM,kNN and SVM+kNN classifiers[J].Cluster Computing,2017(3-4):1-10.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_4" title=" Jalal A,Kamal S,Kim D.Depth silhouettes context:A new robust feature for human tracking and activity recognition based on embedded HMMs[C]//Proc of International Conference on Ubiquitous Robots and Ambient Intelligence,2015:294-299." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depth silhouettes context:A new robust feature for human tracking and activity recognition based on embedded HMMs">
                                        <b>[4]</b>
                                         Jalal A,Kamal S,Kim D.Depth silhouettes context:A new robust feature for human tracking and activity recognition based on embedded HMMs[C]//Proc of International Conference on Ubiquitous Robots and Ambient Intelligence,2015:294-299.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_5" title=" Ronao C A,Cho S B.Human activity recognition using smartphone sensors with two-stage continuous hidden Markov models[C]//Proc of International Conference on Natural Computation,2014:681-686." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human activity recognition using smartphone sensors with two-stage continuous hidden Markov models">
                                        <b>[5]</b>
                                         Ronao C A,Cho S B.Human activity recognition using smartphone sensors with two-stage continuous hidden Markov models[C]//Proc of International Conference on Natural Computation,2014:681-686.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_6" title=" Ordonez F J,Englebienne G,de Toledo P,et al.In-home activity recognition:Bayesian inference for hidden Markov models[J].IEEE Pervasive Computing,2014,13(3):67-75." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In-home activity recognition:Bayesian inference for hidden markov models">
                                        <b>[6]</b>
                                         Ordonez F J,Englebienne G,de Toledo P,et al.In-home activity recognition:Bayesian inference for hidden Markov models[J].IEEE Pervasive Computing,2014,13(3):67-75.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_7" title=" Panwar M,Dyuthi S R,Chandra P K,et al.CNN based approach for activity recognition using a wrist-worn accelerometer[C]//Proc of International Conference of the IEEE Engineering in Medicine &amp;amp; Biology Society,2017:2438-2441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CNN based approach for activity recognition using a wrist-worn accelerometer">
                                        <b>[7]</b>
                                         Panwar M,Dyuthi S R,Chandra P K,et al.CNN based approach for activity recognition using a wrist-worn accelerometer[C]//Proc of International Conference of the IEEE Engineering in Medicine &amp;amp; Biology Society,2017:2438-2441.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_8" title=" Khan M A A H,Roy N,Misra A.Scaling human activity recognition via deep learning-based domain adaptation[C]//Proc of 2018 IEEE International Conference on Pervasive Computing and Communications (PerCom),2018:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scaling human activity recognition via deep learning-based domain adaptation">
                                        <b>[8]</b>
                                         Khan M A A H,Roy N,Misra A.Scaling human activity recognition via deep learning-based domain adaptation[C]//Proc of 2018 IEEE International Conference on Pervasive Computing and Communications (PerCom),2018:1-9.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_9" title=" Nakano K,Chakraborty B.Effect of dynamic feature for human activity recognition using smartphone sensors[C]//Proc of 2017 IEEE 8th International Conference on Awareness Science and Technology (iCAST),2017:539-543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effect of dynamic feature for human activity recognition using smartphone sensors">
                                        <b>[9]</b>
                                         Nakano K,Chakraborty B.Effect of dynamic feature for human activity recognition using smartphone sensors[C]//Proc of 2017 IEEE 8th International Conference on Awareness Science and Technology (iCAST),2017:539-543.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_10" title=" Lee S M,Sang M Y,Cho H.Human activity recognition from accelerometer data using convolutional neural network[C]//Proc of IEEE International Conference on Big Data and Smart Computing,2017:131-134." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human activity recognition from accelerometer data using Convolutional Neural Network">
                                        <b>[10]</b>
                                         Lee S M,Sang M Y,Cho H.Human activity recognition from accelerometer data using convolutional neural network[C]//Proc of IEEE International Conference on Big Data and Smart Computing,2017:131-134.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_11" title=" Zeng M,Le T N,Yu B,et al.Convolutional neural networks for human activity recognition using mobile sensors[C]//Proc of International Conference on Mobile Computing,Applications and Services,2015:197-205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for human activity recognition using mobile sensors">
                                        <b>[11]</b>
                                         Zeng M,Le T N,Yu B,et al.Convolutional neural networks for human activity recognition using mobile sensors[C]//Proc of International Conference on Mobile Computing,Applications and Services,2015:197-205.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_12" title=" Guan Y,Pl&#246;tz T.Ensembles of deep LSTM learners for activity recognition using wearables[J].Proceedings of the ACM on Interactive,Mobile,Wearable and Ubiquitous Technologies,2017,1(2):Article No.11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensembles of deep Istm learners for activity recognition using wearables">
                                        <b>[12]</b>
                                         Guan Y,Pl&#246;tz T.Ensembles of deep LSTM learners for activity recognition using wearables[J].Proceedings of the ACM on Interactive,Mobile,Wearable and Ubiquitous Technologies,2017,1(2):Article No.11.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_13" title=" Singh D,Merdivan E,Psychoula I,et al.Human activity recognition using recurrent neural networks[C]//Proc of International Cross-Domain Conference for Machine Learning and Knowledge Extraction,2017:267-274." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human activity recognition using recurrent neural networks">
                                        <b>[13]</b>
                                         Singh D,Merdivan E,Psychoula I,et al.Human activity recognition using recurrent neural networks[C]//Proc of International Cross-Domain Conference for Machine Learning and Knowledge Extraction,2017:267-274.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_14" title=" Ma C Y,Chen M H,Kira Z,et al.TS-LSTM and temporal-inception:Exploiting spatiotemporal dynamics for activity recognition[J].Signal Processing:Image Communication,2019,71:76-87." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7D5AADFAD3177DC7B754CAB0C800827F&amp;v=MDg3MDRXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3JtOXhLdz1OaWZPZmJUTUc2QzkyL2swRU9nT0MzdE52QkZoN1RwNU93NlFyR0U5ZWJLY1I3M3BDT052RlNpVw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Ma C Y,Chen M H,Kira Z,et al.TS-LSTM and temporal-inception:Exploiting spatiotemporal dynamics for activity recognition[J].Signal Processing:Image Communication,2019,71:76-87.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_15" title=" Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTI0MjRaZVp1SHlqbVVMZklKbDhXYmhZPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_16" title=" Ullah A,Ahmad J,Muhammad K,et al.Action recognition in video sequences using deep bi-directional LSTM with CNN features[J].IEEE Access,2018,6:1155-1166." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition in video sequences using deep bi-directional LSTM with CNN features">
                                        <b>[16]</b>
                                         Ullah A,Ahmad J,Muhammad K,et al.Action recognition in video sequences using deep bi-directional LSTM with CNN features[J].IEEE Access,2018,6:1155-1166.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_17" title=" Wang X,Gao L,Song J,et al.Beyond frame-level CNN:Saliency-aware 3-D CNN with LSTM for video action recogni- tion[J].IEEE Signal Processing Letters,2017,24(4):510-514." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond frame-level CNNSaliency-aware 3D CNN with LSTM for video action recognition">
                                        <b>[17]</b>
                                         Wang X,Gao L,Song J,et al.Beyond frame-level CNN:Saliency-aware 3-D CNN with LSTM for video action recogni- tion[J].IEEE Signal Processing Letters,2017,24(4):510-514.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1708-1714 DOI:10.3969/j.issn.1007-130X.2019.09.025            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于CNN-LSTM的活动识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%85%81&amp;code=42876327&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李允</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%9F%E5%87%A1%E8%8D%A3&amp;code=10363187&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孟凡荣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%A3%8A&amp;code=09658041&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B5%E9%95%BF%E5%85%B4&amp;code=42876328&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邵长兴</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E6%B7%91%E6%95%8F&amp;code=42876329&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔淑敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E5%B0%91%E6%9D%B0&amp;code=42876330&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱少杰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%BF%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0041682&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国矿业大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在识别活动时,传统的循环神经网络RNN识别方法不考虑传感器活动数据之间依赖性强的问题,导致识别准确率降低。为了提高识别准确率,解决活动数据依赖性强的问题,用长短期记忆网络LSTM进行活动识别,LSTM在考虑当前点输入的同时考虑先前点的输出,能够保持数据之间的强依赖性。但是,LSTM在处理传感器活动数据的特征提取方面时间效率不高,而卷积神经网络CNN能共享卷积核,且可以从杂乱无章的数据中提取出明显特征向量。提出一种基于CNN-LSTM的活动识别方法CLAR,利用CNN能够很好地提取出活动序列数据中的特征向量,并将提取出的特征向量作为LSTM的输入,利用LSTM门限之间的相互作用进行活动识别,使得依赖性很强的活动数据成为活动识别的优势,进而提高活动识别的准确率和时间效率。实验表明,CLAR方法的识别准确率比单一神经网络活动识别方法的准确率提高了9%,时间平均缩短了10%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B4%BB%E5%8A%A8%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">活动识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=LSTM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">LSTM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李允（1994-），女，安徽萧县人，硕士生，研究方向为传感器数据活动识别。E-mail:1637557500@qq.com,通信地址：221116江苏省徐州市中国矿业大学南湖校区计算机学院B513&lt;image id="126" type="formula" href="images/JSJK201909026_12600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    孟凡荣（1962-），女，辽宁沈阳人，博士，教授，博士生导师，研究方向为数据库和数据挖掘。E-mail:mengfr@cumt.edu.cn,通信地址：221116江苏省徐州市中国矿业大学南湖校区计算机学院B513&lt;image id="128" type="formula" href="images/JSJK201909026_12800.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    张磊（1977-），男，江苏徐州人，博士，副教授，研究方向为轨迹数据挖掘。E-mail:zhanglei@cumt.edu.cn,通信地址：221116江苏省徐州市中国矿业大学南湖校区计算机学院B513&lt;image id="130" type="formula" href="images/JSJK201909026_13000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    邵长兴（1996-），男，山东枣庄人，硕士生，研究方向为轨迹活动预测。E-mail:1434797896@qq.com,通信地址：221116江苏省徐州市中国矿业大学南湖校区计算机学院B513&lt;image id="132" type="formula" href="images/JSJK201909026_13200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    崔淑敏（1995-），女，山东桓台人，硕士生，研究方向为轨迹数据挖掘。E-mail:17862962201@163.com,通信地址：221116江苏省徐州市中国矿业大学南湖校区计算机学院B513&lt;image id="134" type="formula" href="images/JSJK201909026_13400.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    朱少杰（1996-），男，江苏睢宁人，博士生，研究方向为多模态语义轨迹预测。E-mail:1208162499@qq.com,通信地址：221116江苏省徐州市中国矿业大学南湖校区计算机学院B513&lt;image id="136" type="formula" href="images/JSJK201909026_13600.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中央高校基本科研业务费专项资金(2014XT04);</span>
                                <span>教育部博士点基金(20110095110010);</span>
                                <span>江苏省自然科学基金(BK20130208);</span>
                    </p>
            </div>
                    <h1><b>An activity recognition method based on CNN-LSTM</b></h1>
                    <h2>
                    <span>LI Yun</span>
                    <span>MENG Fan-rong</span>
                    <span>ZHANG Lei</span>
                    <span>SHAO Chang-xing</span>
                    <span>CUI Shu min</span>
                    <span>ZHU Shao-jie</span>
            </h2>
                    <h2>
                    <span>School of Computer Science,China University of Mining and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>When identifying activities, the traditional recurrent neural network(RNN) recognition method does not consider the problem of strong dependency among sensor activity data, resulting in a decrease in recognition accuracy. In order to improve the recognition accuracy and solve the problem of strong dependency among sensor activity data, the long-short term memory network(LSTM) is used for activity recognition. The LSTM considers both the input of current points and the output of the previous points to maintain strong dependency among data. However, it has low time efficiency in feature extraction for sensor activity data. The convolutional neural network(CNN) can share convolution kernels and extract obvious feature vectors from disordered data. We present a CNN-LSTM activity recognition(CLAR) method. The CLAR uses the CNN to extract the feature vectors in the activity sequence data, takes the extracted feature vectors as the input of the LSTM, and uses the interaction between thresholds to recognize activities, which makes the highly dependency among activity data become an advantage of activity recognition, thus improving the accuracy and time efficiency of activity recognition. Experimental results show that the CLAR method is 9% more accurate than the single neural network activity recognition method, and it consumes 10% less time on average.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=activity%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">activity recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=LSTM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">LSTM;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Yun,born in 1994,MS candidate, her research interest includes activity recognition of sensor data.Address:B513,School of Computer Science,Nanhu Campus,China University of Mining and Technology,Xuzhou 221116,Jiangsu,P. R.China;
                                </span>
                                <span>
                                    MENG Fan-rong,born in 1962,PhD,professor,PhD supervisor,her research interests include database,and data mining.Address:B513,School of Computer Science,Nanhu Campus,China University of Mining and Technology,Xuzhou 221116,Jiangsu,P. R.China;
                                </span>
                                <span>
                                    ZHANG Lei,born in 1977,PhD,associate professor,his research interest includes trajectory data mining.Address:B513,School of Computer Science,Nanhu Campus,China University of Mining and Technology,Xuzhou 221116,Jiangsu,P. R.China;
                                </span>
                                <span>
                                    SHAO Chang-xing,born in 1996,MS candidate,his research interest includes spatiotemporal semantic activity prediction.Address:B513,School of Computer Science,Nanhu Campus,China University of Mining and Technology,Xuzhou 221116,Jiangsu,P. R.China;
                                </span>
                                <span>
                                    CUI Shu min,born in 1995,MS candidate,her research interest includes trajectory data mining.Address:B513,School of Computer Science,Nanhu Campus,China University of Mining and Technology,Xuzhou 221116,Jiangsu,P. R.China;
                                </span>
                                <span>
                                    ZHU Shao-jie,born in 1996,PhD candidate,his research interest includes multimodal semantic trajectory prediction.Address:B513,School of Computer Science,Nanhu Campus,China University of Mining and Technology,Xuzhou 221116,Jiangsu,P. R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-12-12</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="38">深度学习方法由于在其他机器学习领域的成功使得其在活动识别领域受到广泛的关注。能够正确识别活动传感器数据对于异常活动检测、生活援助和老年人看护从而避免意外事故的发生起着至关重要的作用。日常活动过程中产生大量传感器数据,这些数据的积累使识别活动成为可能。</p>
                </div>
                <div class="p1">
                    <p id="39">早期的研究大多基于马尔科夫模型和支持向量机SVM(Support Vector Machine)等统计学方法进行活动识别。Chen等<citation id="171" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>基于坐标变换-主成分分析消除方向变化对活动的影响,利用在线支持向量机更新SVM的参数从而提高准确率。Kalischewski等<citation id="172" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>采用SVM与惯性测量单元的机器学习方法来确定不同类型的活动,将检测到的人类活动与惯性测量单元的加速度计数据结合确定活动路径来支持室内定位系统。Sanal Kumar等<citation id="173" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>从活动视频中检索灰度共生矩阵和局部二值模型等特征,用SVM分类器对中心活动数据分类并识别活动。Jalal等<citation id="174" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>用多特征和嵌入式隐马尔科夫的模型方法在视频深度图的基础上进行活动识别。Ronao等<citation id="175" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>根据传感器数据的固有时间序列和连续特征,构建两阶段连续隐马尔科夫模型框架提高活动分类识别性能。Ordonez等<citation id="176" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>采用隐马尔科夫模型的贝叶斯推理进行活动识别。</p>
                </div>
                <div class="p1">
                    <p id="40">随着神经网络的兴起,很多学者倾向于利用神经网络研究活动识别。Panwar等<citation id="177" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>用卷积神经网络CNN(Convolutional Neural Network)识别,使用单个三轴加速度计在真实条件下与日常生活活动相关的基本手臂动作。Khan等<citation id="178" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出一种利用基于CNN的活动识别的局部依赖性和维度不变性,从加速度计数据中提取与上下文无关的活动特征的方法。Nakano等<citation id="179" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>利用时间序列数据中有效的特征进行活动识别,使用CNN提取标准数据集中的动态特征,并与多层感知机、SVM和k-近邻等分类算法进行了比较。Lee等<citation id="180" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>利用一维深度CNN的方法将从用户智能手机收集到的三轴加速度计数据处理成图谱信号以识别人类活动。Zeng等<citation id="181" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>用CNN捕获加速度信号的局部特性,并利用权重共享技术进行基于加速度信号的活动识别。但是,CNN在处理具有依赖性序列数据的时候无法分析数据之间的关系。</p>
                </div>
                <div class="p1">
                    <p id="41">为了很好地解决依赖性序列数据的问题,很多学者利用长短期记忆网络LSTM(Long Short-Term Memory)模型来训练数据并进行活动识别。Guan等<citation id="182" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将多个深度LSTM集合成分类器,解决可穿戴设备采集的数据集的不平衡问题,将传感器数据流作为模型输入,通过LSTM有效的记忆功能进行活动识别。Singh等<citation id="183" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>使用公共数据集上的长短期记忆分类器进行人类活动的识别。Ma等<citation id="184" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>证明在时空特征矩阵上同时使用循环神经网络RNN(Recurrent Neural Network)(具有LSTM)和时间相结合,利用时空动态来改善整体性能识别活动。Hochreiter等<citation id="185" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>也针对时间序列数据需要解决的长期依赖问题提出了LSTM模型,在RNN的基础上保证了数据之间的依赖关系。</p>
                </div>
                <div class="p1">
                    <p id="42">目前,很多研究人员结合CNN和LSTM来实现活动识别。Ullah等<citation id="186" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>使用CNN和深度双向LSTM网络处理视频数据,并在UCF-101、YouTube 11动作和HMDB51在内的3个基准数据集上通过实验证明了CNN和LSTM结合在活动识别中能取得良好的结果。Wang等<citation id="187" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>利用在视频镜头上集成LSTM和3-D CNN特征来进行视频动作识别,并通过基准数据集验证了该方法的可行性。</p>
                </div>
                <div class="p1">
                    <p id="43">基于以上研究,本文利用CNN能够实现特征提取且能缩短特征提取时间和LSTM能够记忆依赖性很强的数据之间的关系的优势,提出一种基于CNN-LSTM的活动识别方法CLAR(CNN-LSTM Activity Recognition),解决活动序列数据特征提取和数据依赖问题。该方法首先将预处理过的序列数据输入到CNN中提取特征向量;其次将上一步提取的不同活动特征向量作为LSTM的输入,利用LSTM中输入门、遗忘门和输出门的相互作用解决依赖性强的数据对实验的影响问题,从而进行活动识别。实验结果表明,CLAR方法提高了模型训练的准确率并缩短了训练时间。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag"><b>2 方法框架</b></h3>
                <div class="p1">
                    <p id="45">相对于传统的活动识别方法存在的弊端,CLAR方法能够处理序列活动数据和具有很强依赖性的活动数据,从而提高了活动识别的准确率。CLAR方法主要分为2部分:深度特征提取和LSTM分类识别。总体框架如图1所示。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CLAR方法总体框架" src="Detail/GetImg?filename=images/JSJK201909026_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 CLAR方法总体框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 The overall framework 
 of the CLAR method</p>

                </div>
                <div class="p1">
                    <p id="47">如图1所示为CLAR方法的总体框架,第1步将预处理过的序列数据作为模型初始输入输入到卷积神经网络中进行特征提取,提取出不同的活动特征。第2步将提取的不同的活动特征向量作为LSTM的输入,通过学习模型分类出不同的活动类型。活动输入数据为预处理之后的序列数据,将该一维序列数据作为CNN活动特征提取模型的输入,通过卷积神经网络的卷积和池化交替得到活动特征向量,将CNN得到的深度活动特征向量输入到LSTM模型中时,根据遗忘门控制细胞状态中的信息选择性遗忘,输入门确定哪些信息留在细胞状态中并更新细胞状态,输出门运行 sigmoid 层来确定将输出细胞状态的哪部分。以<i>t</i>时刻的传感器数据时间序列<i><b>x</b></i>(<i>t</i>)为例,<i><b>c</b></i>(<i>t</i>)表示<i>t</i>时刻的单元状态,<i><b>h</b></i>(<i>t</i>)为<i>t</i>时刻隐藏层状态,通过LSTM的长期记忆功能,根据门控制单元选择性地保留输入的活动数据信息,从而准确识别出活动,提高了活动识别的准确率。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>2.1 基于传感器数据的活动识别</b></h4>
                <div class="p1">
                    <p id="49">活动识别以智能家居中传感器采集数据为输入,通过活动数据之间的关系,来进行活动的识别和分类。</p>
                </div>
                <div class="p1">
                    <p id="50"><b>定义1</b> 传感器数据序列<i><b>S</b></i>:设有<i>N</i>个传感器,则传感器产生的传感器数据序列<i><b>S</b></i>=(<i><b>S</b></i><sub>1</sub>,<i><b>S</b></i><sub>2</sub>,…,<i><b>S</b></i><sub><i>i</i></sub>,…,<i><b>S</b></i><sub><i>N</i></sub>),其中1≤<i>i</i>≤<i>N</i>。</p>
                </div>
                <div class="p1">
                    <p id="51"><b>定义2</b> 传感器<i><b>S</b></i><sub><i>i</i></sub>产生的传感器序列<i><b>S</b></i><sub><i>i</i></sub>=(<i>d</i><sub><i>i</i></sub><sub>1</sub>,<i>d</i><sub><i>i</i></sub><sub>2</sub>,…,<i>d</i><sub><i>iL</i></sub>),<i>L</i>表示传感器<i><b>S</b></i><sub><i>i</i></sub>数据序列的长度。</p>
                </div>
                <div class="p1">
                    <p id="52"><b>定义3</b> 活动集合<i>A</i>:数据集中所有活动集合<i>A</i>=(<i><b>A</b></i><sub>1</sub>,<i><b>A</b></i><sub>2</sub>,…,<i><b>A</b></i><sub><i>i</i></sub>,…,<i><b>A</b></i><sub><i>m</i></sub>),其中1≤<i>i</i>≤<i>m</i>。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53"><b>2.2 基于CNN-LSTM的活动识别</b></h4>
                <div class="p1">
                    <p id="54">基于CNN-LSTM的活动识别是在CNN模型的基础上加入LSTM模型,利用CNN处理特征提取和LSTM处理长时间序列数据的优势,根据传感器活动序列数据的特征,进行活动识别,解决时间序列数据之间的依赖性问题,提高识别准确率。在CLAR中CNN模型在活动识别中的具体模型如图2所示。</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN用于提取活动特征" src="Detail/GetImg?filename=images/JSJK201909026_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CNN用于提取活动特征  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 CNN used to extract activity features</p>

                </div>
                <div class="p1">
                    <p id="56">在图2中,基于CLAR方法的CNN有2层结构,一为特征提取层,特征提取结构采用ReLU函数作为卷积网络的激活函数。二是特征映射层,特征映射结构采用sigmoid函数作为卷积网络的激活函数。将预处理之后的活动序列数据作为模型的输入,并设置长度为<i>l</i>的卷积核,经过卷积操作得到长度为<i>le</i>=(<i>n</i>-<i>l</i>+1)的新特征序列,再经过最大池化过程缩短序列长度,最后得到提取的活动特征向量<i><b>v</b></i>=(<i>v</i><sub>1</sub>,<i>v</i><sub>2</sub>,…,<i>v</i><sub><i>g</i></sub>)。</p>
                </div>
                <div class="p1">
                    <p id="57">而LSTM作为循环神经网络的延伸,能够很好地解决时间序列中数据依赖性很强的问题。LSTM模型在活动识别中的具体模型如图3所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 单个LSTM神经元处理数据依赖" src="Detail/GetImg?filename=images/JSJK201909026_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 单个LSTM神经元处理数据依赖  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Single LSTM neuron processing data dependencies</p>

                </div>
                <div class="p1">
                    <p id="59">图3表示单个LSTM神经元解决数据之间依赖性的结构。因此,LSTM代替了CNN网络层的全连接层,将得到的活动特征向量<i><b>x</b></i>={<i>v</i><sub>1</sub>,<i>v</i><sub>2</sub>,…,<i>v</i><sub><i>g</i></sub>}={<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>g</i></sub>}作为LSTM的输入,每一个特征向量对应着一个活动标签<i><b>A</b></i><sub><i>i</i></sub>。LSTM根据其输入门、遗忘门和输出门之间的控制来决定细胞状态的保留情况,保留依赖性强的数据之间的关系,解决长期依赖性问题,从而正确识别出活动。<i><b>c</b></i>(<i>t</i>-1)表示上一时刻的单元状态,LSTM的思想就是当前时刻<i>t</i>的状态取决于相同神经元的前一时刻<i>t</i>-1的状态。其中,我们定义<i><b>h</b></i>(<i>t</i>)表示当前时刻隐藏层的输出状态,如公式(1)所示。</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mspace width="0.25em" /><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中,<i><b>x</b></i>(<i>t</i>)表示在<i>t</i>时刻神经元的输入数据,<i><b>h</b></i>(<i>t</i>-1)表示上一时刻即<i>t</i>-1时刻 LSTM的输出。通过遗忘门限、输入门限、输出门限后的值分别用式(2)～式(4)表示为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>f</mtext><mo stretchy="false">)</mo></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>f</mtext><mo stretchy="false">)</mo></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>f</mtext><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">i</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>i</mtext><mo stretchy="false">)</mo></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>i</mtext><mo stretchy="false">)</mo></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>i</mtext><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">o</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>o</mtext><mo stretchy="false">)</mo></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>o</mtext><mo stretchy="false">)</mo></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>o</mtext><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中,<i><b>f</b></i>(<i>t</i>)、<i><b>i</b></i>(<i>t</i>)、<i><b>o</b></i>(<i>t</i>)代表相应门限的输出状态,<i><b>W</b></i><b>、</b><i><b>U</b></i>代表各个分支的权重,<i><b>b</b></i>代表各个门限的偏置值。输入门的激活函数sigmoid决定更新哪些信息,然后tanh层创造一个新单元状态<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>˜</mo></mover></math></mathml>(<i>t</i>),如式(5)所示。</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>tanh</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">(</mo><mtext>c</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">U</mi><mo stretchy="false">(</mo><mtext>c</mtext><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">同时将旧单元状态更新到新单元状态<i><b>c</b></i>(<i>t</i>),如式(6)所示。</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="bold-italic">i</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>⋅</mo><mover accent="true"><mi mathvariant="bold-italic">c</mi><mo>˜</mo></mover><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">输出门的sigmoid函数决定输出哪些信息,并通过tanh函数得到当前隐藏层的输出<i><b>h</b></i>(<i>t</i>),如式(7)所示。</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold-italic">o</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>tanh</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">c</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">在LSTM整个计算过程中,<i>t</i>-1时刻神经元的隐藏层的输出<i><b>h</b></i>(<i>t</i>-1)和<i><b>x</b></i>(<i>t</i>)作为<i>t</i>时刻神经元的输入,输入层神经元的输入在激活函数的作用下产生神经元输出<i><b>h</b></i>(<i>t</i>)。<i>t</i>-1时刻神经单元的记忆状态<i><b>c</b></i>(<i>t</i>-1),形成<i>t</i>时刻的神经单元记忆状态<i><b>c</b></i>(<i>t</i>)。在学习过程中,式(5)和式(7)表明激活函数tanh作用于<i><b>c</b></i>(<i>t</i>),作用后的<i><b>c</b></i>(<i>t</i>)点乘输出门限控制后的输出状态<i><b>o</b></i>(<i>t</i>),从而得到在<i>t</i>时刻隐藏层输出<i><b>h</b></i>(<i>t</i>),然后由softmax激活函数将隐藏层的权重<i>α</i>的值归一化,并将隐藏层的权重<i>α</i><sub>(i)</sub>与当前时刻隐藏层的输出<i><b>h</b></i>(<i>t</i>)相乘并求和得到活动识别样本最终的特征表达<i><b>y</b></i>(<i>t</i>),进而得出识别活动在样本集中各自所占概率,如式(8)所示。</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><mo>∑</mo><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mtext>i</mtext><mo stretchy="false">)</mo></mrow></msub><mi mathvariant="bold-italic">h</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">所以,LSTM输入门、输出门和遗忘门之间的交互决定保留或舍弃的信息,从而解决了数据依赖性强的问题。如此循环,在迭代完一次 batch(训练的一次样本)之后,计算准确率作为预测活动和实际活动之间的差距。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag"><b>3 实验及分析</b></h3>
                <h4 class="anchor-tag" id="73" name="73"><b>3.1 实验数据与实验环境</b></h4>
                <div class="p1">
                    <p id="74">本文的数据集是收集测试人员从2016年7月25日到2016年8月01日和2016年11月14日到2016年12月05日期间在公寓生活的传感器数据集ContextAct@A4H。该数据采集过程中共安装371个传感器,每个传感器有不同的传感器ID。从364 000多条原始传感器数据集样本中选取6个活动传感器的所有数据作为数据集,将活动的开始时间和结束时间内同一个传感器数据序列化,并预处理数据集中会对实验结果造成影响的数据,将归一化处理之后的数据集作为输入数据集。训练数据和测试数据按8∶2从预处理之后的数据集中切分。在实验过程中,设置时间数据序列长度<i>L</i><sub>d</sub>=8,设置学习率<i>η</i> =0.001,目的是保持训练过程的相对稳定;<i>epochs</i>=1000,即对样本数据集进行1 000次训练;<i>batch</i>_<i>size</i>=200,即每次训练从训练集中取 200个样本进行训练。在确定好初始参数之后,将活动数据和对应的参数数据共同作为CLAR方法的输入,开始训练。</p>
                </div>
                <div class="p1">
                    <p id="75">模型训练完之后,采用测试数据集来测试训练好的CLAR方法的优劣。</p>
                </div>
                <div class="p1">
                    <p id="76">实验程序代码采用Python 2.7编写。实验操作系统采用Ubuntu 16.04。实验硬件环境:CPU四核,Core i7处理器2.0 GHz,内存为8 GB。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>3.2 实验及结果分析</b></h4>
                <h4 class="anchor-tag" id="78" name="78">3.2.1 评价指标</h4>
                <div class="p1">
                    <p id="79">为了评估CLAR方法在活动识别上的性能,参考相关文献,我们将CLAR方法识别的准确率和时间与CNN和LSTM识别方法作比较,同时采用平均绝对误差(<i>MAE</i>)衡量方法性能。我们使用准确率<i>Acc</i>和平均绝对误差<i>MAE</i>来量化CLAR的性能。</p>
                </div>
                <div class="p1">
                    <p id="80"><b>定义4</b>(准确率<i>Acc</i>) 用准确率<i>Acc</i>来评估基于CLAR方法中识别活动<i><b>A</b></i>′<sub><i>i</i></sub>和真实活动<i><b>A</b></i><sub><i>i</i></sub>之间分类结果的好坏(其中0≤<i>Acc</i>≤1)。<i>Acc</i>越高,表示模型越好,结果越好,如式(9)所:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mo>=</mo><mi>Τ</mi><mi>R</mi><mo>/</mo><mo stretchy="false">(</mo><mi>Τ</mi><mi>R</mi><mo>+</mo><mi>F</mi><mi>R</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">其中,<i>TR</i>表示为识别成功的样本数,<i>FR</i>表示为识别失败的样本数。</p>
                </div>
                <div class="p1">
                    <p id="83"><b>定义5</b>(平均绝对误差<i>MAE</i>) <i>MAE</i>的指标用来表明预测结果<i>y</i>′<sub><i>t</i></sub>是否接近真实值<i>y</i><sub><i>t</i></sub>,<i>MAE</i>越小,证明模型性能好,特征表达能力强,如式(10)所示:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>*</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy="false">|</mo></mstyle><mrow><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">3.2.2 结果分析</h4>
                <div class="p1">
                    <p id="86">经过实验对比,确定分类时的活动概率达到70%以上为识别正确。以不同时刻的活动数据序列为例,实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表1 不同活动在识别时的概率分布</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Probability distribution of different activities at the time of identification</b></p>
                    <p class="img_note">%</p>
                    <table id="87" border="1"><tr><td rowspan="2"><br />活动</td><td colspan="6"><br />识别时刻</td></tr><tr><td><br /><i>t</i><sub>1</sub></td><td><i>t</i><sub>2</sub></td><td><i>t</i><sub>3</sub></td><td><i>t</i><sub>4</sub></td><td><i>t</i><sub>5</sub></td><td><i>t</i><sub>6</sub></td></tr><tr><td><br /><b><i>A</i></b><sub>1</sub></td><td>5.324</td><td>3.541</td><td>10.642</td><td>1.563</td><td>5.852</td><td>69.342</td></tr><tr><td><br /><b><i>A</i></b><sub>2</sub></td><td>4.342</td><td>9.785</td><td>70.320</td><td>5.902</td><td>4.824</td><td>7.110</td></tr><tr><td><br /><b><i>A</i></b><sub>3</sub></td><td>6.441</td><td>7.483</td><td>5.862</td><td>71.798</td><td>3.748</td><td>4.784</td></tr><tr><td><br /><b><i>A</i></b><sub>4</sub></td><td>72.502</td><td>4.312</td><td>5.273</td><td>7.472</td><td>6.974</td><td>6.320</td></tr><tr><td><br /><b><i>A</i></b><sub>5</sub></td><td>2.394</td><td>71.451</td><td>2.890</td><td>8.243</td><td>5.957</td><td>8.585</td></tr><tr><td><br /><b><i>A</i></b><sub>6</sub></td><td>8.997</td><td>3.473</td><td>5.013</td><td>5.022</td><td>72.649</td><td>3.859</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="88">从表1中的不同活动识别结果可以看出,活动在不同时刻的分类概率一般分布在70%左右,因此认为不同活动在识别时的概率达到70%时则认为识别准确。</p>
                </div>
                <div class="p1">
                    <p id="89">首先,分别比较CLAR识别方法和CNN模型识别、LSTM模型识别以及文献<citation id="188" type="reference">[<a class="sup">16</a>]</citation>的卷积神经网络和深度双向LSTM网络CDL(CNN and Deep bidirectional LSTM network)的准确率,结果如图4所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CLAR识别方法分别与其他3种方法的准确率比较" src="Detail/GetImg?filename=images/JSJK201909026_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 CLAR识别方法分别与其他3种方法的准确率比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Accuracy comparison of CLAR recognition 
 method with the other three methods</p>

                </div>
                <div class="p1">
                    <p id="91">从图4a和图4b可以看出,随着迭代次数的增加,识别的准确率也持续增加。在迭代次数为400时,CNN方法和LSTM方法相对于CLAR方法,增长幅度基本一致。在迭代次数为500时,CLAR方法明显增长较快且最终趋于稳定,稳定在73%,相对于其他2种方法,准确率提高了9%。从图4c可以看出,CLAR方法的准确率整体比文献<citation id="189" type="reference">[<a class="sup">16</a>]</citation>中提出的CDL方法的准确率高出2%左右。</p>
                </div>
                <div class="p1">
                    <p id="92">其次,在衡量标准上,我们采用了平均绝对误差(<i>MAE</i>)作为评价指标。通过比较不同识别方法预测出来的活动和真实活动之间的平均绝对误差来判定CLAR方法的好坏。实验结果如图5所示。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 CLAR识别方法分别与其他3种方法的平均绝对误差比较" src="Detail/GetImg?filename=images/JSJK201909026_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 CLAR识别方法分别与其他3种方法的平均绝对误差比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Mean absolute error comparison of 
 CLAR recognition method with the other three methods</p>

                </div>
                <div class="p1">
                    <p id="94">从图5可以看出,随着迭代次数的增加,平均绝对误差稳定下降。在迭代次数为300时,相比于CLAR方法,CNN方法、LSTM方法和CDL方法的平均绝对误差下降幅度基本保持一致。在迭代次数为400时,CLAR方法的平均绝对误差明显下降较快且最终稳定在3.6%,相对于其他3种方法,其平均绝对误差减小了0.7%。</p>
                </div>
                <div class="p1">
                    <p id="95">然后,比较CLAR方法在训练过程中和CNN方法、LSTM方法、CDL方法针对同样的数据集模型完成训练所需要的时间,比较结果如图6所示。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种方法的时间比较" src="Detail/GetImg?filename=images/JSJK201909026_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种方法的时间比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Compare the time of the four methods</p>

                </div>
                <div class="p1">
                    <p id="97">在图6中,CLAR方法的模型训练完成时间明显低于单一的CNN方法、LSTM方法和文献<citation id="190" type="reference">[<a class="sup">16</a>]</citation>中的CDL方法,CLAR最好的结果是8 891 s,而CNN和LSTM方法最好的结果分别为9 676 s和10 143 s,CDL方法的最好结果为9 356 s,CLAR识别方法在时间耗费方面比CNN和LSTM方法分别降低了8%和12%,比CDL识别方法降低了5%。</p>
                </div>
                <div class="p1">
                    <p id="98">最后,测试CLAR方法中时间序列长度对识别准确率和平均绝对误差的影响,在CLAR方法中通过对<i>L</i><sub>d</sub>取不同的值来观察<i>L</i><sub>d</sub>的敏感性,结果如图7所示。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909026_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 Ld取不同值时的平均绝对误差和准确率" src="Detail/GetImg?filename=images/JSJK201909026_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 <i>L</i><sub>d</sub>取不同值时的平均绝对误差和准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909026_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Mean absolute error and accuracy 
 with different <i>L</i><sub>d</sub> values</p>

                </div>
                <div class="p1">
                    <p id="100">在图7a和图7b中,根据每次输入的时间序列长度的不同比较其平均绝对误差和准确率,通过比较可以看出,当<i>L</i><sub>d</sub>=8时的平均绝对误差最小且准确率最高,说明其效果最好。当<i>L</i><sub>d</sub>分别设置成6和7时,由于数据之间的依赖关系不明确,导致准确率和平均绝对误差相对较大;当<i>L</i><sub>d</sub>分别设置成9和10时,传感器的数据随着时间呈现出较大波动,同样影响数据之间的关系,导致准确率和平均绝对误差相对较大。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag"><b>4 结束语</b></h3>
                <div class="p1">
                    <p id="102">本文利用从智能家居中真实采集到的传感器活动数据,针对活动传感器数据依赖性问题,提出一种基于<i>CNN</i>-<i>LSTM</i>的活动识别方法<i>CLAR</i>,解决活动传感器序列数据之间强依赖性问题的同时提高效率。通过将原始数据预处理之后作为<i>CNN</i>输入,<i>CNN</i>能够自动提取数据特征向量,减少了特征提取时间,<i>LSTM</i>能够处理强依赖性数据,并通过不断调整实验中相关参数,得到可靠的实验结果。实验表明,<i>CLAR</i>方法准确率相比<i>CNN</i>模型和<i>LSTM</i>模型有一定的提高,同时降低了平均绝对误差和缩短了训练时间,充分证明了<i>CLAR</i>方法的可行性和完整性。</p>
                </div>
                <div class="p1">
                    <p id="103">本文直接将传感器数据输入到<i>CLAR</i>中,数据安全问题不能保证,可能会因此造成安全问题。因此,未来研究方向为计划对传感器数据增加一定的噪音或者干扰,在保证数据完整性的前提下保证其安全性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="137">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust human activity recognition using smartphone sensors via CT-PCA and online SVM">

                                <b>[1]</b> Chen Z,Zhu Q,Soh Y C,et al.Robust human activity recognition using smartphone sensors via CT-PCA and online SVM[J].IEEE Transactions on Industrial Informatics,2017,13(6):3070-3080.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Activity recognition for indoor movement and estimation of travelled path">

                                <b>[2]</b> Kalischewski K,Wagner D,Velten J,et al.Activity recognition for indoor movement and estimation of travelled path[C]//Proc of 2017 10th International Workshop on Multidimensional (nD) Systems (nDS),2017:1-5.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human activity recognition in egocentric video using PNN SVM kNN and SVM+kNN classifiers">

                                <b>[3]</b> Sanal Kumar K P,Bhavani R.Human activity recognition in egocentric video using PNN,SVM,kNN and SVM+kNN classifiers[J].Cluster Computing,2017(3-4):1-10.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depth silhouettes context:A new robust feature for human tracking and activity recognition based on embedded HMMs">

                                <b>[4]</b> Jalal A,Kamal S,Kim D.Depth silhouettes context:A new robust feature for human tracking and activity recognition based on embedded HMMs[C]//Proc of International Conference on Ubiquitous Robots and Ambient Intelligence,2015:294-299.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human activity recognition using smartphone sensors with two-stage continuous hidden Markov models">

                                <b>[5]</b> Ronao C A,Cho S B.Human activity recognition using smartphone sensors with two-stage continuous hidden Markov models[C]//Proc of International Conference on Natural Computation,2014:681-686.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In-home activity recognition:Bayesian inference for hidden markov models">

                                <b>[6]</b> Ordonez F J,Englebienne G,de Toledo P,et al.In-home activity recognition:Bayesian inference for hidden Markov models[J].IEEE Pervasive Computing,2014,13(3):67-75.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CNN based approach for activity recognition using a wrist-worn accelerometer">

                                <b>[7]</b> Panwar M,Dyuthi S R,Chandra P K,et al.CNN based approach for activity recognition using a wrist-worn accelerometer[C]//Proc of International Conference of the IEEE Engineering in Medicine &amp; Biology Society,2017:2438-2441.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scaling human activity recognition via deep learning-based domain adaptation">

                                <b>[8]</b> Khan M A A H,Roy N,Misra A.Scaling human activity recognition via deep learning-based domain adaptation[C]//Proc of 2018 IEEE International Conference on Pervasive Computing and Communications (PerCom),2018:1-9.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effect of dynamic feature for human activity recognition using smartphone sensors">

                                <b>[9]</b> Nakano K,Chakraborty B.Effect of dynamic feature for human activity recognition using smartphone sensors[C]//Proc of 2017 IEEE 8th International Conference on Awareness Science and Technology (iCAST),2017:539-543.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human activity recognition from accelerometer data using Convolutional Neural Network">

                                <b>[10]</b> Lee S M,Sang M Y,Cho H.Human activity recognition from accelerometer data using convolutional neural network[C]//Proc of IEEE International Conference on Big Data and Smart Computing,2017:131-134.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for human activity recognition using mobile sensors">

                                <b>[11]</b> Zeng M,Le T N,Yu B,et al.Convolutional neural networks for human activity recognition using mobile sensors[C]//Proc of International Conference on Mobile Computing,Applications and Services,2015:197-205.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensembles of deep Istm learners for activity recognition using wearables">

                                <b>[12]</b> Guan Y,Plötz T.Ensembles of deep LSTM learners for activity recognition using wearables[J].Proceedings of the ACM on Interactive,Mobile,Wearable and Ubiquitous Technologies,2017,1(2):Article No.11.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human activity recognition using recurrent neural networks">

                                <b>[13]</b> Singh D,Merdivan E,Psychoula I,et al.Human activity recognition using recurrent neural networks[C]//Proc of International Cross-Domain Conference for Machine Learning and Knowledge Extraction,2017:267-274.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7D5AADFAD3177DC7B754CAB0C800827F&amp;v=MTk2MjZwNU93NlFyR0U5ZWJLY1I3M3BDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOQmh3cm05eEt3PU5pZk9mYlRNRzZDOTIvazBFT2dPQzN0TnZCRmg3VA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Ma C Y,Chen M H,Kira Z,et al.TS-LSTM and temporal-inception:Exploiting spatiotemporal dynamics for activity recognition[J].Signal Processing:Image Communication,2019,71:76-87.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTYzODc9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw4V2JoWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition in video sequences using deep bi-directional LSTM with CNN features">

                                <b>[16]</b> Ullah A,Ahmad J,Muhammad K,et al.Action recognition in video sequences using deep bi-directional LSTM with CNN features[J].IEEE Access,2018,6:1155-1166.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond frame-level CNNSaliency-aware 3D CNN with LSTM for video action recognition">

                                <b>[17]</b> Wang X,Gao L,Song J,et al.Beyond frame-level CNN:Saliency-aware 3-D CNN with LSTM for video action recogni- tion[J].IEEE Signal Processing Letters,2017,24(4):510-514.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909026" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909026&amp;v=MDMxNzVrVWJ2TUx6N0JaYkc0SDlqTXBvOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
