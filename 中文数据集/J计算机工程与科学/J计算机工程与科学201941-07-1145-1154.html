<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132370761748750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201907001%26RESULT%3d1%26SIGN%3drQcG41YZMfuMfNhwtWAnFU%252bDKOE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907001&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907001&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907001&amp;v=MjcxNTdCdEdGckNVUkxPZVplUm1GeTdtVnIzUEx6N0JaYkc0SDlqTXFJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;2 多流编程机制介绍&lt;/b&gt; "><b>2 多流编程机制介绍</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;2.1 资源时间共享&lt;/b&gt;"><b>2.1 资源时间共享</b></a></li>
                                                <li><a href="#43" data-title="&lt;b&gt;2.2 资源空间共享&lt;/b&gt;"><b>2.2 资源空间共享</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;2.3 hStreams&lt;/b&gt;"><b>2.3 hStreams</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;3 相关工作&lt;/b&gt; "><b>3 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;4 性能模型&lt;/b&gt; "><b>4 性能模型</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="&lt;b&gt;5 性能分析&lt;/b&gt; "><b>5 性能分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;5.1 实验配置&lt;/b&gt;"><b>5.1 实验配置</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;5.2 传输性能测试分析&lt;/b&gt;"><b>5.2 传输性能测试分析</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;5.3 计算性能测试分析&lt;/b&gt;"><b>5.3 计算性能测试分析</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;5.4 性能模型的应用&lt;/b&gt;"><b>5.4 性能模型的应用</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#35" data-title="图1 多流编程机制时间共享示意图">图1 多流编程机制时间共享示意图</a></li>
                                                <li><a href="#47" data-title="图2 多流编程机制空间共享示意图">图2 多流编程机制空间共享示意图</a></li>
                                                <li><a href="#51" data-title="图3 hStreams资源管理视图">图3 hStreams资源管理视图</a></li>
                                                <li><a href="#83" data-title="图4 资源单分区下2个和4个流计算与传输的重叠">图4 资源单分区下2个和4个流计算与传输的重叠</a></li>
                                                <li><a href="#111" data-title="图5 不同数据量单向传输时间">图5 不同数据量单向传输时间</a></li>
                                                <li><a href="#113" data-title="图6 不同数据量单向传输带宽">图6 不同数据量单向传输带宽</a></li>
                                                <li><a href="#115" data-title="图8 不同数据规模不同分区DGEMM性能">图8 不同数据规模不同分区DGEMM性能</a></li>
                                                <li><a href="#117" data-title="图7 不同数据量多流双向传输带宽">图7 不同数据量多流双向传输带宽</a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表1 性能模型估算与实测结果&lt;/b&gt;"><b>表1 性能模型估算与实测结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Diaz J, Munoz-Caro C, Nino A.A survey of parallel programming models and tools in the multi and many-core era[J].IEEE Transactions on Parallel &amp;amp; Distributed Systems, 2012, 23 (8) :1369-1386." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey of Parallel Programming Models and Tools in the Multi and Many-Core Era">
                                        <b>[1]</b>
                                         Diaz J, Munoz-Caro C, Nino A.A survey of parallel programming models and tools in the multi and many-core era[J].IEEE Transactions on Parallel &amp;amp; Distributed Systems, 2012, 23 (8) :1369-1386.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Brodtkorb A R, Dyken C, Hagen T R, et al.State-of-the-art in heterogeneous computing[J].Scientific Programming, 2010, 18 (1) :1-33." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD15112600083383&amp;v=MjA0NzJhcks5SDlET3FZOUZaT01NRDNRNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsMFJhQlU9TmlUQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Brodtkorb A R, Dyken C, Hagen T R, et al.State-of-the-art in heterogeneous computing[J].Scientific Programming, 2010, 18 (1) :1-33.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Adriaens J T, Compton K, Kim N S, et al.The case for GPGPU spatial multitasking[C]//Proc of IEEE International Symposium on High Performance Computer Architecture, 2012:1-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The case for GPGPU spatial multitasking">
                                        <b>[3]</b>
                                         Adriaens J T, Compton K, Kim N S, et al.The case for GPGPU spatial multitasking[C]//Proc of IEEE International Symposium on High Performance Computer Architecture, 2012:1-12.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" hStreams architecture document for Intel&lt;sup&gt;&#174;&lt;/sup&gt; MPSS 3.5[Z].Santa Clara:Intel Corporation, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=hStreams architecture document for Intel? MPSS 3.5">
                                        <b>[4]</b>
                                         hStreams architecture document for Intel&lt;sup&gt;&#174;&lt;/sup&gt; MPSS 3.5[Z].Santa Clara:Intel Corporation, 2015.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Munshi A, Gaster B, Mattson T G, et al.OpenCL programming guide[M].Beijing:Science Press, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OpenCL programming guide">
                                        <b>[5]</b>
                                         Munshi A, Gaster B, Mattson T G, et al.OpenCL programming guide[M].Beijing:Science Press, 2012.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Tariq S.An introduction to GPU computing and CUDA architecture[Z].Santa Clara:NVIDIA Corporation, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An introduction to GPU computing and CUDA architecture">
                                        <b>[6]</b>
                                         Tariq S.An introduction to GPU computing and CUDA architecture[Z].Santa Clara:NVIDIA Corporation, 2011.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Hennessy L, Patterson D A.Computer architecture:A quantitative approach[M].4th Edition.San Francisco, CA:Morgan Kaufmann Publishers Inc., 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computer architecture:A quantitative approach">
                                        <b>[7]</b>
                                         Hennessy L, Patterson D A.Computer architecture:A quantitative approach[M].4th Edition.San Francisco, CA:Morgan Kaufmann Publishers Inc., 2006.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Wende F, Steinke T, Cordes F.Multi-threaded kernel offloading to GPGPU using Hyper-Q on Kepler architecture:Tech.Rep.14-19[R].Berlin:Zuse Institute Berlin, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-threaded kernel offloading to GPGPU using Hyper-Q on Kepler architecture:Tech.Rep.14-19">
                                        <b>[8]</b>
                                         Wende F, Steinke T, Cordes F.Multi-threaded kernel offloading to GPGPU using Hyper-Q on Kepler architecture:Tech.Rep.14-19[R].Berlin:Zuse Institute Berlin, 2014.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Wende F, Steinke T, Cordes F.Concurrent kernel execution on Xeon Phi within parallel heterogeneous workloads[C]//Proc of Euro-Par 2014 Parallel Processing, 2014:788-799." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Concurrent kernel execution on Xeon Phi within parallel heterogeneous workloads">
                                        <b>[9]</b>
                                         Wende F, Steinke T, Cordes F.Concurrent kernel execution on Xeon Phi within parallel heterogeneous workloads[C]//Proc of Euro-Par 2014 Parallel Processing, 2014:788-799.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Fumihiko I N O, Nakagawa S, Hagihara K.GPU-Chariot:A programming framework for stream applications running on multi-GPU systems[J].IEEE Transactions on Information &amp;amp; Systems, 2013, 96-D (12) :2604-2616." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPU-Chariot:A programming framework for stream applications running on multi-GPU systems">
                                        <b>[10]</b>
                                         Fumihiko I N O, Nakagawa S, Hagihara K.GPU-Chariot:A programming framework for stream applications running on multi-GPU systems[J].IEEE Transactions on Information &amp;amp; Systems, 2013, 96-D (12) :2604-2616.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" G&#243;mez-Luna J, Gonz&#225;Lez-Linares J M, Benavides J I, et al.Performance models for asynchronous data transfers on consumer graphics processing units[J].Journal of Parallel &amp;amp; Distributed Computing, 2012, 72 (9) :1117-1126." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100263312&amp;v=MjM2MjJNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmwwUmFCVT1OaWZPZmJLN0h0RE9ybzlGWnUwTUQzMDdvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         G&#243;mez-Luna J, Gonz&#225;Lez-Linares J M, Benavides J I, et al.Performance models for asynchronous data transfers on consumer graphics processing units[J].Journal of Parallel &amp;amp; Distributed Computing, 2012, 72 (9) :1117-1126.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" van Werkhoven B, Maassen J, Seinstra F J.Performance models for CPU-GPU data transfers[C]//Proc of International Symposium on Cluster, Cloud and Grid Computing, 2014:11-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance models for CPU-GPU data transfers">
                                        <b>[12]</b>
                                         van Werkhoven B, Maassen J, Seinstra F J.Performance models for CPU-GPU data transfers[C]//Proc of International Symposium on Cluster, Cloud and Grid Computing, 2014:11-20.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Liu Bo-zhong, Qiu Wei-dong, Jiang Lin, et al.Software pipelining for graphic processing unit acceleration[J].International Journal of High Performance Computing Applications, 2016, 30 (2) :169-185." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Software pipelining for graphic processing unit acceleration">
                                        <b>[13]</b>
                                         Liu Bo-zhong, Qiu Wei-dong, Jiang Lin, et al.Software pipelining for graphic processing unit acceleration[J].International Journal of High Performance Computing Applications, 2016, 30 (2) :169-185.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Hetero streams library 1.0 reference codes [Z].Santa Clara:Intel Corporation, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hetero streams library 1.0 reference codes">
                                        <b>[14]</b>
                                         Hetero streams library 1.0 reference codes [Z].Santa Clara:Intel Corporation, 2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(07),1145-1154 DOI:10.3969/j.issn.1007-130X.2019.07.001            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>异构平台多流编程机制的性能模型研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E6%9E%97&amp;code=20960313&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E9%B9%8F&amp;code=20325143&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%96%B9%E5%BB%BA%E6%BB%A8&amp;code=22371209&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">方建滨</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E6%98%A5&amp;code=20653533&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄春</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E6%BB%94&amp;code=20218453&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐滔</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0269230&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国防科技大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>多流编程机制为异构众核加速器提供流水、资源划分等多种资源使用方式, 但如何选择有效使用方式目前缺乏指导。基于异构众核处理器Intel MIC上的hStreams, 提出了针对单应用多流程序多硬件分区执行的性能模型, 分析不同配置下多流程序性能差异的原因, 指出了影响多流程序性能的关键因素, 提出多流程序划分优化策略, 同时所提性能模型能够帮助判断算法实现的效果。实验结果表明, 性能模型与多流配置实际测试结果误差小于1%, 根据性能模型指导调优稠密矩阵乘的多流程序, 比单流程序获得了5.83%的性能提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%B5%81%E7%BC%96%E7%A8%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多流编程;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%81%E6%B0%B4%E7%BA%BF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">流水线;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B5%84%E6%BA%90%E5%88%92%E5%88%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">资源划分;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hStreams&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hStreams;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E6%9E%84%E5%B9%B3%E5%8F%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异构平台;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    方建滨, 通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                                <span>
                                    黄春, 通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                                <span>
                                    唐滔, 通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                                <span>
                                    彭林 (1979-) , 男, 湖南湘乡人, 博士, 助理研究员, CCF会员 (11357M) , 研究方向为高性能计算、编译优化和性能分析。E-mail:penglin@nudt.edu.cn通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                                <span>
                                    张鹏 (1989-) , 男, 河南信阳人, 博士生, 研究方向为并行程序性能预测、编译优化和性能分析。E-mail:zhangpeng13a@nudt.edu.cn通信地址:410073湖南省长沙市国防科技大学计算机学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFB0202004);</span>
                    </p>
            </div>
                    <h1><b>Performance modeling of multi-stream programming mechanism on heterogeneous platforms</b></h1>
                    <h2>
                    <span>PENG Lin</span>
                    <span>ZHANG Peng</span>
                    <span>FANG Jian-bin</span>
                    <span>HUANG Chun</span>
                    <span>TANG Tao</span>
            </h2>
                    <h2>
                    <span>School of Computer, National University of Defense Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Multi-stream programming mechanism can fully provide a variety of resource utilization methods such as pipelining and resource partitioning for heterogeneous many-core accelerators, but there is currently no effective guidance on how to choose effective resource utilization methods. Based on hStreams on heterogeneous many-core processor Intel MIC, we design a performance model for multi-stream program's multi-hardware partitioning execution. Based on our performance model, we can identify the reasons for the performance difference of multi-stream programs under varied configurations, find out key factors that affect the performance, and provide a partitioning optimization strategy for multi-stream programs. In addition, it can also judge the effect of algorithm implementation. Our evaluation results show that the error between the estimated results of the performance model and the actual test results of multi-stream configuration is within 1%. Compared to the single-streamed version, our model also realizes a 5.83% performance improvement when guiding multi-stream programs of the dense matrix multiplication.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-stream%20programming&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-stream programming;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pipelining&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pipelining;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=resource%20partitioning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">resource partitioning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hStreams&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hStreams;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=heterogeneous%20platform&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">heterogeneous platform;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    FANG Jian-bin, Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                                <span>
                                    HUANG Chun, Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                                <span>
                                    TANG Tao, Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                                <span>
                                    PENG Lin, born in 1979, PhD, assistant research fellow, CCF member (11357M) , his research interests include high performance computing, compiler optimization, and performance analysis.Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                                <span>
                                    ZHANG Peng, born in 1989, PhD can-didate, his research interests include performance predic-tion, compiler optimization, and performance analysis.Address:School of Computer, National University of Defense Technology, Changsha 410073, Hunan, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-18</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="31" name="31" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="32">近年来, 为了进一步优化计算结点的功耗和性能, 业界大力发展异构平台。通常而言, 一个异构平台包含宿主机和加速卡2部分。常见的异构平台有CPU+GPU (Graphic Processing Unit) 、CPU+MIC (Intel Many Integrated Core) 和CPU+FPGA (Field Programmable Gate Array) <citation id="135" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。在异构平台上程序员通过编程环境可以将应用的不同部分指派到不同的设备上运行, 比如, 可将分支判断复杂或者强调延迟的应用放置在CPU上执行, 并将访存规则强调吞吐量的应用放置在加速卡上执行, 达到充分发挥不同计算部件性能的目的。异构平台具有高峰值性能和低功耗的优点, 被认为是构建未来E级高性能计算系统的热门候选<citation id="134" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="33">然而, 目前异构平台的使用存在的突出问题:由于很多应用因受限于程序并发度等原因而无法充分利用异构平台丰富的处理器核、访存带宽及数据传输带宽, 导致资源浪费, 如有些应用受限于计算, 而另外一些应用可能受限于访存。当仅运行一个应用时, 可能无法充分发挥系统的整体性能。为此现有的异构编程模型引入了多流 (Multiple Streams) 编程机制, 将来自一个或多个应用的任务 (Task) 提交到不同的任务队列, 这些任务在同一时刻使用不同的资源并以异步流水的形式执行, 从而提高整体系统的资源利用率<citation id="136" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="34">我们观察到异构平台提高总体计算效率有2种主要途径: (1) 隐藏宿主机和加速卡之间的数据传输开销; (2) 通过对程序的各种优化, 提高加速卡和宿主机处理器上任务的计算效率。多流编程机制关注将一个或者多个应用的计算负载划分成不同的任务, 同时将硬件资源划分成不同的分区 (partition) , 并且有机组织任务的执行过程, 来隐藏数据传输开销和提高任务计算效率, 从而达到提高程序整体在异构平台上的执行效率的目的。</p>
                </div>
                <div class="area_img" id="35">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多流编程机制时间共享示意图" src="Detail/GetImg?filename=images/JSJK201907001_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 多流编程机制时间共享示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Time share of multi-stream programming mechanism</p>

                </div>
                <div class="p1">
                    <p id="36">多流编程机制融合了流水线思想和资源划分机制。流水线思想是将流水线分段映射到不同的系统部件上, 使系统资源尽可能处于忙状态, 从而最大化发挥整体性能。此外, 现有的许多应用因为任务规模小而无法利用加速卡丰富的处理器核, 引入资源划分机制可将整个加速卡资源划分为多份, 并将不同的任务映射到不同的分区, 从而实现资源的空间共享, 而且相同的任务在较少的处理器核上执行效率会更高。因此, 多流编程机制是通过资源时间共享和空间共享来提高异构系统资源利用率的。这种多流编程机制已经在OpenCL<citation id="137" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、CUDA<citation id="138" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和hStreams<citation id="139" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等异构编程模型中实现。OpenCL采用命令队列 (Command Queue) 的形式实现多流, 命令队列之间以事件的方式进行同步。CUDA是以CUDA流 (CUDA Stream) 的形式实现多流。hStreams多流是基于COI (Coprocessor Offload Interface) 的流水线实现, 单个流水线内采用OpenMP进行并行化以利用多个处理器核。此外, OpenCL和hStreams支持资源的划分访问, 而CUDA并未将该资源管理接口提供给程序员。</p>
                </div>
                <div class="p1">
                    <p id="37">虽然多流编程机制的提出已有一段时间, 但是缺乏针对单应用多流程序在硬件多分区情况下的性能模型, 对多流程序的性能调优缺乏有力的指导。本文以hStreams为原型, 在Intel MIC平台构建了针对单应用多流机制的性能模型, 通过测试验证了性能模型的适用性, 性能模型给出了多流程序性能优化能够达到的上限, 可以辅助算法优化分析, 同时结合性能模型提出了多流程序划分优化策略。多应用多流性能模型不在本文讨论范围之内, 但可以在单应用多流性能模型基础上进行扩展。</p>
                </div>
                <div class="p1">
                    <p id="38">本文的组织结构如下:第2节介绍多流编程机制的基本原理和hStreams;第3节是相关工作介绍;第4节是多流机制的性能模型介绍;第5节以矩阵乘为例, 分析不同配置下多流程序性能差异的原因, 找出对多流程序性能影响的关键因素, 第6节是论文总结。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>2 多流编程机制介绍</b></h3>
                <h4 class="anchor-tag" id="40" name="40"><b>2.1 资源时间共享</b></h4>
                <div class="p1">
                    <p id="41">多流编程采用流水线将数据传输与计算尽可能地重叠进行, 从而实现资源时间共享。整体而言, 可将面向异构平台应用的任务划分为3类:在宿主机与加速卡间进行数据传输 (XFER) 、访存 (MEM) 和计算 (CAL) , 如图1所示。3类任务分别使用不同的系统资源:数据传输主要利用系统PCIe总线, 访存利用加速卡的存储部件, 计算主要利用加速卡处理器核。</p>
                </div>
                <div class="p1">
                    <p id="42">图1中, 假设系统有3个任务需要执行 (<i>T</i>1、<i>T</i>2和<i>T</i>3) 。若采用单流执行方式, 3个任务串行执行, 数据传输、访存、计算3种操作也是串行执行, 共需9个时间单位。这种单流执行方式在某一时刻仅用到了系统的1类资源, 而其他2类资源处于闲置状态, 导致系统资源不能被充分利用。如果采用多流编程机制, 即启动多个流 (stream) 同时执行 (如图1所示) :当第1个流数据传输结束, 进行访存时, 启动第2个流进行数据传输, 二者使用不同的硬件部件, 可并发执行。由图1可知, 在理想状态下3个任务总执行时间是5个时间单位, 相对于单流执行时9个时间单位的耗时大大缩短了执行时间, 提升了系统性能<citation id="140" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="43" name="43"><b>2.2 资源空间共享</b></h4>
                <div class="p1">
                    <p id="44">人们发现对并发度有限的应用, 减少每个任务使用的硬件资源数量, 能够提高任务的计算效率, 但由此带来的问题是会有更多的硬件资源空闲出来, 多流编程机制将系统资源 (物理线程或物理核) 在逻辑上划分为多个不重叠的分区 (Partition) , 来自一个或者多个应用的计算负载划分得到的多个任务, 可以通过某种方式调度到这些硬件分区上执行, 如每个分区可以分派给一个或多个流使用, 一个或多个任务可以映射到一个流上执行, 从而实现资源空间共享<citation id="141" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 提高硬件资源利用率的同时获得不同任务较高的计算效率。</p>
                </div>
                <div class="p1">
                    <p id="45">以Intel的31SP MIC卡 (众核协处理器) 为例, 每块MIC卡拥有57个处理器核, 其中1个处理器核用于操作系统控制, 其他56个处理器核, 每个包含4个硬件线程, 共可提供224个物理线程。当任务本身并发度较小时, 不能充分发挥处理器的计算能力。例如, 任务最大并发度仅为50时, 最多只能利用50个物理线程, 而剩余近80%的资源将处于闲置状态。多流编程机制通过资源空间共享方法有效解决该问题。如图2所示, 将MIC卡在逻辑上平均划分为4个分区, 即<i>partition</i>0、<i>partition</i>1、<i>partition</i>2、<i>partition</i>3。此时启动4个流<i>stream</i>0、<i>stream</i>1、<i>stream</i>2、<i>stream</i>3, 分别占用不同的分区 (4个流执行相同代码, 但处理不同的数据) , 每个流的可用硬件线程数量等于56。然后将多个任务分别调度到不同流上执行, 每个流仅有约11%的物理线程处于闲置状态, 整个MIC卡的闲置资源降到11%, 提高了硬件利用率, 使得系统资源能够得到充分利用。</p>
                </div>
                <div class="p1">
                    <p id="46">此外, 多流机制支持设备复用, 可以同时执行不同的多流应用<citation id="142" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。如:启动多个多流应用, 每个应用占用不同的分区, 则不同应用之间互不影响, 可并发执行, 提高系统资源的利用效率。</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多流编程机制空间共享示意图" src="Detail/GetImg?filename=images/JSJK201907001_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多流编程机制空间共享示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Space share of multi-stream programming mechanism</p>

                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>2.3 hStreams</b></h4>
                <div class="p1">
                    <p id="49">hStreams是面向Intel MIC的一种多流编程机制实现, 提供了对流的抽象并完成从应用任务到异构平台的映射<citation id="143" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。它基于Intel提供的底层通信库SCIF (Symmetric Communication InterFace) 和COI实现, 并以库的形式提供给用户使用。hStreams为程序员提供2种不同层次的接口:hStreams core APIs和hStreams app APIs。前者更接近于底层, 系统资源控制编程更灵活;后者是对前者的更高层封装, 为用户提供更加简单的编程接口。</p>
                </div>
                <div class="p1">
                    <p id="50">图3所示为hStreams资源管理示意图<citation id="144" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。hStreams将系统结点内一块MIC卡称为物理域 (Physical Domain) , 物理域内所包含资源再划分为不同分区 (Partitions) 。逻辑域 (Logical Domain) 是对物理域的抽象, 用于指定实际应用任务执行所需的物理资源。逻辑域同物理域之间存在多对一的关系, 逻辑域不能跨物理域存在。流实现了一种先进先出FIFO (First Input First Output) 队列, 队列元素是一组执行操作, 操作主要分为计算、数据传输和同步。流绑定在逻辑域, 实际执行时映射到系统物理域的某个分区。流与分区之间映射关系为多对一。如图3所示为2个物理域<i>Physical Domain</i> 0和<i>Physical Domain</i> 1, 其中<i>Physical Domain</i> 0映射到2个逻辑域<i>Logical</i><i>Domain</i> 0和<i>Logical Domain</i> 1, <i>Physical Domain</i> 1映射到单个逻辑域<i>Logical Domain</i> 2。同一物理域中不同逻辑域之间互不影响, 可同时执行应用任务, 使系统资源得到充分利用。<i>Logical Domain</i> 0占用<i>Physical Domain</i> 0的3个分区, <i>Logical Domain</i> 1占用<i>Physical Domain</i> 0的2个分区, <i>Logical Domain</i> 2占用<i>Physical Domain</i> 1的1个分区, 每个分区上映射4个流。每个流上可以按FIFO的方式执行多个任务, 包括双向数据传输和计算等。流上执行的任务通过主机端异步加载, 提供必要的同步函数, 以保持计算之间的依赖关系, 映射在同一分区中的不同流之间按照流水方式执行。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 hStreams资源管理视图" src="Detail/GetImg?filename=images/JSJK201907001_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 hStreams资源管理视图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 hStreams resource management view</p>

                </div>
                <div class="p1">
                    <p id="52">hStreams编程包含Source端 (即主机端, Host端) 编程和Sink端 (即加速卡端, Device端) 编程2部分。Source端编程主要分为5步:上下文环境初始化、分配存储空间、将数据由Source端传输至Sink端、启动Sink端kernel函数进行计算、将计算结果由Sink端传输至Source端。Sink端主要负责执行Sink端函数, 进行数据计算。环境初始化通过控制参数<i>partitions</i>与<i>log</i>_<i>streams</i>_<i>per</i>_<i>partition</i>来配置上下文环境, 其中, <i>partitions</i>代表资源分区数量, <i>log</i>_<i>stream</i>_<i>per</i>_<i>partition</i>代表映射到每个分区的逻辑流数量。</p>
                </div>
                <div class="p1">
                    <p id="53">下面给出了一段hStream代码的例子, 描述了在多个流中任务的分配, 包括主机端向加速卡端传送数据, 启动加速卡端计算, 将数据传回主机端的过程。在初始化资源的分区数量和每个分区映射的流数量后, 异步地在每个流中增加任务, 包括主机端向加速卡端传送数据和启动加速卡端任务的操作, 而hStream提供机制维持提交的操作的队列, 提交到同一个流的任务按加载的顺序依次执行, 映射到不同的硬件分区上的各个流并发执行任务。<i>MAX</i>_<i>STR</i>表示最大的流的数量。为了保持任务之间的依赖关系, hStream提供同步机制, 如对指定事件、指定流或者全局的同步, 同步机制可以保证同步点之前的任务完成后, 才开始继续执行后续代码。</p>
                </div>
                <div class="area_img" id="168">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201907001_16800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="168">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201907001_16801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>3 相关工作</b></h3>
                <div class="p1">
                    <p id="78">传统的流水线技术是Intel在486芯片中引入的, 在硬件中将一条指令的执行分成多个阶段, 每个阶段专门完成一部分工作, 多条指令可以流水地在硬件上执行, 达到提高硬件利用率的目的<citation id="145" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。异构计算采用类似的流水线技术, 将数据传输与数值计算过程尽可能重叠进行, 从而提高不同部件的利用率。现在典型的多流实现包括面向CPU+MIC的hStreams<citation id="146" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 面向CPU+NVIDIA GPU的CUDA Streams<citation id="147" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和OpenCL中的Command Queue<citation id="148" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。本文主要以hStreams为原型系统量化采用多流的性能效果, 并建立了性能模型。</p>
                </div>
                <div class="p1">
                    <p id="79">Adriaens等人<citation id="149" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了称为空间多任务的GPU多任务技术, 并用实验表明空间多任务能够比协作多任务获得更高性能。Wende等人<citation id="150" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>研究了在Kepler GPU上同时执行多个小kernel的并发多kernel执行机制, 同时也评估了在Intel MIC上卸载模型下多线程程序和多个host程序并发卸载任务到MIC上的程序性能<citation id="151" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。多任务和多流都包含资源空间共享的思想, 不同在于多流是将同一个应用的工作分成多个任务, 而多任务则是针对多个程序而言。Fumihiko等人<citation id="152" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了一种面向多GPU系统的流编程框架, 该框架通过乱序执行提升流水性能。本文主要同时从资源的空间共享和时间共享角度对异构多流模型进行研究。</p>
                </div>
                <div class="p1">
                    <p id="80">Gómez-Luna等人<citation id="153" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>针对不同类型GPU的异步数据传输构建了性能模型, 根据性能模型对GPU任务的最佳划分数目进行估计。Werkhoven 等人<citation id="154" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出GPU上利用性能分析模型指导的计算和传输重叠技术。Liu等人<citation id="155" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过构建性能模型, 系统研究了通过数据划分、数据调度和数据粒度的合理选择, 最大化发挥GPU系统性能的方法。在异构计算性能模型的已有研究<citation id="156" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>中, 主要针对计算主导和传输主导2种类型的异构程序进行建模, 这2种类型的异构程序在不同执行模式下, 如数据双向传输的并发性 (数据传输在host端和device端双向并发还是单向进行) 和数据回传模式 (数据回传host端是等待所有任务完成还是尽早开始) 等情况, 讨论了不同任务划分下能够得到最好性能的任务划分方式。由于GPU并未提供对硬件资源划分的接口, 所以相关研究并没有解决多流在资源划分情况下的性能模型。hStreams具有资源划分的能力, 我们需要讨论多流程序在硬件资源可以划分成多个分区的情况下的异构计算性能模型。本文只对单个kernel函数在单个加速卡上计算的情况进行讨论, 但相关结论可以推广到多个kernel函数和多个加速卡。</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>4 性能模型</b></h3>
                <div class="p1">
                    <p id="82">GPU平台上的已有研究主要是针对加速卡硬件资源单分区 (整个加速卡的资源作为一个分区使用) 的情况<citation id="157" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。图4给出了在资源单分区情况下计算主导和传输主导2类应用, 从图4中可以看到, 通过组织多流多任务, 计算和传输的时间可以重叠, 因此以流水线方式执行相比单流执行能够获得性能提升。对于可以进行硬件资源划分的加速卡来说, 图4只描绘了资源单分区的情况, 对于资源多分区情况, 多个分区不能同时加载作业, 各个资源分区必须依次逐步加载作业, 存在资源分区流水式填满和排空的阶段, 因此需要建立新的性能模型。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 资源单分区下2个和4个流计算与传输的重叠" src="Detail/GetImg?filename=images/JSJK201907001_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 资源单分区下2个和4个流计算与传输的重叠  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Overlap between computation and  communication of 2 and 4 streams with 1 partition</p>

                </div>
                <div class="p1">
                    <p id="84">加入了硬件资源分区后, 多流程序的执行模式变得复杂。根据硬件资源分区和多流程序的使用方法, 我们认为多流程序有3种执行模式: (1) 单流单分区。对数据和硬件资源不进行划分, 依次进行输入数据传输、计算和结果数据回传。 (2) 多流单分区。对数据进行划分, 但对硬件资源不进行划分, 以流水的方式进行各任务的数据传输和计算, 每个任务使用整个加速卡进行计算。 (3) 多流多分区。对数据和硬件资源进行划分, 以流水的方式进行各任务的数据传输和计算, 每个任务使用一个硬件分区进行计算, 受限于输入数据传输的串行性, 各个硬件分区的使用存在填满和排空的过程。需要指出的是, 本文建立的性能模型同时也描述了异构多流程序能够达到的最好性能。</p>
                </div>
                <div class="p1">
                    <p id="85">我们认为整个异构程序执行时间<i>time</i>包含4类:<i>t</i><sub>h</sub>表示主机到加速卡h2d (host to device) 数据传输时间;<i>t</i><sub>d</sub>表示加速卡到主机d2h (device to host) 数据传输时间;<i>t</i><sub>e</sub>表示kernel函数在加速卡上的执行时间;<i>t</i><sub>p</sub>表示多硬件分区流水填满排空过程中整个加速卡的空转时间。</p>
                </div>
                <div class="p1">
                    <p id="86">式 (1) 和式 (2) 分别给出了<i>t</i><sub>h</sub>和<i>t</i><sub>d</sub>的计算方法, 这2个数据传输时间都等于每次传输的数据量除以对应带宽, 再加上开销的累加。</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow><mrow><mi>B</mi><msub><mrow></mrow><mtext>h</mtext></msub></mrow></mfrac><mo>+</mo><mi>a</mi><mo>, </mo><mtext>单</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>t</mi></mstyle><msubsup><mrow></mrow><mtext>h</mtext><mi>i</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mfrac><mrow><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow><mi>i</mi></msubsup></mrow><mrow><mi>B</mi><msubsup><mrow></mrow><mtext>h</mtext><mi>i</mi></msubsup></mrow></mfrac><mo>+</mo><mi>a</mi><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext><mtext>或</mtext><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow><mrow><mi>B</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow></mfrac><mo>+</mo><mi>b</mi><mo>, </mo><mtext>单</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>t</mi></mstyle><msubsup><mrow></mrow><mtext>d</mtext><mi>i</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mfrac><mrow><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow><mi>i</mi></msubsup></mrow><mrow><mi>B</mi><msubsup><mrow></mrow><mtext>d</mtext><mi>i</mi></msubsup></mrow></mfrac><mo>+</mo><mi>b</mi><mo stretchy="false">) </mo><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext><mtext>或</mtext><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中, <i>D</i><sub>in</sub>和<i>D</i><sub>out</sub>分别表示输入数据和输出数据, 多数情况下输入数据与输出数据存在对应关系, 为方便讨论, 假设输入输出数据的划分数量均为<i>n</i>。<i>B</i><sub>h</sub>和<i>B</i><sub>d</sub>分别表示从h2d和d2h的PCIe传输带宽, <i>a</i>表示每次h2d传输开销, <i>b</i>表示每次d2h传输开销。</p>
                </div>
                <div class="p1">
                    <p id="89"><i>m</i>表示数据划分后对应的任务数量, 输入输出数据的划分数量<i>n</i>与计算任务数量<i>m</i>存在对应关系, 一般情况下是相等。单流单分区和多流单分区时, 每个任务会使用整个加速卡来进行计算, 而多流多分区每个任务使用一个资源分区, 需要多个任务并发执行将整个加速卡利用起来。</p>
                </div>
                <div class="p1">
                    <p id="90">为了简化程序进行任务负载均衡过程, 通常对硬件资源进行均分, 同时保持每个硬件分区上任务的数量相同。为此需要满足下面的约束:<i>p</i>表示对MIC卡硬件资源进行平均划分后的分区数量, MIC卡最小划分粒度为每个分区2个硬件线程, 224个硬件线程最多被分成112个分区, 因此有1≤<i>p</i>≤112。<i>str</i>表示流的数量, 为负载均衡, 需满足<i>str</i>|<i>m</i>且<i>p</i>|<i>str</i>, <i>m</i>能被<i>str</i>整除, 每个流上任务的数量相同, <i>str</i>能被<i>p</i>整除, 每个分区上流的数量相同。</p>
                </div>
                <div class="p1">
                    <p id="91"><i>Perf</i>表示kernel函数使用整个加速卡的计算性能。<i>Cmpt</i>表示对于输入<i>D</i><sub>in</sub>, kernel函数的计算量, <i>c</i>表示每次调用kernel函数的开销。3种执行模式下, kernel函数在加速卡上的计算时间如式 (3) 所示。对于单流单分区, 只需要调用1次kernel函数。对于多流单分区, 任务分成<i>m</i>份, 需要调用<i>m</i>次kernel函数。对于多流多分区, 任务分成<i>m</i>份, 在<i>p</i>个分区的加速卡上并发执行, 每次能够执行<i>p</i>个任务, 共需要调用<i>m</i>/<i>p</i>次kernel函数。</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi></mrow></mfrac><mo>+</mo><mi>c</mi><mo>, </mo><mtext>单</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>t</mi></mstyle><msubsup><mrow></mrow><mtext>e</mtext><mi>i</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false"> (</mo></mstyle><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></mfrac><mo>+</mo><mi>c</mi><mo stretchy="false">) </mo><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>/</mo><mi>p</mi></mrow></munderover><mi>t</mi></mstyle><msubsup><mrow></mrow><mtext>e</mtext><mi>i</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>/</mo><mi>p</mi></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></mfrac><mo>+</mo><mi>c</mi><mo stretchy="false">) </mo><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">对于kernel函数的多流多分区执行模式, 根据输入数据与任务之间的依赖关系, 每个任务所依赖的数据传输完成后, 立刻开始执行这个任务。从第一次传输结束后, 第1个硬件分区启动任务的执行, 之后每完成传输一次必要的数据, 有新的硬件分区启动任务执行, 直到所有分区都填满。相应地, 也存在对称的硬件分区排空过程, 到最后1个分区完成计算为止。我们把多硬件分区流水填满排空过程中整个加速卡的空转时间<i>t</i><sub>p</sub>定义为从第1个硬件分区开始计算到硬件分区填满为止之间的h2d输入数据传输时间。计算方法如式 (4) 所示。</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>k</mi></munderover><mi>t</mi></mstyle><msubsup><mrow></mrow><mtext>h</mtext><mi>i</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>2</mn></mrow><mi>k</mi></munderover><mo stretchy="false"> (</mo></mstyle><mfrac><mrow><mi>D</mi><msubsup><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow><mi>i</mi></msubsup></mrow><mrow><mi>B</mi><msubsup><mrow></mrow><mtext>h</mtext><mi>i</mi></msubsup></mrow></mfrac><mo>+</mo><mi>a</mi><mo stretchy="false">) </mo><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中, <i>k</i>表示把流水线填满需要进行的数据传输次数。</p>
                </div>
                <div class="p1">
                    <p id="96">前面讨论了异构程序不同类型资源使用方式下4类时间开销的计算方法, 而异构程序总的执行时间, 不仅与硬件资源使用方式相关, 也与程序本身特点相关。从图4中可以看出, 异构程序主要分成计算主导和传输主导2种类型, 下面分这2种情况进行讨论。</p>
                </div>
                <div class="p1">
                    <p id="97">对于计算主导类型的程序, 计算时间比传输时间长, 流水执行时计算时间是每个流水阶段的上限, 从计算时间的角度考虑总的执行时间是合理的, 程序总的执行时间如式 (5) 所示。在单流单分区模式下, 程序总的执行时间是<i>t</i><sub>h</sub>、 <i>t</i><sub>d</sub>和<i>t</i><sub>e</sub> 3部分时间的总和。在多流单分区模式下, 执行除启动第1个任务的输入和最后1个任务结束的输出传输时间外, 其他传输时间能够与计算时间重叠, 从而被隐藏, 总的执行时间如式 (5) 第2项多流单分区所示, 在多流多分区模式下, 执行模式与多流单分区的执行模式相比, 要增加硬件分区流水中的空转时间<i>t</i><sub>p</sub>。</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>, </mo><mtext>单</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub></mrow><mi>n</mi></mfrac><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow><mi>n</mi></mfrac><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub></mrow><mi>n</mi></mfrac><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow><mi>n</mi></mfrac><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">对于传输主导类型的异构程序, 传输时间比计算时间长, 流水执行时传输时间是每个流水阶段的上限, 从传输时间的角度考虑总的执行时间是合理的, 程序总的执行时间如式 (6) 所示。对于单流单分区的执行模式, 传输主导程序总的执行时间的计算方式与计算主导程序的相同。在多流单分区和多流多分区模式下根据h2d和d2h数据传输时间的大小, 以传输时间长的为主导, 则计算另一个方向的传输时间被隐藏。在多流多分区模式下引入的流水线填满和排空的开销, 如果传输时间仍然处于主导地位, 则通过式 (6) 第3项来计算, 但是如果计算时间<i>t</i><sub>e</sub>和空转时间<i>t</i><sub>p</sub>之和超过了传输时间, 那么需要按式 (5) 来计算。</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>, </mo><mtext>单</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub></mrow><mi>m</mi></mfrac><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow><mi>n</mi></mfrac><mo>, </mo><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>≥</mo><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext><mtext>或</mtext><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub></mrow><mi>n</mi></mfrac><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub></mrow><mi>m</mi></mfrac><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>≤</mo><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext><mtext>或</mtext><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">从式 (1) ～式 (6) 可以看出, 与单流单分区模式执行的程序相比, 多流程序可以通过流水执行隐藏双向传输和计算时间中2项的大部分时间, 但是随着数据划分和任务数量增加, 相应地会增加通信次数和kernel函数调用次数, 导致通信开销和调用开销增加, 而在多分区情况下还会产生硬件分区的流水线填充和排空开销。相关性能模型<citation id="159" type="reference"><link href="19" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">12</a>]</sup></citation>中假设传输速度和加速卡对不同规模输入的计算速度都不变, 但实际上这2个关键性能是随着数据规模变化而变化的, 我们在后面将结合测试进一步讨论。根据文献<citation id="158" type="reference">[<a class="sup">9</a>]</citation>的讨论, 等分任务大多数情况下能够获得更好的性能, 因此后续讨论中将对任务划分采取等分策略, 相应地硬件资源也采取等分方式划分。对任务进行等分后, 每个任务的kernel函数计算时间都对应相等。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag"><b>5 性能分析</b></h3>
                <div class="p1">
                    <p id="103">本节将结合矩阵乘在异构平台的多流版本进行性能分析。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>5.1 实验配置</b></h4>
                <div class="p1">
                    <p id="105">本文用到的实验平台是由CPU和MIC组成的异构平台。主机端2路Intel Xeon CPU, 每个CPU拥有12个处理器核, CPU频率为2.3 GHz, 三级缓存为30 MB, 主机端主存为128 GB。加速卡端为一块MIC卡 (Intel Xeon Phi 31SP) , MIC卡拥有57个处理器核, 其中1个处理器核为MIC上操作系统使用, hStreams可以使用56个处理器核, 每个处理器核包含4个硬件线程, 共计224个硬件线程, 核心频率为1.05 GHz, 内存使用GDDR5, 大小为5 952 MB。</p>
                </div>
                <div class="p1">
                    <p id="106">本文实验用到Intel的软件栈MPSS v3.5.2 (含hStreams v3.5.2) , CPU端的操作系统为Red Hat Enterprise Linux x86 64-bit 6.5 (内核版本为2.6.32-431) , 所用的编译器是Intel Composer XE v15.0.0。</p>
                </div>
                <div class="p1">
                    <p id="107">双精度矩阵乘的测试用例取自hStreams自带多流版本<citation id="160" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 为进行测试, 进行了相应的优化修改。定义<b><i>A</i></b>为<i>i</i>×<i>j</i>矩阵, <b><i>B</i></b>为<i>j</i>×<i>k</i>矩阵, <b><i>C</i></b>为矩阵<b><i>A</i></b>与<b><i>B</i></b>的乘积矩阵, 即<b><i>C</i></b>=<b><i>A</i></b>×<b><i>B</i></b>, 为<i>i</i>×<i>k</i>矩阵。首先将矩阵<b><i>A</i></b>和<b><i>B</i></b>传输到加速卡, 然后启动任务计算, 计算完成后从加速卡将矩阵<b><i>C</i></b>传回主机端, 计算过程共有 (2<i>i</i>-1) ×<i>j</i>×<i>k</i>个浮点操作。为了方便讨论, 我们均采用方阵, 即<i>i</i>=<i>j</i>=<i>k</i>。矩阵乘算法采用等分分块方式进行, 每个<b><i>A</i></b>子矩阵和<b><i>B</i></b>子矩阵的相乘作为一个任务, <b><i>A</i></b>、<b><i>B</i></b>中2个子矩阵的传输和计算通过多流的方式来组织, 这些子矩阵的数据传输和任务会轮转分配到每个流的工作队列中, 直到所有子矩阵传输和任务的提交完毕。主机端通过事件等待方式等待加速卡的反馈, <b><i>A</i></b>、<b><i>B</i></b>子矩阵对应<b><i>C</i></b>的子矩阵计算完成时, 加速卡启动往主机端回传的任务, 直到所有<b><i>C</i></b>子矩阵结果传输结束。令<i>blocks</i>表示每个子矩阵规模, 则矩阵<b><i>A</i></b>共分为 (<i>i</i>/<i>blocks</i>) × (<i>j</i>/<i>blocks</i>) 块, 矩阵<b><i>B</i></b>分为 (<i>j</i>/<i>blocks</i>) × (<i>k</i>/<i>blocks</i>) 块, 矩阵<b><i>C</i></b>分为 (<i>i</i>/<i>blocks</i>) × (<i>k</i>/<i>blocks</i>) 块, 子任务的数量为 (<i>i</i>/<i>blocks</i>) × (<i>j</i>/<i>blocks</i>) × (<i>k</i>/<i>blocks</i>) 个。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>5.2 传输性能测试分析</b></h4>
                <div class="p1">
                    <p id="109">根据式 (1) 和式 (2) 进行传输的开销和带宽测试, 单向带宽测试只有1个方向的数据传输, 双向带宽测试在主机端和加速卡端2个方向同时传输数据, 连续进行1 000次传输取平均值。通过发送单个字节来测试h2d开销<i>a</i>和d2h开销<i>b</i>, 测试结果为:h2d传输开销<i>a</i>=16.19 μs, d2h传输开销<i>b</i>=14.41 μs。我们注意到这样的延迟测试存在不稳定性, 会出现波动。</p>
                </div>
                <div class="p1">
                    <p id="110">100次1 KB～64 MB数据传输时间的平均值如图5所示。从图5中可以看到, 32 KB以下规模数据的传输时间基本相同, 我们认为除开数据传输的基本开销, PCIe总线需要传输一定的数据量后才能够发挥其带宽。这也意味着如果每次传输的数据小于32 KB, 传输的时间将保持不变, 进一步划分任务, 减少每次传输数据量已经不能获得收益, 因此任务过度细分对性能是有损害的。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同数据量单向传输时间" src="Detail/GetImg?filename=images/JSJK201907001_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同数据量单向传输时间  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Transfer time of data of different  sizes in single direction mode</p>

                </div>
                <div class="p1">
                    <p id="112">图6给出了减去h2d开销<i>a</i>和d2h开销<i>b</i>后计算得到的带宽, 注意到128 KB～1 MB的数据传输速度处于较高水平, 而2 MB～16 MB的数据传输速度出现下降, 32 MB～1 GB的数据传输速度又恢复到较高水平。我们尚不清楚其原因, 但即便如此, 由于其带宽比较高, 进行任务划分仍然能够获得收益。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同数据量单向传输带宽" src="Detail/GetImg?filename=images/JSJK201907001_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同数据量单向传输带宽  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Bandwidth of data of different  sizes in single direction mode</p>

                </div>
                <div class="p1">
                    <p id="114">图7是多流情况下的双向带宽测试结果, 图中分别同时启动2～112个流, 奇数流进行h2d传输, 偶数流进行d2h传输, 每个流传输数据量从32 KB～32 MB, 用总数据量除以完成时间, 即可得到这些流总的传输带宽。受限于MIC的内存容量, 我们没有测试更大并发数的数据传输。从图7中可以看到, 随着数据量的增大, 双向传输总带宽逐步增大, 最多可以达到13.74 GB/s, 是单向传输带宽的2倍。与图6中的单向传输带宽相比, 128 KB～512 KB并发双向传输会导致总的带宽明显下降, 这可能与PCIe传输特性有关。数据量比较小, PCIe带宽还未用满时, 流越多总带宽越低, 这是由于多个流管理开销导致的。当每个流传输数据量超过1 MB时, 总带宽较为稳定。显然流越多, 每个流获得的平均带宽越低。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同数据规模不同分区DGEMM性能" src="Detail/GetImg?filename=images/JSJK201907001_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同数据规模不同分区DGEMM性能  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_115.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 DGEMM performance under different data sizes and different partitions</p>

                </div>
                <div class="p1">
                    <p id="116">由此可以看到传输速度会随着传输量和传输模式变化而变化, 我们需要根据测试获得的数据, 为不同数据传输大小和不同传输模式选择实际带宽, 进行传输时间计算。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907001_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同数据量多流双向传输带宽" src="Detail/GetImg?filename=images/JSJK201907001_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同数据量多流双向传输带宽  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907001_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Bandwidth of data of different  sizes in bi-direction mode</p>

                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>5.3 计算性能测试分析</b></h4>
                <div class="p1">
                    <p id="119">为了测试式 (3) 中kernel函数加载开销<i>c</i>, 我们连续加载执行1 000次空的kernel函数 (通过查看汇编代码确认) , 计算出每个kernel函数的开销<i>c</i>=53.91 μs。</p>
                </div>
                <div class="p1">
                    <p id="120">为了测试式 (3) 中<i>Perf</i>, 我们在每个分区上加载执行1个矩阵乘任务。整个加速卡作为1个分区时, 整个卡运行1个矩阵乘, 加速卡分为2个分区时, 运行2个矩阵乘, 依此类推, 保证有足够的任务使加速卡的全部分区都被使用。</p>
                </div>
                <div class="p1">
                    <p id="121">图8给出了1个分区、2个分区和4个分区 (即<i>p</i>分别取值1、2和4) 时的<i>Perf</i>值, 矩阵规模是指每个任务中子矩阵的大小<i>blocks</i>, 测试结果没有统计传输时间。从图8中可以看到, 矩阵乘性能受子矩阵规模影响很大, 在不同规模下性能表现出明显的起伏, 子任务数为7 500时尤为明显, 而且随着子矩阵规模减小, 总体性能不断下降。若要发挥硬件资源的性能, 矩阵必须具备一定的规模, 一般规模越大, 性能越高。而性能的起伏波动与矩阵乘在MIC上的实现有关, 在Intel Xeon处理器平台上也存在类似性能波动现象, 只是具体数据规模不同。</p>
                </div>
                <div class="p1">
                    <p id="122">总的来说, 多分区情况下的计算性能优于较少分区的情况, 原因是每个分区执行相同的任务, 4个分区并发执行的任务量是2个分区的2倍, 是1个分区的4倍, 因此能够获得更高的性能。但是, 也有个别例外情况, 比如矩阵规模为1 600时, 2个分区时的计算性能就比1个分区的计算性能低。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>5.4 性能模型的应用</b></h4>
                <div class="p1">
                    <p id="124">异构多流矩阵乘通过算法优化, <b><i>A</i></b>、<b><i>B</i></b> 2个矩阵的第1个子矩阵传输完成后, 就可以开始计算, 后续每传输<b><i>A</i></b>或者<b><i>B</i></b>的1个子矩阵块, 就可以开始1个或者多个计算。当<b><i>C</i></b>的子矩阵计算完成后, 通过事件等待方式启动数据回传, 当所有计算完成后, 最后1个<b><i>C</i></b>子矩阵回传。<b><i>A</i></b>、<b><i>B</i></b>和<b><i>C</i></b>的子矩阵大小都相等, 整个程序输入的数据是输出数据的2倍, 而双向传输速率也相等。相对于程序执行时间<i>time</i>, 传输开销和kernel函数调用开销都可以忽略。因此, 我们根据式 (5) 可以推出式 (7) 。</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo>=</mo><mfrac><mi>D</mi><mi>B</mi></mfrac><mo>×</mo><mn>3</mn><mo>+</mo><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi></mrow></mfrac><mo>, </mo><mtext>单</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub></mrow><mi>n</mi></mfrac><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow><mi>n</mi></mfrac><mo>=</mo><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mn>1</mn></msup></mrow><mi>B</mi></mfrac><mo>×</mo><mn>2</mn><mo>+</mo><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mi>n</mi></msup></mrow><mi>B</mi></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mi>B</mi></mfrac><mo>×</mo><mn>3</mn><mo>+</mo><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi></mrow></mfrac><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>单</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr><mtr><mtd><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>h</mtext></msub></mrow><mi>n</mi></mfrac><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>e</mtext></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mtext>p</mtext></msub><mo>+</mo><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow><mi>n</mi></mfrac><mo>=</mo><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mn>1</mn></msup></mrow><mi>B</mi></mfrac><mo>×</mo><mn>2</mn><mo>+</mo><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi></mrow></mfrac><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mi>B</mi></mfrac><mo>×</mo><mo stretchy="false"> (</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>+</mo><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mi>n</mi></msup></mrow><mi>B</mi></mfrac><mo>=</mo><mfrac><mrow><mi>D</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mi>B</mi></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mo stretchy="false"> (</mo><mi>p</mi><mo>+</mo><mn>2</mn><mo stretchy="false">) </mo><mo>+</mo><mfrac><mrow><mi>C</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>Ρ</mi><mi>e</mi><mi>r</mi><mi>f</mi></mrow></mfrac><mo>, </mo><mtext>多</mtext><mtext>流</mtext><mtext>多</mtext><mtext>分</mtext><mtext>区</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="126">其中, <i>D</i>表示<b><i>A</i></b>、<b><i>B</i></b>和<b><i>C</i></b>中一个矩阵的数据量, <i>D</i><sup><i>i</i></sup>表示第<i>i</i>个子矩阵的数据量, 由于是等分, 每个子矩阵的数据量都相等。由于每个子矩阵数据量相同, 因此双向传输带宽相同, 均为<i>B</i>, 而每个任务的计算量也是相同的, 为<i>Cmpt</i>。</p>
                </div>
                <div class="p1">
                    <p id="127">本文通过矩阵乘的异构多流程序测试来验证模型的有效性。hStreams提供了一个参考版本的异构多流矩阵乘实现, 但是测试发现其性能比利用性能模型计算出的最好性能要差。通过分析发现, 参考版本依据<b><i>C</i></b>子矩阵的索引在多个流上分配任务, 而参考实现对<b><i>C</i></b>子矩阵的访问并不是顺序的, 因此在参考实现中会出现将连续的2个子任务分配在1个流上的情况, 导致后1个任务的计算不能尽早开始, 增加了加速卡空转时间。本文修改了参考实现, 不依据<b><i>C</i></b>子矩阵索引来分配子任务, 而是按任务到达顺序将其依次分配到不同流上, 提高了加速卡的利用率, 从而提高了性能, 达到性能模型描述的最佳性能。</p>
                </div>
                <div class="p1">
                    <p id="128">输入矩阵<b><i>A</i></b>、<b><i>B</i></b>和<b><i>C</i></b>为16 800的方阵, <i>blocks</i>分别取值16 800, 8 400, 4 200和2 100, 分区数<i>p</i>分别取值1, 2和4。根据<i>blocks</i>大小可以得到每个子矩阵的数据量, 通过查询之前测试得到的不同数据量的带宽<i>B</i>, 可以计算传输时间。根据<i>blocks</i>大小和分区数<i>p</i>, 可以查询之前不同子矩阵规模和分区的测试结果得到计算性能<i>Perf</i>, <i>blocks</i>大小确定了计算量<i>Cmpt</i>, 计算量除以计算性能得到计算时间。将以上的值代入式 (7) , 我们可以估算不同方式执行的异构程序的总执行时间<i>time</i>, <i>time</i>与实际测试结果相比较如表1所示。从表1中可以看到, 性能模型得到的结果与实测结果误差不超过1%, 说明了性能模型的有效性。</p>
                </div>
                <div class="p1">
                    <p id="129">从表1中结果中可以看出, 对于异构多流双精度矩阵乘, 相比单流单分区的执行模式, 多流程序增加数据划分和分区数量, 对性能会产生明显的影响。多流程序在单分区情况下, 随着数据划分增加, 能够隐藏更多传输时间, 但是每个任务的计算量下降会带来计算性能下降, 导致执行时间增加。当每个任务的计算量过小, 却使用整个加速卡的硬件资源, 计算性能会变得很低, 此时任务执行时间增加量会抵消传输时间隐藏带来的收益, 导致整个异构程序执行时间反而上升。此时, 可以考虑增加分区数, 每个任务使用的硬件资源数量变少, 计算效率能够提高, 同时随着分区数量增加, 加速卡上并发执行的任务数量也会增加, 2个方面的变化提高了加速卡的整体计算性能。但是, 随着任务的不断变小, 计算量也会变小, 在分区数需要维持在一定数量的情况下, 每个任务计算性能持续下降, 将成为整体性能下降的主要原因, 传输时间的隐藏收益会越来越小。在表1的10组测试中, 相对于单流单分区执行 (12.76 s) , 多流4分区获得了更好的总体性能 (12.015 s) , 性能提高了5.83%。从中我们也可以看到多流程序性能调优的复杂性, 需要找到一个合适的配置需要考虑应用特点和平台特性。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表1 性能模型估算与实测结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Total time evaluated by the performance model and measured results</b></p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td>执行<br />时间</td><td><i>blocks</i></td><td><i>B</i> (GB/s) </td><td><i>D</i><sup><i>i</i></sup>/<br />B (s) </td><td><i>Perf</i><br /> (GFLOPs/s) </td><td><i>Cmpt</i>/<br />Perf (s) </td><td>估计<br />时间/s</td><td>实测<br />时间/s</td><td>误差<br />/%</td></tr><tr><td>单流单分区</td><td>16 800</td><td>6.731</td><td>0.335</td><td>805.562</td><td>11.794</td><td>12.799</td><td>12.760</td><td>0.303</td></tr><tr><td><br /></td><td>8 400</td><td>6.667</td><td>0.085</td><td>791.807</td><td>11.958</td><td>12.212</td><td>12.206</td><td>0.051</td></tr><tr><td><br />多流单分区</td><td>4 200</td><td>6.587</td><td>0.021</td><td>776.613</td><td>12.211</td><td>12.275</td><td>12.279</td><td>-0.030</td></tr><tr><td><br /></td><td>2 100</td><td>6.335</td><td>0.006</td><td>711.702</td><td>13.325</td><td>13.341</td><td>13.342</td><td>-0.004</td></tr><tr><td><br /></td><td>8 400</td><td>6.667</td><td>0.085</td><td>797.038</td><td>11.867</td><td>12.205</td><td>12.201</td><td>0.033</td></tr><tr><td><br />多流2分区</td><td>4 200</td><td>6.587</td><td>0.021</td><td>784.222</td><td>12.093</td><td>12.178</td><td>12.150</td><td>0.233</td></tr><tr><td><br /></td><td>2 100</td><td>6.335</td><td>0.006</td><td>716.849</td><td>13.229</td><td>13.251</td><td>13.223</td><td>0.215</td></tr><tr><td><br /></td><td>8 400</td><td>6.667</td><td>0.085</td><td>809.895</td><td>11.711</td><td>12.219</td><td>12.181</td><td>0.310</td></tr><tr><td><br />多流4分区</td><td>4 200</td><td>6.587</td><td>0.021</td><td>798.362</td><td>11.878</td><td>12.007</td><td>12.015</td><td>-0.067</td></tr><tr><td><br /></td><td>2 100</td><td>6.335</td><td>0.006</td><td>742.618</td><td>12.770</td><td>12.803</td><td>12.855</td><td>-0.401</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="132" name="132" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="133">本文研究了Intel MIC平台上的hStreams异构多流编程模型, 提出了针对多流程序多硬件分区执行的性能模型, 采用性能模型与测试相结合的方法验证了模型的有效性。式 (1) ～式 (6) 给出了概念层面异构多流性能模型, 但是, 通过实际测试可以看到, 异构多流程序的性能受硬件特性、程序算法特点和多流组织方式等多个因素影响, 更准确的性能模型需要结合具体程序和实现来细化, 如式 (7) 所示, 需要找出多流程序在不同情况下影响性能的关键因素, 并进行针对性优化。因此, 本文提出的性能模型能够辅助分析多流程序, 找出多流异构矩阵乘算法实现上存在的问题, 通过改进获得更好的性能。下一步将利用性能模型针对不同类型的程序展开异构多流性能优化研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="166" type="formula" href="images/JSJK201907001_16600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">黄春</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="167" type="formula" href="images/JSJK201907001_16700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">唐滔</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey of Parallel Programming Models and Tools in the Multi and Many-Core Era">

                                <b>[1]</b> Diaz J, Munoz-Caro C, Nino A.A survey of parallel programming models and tools in the multi and many-core era[J].IEEE Transactions on Parallel &amp; Distributed Systems, 2012, 23 (8) :1369-1386.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD15112600083383&amp;v=MTg1MzhxUVRNbndaZVp1SHlqbVVMZklKbDBSYUJVPU5pVEJhcks5SDlET3FZOUZaT01NRDNRNm9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Brodtkorb A R, Dyken C, Hagen T R, et al.State-of-the-art in heterogeneous computing[J].Scientific Programming, 2010, 18 (1) :1-33.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The case for GPGPU spatial multitasking">

                                <b>[3]</b> Adriaens J T, Compton K, Kim N S, et al.The case for GPGPU spatial multitasking[C]//Proc of IEEE International Symposium on High Performance Computer Architecture, 2012:1-12.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=hStreams architecture document for Intel? MPSS 3.5">

                                <b>[4]</b> hStreams architecture document for Intel<sup>®</sup> MPSS 3.5[Z].Santa Clara:Intel Corporation, 2015.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OpenCL programming guide">

                                <b>[5]</b> Munshi A, Gaster B, Mattson T G, et al.OpenCL programming guide[M].Beijing:Science Press, 2012.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An introduction to GPU computing and CUDA architecture">

                                <b>[6]</b> Tariq S.An introduction to GPU computing and CUDA architecture[Z].Santa Clara:NVIDIA Corporation, 2011.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computer architecture:A quantitative approach">

                                <b>[7]</b> Hennessy L, Patterson D A.Computer architecture:A quantitative approach[M].4th Edition.San Francisco, CA:Morgan Kaufmann Publishers Inc., 2006.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-threaded kernel offloading to GPGPU using Hyper-Q on Kepler architecture:Tech.Rep.14-19">

                                <b>[8]</b> Wende F, Steinke T, Cordes F.Multi-threaded kernel offloading to GPGPU using Hyper-Q on Kepler architecture:Tech.Rep.14-19[R].Berlin:Zuse Institute Berlin, 2014.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Concurrent kernel execution on Xeon Phi within parallel heterogeneous workloads">

                                <b>[9]</b> Wende F, Steinke T, Cordes F.Concurrent kernel execution on Xeon Phi within parallel heterogeneous workloads[C]//Proc of Euro-Par 2014 Parallel Processing, 2014:788-799.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPU-Chariot:A programming framework for stream applications running on multi-GPU systems">

                                <b>[10]</b> Fumihiko I N O, Nakagawa S, Hagihara K.GPU-Chariot:A programming framework for stream applications running on multi-GPU systems[J].IEEE Transactions on Information &amp; Systems, 2013, 96-D (12) :2604-2616.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100263312&amp;v=MTIyOTJIeWptVUxmSUpsMFJhQlU9TmlmT2ZiSzdIdERPcm85Rlp1ME1EMzA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Gómez-Luna J, GonzáLez-Linares J M, Benavides J I, et al.Performance models for asynchronous data transfers on consumer graphics processing units[J].Journal of Parallel &amp; Distributed Computing, 2012, 72 (9) :1117-1126.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance models for CPU-GPU data transfers">

                                <b>[12]</b> van Werkhoven B, Maassen J, Seinstra F J.Performance models for CPU-GPU data transfers[C]//Proc of International Symposium on Cluster, Cloud and Grid Computing, 2014:11-20.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Software pipelining for graphic processing unit acceleration">

                                <b>[13]</b> Liu Bo-zhong, Qiu Wei-dong, Jiang Lin, et al.Software pipelining for graphic processing unit acceleration[J].International Journal of High Performance Computing Applications, 2016, 30 (2) :169-185.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hetero streams library 1.0 reference codes">

                                <b>[14]</b> Hetero streams library 1.0 reference codes [Z].Santa Clara:Intel Corporation, 2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201907001" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907001&amp;v=MjcxNTdCdEdGckNVUkxPZVplUm1GeTdtVnIzUEx6N0JaYkc0SDlqTXFJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
