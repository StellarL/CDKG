<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132373167842500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201907016%26RESULT%3d1%26SIGN%3dw6BqkVjzOxNdyJ%252fC9G0H3K9KQWg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201907016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907016&amp;v=MDE3MjRSTE9lWmVSbUZ5N21XcjNQTHo3QlpiRzRIOWpNcUk5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="&lt;b&gt;2 相关工作&lt;/b&gt; "><b>2 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="&lt;b&gt;3 语料数据的预处理&lt;/b&gt; "><b>3 语料数据的预处理</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="&lt;b&gt;3.1 双语语料的预处理&lt;/b&gt;"><b>3.1 双语语料的预处理</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;3.2 预训练的词向量&lt;/b&gt;"><b>3.2 预训练的词向量</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="&lt;b&gt;4 神经机器翻译模型的改进&lt;/b&gt; "><b>4 神经机器翻译模型的改进</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="&lt;b&gt;4.1 不同的中文文本预处理方法&lt;/b&gt;"><b>4.1 不同的中文文本预处理方法</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;4.2 不同的嵌入层参数初始化方法&lt;/b&gt;"><b>4.2 不同的嵌入层参数初始化方法</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;4.3 转换层&lt;/b&gt;"><b>4.3 转换层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="&lt;b&gt;5 神经翻译系统的评估及实验结果分析&lt;/b&gt; "><b>5 神经翻译系统的评估及实验结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="&lt;b&gt;5.1 翻译系统评估BLEU&lt;/b&gt;"><b>5.1 翻译系统评估BLEU</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;5.2 实验结果及分析&lt;/b&gt;"><b>5.2 实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#149" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="图1 中文语料预处理的过程">图1 中文语料预处理的过程</a></li>
                                                <li><a href="#78" data-title="图2 添加转换层的seq2seq模型示意图">图2 添加转换层的seq2seq模型示意图</a></li>
                                                <li><a href="#81" data-title="图3 转换层结构示意图">图3 转换层结构示意图</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表1 模型对比&lt;/b&gt;"><b>表1 模型对比</b></a></li>
                                                <li><a href="#117" data-title="图4 模型1在TED2013上的训练结果">图4 模型1在TED2013上的训练结果</a></li>
                                                <li><a href="#121" data-title="图5 模型2在TED2013上的训练结果">图5 模型2在TED2013上的训练结果</a></li>
                                                <li><a href="#125" data-title="图6 模型3在TED2013上的训练结果">图6 模型3在TED2013上的训练结果</a></li>
                                                <li><a href="#133" data-title="图7 模型4在TED2013上的训练结果">图7 模型4在TED2013上的训练结果</a></li>
                                                <li><a href="#137" data-title="图8 在中文端使用预训练的词向量 (模型3) 和随机初始化 (模型4) 2种方式训练时的损失对比">图8 在中文端使用预训练的词向量 (模型3) 和随机初始化 (模型4) 2种方式训练时的损失对比</a></li>
                                                <li><a href="#139" data-title="图9 使用添加转换层的模型5在TED2013上的训练结果">图9 使用添加转换层的模型5在TED2013上的训练结果</a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表2 模型效果对比&lt;/b&gt;"><b>表2 模型效果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="170">


                                    <a id="bibliography_1" title=" Marcu D, Wong W.A phrase-based, joint probability model for statistical machine translation[C]//Proc of the ACL-02 Conference on Empirical Methods in Natural Language Processing, 2002:133-139." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A phrase-based joint probability model for statistical machine translation">
                                        <b>[1]</b>
                                         Marcu D, Wong W.A phrase-based, joint probability model for statistical machine translation[C]//Proc of the ACL-02 Conference on Empirical Methods in Natural Language Processing, 2002:133-139.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     Bengio Y, Ducharme R, Vincent P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155.</a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_3" title=" Vaswani A, Zhao Y, Fossum V, et al.Decoding with large-scale neural language models improves translation[C]//Proc of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013:1387-1392." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decoding with Large-scale Neural Language Models Improves Translation">
                                        <b>[3]</b>
                                         Vaswani A, Zhao Y, Fossum V, et al.Decoding with large-scale neural language models improves translation[C]//Proc of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013:1387-1392.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_4" title=" Cho K, van Merrienboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]//Proc of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014:1724-1734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning phrase representations using RNN encoder-decoder for statistical machine translation">
                                        <b>[4]</b>
                                         Cho K, van Merrienboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]//Proc of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014:1724-1734.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_5" title=" Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks[C]//Proc of the 27th Conference on Neural Information Processing Systems, 2014:3104-3112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequence to sequence learning with neural networks">
                                        <b>[5]</b>
                                         Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks[C]//Proc of the 27th Conference on Neural Information Processing Systems, 2014:3104-3112.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_6" title=" Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate[J].arXiv preprint arXiv:1409.0473v1, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">
                                        <b>[6]</b>
                                         Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate[J].arXiv preprint arXiv:1409.0473v1, 2014.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     Luong M T, Pham H, Manning C D.Effective approaches to attention-based neural machine translation[C]//Proc of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015:1412-1421.</a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_8" title=" Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C]//Proc of the 31st Conference on Neural Information Processing Systems, 2017:5998-6008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention is All You Need">
                                        <b>[8]</b>
                                         Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C]//Proc of the 31st Conference on Neural Information Processing Systems, 2017:5998-6008.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166.</a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_10" title=" Pascanu R, Mikolov T, Bengio Y.On the difficulty of training recurrent neural networks[C]//Proc of International Conference on Machine Learning, 2013:1310-1318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the difficulty of training recurrent neural networks">
                                        <b>[10]</b>
                                         Pascanu R, Mikolov T, Bengio Y.On the difficulty of training recurrent neural networks[C]//Proc of International Conference on Machine Learning, 2013:1310-1318.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_11" title=" Tiedemann J O.Parallel data, tools and interfaces in OPUS[C]//Proc of the 8th International Conference on Language Resources and Evaluation 2012, 2012:2214-2218." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel data tools and interfaces in OPUS">
                                        <b>[11]</b>
                                         Tiedemann J O.Parallel data, tools and interfaces in OPUS[C]//Proc of the 8th International Conference on Language Resources and Evaluation 2012, 2012:2214-2218.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_12" title=" Tian L, Wong D F, Chao L S, et al.UM-corpus:A large English-Chinese parallel corpus for statistical machine translation[C]//Proc of the 9th International Conference on Lan- guage Resources and Evaluation 2014, 2014:1837-1842." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UM-corpus A large English-Chinese parallel corpus for statistical machine translation">
                                        <b>[12]</b>
                                         Tian L, Wong D F, Chao L S, et al.UM-corpus:A large English-Chinese parallel corpus for statistical machine translation[C]//Proc of the 9th International Conference on Lan- guage Resources and Evaluation 2014, 2014:1837-1842.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_13" title=" Ziemski M, Junczys-Dowmunt M, Pouliquen B.The united nations parallel corpus v1.0[C]//Proc of the 11th International Conference on Language Resources and Evaluation 2016, 2016:3530-3534." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The United Nations Parallel Corpus">
                                        <b>[13]</b>
                                         Ziemski M, Junczys-Dowmunt M, Pouliquen B.The united nations parallel corpus v1.0[C]//Proc of the 11th International Conference on Language Resources and Evaluation 2016, 2016:3530-3534.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Han Dong-xu, Chang Bao-bao.Approaches to domain adaptive Chinese segmentation model[J].Chinese Journal of Computers, 2015, 38 (2) :272-281. (in Chinese) </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_15" title=" Mikolov T, Chen K, Corrado G, et al.Efficient estimation of word representations in vector space[J].arXiv preprint arXiv:1301.3781v3, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">
                                        <b>[15]</b>
                                         Mikolov T, Chen K, Corrado G, et al.Efficient estimation of word representations in vector space[J].arXiv preprint arXiv:1301.3781v3, 2013.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_16" title=" Pennington J, Socher R, Manning C D.Glove:Global vectors for word representation[C]//Proc of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014:1532-1543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">
                                        <b>[16]</b>
                                         Pennington J, Socher R, Manning C D.Glove:Global vectors for word representation[C]//Proc of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014:1532-1543.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_14" title=" 韩冬煦, 常宝宝.中文分词模型的领域适应性方法[J].计算机学报, 2015, 38 (2) :272-281." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201502005&amp;v=MjY0NzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XcjNPTHo3QmRyRzRIOVRNclk5Rlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         韩冬煦, 常宝宝.中文分词模型的领域适应性方法[J].计算机学报, 2015, 38 (2) :272-281.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(07),1257-1265 DOI:10.3969/j.issn.1007-130X.2019.07.016            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进seq2seq模型的英汉翻译研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%82%96%E6%96%B0%E5%87%A4&amp;code=38826903&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肖新凤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%9F%B3%E5%90%9B&amp;code=09005340&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李石君</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E4%BC%9F&amp;code=08992551&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%9D%B0&amp;code=08985465&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%80%8D%E9%9B%84&amp;code=38184778&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘倍雄</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E4%B8%9C%E7%8E%AF%E5%A2%83%E4%BF%9D%E6%8A%A4%E5%B7%A5%E7%A8%8B%E8%81%8C%E4%B8%9A%E5%AD%A6%E9%99%A2%E6%9C%BA%E7%94%B5%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=1694540&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广东环境保护工程职业学院机电工程系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目前机器翻译主要对印欧语系进行优化与评测, 很少有对中文进行优化的, 而且机器翻译领域效果最好的基于注意力机制的神经机器翻译模型—seq2seq模型也没有考虑到不同语言间语法的变换。提出一种优化的英汉翻译模型, 使用不同的文本预处理和嵌入层参数初始化方法, 并改进seq2seq模型结构, 在编码器和解码器之间添加一层用于语法变化的转换层。通过预处理, 能缩减翻译模型的参数规模和训练时间20%, 且翻译性能提高0.4 <i>BLEU</i>。使用转换层的seq2seq模型在翻译性能上提升0.7～1.0 <i>BLEU</i>。实验表明, 在规模大小不同的语料英汉翻译任务中, 该模型与现有的基于注意力机制的seq2seq主流模型相比, 训练时长一致, 性能提高了1～2 <i>BLEU</i>。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经机器翻译;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=seq2seq%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">seq2seq模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">命名实体识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    肖新凤 (1978-) , 女, 湖南邵阳人, 硕士, 讲师, 研究方向为软件技术、大数据和数据挖掘。E-mail:xghbl_0515@163.com, 85043766@qq.com通信地址:528216广东省佛山市南海区桂丹西路98号广东环境保护工程职业学院机电工程系;
                                </span>
                                <span>
                                    李石君 (1964-) , 男, 湖南岳阳人, 博士, 教授, 研究方向为大数据、互联网搜索与挖掘、数据挖掘、数据库技术、移动数据挖掘与时空一致性研究。E-mail:shjli@whu.edu.cn;
                                </span>
                                <span>
                                    余伟 (1987-) , 男, 湖北荆州人, 博士, 讲师, CCF会员 (33109M) , 研究方向为数据质量评估、数据抽取和数据融合。E-mail:weiy@whu.edu.cn;
                                </span>
                                <span>
                                    刘倍雄, 通信地址:528216广东省佛山市南海区桂丹西路98号广东环境保护工程职业学院机电工程系;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61502350);</span>
                                <span>2017广东高校省级重点平台和重大科研项目 (2017GKTSCX042);</span>
                    </p>
            </div>
                    <h1><b>English-Chinese translation based on an improved seq2seq model</b></h1>
                    <h2>
                    <span>XIAO Xin-feng</span>
                    <span>LI Shi-jun</span>
                    <span>YU Wei</span>
                    <span>LIU Jie</span>
                    <span>LIU Bei-xiong</span>
            </h2>
                    <h2>
                    <span>Department of Mechanical and Electrical Engineering, Guangdong Polytechnic of Environmental Protection Engineering</span>
                    <span>School of Computer Science, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Current machine translation systems optimize and evaluate the translation process in Indo-European languages to enhance translation accuracy. But researches about Chinese language are few. At present the seq2 seq model is the best method in the field of machine translation, which is a neural machine translation model based on the attention mechanism. However, it does not take into account the grammar transformation between different languages. We propose a new optimized English-Chinese translation model. It uses different methods to preprocess texts and initialize embedding layer parameters. Additionally, to improve the seq2 seq model structure, a transform layer between the encoder and the decoder is added to deal with grammar transformation problems. Preprocessing can reduce the parameter size and training time of the translation model by 20%, and the translation performance is increased by 0.4 BLEU. The translation performance of the seq2 seq model with a transform layer is improved by 0.7 to 1.0 BLEU. Experiments show that compared to the existing seq2 seq mainstream model based on the attention mechanism, the training time for English-Chinese translation tasks is the same for corpus of different sizes, but the translation performance of the proposal is improved by 1 to 2 BLEU.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20machine%20translation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural machine translation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=seq2seq%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">seq2seq model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=named%20entity%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">named entity recognition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XIAO Xin-feng, born in 1978, MS, lecturer, her research interests include software technology, big data, and data mining.Address:Department of Mechanical and Electrical Engineering, Guangdong Polytechnic of Environmental Protection Engineering, 98 Guidan West Road, Nanhai District, Foshan 528216, Guangdong, P.R.China;
                                </span>
                                <span>
                                    LI Shi-jun, born in 1964, PhD, professor, his research interests include big data, internet search and mining, data mining, database technology, mobile data mining and spacetime consistency research.;
                                </span>
                                <span>
                                    YU Wei, born in 1987, PhD, lecturer, CCF member (33109M) , his research interests include data quality assessment, data extraction, and data fusion.;
                                </span>
                                <span>
                                    LIU Bei-xiong, Address:Department of Mechanical and Electrical Engineering, Guangdong Polytechnic of Environmental Protection Engineering, 98 Guidan West Road, Nanhai District, Foshan 528216, Guangdong, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-10-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="38" name="38" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="39">高效地处理各类语言间的互译已成为当代人们的普遍需求, 机器翻译已逐步融入到人们的生产生活中。而汉语和英语作为世界上使用人口最多的2种语言, 针对英语到汉语的翻译研究意义重大。IBM的科学家Brown等的开创性工作开启了统计机器翻译的篇章, 从大量的双语平行语料中自动学习概率性的词对齐规则。Marcu等<citation id="204" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出基于短语 (Phase-based) 的统计机器翻译。基于短语的词对齐仍然是统计机器翻译的核心组件。</p>
                </div>
                <div class="p1">
                    <p id="40">统计机器翻译的另一重要组件是语言模型 (Language Model) 。Bengio等<citation id="205" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>于2003年提出的神经语言模型, 用稠密的固定长度向量来表征词汇表中的词语并依次计算语句概率, 使用神经网络进行训练, 克服了传统语言模型的维度爆炸问题。之后, Vaswani等<citation id="206" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>在研究中将神经语言模型与统计机器翻译结合, 大幅提升了翻译性能。现在神经机器翻译的主流模型是Cho等<citation id="207" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>在2014年提出的编码器-解码器 (Encoder-Decoder) 模型结构。神经机器翻译领域使用最广、效果最好的模型是Sutskever等<citation id="208" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出的seq2seq模型和Bahdanau等<citation id="209" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>与Luong等<citation id="210" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的基于注意力机制的seq2seq模型。其中Sutskever等<citation id="211" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的seq2seq模型是首个在大规模翻译任务中超过统计机器翻译的神经机器翻译模型, 是如今大部分神经机器翻译模型的原型, 其效果可以作为评价翻译系统的基准线 (baseline) 。Luong等<citation id="212" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>的基于注意力机制的seq2seq模型是如今最好的神经机器翻译模型之一。Google完全采用注意力机制构造神经机器翻译模型<citation id="213" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">汉语属于汉藏语系, 其与英语、法语、意大利语、西班牙语等印欧语系有很大差异。在自然语言处理中, 他们最大的差异体现在汉语的词间没有间隔, 而印欧语系的词之间都会用空格隔开。这造成在以词为单位做分析处理时很不便利。中文具有字符种类多、字符信息熵大、表意能力强等特点, 而现有的大部分seq2seq模型都是在印欧语系 (如英语、法语、德语等) 上进行训练与评估, 很少有针对中文特点进行优化的。</p>
                </div>
                <div class="p1">
                    <p id="42">为此, 本文优化了英汉翻译的方法及模型, 在现有seq2seq模型基础上, 针对中文特点进行相应改进, 使用了不同的文本预处理方法和嵌入层参数初始化方法, 对seq2seq模型结构进行了改进, 在编码器和解码器间增加了一层用于语法变化的转换层 (Transform Layer) 。本文主要贡献有:</p>
                </div>
                <div class="p1">
                    <p id="43"> (1) 针对中文字符种类多、字符信息熵大、表意能力强等特点, 提出将中文句子预处理成字符+命名实体序列, 代替传统的通过中文分词将句子预处理成词语序列的做法。</p>
                </div>
                <div class="p1">
                    <p id="44"> (2) 在英汉翻译模型中, 英文端使用GloVe (Global Vectors for word representation) 进行嵌入层参数初始化, 中文端使用随机初始化。预训练的词向量在不同语言的语料中训练更加契合。</p>
                </div>
                <div class="p1">
                    <p id="45"> (3) 改进了seq2seq模型的结构, 在编码器和解码器之间添加了一层用于语法变化的转换层, 对语言的语法作出相应转换。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag"><b>2 相关工作</b></h3>
                <div class="p1">
                    <p id="47">机器翻译中的2大核心组件是词对齐与语言模型, 但词对齐无法解决词语的歧义性问题, 不同语言的语法不同, 直接通过对齐来翻译会产生不通顺的翻译结果。虽然语言模型在一定程度上可以解决以上2个问题, 但传统的语言模型存在维度爆炸的问题。神经语言模型用于解决传统语言模型中的维度爆炸问题<citation id="214" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 统计机器翻译模型中存在需要对很多中间组件进行学习的问题。神经机器翻译的主流模型Encoder-Decoder模型结构<citation id="215" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>是一种端到端 (end-to-end) 的学习模型, 解决了统计机器翻译模型中需要对很多中间组件进行学习的问题。</p>
                </div>
                <div class="p1">
                    <p id="48">循环神经网络RNN (Recurrent Neural Networks) 结构克服长依赖问题且理论上可以解决任意长度的依赖问题, 但是在实际应用中却并不是很理想, 存在梯度爆炸<citation id="216" type="reference"><link href="186" rel="bibliography" /><link href="188" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>且离输出越远的梯度越小的问题。当梯度爆炸发生时, 参数的更新变得极不稳定, 模型剧烈震荡无法收敛。当梯度消失发生时, 距输出较远的长距离项对梯度的贡献变得可以忽略不计, 因此模型获取不到长距离依赖信息。梯度爆炸一般可以通过如梯度裁剪 (gradient clipping) 来解决, 但是梯度消失却很难解决, 直至今日仍是循环神经网络中的一个难题。目前使用最广、效果最好的方法是使用门控单元来构建循环神经网络。其中, LSTM (Long Short-Term Memory) 与GRU (Gated Recurrent Unit) 是使用最广的门控单元。</p>
                </div>
                <div class="p1">
                    <p id="49">本文提出将中文句子预处理成字符+命名实体序列, 代替传统的通过中文分词将句子预处理成词语序列的做法。由于翻译系统中需要使用2种不同语言的词嵌入表示, 而预训练的词向量在不同语言的语料中训练, 导致不同语言的词向量并不契合。不同语言间的语法相差甚远, 翻译模型应在保持源语言语义的前提下, 对语言的语法作出相应转换。本文实现了用于英汉翻译的seq2seq模型, 并在小、中、大3个规模的双语语料上对本文提出的3种改进方法进行验证实验。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag"><b>3 语料数据的预处理</b></h3>
                <div class="p1">
                    <p id="51">本文所做的机器翻译, 需要句子级别的双语对照语料。使用的命名实体识别工具是哈尔滨工业大学社会计算与信息检索研究中心研制的语言技术平台, 该工具包中的命名实体识别部分是通过在线机器学习算法框架从标注数据中训练的。本文收集了小规模、中规模和大规模3个规模等级的数据来训练和验证本文使用的模型。 (1) 小规模数据TED2013<citation id="217" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>来自TED2013所有演讲的中英字幕。 (2) 中规模数据UM-Corpus<citation id="218" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>来自NLP<sup>2</sup>CT的包括教育、法律、微博、新闻等8个领域的文章组成的句子级别的中英语料对。 (3) 大规模数据UNv1.0<citation id="219" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>来自联合国平行语料库, 由已进入公有领域的联合国正式记录和其他会议文件组成。</p>
                </div>
                <div class="p1">
                    <p id="52">通过数据清洗和预处理后, 中文和英文的文本语料被转换成了词语、字符、标点符号或命名实体的序列, 但这些文本符号仍然是非结构化的自然信号, 无法直接输入到神经网络中处理。嵌入层作为神经网络的第一层, 其主要功能是将自然信号转换为结构化的数字信号, 即将文本符号嵌入到固定维度的向量空间中, 嵌入层的参数可以是随机初始化的, 也可以是预训练的词向量。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53"><b>3.1 双语语料的预处理</b></h4>
                <h4 class="anchor-tag" id="54" name="54">3.1.1 中文语料预处理</h4>
                <div class="p1">
                    <p id="55">对不同语料进行分析、清洗后, 在应用于模型之前, 还要先进行相应预处理。在中文语料中, 除了中文字符外, 还含有数字和英文字符 (如DNA, USA等) , 本文将数字和英文字符都当作命名实体进行处理。在将中文句子当作词语序列进行处理时, 也可以同时进行句子的分词与命名实体识别, 以提高分词的准确率。因此, 整个中文语料数据预处理的过程如图1所示。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 中文语料预处理的过程" src="Detail/GetImg?filename=images/JSJK201907016_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 中文语料预处理的过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Flowchart for the Chinese  corpus preprocessing methods</p>

                </div>
                <h4 class="anchor-tag" id="57" name="57">3.1.2 英文语料预处理</h4>
                <div class="p1">
                    <p id="58">在英文语料的句子中, 词语间已经以空格隔开, 因此对英文语料的预处理要简单得多。但是, 需要注意的是, 英语词语的标点符号通常没有和前一个词语以空格隔开。因此, 在以空格将英语语料分割成序列时, 需要在标点符号和前一个单词间插入空格。另外, 也可以将命名实体识别应用于英文语料, 将实体当作一个词语, 有利于句子的意思表示与翻译。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>3.2 预训练的词向量</b></h4>
                <div class="p1">
                    <p id="60">通过数据清洗和预处理后, 中文和英文的文本语料被转换成了词语、字符、标点符号或命名实体的序列, 本文将词语、字符、标点符号或命名实体等统称为文本符号。文本符号与神经网络的输入层之间添加一层转换层, 即嵌入层 (Embedding Layer) , 将文本符号嵌入到固定维度的向量空间中。嵌入层的参数可以是随机初始化的, 与神经网络中的其他参数一起进行训练;也可以是预训练的词向量, 预训练的词向量一般通过无监督的方式word2vec<citation id="220" type="reference"><link href="196" rel="bibliography" /><link href="202" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">14</a>]</sup></citation>或GloVe<citation id="221" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>学习到。神经语言模型的一种应用skip-gram模型的目标函数用目标词前后的各<i>n</i>个词的联合概率的对数表示:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><msub><mrow></mrow><mi>θ</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo>-</mo><mi>n</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>n</mi><mo>, </mo><mi>j</mi><mo>≠</mo><mn>0</mn></mrow></munder><mspace width="0.25em" /></mstyle><mi>log</mi><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中, <i>T</i>表示词料库中词的个数, 以<i>w</i><sub><i>t</i></sub>为中心词的上下文中的词<i>w</i><sub><i>t</i>+<i>j</i></sub>的出现概率<i>p</i> (<i>w</i><sub><i>t</i>+<i>j</i></sub>|<i>w</i><sub><i>t</i></sub>) , 通过softmax函数求得:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">v</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>j</mi></mrow></msub></mrow></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>V</mi></mrow></msub><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mspace width="0.25em" /><mo stretchy="false"> (</mo><msup><mi mathvariant="bold-italic">v</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中, <i>V</i>表示所有词语组成的词汇表, <b><i>v</i></b><sub><i>w</i><sub><i>i</i></sub></sub>表示词语<i>w</i><sub><i>i</i></sub>的嵌入向量, <b><i>v</i></b>′<sub><i>w</i><sub><i>i</i></sub></sub>表示词语<i>w</i><sub><i>i</i></sub>在输出层的向量表示, 式 (2) 中分母涉及到对词汇表中所有的词进行分别计算, 计算代价高昂, 是word2vec的瓶颈所在。GloVe在word2vec的基础上添加了全局的基于词语共现频率的统计信息, 训练了比word2vec泛化能力更强的词向量<citation id="222" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。在本文的翻译模型中, 将无监督学习得到的词向量应用于神经网络模型时, 训练数据充足, 因此使用预训练的词向量时, 词向量都设为可再训练的。</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag"><b>4 神经机器翻译模型的改进</b></h3>
                <div class="p1">
                    <p id="66">现有seq2seq模型主要对印欧语系进行优化与评测, 少有针对中文的优化, 且现有模型没有考虑到不同语言间语法的变换。针对这一现象, 本文提出了3种改进seq2seq模型的方法。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>4.1 不同的中文文本预处理方法</b></h4>
                <div class="p1">
                    <p id="68">针对中文文本, 在seq2seq模型的基础上进行了部分改进, 使用了适用于中文文本的预处理方法与嵌入层参数初始化方法。并改进了seq2seq模型的结构, 在编码器和解码器间添加了一层转换层。根据中文字符信息量大、表意能力强的特点, 提出以字符+命名实体作为文本符号进行预处理。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>4.2 不同的嵌入层参数初始化方法</b></h4>
                <div class="p1">
                    <p id="70">本文尝试使用预训练的GloVe进行嵌入层参数初始化和随机初始化方式进行对比实验。具体做法是在英文端和中文端都使用预训练的GloVe词向量进行初始化, 或在英文端和中文端都使用随机初始化, 或在英文端使用GloVe进行初始化、在中文端使用随机初始化。</p>
                </div>
                <div class="p1">
                    <p id="71">本文使用的随机参数初始化方法是截断正态分布。由于正态分布的取值为 (-∞, +∞) , 而取值较大的参数会影响模型的稳定性, 因而在初始化时将参数限制在一定范围内有利于模型的训练。本文使用了均值为0, 标准差为1的正态分布:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi><mglyph src="数学符号.jpg" height="100%" width="100%" /></mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><msqrt><mrow><mn>2</mn><mtext>π</mtext></mrow></msqrt></mrow></mfrac><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">将其截断到3个标准差内, 即限制<i>x</i>∈ (-3, +3) , 截断后的分布为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi><mglyph src="数学符号.jpg" height="100%" width="100%" /></mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Φ</mi><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo><mo>-</mo><mi>Φ</mi><mo stretchy="false"> (</mo><mo>-</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中Φ为累积分布函数。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>4.3 转换层</b></h4>
                <div class="p1">
                    <p id="77">在翻译系统中, 源语言和目标语言的语义信息要尽可能保持不变, 但2种语言的语法可能会有很大差异, 特别是汉语和英语这2种属于不同语系的语言, 无论是在词法或句法方面都有显著的差异。在传统seq2seq模型的基础上, 在编码器和解码器之间, 添加了一层转换层, 起到语法转换的作用。其结构如图2所示。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 添加转换层的seq2seq模型示意图" src="Detail/GetImg?filename=images/JSJK201907016_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 添加转换层的seq2seq模型示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Schematic diagram of the seq2seq model  after adding a conversion layer</p>

                </div>
                <div class="p1">
                    <p id="79">连接编码器和解码器的表示向量实际上是编码器中最后一个RNN单元的状态向量, 如果使用LSTM作为RNN单元, 状态向量由2部分组成, 分别为神经元细胞状态<b><i>c</i></b>和输出状态<b><i>h</i></b>。以二元组 (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 表示编码器最后一个时刻的LSTM单元输出的状态向量。该二元组经过本文提到的转换层的变换, 得到与 (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 维度相同的新的二元组 (<b><i>c</i></b><sub>trans</sub>, <b><i>c</i></b><sub>trans</sub>) , 之后再作为解码器中LSTM的初始状态, 进行目标语言的解码。</p>
                </div>
                <div class="p1">
                    <p id="80">本文所使用的转换层中, (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 先通过前向神经网络层 (Forward Layer) 变换为 (<b><i>c</i></b>′, <b><i>h</i></b>′) , 并通过残差连接 (Residual Connection) 跳过前向神经网络层, 使 (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 直接作用于 (<b><i>c</i></b>′, <b><i>h</i></b>′) 得到 (<b><i>c</i></b><sub>res</sub>, <b><i>h</i></b><sub>res</sub>) , 最后输出到规范化层 (Layer Normalization) 。转换层的结构如图3所示。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 转换层结构示意图" src="Detail/GetImg?filename=images/JSJK201907016_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 转换层结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Schematic diagram of  the conversion layer structure</p>

                </div>
                <div class="p1">
                    <p id="82">转换层中的各个组件如下所示:</p>
                </div>
                <div class="p1">
                    <p id="83"> (1) 前向神经网络层。</p>
                </div>
                <div class="p1">
                    <p id="84">本文选择前向神经网络层的非线性变换为ReLU (Rectified Linear Unit) 激活函数。ReLU又称修正线性单元, 是神经网络中一种常用的非线性变换:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">虽然二元组中<b><i>c</i></b><sub>encoder</sub>和<b><i>h</i></b><sub>encoder</sub>都会经过前向神经网络层, 但它们使用了独立的参数。二元组 (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 通过前向神经网络层变换为 (<b><i>c</i></b>′, <b><i>h</i></b>′) 。</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><msup><mi mathvariant="bold-italic">c</mi><mo>′</mo></msup><mo>=</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext><mtext>c</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msub><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>c</mi><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>c</mi><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>c</mi><mn>2</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><msup><mi mathvariant="bold-italic">h</mi><mo>′</mo></msup><mo>=</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext><mtext>c</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msub><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mn>1</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>h</mi><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mn>2</mn></mrow></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mrow><mi>h</mi><mn>2</mn></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中, <b><i>W</i></b><sub><i>c</i>1</sub>和<b><i>b</i></b><sub><i>c</i>1</sub>是作用于<b><i>c</i></b><sub>encoder</sub>的第1个线性变换的变换矩阵和偏置向量, <b><i>W</i></b><sub><i>c</i>2</sub>和<b><i>b</i></b><sub><i>c</i>2</sub>是第2个线性变换的变换矩阵和偏置向量, <b><i>W</i></b><sub><i>h</i>1</sub>, <b><i>b</i></b><sub><i>h</i>1</sub>, <b><i>W</i></b><sub><i>h</i>2</sub>, <b><i>b</i></b><sub><i>h</i>2</sub>与之类似。为了保持在编码器和解码器中LSTM的单元状态向量维度的一致性, 转换层的输入和输出的维度是一样的, 而规范化层不会改变向量的维度, 因此二元组中<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>, <b><i>c</i></b>′, <b><i>h</i></b>′, <b><i>c</i></b><sub>trans</sub>, <b><i>h</i></b><sub>trans</sub>都是维度一样的向量。以<i>d</i>表示模型的状态向量维度, 第1个线性变换后的维度为<i>h</i>, 则<b><i>W</i></b><sub><i>c</i>1</sub>与<b><i>W</i></b><sub><i>h</i>1</sub>的维度为<i>d</i>×<i>h</i>, 偏置<b><i>b</i></b><sub><i>c</i>1</sub>与<b><i>b</i></b><sub><i>h</i>1</sub>的维度为<i>h</i>, <b><i>W</i></b><sub><i>c</i>2</sub>与<b><i>W</i></b><sub><i>h</i>2</sub>的大小为<i>h</i>×<i>d</i>, 偏置<b><i>b</i></b><sub><i>c</i>2</sub>与<b><i>b</i></b><sub><i>h</i>2</sub>的维度为<i>d</i>。该层总共含有的参数个数为4<i>dh</i>+2<i>d</i>+2<i>h</i>, 在本文使用的模型中, <i>h</i>=4<i>d</i>, 因此每一层的参数个数为16<i>d</i><sup>2</sup>+10<i>d</i>。如果RNNs共有<i>n</i>层堆叠, 那么模型中转换层的前向神经网络层共有参数16<i>nd</i><sup>2</sup>+10<i>nd</i>个。运算时间主要在向量与矩阵相乘, 共有16<i>nd</i><sup>2</sup>次乘法运算。</p>
                </div>
                <div class="p1">
                    <p id="89"> (2) 残差连接。</p>
                </div>
                <div class="p1">
                    <p id="90">在本文所使用的转换层的结构中, (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 通过残差连接直接作用于前向神经网络层的输出 (<b><i>c</i></b>′, <b><i>h</i></b>′) 得到 (<b><i>c</i></b><sub>res</sub>, <b><i>h</i></b><sub>res</sub>) :</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><msup><mi mathvariant="bold-italic">c</mi><mo>′</mo></msup><mo>+</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext><mtext>c</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><msup><mi mathvariant="bold-italic">h</mi><mo>′</mo></msup><mo>+</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>n</mtext><mtext>c</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>r</mtext></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">这里使用残差连接也是因为考虑到通过前向神经网络层的变换, (<b><i>c</i></b><sub>encoder</sub>, <b><i>h</i></b><sub>encoder</sub>) 中可能丢失部分语义信息, 通过残差连接, 可以让模型重新拾取到可能丢失的语义信息。残差连接不会产生额外的参数, 也几乎不会增加模型的计算复杂度。而且, 整个网络仍然可以通过反向传播来训练。</p>
                </div>
                <div class="p1">
                    <p id="93"> (3) 规范化层。</p>
                </div>
                <div class="p1">
                    <p id="94">规范化 (Normalization) 是深度学习中的常用技术。在深度学习中, 不仅在预处理阶段可能需要规范化, 在网络的每一层之间, 也需要对各层的数据输入进行规范化。在本文所使用的规范化层中, 规范化的期望和方差分别设为<i>b</i>和<i>g</i>, 则对于输入<b><i>c</i></b><sub>res</sub>层标准化的具体操作是:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>n</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mi>g</mi><mi>σ</mi></mfrac><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi></mrow></msub><mo>-</mo><mi>μ</mi><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中, <i>μ</i>和<i>σ</i>分别为输入特征的原始期望和方差:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>μ</mi><mo>=</mo><mfrac><mn>1</mn><mi>d</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>d</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><mo stretchy="false"> (</mo></mstyle><mi>c</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>-</mo><mi>μ</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中, <i>d</i>为特征的维度, <i>c</i><sub><i>res</i><sub><i>i</i></sub></sub>表示<b><i>c</i></b><sub>res</sub>的第<i>i</i>维的特征数值。对<b><i>h</i></b><sub>res</sub>也有类似的规范化操作。该层将 (<b><i>c</i></b><sub>res</sub>, <b><i>h</i></b><sub>res</sub>) 规范化为 (<b><i>c</i></b><sub>trans</sub>, <b><i>h</i></b><sub>trans</sub>) , 作为后续解码器LSTM的初始状态。另外, 规范化层不仅存在于转换层中, 在编码器和解码器的LSTM单元中, 也存在相同的规范化层。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag"><b>5 神经翻译系统的评估及实验结果分析</b></h3>
                <h4 class="anchor-tag" id="100" name="100"><b>5.1 翻译系统评估BLEU</b></h4>
                <div class="p1">
                    <p id="101">BLEU算法实际上做的是评估机器翻译的句子与标准翻译的相似度, 最简单的方法是统计翻译出来的句子中出现的词在标准翻译中出现的次数, 即评测翻译中的词的准确度。分母是评测翻译的长度, 分子是评测翻译中的词在标准翻译中出现的次数。但是, 这种评测方式有个明显的缺陷, 翻译系统通过不断输出常用词来干扰评测称为常用词干扰。针对此现象, 本文稍微做下改进, 使标准翻译中的每个词只计数一次。因此, 首先统计标准翻译中每个词出现的次数, 然后将评测翻译中的词的次数裁剪 (clip) 到不超过对应的标准翻译中这个词出现的次数, 之后再将所有词的裁剪值相加再除以评测翻译的长度, 即:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>s</mi></mrow></msub><mi>c</mi></mstyle><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>i</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>s</mi></mrow></msub><mi>c</mi></mstyle><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">其中, <i>count</i> (<i>w</i><sub><i>i</i></sub>) 表示词<i>w</i><sub><i>i</i></sub>在评测翻译中的计数, <i>count</i><sub>clip</sub> (<i>w</i><sub><i>i</i></sub>) 表示在裁剪后的计数, <i>s</i>表示评测翻译的结果。以上计算出的<i>p</i><sub>1</sub>评测翻译中词语的准确率, 采用的是<i>uni</i>-<i>gram</i>, 即只考虑了单个词语的准确率, 没有考虑词语间的顺序, 不能保证评测翻译的通顺性。因此, <i>BLEU</i> (Bilingual Evaluation Understudy) 不止使用了<i>uni</i>-<i>gram</i>, 还使用了<i>n</i> -<i>gram</i>, <i>n</i> -<i>gram</i>用于衡量翻译的流畅性, 其计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>n</mi><mo>-</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi></mrow></msub><mi>c</mi></mstyle><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>i</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>n</mi><mo>-</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>∈</mo><mi>s</mi></mrow></msub><mi>c</mi></mstyle><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105"><i>BLEU</i>将<i>uni</i>-<i>gram</i>和<i>n</i>-<i>gram</i>进行了平均。实验发现, 无论是机器翻译还是人工翻译, 其<i>p</i><sub><i>n</i></sub>值随<i>n</i>成指数级下降, 因此考虑用几何平均来求<i>p</i><sub><i>n</i></sub>的平均值是比较合理的。几何平均可以用对数的算术平均表示, 即:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo>=</mo><mi>exp</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mi>w</mi></mstyle><msub><mrow></mrow><mi>n</mi></msub><mspace width="0.25em" /><mi>log</mi><mspace width="0.25em" /><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">其中, <i>w</i><sub><i>n</i></sub>是标准化权值, <i>w</i><sub><i>n</i></sub>=1/<i>N</i>。<i>p</i>的值在0～1。值得注意的是, 几何平均是非常严格的, <i>p</i><sub><i>n</i></sub>中一旦有一个值为0, 则其几何平均也是0。</p>
                </div>
                <div class="p1">
                    <p id="108">除了翻译的准确率外, 翻译句子的长度也应该被衡量。翻译出来的句子应该和标准翻译的长度相当。由于在计算准确率时, 较短的评测翻译更容易得到高的准确率, 因此<i>BLEU</i>只对评测翻译长度小于标准翻译长度做了简单惩罚<i>BP</i>, 即:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mi>Ρ</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>, </mo><mi>s</mi><mo>&gt;</mo><mi>r</mi></mtd></mtr><mtr><mtd><mi>exp</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>r</mi><mo>/</mo><mi>s</mi><mo stretchy="false">) </mo><mo>, </mo><mi>s</mi><mo>≤</mo><mi>r</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中, <i>r</i>是标准翻译长度, <i>s</i>是评测翻译长度, 最终<i>BLEU</i>的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><mi>L</mi><mi>E</mi><mi>U</mi><mo>=</mo><mi>B</mi><mi>Ρ</mi><mo>×</mo><mi>exp</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>n</mi></msub><mspace width="0.25em" /><mi>log</mi><mspace width="0.25em" /><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">实验发现, BLEU与人工的评估具有很强的相关性, 在5个翻译系统中计算出的<i>BLEU</i>值与人工评估的相关系数达到0.99。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>5.2 实验结果及分析</b></h4>
                <div class="p1">
                    <p id="114">本文所使用的seq2seq模型中, 编码器和解码器由2层双向LSTM组成, 一共4层LSTM, LSTM单元维度为256维, 词嵌入的维度为256维, 训练时每批数据共128个句子对。本文对现有的基于seq2seq模型的翻译系统与基于注意力机制的seq2seq模型的翻译系统以及文中提到的3种改进方法进行了实验验证。并对模型的实验结果进行分析, 结合各模型在小、中、大3个规模数据集上的表现, 对各模块做全面分析。其中, 各模型的对比如表1所示, 表中NER (Named Entity Recognition) 代表命名实体识别。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表1 模型对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Comparison of models</b></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />模型<br />编号</td><td>模型结构</td><td>中文文本<br />预处理方法</td><td>嵌入层参数<br />初始化方法</td></tr><tr><td>1</td><td>seq2seq</td><td>中文分词</td><td>英文GloVe, 中文GloVe</td></tr><tr><td><br />2</td><td>seq2seq+注意力层</td><td>中文分词</td><td>英文GloVe, 中文GloVe</td></tr><tr><td><br />3</td><td>seq2seq+注意力层</td><td>字符+NER</td><td>英文GloVe, 中文GloVe</td></tr><tr><td><br />4</td><td>seq2seq+注意力层</td><td>字符+NER</td><td>英文GloVe, 中文随机</td></tr><tr><td><br />5</td><td>seq2seq+注意力层+转换层</td><td>字符+NER</td><td>英文GloVe, 中文随机</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">使用模型1在数据集TED2013上训练的损失函数曲线和在测试集上评估指标<i>BLEU</i>如图4所示。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 模型1在TED2013上的训练结果" src="Detail/GetImg?filename=images/JSJK201907016_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 模型1在TED2013上的训练结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Training results of the first model on TED2013</p>

                </div>
                <div class="p1">
                    <p id="118">图4a为模型在训练时损失函数值随训练步数的变化曲线, 阴影为每一步的真实损失函数值。由于每次计算损失函数选用不同批次的数据, 因此真实损失函数值会有较大的震荡, 这是随机梯度下降的特点造成的。为使曲线更好地可视化, 图中实线是平滑之后的曲线。</p>
                </div>
                <div class="p1">
                    <p id="119">分析损失曲线的下降趋势和<i>BLEU</i>曲线的趋势, 模型在第10 000步趋于稳定, <i>BLEU</i>值在第26 000步达到最高值13.71。模型每秒处理1 940个词, 每次批量训练耗时2.09 s, 总耗时约17.4 h。</p>
                </div>
                <div class="p1">
                    <p id="120">使用模型2在数据集TED2013上训练的损失函数曲线和在测试集上评估指标<i>BLEU</i>如图5所示。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 模型2在TED2013上的训练结果" src="Detail/GetImg?filename=images/JSJK201907016_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 模型2在TED2013上的训练结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Training results of the second model on TED2013</p>

                </div>
                <div class="p1">
                    <p id="122">引入注意力机制后, 模型在测试集上的<i>BLEU</i>达到15.53, 比普通的seq2seq提高了1.82。</p>
                </div>
                <div class="p1">
                    <p id="123">而模型的参数数量与训练和推断时的计算度略有提高。新增的参数是<i>W</i><sub><i>c</i></sub>与<i>W</i>, <i>W</i><sub><i>c</i></sub>维度为<i>d</i>*2<i>d</i> (<i>d</i>为模型的RNN单元状态维度, 本文中为256) , <i>W</i>维度为<i>d</i>*<i>d</i>, 因此总共增加3<i>d</i><sup>2</sup>个参数。模型每秒处理1 750个词, 每次批量训练耗时2.32 s, 总耗时约19.3 h。</p>
                </div>
                <div class="p1">
                    <p id="124">模型3在数据集TED2013上训练的损失函数曲线和在测试集上评估指标<i>BLEU</i>如图6所示。为了保持与模型1和模型2中<i>BLEU</i>评估的一致性, 该方法进行评估时, 仍然是将字符输出序列转换成词语序列后再进行<i>BLEU</i>值计算。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 模型3在TED2013上的训练结果" src="Detail/GetImg?filename=images/JSJK201907016_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 模型3在TED2013上的训练结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Training results of the third model on TED2013</p>

                </div>
                <div class="p1">
                    <p id="126">将中文按照字符+命名实体进行预处理的模型的<i>BLEU</i>最高达到15.93, 对比按照词语进行预处理的模型提高了0.40。考虑到<i>BLEU</i>是按照词语序列输出的方法计算的, 该模型实际翻译效果提高可能更多。</p>
                </div>
                <div class="p1">
                    <p id="127">本文中实验的前3个模型都以预训练的GloVe词向量作为词嵌入矩阵参数的初始化值。模型4结合翻译模型的特点分析了使用不同的参数初始化方法的优缺点。</p>
                </div>
                <div class="p1">
                    <p id="128">模型4使用不同的初始化方法进行了实验, 具体为:</p>
                </div>
                <div class="p1">
                    <p id="129"> (1) 英文端和中文端都使用预训练的GloVe词向量进行初始化;</p>
                </div>
                <div class="p1">
                    <p id="130"> (2) 在英文端和中文端都使用随机初始化;</p>
                </div>
                <div class="p1">
                    <p id="131"> (3) 在英文端使用GloVe进行初始化、在中文端使用随机初始化。</p>
                </div>
                <div class="p1">
                    <p id="132">在本文提到的前3个模型中, 使用了初始化方式 (1) 。在数据集TED2013上, 使用初始化方式 (2) 进行实验, 发现模型在30 000步内, 在测试集上的<i>BLEU</i>最大达到15.61, 略小于初始化方式 (1) 的, 可能是因为数据集TED2013数据量较小, 而英文端词表较大, 导致数据不足以训练出好于非监督的GloVe方法训练出的词向量。使用初始化方式 (3) 进行实验 (模型4) , 其在TED2013上训练的损失函数曲线和在测试集上评估指标<i>BLEU</i>如图7所示。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 模型4在TED2013上的训练结果" src="Detail/GetImg?filename=images/JSJK201907016_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 模型4在TED2013上的训练结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Training results of the fourth model on TED2013</p>

                </div>
                <div class="p1">
                    <p id="134">如图7所示, 在英文端使用预训练的GloVe词向量进行初始化, 在中文端使用随机初始化时, 模型的<i>BLEU</i>最高可以达到16.58, 比通常使用的初始化方式 (1) 的<i>BLEU</i>提高了0.65。使用随机初始化的模型4与使用GloVe词向量进行初始化的模型3的模型参数数量以及训练和推断的计算复杂程度完全一致。</p>
                </div>
                <div class="p1">
                    <p id="135">通过对初始化方式 (1) 和方式 (3) 中模型训练时的损失曲线下降趋势进行对比, 我们发现由于词向量是在大规模语料上训练收敛后得到的, 词向量的表达能力 (对比周围空间内的其它值) 已经比较出色, 因而使用预训练的词向量时, 有可能使模型很快陷入到局部最优解。</p>
                </div>
                <div class="p1">
                    <p id="136">如图8所示, 实曲线是使用预训练的词向量时模型的训练误差曲线, 误差在刚开始训练时下降得很快, 到10 000步左右时达到55左右, 之后误差下降得较慢, 最后稳定在50左右;虚曲线是使用随机初始化时模型的训练误差曲线, 误差在20 000步之前都下降得比较快, 在20 000步之后下降较缓, 最后稳定在45左右。正如前面分析, 使用预训练的词向量的模型在训练初期损失小于使用随机初始化的, 但在训练后期, 使用随机初始化的模型能收敛到更加好的结果。</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 在中文端使用预训练的词向量 (模型3) 和随机初始化 (模型4) 2种方式训练时的损失对比" src="Detail/GetImg?filename=images/JSJK201907016_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 在中文端使用预训练的词向量 (模型3) 和随机初始化 (模型4) 2种方式训练时的损失对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 8 Training loss comparison between  the third and the fourth models</p>

                </div>
                <div class="p1">
                    <p id="138">使用模型5在TED2013上训练的损失函数曲线和在测试集上评估指标<i>BLEU</i>曲线如图9所示。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 使用添加转换层的模型5在TED2013上的训练结果" src="Detail/GetImg?filename=images/JSJK201907016_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 使用添加转换层的模型5在TED2013上的训练结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 9 Training results of the fifth model  after adding a conversion layer on TED2013</p>

                </div>
                <div class="p1">
                    <p id="140">使用添加了转换层的模型, 模型新增加的参数主要在转换层中的前向神经网络层在TED2013数据集上进行训练时, 30 000步内在测试集上的<i>BLEU</i>最大值达到17.61, 比模型4的<i>BLEU</i>提高了1.03, 模型性能有较大提高。与使用字符+命名实体序列输入的中文端的嵌入层和softmax层参数总数2.89 MB数量级相当, 考虑到英文端的嵌入层 (参数10 MB左右) LSTM层以及注意力层 (参数1 MB以内) , 模型增加参数30%左右, 模型训练和推断时的计算量增加也在30%左右。模型5在TED2013上每秒处理3.07 KB词, 每次批量训练耗时2.36 s, 总耗时约19.6 h。相比以词语序列处理中文时, 时长增加了28.9%。</p>
                </div>
                <div class="p1">
                    <p id="141">综上所述, 使用模型1～模型5在数据集TED2013、UM-Corpus、UNv1.0中, 训练时长以及在测试集上的评估指标<i>BLEU</i>值如表2所示。</p>
                </div>
                <div class="area_img" id="142">
                                            <p class="img_tit">
                                                <b>表2 模型效果对比</b>
                                                    <br />
                                                <b>Table 2 Comparison of model effect</b>
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201907016_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJK201907016_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201907016_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 模型效果对比" src="Detail/GetImg?filename=images/JSJK201907016_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="143">分析表2可以得出以下结论:</p>
                </div>
                <div class="p1">
                    <p id="144"> (1) 对比模型2和模型1, 注意力机制以引入少量计算量为代价, 使模型的性能得到大幅度的提升。模型的训练时长增加10%左右, <i>BLEU</i>提高2.28。</p>
                </div>
                <div class="p1">
                    <p id="145"> (2) 对比模型3和模型2, 以字符+NER (Named Entity Recognition) 处理中文句子, 使模型的参数规模和计算量都有显著降低, 性能有一定提高。模型训练时长减少20%左右, <i>BLEU</i>提高0.4左右。</p>
                </div>
                <div class="p1">
                    <p id="146"> (3) 对比模型4和模型3, 选择不同的嵌入层参数初始化方法, 对模型训练时长无影响。在中小型数据集 (2 MB大小的中英句子对) 上, 在中文端使用随机初始化的方法, 最后得到的模型性能优于使用GloVe初始化的方法, <i>BLEU</i>提高0.5左右;在大规模数据集上不同的嵌入层参数初始化方法无明显差异。</p>
                </div>
                <div class="p1">
                    <p id="147"> (4) 对比模型5和模型4, 本文中提出的转换层结构在引入一定计算量的同时, 使模型性能有较为显著的提升。模型的训练时长增加20%左右, 模型<i>BLEU</i>值提高0.7～1.0。</p>
                </div>
                <div class="p1">
                    <p id="148"> (5) 对比模型5和模型2, 使用本文提出的3种改进方法, 在不同规模的中英双语数据集上, 与现有的基于注意力机制的seq2seq模型在模型复杂度和训练时长上基本持平, 在评价指标BLEU上有较为显著的提升, 提升幅度在1～2 <i>BLEU</i>之间。</p>
                </div>
                <h3 id="149" name="149" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="150">本文对神经机器翻译中的seq2seq模型的结构进行了改进, 针对中文特点, 对文本预处理和词嵌入参数初始化方法进行了相应的改进, 实验验证在英汉翻译任务中获得了更好的性能表现。在下一步的工作中将尝试不同结构的转换层, 在其它语言上进行实验, 测试转换层的效果, 在更大规模的数据集上进行实验, 后期可以针对未登录词进行优化。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="224" type="formula" href="images/JSJK201907016_22400.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">肖新凤</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="226" type="formula" href="images/JSJK201907016_22600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">李石君</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="228" type="formula" href="images/JSJK201907016_22800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">余伟</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="170">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A phrase-based joint probability model for statistical machine translation">

                                <b>[1]</b> Marcu D, Wong W.A phrase-based, joint probability model for statistical machine translation[C]//Proc of the ACL-02 Conference on Empirical Methods in Natural Language Processing, 2002:133-139.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 Bengio Y, Ducharme R, Vincent P, et al.A neural probabilistic language model[J].Journal of Machine Learning Research, 2003, 3 (6) :1137-1155.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decoding with Large-scale Neural Language Models Improves Translation">

                                <b>[3]</b> Vaswani A, Zhao Y, Fossum V, et al.Decoding with large-scale neural language models improves translation[C]//Proc of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013:1387-1392.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning phrase representations using RNN encoder-decoder for statistical machine translation">

                                <b>[4]</b> Cho K, van Merrienboer B, Gulcehre C, et al.Learning phrase representations using RNN encoder-decoder for statistical machine translation[C]//Proc of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014:1724-1734.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequence to sequence learning with neural networks">

                                <b>[5]</b> Sutskever I, Vinyals O, Le Q V.Sequence to sequence learning with neural networks[C]//Proc of the 27th Conference on Neural Information Processing Systems, 2014:3104-3112.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural machine translation by jointly learning to align and translate">

                                <b>[6]</b> Bahdanau D, Cho K, Bengio Y.Neural machine translation by jointly learning to align and translate[J].arXiv preprint arXiv:1409.0473v1, 2014.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 Luong M T, Pham H, Manning C D.Effective approaches to attention-based neural machine translation[C]//Proc of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015:1412-1421.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention is All You Need">

                                <b>[8]</b> Vaswani A, Shazeer N, Parmar N, et al.Attention is all you need[C]//Proc of the 31st Conference on Neural Information Processing Systems, 2017:5998-6008.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Bengio Y, Simard P, Frasconi P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the difficulty of training recurrent neural networks">

                                <b>[10]</b> Pascanu R, Mikolov T, Bengio Y.On the difficulty of training recurrent neural networks[C]//Proc of International Conference on Machine Learning, 2013:1310-1318.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel data tools and interfaces in OPUS">

                                <b>[11]</b> Tiedemann J O.Parallel data, tools and interfaces in OPUS[C]//Proc of the 8th International Conference on Language Resources and Evaluation 2012, 2012:2214-2218.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UM-corpus A large English-Chinese parallel corpus for statistical machine translation">

                                <b>[12]</b> Tian L, Wong D F, Chao L S, et al.UM-corpus:A large English-Chinese parallel corpus for statistical machine translation[C]//Proc of the 9th International Conference on Lan- guage Resources and Evaluation 2014, 2014:1837-1842.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The United Nations Parallel Corpus">

                                <b>[13]</b> Ziemski M, Junczys-Dowmunt M, Pouliquen B.The united nations parallel corpus v1.0[C]//Proc of the 11th International Conference on Language Resources and Evaluation 2016, 2016:3530-3534.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Han Dong-xu, Chang Bao-bao.Approaches to domain adaptive Chinese segmentation model[J].Chinese Journal of Computers, 2015, 38 (2) :272-281. (in Chinese) 
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">

                                <b>[15]</b> Mikolov T, Chen K, Corrado G, et al.Efficient estimation of word representations in vector space[J].arXiv preprint arXiv:1301.3781v3, 2013.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">

                                <b>[16]</b> Pennington J, Socher R, Manning C D.Glove:Global vectors for word representation[C]//Proc of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014:1532-1543.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201502005&amp;v=MjIwNDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N21XcjNPTHo3QmRyRzRIOVRNclk5RllZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 韩冬煦, 常宝宝.中文分词模型的领域适应性方法[J].计算机学报, 2015, 38 (2) :272-281.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201907016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201907016&amp;v=MDE3MjRSTE9lWmVSbUZ5N21XcjNQTHo3QlpiRzRIOWpNcUk5RVlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
