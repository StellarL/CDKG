<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132365953936250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201908016%26RESULT%3d1%26SIGN%3dROwNzncduLxmmTaeX3TsvQmO6bc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908016&amp;v=MDI1NDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkwzTUx6N0JaYkc0SDlqTXA0OUVZb1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="&lt;b&gt;2 卷积神经网络原理&lt;/b&gt; "><b>2 卷积神经网络原理</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;2.1 卷积神经网络&lt;/b&gt;"><b>2.1 卷积神经网络</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;2.2 卷积层&lt;/b&gt;"><b>2.2 卷积层</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;2.3 池化层&lt;/b&gt;"><b>2.3 池化层</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;2.4 全连接层和输出层&lt;/b&gt;"><b>2.4 全连接层和输出层</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;2.5 接收域及接收域的计算&lt;/b&gt;"><b>2.5 接收域及接收域的计算</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="&lt;b&gt;3 改进的3D CNN方法&lt;/b&gt; "><b>3 改进的3D CNN方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="&lt;b&gt;3.1 构建改进的3D CNN网络模型&lt;/b&gt;"><b>3.1 构建改进的3D CNN网络模型</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;3.2 卷积和膨胀卷积&lt;/b&gt;"><b>3.2 卷积和膨胀卷积</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;3.3 残差连接&lt;/b&gt;"><b>3.3 残差连接</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="&lt;b&gt;4 实验验证&lt;/b&gt; "><b>4 实验验证</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#115" data-title="&lt;b&gt;4.1 实验数据及设置&lt;/b&gt;"><b>4.1 实验数据及设置</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;4.2 性能指标&lt;/b&gt;"><b>4.2 性能指标</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;4.3 实验结果与分析&lt;/b&gt;"><b>4.3 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#155" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="图1 LeNet-5模型框架">图1 LeNet-5模型框架</a></li>
                                                <li><a href="#94" data-title="图2 鼻咽癌分割的3D CNN网络框架">图2 鼻咽癌分割的3D CNN网络框架</a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表1 改进的3D CNN网络结构参数配置&lt;/b&gt;"><b>表1 改进的3D CNN网络结构参数配置</b></a></li>
                                                <li><a href="#110" data-title="图3 残差学习示意图">图3 残差学习示意图</a></li>
                                                <li><a href="#112" data-title="图4 改进的3D CNN框架所有路径的接收域的分布">图4 改进的3D CNN框架所有路径的接收域的分布</a></li>
                                                <li><a href="#117" data-title="图5 从DICOM文件提取CT图像以及标签">图5 从DICOM文件提取CT图像以及标签</a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表2 模型不同层数的性能比较&lt;/b&gt;"><b>表2 模型不同层数的性能比较</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表3 20层的改进的3D CNN与20层其他方法的性能比较&lt;/b&gt; %"><b>表3 20层的改进的3D CNN与20层其他方法的性能比较</b> %</a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表4 18层的改进的3D CNN与18层其他方法的性能比较&lt;/b&gt;"><b>表4 18层的改进的3D CNN与18层其他方法的性能比较</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;表5 16层的改进的3D CNN与16层其他方法的性能比较&lt;/b&gt;"><b>表5 16层的改进的3D CNN与16层其他方法的性能比较</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表6 14层的改进的3D CNN与14层其他方法的性能比较&lt;/b&gt;"><b>表6 14层的改进的3D CNN与14层其他方法的性能比较</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表7 8层的改进的3D CNN与8层其他方法的性能比较&lt;/b&gt;"><b>表7 8层的改进的3D CNN与8层其他方法的性能比较</b></a></li>
                                                <li><a href="#152" data-title="图6 4种方法在2个病人的CT图像的最终分割">图6 4种方法在2个病人的CT图像的最终分割</a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表8 不同方法的综合评估&lt;/b&gt;"><b>表8 不同方法的综合评估</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="190">


                                    <a id="bibliography_1" title=" Feng B J, Huang W, Shugart Y Y, et al.Genome-wide scan for familial nasopharyngeal carcinoma reveals evidence of linkage to chromosome 4[J].Nature Genetics, 2002, 31 (4) :395-399." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Genome-wide scan for familial nasopharyngeal carcinoma reveals evidence of linkage to chromosome 4">
                                        <b>[1]</b>
                                         Feng B J, Huang W, Shugart Y Y, et al.Genome-wide scan for familial nasopharyngeal carcinoma reveals evidence of linkage to chromosome 4[J].Nature Genetics, 2002, 31 (4) :395-399.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     Peng Tian-qiang, Li Fang.Image retrieval based on deep convolutional neural networks and binary hashing learning[J].Jounal of Electronics &amp;amp; Information Technology, 2016, 38 (8) :2068-2075. (in Chinese) </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     Ou Xin-yu, Wu Jia, Zhu Heng, et al.Image hashing retrieval method based on deep self-learning[J].Computer Engineering &amp;amp; Science, 2015, 37 (12) :2386-2392. (in Chinese) </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_4" title=" Chen Wei-hong, An Ji-yao, Li Ren-fa, et al.Review on deep-learning-based cognitive computing[J].Acta Automatica Sinica, 2017, 43 (11) :1886-1897. (in Chinese) " target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201711002&amp;v=MDA3OTFOcm85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWTDNNS0NMZlliRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Chen Wei-hong, An Ji-yao, Li Ren-fa, et al.Review on deep-learning-based cognitive computing[J].Acta Automatica Sinica, 2017, 43 (11) :1886-1897. (in Chinese) 
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_5" title=" Shelhamer E, Long J, Darrell T, et al.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 79 (10) :1337-1342." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic">
                                        <b>[5]</b>
                                         Shelhamer E, Long J, Darrell T, et al.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 79 (10) :1337-1342.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_6" title=" Chen H, Dou Q, Wang X, et al.3D fully convolutional networks for intervertebral disc localization and segmentation[C]//Proc of International Conference on Medical Imaging and Virtual Reality, 2016:375-382." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D fully convolutional networks for intervertebral disc localization and segmentation">
                                        <b>[6]</b>
                                         Chen H, Dou Q, Wang X, et al.3D fully convolutional networks for intervertebral disc localization and segmentation[C]//Proc of International Conference on Medical Imaging and Virtual Reality, 2016:375-382.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_7" title=" Kamnitsas K, Ledig C, Newcombe V F, et al.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J].Medical Image Analysis, 2016, 36 (10) :61-78." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES75987FC868EC3E9704EAA1312A88E06D&amp;v=MjE4NjI3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaHdyaTR3cXc9TmlmT2ZiUzlGOW5MMmZ4Tll1TjZmMzlNeGhFVDdrb01PWDdoclJCRWNicmhSYnpyQ09OdkZTaVdXcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Kamnitsas K, Ledig C, Newcombe V F, et al.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J].Medical Image Analysis, 2016, 36 (10) :61-78.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_8" title=" &#199;i&#231;ek &#214;, Abdulkadir A, Lienkamp S S, et al.3D U-Net:learning dense volumetric segmentation from sparse annotation[J].MICCAI, 2016, 990 (5) :424-432." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D U-Net:learning dense volumetric segmentation from sparse annotation">
                                        <b>[8]</b>
                                         &#199;i&#231;ek &#214;, Abdulkadir A, Lienkamp S S, et al.3D U-Net:learning dense volumetric segmentation from sparse annotation[J].MICCAI, 2016, 990 (5) :424-432.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_9" title=" Milletari F, Navab N, Ahmadi S A.et al.V-Net:Fully convolutional neural networks for volumetric medical image segmentation[C]//Proc of the 4th International Conference on 3D Vision, 2016:565-571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=V-Net:fully convolutional neural networks for volumetric medical image segmentation">
                                        <b>[9]</b>
                                         Milletari F, Navab N, Ahmadi S A.et al.V-Net:Fully convolutional neural networks for volumetric medical image segmentation[C]//Proc of the 4th International Conference on 3D Vision, 2016:565-571.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_10" title=" Chen L C, Papandreou G, Kokkinos I, et al.DeepLab:Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2018, 40 (4) :834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[10]</b>
                                         Chen L C, Papandreou G, Kokkinos I, et al.DeepLab:Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2018, 40 (4) :834-848.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_11" title=" He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2015:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[11]</b>
                                         He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2015:770-778.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_12" title=" He K, Zhang X, Ren S, Sun, et al.Identity mappings in deep residual networks[C]//Proc of European Conference on Computer Vision, 2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[12]</b>
                                         He K, Zhang X, Ren S, Sun, et al.Identity mappings in deep residual networks[C]//Proc of European Conference on Computer Vision, 2016:630-645.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_13" title=" Veit A, Wilber M, Belongie S, et al.Residual Networks are exponential ensembles of relatively shallow networks[J].Advances in Neural Information Processing Systems, 2016, 98 (5) :60-69." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Residual Networks are Exponential Ensembles of Relatively Shallow Networks.&amp;quot;">
                                        <b>[13]</b>
                                         Veit A, Wilber M, Belongie S, et al.Residual Networks are exponential ensembles of relatively shallow networks[J].Advances in Neural Information Processing Systems, 2016, 98 (5) :60-69.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_14" title=" Li W, Wang G, Fidon L, et al.On the compactness, efficiency, and representation of 3D convolutional networks:Brain parcellation as a pretext task[C]//Proc of International Conference on Information Processing in Medical Imaging, 2017:348-360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the Compactness,Efficiency,and Representation of 3D Convolutional Networks:Brain Parcellation as a Pretext Task">
                                        <b>[14]</b>
                                         Li W, Wang G, Fidon L, et al.On the compactness, efficiency, and representation of 3D convolutional networks:Brain parcellation as a pretext task[C]//Proc of International Conference on Information Processing in Medical Imaging, 2017:348-360.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_15" title=" Zhou J, Chong V, Lim T K, et al.MRI tumor segmentation for nasopharyngeal carcinoma using knowledge based fuzzy clustering[J].International Journal of Information, 2002, 8 (2) :36-45." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MRI tumor segmentation for nasopharyngeal carcinoma using knowledge-based fuzzy clustering">
                                        <b>[15]</b>
                                         Zhou J, Chong V, Lim T K, et al.MRI tumor segmentation for nasopharyngeal carcinoma using knowledge based fuzzy clustering[J].International Journal of Information, 2002, 8 (2) :36-45.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_16" title=" Ritthipravat P, Tatanun C, Bhongmakapat T, et al.Automatic segmentation of nasopharyngeal carcinoma from CT images[C]//Proc of International Conference on Biomedical Engineering and Informatics, 2008:18-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic segmentation of nasopharyngeal carcinoma from CT images">
                                        <b>[16]</b>
                                         Ritthipravat P, Tatanun C, Bhongmakapat T, et al.Automatic segmentation of nasopharyngeal carcinoma from CT images[C]//Proc of International Conference on Biomedical Engineering and Informatics, 2008:18-22.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_17" >
                                        <b>[17]</b>
                                     Hong Rong-rong, Ye Shao-zhen.Segmentation of nasopharyngeal MRI medical image based on improved region growing[J].Journal of Fuzhou University (Natural Science Edition) , 2014, 42 (5) :683-687. (in Chinese) </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                     Lecun Y, Bottou L, Bengio Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.</a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_19" >
                                        <b>[19]</b>
                                     Chang Liang, Deng Xiao-ming, Zhou Ming-quan, et al.Convolutional neural networks in image understanding[J].Acta Automatica Sinica, 2016, 42 (9) :1300-1312. (in Chinese) </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_20" title=" Ioffe S, Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of International Conference on Machine Learning, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[20]</b>
                                         Ioffe S, Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of International Conference on Machine Learning, 2015:448-456.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_2" title=" 彭天强, 粟芳.基于深度卷积神经网络和二进制哈希学习的图像检索方法[J].电子与信息学报, 2016, 38 (8) :2068-2075." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201608034&amp;v=MDE1MTFNcDQ5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWTDNNSVRmU2RyRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         彭天强, 粟芳.基于深度卷积神经网络和二进制哈希学习的图像检索方法[J].电子与信息学报, 2016, 38 (8) :2068-2075.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_3" title=" 欧新宇, 伍嘉, 朱恒, 等.基于深度自学习的图像哈希检索方法[J].计算机工程与科学, 2015, 37 (12) :2386-2392." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201512029&amp;v=MDcxNzVxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWTDNNTHo3QlpiRzRIOVROclk5SGJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         欧新宇, 伍嘉, 朱恒, 等.基于深度自学习的图像哈希检索方法[J].计算机工程与科学, 2015, 37 (12) :2386-2392.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     陈伟宏, 安吉尧, 李仁发, 等.深度学习认知计算综述[J].自动化学报, 2017, 43 (11) :1886-1897.</a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_17" title=" 洪容容, 叶少珍.基于改进的区域生长鼻咽癌MR医学图像分割[J].福州大学学报 (自然科学版) , 2014, 42 (5) :683-687." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FZDZ201405006&amp;v=MTg2Njc0SDlYTXFvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkwzTUl6ZlBkTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         洪容容, 叶少珍.基于改进的区域生长鼻咽癌MR医学图像分割[J].福州大学学报 (自然科学版) , 2014, 42 (5) :683-687.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_19" title=" 常亮, 邓小明, 周明全, 等.图像理解中的卷积神经网络[J].自动化学报, 2016, 42 (9) :1300-1312." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201609002&amp;v=MDA0ODNmWWJHNEg5Zk1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bFZMM01LQ0w=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         常亮, 邓小明, 周明全, 等.图像理解中的卷积神经网络[J].自动化学报, 2016, 42 (9) :1300-1312.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(08),1444-1452 DOI:10.3969/j.issn.1007-130X.2019.08.015            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于3D CNN的鼻咽癌CT图像分割</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%82%96%E9%93%B6%E7%87%95&amp;code=42704880&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肖银燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%85%A8%E6%83%A0%E6%95%8F&amp;code=07431865&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全惠敏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B9%96%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0060047&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">湖南大学电气与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>鼻咽癌CT图像分割是鼻咽癌诊断和治疗的先行任务, 然而, 由于鼻咽癌细胞的外形多样、灰度不均匀、边界模糊、病变形状复杂等因素使得分割难以准确。针对这一问题, 提出了一种基于三维深度卷积神经网络的鼻咽癌CT图像分割方法, 三维深度卷积神经网络框架的前5层采用卷积核为3<sup>3</sup>的普通卷积, 中间6层采用空洞率为2的膨胀卷积, 后6层采用空洞率为4的膨胀卷积, 每2个卷积层之间有一个残差连接, 最后利用Softmax函数对每个像素点进行分类。膨胀卷积有助于得到精确的密集预测和沿物体边界的精细分割图, 残差连接使深度卷积神经网络中的信息传播平滑, 并能提高训练速度。实验结果表明, 在鼻咽癌CT图像分割中该方法与其他主流方法相比有更好的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%BC%BB%E5%92%BD%E7%99%8C%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鼻咽癌图像分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%86%A8%E8%83%80%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">膨胀卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">残差连接;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    肖银燕 (1995-) , 女, 湖南娄底人, 硕士生, 研究方向为深度学习。E-mail:1409108667@qq.com通信地址:410082湖南省长沙市湖南大学电气与信息工程学院;
                                </span>
                                <span>
                                    全惠敏 (1971-) , 女, 湖北钟祥人, 博士, 副教授, 研究方向为信号与信息处理。E-mail:315191163@qq.com通信地址:410082湖南省长沙市湖南大学电气与信息工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61671204);</span>
                                <span>湖南省重点研发计划 (2016WK2001);</span>
                    </p>
            </div>
                    <h1><b>A nasopharyngeal carcinoma CT image segmentation method based on 3D CNNs</b></h1>
                    <h2>
                    <span>XIAO Yin-yan</span>
                    <span>QUN Hui-min</span>
            </h2>
                    <h2>
                    <span>College of Electrical and Information Engineering, Hunan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Nasopharyngeal carcinoma computed tomography (CT) image segmentation is an essential task for diagnosis and treatments of nasopharyngeal carcinoma. However, nasopharyngeal carcinoma cells have various shapes, uneven gray scales, fuzzy boundaries, and complicated shapes of lesion cells, so it is difficult to accurately segment the image. In order to solve this problem, we propose a nasopharyngeal carcinoma CT image segmentation method based on three-dimensional convolutional neural networks (3 D CNNs) . In our three-dimensional deep convolutional neural network framework, ordinary convolutions with 3<sup>3</sup> convolution kernel are employed in the first 5 layers, the dilated convolutions with a dilation factor of 2 are employed in the middle 6 layers, and the dilated convolutions with adilation factor of 4 are employed in the last 6 layers. The residual connection is used between every two convolutional layers, and the softmax function is used to classify pixels. Dilated convolutions help to obtain accurate density prediction and fine segmentation maps along object boundaries. Residual connections smooth the information propagation in the deep convolutional neural network and improve the training speed. Experimental results show that the proposed method has better performance than other mainstream methods for nasopharyngeal carcinoma CT image segmentation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=nasopharyngeal%20carcinoma%20image%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">nasopharyngeal carcinoma image segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dilated%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dilated convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=residual%20connection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">residual connection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XIAO Yin-yan, born in 1995, MS candidate, her research interest includes deep learning.Address:College of Electrical and Information Engineering, Hunan University, Changsha 410082, Hunan, P.R.China;
                                </span>
                                <span>
                                    QUN Hui-min, born in 1971, PhD, associate professor, her research interest includes signal &amp;information processing.Address:College of Electrical and Information Engineering, Hunan University, Changsha 410082, Hunan, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-05-07</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="54">癌症一直以来是导致死亡率增加的主要疾病, 而鼻咽癌NPC (NasoPharyngeal Carcinoma) 是中国南方以及东南亚地区高发的恶性肿瘤<citation id="240" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。利用计算机断层扫描CT (Computerised Tomography) 图像准确地分割鼻咽癌肿瘤是成功实施放疗的首要任务。鼻咽癌肿瘤的人工手动分割不仅耗时费力, 而且其分割结果好坏取决于专家是否具有解剖学知识和丰富的经验, 分割结果人为主观性强, 再现性差。因此, 医学界迫切需要自动和半自动肿瘤分割算法以辅助医生进行肿瘤判断。</p>
                </div>
                <div class="p1">
                    <p id="55">目前, 深度学习作为一种新的机器学习方法已成功应用在计算机视觉领域, 并取得了一些突破性成果<citation id="247" type="reference"><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><link href="196" rel="bibliography" /><link href="230" rel="bibliography" /><link href="232" rel="bibliography" /><link href="234" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。一些研究已成功运用了卷积神经网络CNN (Convolutional Neural Network) 方法来解决医学图像分割问题, 但应用于鼻咽癌CT图像分割的相对较少。将一般的CNN结构用于CT图像分割存在以下缺陷:一方面, 计算效率低;另一方面, 图像块的大小限制了接收域RF (Receptive Field) 的大小。图像块提取的特征仅包含用于像素分类的局部特征, 因此直接利用一般的CNN结构来进行图像分割的性能是有限的。在这种情况下, Shelhamer等<citation id="241" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了一个全卷积神经网络FCN (Full Convolutional Neural network) 的图像分割方法, 与传统CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类不同, FCN可以接受任意尺寸的输入图像, 经过一系列交替的卷积层和池化层提取不同的特征图之后, 使用1×1卷积层代替全连接层, 然后通过反卷积层对最后一个卷积层的特征图进行上采样, 使它恢复到与输入图像相同的尺寸, 从而可以对每个像素都产生一个预测, 同时保留了原始输入图像中的空间信息, 这样的像素级分类避免了图像块之间的重叠而导致重复卷积计算的问题。最近, 出现了很多遵循完全卷积的先卷积再反卷积的网络体系结构来分割3D图像<citation id="248" type="reference"><link href="200" rel="bibliography" /><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><link href="206" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。如香港中文大学 Chen等<citation id="242" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>将FCN扩展到3D FCN, 提出的基于 3D FCN 的定位和分割方法, 在 2015 脊椎盘定位和分割挑战赛中取得了非常好的成绩。Kamnitsas等<citation id="243" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了一个新的11层深度的三维卷积神经网络框架, 提高了脑肿瘤分割的准确度。在文献<citation id="249" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>中, 3D FCN、3D U-Net、V-Net等网络框架对中间特征图进行下采样, 显著地降低了图像的空间分辨率。比如3D U-net网络大量采用每个维度的步幅为2, 核的大小为2×2×2的最大池化, 每个最大池将前一层的特征图减小到其空间分辨率的1/8, 然后又需要用上采样层恢复前一层的空间分辨率, 这个过程增加了大量的参数, 带来了额外的计算代价<citation id="244" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。为了解决下采样使特征图的分辨率降低和网络参数增多的问题, Chen等<citation id="245" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>用膨胀卷积核 (Dilated Convolutions) 代替传统的卷积核来进行语义图像分割。膨胀卷积的优点是可以在空间分辨率高的图像块上得到特征图, 并且可以任意改变接收域的大小。膨胀卷积可得到准确的密集预测分类和边缘清晰的分割图。同时, 在文献<citation id="250" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>中, 3D FCN、3D U-Net、V-Net等网络的层数很多, 意味着提取到的特征很丰富, 但是深度网络不容易优化。而残差网络可以在不改变网络表达力和复杂度的情况下, 改变损失函数势能的状况, 从而更容易使优化达到最小值<citation id="251" type="reference"><link href="210" rel="bibliography" /><link href="212" rel="bibliography" /><link href="214" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。Li等<citation id="246" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>设计了高分辨率和紧凑的网络体系结构, 实现了准确分类大脑的160个神经解剖结构, 并且提高了运算效率。</p>
                </div>
                <div class="p1">
                    <p id="56">近年来, 应用于鼻咽癌的自动或半自动医学图像分割算法基本上都是基于分类器和聚类方法以及区域生长。Zhou等<citation id="252" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了知识模糊聚类方法分割鼻咽癌MRI (Magnetic Resonance Imaging) , 即通过对称性、连通性和聚类中心等空间知识以及数学形态学来区分肿瘤区域中癌细胞和正常细胞。虽然该聚类方法对分布均匀的影像分割效果良好, 同时不需要提取和选择特征, 但需要初始化参数 (比如类数、聚类中心) , 参数初始化的好坏直接影响分割的效果。Ritthipravat等<citation id="253" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>则提出基于区域生长技术来分割鼻咽癌CT图像, 采用概率地图初始化种子以实现图像的自动分割, 但该方法存在只能设置一个初始种子点、候选种子不是最优以及过分割的情况。洪容容等<citation id="254" type="reference"><link href="222" rel="bibliography" /><link href="236" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">17</a>]</sup></citation>提出基于改进区域生长的分割鼻咽癌图像的方法, 该方法在区域生长的自动分割的基础上, 利用概率矩阵自动生成初始种子, 有效分割鼻咽癌MR医学图像, 提高了分割精度, 但概率函数的引入使得实现分割的计算时间较长。</p>
                </div>
                <div class="p1">
                    <p id="57">为了自动准确地分割鼻咽癌, 本文提出基于3D CNN的方法来分割鼻咽癌CT图像, 构建一个有20层的深度卷积网络, 前5层采用卷积核为3<sup>3 </sup>的普通卷积, 中间6层采用空洞率 (Diated Rate) 为2的膨胀卷积, 后6层采用空洞率为4的膨胀卷积, 每2个卷积层之间会有一个残差连接, 最后利用Softmax函数对每个像素点进行分类。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag"><b>2 卷积神经网络原理</b></h3>
                <h4 class="anchor-tag" id="59" name="59"><b>2.1 卷积神经网络</b></h4>
                <div class="p1">
                    <p id="60">卷积神经网络是一种多层神经网络, 它的网络框架是由交替的卷积层和池化层构成, 其中比较著名的网络结构是卷积神经网络LeNet-5, 如图1所示。卷积神经网络LeNet-5是由Lecun等<citation id="255" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出的, 由卷积层、下采样层、全连接层和输出层构成, 通过卷积层的权值共享和最大池化层充分提取了手写数字的局部性特征, 同时也保证了手写数字的位移不变性, 提高了手写数字的识别准确度。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908016_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 LeNet-5模型框架" src="Detail/GetImg?filename=images/JSJK201908016_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 LeNet-5模型框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908016_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Framework of the LeNet-5 model</p>

                </div>
                <h4 class="anchor-tag" id="62" name="62"><b>2.2 卷积层</b></h4>
                <div class="p1">
                    <p id="63">卷积神经网络有多个卷积层, 每个卷积层由多个特征图组成。在卷积层, 上一层的特征图被一个可学习的卷积核进行卷积, 然后使用一个激活函数进行非线性变换, 就可以得到输出特征图<citation id="256" type="reference"><link href="226" rel="bibliography" /><link href="238" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">19</a>]</sup></citation>。这个过程形式化为:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">F</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mi>Η</mi></msub></mrow></munderover><mi mathvariant="bold-italic">F</mi></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mi>i</mi></msubsup><mo>*</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>l</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">其中, <b><i>F</i></b><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow><mi>i</mi></msubsup></mrow></math></mathml>表示第<i>l</i>-1层的第<i>i</i>个特征图, <b><i>b</i></b><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup></mrow></math></mathml>表示第<i>l</i>层的偏置, <i>M</i><sub><i>l</i>-1</sub>为第<i>l</i>-1层的特征映射数目, <b><i>W</i></b><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mrow></math></mathml>表示第<i>l</i>层第<i>j</i>个特征图与上一层第<i>i</i>个特征图之间的一组卷积核 (又称滤波器) 。卷积神经网络第<i>l</i>层的第<i>i</i> 个特征图<b><i>F</i></b><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>i</mi></msubsup></mrow></math></mathml>由上层多个特征图经过滤波器卷积后输入到激活函数的输出构成, 那么卷积层可以通过不同的可学习的滤波器 (卷积核) 对图像提取不同的特征<citation id="257" type="reference"><link href="226" rel="bibliography" /><link href="238" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>2.3 池化层</b></h4>
                <div class="p1">
                    <p id="71">池化层的表达方式如下:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Y</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">β</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mi>d</mi><mi>o</mi><mi>w</mi><mi>n</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><msubsup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow><mi>j</mi></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中, <b><i>Y</i></b><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow><mi>j</mi></msubsup></mrow></math></mathml>是第<i>l</i>-1层第<i>j</i>个特征图;<b><i>b</i></b><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup></mrow></math></mathml>表示第<i>l</i>层的偏置;<i>β</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup></mrow></math></mathml>表示下采样层的权重系数;<i>down</i> () 表示一个池化函数, 它通过滑动窗口方法将输入特征图<b><i>Y</i></b><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow><mi>j</mi></msubsup></mrow></math></mathml>划分为多个不重叠的<i>n</i>×<i>n</i>图像块, 然后对每个图像块内的像素求和、求均值或最大值, 于是输出图像在2个维度上都缩小了<i>n</i>倍。第<i>j</i>个特征图<b><i>Y</i></b><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup></mrow></math></mathml>由<b><i>Y</i></b><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow><mi>j</mi></msubsup></mrow></math></mathml>池化后通过一个激活函数<i>f</i> () 得到。非线性激活函数的作用是增强网络结构的泛化性, 每个输出图都有一个对应的乘性偏置和一个加性偏置<b><i>b<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup></mrow></math></mathml></i></b><citation id="258" type="reference"><link href="226" rel="bibliography" /><link href="238" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>2.4 全连接层和输出层</b></h4>
                <div class="p1">
                    <p id="82">全连接层相当于对输入进行矩阵乘法运算或一个特征空间变换, 可以把上一层有用的信息提取整合, 再加上激活函数的非线性映射。全连接层的表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi mathvariant="bold-italic">X</mi></mstyle><msubsup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow><mi>j</mi></msubsup><mo>*</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>l</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mi>l</mi><mi>j</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <b><i>W</i></b><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mrow></math></mathml>表示第<i>l</i>层的第<i>j</i>个输出和第<i>l</i>-1层的第<i>i</i>个输入的连接权重。</p>
                </div>
                <div class="p1">
                    <p id="86">输出层常常采用Softmax函数来计算输入图像属于各类别的概率, 然后将输入图像归类到所属概率最大的类别, 实现分类目的。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.5 接收域及接收域的计算</b></h4>
                <div class="p1">
                    <p id="88">在卷积神经网络中, 接收域的定义是卷积神经网络每一层输出的特征图上的像素点在原始图像上映射的区域大小。对单层卷积网络来说, 其特征图上每个特征点对应原图上的感受野大小等于卷积层卷积核的大小。对多层卷积网络来说, 可由此逐层往回反馈, 通过反复迭代获得原始输入图像中感受野大小, 即后面深层的卷积层接收域大小与之前所有网络层的卷积核大小和步长有关。计算接收域的公式为:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mi>l</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mi>s</mi><msub><mrow></mrow><mi>l</mi></msub><mo>+</mo><mi>k</mi><msub><mrow></mrow><mi>l</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <i>r</i><sub><i>l</i></sub>表示为第<i>l</i>层感受野大小, <i>k</i><sub><i>l</i></sub>表示第<i>l</i>层卷积核大小, <i>s</i><sub><i>l</i></sub>是之前所有层步长的乘积。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag"><b>3 改进的3D CNN方法</b></h3>
                <h4 class="anchor-tag" id="92" name="92"><b>3.1 构建改进的3D CNN网络模型</b></h4>
                <div class="p1">
                    <p id="93">为了获得比较好的分割效果, 本文设计了一个有20层的3D维深度卷积网络, 前5层采用卷积核为3<sup>3 </sup>的普通卷积, 中间6层采用空洞率为2的膨胀卷积, 后6层采用空洞率为4的膨胀卷积, 每2个卷积层之间会有一个残差连接, 通过卷积层的学习最终得到64个特征图, 最后将学习到的特征图输入到Softmax函数进行二分类, 网络框架如图2所示。表1给出了具有18层的3D深度卷积网络的结构参数, 实验验证了18层的网络框架有助于得到更好的分割效果。利用膨胀卷积核代替传统卷积神经的标准卷积核, 可以通过改变膨胀率<i>R</i>来任意改变接收域的大小, 减少参数数量, 可以在空间分辨率高的图像块上提取更好的特征图。卷积核为3<sup>3 </sup>的普通卷积的前7层用于捕捉低级图像特征, 如边缘和角落。在随后的卷积层中, 核心膨胀了2～4倍。这些具有扩大内核的较深层有利于提取中高级图像特征。每2个卷积层之间搭建一个残差单元, 在每个残差单元, 每个卷积层与一个元素修正线性单元ReLU (Rectified Linear Unit) 层和一个批量归一化层 (Batch Normalisation Layer ) 相关联<citation id="259" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。ReLU、批量归一化和卷积层按预激活顺序排列<citation id="260" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。残差连接使深度卷积神经网络中的信息传播平滑, 并提高训练速度。网络可以进行端到端的训练。在训练阶段, 网络的输入图像块是96×96×96, 最终的Softmax层为每个96×96×96图像块的类别标签提供分类评分。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908016_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 鼻咽癌分割的3D CNN网络框架" src="Detail/GetImg?filename=images/JSJK201908016_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 鼻咽癌分割的3D CNN网络框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908016_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 3D CNN network framework for nasopharyngeal carcinoma segmentation</p>

                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表1 改进的3D CNN网络结构参数配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Parameter configuration of the improved 3D CNN network structure</b></p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td><br />层数</td><td>层类型</td><td>特征图大小FM</td><td>核大小</td></tr><tr><td><br />1</td><td>卷积</td><td>16×16×16</td><td>3×3×3</td></tr><tr><td><br />2～5</td><td>卷积</td><td>16×16×16</td><td>3×3×3</td></tr><tr><td><br />6～11</td><td>膨胀卷积</td><td>32×32×32 (<i>R</i>=2) </td><td>3×3×3</td></tr><tr><td><br />12～17</td><td>膨胀卷积</td><td>64×32×32 (<i>R</i>=4) </td><td>3×3×3</td></tr><tr><td><br />18</td><td>Softmax</td><td>2×2×2</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>3.2 卷积和膨胀卷积</b></h4>
                <div class="p1">
                    <p id="97">很多遵循完全卷积的先卷积再反卷积的网络体系结构, 为了进一步扩大接受范围以捕获大图像上下文, 对中间特征图进行反卷积 (下采样) , 不仅降低了特征图的空间分辨率, 而且增加了参数量<citation id="261" type="reference"><link href="200" rel="bibliography" /><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><link href="206" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。本文提出的方法采用膨胀卷积取代传统卷积神经网络的下采样, 是因为膨胀卷积可以以高空间分辨率提取特征, 并且相对地降低了参数量。膨胀卷积可用于产生精确的密集预测和沿物体边界的详细分割图。</p>
                </div>
                <div class="p1">
                    <p id="98">如果使用膨胀率为<i>R</i>的卷积核对<i>M</i>个通道的输入特征图<b><i>I</i></b>进行上采样, 那么生成的输出特征通道<i>O</i>可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>z</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></munderover><mi>w</mi></mstyle></mrow></mstyle></mrow></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>m</mi></mrow></msub><mi>Ι</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>i</mi><mi>r</mi><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mi>y</mi><mo>+</mo><mi>j</mi><mi>r</mi><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false"> (</mo><mi>z</mi><mo>+</mo><mi>k</mi><mi>r</mi><mo stretchy="false">) </mo><mo>, </mo><mi>m</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">其中, 索引元组 (<i>x</i>, <i>y</i>, <i>z</i>) 表示空间位置;内核<b><i>w</i></b>由3<sup>3</sup>×<i>M</i>个可训练参数组成。式 (5) 中的膨胀卷积与标准的3×3×3卷积具有相同的参数数量。它保留了空间分辨率并提供了体素为 (2<i>R</i>+1) <sup>3</sup>的接收域。膨胀率<i>R</i>为1的膨胀卷积相当于标准3×3×3卷积。在实验中, 本文采用拆分和合并策略来实现3D膨胀卷积, 以方便利用现有的卷积代码例程<citation id="262" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="101">另外, 为了保持相对较少的参数, 本文提出的方法选择只有3<sup>3</sup>个参数的小型3D卷积核用于所有卷积层。这个最小内核可以表示中心体素在所有方向上的3D特征。尽管具有5×5×5体素的卷积核能得到与堆叠2层3×3×3体素卷积核相同的接受域, 与前者相比, 后者参数约减少57%。使用较小的内核是间接地在卷积神经网络上施加更多的正则化, 同时实现相同的接受域。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>3.3 残差连接</b></h4>
                <div class="p1">
                    <p id="103">本文提出的改进的3D CNN层数相对较多, 意味着提取到的特征很丰富, 但深度网络不容易优化。因此, 本文采用残差网络以解决网络优化问题, 残差网络可以在不改变网络表达力和复杂度的情况下, 改变损失函数势能的状况, 从而更容易使优化达到最小值<citation id="263" type="reference"><link href="210" rel="bibliography" /><link href="212" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="104">图3是残差学习模块的示意图, 残差学习模块可以作为神经网络的一部分或多个部分。假设第<i>k</i>层的输入为<b><i>x</i></b><sub><i>k</i></sub>, 那么第<i>k</i>+1层的函数映射 (即输出<b><i>H</i></b><sub><i>k</i>+1</sub> (<b><i>x</i></b><sub><i>k</i></sub>) ) 为:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi mathvariant="bold-italic">F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中, <b><i>F</i></b> (<b><i>x</i></b><sub><i>k</i></sub>, <b><i>w</i></b><sub><i>k</i></sub>) 表示块中具有非线性函数的路径。如果叠加残差块, 那么最后一层<i>L</i>的输出可以表示为<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>L</mi></msub><mo>=</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>k</mi></mrow><mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi mathvariant="bold-italic">F</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>。式 (6) 中的快捷连接<citation id="264" type="reference"><link href="210" rel="bibliography" /><link href="212" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>并未引入新的参数或计算复杂度。在式 (6) 中, <b><i>x</i></b><sub><i>k</i></sub>和<b><i>F</i></b> (<b><i>x</i></b><sub><i>k</i></sub>, <b><i>w</i></b><sub><i>k</i></sub>) 的维度必须相同, 如果维度不相同的话 (当输入或输出通道的数量发生改变时) , 有2种策略: (1) 快捷连接 (图3中虚线所示) 依然采用恒等映射, 对于维数增加带来的空缺元素补零, 这种策略将引入新的参数; (2) 在快捷连接上作一个线性投影来匹配维数 ( (即式 (7) 所示的投影捷径, 通过1×1的卷积层实现) 。</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>+</mo><mi mathvariant="bold-italic">F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">文献<citation id="265" type="reference">[<a class="sup">11</a>]</citation>的实验已经表明了投影快捷连接并不是退化问题的关键, 而且工程上采用补零的方法更易实现。因此, 为了降低时间复杂度, 本文采用的是恒等映射。残差连接的关键思想是创建等效身份映射连接以绕过网络中的参数化层。残差块的输入通过加法直接合并到输出中。已经证明, 残差连接可以使信息传播平滑并提高训练速度<citation id="266" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908016_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 残差学习示意图" src="Detail/GetImg?filename=images/JSJK201908016_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 残差学习示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908016_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"><i>Figure</i> 3 <i>Diagram of residual learning</i></p>

                </div>
                <div class="p1">
                    <p id="111">在文献<citation id="267" type="reference">[<a class="sup">13</a>]</citation>中, <i>Veit</i>等认为深度残差网络可以看作许多路径组成的神经网络模型的一个集合, 集合具有<i>n</i>个残差块的网络 (即具有2<sup><i>n</i></sup>个独特路径) 。为了验证这些路径的独立性, Veit等通过移除残差网络中的某些路径的实验得到结论:深度残差网络是多个神经网络模型的集合, 逐渐移除路径, 对结果的影响是平滑的;深度残差网络的路径不同, 深度不同;路径的分布服从二项分布, 意味着一个100层的网络实际大概深度只有50层。没有残差连接的传统卷积神经网络的接受域是固定的。本文提出的具有9个残差块的神经网络模型的最大接收域是87×87×87。根据残差网络展开后的结构图, 本文神经网络模型由29个独特的路径组成, 图4显示了本文改进的3D CNN网络所有路径的感受域的分布, 本文神经网络模型的接收域的范围是[3×3×3, 87×87×87], 服从二项分布。本文带有残差块的神经网络模型与其他不带有残差块的神经网络模型相比, 具有更多的路径以及不同范围的接收域, 比如文献<citation id="268" type="reference">[<a class="sup">6</a>]</citation>中的Deepmedic模型只有2条路径, 而且接收域也只有2个, 分别是17×17×17和42×42×42。因此, 当加入<i>n</i>个残差块进行训练时, 神经网络模型就具有不同的路径, 可以在许多的不同接收域中学习不同的特征, 获得更好的分割结果。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908016_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 改进的3D CNN框架所有路径的接收域的分布" src="Detail/GetImg?filename=images/JSJK201908016_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 改进的3D CNN框架所有路径的接收域的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908016_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Distribution of receptive fields for  all paths of the improved 3D CNN framework</p>

                </div>
                <h3 id="113" name="113" class="anchor-tag"><b>4 实验验证</b></h3>
                <div class="p1">
                    <p id="114">为了验证本文所提方法的性能, 本节从2个方面进行分析, (1) 通过分析模型层数的影响分析了改进的3D CNN模型的优化结构; (2) 与已有的医疗图像分割算法比较了分割性能。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>4.1 实验数据及设置</b></h4>
                <div class="p1">
                    <p id="116">实验数据来自于中南大学湘雅医学院附属湖南省肿瘤医院PET/CT 中心, 通过GE Discovery ST PET/CT 扫描系统采集鼻咽癌 PET/CT 影像数据。CT图像尺寸为512×512×100, 层厚为3.27 mm, 像素分辨率为0.9766 mm×0.9766 mm。图5显示了从DICOM文件提取图像的过程。首先, 解析CT DICOM文件, 从中获得CT 图像的矩阵数据以及有用的信息 (比如病人的扫描时间、姓名等) , 然后解析 RS DICOM文件, 从中获得医生在CT 图像上对肿瘤勾画的数据, 从而得到标签。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908016_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 从DICOM文件提取CT图像以及标签" src="Detail/GetImg?filename=images/JSJK201908016_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 从DICOM文件提取CT图像以及标签  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908016_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Extracting CT images and labels from DICOM files</p>

                </div>
                <div class="p1">
                    <p id="118">本文实验数据总共包括44组来自不同病人的临床CT图像, 训练集包括36个病人的CT图像 (3 600幅) , 验证集包括4个病人的CT图像 (400幅) , 测试集包括4个病人的CT图像 (400幅) 。训练采用基于直方图的尺度标准化方法对图像进行归一化处理。 在归一化过程中引入随机化, 在图像强度最小值和平均强度之间随机选择一个前景阈值进行图像数据扩充 (Data Augmentation) 。每幅图像进一步标准化为零均值和单位标准偏差。在CT图像上随机采样大小为96×96×96的子图像块作为深度卷积网络的输入, 并且在[-20°, 20°]的随机角度对子图像块进行旋转和在[0.9, 1.0]对子图像块随机缩放。在不改变图像类别的情况下, 对图像进行旋转, 进行一定程度的缩放以增加数据量, 还能提高模型的泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="119">采用标准正态分布<i>N</i> (0, 1) 来初始化卷积层中的所有参数;整个网络使用Adam方法来优化参数;在所有情况下, 学习率<i>lr</i>设置为0.000 1, 超参数步长<i>β</i><sub>1</sub>设置为0.9, <i>β</i><sub>2</sub>设置为0.99;batch size设置为36;训练周期设置为1 000次。</p>
                </div>
                <div class="p1">
                    <p id="120">实验基于64位的Linux操作系统和NVIDIA GTX Geforce 940 GPU, 采用的软件有Python 3.6, 采用的深度学习框架为Tensorflow。</p>
                </div>
                <div class="p1">
                    <p id="121">为了验证所提方法的有效性, 与4个方法进行了实验比较, 分别是3D FCN、Deepmedic、3D U-Net和V-Net。3D FCN是完全卷积的经典语义图像分割方法, Deepmedic、3D U-Net、V-Net是遵循完全卷积的先卷积再反卷积的网络体系结构的主流医疗图像分割方法。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>4.2 性能指标</b></h4>
                <div class="p1">
                    <p id="123">为了评估方法的性能, 本文使用<i>DSC</i> (Dice Similarity Coefficient) 、准确度 (<i>Precision</i>) 和召回率 (<i>Recall</i>) 作为评价指标, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>D</mi><mi>S</mi><mi>C</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo>×</mo><mo stretchy="false">|</mo><mi>A</mi><mstyle displaystyle="true"><mo>∩</mo><mi>B</mi></mstyle><mo stretchy="false">|</mo><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">|</mo><mo>+</mo><mo stretchy="false">|</mo><mi>B</mi><mo stretchy="false">|</mo></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mtd></mtr><mtr><mtd><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mtd></mtr><mtr><mtd><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">其中, <i>A</i>是标准结果, <i>B</i>是分割结果。<i>TP</i>、<i>FP</i>、<i>FN</i>分别是真实性、假阳性和假阴性。<i>DSC</i>、<i>Precision</i>和<i>Recall</i>值越大, 算法分割区域与标准结果区域之间的重叠区域越大。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>4.3 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="127">改进的3D CNN的层数由卷积层数量、膨胀卷积层数量和堆叠的残差单元数决定。一般地, 图像分割模型层数越多, 其分割准确度越高, 但训练模型的计算复杂度越大。因此, 权衡模型性能和计算量至关重要。本实验分析了随层数变化改进的3D CNN模型的性能。在3D FCN、Deepmedic、V-Net和改进的3D CNN 4个方法中, 进行了以下几种方式的参数设置:</p>
                </div>
                <div class="p1">
                    <p id="128">方式1:普通卷积层数为7层, 空洞率为2的膨胀卷积层为6层, 空洞率为4的膨胀卷积层为6层, 此时总层数为20层;</p>
                </div>
                <div class="p1">
                    <p id="129">方式2:普通卷积层数为5层, 空洞率为2的膨胀卷积层为6层, 空洞率为4的膨胀卷积层为6层, 此时总层数为18层;</p>
                </div>
                <div class="p1">
                    <p id="130">方式3:普通卷积层数为3层, 空洞率为2的膨胀卷积层为6层, 空洞率为4的膨胀卷积层为6层, 此时总层数为16层;</p>
                </div>
                <div class="p1">
                    <p id="131">方式4:普通卷积层数为7层, 空洞率为2的膨胀卷积层为4层, 空洞率为4的膨胀卷积层为6层, 此时总层数为18层;</p>
                </div>
                <div class="p1">
                    <p id="132">方式5:普通卷积层数为7层, 空洞率为2的膨胀卷积层为2层, 空洞率为4的膨胀卷积层为6层, 此时总层数为16层;</p>
                </div>
                <div class="p1">
                    <p id="133">方式6:普通卷积层数为7层, 空洞率为2的膨胀卷积层为6层, 空洞率为4的膨胀卷积层为4层, 此时总层数为18层;</p>
                </div>
                <div class="p1">
                    <p id="134">方式7:普通卷积层数为7层, 空洞率为2的膨胀卷积层为6层, 空洞率为4的膨胀卷积层为2层, 此时总层数为16层;</p>
                </div>
                <div class="p1">
                    <p id="135">方式8:普通卷积层数为5层, 空洞率为2的膨胀卷积层为4层, 空洞率为4的膨胀卷积层为4层, 此时总层数为14层;</p>
                </div>
                <div class="p1">
                    <p id="136">方式9:普通卷积层数为3层, 空洞率为2的膨胀卷积层为2层, 空洞率为4的膨胀卷积层为2层, 此时总层数为8层。</p>
                </div>
                <div class="p1">
                    <p id="137">实验对20、18、16、8层改进的3D CNN、3D FCN、deepMedic和V-Net对鼻咽癌CT图像分割的效果进行了对比。表2给出了不同层数改进的3D CNN性能比较。如表2所示, 随着层数的增加, 改进的3D CNN的<i>DSC</i>上升。当改进的3D CNN模型总层数为18层和普通卷积层为5层时, 改进的3D CNN的<i>DSC</i>为79.53%, 这比20层的<i>DSC</i>高出了10%以上。当膨胀卷积层数不变 (都为6层) , 普通卷积层分别为5层时, <i>DSC</i>、准确度和召回率都达到了最高值。这表示在保证鼻咽癌CT图像的分割准确度较高的情况下可以采用普通卷积层为5层, 空洞率为2、4的膨胀卷积层都为6层, 最高层为Softmax层的较简单模型。当模型总层数都为18层时, 6层的膨胀卷积层比4层的膨胀卷积层的<i>DSC</i>要高出4.00%以上, 也就是说, 膨胀卷积能提高鼻咽癌CT图像分割的准确度。</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表2 模型不同层数的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Performance comparison of different layers of the model</b></p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />层数设置</td><td>总层数</td><td><i>DSC</i><br />/%</td><td><i>Precision</i><br />/%</td><td><i>Recall</i><br />/%</td></tr><tr><td><br />普通卷积:7层;<i>R</i>=2:6层;<i>R</i>=4:6层</td><td>20</td><td>76.65</td><td>76.23</td><td>77.26</td></tr><tr><td><br />普通卷积:5层;<i>R</i>=2:6层;<i>R</i>=4:6层</td><td>18</td><td>79.53</td><td>78.35</td><td>77.23</td></tr><tr><td><br />普通卷积:3层;<i>R</i>=2:6层;<i>R</i>=4:6层</td><td>16</td><td>74.34</td><td>74.01</td><td>74.31</td></tr><tr><td><br />普通卷积:7层;<i>R</i>=2:4层;<i>R</i>=4:6层</td><td>18</td><td>74.25</td><td>73.81</td><td>72.39</td></tr><tr><td><br />普通卷积:7层;<i>R</i>=2:2层;<i>R</i>=4:6层</td><td>16</td><td>72.11</td><td>71.30</td><td>72.53</td></tr><tr><td><br />普通卷积:7层;<i>R</i>=2:6层;<i>R</i>=4:4层</td><td>18</td><td>74.49</td><td>73.56</td><td>72.71</td></tr><tr><td><br />普通卷积:7层;<i>R</i>=2:6层;<i>R</i>=4:2层</td><td>16</td><td>71.98</td><td>71.90</td><td>71.13</td></tr><tr><td><br />普通卷积:5层;<i>R</i>=2:4层;<i>R</i>=4:4层</td><td>14</td><td>66.17</td><td>65.89</td><td>66.35</td></tr><tr><td><br />普通卷积:3层;<i>R</i>=2:2层;<i>R</i>=4:2层</td><td>8</td><td>50.04</td><td>50.00</td><td>50.73</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139">表3给出了总层数为20层 (普通卷积:7层;<i>R</i>=2:6层;<i>R</i>=4:6层) 的改进的3D CNN与20层的其他方法的性能比较结果。</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表3 20层的改进的3D CNN与20层其他方法的性能比较</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Performance comparison of 20-layer 3D CNN and 20-layer other methods</b></p>
                    <p class="img_note"></p>
                    <table id="140" border="1"><tr><td><br />方法</td><td><i>DSC</i></td><td><i>Precision</i></td><td><i>Recall</i></td></tr><tr><td><br />Deepmedic</td><td>59.52</td><td>59.64</td><td>59.74</td></tr><tr><td><br />3D FCN</td><td>68.18</td><td>67.47</td><td>70.76</td></tr><tr><td><br />V-Net</td><td>70.76</td><td>71.94</td><td>70.85</td></tr><tr><td><br />改进的3D CNN</td><td>76.65</td><td>76.23</td><td>77.26</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="141">表4给出了总层数为18层 (普通卷积:5层;<i>R</i>=2:6层;<i>R</i>=4:6层) 的改进的3D CNN与18层的其他方法的性能比较结果。</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表4 18层的改进的3D CNN与18层其他方法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Performance comparison of 18-layer 3D CNN and 18-layer other methods</b></p>
                    <p class="img_note">%</p>
                    <table id="142" border="1"><tr><td><br />方法</td><td><i>DSC</i></td><td><i>Precision</i></td><td><i>Recall</i></td></tr><tr><td><br />Deepmedic</td><td>59.89</td><td>59.54</td><td>59.53</td></tr><tr><td><br />3D FCN</td><td>67.64</td><td>67.75</td><td>67.77</td></tr><tr><td><br />V-Net</td><td>70.85</td><td>70.64</td><td>70.82</td></tr><tr><td><br />改进的3D CNN</td><td>79.53</td><td>78.35</td><td>77.23</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="143">表5给出了总层数为16层 (普通卷积:3层;<i>R</i>=2:6层;<i>R</i>=4:6层) 的改进的3D CNN与16层的其他方法的性能比较结果。</p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit"><b>表5 16层的改进的3D CNN与16层其他方法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Performance comparison of 16-layer 3D CNN and 16-layer other methods</b></p>
                    <p class="img_note">%</p>
                    <table id="144" border="1"><tr><td><br />方法</td><td><i>DSC</i></td><td><i>Precision</i></td><td><i>Recall</i></td></tr><tr><td><br />Deepmedic</td><td>57.74</td><td>57.81</td><td>57.12</td></tr><tr><td><br />3D FCN</td><td>67.39</td><td>67.08</td><td>67.41</td></tr><tr><td><br />V-Net</td><td>69.46</td><td>69.75</td><td>69.61</td></tr><tr><td><br />改进的3D CNN</td><td>74.34</td><td>74.01</td><td>74.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="145">表6给出了总层数为14层 (普通卷积:5层;<i>R</i>=2:4层;<i>R</i>=4:4层) 的改进的3D CNN与14层的其他方法的性能比较结果。</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表6 14层的改进的3D CNN与14层其他方法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 Performance comparison of 14-layer 3D CNN and 14-layer other methods</b></p>
                    <p class="img_note">%</p>
                    <table id="146" border="1"><tr><td><br />方法</td><td><i>DSC</i></td><td><i>Precision</i></td><td><i>Recall</i></td></tr><tr><td><br />Deepmedic</td><td>53.52</td><td>54.27</td><td>53.10</td></tr><tr><td><br />3D FCN</td><td>65.46</td><td>64.67</td><td>63.43</td></tr><tr><td><br />V-Net</td><td>69.00</td><td>69.24</td><td>69.51</td></tr><tr><td><br />改进的3D CNN</td><td>66.17</td><td>65.89</td><td>66.35</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="147">表7给出了总层数为8层 (普通卷积:3层;<i>R</i>=2:2层;<i>R</i>=4:2层) 的改进的3D CNN与8层的其他方法的性能比较结果。</p>
                </div>
                <div class="p1">
                    <p id="148">从表3～表7可知, 当层数大于等于16层的情况下, 改进的3D CNN比3D FCN, Deepmedic和V-net有更高的<i>DSC</i>, 其<i>DSC</i>达到79.53%。当层数小于16层的情况下, 在<i>DSC</i>、<i>Precision</i>和<i>Recall</i>方面, V-net方法优于3D FCN, Deepmedic和改进的3D CNN方法, 其<i>DSC</i>达到69.00%。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表7 8层的改进的3D CNN与8层其他方法的性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 7 Performance comparison of 8-layer 3D CNN and 8-layer other methods</b></p>
                    <p class="img_note">%</p>
                    <table id="149" border="1"><tr><td><br />方法</td><td><i>DSC</i></td><td><i>Precision</i></td><td><i>Recall</i></td></tr><tr><td><br />Deepmedic</td><td>51.87</td><td>51.58</td><td>51.75</td></tr><tr><td><br />3D FCN</td><td>52.54</td><td>51.54</td><td>52.67</td></tr><tr><td><br />V-Net</td><td>50.87</td><td>50.75</td><td>50.64</td></tr><tr><td><br />改进的3D CNN</td><td>50.04</td><td>50.00</td><td>50.73</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="150">对表3～表7进行比较可知, 当普通卷积层为5层, 空洞率为2、4的膨胀卷积层都为6层, 改进的3D CNN的性能达到最佳, 其<i>DSC</i>值为79.53%, <i>Precision</i>值为78.35%, <i>Recall</i>值为77.23%, 与其他方法相比, 有更高的分割准确度。</p>
                </div>
                <div class="p1">
                    <p id="151">为了验证本文提出方法的性能, 本节与主流的医疗图像分割方法进行了对比分析, 它们是Deepmedic、3D U-Net和V-Net, 实验选取4种模型层数为20层。图6为Deepmedic、3D U-Net、V-Net和本文方法的最终分割结果与标签的对比。从图6中可以看出, Deepmedic 和V-Net 方法倾向于欠分割, 将正常组织错误归类为肿瘤, 3D-Unet也有同样的问题, 同时出现分割有空洞的问题, 而本文改进的3D CNN模型具有较好的鼻咽癌边界特征, 分割效果较好, 并且优于其它3种方法。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908016_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种方法在2个病人的CT图像的最终分割" src="Detail/GetImg?filename=images/JSJK201908016_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种方法在2个病人的CT图像的最终分割  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908016_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Final segmentation of the CT images of  two patients by the four different methods</p>

                </div>
                <div class="p1">
                    <p id="153">表8给出了改进的3D CNN与Deepmedic、3D U-Net、V-Net用以分割的实验结果。从表4可以看出, 20层的改进的3D CNN模型的<i>DSC</i>值为76.65%, 比3D U-Net高出了16.07%。同时, 3D CNN模型在准确度、<i>DSC</i>和召回率方面的性能优于目前主流的卷积神经网络方法, 说明膨胀卷积和残差连接确实能提高准确度, 一方面, 由于膨胀卷积可以解决传统的深度卷积神经网络参数量多和特征图的空间分辨率低的问题;另一方面, 残差连接可以提高训练速度, 同时可以结合不同范围的接收域来学习到更好的特征。</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表8 不同方法的综合评估</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 8 Comprehensive assessment of different methods</b></p>
                    <p class="img_note">%</p>
                    <table id="154" border="1"><tr><td><br />方法</td><td><i>DSC</i></td><td><i>Precision</i></td><td><i>Recall</i></td></tr><tr><td><br />改进的3D CNN</td><td>76.65</td><td>76.23</td><td>77.26</td></tr><tr><td><br />Deepmedic</td><td>68.83</td><td>68.18</td><td>67.47</td></tr><tr><td><br />3D U-Net</td><td>60.34</td><td>59.01</td><td>58.89</td></tr><tr><td><br />V-Net</td><td>70.76</td><td>71.94</td><td>70.85</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="155" name="155" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="156">本文提出了一种结合膨胀卷积和残差连接的卷积神经网络方法, 用于从CT 图像中分割鼻咽癌病灶。该方法采用膨胀卷积解决了传统的深度卷积神经网络参数量多和特征图的空间分辨率低的问题, 以及采用残差连接结合不同范围的接收域来学习到更好的特征, 以更好地分割NPC病灶, 并且残差连接使深度卷积神经网络中的信息传播平滑, 并提高了训练速度。用4个鼻咽癌患者CT图像进行了测试, 以验证训练有效的分割模型的性能。实验表明, 在鼻咽癌CT图像分割中, 基于3D CNN的分割方法比其它方法具有更高的准确性和更好的鲁棒性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="188" type="formula" href="images/JSJK201908016_18800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">肖银燕</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="189" type="formula" href="images/JSJK201908016_18900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">全惠敏</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="190">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Genome-wide scan for familial nasopharyngeal carcinoma reveals evidence of linkage to chromosome 4">

                                <b>[1]</b> Feng B J, Huang W, Shugart Y Y, et al.Genome-wide scan for familial nasopharyngeal carcinoma reveals evidence of linkage to chromosome 4[J].Nature Genetics, 2002, 31 (4) :395-399.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 Peng Tian-qiang, Li Fang.Image retrieval based on deep convolutional neural networks and binary hashing learning[J].Jounal of Electronics &amp; Information Technology, 2016, 38 (8) :2068-2075. (in Chinese) 
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 Ou Xin-yu, Wu Jia, Zhu Heng, et al.Image hashing retrieval method based on deep self-learning[J].Computer Engineering &amp; Science, 2015, 37 (12) :2386-2392. (in Chinese) 
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201711002&amp;v=MTIyMjhaZVJtRnk3bFZMM01LQ0xmWWJHNEg5Yk5ybzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Chen Wei-hong, An Ji-yao, Li Ren-fa, et al.Review on deep-learning-based cognitive computing[J].Acta Automatica Sinica, 2017, 43 (11) :1886-1897. (in Chinese) 
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic">

                                <b>[5]</b> Shelhamer E, Long J, Darrell T, et al.Fully convolutional networks for semantic segmentation [J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 79 (10) :1337-1342.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D fully convolutional networks for intervertebral disc localization and segmentation">

                                <b>[6]</b> Chen H, Dou Q, Wang X, et al.3D fully convolutional networks for intervertebral disc localization and segmentation[C]//Proc of International Conference on Medical Imaging and Virtual Reality, 2016:375-382.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES75987FC868EC3E9704EAA1312A88E06D&amp;v=MDUzMDFyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOQmh3cmk0d3F3PU5pZk9mYlM5RjluTDJmeE5ZdU42ZjM5TXhoRVQ3a29NT1g3aHJSQkVjYnJoUmJ6ckNPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Kamnitsas K, Ledig C, Newcombe V F, et al.Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation[J].Medical Image Analysis, 2016, 36 (10) :61-78.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D U-Net:learning dense volumetric segmentation from sparse annotation">

                                <b>[8]</b> Çiçek Ö, Abdulkadir A, Lienkamp S S, et al.3D U-Net:learning dense volumetric segmentation from sparse annotation[J].MICCAI, 2016, 990 (5) :424-432.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=V-Net:fully convolutional neural networks for volumetric medical image segmentation">

                                <b>[9]</b> Milletari F, Navab N, Ahmadi S A.et al.V-Net:Fully convolutional neural networks for volumetric medical image segmentation[C]//Proc of the 4th International Conference on 3D Vision, 2016:565-571.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[10]</b> Chen L C, Papandreou G, Kokkinos I, et al.DeepLab:Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2018, 40 (4) :834-848.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[11]</b> He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition, 2015:770-778.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[12]</b> He K, Zhang X, Ren S, Sun, et al.Identity mappings in deep residual networks[C]//Proc of European Conference on Computer Vision, 2016:630-645.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Residual Networks are Exponential Ensembles of Relatively Shallow Networks.&amp;quot;">

                                <b>[13]</b> Veit A, Wilber M, Belongie S, et al.Residual Networks are exponential ensembles of relatively shallow networks[J].Advances in Neural Information Processing Systems, 2016, 98 (5) :60-69.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the Compactness,Efficiency,and Representation of 3D Convolutional Networks:Brain Parcellation as a Pretext Task">

                                <b>[14]</b> Li W, Wang G, Fidon L, et al.On the compactness, efficiency, and representation of 3D convolutional networks:Brain parcellation as a pretext task[C]//Proc of International Conference on Information Processing in Medical Imaging, 2017:348-360.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MRI tumor segmentation for nasopharyngeal carcinoma using knowledge-based fuzzy clustering">

                                <b>[15]</b> Zhou J, Chong V, Lim T K, et al.MRI tumor segmentation for nasopharyngeal carcinoma using knowledge based fuzzy clustering[J].International Journal of Information, 2002, 8 (2) :36-45.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic segmentation of nasopharyngeal carcinoma from CT images">

                                <b>[16]</b> Ritthipravat P, Tatanun C, Bhongmakapat T, et al.Automatic segmentation of nasopharyngeal carcinoma from CT images[C]//Proc of International Conference on Biomedical Engineering and Informatics, 2008:18-22.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_17" >
                                    <b>[17]</b>
                                 Hong Rong-rong, Ye Shao-zhen.Segmentation of nasopharyngeal MRI medical image based on improved region growing[J].Journal of Fuzhou University (Natural Science Edition) , 2014, 42 (5) :683-687. (in Chinese) 
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                 Lecun Y, Bottou L, Bengio Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_19" >
                                    <b>[19]</b>
                                 Chang Liang, Deng Xiao-ming, Zhou Ming-quan, et al.Convolutional neural networks in image understanding[J].Acta Automatica Sinica, 2016, 42 (9) :1300-1312. (in Chinese) 
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[20]</b> Ioffe S, Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of International Conference on Machine Learning, 2015:448-456.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201608034&amp;v=Mjg1MDFUZlNkckc0SDlmTXA0OUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkwzTUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 彭天强, 粟芳.基于深度卷积神经网络和二进制哈希学习的图像检索方法[J].电子与信息学报, 2016, 38 (8) :2068-2075.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201512029&amp;v=MDA5MDF6N0JaYkc0SDlUTnJZOUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkwzTUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 欧新宇, 伍嘉, 朱恒, 等.基于深度自学习的图像哈希检索方法[J].计算机工程与科学, 2015, 37 (12) :2386-2392.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 陈伟宏, 安吉尧, 李仁发, 等.深度学习认知计算综述[J].自动化学报, 2017, 43 (11) :1886-1897.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FZDZ201405006&amp;v=MjI5MDJDVVJMT2VaZVJtRnk3bFZMM01JemZQZExHNEg5WE1xbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 洪容容, 叶少珍.基于改进的区域生长鼻咽癌MR医学图像分割[J].福州大学学报 (自然科学版) , 2014, 42 (5) :683-687.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201609002&amp;v=MjU2MTlNS0NMZlliRzRIOWZNcG85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2xWTDM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 常亮, 邓小明, 周明全, 等.图像理解中的卷积神经网络[J].自动化学报, 2016, 42 (9) :1300-1312.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201908016" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908016&amp;v=MDI1NDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkwzTUx6N0JaYkc0SDlqTXA0OUVZb1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
