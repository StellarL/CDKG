<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132340872686250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJK201911012%26RESULT%3d1%26SIGN%3dCp7I8DRjGd0wFrD0H7NvE0NebEo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201911012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911012&amp;v=MTE2NzlHRnJDVVJMT2VaZVJtRnkvbldyL0FMejdCWmJHNEg5ak5ybzlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#29" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;2 数据集&lt;/b&gt; "><b>2 数据集</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;3 基于条件生成对抗网络的皮肤分割&lt;/b&gt; "><b>3 基于条件生成对抗网络的皮肤分割</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;3.1 条件生成对抗网络&lt;/b&gt;"><b>3.1 条件生成对抗网络</b></a></li>
                                                <li><a href="#47" data-title="&lt;b&gt;3.2 多尺度特征融合网络&lt;/b&gt;"><b>3.2 多尺度特征融合网络</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;3.3 分割实验与结果&lt;/b&gt;"><b>3.3 分割实验与结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="&lt;b&gt;4 面色分级&lt;/b&gt; "><b>4 面色分级</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;4.1 局部二值模型&lt;/b&gt;"><b>4.1 局部二值模型</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;4.2 面色分类实验与结果&lt;/b&gt;"><b>4.2 面色分类实验与结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 数据集实例">图1 数据集实例</a></li>
                                                <li><a href="#46" data-title="图2 条件生成对抗网络结构">图2 条件生成对抗网络结构</a></li>
                                                <li><a href="#45" data-title="图3 多尺度特征融合网络结构">图3 多尺度特征融合网络结构</a></li>
                                                <li><a href="#52" data-title="图4 皮肤分割流程图">图4 皮肤分割流程图</a></li>
                                                <li><a href="#54" data-title="图5 分割结果">图5 分割结果</a></li>
                                                <li><a href="#56" data-title="图6 4种网络的分割结果">图6 4种网络的分割结果</a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;表1 4种网络分割结果的平均交并比&lt;/b&gt;"><b>表1 4种网络分割结果的平均交并比</b></a></li>
                                                <li><a href="#63" data-title="图7 LBP算子">图7 LBP算子</a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;表2 特征1作为面色特征的分类结果&lt;/b&gt;"><b>表2 特征1作为面色特征的分类结果</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表3 特征2作为面色特征的分类结果&lt;/b&gt;"><b>表3 特征2作为面色特征的分类结果</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;表4 特征3作为面色特征的分类结果&lt;/b&gt;"><b>表4 特征3作为面色特征的分类结果</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表5 特征4作为面色特征的分类结果&lt;/b&gt;"><b>表5 特征4作为面色特征的分类结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" UngureanuA S,Javidnia H,Costache C,et al.A review and comparative study of skin segmentation techniques for handheld imaging devices[C]//Proc of 2016 IEEE International Conference on Consumer Electronics (ICCE),2016:530-531." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A review and comparative study of skin segmentation techniques for handheld imaging devices">
                                        <b>[1]</b>
                                         UngureanuA S,Javidnia H,Costache C,et al.A review and comparative study of skin segmentation techniques for handheld imaging devices[C]//Proc of 2016 IEEE International Conference on Consumer Electronics (ICCE),2016:530-531.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Phung S L,Bouzerdoum A,Chai D.Skin segmentation using color pixel classification:Analysis and comparison[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2005,27(1):148-154." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Skin Segmentation Using Color Pixel Classification: Analysis and Comparison">
                                        <b>[2]</b>
                                         Phung S L,Bouzerdoum A,Chai D.Skin segmentation using color pixel classification:Analysis and comparison[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2005,27(1):148-154.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Yang Yun-cong.Research on the facial color recognition of traditional Chinese medicine based on image analysis technology[D].Beijing:Beijing University of Technology,2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on the facial color recognition of traditional Chinese medicine based on image analysis technology">
                                        <b>[3]</b>
                                         Yang Yun-cong.Research on the facial color recognition of traditional Chinese medicine based on image analysis technology[D].Beijing:Beijing University of Technology,2013.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Mao Hong-zhao.Diagnostic information extraction for face diagnosis in traditional Chinese medicine[D].Xiamen :Xiamen University,2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diagnostic information extraction for face diagnosis in traditional Chinese medicine">
                                        <b>[4]</b>
                                         Mao Hong-zhao.Diagnostic information extraction for face diagnosis in traditional Chinese medicine[D].Xiamen :Xiamen University,2007.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Goodfellow I,Pouget-Abadie J,Mirza M,et al.Generative adversarial nets[C]//Proc of Advances in Neural Information Processing Systems,2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[5]</b>
                                         Goodfellow I,Pouget-Abadie J,Mirza M,et al.Generative adversarial nets[C]//Proc of Advances in Neural Information Processing Systems,2014:2672-2680.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Mirza M,Osindero S.Conditional generative adversarial nets[J].Computer Science,2014,28(2):2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional Generative Adversarial Nets">
                                        <b>[6]</b>
                                         Mirza M,Osindero S.Conditional generative adversarial nets[J].Computer Science,2014,28(2):2672-2680.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[C]//Proc of International Conference on Learning Representations,2016:161-177." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">
                                        <b>[7]</b>
                                         Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[C]//Proc of International Conference on Learning Representations,2016:161-177.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Denton E L,Chintala S,Fergus R.Deep generative image models using a Laplacian pyramid of adversarial networks[C]//Proc of Advances in Neural Information Processing Systems,2015:1486-1494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep generative image models using a Laplacian pyramid of adversarial networks">
                                        <b>[8]</b>
                                         Denton E L,Chintala S,Fergus R.Deep generative image models using a Laplacian pyramid of adversarial networks[C]//Proc of Advances in Neural Information Processing Systems,2015:1486-1494.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[9]</b>
                                         He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[10]</b>
                                         Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,39(4):640-651.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Badrinarayanan V,Kendall A,Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for scene segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.附中文参考文献:</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     杨云聪.基于图像分析的中医面色识别方法研究[D].北京:北京工业大学,2013.</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_4" title=" 毛红朝.面向中医面诊的诊断信息提取—关键算法研究与实现[D].厦门:厦门大学,2007." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008058115.nh&amp;v=MDI3NDBGckNVUkxPZVplUm1GeS9uV3IvQVYxMjdGck85RnRETnFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         毛红朝.面向中医面诊的诊断信息提取—关键算法研究与实现[D].厦门:厦门大学,2007.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(11),1985-1990 DOI:10.3969/j.issn.1007-130X.2019.11.012            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>像素级的皮肤分割与面色分级</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E4%BB%8E%E4%B8%AD&amp;code=07064753&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴从中</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BE%AF%E5%9B%BD%E6%9D%BE&amp;code=42928921&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">侯国松</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%81%E6%AD%A3%E9%BE%99&amp;code=37446277&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">丁正龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E8%89%AF%E5%87%A4&amp;code=07064835&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许良凤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A9%B9%E6%9B%99&amp;code=07071116&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">詹曙</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0083575&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥工业大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BE%BD%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1699765&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安徽信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>皮肤是人体最大的器官,面色相对于人体其他生物属性具有更便捷、更稳定的特性。因此,设计一个完整有效的面色分级系统是非常有意义的。本文中,面色分级系统被分为皮肤分割和面色分级2部分。针对皮肤分割任务,在生成对抗网络框架下搭建了一个多尺度特征融合网络,相对于传统的语义分割网络,本文的分割模型充分地利用了每一层特征图的信息。在面色分级实验中,首先在归一化rgb、HSV和Lab颜色空间下使用1 000幅图像分别训练了支持向量机(SVM)和BP神经网络分类器,128幅皮肤图像被用作测试集,正确率在73%～76%;之后将颜色特征与皮肤区域纹理特征融合进行学习,使用SVM分类的正确率为85%,使用BP神经网络分类的正确率达到了91%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度特征融合网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9A%AE%E8%82%A4%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">皮肤分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%A2%E8%89%B2%E5%88%86%E7%BA%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">面色分级;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    吴从中(1964-),男,安徽安庆人,硕士,副教授,研究方向为信息与信号处理。E-mail:329161005@qq.com,通信地址:230601安徽省合肥市蜀山区合肥工业大学计算机与信息学院;
                                </span>
                                <span>
                                    侯国松(1994-),男,河北衡水人,硕士生,研究方向为图像语义分割。E-mail:Hgs29@mail.hfut.edu.cn,通信地址:230601安徽省合肥市蜀山区合肥工业大学计算机与信息学院;
                                </span>
                                <span>
                                    丁正龙(1988-),男,安徽芜湖人,硕士,讲师,研究方向为机器视觉和机电一体化。E-mail:leng8feng@126.com;
                                </span>
                                <span>
                                    许良凤(1967-),女,安徽合肥人,硕士,副教授,研究方向为无线通信、图像处理和人脸表情识别。E-mail:906110272@qq.com,通信地址:230601安徽省合肥市蜀山区合肥工业大学计算机与信息学院;
                                </span>
                                <span>
                                    詹曙(1968-),男,安徽合肥人,博士,教授,博士生导师,研究方向为医学影像分析和医学成像系统、三维人脸图像分析和识别、生物特征识别、计算机视觉和模式识别。E-mail:shu_zhan@hfut.edu.cn,通信地址:230601安徽省合肥市蜀山区合肥工业大学计算机与信息学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61371156);</span>
                    </p>
            </div>
                    <h1><b>Pixel-level skin segmentation and face color grading</b></h1>
                    <h2>
                    <span>WU Cong-zhong</span>
                    <span>HOU Guo-song</span>
                    <span>DING Zheng-long</span>
                    <span>XU Liang-feng</span>
                    <span>ZHAN Shu</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Information Engineering,Hefei University of Technology</span>
                    <span>Anhui Institute of Information Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Skin is the largest organ in the human body, and skin color is more convenient and stable than other biological properties of the human body. Therefore, it is very meaningful to design an effective skin color grading system. In this paper, the skin color grading system is divided into two parts: skin segmentation and skin color grading. For the skin segmentation,a multi-scale feature fusion network is built under the framework of the generative adversarial network. Compared with the traditional semantic segmentation networks, the proposed segmentation model makes full use of the information of each layer's feature map. In the face color grading experiment, the SVM classifier and the BP neural network are trained with 1 000 images in the normalized rgb, HSV, and Lab color spaces. 128 skin images are used as test sets, and the correct rate is between 73% and 76%. Then,the color features are combined with the LBP texture features of the skin region to do the learning. The correct rate of the SVM classifier is 85%, and the correct rate of the BP neural network is 91%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20feature%20fusion%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale feature fusion network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generative%20adversarial%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generative adversarial network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=skin%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">skin segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20color%20grading&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face color grading;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    WU Cong-zhong,born in 1964,MS,associate professor,his research interests include information and signal processing.Address:School of Computer Science and Information Engineering,Hefei University of Technology,Shushan District,Hefei 230601, Anhui,P.R.China;
                                </span>
                                <span>
                                    HOU Guo-song,born in 1994,MS candidate,his research interest includes image semantic segmentation.Address:School of Computer Science and Information Engineering,Hefei University of Technology,Shushan District,Hefei 230601, Anhui,P.R.China;
                                </span>
                                <span>
                                    DING Zheng-long,born in 1988,MS, lecturer,his research interests include machine vision,and mechatronics.;
                                </span>
                                <span>
                                    XU Liang-feng,born in 1967,MS,associate professor,her research interests include wireless communication,image processing,facial expression recognition.Address:School of Computer Science and Information Engineering,Hefei University of Technology,Shushan District,Hefei 230601, Anhui,P.R.China;
                                </span>
                                <span>
                                    ZHAN Shu,born in 1968,PhD,professor,PhD supervisor,his research interests include medical image analysis &amp;amp; medical imaging systems,3D face image analysis &amp;amp; recognition,biometrics,computer vision,and pattern recognition.Address:School of Computer Science and Information Engineering,Hefei University of Technology,Shushan District,Hefei 230601, Anhui,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-04-03</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="29" name="29" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="30">肤色是人体非常重要的一种属性,相较于人体其他属性,肤色更易获取、更加稳定,对于图像的旋转、缩放和遮挡都有很好的鲁棒性。本文通过皮肤分割和面色分级2个部分搭建一个完整的面色分级系统。</p>
                </div>
                <div class="p1">
                    <p id="31">皮肤分割是一项非常复杂的任务<citation id="78" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。皮肤的颜色域非常广,并且很容易被光照影响。因此,设计一个能够克服这些困难的皮肤分割系统是非常有意义的。在传统的皮肤分割模型中,研究人员在不同的颜色空间中利用皮肤数据建立模型来区分皮肤像素和非皮肤像素。非参数法和参数法<citation id="79" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>是2种主要的建立皮肤分割模型的方法。近年来随着深度学习的快速发展,越来越多的研究人员使用了深度卷积神经网络完成语义分割任务。与传统分割方法相比,深度神经网络在分割的准确性上有了很大的提升。本文在条件生成对抗网络框架下搭建了一个多尺度特征融合的网络进行皮肤分割。随着网络深度的不断加深,高维特征图中会丢失低维特征图中的一些信息,所以我们将不同尺度的特征图进行了融合,充分利用各个分辨率的特征图,并使用条件生成对抗网络的框架进行了训练,以此来提高皮肤分割准确性。</p>
                </div>
                <div class="p1">
                    <p id="32">相比于其他生物特征,面色不仅更加稳定且更容易获取。面色的准确识别对于面部追踪、表情识别、中医问诊等都有非常大的帮助。由于机器设备、光照变化和人类不同种族肤色等因素的影响,面色的准确分类非常具有挑战性。目前关于面色分类的研究较少,研究人员大多是在不同的颜色空间提取面色的颜色特征对分类器进行学习训练,杨云聪<citation id="80" type="reference"><link href="7" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">3</a>]</sup></citation>在Lab颜色空间对面色特征进行提取,然后使用颜色建模的模糊隶属度的计算方法,将基于距离的隶属度计算方法与基于颜色建模的面色识别方法相结合,确定训练样本的最终隶属度,最终将得到的模糊隶属度输入到支持向量机模型中,对提取到的面色特征进行分类。毛红朝<citation id="81" type="reference"><link href="9" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">4</a>]</sup></citation>在中医面诊中,提取了面色在各个颜色空间的颜色均值作为特征,然后使用支持向量机对提取到的面色特征进行分类。但是,因为颜色特征的区分性较弱,所以分类正确率有限。</p>
                </div>
                <div class="p1">
                    <p id="33">颜色空间对于面色的识别非常重要,不同的颜色空间对于面色识别的性能会有较大影响,数码摄影中最常用的是RGB颜色空间,但照度和色度信息在RGB颜色空间中混合,因此通常情况下,研究人员会将其从RGB颜色空间转换到其他颜色空间。本文中选用了归一化rgb颜色空间、HSV颜色空间和Lab颜色空间。归一化的rgb特征通过亮度的改变来缩小光照对于面色识别的影响;HSV特征是基于用户色彩感官的一种色彩模型,偏向于对颜色的表达;Lab特征具有很高的色彩分辨率,它通过数字的方式来描述人的视觉感官,Lab颜色空间更适合于较小的颜色测量和比较。</p>
                </div>
                <div class="p1">
                    <p id="34">生成对抗网络GAN(Generative Adversarial Networks)是一种非常有效的网络结构。Goodfellow等人<citation id="82" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>在2014年提出了生成对抗网络的结构,生成对抗网络主要包括2部分:生成网络和判别网络。生成网络会模拟训练数据的分布并且生成相似的样本,判别网络用来识别输入是真实的样本还是生成网络生成的样本。但是,原始的GAN存在很多问题,难以训练,许多研究人员对原始GAN进行了改进。同年,Mirza等人<citation id="83" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了条件生成对抗网络,将真实样本同时输入到生成网络和对抗网络作为约束条件。在之后的工作中,Radford等人<citation id="84" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>对原始生成对抗网络进行了一系列的改进,以确保生成对抗网络可以被稳定地训练,从而生成较为逼真的图像。Denton等人<citation id="85" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>使用拉普拉斯金字塔方法学习生成了更加精细的图像。</p>
                </div>
                <div class="p1">
                    <p id="35">本文主要创新点如下:</p>
                </div>
                <div class="p1">
                    <p id="36">(1)传统皮肤分割网络只利用了最低分辨率的特征图,这会导致信息的丢失。本文使用多尺度特征融合网络,充分利用了不同尺度特征图中的信息,提高了皮肤分割的准确性。</p>
                </div>
                <div class="p1">
                    <p id="37">(2)由于条件生成对抗网络具有强大的性能和较为简单的网络结构,因此本文在皮肤分割实验中使用条件生成对抗网络的结构,对于皮肤的准确分割也有很大帮助。</p>
                </div>
                <div class="p1">
                    <p id="38">(3)针对颜色特征区分性较弱的问题,本文将纹理特征与颜色特征相融合作为面色分级的特征。将获取的特征分别输入到SVM分类器和BP神经网络分类器中,得到了较好的分类结果。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>2 数据集</b></h3>
                <div class="p1">
                    <p id="40">本文使用的数据集是在标准光源下获得的人脸图像,共1 128幅。数据集中包含不同姿态、表情和遮挡物的人脸图像,同时含有相对应的皮肤区域的标签图像。数据集图像实例如图1所示。其中每对图像中左边的为原始图像,右边的为标签图像。人脸图像肤色等级被划分为深、中和浅3个等级。在实验中,本文使用了1 000幅图像作为训练集,对皮肤分割模型和面色分级模型进行训练,并使用128幅图像作为测试集对模型的性能进行测试。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 数据集实例" src="Detail/GetImg?filename=images/JSJK201911012_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 数据集实例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Data set examples</p>

                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>3 基于条件生成对抗网络的皮肤分割</b></h3>
                <h4 class="anchor-tag" id="43" name="43"><b>3.1 条件生成对抗网络</b></h4>
                <div class="p1">
                    <p id="44">与生成对抗网络不同,条件生成对抗网络将输入的图像作为限制条件指导网络的训练和学习。本文使用的条件生成对抗网络结构如图2所示。人脸图像被同时输入到生成网络和对抗网络中,这样生成对抗网络就可以有目标地进行学习。本文将皮肤分割模型作为生成器,输出人脸图像的分割结果。生成器的分割结果和标签图像被随机输入到判别器中进行识别分类,判别器将识别结果反馈到生成器,促进分割模型的训练。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 条件生成对抗网络结构" src="Detail/GetImg?filename=images/JSJK201911012_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 条件生成对抗网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Structure of condition generative adversarial nets</p>

                </div>
                <h4 class="anchor-tag" id="47" name="47"><b>3.2 多尺度特征融合网络</b></h4>
                <div class="p1">
                    <p id="48">在深度神经网络中,随着网络的加深,高维特征图会丢失低维特征图的一些信息,这些信息可能对像素的定位有很大帮助。因此,本文在条件生成对抗网络框架下搭建了一个多尺度特征融合的网络。这个网络将高维与低维的特征图进行了融合,充分利用不同尺度的特征图的信息,网络结构如图3所示。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 多尺度特征融合网络结构" src="Detail/GetImg?filename=images/JSJK201911012_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 多尺度特征融合网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Structure of multi-scale feature fusion network</p>

                </div>
                <div class="p1">
                    <p id="49">本文从预训练好的ResNet-101<citation id="86" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>网络中获得不同分辨率的特征图,然后通过卷积层和残差模块调整特征图权重,之后将最低分辨率特征图上采样与上层特征图融合,不断重复直至获得最后高分辨率特征图。最后通过一个softmax层得到预测图的score map,实现皮肤分割。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50"><b>3.3 分割实验与结果</b></h4>
                <div class="p1">
                    <p id="51">首先将人脸图像输入到ResNet-101中获得不同尺度的特征图,将获取到的1/4,1/8,1/16和1/32大小的特征图分别输入到条件生成对抗网络,通过多尺度特征融合网络对特征图进行学习融合,最终实现皮肤分割。实验流程图如图4所示。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 皮肤分割流程图" src="Detail/GetImg?filename=images/JSJK201911012_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 皮肤分割流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Flow chart of skin segmentation</p>

                </div>
                <div class="p1">
                    <p id="53">实验中使用了随机梯度下降法寻找参数,得到的最终分割结果如图5所示。图5a～图5c分别为原始人脸图像、皮肤分割结果与标签图像。从分割结果可以看出,本文使用的网络基本实现了皮肤的准确分割,与标签图像的差异并不大。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 分割结果" src="Detail/GetImg?filename=images/JSJK201911012_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Segmentation results</p>

                </div>
                <div class="p1">
                    <p id="55">同时,本文使用平均交并比<i>MIoU</i>(Mean Intersection over Union)作为评价指标,在相同训练集与测试集上对语义分割经典网络FCN<citation id="87" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和SegNet<citation id="88" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>进行了对比,同时和单独使用多尺度特征融合网络的结果进行了对比,实验结果如图6和表1所示。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 4种网络的分割结果" src="Detail/GetImg?filename=images/JSJK201911012_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 4种网络的分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Segmentation result with four networks</p>

                </div>
                <div class="area_img" id="57">
                    <p class="img_tit"><b>表1 4种网络分割结果的平均交并比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Mean Intersection over union of four networks</b></p>
                    <p class="img_note"></p>
                    <table id="57" border="1"><tr><td><br />Networks</td><td><i>MIoU</i>/%</td></tr><tr><td><br />FCN</td><td>60.3</td></tr><tr><td><br />SegNet</td><td>67.2</td></tr><tr><td><br />多尺度特征融合网络</td><td>82.7</td></tr><tr><td><br />CGAN(Conditional GAN)下的多尺度特征融合网络</td><td>88.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="58" name="58" class="anchor-tag"><b>4 面色分级</b></h3>
                <div class="p1">
                    <p id="59">由于面部的颜色特征具有较强的稳定性,因此常常在计算机视觉中被用作人脸的特征来进行研究处理。但是,颜色特征的区分性较弱,本文将颜色特征与纹理特征相融合对分类器进行学习。纹理特征是基于灰度层次的,描述图像的像素与像素之间在灰度空间的分布规律,它是物体表面共享的一种特性,它的存在不取决于目标颜色或者亮度的变化。纹理特征的描述现阶段主要分为2类:统计型纹理特征和模型纹理特征。最经典的2种方法是灰度共生矩阵GLCM(Gray Level Cooccurrence Matrix)与局部二值模型LBP (Local Binary Pattern)。本文使用的是局部二值模型。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>4.1 局部二值模型</b></h4>
                <div class="p1">
                    <p id="61">LBP是一种常被用来描述图像局部纹理特征的算子,原始的LBP算子使用图像中的每个像素的3×3邻域,以邻域中的中心像素灰度的值为阈值,在邻域中进行阈值化,灰度值大于中心像素的为1,否则为0。这样在3×3的邻域中就会生成8位二进制数,构成该中心像素的LBP值,这个值反映了该区域的纹理信息,通常情况下会将8位二进制数转换为十进制数。</p>
                </div>
                <div class="p1">
                    <p id="62">如图7所示,中心像素的LBP码为01101011,转换为十进制为107。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201911012_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 LBP算子" src="Detail/GetImg?filename=images/JSJK201911012_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 LBP算子  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201911012_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 LBP operator</p>

                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>4.2 面色分类实验与结果</b></h4>
                <div class="p1">
                    <p id="65">本文首先只使用颜色特征进行面色分类,分别使用了特征<i>F</i>1、<i>F</i>2和<i>F</i>3作为面色特征输入到SVM分类器和BP神经网络中进行面色分类实验,实验结果分别如表2～表4所示。为了评估特征的分类性能,选择1 000幅皮肤图像作为训练集,128幅皮肤图像作为测试样本。面色等级分为深、中和浅3个等级。训练集中,肤色为深的有312幅,面色为中的有353幅,面色为浅的有335幅;测试集中,面色为深的有41幅,面色为中的有43幅,面色为浅有44幅。之后我们将LBP纹理特征与颜色特征相融合作为面色特征<i>F</i>4,同样输入到SVM分类器和BP神经网络中对面色进行分类 。</p>
                </div>
                <div class="p1">
                    <p id="66">特征1:<i>F</i>1={r,g,b}</p>
                </div>
                <div class="p1">
                    <p id="67">特征2:<i>F</i>2={H,S,V}</p>
                </div>
                <div class="p1">
                    <p id="68">特征3:<i>F</i>3={L,a,b}</p>
                </div>
                <div class="p1">
                    <p id="69">特征4:<i>F</i>4={r,g,b,H,S,V,L,a,b,LBP}</p>
                </div>
                <div class="p1">
                    <p id="70">分类结果分别如表2～表5所示。</p>
                </div>
                <div class="area_img" id="71">
                    <p class="img_tit"><b>表2 特征1作为面色特征的分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Classification results with </b><i><b>F</b></i><b>1</b></p>
                    <p class="img_note"></p>
                    <table id="71" border="1"><tr><td><br />面色等级</td><td>SVM/%</td><td>BP神经网络/%</td></tr><tr><td><br />深</td><td>75.75</td><td>77.05</td></tr><tr><td><br />中</td><td>73.77</td><td>73.60</td></tr><tr><td><br />浅</td><td>75.63</td><td>75.21</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"><b>表3 特征2作为面色特征的分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Classification results with </b><i><b>F</b></i><b>2</b></p>
                    <p class="img_note"></p>
                    <table id="72" border="1"><tr><td><br />面色等级</td><td>SVM/%</td><td>BP神经网络/%</td></tr><tr><td><br />深</td><td>75.08</td><td>75.40</td></tr><tr><td><br />中</td><td>73.03</td><td>74.67</td></tr><tr><td><br />浅</td><td>74.45</td><td>75.84</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="73">
                    <p class="img_tit"><b>表4 特征3作为面色特征的分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Classification results with </b><i><b>F</b></i><b>3</b></p>
                    <p class="img_note"></p>
                    <table id="73" border="1"><tr><td><br />面色等级</td><td>SVM/%</td><td>BP神经网络/%</td></tr><tr><td><br />深</td><td>74.04</td><td>77.06</td></tr><tr><td><br />中</td><td>72.04</td><td>74.55</td></tr><tr><td><br />浅</td><td>73.38</td><td>76.66</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表5 特征4作为面色特征的分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Classification results with </b><i><b>F</b></i><b>4</b></p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td><br />面色等级</td><td>SVM/%</td><td>BP神经网络/%</td></tr><tr><td><br />深</td><td>86.17</td><td>92.05</td></tr><tr><td><br />中</td><td>83.61</td><td>90.33</td></tr><tr><td><br />浅</td><td>85.76</td><td>91.29</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="75">从表2～表4可以看出,在单独使用颜色特征作为面色分类特征的时候,SVM分类正确率在73%～75%,BP神经网络分类正确率在73%～77%,分类效果较为一般。从表5可以看出,在加入LBP纹理特征后,分类正确率有了显著的提升,SVM分类正确率达到了83%～86%,而BP神经网络的分类正确率达到了90%～92%。在整个实验中,BP神经网络的分类正确率都略高于SVM分类器的。实验中,BP神经网络的平均训练时间约为17 min,SVM分类器的平均训练时间约为9 min。因此,BP神经网络的分类效果虽然优于SVM分类器的,但是所需的训练时间也更长。这是由于BP神经网络所需参数更多,学习时间更长的缘故。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="77">本文分别实现了皮肤的分割和面色的分类,搭建了一个完整的面色分级系统,面色分级的平均正确率达到了91%,基本实现了面色分级的任务。在皮肤分割实验中,本文通过在条件生成对抗网络框架下使用多尺度特征融合网络,有效地提高了皮肤分割的准确性,分割结果的像素精度达到了97%。在面色分级实验中,本文将纹理特征与颜色特征相融合,有效地提高了面色分类的正确率,在SVM分类器中平均分类正确率达到了85%,在BP神经网络中平均分类正确率到达了91%。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="99" type="formula" href="images/JSJK201911012_09900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">吴从中</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="100" type="formula" href="images/JSJK201911012_10000.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">侯国松</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="96" type="formula" href="images/JSJK201911012_09600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">丁正龙</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="101" type="formula" href="images/JSJK201911012_10100.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">许良凤</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="102" type="formula" href="images/JSJK201911012_10200.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">詹曙</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A review and comparative study of skin segmentation techniques for handheld imaging devices">

                                <b>[1]</b> UngureanuA S,Javidnia H,Costache C,et al.A review and comparative study of skin segmentation techniques for handheld imaging devices[C]//Proc of 2016 IEEE International Conference on Consumer Electronics (ICCE),2016:530-531.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Skin Segmentation Using Color Pixel Classification: Analysis and Comparison">

                                <b>[2]</b> Phung S L,Bouzerdoum A,Chai D.Skin segmentation using color pixel classification:Analysis and comparison[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2005,27(1):148-154.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on the facial color recognition of traditional Chinese medicine based on image analysis technology">

                                <b>[3]</b> Yang Yun-cong.Research on the facial color recognition of traditional Chinese medicine based on image analysis technology[D].Beijing:Beijing University of Technology,2013.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diagnostic information extraction for face diagnosis in traditional Chinese medicine">

                                <b>[4]</b> Mao Hong-zhao.Diagnostic information extraction for face diagnosis in traditional Chinese medicine[D].Xiamen :Xiamen University,2007.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[5]</b> Goodfellow I,Pouget-Abadie J,Mirza M,et al.Generative adversarial nets[C]//Proc of Advances in Neural Information Processing Systems,2014:2672-2680.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional Generative Adversarial Nets">

                                <b>[6]</b> Mirza M,Osindero S.Conditional generative adversarial nets[J].Computer Science,2014,28(2):2672-2680.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">

                                <b>[7]</b> Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[C]//Proc of International Conference on Learning Representations,2016:161-177.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep generative image models using a Laplacian pyramid of adversarial networks">

                                <b>[8]</b> Denton E L,Chintala S,Fergus R.Deep generative image models using a Laplacian pyramid of adversarial networks[C]//Proc of Advances in Neural Information Processing Systems,2015:1486-1494.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[9]</b> He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[10]</b> Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,39(4):640-651.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Badrinarayanan V,Kendall A,Cipolla R.SegNet:A deep convolutional encoder-decoder architecture for scene segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.附中文参考文献:
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 杨云聪.基于图像分析的中医面色识别方法研究[D].北京:北京工业大学,2013.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008058115.nh&amp;v=MjUwODExMjdGck85RnRETnFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9uV3IvQVY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 毛红朝.面向中医面诊的诊断信息提取—关键算法研究与实现[D].厦门:厦门大学,2007.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201911012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201911012&amp;v=MTE2NzlHRnJDVVJMT2VaZVJtRnkvbldyL0FMejdCWmJHNEg5ak5ybzlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
