<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132353011436250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201910020%26RESULT%3d1%26SIGN%3dRRfJNylgnrmUiy7yGSrMa4Oxmto%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910020&amp;v=MjcxMjViRzRIOWpOcjQ5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2hWNzdJTHo3Qlo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#44" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;2 相关理论&lt;/b&gt; "><b>2 相关理论</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="&lt;b&gt;2.1 多标记问题的定义&lt;/b&gt;"><b>2.1 多标记问题的定义</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.2 局部正、负成对标记相关性&lt;/b&gt;"><b>2.2 局部正、负成对标记相关性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="&lt;b&gt;3 基于局部正、负标记相关性的&lt;/b&gt;&lt;i&gt;&lt;b&gt;k&lt;/b&gt;&lt;/i&gt;&lt;b&gt;近邻多标记分类新算法&lt;/b&gt; "><b>3 基于局部正、负标记相关性的</b><i><b>k</b></i><b>近邻多标记分类新算法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;4 实验结果分析&lt;/b&gt; "><b>4 实验结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#122" data-title="&lt;b&gt;4.1 实验数据集&lt;/b&gt;"><b>4.1 实验数据集</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;4.2 实验结果与分析&lt;/b&gt;"><b>4.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#138" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="&lt;b&gt;表1 多标记符号定义&lt;/b&gt;"><b>表1 多标记符号定义</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表2 多标记数据集&lt;/b&gt;"><b>表2 多标记数据集</b></a></li>
                                                <li><a href="#128" data-title="图1 在yeast上PNLC在不同&lt;i&gt;α&lt;/i&gt;时
 随着&lt;i&gt;k&lt;/i&gt;值的变化Hamming Loss变化曲线">图1 在yeast上PNLC在不同<i>α</i>时
 随着<i>k</i>值的变化Hamming Loss变化曲线</a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表3 &lt;/b&gt;&lt;i&gt;&lt;b&gt;k&lt;/b&gt;&lt;/i&gt;&lt;b&gt;=13时PNLC算法与LPLC算法 取不同&lt;/b&gt;&lt;i&gt;&lt;b&gt;α&lt;/b&gt;&lt;/i&gt;&lt;b&gt;值的结果比较&lt;/b&gt;"><b>表3 </b><i><b>k</b></i><b>=13时PNLC算法与LPLC算法 取不同</b><i><b>α</b></i><b>值的结果比较</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表4 &lt;/b&gt;&lt;i&gt;&lt;b&gt;k&lt;/b&gt;&lt;/i&gt;&lt;b&gt;=15时PNLC算法与LPLC算法取不同&lt;/b&gt;&lt;i&gt;&lt;b&gt;α&lt;/b&gt;&lt;/i&gt;&lt;b&gt;值的结果比较&lt;/b&gt;"><b>表4 </b><i><b>k</b></i><b>=15时PNLC算法与LPLC算法取不同</b><i><b>α</b></i><b>值的结果比较</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表5 本文算法在yeast数据集上与其他算法的实验结果比较&lt;/b&gt;"><b>表5 本文算法在yeast数据集上与其他算法的实验结果比较</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表6 本文算法在image数据集上与其他算法的实验结果比较&lt;/b&gt;"><b>表6 本文算法在image数据集上与其他算法的实验结果比较</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;表7 本文算法在cal500数据集上与其他算法的实验结果比较&lt;/b&gt;"><b>表7 本文算法在cal500数据集上与其他算法的实验结果比较</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表8 本文算法在corel5k数据集上与其他算法的实验结果比较&lt;/b&gt;"><b>表8 本文算法在corel5k数据集上与其他算法的实验结果比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="8">


                                    <a id="bibliography_1" title=" Zhang M L,Zhou Z H.A review on multi-label learning algorithms [J].IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">
                                        <b>[1]</b>
                                         Zhang M L,Zhou Z H.A review on multi-label learning algorithms [J].IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_2" title=" Zhang M L,Zhou Z H.Multilabel neural networks with applications to functional genomics and text categorization[J].IEEE Transactions on Knowledge and Data Engineering,2006,18(10):1338-1351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization">
                                        <b>[2]</b>
                                         Zhang M L,Zhou Z H.Multilabel neural networks with applications to functional genomics and text categorization[J].IEEE Transactions on Knowledge and Data Engineering,2006,18(10):1338-1351.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_3" title=" Jia X,Sun F M,Li H J,et al.Image multi-label annotation based on supervised nonnegative matrix factorization with new matching measurement[J].Neurocomputing,2017,219:518-525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image multi-label annotation based on supervised nonnegative matrix factorization with new matching measurement">
                                        <b>[3]</b>
                                         Jia X,Sun F M,Li H J,et al.Image multi-label annotation based on supervised nonnegative matrix factorization with new matching measurement[J].Neurocomputing,2017,219:518-525.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_4" title=" Xia S,Chen P,Zhang J,et al.Utilization of rotation-invariant uniform LBP histogram distribution and statistics of connected regions in automatic image annotation based on multi-label learning[J].Neurocomputing,2017,228:11-18." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDDEE661807878AA6CEA6476EB79269D0&amp;v=MjEwMzhwbWFCdUhZZk9HUWxmQ3BiUTM1TkJodzd5N3dhZz1OaWZPZmNmTWE2VEtxWTVOWk93SEMzUkl2aEJnbjA1N1RIamsyV0F5Y0xDU1RNNmZDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Xia S,Chen P,Zhang J,et al.Utilization of rotation-invariant uniform LBP histogram distribution and statistics of connected regions in automatic image annotation based on multi-label learning[J].Neurocomputing,2017,228:11-18.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_5" title=" Wang X,Sukthankar G.Multi-label relational neighbor classification using social context features[C]//Proc of the 19th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining,2013:464-472." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label relational neighbor classification using social context features">
                                        <b>[5]</b>
                                         Wang X,Sukthankar G.Multi-label relational neighbor classification using social context features[C]//Proc of the 19th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining,2013:464-472.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_6" title=" Bianco S,Ciocca G,Napoletano P,et al.An interactive tool for manual,semi-automatic and automatic video annotation[J].Computer Vision &amp;amp; Image Understanding,2015,131(C):88-99." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBC7CB8F92330E421B8499871B8587947&amp;v=MzEwMDljSExHYUsrcC9sTVp1Z01EQWs5elJkaDRqdDBRWGZscldBOWZMcVRUTDZZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJodzd5N3dhZz1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Bianco S,Ciocca G,Napoletano P,et al.An interactive tool for manual,semi-automatic and automatic video annotation[J].Computer Vision &amp;amp; Image Understanding,2015,131(C):88-99.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_7" title=" Boutell M R,Luo J B,Shen X P,et al.Learning multi-label scene classification[J].Pattern Recognition,2004,37(9):1757-1771." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740311&amp;v=MDMxNzFRVE1ud1plWnVIeWptVUxmSUoxb1FheEk9TmlmT2ZiSzdIdEROcVk5RlkrOFBEMzA0b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Boutell M R,Luo J B,Shen X P,et al.Learning multi-label scene classification[J].Pattern Recognition,2004,37(9):1757-1771.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_8" title=" de Comit&#233; F,Gilleron R,Tommasi M.Learning multi-label alternating decision trees from texts and data[C]//Proc of the 3rd International Conference on Machine Learning and Data Mining in Pattern Recognition,2003:35-49." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multi-label alternating decision trees from texts and data">
                                        <b>[8]</b>
                                         de Comit&#233; F,Gilleron R,Tommasi M.Learning multi-label alternating decision trees from texts and data[C]//Proc of the 3rd International Conference on Machine Learning and Data Mining in Pattern Recognition,2003:35-49.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_9" title=" Huang S J,Zhou Z H.Multi-label learning by exploiting label correlations locally[C]//Proc of the 26th AAAI Conference on Artificial Intelligence,2012:949-955." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label learning by exploiting label correlations locally">
                                        <b>[9]</b>
                                         Huang S J,Zhou Z H.Multi-label learning by exploiting label correlations locally[C]//Proc of the 26th AAAI Conference on Artificial Intelligence,2012:949-955.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_10" title=" Huang S J,Yu Y,Zhou Z H.Multi-label hypothesis reuse[C]//Proc of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2012:525-533." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-label hypothesis reuse">
                                        <b>[10]</b>
                                         Huang S J,Yu Y,Zhou Z H.Multi-label hypothesis reuse[C]//Proc of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2012:525-533.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_11" title=" He Zhi-fen,Yang Ming,Liu Hui-dong.Joint learning of multi-label classification and label correlations[J].Journal of Software,2014,25(9):1967-1981.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201409006&amp;v=MDE0MDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2hWNzdJTnlmVGJMRzRIOVhNcG85RllvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         He Zhi-fen,Yang Ming,Liu Hui-dong.Joint learning of multi-label classification and label correlations[J].Journal of Software,2014,25(9):1967-1981.(in Chinese)
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_12" title=" Zhang M L,Wu L.Lift:Multi-label learning with label-specific features[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,37(1):107-120." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LIFT:Multi-label learning with label-specific features">
                                        <b>[12]</b>
                                         Zhang M L,Wu L.Lift:Multi-label learning with label-specific features[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,37(1):107-120.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_13" title=" Huang J,Li G R,Wang S H,et al.Multi-label classification by exploiting local positive and negative pairwise label correlation[J].Neurocomputing,2017,257:164-174." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFAB7DADC4511DB5C716463FC6277A133&amp;v=MjQ1NTNOQmh3N3k3d2FnPU5pZk9mY1hKYk5hNDN2czJZTzRPRFFoTHltVVU2emw1VG55VTN4UTNmclhsUkxtY0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Huang J,Li G R,Wang S H,et al.Multi-label classification by exploiting local positive and negative pairwise label correlation[J].Neurocomputing,2017,257:164-174.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_14" title=" Zhang M L,Zhou Z H.ML-KNN:A lazy learning approach to multi-label learning[J].Pattern Recognition,2007,40(7):2038-2048." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MDkyNDRheEk9TmlmT2ZiSzdIdEROcVk5RlkrZ0dDWHc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSjFvUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Zhang M L,Zhou Z H.ML-KNN:A lazy learning approach to multi-label learning[J].Pattern Recognition,2007,40(7):2038-2048.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     Zhang Min-ling.An improved multi-label lazy learning approach[J].Journal of Computer Research and Development,2012,49(11):2271-2282.(in Chinese)</a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_16" title=" Zhang M L,Yu F,Tang C Z.Disambiguation-free partial label learning[J].IEEE Transactions on Knowledge &amp;amp; Data Engineering,2017,29(10):2155-2167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Disambiguation-free partial label learning">
                                        <b>[16]</b>
                                         Zhang M L,Yu F,Tang C Z.Disambiguation-free partial label learning[J].IEEE Transactions on Knowledge &amp;amp; Data Engineering,2017,29(10):2155-2167.
                                    </a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     何志芬,杨明,刘会东.多标记分类和标记相关性的联合学习[J].软件学报,2014,25(9):1967-1981.</a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_15" title=" 张敏灵.一种新型多标记懒惰学习算法[J].计算机研究与发展,2012,49(11):2271-2282." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201211002&amp;v=MTIwNzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvaFY3N0lMeXZTZExHNEg5UE5ybzlGWm8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         张敏灵.一种新型多标记懒惰学习算法[J].计算机研究与发展,2012,49(11):2271-2282.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(10),1854-1860 DOI:10.3969/j.issn.1007-130X.2019.10.019            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于局部正、负标记相关性的</b><i><b>k</b></i><b>近邻多标记分类新算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E8%8A%B8&amp;code=09140808&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋芸</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%82%96%E6%BD%87&amp;code=38752695&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肖潇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BE%AF%E9%87%91%E6%B3%89&amp;code=41473097&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">侯金泉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E8%8E%89&amp;code=11390184&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈莉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8C%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0012645&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西北师范大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在多标记学习中,每个样本都由一个实例表示,并与多个类标记相关联。现有的多标记学习算法大多是在全局利用标记相关性,即假设所有的样本共享不同类别标记之间的正相关性。然而,在实际应用中,不同的样本共享不同的标记相关性,标记间不仅存在正相关性,而且存在相互排斥的现象,即负相关性。针对这一问题,提出了基于局部正、负成对标记相关性的<i>k</i>近邻多标记分类算法PNLC。首先,对多标记数据的特征向量进行预处理,分别为每类标记构造对该类标记最具有判别能力的属性特征;然后,在训练阶段,PNLC算法通过所有训练样本中各样本的每个<i>k</i>近邻的真实标记构建标记之间的正、负局部成对相关性矩阵;最后,在测试阶段,首先得到每个测试样例的<i>k</i>近邻及其对应的正、负成对标记关系,利用该标记关系计算最大后验概率对测试样例进行预测。实验结果表明,PNLC算法在yeast和image数据集上的分类准确率明显优于其他常用的多标记分类算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%87%E8%AE%B0%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多标记学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A3%E3%80%81%E8%B4%9F%E7%9B%B8%E5%85%B3%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">正、负相关性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E8%AE%B0%E7%8B%AC%E6%9C%89%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标记独有特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3Ek%3C%2Fi%3E%E8%BF%91%E9%82%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>k</i>近邻;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    蒋芸（1970-），女，浙江绍兴人，博士，教授，CCF会员（E200028010M），研究方向为数据挖掘、粗糙集理论及应用。E-mail:jiangyun@nwnu.edu.cn，通信地址:730070甘肃省兰州市西北师范大学计算机科学与工程学院&lt;image id="193" type="formula" href="images/JSJK201910020_19300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    肖潇（1995-），女，四川乐至人，硕士，研究方向为数据挖掘。E-mail:1511479236@qq.com，通信地址:730070甘肃省兰州市西北师范大学计算机科学与工程学院&lt;image id="195" type="formula" href="images/JSJK201910020_19500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    侯金泉，通信地址:730070甘肃省兰州市西北师范大学计算机科学与工程学院;
                                </span>
                                <span>
                                    陈莉，通信地址:730070甘肃省兰州市西北师范大学计算机科学与工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61163036);</span>
                                <span>甘肃省科技计划资助自然科学基金(1606RJZA047);</span>
                                <span>2012年度甘肃省高校基本科研业务费专项资金;</span>
                                <span>甘肃省高校研究生导师项目(1201-16);</span>
                                <span>西北师范大学第三期知识与创新工程科研骨干项目(nwnu-kjcxgc-03-67);</span>
                    </p>
            </div>
                    <h1><b>A new knn multi-label classification algorithm based on local positive and negative labeling correlation</b></h1>
                    <h2>
                    <span>JIANG Yun</span>
                    <span>XIAO Xiao</span>
                    <span>HOU Jin Quan</span>
                    <span>CHEN Li</span>
            </h2>
                    <h2>
                    <span>College of Computer Science &amp; Engineering,Northwest Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In multi-label learning, each sample is represented by a single instance and associates with multiple class labels. Most of existing multi-label learning algorithms explore label correlations globally, by assuming that the positive label correlations are shared by all examples. However, in practical applications, different samples share different label correlations, and there is not only positive correlation among labels, but also mutually exclusive one(i.e., negative correlation). To solve this problem, we propose a KNN multi-label classification algorithm based on local positive and negative label correlation, named PNLC. Firstly, we preprocess the feature vector of multi-label data and construct the most discriminative features for each class. Then, in the training stage, the PNLC algorithm constructs the positive and negative label correlation matrixes by using the truth label of each k-nearest neighbor for all the training samples. Finally, in the test phase, the k-nearest neighbors and corresponding positive and negative pairwise label correlations for each test example are identified to calculate the maximum posterior probability so as to make prediction. Experimental results show that the PNLC algorithm is obviously superior to other well-established multi-label classification algorithms on the yeast and image datasets.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-label%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-label learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=positive%20and%20negative%20correlation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">positive and negative correlation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label%20specific%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label specific feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=KNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">KNN;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JIANG Yun,born in 1970,PhD,professor,CCF member(E200028010M),her research interests include data mining,and rough set theory &amp;amp;application.Address:College of Computer Science &amp;amp; Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    XIAO Xiao,born in 1995,MS,her research interest includes data mining.Address:College of Computer Science &amp;amp; Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    HOU Jin Quan,Address:College of Computer Science &amp;amp; Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    CHEN Li,Address:College of Computer Science &amp;amp; Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-06-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="44" name="44" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="45">多标记学习是为了解决文本分类中的歧义问题而提出的,是机器学习领域的研究热点之一。许多现实世界的分类问题都需要用多标记学习框架来解决<citation id="176" type="reference"><link href="8" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。与传统的单标记学习不同,多标记学习的处理对象为同时具有多个类标记的样本,并且每个样本都仅由一个实例表示。在实际应用中,标记之间往往具有相关性,利用标记相关性可以显著地提高算法的分类性能。如何通过利用标记相关性来构造一个性能良好的分类模型,从而更准确地为测试样本预测一组可能的标记是一个挑战。</p>
                </div>
                <div class="p1">
                    <p id="46">多标记学习涉及许多研究领域,例如文本分类<citation id="177" type="reference"><link href="10" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、图像标注<citation id="184" type="reference"><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、社交网络<citation id="178" type="reference"><link href="16" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和视频标注<citation id="179" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。在过去的几十年里,提出了许多成熟的方法来解决多标记学习问题。现有的方法一般可分为基于问题转换的方法PT(Problem Transformation)和基于算法适应的方法AA(Algorithm Adaptation)。PT方法对多标记学习数据进行处理,将其转化为单标记学习数据,从而运用现有的单标记分类方法来解决多标记分类问题。最具代表性的PT方法是Boutell等人<citation id="180" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出的二元关联方法BR(Binary Relevance)。AA方法通过对现有的单标记学习算法进行改进,使现有的单标记学习算法能够直接处理多标记数据。例如,基于决策树的多标记学习算法<citation id="181" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,融合标记独有属性特征的<i>k</i>近邻多标记分类算法。2012年,Huang等人<citation id="182" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了利用局部标签相关性进行多标记学习的算法。该算法的核心是:为了对标签相关性的局部影响进行编码,导出了一个局部标记相关码来增强每个实例的特征表示,将全局判别拟合和局部相关灵敏度结合到一个统一的框架中,并给出了一种交替的优化解决方案。同年,Huang等人<citation id="183" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>还提出了多标签假设重用的方法,它能够在学习过程中自动发现和利用标记关系。其基本思想是,如果两个标记是相关的,为一个标记生成的假设可以有助于另一个标记。多标签假设重用MAHR(Multi-label Hypothesis Reuse)算法利用假设重用机制将其作为一种增强方法加以实现。此外,它揭示了标记关系通常是不对称的。以上算法均为典型的具有代表性的多标记分类算法。我们面临的挑战是如何通过利用标记相关性来构造一个良好的分类模型,该模型可以预测未知样例的一组可能标记。</p>
                </div>
                <div class="p1">
                    <p id="47">为了解决这个问题,研究者们通过利用二阶或高阶标记相关性提出了大量的多标记学习算法<citation id="185" type="reference"><link href="24" rel="bibliography" /><link href="26" rel="bibliography" /><link href="28" rel="bibliography" /><link href="40" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">11</a>]</sup></citation>。这些算法大多是在全局利用标记相关性,即假设所有的样本共享不同类别标记之间的正相关性。然而,在实际应用中,不同的样本共享不同的标记相关性,标记间不仅存在正相关性,而且还存在相互排斥的现象,即负相关性。例如,“沙”和“船”与“海”标记有很强的相关性。但是,这些相关性在不同的图像上有所不同,“沙”与“海”之间的相关性只有在标注为“沙”和“海”的图像中才能共享;“船”与“海”之间的相关性仅由图中标注为“船”和“海”的图像共享(不同于全局标记相关性)。此外,如果两个标记是负关联的,即它们是相互排斥的,比如,“海”与“火车”“船”与“车”之间的相关性,这2对标记不可能以较高的概率同时发生,结合这些负相关性也可以提高多标记分类器的性能。由此可见,标记间不仅存在正相关性,而且还存在负相关性。显然,如果可以利用标记间的局部正、负相关性来预测未见样本,由此可以提出一种更有效的多标记分类算法。</p>
                </div>
                <div class="p1">
                    <p id="48">基于此,本文提出了基于局部正、负标记相关性的<i>k</i>近邻多标记分类新算法—PNLC(Positive and Negative Labeling Correlation)算法,并通过在yeast与image数据集上的实验验证了PNLC算法的有效性。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>2 相关理论</b></h3>
                <h4 class="anchor-tag" id="50" name="50"><b>2.1 多标记问题的定义</b></h4>
                <div class="p1">
                    <p id="51">在多标记学习中,设<i>X</i>∈<b>R</b><sup><i>d</i></sup>是一个<i>d</i>维的输入空间,<i>Y</i> ={<i><b>y</b></i><sub>1</sub>,<i><b>y</b></i><sub>2</sub>,…,<i><b>y</b></i><sub><i>q</i></sub>}是一个具有<i>q</i>类标记的标记空间。<i>D</i>={(<i><b>x</b></i><sub><i>i</i></sub>,<i><b>y</b></i><sub><i>i</i></sub>)|1≤<i>i</i>≤<i>n</i>}是一个具有<i>n</i>个样本的训练数据集。第<i>i</i>个样本由一个<i>d</i>维的属性特征向量表示为<i><b>x</b></i><sub><i>i</i></sub> =[<i>x</i><sub><i>i</i></sub><sub>1</sub>,<i>x</i><sub><i>i</i></sub><sub>2</sub>,…,<i>x</i><sub><i>id</i></sub>],<i><b>x</b></i><sub><i>i</i></sub>∈<i>X</i>。<i><b>y</b></i><sub><i>i</i></sub> =[<i>y</i><sub><i>i</i></sub><sub>1</sub>,<i>y</i><sub><i>i</i></sub><sub>2</sub>,…,<i>y</i><sub><i>iq</i></sub>]是样本<i><b>x</b></i><sub><i>i</i></sub>对应的标记向量。如果标记<i><b>y</b></i><sub><i>j</i></sub>与<i><b>x</b></i><sub><i>i</i></sub>相关联,则元素<i>y</i><sub><i>ij</i></sub>=1,否则<i>y</i><sub><i>ij</i></sub>=0。</p>
                </div>
                <div class="p1">
                    <p id="52"><i>N</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示样本<i><b>x</b></i><sub><i>i</i></sub>相对于标记<i><b>y</b></i><sub><i>l</i></sub>的<i>k</i>个最近邻,<i>k</i>是最近邻样例的个数,1≤<i>l</i>≤<i>q</i>。本文采用<i>r</i>阶Minkowski距离度量两个样本<i><b>x</b></i><sub><i>i</i></sub>与<i><b>x</b></i><sub><i>j</i></sub>间的相似度。用一个<i>k</i>维的列向量<i><b>c</b></i><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>j</mi></msubsup></mrow></math></mathml>=[<i>c</i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msubsup></mrow></math></mathml>,<i>c</i><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msubsup></mrow></math></mathml>,…,<i>c</i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>k</mi><mi>j</mi></mrow></msubsup></mrow></math></mathml>]<sup>T</sup>表示<i><b>x</b></i><sub><i>i</i></sub>的近邻样例中具有标记<i><b>y</b></i><sub><i>j</i></sub>的情况,其中第<i>i</i>个分量表示<i><b>x</b></i><sub><i>i</i></sub>的近邻样例是否具有标记<i><b>y</b></i><sub><i>j</i></sub>。例如,<i>c</i><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mspace width="0.25em" /></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mrow></math></mathml>= <i>y</i><sub><i>ij</i></sub>,∀<i><b>x</b></i><sub><i>i</i></sub>∈<i>N</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>。本文使用的所有符号及其定义如表1所示。</p>
                </div>
                <div class="area_img" id="53">
                    <p class="img_tit"><b>表1 多标记符号定义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Definitions of multi-label symbols</b></p>
                    <p class="img_note"></p>
                    <table id="53" border="1"><tr><td><br />符号</td><td>定义</td></tr><tr><td><br /><i>k</i></td><td>近邻样例的个数</td></tr><tr><td><br /><b><i>x</i></b><sub><i>i</i></sub> =[<i>x</i><sub><i>i</i>1</sub>,<i>x</i><sub><i>i</i>2</sub>,…,<i>x</i><sub><i>id</i></sub>]</td><td>第<i>i</i>个训练样例的特征向量</td></tr><tr><td><br /><b><i>y</i></b><sub><i>i</i></sub> =[<i>y</i><sub><i>i</i>1</sub>,<i>y</i><sub><i>i</i>2</sub>,…,<i>y</i><sub><i>iq</i></sub>]</td><td>样例<b><i>x</i></b><sub><i>i</i></sub>的标记向量,<i>y</i><sub><i>ij</i></sub>∈{0,1},1≤ <i>j</i> ≤<i>q</i></td></tr><tr><td><br /><i>D</i>={(<b><i>x</i></b><sub><i>i</i></sub>,<b><i>y</i></b><sub><i>i</i></sub>),1≤<i>i</i>≤<i>n</i>}</td><td>具有<i>n</i>个样例的训练数据集</td></tr><tr><td><br /><i>N</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math>={<i>a</i>|<i>a</i> is the knn of <b><i>x</i></b><sub><i>i</i></sub> in <i>p</i><sub><i>l</i></sub>}</td><td><b><i>x</i></b><sub><i>i</i></sub>在正样例集合<i>p</i><sub><i>l</i></sub>中的<i>k</i>个近邻样例,1≤ <i>l</i> ≤<i>q</i></td></tr><tr><td><br /><b><i>c</i></b><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>j</mi></msubsup></mrow></math>=[<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msubsup></mrow></math>,<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msubsup></mrow></math>,…,<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>k</mi><mi>j</mi></mrow></msubsup></mrow></math>]<sup>T</sup></td><td>一个<i>k</i>维列向量,<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mrow></math>=<i>y</i><sub><i>ij</i></sub>,∀<b><i>x</i></b><sub><i>i</i></sub>∈<i>N</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math>,<i>c</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msubsup></mrow></math>∈{0,1},1≤ <i>i</i> ≤<i>k</i></td></tr><tr><td><br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>q</mi></msub><mo>∈</mo><mi>Y</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>Ν</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">x</mi><mi>q</mi></msubsup></mrow></munder><mo stretchy="false">[</mo></mstyle></mrow></mstyle></mrow></math><br />|<i>y</i><sub><b><i>x</i></b><sub><i>i</i></sub></sub>(<i>l</i>)==1|]</td><td>给出了<b><i>x</i></b><sub><i>i</i></sub>与所有类别对应的近邻样例集合中隶属于第<i>l</i>类的近邻个数,1≤ <i>l</i>≤<i>q</i></td></tr><tr><td><br /><b><i>P</i></b>∈ <b>R</b><sup><i>n</i>×<i>q</i></sup></td><td>存储与每个训练样例<b><i>x</i></b><sub><i>i</i></sub>所属的每个标记正相关的标记索引</td></tr><tr><td><br /><b><i>N</i></b>∈ <b>R</b><sup><i>n</i>×<i>q</i></sup></td><td>存储与每个训练样例<b><i>x</i></b><sub><i>i</i></sub>所属的每个标记负相关的标记索引</td></tr><tr><td><br /><b><i>NC</i></b><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mi>l</mi></msubsup></mrow></math></td><td><i>N</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup></mrow></math>中样例共享的正相关的标记向量</td></tr><tr><td><br /><b><i>NC</i></b><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ν</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mi>l</mi></msubsup></mrow></math></td><td><i>N</i><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup></mrow></math>中样例共享的负相关的标记向量</td></tr><tr><td><br /><b><i>pa</i></b><sup>+</sup><sub><i>i</i></sub>(<b><i>x</i></b><sub><i>t</i></sub>)=[<b><i>pa</i></b><sup>+</sup><sub><i>i</i>1</sub>(<b><i>x</i></b><sub><i>t</i></sub>),<b><i>pa</i></b><sup>+</sup><sub><i>i</i>2</sub>(<b><i>x</i></b><sub><i>t</i></sub>),…,<b><i>pa</i></b><sup>+</sup><sub><i>iq</i></sub>(<b><i>x</i></b><sub><i>t</i></sub>)]</td><td><b><i>NC</i></b><sub><i>P</i></sub>(<b><i>x</i></b><sub><i>t</i></sub>)中与标记<b><i>y</i></b><sub><i>i</i></sub>正相关的标记矩阵,<i>P</i>(<b><i>x</i></b><sub><i>t</i></sub>)为<b><i>x</i></b><sub><i>t</i></sub>的正样例集合</td></tr><tr><td><br /><b><i>pa</i></b><sup>-</sup><sub><i>i</i></sub>(<b><i>x</i></b><sub><i>t</i></sub>)=[ <b><i>pa</i></b><sup>-</sup><sub><i>i</i>1</sub>(<b><i>x</i></b><sub><i>t</i></sub>),<b><i>pa</i></b><sup>-</sup><sub><i>i</i>2</sub>(<b><i>x</i></b><sub><i>t</i></sub>),…,<b><i>pa</i></b><sup>-</sup><sub><i>iq</i></sub>(<b><i>x</i></b><sub><i>t</i></sub>)]</td><td><b><i>NC</i></b><sub><i>N</i></sub>(<b><i>x</i></b><sub><i>t</i></sub>)中与标记<b><i>y</i></b><sub><i>i</i></sub>负相关的标记矩阵,<i>N</i>(<b><i>x</i></b><sub><i>t</i></sub>)为<b><i>x</i></b><sub><i>t</i></sub>的负样例集合</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.2 局部正、负成对标记相关性</b></h4>
                <div class="p1">
                    <p id="55">在多标记学习中,类标记之间通常存在正相关性和负相关性。例如,一个属于标记<i><b>y</b></i><sub>1</sub>的样例也很可能属于标记<i><b>y</b></i><sub>2</sub>,由此认为<i><b>y</b></i><sub>1</sub>和<i><b>y</b></i><sub>2</sub>之间是正相关的。相反,如果一个不具有标记<i><b>y</b></i><sub>1</sub>的样例有较高的概率也不具有标记<i><b>y</b></i><sub>3</sub>,此时认为<i><b>y</b></i><sub>1</sub>和<i><b>y</b></i><sub>3</sub>是负相关的。利用类标记之间的正、负相关性可以提高多标记分类的性能。</p>
                </div>
                <div class="p1">
                    <p id="56">考虑到不同的样例共享不同的标记关系,本文试图在局部挖掘标记间的正、负成对相关性。我们定义了两个矩阵<i><b>P</b></i>∈ <b>R</b><sup><i>n</i></sup><sup>×</sup><sup><i>q</i></sup>和<i><b>N</b></i>∈ <b>R</b><sup><i>n</i></sup><sup>×</sup><sup><i>q</i></sup>,<i><b>P</b></i>为与每个训练样本<i><b>x</b></i><sub><i>i</i></sub>所属的每个标记正相关的标记,若<i>y</i><sub><i>ij</i></sub>=1,则<i>P</i><sub><i>ij</i></sub>存储与<i><b>y</b></i><sub></sub><sub><i>j</i></sub>正相关的类标号(例如,<i><b>y</b></i><sub><i>c</i></sub>),则<i>P</i><sub><i>ij</i></sub>=<i>c</i>,否则,<i>P</i><sub><i>ij</i></sub> =0。如果有多个与<i><b>y</b></i><sub></sub><sub><i>j</i></sub>正相关的类标记,则将选择该|<i>c</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>c</mi></msubsup></mrow></math></mathml>|值最大时的标记作为最正相关的标记。<i><b>N</b></i>为与每个训练样本<i><b>x</b></i><sub><i>i</i></sub>所属的每个标记负相关的标记,若<i>y</i><sub><i>ij</i></sub>=1,<i>N</i><sub><i>ij</i></sub>存储与<i><b>y</b></i><sub></sub><sub><i>j</i></sub>负相关的类标记(例如,<i><b>y</b></i><sub><i>c</i></sub>),则<i>N</i><sub><i>ij</i></sub>=<i>c</i>,否则,<i>N</i><sub><i>ij</i></sub> =0。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">2.2.1 标记正相关性</h4>
                <div class="p1">
                    <p id="58">在本文模型中,根据<i><b>x</b></i><sub><i>i</i></sub>所属的每个标记<i><b>y</b></i><sub><i>l</i></sub>及其<i>k</i>近邻样例<i>N</i>(<i><b>x</b></i><sub><i>i</i></sub>)具有标记<i><b>y</b></i><sub><i>j</i></sub>的后验概率来探索样例<i><b>x</b></i><sub><i>i</i></sub>具有的最正相关的标记<i><b>y</b></i><sub><i>j</i></sub>,其后验概率计算如式(1)所示:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mo>∀</mo><mi>l</mi><mo>:</mo><mi>l</mi><mo>≠</mo><mi>j</mi><mo>&amp;</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>,</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">如果两个类标记有很强的相关性,那么它们的后验概率就会很大,否则就会很小,本文假设每个训练样本所属的标记存在很强的成对标记正相关性。通过式(1)计算<i>P</i><sub><i>ij</i></sub>,发现样例<i><b>x</b></i><sub><i>i</i></sub>和与标记<i><b>y</b></i><sub></sub><sub><i>j</i></sub>和<i><b>y</b></i><sub><i>l</i></sub>均相关联,即<i>y</i><sub><i>il</i></sub>=1,<i>y</i><sub><i>ij</i></sub>=1。因为,如果2个标记是强正相关的,它们通常会同时出现,因此式(1)的值将较大。这种约束将使本文的算法能够有效地利用局部成对标记相关性。值得注意的是,<i>P</i><sub><i>ij</i></sub>可能不等于<i>P</i><sub><i>il</i></sub>。例如,在图像标注中,如果已知一个图像被标注为“船”,那么它将有较高的概率被标注为“海”,反之却并非如此。如果对于<i><b>x</b></i><sub><i>i</i></sub>认为标记<i><b>y</b></i><sub><i>j</i></sub>是<i><b>y</b></i><sub><i>l</i></sub>的最强正相关类标号,则将得到最大条件概率<i>p</i>(<i>y</i><sub><i>ij</i></sub>=1|<i>y</i><sub><i>il</i></sub>=1,<i>N</i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>)。其概率计算如式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mn>1</mn><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mn>1</mn><mo>,</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">)</mo><mspace width="0.25em" /><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>j</mi></msubsup><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">因此,概率<i>p</i>(<i>y</i><sub><i>ij</i></sub>=0|<i>y</i><sub><i>il</i></sub>=1,<i>N</i><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>)=1-<i>p</i>(<i>y</i><sub><i>ij</i></sub>=1|<i>y</i><sub><i>il</i></sub>=1,<i>N</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>)。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63">2.2.2 标记负相关性</h4>
                <div class="p1">
                    <p id="64">对于负标记相关性,如果样例<i><b>x</b></i><sub><i>i</i></sub>与标记<i><b>y</b></i><sub></sub><sub><i>l</i></sub>不相关,反而<i><b>x</b></i><sub><i>i</i></sub>可能有较高的概率与<i><b>y</b></i><sub></sub><sub><i>j</i></sub>相关联。因此,这种负标记相关性有助于对<i><b>y</b></i><sub></sub><sub><i>j</i></sub>的标注,并根据<i><b>x</b></i><sub><i>i</i></sub>不相关的标记<i><b>y</b></i><sub></sub><sub><i>l</i></sub>及其<i>k</i>近邻样例<i>N</i>(<i><b>x</b></i><sub><i>i</i></sub>)具有标记<i><b>y</b></i><sub></sub><sub><i>j</i></sub>的后验概率来探索样例<i><b>x</b></i><sub><i>i</i></sub>具有的最负相关的标记<i><b>y</b></i><sub></sub><sub><i>j</i></sub>,其后验概率计算如式(3)所示:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mo>∀</mo><mi>l</mi><mo>:</mo><mi>l</mi><mo>≠</mo><mi>j</mi><mo>&amp;</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>0</mn><mo>,</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">如果2个类标记是强负相关的,则后验概率将很大;否则,它将很小。本文假设每个训练样本不相关的标记存在很强的成对标记负相关性。通过式(3)计算<i>N</i><sub><i>ij</i></sub>,样例<i><b>x</b></i><sub><i>i</i></sub>与标记<i><b>y</b></i><sub></sub><sub><i>j</i></sub>相关联且与<i><b>y</b></i><sub></sub><sub><i>l</i></sub>不相关,即<i>y</i><sub><i>il</i></sub>=0,<i>y</i><sub><i>ij</i></sub>=1。因为,如果两个标记之间存在强烈的负相关性,它们有很大概率不会同时发生,从而导致式(3)的值会变大。其概率计算如式(4)所示:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>0</mn><mo>,</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">)</mo><mspace width="0.25em" /><mo>=</mo><mspace width="0.25em" /><mfrac><mrow><mi mathvariant="bold-italic">c</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>j</mi></msubsup><msup><mrow></mrow><mtext>Τ</mtext></msup><msup><mi mathvariant="bold-italic">c</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow><mrow><mo stretchy="false">∥</mo><msup><mi mathvariant="bold-italic">c</mi><mo>′</mo></msup><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中<i><b>c</b></i>′<sup><i>l</i></sup><sub><i><b>x</b></i></sub><sub><i>i</i></sub>=1-<i><b>c</b></i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>。概率<i>p</i>(<i>y</i><sub><i>ij</i></sub>=0|<i>y</i><sub><i>il</i></sub>=0,<i>N</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>)=1-<i>p</i>(<i>y</i><sub><i>ij</i></sub>=1|<i>y</i><sub><i>il</i></sub>=0,<i>N</i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>)。</p>
                </div>
                <h3 id="69" name="69" class="anchor-tag"><b>3 基于局部正、负标记相关性的</b><i><b>k</b></i><b>近邻多标记分类新算法</b></h3>
                <div class="p1">
                    <p id="70">在实际应用中,不同的样本共享不同的标记相关性,标记间不仅存在正相关性,而且还存在相互排斥的现象,即负相关性。同时,LPLC(multi-Label classification by exploiting Local Positive and negative pairwise Label Correlation)算法使用相同的属性特征集合预测所有的类别标记,它并非最佳选择。基于以上分析,本文提出了基于局部正、负标记相关性的<i>k</i>近邻多标记分类新算法—PNLC算法。首先对多标记训练样例数据的特征向量进行预处理,分别为每类标记构造对该类标记最具有判别能力的属性特征<citation id="186" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。然后,在训练阶段,PNLC算法通过所有训练样本中各样本的每个<i>k</i>近邻的真实标记构建标记之间的正、负局部成对相关性矩阵。最后,在测试阶段,首先得到每个测试样例的<i>k</i>近邻及其对应的正、负成对标记关系,利用该标记关系计算最大后验概率对测试样例进行预测。</p>
                </div>
                <div class="p1">
                    <p id="71">本文算法分为两部分:训练过程和测试过程(见算法1和算法2)。</p>
                </div>
                <div class="p1">
                    <p id="72">算法1为构建标记之间的正、负局部成对相关性矩阵的全部过程。一旦得到每个训练样例的局部正、负成对标记相关性,就可以将其合并到多标记分类模型中以提高多标记分类算法的性能。本文假设相似的样例可能具有相同的标记相关性。在测试阶段,我们首先找到每个测试示例分别对应于标记<i>l</i>的<i>k</i>个最近邻,以及这些最近邻对应的局部成对标记关系。测试样例将共享其<i>k</i>最近邻的成对标记关系。给出一个测试样例<i><b>x</b></i><sub><i>t</i></sub>,<i>N</i><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>是其<i>k</i>近邻,<i><b>NC</b></i><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mi>l</mi></msubsup></mrow></math></mathml>和<i><b>NC</b></i><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>Ν</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mi>l</mi></msubsup></mrow></math></mathml>是<i>N</i><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>中样例共享的正、负相关的标记关系,可以分别从<i><b>P</b></i>和<i><b>N</b></i>得到。测试样例<i><b>x</b></i><sub><i>t</i></sub>与其<i>k</i>近邻<i>N</i><mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>共享相同的局部成对标记相关性,<i><b>pa</b></i><sup>+</sup><sub><i>i</i></sub>(<i><b>x</b></i><sub><i>t</i></sub>)是<i><b>NC</b></i><sub><i>P</i></sub><sub>(</sub><sub><i><b>x</b></i></sub><sub><i>t</i></sub><sub>)</sub>中与标记<i><b>y</b></i><sub><i>i</i></sub>正相关的标记矩阵,<i><b>pa</b></i><sup>-</sup><sub><i>i</i></sub>(<i><b>x</b></i><sub><i>t</i></sub>)是<i><b>NC</b></i><sub><i>N</i></sub><sub>(</sub><sub><i><b>x</b></i></sub><sub><i>t</i></sub><sub>)</sub>中与标记<i><b>y</b></i><sub><i>i</i></sub>负相关的标记矩阵。为了本文使用最大后验(MAP)概率准则预测<i><b>x</b></i><sub><i>t</i></sub>是否具有标记<i><b>y</b></i><sub><i>i</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mrow><mi>b</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></munder><mspace width="0.25em" /><mi>Ρ</mi><mi>r</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub><mo>=</mo><mi>b</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mi>Ζ</mi></mfrac><mo stretchy="false">(</mo><mi>α</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>c</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">a</mi><msubsup><mrow></mrow><mi>i</mi><mo>+</mo></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></munder><mi>p</mi></mstyle><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub><mo>=</mo><mi>b</mi><mo>,</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>c</mi></mrow></msub><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>c</mi></msubsup><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>c</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">a</mi><msubsup><mrow></mrow><mi>i</mi><mo>-</mo></msubsup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></munder><mi>p</mi></mstyle><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub><mo>=</mo><mi>b</mi><mo>,</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>c</mi></mrow></msub><mo>=</mo><mn>0</mn><mo stretchy="false">|</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>c</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中,<i>Z</i>是规范化常量,参数<i>a</i>∈[0,1]控制了正负相关性之间的权重。概率<i>p</i>(<i>y</i><sub><i>tc</i></sub>=<i>b</i>|<i>N</i><mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup></mrow></math></mathml>)可以通过式(6)计算:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>c</mi></mrow></msub><mo>=</mo><mi>b</mi><mo stretchy="false">|</mo><mi>Ν</mi><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mi>l</mi></msubsup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>q</mi><mo>*</mo><mi>k</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>b</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></msup><mi>C</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo>*</mo><mi>k</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">最终对<i><b>x</b></i><sub>t</sub>的预测结果为<i>h</i>(<i><b>x</b></i><sub><i>t</i></sub>):</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">{</mo><mi>h</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>h</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>h</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">PNLC的预测过程可归纳为算法2。</p>
                </div>
                <div class="p1">
                    <p id="79"><b>算法1</b> 构建正、负成对相关标记</p>
                </div>
                <div class="area_img" id="198">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201910020_19800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="107"><b>算法2</b> PNLC预测</p>
                </div>
                <div class="area_img" id="199">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201910020_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="199">
                                <img alt="" src="Detail/GetImg?filename=images/JSJK201910020_19901.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>4 实验结果分析</b></h3>
                <h4 class="anchor-tag" id="122" name="122"><b>4.1 实验数据集</b></h4>
                <div class="p1">
                    <p id="123">实验主要在基因功能分析、自动图像标注、音乐分析3个不同领域的标准数据集上进行结果的比较,数据集的详细信息如表2所示。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表2 多标记数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Multi label datasets</b></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td>数据集</td><td>训练<br />样例数</td><td>测试<br />样例数</td><td>属性<br />个数</td><td>标记<br />个数</td><td><i>cardinality</i></td><td>所属<br />领域</td></tr><tr><td>cal500</td><td>302</td><td>200</td><td>68</td><td>174</td><td>26.044</td><td>music</td></tr><tr><td><br />image</td><td>1 200</td><td>800</td><td>294</td><td>5</td><td>1.236</td><td>images</td></tr><tr><td><br />yeast</td><td>1 500</td><td>917</td><td>294</td><td>14</td><td>4.237</td><td>biology</td></tr><tr><td><br />corel5k</td><td>3 000</td><td>2 000</td><td>499</td><td>374</td><td>3.522</td><td>images</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="125">其中<i>cardinality</i>表示标记的基数,为样例的平均标记个数。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>4.2 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="127">在本节中,对我们所提出的算法PNLC的有效性进行评估。将本文算法与LPLC<citation id="187" type="reference"><link href="32" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> 、ML-KNN<citation id="188" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、IML-KNN、IMLLA<citation id="189" type="reference"><link href="36" rel="bibliography" /><link href="42" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">15</a>]</sup></citation>以及BoostTEXTER算法在cal500、yeast、image、corel5k<citation id="190" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>3个不同领域的5个基准数据集上进行比较。利用多标记算法评测指标Average Precision、One-Error、Coverage、Hamming Loss、Ranking Loss<citation id="191" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>对算法性能进行评估。在本文提出的算法中,有2个重要的参数<i>k</i>和<i>α</i>。<i>k</i>是最近邻数,<i>α</i>控制正相关和负相关性之间的权重。本文在yeast数据集上对参数<i>k</i>和<i>α</i>进行了参数敏感性分析,如图1所示。表3与表4分别为近邻样例个数<i>k</i>取13和15时的实验结果;表5～表8为各算法在yeast、image、cal500以及corel5k数据集上的实验结果比较。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910020_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 在yeast上PNLC在不同α时
 随着k值的变化Hamming Loss变化曲线" src="Detail/GetImg?filename=images/JSJK201910020_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 在yeast上PNLC在不同<i>α</i>时
 随着<i>k</i>值的变化Hamming Loss变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910020_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Hamming loss changes with the number of <i>k</i> 
 under different <i>α</i> values on yeast by the PNLC algorithm</p>

                </div>
                <div class="area_img" id="129">
                    <p class="img_tit"><b>表3 </b><i><b>k</b></i><b>=13时PNLC算法与LPLC算法 取不同</b><i><b>α</b></i><b>值的结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Result comparison between the LPLC algorithm and the PNLC algorithm when </b><i><b>k</b></i><b>=13</b></p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td rowspan="2"><br />数据集</td><td rowspan="2">评价指标</td><td rowspan="2">LPLC</td><td colspan="3"><br />本文算法</td></tr><tr><td><br /><i>α</i>=0.1</td><td><i>α</i>=0.7</td><td><i>α</i>=1</td></tr><tr><td><br /></td><td>Average <br />Precision↑</td><td>0.765 4</td><td>0.754 3</td><td><b>0.778 9</b></td><td>0.769 8</td></tr><tr><td><br /></td><td>One-Error↓</td><td>0.227 5</td><td>0.239 1</td><td><b>0.218 8</b></td><td>0.221 7</td></tr><tr><td><br />yeast</td><td>Coverage↓</td><td>6.191 7</td><td>6.209 1</td><td><b>6.172 1</b></td><td>6.187 6</td></tr><tr><td><br /></td><td>Hamming Loss↓</td><td>0.195 4</td><td>0.197 5</td><td><b>0.193 1</b></td><td>0.194 9</td></tr><tr><td><br /></td><td>Ranking Loss↓</td><td>0.167 6</td><td>0.169 7</td><td><b>0.158 6</b></td><td>0.162 3</td></tr><tr><td><br /></td><td>Average <br />Precision↑</td><td>0.806 7</td><td>0.801 1</td><td><b>0.817 2</b></td><td>0.812 1</td></tr><tr><td><br /></td><td>One-Error↓</td><td>0.295 1</td><td>0.296 4</td><td><b>0.290 3</b></td><td>0.292 5</td></tr><tr><td><br />image</td><td>Coverage↓</td><td>0.921 4</td><td>0.933 1</td><td><b>0.873 4</b></td><td>0.895 1</td></tr><tr><td><br /></td><td>Hamming Loss↓</td><td>0.177 9</td><td>0.184 2</td><td><b>0.161 7</b></td><td>0.164 2</td></tr><tr><td><br /></td><td>Ranking Loss↓</td><td>0.172 1</td><td>0.178 6</td><td><b>0.161 5</b></td><td>0.163 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="130">图1给出了PNLC在yeast上<i>α</i>取不同值时随着近邻样例个数<i>k</i>变化的Hamming Loss的变化曲线。从图1中可以看出,首先在加入负标记相关性后(<i>α</i>&lt;1),其结果优于仅考虑正标记相关性的(当<i>α</i>=1时,PNLC只考虑正标记相关性),由此验证了挖掘负标记相关性的有效性。其次,如果<i>α</i>太小(当<i>α</i>=0.1时),正标记相关性的影响远小于负标记相关性,此时PNLC的性能较差。其主要原因可能是由于多标记数据标记矩阵的稀疏性,标记对不共同发生的次数远大于共同发生的次数,因此得知PNLC的负标记关系的可靠性要弱于正标记关系的。我们建议<i>α</i>取值应该大于或等于0.5,特别是对于具有大量标记但基数较小的数据集。最后在给定<i>α</i>的情况下,PNLC的性能随着<i>k</i>的增加而得到了改善,实验结果表明,PNLC算法在<i>k</i>=15,<i>α</i>=0.7时,算法评价指标取得最佳值。</p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表4 </b><i><b>k</b></i><b>=15时PNLC算法与LPLC算法取不同</b><i><b>α</b></i><b>值的结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Result comparison between the LPLC algorithm and the PNLC algorithm when </b><i><b>k</b></i><b>=15</b></p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td rowspan="2"><br />数据集</td><td rowspan="2">评价指标</td><td rowspan="2">LPLC</td><td colspan="3"><br />本文算法</td></tr><tr><td><br /><i>α</i>=0.1</td><td><i>α</i>=0.7</td><td><i>α</i>=1</td></tr><tr><td><br /></td><td>Average <br />Precision↑</td><td>0.772 4</td><td>0.756 4</td><td><b>0.785 1</b></td><td>0.778 2</td></tr><tr><td><br /></td><td>One-Error↓</td><td>0.241 4</td><td>0.255 9</td><td><b>0.238 2</b></td><td>0.238 7</td></tr><tr><td><br />yeast</td><td>Coverage↓</td><td>6.187 5</td><td>6.201 3</td><td><b>6.167 4</b></td><td>6.172 1</td></tr><tr><td><br /></td><td>Hamming Loss↓</td><td>0.194 4</td><td>0.196 3</td><td><b>0.192 1</b></td><td>0.194 2</td></tr><tr><td><br /></td><td>Ranking Loss↓</td><td>0.161 3</td><td>0.162 4</td><td><b>0.153 2</b></td><td>0.157 6</td></tr><tr><td><br /></td><td>Average <br />Precision↑</td><td>0.816 5</td><td>0.809 5</td><td><b>0.821 3</b></td><td>0.818 7</td></tr><tr><td><br /></td><td>One-Error↓</td><td>0.301 0</td><td>0.305 6</td><td><b>0.296 5</b></td><td>0.299 1</td></tr><tr><td><br />image</td><td>Coverage↓</td><td>0.914 6</td><td>0.923 4</td><td><b>0.868 9</b></td><td>0.882 5</td></tr><tr><td><br /></td><td>Hamming Loss↓</td><td>0.171 6</td><td>0.180 7</td><td><b>0.152 1</b></td><td>0.163 1</td></tr><tr><td><br /></td><td>Ranking Loss↓</td><td>0.167 2</td><td>0.172 6</td><td><b>0.155 4</b></td><td>0.158 9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">由表3与表4可以看出,近邻样例个数越多,分类效果越好,这是由于<i>k</i>值越大,近邻中蕴含的数据分布信息越充分,从而能更好地利用标记之间的相关性使得预测结果更准确;还可以看出,对于PNLC算法,当<i>k</i>=15,<i>α</i>=0.7时,算法各项评价指标的性能最优。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表5 本文算法在yeast数据集上与其他算法的实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Result comparison between the PNLC algorithm and other algorithms on yeast</b></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td>评价指标</td><td>本文算法</td><td>LPLC</td><td>IMLLA</td><td>ML-KNN</td><td>Boost-<br />TEXTER</td></tr><tr><td><br />Average <br />Precision↑</td><td><b>0.785 1</b></td><td>0.772 4</td><td>0.765 0</td><td>0.759 7</td><td>0.736 8</td></tr><tr><td><br />One-Error↓</td><td>0.238 2</td><td>0.241 4</td><td><b>0.235 6</b></td><td>0.241 4</td><td>0.277 7</td></tr><tr><td><br />Coverage↓</td><td><b>6.167 4</b></td><td>6.187 5</td><td>6.254 1</td><td>6.361 5</td><td>6.550 3</td></tr><tr><td><br />Hamming <br />Loss↓</td><td><b>0.192 1</b></td><td>0.194 4</td><td>0.193 2</td><td>0.196 4</td><td>0.220 2</td></tr><tr><td><br />Ranking <br />Loss↓</td><td><b>0.153 2</b></td><td>0.161 3</td><td>0.162 1</td><td>0.160 8</td><td>0.186 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表6 本文算法在image数据集上与其他算法的实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 6 Result comparison between the PNLC algorithm and other algorithms on image</b></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td>评价指标</td><td>本文算法</td><td>LPLC</td><td>IMLLA</td><td>ML-KNN</td><td>Boost-<br />TEXTER</td></tr><tr><td><br />Average <br />Precision↑</td><td><b>0.821 3</b></td><td>0.816 5</td><td>0.815 4</td><td>0.795 8</td><td>0.797 9</td></tr><tr><td><br />One-Error↓</td><td>0.296 5</td><td>0.301 0</td><td><b>0.291 2</b></td><td>0.316 5</td><td>0.310 9</td></tr><tr><td><br />Coverage↓</td><td><b>0.868 9</b></td><td>0.914 6</td><td>0.875 9</td><td>0.941 1</td><td>0.932 9</td></tr><tr><td><br />Hamming <br />Loss↓</td><td><b>0.152 1</b></td><td>0.171 6</td><td>0.157 1</td><td>0.172 3</td><td>0.178 9</td></tr><tr><td><br />Ranking<br /> Loss↓</td><td>0.155 4</td><td>0.167 2</td><td><b>0.153 5</b></td><td>0.169 4</td><td>0.167 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="135">
                    <p class="img_tit"><b>表7 本文算法在cal500数据集上与其他算法的实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 7 Result comparison between the PNLC algorithm and other algorithms on cal500</b></p>
                    <p class="img_note"></p>
                    <table id="135" border="1"><tr><td>评价指标</td><td>本文算法</td><td>LPLC</td><td>IMLLA</td><td>ML-KNN</td><td>Boost-<br />TEXTER</td></tr><tr><td><br />Average <br />Precision↑</td><td><b>0.470 5</b></td><td>0.451 2</td><td>0.440 7</td><td>0.431 2</td><td>0.435 7</td></tr><tr><td><br />One-Error↓</td><td><b>0.351 5</b></td><td>0.357 1</td><td>0.354 6</td><td>0.361 7</td><td>0.365 2</td></tr><tr><td><br />Coverage↓</td><td><b>0.762 4</b></td><td>0.805 2</td><td>0.771 4</td><td>0.812 5</td><td>0.819 6</td></tr><tr><td><br />Hamming <br />Loss↓</td><td><b>0.142 3</b></td><td>0.159 6</td><td>0.155 2</td><td>0.162 7</td><td>0.169 1</td></tr><tr><td><br />Ranking <br />Loss↓</td><td><b>0.185 1</b></td><td>0.191 1</td><td>0.187 5</td><td>0.201 4</td><td>0.202 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表8 本文算法在corel5k数据集上与其他算法的实验结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 8 Result comparison between the PNLC algorithm and other algorithms on corel5k</b></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td>评价指标</td><td>本文算法</td><td>LPLC</td><td>IMLLA</td><td>ML-KNN</td><td>Boost-<br />TEXTER</td></tr><tr><td><br />Average <br />Precision↑</td><td><b>0.325 3</b></td><td>0.307 9</td><td>0.312 6</td><td>0.298 7</td><td>0.291 2</td></tr><tr><td><br />One-Error↓</td><td><b>0.671 3</b></td><td>0.685 1</td><td>0.679 2</td><td>0.701 7</td><td>0.705 1</td></tr><tr><td><br />Coverage↓</td><td><b>0.443 2</b></td><td>0.502 1</td><td>0.471 4</td><td>0.521 1</td><td>0.531 3</td></tr><tr><td><br />Hamming <br />Loss↓</td><td><b>0.072 1</b></td><td>0.090 2</td><td>0.097 1</td><td>0.102 8</td><td>0.107 1</td></tr><tr><td><br />Ranking <br />Loss↓</td><td><b>0.132 1</b></td><td>0.145 4</td><td>0.141 2</td><td>0.153 7</td><td>0.159 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">比较实验中PNLC与LPLC算法的<i>k</i>取值均为15,<i>α</i>取值为0.7。表5的实验结果表明,在yeast数据集上,PNLC算法在One-Error评价指标上的性能略低于IMLLA算法的,但在其他指标上均明显优于其他算法。由表6可知,在image数据集上,PNLC算法在One-Error、Ranking Loss评价指标上的性能略低于IMLLA算法的,但在这5项评价指标上均明显优于LPLC以及除IMLLA外的其他算法。由表7可知,在cal500数据集上,PNLC算法在5项评价指标上均明显优于其他算法。由表8可知,在corel5k数据集上,PNLC算法同样在5项评价指标上均明显优于其他算法。综合以上实验分析可知,PNLC算法在真实世界数据集上的性能优于其他对比多标记分类算法。总之,本文提出的PNLC算法相对于其他成熟的多标记分类算法,取得了良好的分类性能。</p>
                </div>
                <h3 id="138" name="138" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="139">在多标记分类中,每个样本都由一个实例表示,并与多个类标记相关联。现有的多标记学习算法大多是在全局上利用标记相关性,即假设所有的样本共享不同类别标记之间的正相关性。然而,在实际应用中,不同的样本共享不同的标记相关性,标记间不仅存在正相关性,而且还存在相互排斥的现象,即负相关性。针对这一问题,本文提出了基于局部正、负成对标记相关性的<i>k</i>近邻多标记分类算法—PNLC。实验结果表明,改进后的PNLC算法能有效地利用标记相关性显著地提高算法的分类性能。由此可知,多标记分类器可以从标记之间的正、负标记相关性中获益。PNLC算法明显优于LPLC算法以及其他常用的多标记算法。在今后的工作中,还将在构建标记相关性矩阵等方面提出新的方法。另外,针对标记相关性权重的选取也有待进一步深入研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="8">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">

                                <b>[1]</b> Zhang M L,Zhou Z H.A review on multi-label learning algorithms [J].IEEE Transactions on Knowledge and Data Engineering,2014,26(8):1819-1837.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization">

                                <b>[2]</b> Zhang M L,Zhou Z H.Multilabel neural networks with applications to functional genomics and text categorization[J].IEEE Transactions on Knowledge and Data Engineering,2006,18(10):1338-1351.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image multi-label annotation based on supervised nonnegative matrix factorization with new matching measurement">

                                <b>[3]</b> Jia X,Sun F M,Li H J,et al.Image multi-label annotation based on supervised nonnegative matrix factorization with new matching measurement[J].Neurocomputing,2017,219:518-525.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESDDEE661807878AA6CEA6476EB79269D0&amp;v=MjA3NzBPR1FsZkNwYlEzNU5CaHc3eTd3YWc9TmlmT2ZjZk1hNlRLcVk1TlpPd0hDM1JJdmhCZ24wNTdUSGprMldBeWNMQ1NUTTZmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Xia S,Chen P,Zhang J,et al.Utilization of rotation-invariant uniform LBP histogram distribution and statistics of connected regions in automatic image annotation based on multi-label learning[J].Neurocomputing,2017,228:11-18.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label relational neighbor classification using social context features">

                                <b>[5]</b> Wang X,Sukthankar G.Multi-label relational neighbor classification using social context features[C]//Proc of the 19th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining,2013:464-472.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBC7CB8F92330E421B8499871B8587947&amp;v=MDA4Mjc9TmlmT2ZjSExHYUsrcC9sTVp1Z01EQWs5elJkaDRqdDBRWGZscldBOWZMcVRUTDZZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJodzd5N3dhZw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Bianco S,Ciocca G,Napoletano P,et al.An interactive tool for manual,semi-automatic and automatic video annotation[J].Computer Vision &amp; Image Understanding,2015,131(C):88-99.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600740311&amp;v=MTY1ODFMZklKMW9RYXhJPU5pZk9mYks3SHRETnFZOUZZKzhQRDMwNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Boutell M R,Luo J B,Shen X P,et al.Learning multi-label scene classification[J].Pattern Recognition,2004,37(9):1757-1771.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multi-label alternating decision trees from texts and data">

                                <b>[8]</b> de Comité F,Gilleron R,Tommasi M.Learning multi-label alternating decision trees from texts and data[C]//Proc of the 3rd International Conference on Machine Learning and Data Mining in Pattern Recognition,2003:35-49.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label learning by exploiting label correlations locally">

                                <b>[9]</b> Huang S J,Zhou Z H.Multi-label learning by exploiting label correlations locally[C]//Proc of the 26th AAAI Conference on Artificial Intelligence,2012:949-955.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-label hypothesis reuse">

                                <b>[10]</b> Huang S J,Yu Y,Zhou Z H.Multi-label hypothesis reuse[C]//Proc of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2012:525-533.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201409006&amp;v=MzExMjVMT2VaZVJtRnkvaFY3N0lOeWZUYkxHNEg5WE1wbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> He Zhi-fen,Yang Ming,Liu Hui-dong.Joint learning of multi-label classification and label correlations[J].Journal of Software,2014,25(9):1967-1981.(in Chinese)
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LIFT:Multi-label learning with label-specific features">

                                <b>[12]</b> Zhang M L,Wu L.Lift:Multi-label learning with label-specific features[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2015,37(1):107-120.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFAB7DADC4511DB5C716463FC6277A133&amp;v=Mjk3MTE3d2FnPU5pZk9mY1hKYk5hNDN2czJZTzRPRFFoTHltVVU2emw1VG55VTN4UTNmclhsUkxtY0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5CaHc3eQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Huang J,Li G R,Wang S H,et al.Multi-label classification by exploiting local positive and negative pairwise label correlation[J].Neurocomputing,2017,257:164-174.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739502&amp;v=MjcyMThnR0NYdzdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKMW9RYXhJPU5pZk9mYks3SHRETnFZOUZZKw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Zhang M L,Zhou Z H.ML-KNN:A lazy learning approach to multi-label learning[J].Pattern Recognition,2007,40(7):2038-2048.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 Zhang Min-ling.An improved multi-label lazy learning approach[J].Journal of Computer Research and Development,2012,49(11):2271-2282.(in Chinese)
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Disambiguation-free partial label learning">

                                <b>[16]</b> Zhang M L,Yu F,Tang C Z.Disambiguation-free partial label learning[J].IEEE Transactions on Knowledge &amp; Data Engineering,2017,29(10):2155-2167.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 何志芬,杨明,刘会东.多标记分类和标记相关性的联合学习[J].软件学报,2014,25(9):1967-1981.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201211002&amp;v=MDY0MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvaFY3N0lMeXZTZExHNEg5UE5ybzlGWm8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 张敏灵.一种新型多标记懒惰学习算法[J].计算机研究与发展,2012,49(11):2271-2282.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201910020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910020&amp;v=MjcxMjViRzRIOWpOcjQ5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2hWNzdJTHo3Qlo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
