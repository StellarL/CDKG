<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132366022686250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201908019%26RESULT%3d1%26SIGN%3dMmf8e7Fg1BbDC5F9ibJcokH5Lag%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201908019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908019&amp;v=MTg3MTQ3bFZMck5MejdCWmJHNEg5ak1wNDlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="&lt;b&gt;2 图像先验&lt;/b&gt; "><b>2 图像先验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;2.1 梯度和强度的&lt;i&gt;L&lt;/i&gt;&lt;/b&gt;&lt;sub&gt;&lt;b&gt;0&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;正则化先验&lt;/b&gt;"><b>2.1 梯度和强度的<i>L</i></b><sub><b>0</b></sub><b>正则化先验</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;2.2 暗通道先验&lt;/b&gt;"><b>2.2 暗通道先验</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;2.3 基于暗通道和正则化先验去模糊&lt;/b&gt;"><b>2.3 基于暗通道和正则化先验去模糊</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="&lt;b&gt;3 图像去模糊&lt;/b&gt; "><b>3 图像去模糊</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="&lt;b&gt;3.1 预测&lt;/b&gt;"><b>3.1 预测</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;3.2 预测模糊核&lt;/b&gt;"><b>3.2 预测模糊核</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="&lt;b&gt;4 实验结果&lt;/b&gt; "><b>4 实验结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#145" data-title="&lt;b&gt;4.1 文本图像&lt;/b&gt;"><b>4.1 文本图像</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;4.2 自然图像&lt;/b&gt;"><b>4.2 自然图像</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;4.3 低照度图像&lt;/b&gt;"><b>4.3 低照度图像</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;4.4 人脸图像&lt;/b&gt;"><b>4.4 人脸图像</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#161" data-title="&lt;b&gt;5 算法分析&lt;/b&gt; "><b>5 算法分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#162" data-title="&lt;b&gt;5.1 强度&lt;i&gt;L&lt;/i&gt;&lt;/b&gt;&lt;sub&gt;&lt;b&gt;0&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;的正则化先验的有效性&lt;/b&gt;"><b>5.1 强度<i>L</i></b><sub><b>0</b></sub><b>的正则化先验的有效性</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;5.2 暗通道先验的有效性&lt;/b&gt;"><b>5.2 暗通道先验的有效性</b></a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;5.3 局限性&lt;/b&gt;"><b>5.3 局限性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#173" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="图1 清晰图像与模糊图像之间的强度比较">图1 清晰图像与模糊图像之间的强度比较</a></li>
                                                <li><a href="#79" data-title="图2 不同图像块的暗通道图像">图2 不同图像块的暗通道图像</a></li>
                                                <li><a href="#147" data-title="图3 文本图像的去模糊效果">图3 文本图像的去模糊效果</a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;表1 &lt;i&gt;PSNR&lt;/i&gt;&lt;/b&gt;&lt;b&gt;的平均值结果表&lt;/b&gt;"><b>表1 <i>PSNR</i></b><b>的平均值结果表</b></a></li>
                                                <li><a href="#154" data-title="图4 对低照度图像的去模糊结果">图4 对低照度图像的去模糊结果</a></li>
                                                <li><a href="#159" data-title="图5 对人脸图像的去模糊结果">图5 对人脸图像的去模糊结果</a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;表2 我们的图像数据库做人脸图像去模糊结果&lt;/b&gt;"><b>表2 我们的图像数据库做人脸图像去模糊结果</b></a></li>
                                                <li><a href="#166" data-title="图6 与目前比较先进方法进行的对比实验">图6 与目前比较先进方法进行的对比实验</a></li>
                                                <li><a href="#170" data-title="图7 自然图像的去模糊结果">图7 自然图像的去模糊结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="220">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     Chen Hua-hua, Bao Zong-pao.Strong edge-oriented blind deblurring algorithm[J].Journal of Image and Graphics, 2017, 22 (8) :1034-1044. (in Chinese) </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     Fang Shuai, Liu Yuan-dong, Cao Yang, et al.Blur kernel estimation using blurry structure[J].Acta Electronica Sinica, 2017, 45 (5) :1226-1233. (in Chinese) </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_3" title=" Xu L, Jia J Y.Two-phase kernel estimation for robust motion deblurring[C]//Proc of the 11th European Conference on Computer Vision, 2010:157-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-phase kernel estimation for robust motion deblurring">
                                        <b>[3]</b>
                                         Xu L, Jia J Y.Two-phase kernel estimation for robust motion deblurring[C]//Proc of the 11th European Conference on Computer Vision, 2010:157-170.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_4" title=" Xu L, Zheng S C, Jia J Y.Unnatural L0 sparse representation for natural image deblurring[C]//Proc of 2013 IEEE COnference on Computer Vision and Pattern Recognition, 2013:1107-1114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unnatural 10 sparse representation for natural image deblurring">
                                        <b>[4]</b>
                                         Xu L, Zheng S C, Jia J Y.Unnatural L0 sparse representation for natural image deblurring[C]//Proc of 2013 IEEE COnference on Computer Vision and Pattern Recognition, 2013:1107-1114.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_5" title=" Michaeli T, Irani M.Blind deblurring using internal patch recurrence[C]//Proc of the 13th European Conference on Computer Vision, 2014:783-798." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blind deblurring using internal patch recurrence">
                                        <b>[5]</b>
                                         Michaeli T, Irani M.Blind deblurring using internal patch recurrence[C]//Proc of the 13th European Conference on Computer Vision, 2014:783-798.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_6" title=" Pan J S, Hu Z , Su Z X, et al.Deblurring text images via L0-regularized intensity and gradient prior[C]//Proc of 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014:2901-2908." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deblurring Text Images via LO-Regularized Intensity and Gradient Prior">
                                        <b>[6]</b>
                                         Pan J S, Hu Z , Su Z X, et al.Deblurring text images via L0-regularized intensity and gradient prior[C]//Proc of 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014:2901-2908.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_7" title=" Hu Z, Cho S, Wang J, et al.Deblurring low-light images with light streaks[C]//Proc of 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014:3382-3389." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deblurring Low-Light Images with Light Streaks">
                                        <b>[7]</b>
                                         Hu Z, Cho S, Wang J, et al.Deblurring low-light images with light streaks[C]//Proc of 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014:3382-3389.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_8" title=" Pan J S, Sun D Q, Pfister H, et al.Blind image deblurring using dark channel prior[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016:1628-1636." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blind image deblurring using dark channel prior">
                                        <b>[8]</b>
                                         Pan J S, Sun D Q, Pfister H, et al.Blind image deblurring using dark channel prior[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016:1628-1636.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_9" title=" Yan Y Y, Ren W Q, Guo Y F, et al.Image deblurring via extreme channels prior[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition, 2017:6978-6986." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image deblurring via extreme channels prior">
                                        <b>[9]</b>
                                         Yan Y Y, Ren W Q, Guo Y F, et al.Image deblurring via extreme channels prior[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition, 2017:6978-6986.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_10" title=" Zhang K, Zuo W M, Zhang L.Learning a single convolutional super-resolution network for multiple degradations[C]//Proc of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:3262-3271." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Single Convolutional Super-Resolution Network for Multiple Degradations">
                                        <b>[10]</b>
                                         Zhang K, Zuo W M, Zhang L.Learning a single convolutional super-resolution network for multiple degradations[C]//Proc of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:3262-3271.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_11" title=" Zhang Y L, Li K P, Li K, et al.Image super-resolution using very deep residual channel attention networks[C]//Proc of the 15th European Conference on Computer Vision, 2018:294-310." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using very deep residual channel attention networks">
                                        <b>[11]</b>
                                         Zhang Y L, Li K P, Li K, et al.Image super-resolution using very deep residual channel attention networks[C]//Proc of the 15th European Conference on Computer Vision, 2018:294-310.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_12" title=" Zhang Kai, Zuo Wang-meng, Chen Yun-jin, et al.Beyond a Gaussian denoiser:Residual learning of deep CNN for image denoising[J].IEEE Transactions on Image Processing, 2017, 26 (7) :3142-3155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond a Gaussian Denoiser:Residual Learning of Deep CNN for Image Denoising">
                                        <b>[12]</b>
                                         Zhang Kai, Zuo Wang-meng, Chen Yun-jin, et al.Beyond a Gaussian denoiser:Residual learning of deep CNN for image denoising[J].IEEE Transactions on Image Processing, 2017, 26 (7) :3142-3155.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_13" title=" Hradis M, Kotera J, Zemc&#237;k P, et al.Convolutional neural networks for direct text deblurring[C]//Proc of British Machine Vision Conference, 2015:1-13." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Direct Text Deblurring">
                                        <b>[13]</b>
                                         Hradis M, Kotera J, Zemc&#237;k P, et al.Convolutional neural networks for direct text deblurring[C]//Proc of British Machine Vision Conference, 2015:1-13.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_14" title=" Schuler C J, Hirsch M, Harmeling S, et al.Learning to deblur[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 38 (7) :1439-1451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to Deblur">
                                        <b>[14]</b>
                                         Schuler C J, Hirsch M, Harmeling S, et al.Learning to deblur[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 38 (7) :1439-1451.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_15" title=" Xu X Y, Pan J S, Zhang Y J, et al.Motion blur kernel estimation via deep learning[J].IEEE Transactions on Image Processing, 2017, 27 (1) :194-205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Motion Blur Kernel Estimation via Deep Learning">
                                        <b>[15]</b>
                                         Xu X Y, Pan J S, Zhang Y J, et al.Motion blur kernel estimation via deep learning[J].IEEE Transactions on Image Processing, 2017, 27 (1) :194-205.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_16" title=" Kupyn O, Budzan V, Mykhailych M, et al.DeblurGAN:Blind motion deblurring using conditional adversarial networks[C]//Proc of 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:8183-8192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deblur GAN:Blind Motion Deblurring Using Conditional Adversarial Networks">
                                        <b>[16]</b>
                                         Kupyn O, Budzan V, Mykhailych M, et al.DeblurGAN:Blind motion deblurring using conditional adversarial networks[C]//Proc of 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:8183-8192.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_17" title=" Li L R H, Pan J S, Lai W S, et al.Learning a discriminative prior for blind image deblurring[C]//Proc of 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:6166-6125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Prior for Blind Image Deblurring">
                                        <b>[17]</b>
                                         Li L R H, Pan J S, Lai W S, et al.Learning a discriminative prior for blind image deblurring[C]//Proc of 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:6166-6125.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_18" title=" He K M, Sun J, Tang X O.Single image haze removal using dark channel prior[C]//Proc of 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009:1956-1963." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single Image Haze Removal Using Dark Channel Prior">
                                        <b>[18]</b>
                                         He K M, Sun J, Tang X O.Single image haze removal using dark channel prior[C]//Proc of 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009:1956-1963.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_19" title=" Xu L, Lu C W, Xu Y, et al.Image smoothing via L0 gradient minimization[J].ACM Transactions on Graphics, 2011, 30 (6) :Article No.174." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000000999&amp;v=MjI5OTE3SHRqTnI0OUZaT3NQQlhVd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsNFRieGM9TmlmSVk3Sw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Xu L, Lu C W, Xu Y, et al.Image smoothing via L0 gradient minimization[J].ACM Transactions on Graphics, 2011, 30 (6) :Article No.174.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_20" title=" Boracchi G, Foi A.Modeling the performance of image restoration from motion blur[J].IEEE Transactions on Image Processing, 2012, 21 (8) :3502-3517." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling the Performance of Image Restoration from Motion Blur">
                                        <b>[20]</b>
                                         Boracchi G, Foi A.Modeling the performance of image restoration from motion blur[J].IEEE Transactions on Image Processing, 2012, 21 (8) :3502-3517.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_21" title=" Cho H, Wang J, Lee S.Text image deblurring using text-specific properties[C]//Proc of the 12th European Conference on Computer Vision, 2012:524-537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text image deblurring using text-specific properties">
                                        <b>[21]</b>
                                         Cho H, Wang J, Lee S.Text image deblurring using text-specific properties[C]//Proc of the 12th European Conference on Computer Vision, 2012:524-537.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_22" title=" Krishnan D, Tay T, Fergus R.Blind deconvolution using a normalized sparsity measure[C]//Proc of 2011 IEEE Conference on Computer Vision and Pattern Recognition, 2011:233-240." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blind deconvolution using a normalized sparsity measure">
                                        <b>[22]</b>
                                         Krishnan D, Tay T, Fergus R.Blind deconvolution using a normalized sparsity measure[C]//Proc of 2011 IEEE Conference on Computer Vision and Pattern Recognition, 2011:233-240.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_23" title=" Takeda H, Farsiu S, Milanfar P.Deblurring using regularized locally adaptive kernel regression[J].IEEE Transactions on Image Processing, 2008, 17 (4) :550-563." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deblurring using regularized locally adaptive Kernel regression">
                                        <b>[23]</b>
                                         Takeda H, Farsiu S, Milanfar P.Deblurring using regularized locally adaptive kernel regression[J].IEEE Transactions on Image Processing, 2008, 17 (4) :550-563.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_1" title=" 陈华华, 鲍宗袍.强边缘导向的盲去模糊算法[J].中国图像图形学报2017, 22 (8) :1034-1044." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201708002&amp;v=MDk3MTJGeTdsVkxyTlB5cmZiTEc0SDliTXA0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         陈华华, 鲍宗袍.强边缘导向的盲去模糊算法[J].中国图像图形学报2017, 22 (8) :1034-1044.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_2" title=" 方帅, 刘远东, 曹洋, 等.基于模糊结构图的模糊核估计[J].电子学报, 2017, 45 (5) :1226-1233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201705028&amp;v=MDg3NjRlN0c0SDliTXFvOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdsVkxyTklUZlQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         方帅, 刘远东, 曹洋, 等.基于模糊结构图的模糊核估计[J].电子学报, 2017, 45 (5) :1226-1233.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(08),1466-1473 DOI:10.3969/j.issn.1007-130X.2019.08.018            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多种先验的盲图像复原方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E7%85%9C&amp;code=42376372&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许煜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E8%BE%89&amp;code=08529053&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%9A%E6%8C%AF%E5%AE%8F&amp;code=07896524&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尚振宏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%98%86%E6%98%8E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0242668&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">昆明理工大学信息工程与自动化学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于好的还原图像是倾向于清晰图像而不是模糊图像这样一个事实, 提出了一种基于多种先验的有效的盲图像去模糊方法。目前比较好的去模糊方法对于特定场景图像的复原效果不理想, 存在一些模糊, 包括轮廓和细节表示不清晰。为解决这些问题, 结合多个先验知识, 包括暗通道先验、强度图像先验和梯度图像先验知识, 并加以权衡, 就可以在复原过程中为轮廓和细节提供更多的先验信息, 并把这个先验知识放到MAP的框架中, 通过不断地迭代得到估计模糊核, 再利用非盲的图像复原方法对原图像复原。在泛化处理自然环境的多种场景中, 本文方法相较于目前比较先进的方法, 结果的轮廓和细节都有不错的提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B2%E5%9B%BE%E5%83%8F%E5%A4%8D%E5%8E%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">盲图像复原;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9A%97%E9%80%9A%E9%81%93%E5%85%88%E9%AA%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">暗通道先验;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%BA%A6%E5%85%88%E9%AA%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强度先验;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%AF%E5%BA%A6%E5%85%88%E9%AA%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梯度先验;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=MAP%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">MAP算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    许煜 (1992-) , 男, 安徽安庆人, 硕士生, 研究方向为计算机视觉与图像处理。E-mail:1103657742@qq.com通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                                <span>
                                    刘辉 (1969-) , 男, 云南罗平人, 博士, 教授, 研究方向为计算机视觉与模式识别。E-mail:liuhui2169@aliyun.com通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                                <span>
                                    尚振宏 (1975-) , 男, 河南三门峡人, 博士, 副教授, CCF会员 (40175M) , 研究方向为计算机视觉与图像处理。E-mail:shang-zhenhong@126.com通信地址:650500云南省昆明市昆明理工大学信息工程与自动化学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (11873027);</span>
                    </p>
            </div>
                    <h1><b>A blind image deblurring method based on multiple priors</b></h1>
                    <h2>
                    <span>XU Yu</span>
                    <span>LIU Hui</span>
                    <span>SHANG Zhen-hong</span>
            </h2>
                    <h2>
                    <span>Faculty of Information Engineering and Automation, Kunming University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>We propose an effective blind image deblurring method based on multiple priors. Our work is motivated by the fact that a good restored image should favor clear images over blurred images. At present, existing deblurring methods are not ideal for image restoration in specific scenes and there are some blurring, including unclear outlines and details. Aiming at these problems, we propose to combine the prior knowledge of multiple priors, including dark channel priors, intensity priors and gradient priors, and to balance them to provide more priori information for outlines and details during the restoration process. This is of great help for blur kernel estimation. We obtain a total prior knowledge by weighing the three priors, and put it into the maximum a posteriori estimation (MAP) framework. The estimated blur kernel is obtained by iterations, and the original image is restored by using the not blind image restoration method. Our results make great progress compared with current advanced methods, especially the outlines and details in various natural scenarios.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=blind%20image%20restoration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">blind image restoration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dark%20channel%20prior&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dark channel prior;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=intensity%20prior&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">intensity prior;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gradient%20prior&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gradient prior;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=MAP%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">MAP algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Yu, born in 1992, MS candidate, his research interests include computer vision, and image processing.Address:Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, Yunnan, P.R.China;
                                </span>
                                <span>
                                    LIU Hui, born in 1969, PhD, professor, his research interests include computer vision, and pattern recognition.Address:Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, Yunnan, P.R.China;
                                </span>
                                <span>
                                    SHANG Zhen-hong, born in 1975, PhD, associate professor, CCF member (40175M) , his research interests include computer vision, and image processing.Address:Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, Yunnan, P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-09-18</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="54">盲图像去模糊是图像处理和计算机视觉领域中的一个经典问题, 它的目标是将模糊图像中隐藏的图像进行恢复。当模糊形状满足空间不变性的时候, 模糊过程可以用以下的方式进行建模:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">B</mi><mo>=</mo><mi mathvariant="bold-italic">Ι</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>+</mo><mi mathvariant="bold-italic">n</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中, *代表的是卷积算子, <b><i>B</i></b>、<b><i>I</i></b>、<b><i>k</i></b> 和<b><i>n</i></b>分别代表模糊图像、隐藏的清晰图像、模糊核和噪声。式 (1) 中的问题是不适定性问题, 因为<b><i>I</i></b>和<b><i>k</i></b>都是未知的, 存在无穷多个解。为了解决这个问题, 关于模糊核和图像的额外约束和先验知识都是必需的。</p>
                </div>
                <div class="p1">
                    <p id="57">最近的去模糊方法的成功主要归功于有效的图像先验知识和边缘检测策略方面的研究进展。然而, 基于边缘的预测方法常常会涉及到启发式的边缘选择步骤, 当边缘不可预测时, 这种方法表现不佳。为了避免启发式的边缘选择步骤, 人们提出了基于自然图像先验的算法, 包括稀疏归一化、<i>L</i><sub>0</sub>梯度和暗通道先验。这些算法在一般的自然图像上的表现良好, 但是在一些特定领域, 例如文本、人脸以及低照度的图像等, 并不能够很好地适应, 反而有可能得到很差的结果。</p>
                </div>
                <div class="p1">
                    <p id="58">目前最新的边缘增强算法分为隐式算法和显式算法2类<citation id="273" type="reference"><link href="220" rel="bibliography" /><link href="222" rel="bibliography" /><link href="266" rel="bibliography" /><link href="268" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。隐式的边缘增强算法侧重的是得到一个好的图像先验知识, 这个先验知识更接近清晰图像而不是模糊图像。例如梯度稀疏<citation id="270" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、<i>L</i><sub>0</sub>范数梯度<citation id="271" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和自适应算法<citation id="272" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="59">这些典型的先验方法对于自然图像的去模糊非常有效, 但是在一些特定的场景下并不能很好地进行图像复原, 例如低照度的图像和面部图像。所以, Pan等<citation id="274" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>将图像强度和梯度的正则化的先验知识用于文本图像的去模糊。Hu等<citation id="275" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了在极端低照度条件下探测亮条纹来预测模糊核的方法。在处理低照度的图像方面, Pan等<citation id="276" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>又提出了在自然图像中的检测暗通道先验来预测模糊核的方法, 但是在利用暗通道先验处理非暗的像素点时, 该方法并不奏效。所以, Yan等<citation id="277" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>进一步提出了包含一个亮通道的暗通道先验, 提升了算法的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="60">随着深度神经网络的发展, 一些图像复原问题都开始采用深度卷积神经网络, 例如在图像的超分辨率<citation id="284" type="reference"><link href="238" rel="bibliography" /><link href="240" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>和去噪<citation id="278" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等问题。Hradis等<citation id="279" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一种端到端的CNN (Convolutional Neural Network) 方法来处理文本模糊。Schuler等<citation id="280" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>训练了一个深度网络以预测模糊核, 然后采用一个非盲的反卷积方法去模糊。Xu等<citation id="281" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>通过深度神经网络学习模糊图像的边缘来精确预测模糊核, 以达到去模糊的效果。Kupyn等<citation id="282" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>通过GAN (Generative Adversarial Network) 网络学习预测模糊核的运动轨迹, 实现去模糊。还有一些方法是训练深度CNN来当作先验知识, 或者是去噪器来做非盲反卷积。最近Li等<citation id="283" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>训练一个深度CNN作为先验知识, 并把它嵌入到MAP (Maximum A Posteriori estimation) 框架中, 以解决盲图像复原问题。</p>
                </div>
                <div class="p1">
                    <p id="61">为了解决目前盲图像复原的不足之处, 本文结合不同的先验知识, 在图像的复原过程中对估计图像进行约束。例如:暗通道先验有助于自然图像的复原, 强度先验有助于得到显著边缘, 梯度先验对于图像去模糊过程的伪迹抑制非常有效, 从而使得中间图像在还原过程中能够更好地倾向于潜在清晰图像。本文分别为暗通道先验、强度先验和梯度先验分配一个权值, 然后将这些先验放在MAP的框架中, 通过从粗到细的迭代得到最终的模糊核估计值。为了更快得到结果, 我们采用了基于半二次分离的高效数值算法。这个算法在实际使用中可以快速收敛, 并且可以泛化到很多场景中。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag"><b>2 图像先验</b></h3>
                <h4 class="anchor-tag" id="63" name="63"><b>2.1 梯度和强度的<i>L</i></b><sub><b>0</b></sub><b>正则化先验</b></h4>
                <div class="p1">
                    <p id="64">在清晰的图像上, 一般的块区域通常具有接近一致的强度值, 也就是说, 块区域内的像素值通常是非常稀疏的, 而在模糊图像上像素的强度就处于稠密状态。对于模糊图像而言, 像素强度分布都是比较均匀的, 而清晰图像的像素强度就会是稀疏的。如图1所示, 从图1<i>b</i>和图1<i>d</i>的强度直方图可以看出, 清晰图像的像素分布处于离散状态, 最主要的因素是清晰图像的像素值分布在两端, 并且0值很多。而模糊图像的像素分布相对集中, 大部分都不包括0值。为了应用这种稀疏特性, 我们用正则化公式表示:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mtext>i</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中, ‖<b><i>x</i></b>‖<sub>0</sub>表示图像<b><i>x</i></b>中像素值的非零值个数, i表示强度。</p>
                </div>
                <div class="p1">
                    <p id="67">梯度先验被广泛用于图像去模糊过程, 并且在抑制伪迹方面非常有效, 本文采用跟文献<citation id="285" type="reference">[<a class="sup">4</a>]</citation>相同的方法来处理, 梯度先验计算如式 (3) 所示:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false"> (</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mo>∂</mo><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>h</mtext><mo>, </mo><mtext>v</mtext><mo stretchy="false">) </mo></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中, <b><i>x</i></b>表示输入图像, ∇<b><i>x</i></b>表示图像<b><i>x</i></b>的梯度先验, ∂<sub>{h, v}</sub>表示分别对水平方向和垂直方向求偏导, <i>i</i>表示的是像素位置, g是表示梯度的意思, ∂<sub>{h, v}</sub><i>x</i><sub><i>i</i></sub>表示对图像<b><i>x</i></b>在<i>i</i>点处分别对水平方向和垂直方向求偏导。</p>
                </div>
                <div class="p1">
                    <p id="70">梯度先验和强度先验的结合使用, 最早出现在文献<citation id="286" type="reference">[<a class="sup">6</a>]</citation>中, 文中采用的就是由式 (2) 和式 (3) 组合得到的先验。在文献<citation id="287" type="reference">[<a class="sup">6</a>]</citation>中, 梯度和强度先验主要用来处理文本图像模糊, 并且从文献的实验结果可以看出, 梯度先验有利于促进梯度稀疏, 强度先验则有利于显著边缘增强。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 清晰图像与模糊图像之间的强度比较" src="Detail/GetImg?filename=images/JSJK201908019_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 清晰图像与模糊图像之间的强度比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Intensity comparison between clear and blurred images</p>

                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>2.2 暗通道先验</b></h4>
                <div class="p1">
                    <p id="73">暗通道先验最早是由He等<citation id="288" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>应用在去雾算法中, He等指出自然图像通常都存在暗通道。所以, 在自然图像去模糊领域, 也有许多方法利用暗通道去模糊, 比如, Pan等<citation id="289" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的方法。暗通道的稀疏性主要表现在清晰图像的暗通道比模糊图像的暗通道更分散, 而不是通过模糊过程将暗通道的值平滑了。从图2中可以看出, 无论图像块的大小怎么变, 清晰图像的暗通道都比较稀疏, 而且大部分的值都是0, 而模糊图像的暗通道的值总是相对稠密, 大部分的值都是非0的。考虑到自然图像在暗通道先验上体现出的强大稀疏性, 本文也利用暗通道先验对自然图像去模糊。暗通道先验公式如式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>z</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></munder><mo stretchy="false"> (</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>c</mi><mo>∈</mo><mo stretchy="false"> (</mo><mtext>r</mtext><mo>, </mo><mtext>g</mtext><mo>, </mo><mtext>b</mtext><mo stretchy="false">) </mo></mrow></munder><mi>Ι</mi><msup><mrow></mrow><mi>c</mi></msup><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中, <b><i>x</i></b>表示输入图像, <i>z</i>表示像素位置, <i>N</i> (<b><i>x</i></b>) 表示图像<b><i>x</i></b>的一个图像块。<i>c</i>表示颜色, <i>I</i>表示的是颜色通道, 如果是灰度图像的话就只存在一个通道, 则<i>I</i>可以省略。暗通道先验表示的就是一个图像块的最小像素值。</p>
                </div>
                <div class="p1">
                    <p id="76">由于本文在处理过程中将彩色图像转化成灰度图像, 则可以得到以下先验:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>α</mi><mi>Ρ</mi><msub><mrow></mrow><mtext>i</mtext></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>β</mi><mi>Ρ</mi><msub><mrow></mrow><mtext>g</mtext></msub><mo stretchy="false"> (</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>γ</mi><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中, <i>P</i><sub>g</sub> (∇<b><i>x</i></b>) 代表梯度先验, <i>P</i><sub>i</sub> (<b><i>x</i></b>) 代表强度先验, <i>D</i> (<b><i>x</i></b>) 代表暗通道先验, <i>α</i>, <i>β</i>, <i>γ</i>都是权重。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同图像块的暗通道图像" src="Detail/GetImg?filename=images/JSJK201908019_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 不同图像块的暗通道图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Dark channel images of different image blocks</p>

                </div>
                <div class="p1">
                    <p id="80">图2a表示的是在10*10图像块的暗通道图像 (左边是清晰图像, 右边是模糊图像) , 图2b表示的是5*5图像块的暗通道图像, 图2c表示的是2*2图像块的暗通道图像</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>2.3 基于暗通道和正则化先验去模糊</b></h4>
                <div class="p1">
                    <p id="82">我们把先验<i>P</i> (<b><i>x</i></b>) 加入基于MAP的方法中得到下面的能量方程:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">k</mi></mrow></munder><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>δ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <b><i>x</i></b>和<b><i>y</i></b>分别表示清晰图像和模糊图像, <b><i>k</i></b>是模糊核, *是卷积符号, ‖*‖<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>表示<i>L</i><sub>2</sub>正则项。</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag"><b>3 图像去模糊</b></h3>
                <div class="p1">
                    <p id="87">要求得能量方程 (6) 的解, 我们可以分别求解以下子问题:</p>
                </div>
                <div class="p1">
                    <p id="88">关于<b><i>x</i></b>的子问题:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">x</mi></munder><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">关于<i>k</i>的子问题:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">k</mi></munder><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>δ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">我们可以通过分别求解式 (7) 、式 (8) 得到<b><i>x</i></b>和<b><i>k</i></b>。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.1 预测</b></h4>
                <div class="p1">
                    <p id="94">由于式 (7) 中的<i>P</i> (<b><i>x</i></b>) 包括了<i>L</i><sub>0</sub>正则化项, 所以最小化式 (7) 非常困难。这里我们采用了半二次分离的方法来解决此问题。半二次分离方法是高效的迭代最小化方法, 本文引入中间变量<b><i>u</i></b>, <b><i>g</i></b>, <b><i>d</i></b>, 则关于<b><i>x</i></b>的子问题就是:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">x</mi></munder><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>φ</mi><mo stretchy="false">∥</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">g</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>ω</mi><mo stretchy="false">∥</mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>α</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>β</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">g</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mi>γ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中, <i>μ</i>, <i>φ</i>, <i>ω</i>都是超参数, 当<i>μ</i>, <i>φ</i>, <i>ω</i>的值接近于无穷小时, 式 (9) 就相当于式 (7) 的解;可以通过对式 (9) 交替求解得到<b><i>x</i></b>, <b><i>u</i></b>, <b><i>g</i></b>, <b><i>d</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="97">把<b><i>u</i></b>, <b><i>g</i></b>, <b><i>d</i></b>都初始化为零, 在每一次迭代中, <b><i>x</i></b>的值都是通过式 (10) 得到的:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">x</mi></munder><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>φ</mi><mo stretchy="false">∥</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">g</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>ω</mi><mo stretchy="false">∥</mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">关于<b><i>x</i></b>的解为:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo> (</mo><mrow><mfrac><mrow><mover accent="true"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>μ</mi><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">) </mo><mo>+</mo><mi>φ</mi><mi>F</mi><msub><mrow></mrow><mtext>G</mtext></msub><mo>+</mo><mi>ω</mi><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">) </mo></mrow><mrow><mover accent="true"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">) </mo><mo>+</mo><mi>μ</mi><mo>+</mo><mi>φ</mi><mover accent="true"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mo>∇</mo><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover><mi>F</mi><mo stretchy="false"> (</mo><mo>∇</mo><mo stretchy="false">) </mo><mo>+</mo><mi>ω</mi></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">其中, <i>F</i><sup>-1</sup> (·) 和<i>F</i> (·) 分别表示快速傅里叶变换FFT (Fast Fourier Transformation) 的逆变换和快速傅里叶变换FFT, <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mo>⋅</mo><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>表示复共轭, <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mtext>G</mtext></msub><mo>=</mo><mover accent="true"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mo>∇</mo><mtext>h</mtext><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mtext>h</mtext></msub><mo stretchy="false">) </mo><mo>+</mo><mover accent="true"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mo>∇</mo><mtext>v</mtext><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover><mi>F</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mtext>v</mtext></msub><mo stretchy="false">) </mo><mo>, </mo><mo>∇</mo><mtext>h</mtext></mrow></math></mathml>和<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><mtext>v</mtext></mrow></math></mathml>分别表示水平梯度和垂直梯度, <i>g</i><sub>b</sub>和<i>g</i><sub>v</sub>分别表示水平梯度和垂直梯度的值。</p>
                </div>
                <div class="p1">
                    <p id="105">给定<b><i>x</i></b>可以通过式 (12) 计算出<b><i>u</i></b>, <b><i>g</i></b>, <b><i>d</i></b></p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mspace width="0.25em" /><mi mathvariant="bold-italic">u</mi></mrow></munder><mspace width="0.25em" /><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>μ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>α</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">g</mi></munder><mspace width="0.25em" /><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>φ</mi><mo stretchy="false">∥</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo>-</mo><mi mathvariant="bold-italic">g</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>β</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">g</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">d</mi></munder><mspace width="0.25em" /><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">∥</mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>γ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">d</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式 (12) 是求解一个最小值的问题, 因此采用文献<citation id="290" type="reference">[<a class="sup">6</a>,<a class="sup">19</a>]</citation>的方法来求解<b><i>u</i></b>, <b><i>g</i></b>, <b><i>d</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">u</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">x</mi><mo>, </mo><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≥</mo><mi>α</mi><mo>/</mo><mi>μ</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">g</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mo stretchy="false">|</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≥</mo><mi>β</mi><mo>/</mo><mi>φ</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">d</mi><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mo stretchy="false">|</mo><mi>D</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>≥</mo><mi>γ</mi><mo>/</mo><mi>ω</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">计算清晰图像的伪代码如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="110"><b>算法1</b>计算清晰图像</p>
                </div>
                <div class="p1">
                    <p id="111"><b>输入</b>:模糊图像<b><i>y</i></b>和模糊核<b><i>k</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="112"><b>输出</b>:潜在的清晰图像。</p>
                </div>
                <div class="p1">
                    <p id="113"><b><i>x</i></b>←<b><i>y</i></b>, <i>μ</i>←2<i>α</i>//←表示赋值操作。</p>
                </div>
                <div class="p1">
                    <p id="114">while</p>
                </div>
                <div class="p1">
                    <p id="115"> (<i>μ</i>≥<i>μ</i><sub>max</sub>) </p>
                </div>
                <div class="p1">
                    <p id="116">通过式 (13) 求解<b><i>u</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="117"><i>φ</i>←2<i>β</i>;</p>
                </div>
                <div class="p1">
                    <p id="118">while (<i>φ</i>≥<i>φ</i><sub>max</sub>) </p>
                </div>
                <div class="p1">
                    <p id="119">通过式 (14) 求解<b><i>g</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="120"><i>ω</i>←2<i>γ</i>;</p>
                </div>
                <div class="p1">
                    <p id="121">while (<i>ω</i>≥<i>ω</i><sub>max</sub>) </p>
                </div>
                <div class="p1">
                    <p id="122">通过式 (15) 求解<b><i>d</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="123">通过式 (16) 求解<b><i>x</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="124"><i>ω</i>←2<i>ω</i>;</p>
                </div>
                <div class="p1">
                    <p id="125">end</p>
                </div>
                <div class="p1">
                    <p id="126"><i>φ</i>←2<i>φ</i>;</p>
                </div>
                <div class="p1">
                    <p id="127">end</p>
                </div>
                <div class="p1">
                    <p id="128"><i>μ</i>←3.4<i>μ</i>;</p>
                </div>
                <div class="p1">
                    <p id="129">end</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>3.2 预测模糊核</b></h4>
                <div class="p1">
                    <p id="131">在给定<b><i>x</i></b>的情况下, 式 (8) 就是一个最小二乘法的问题, 解决这个问题最好的算法就是FFT算法, 本文在梯度空间中计算这个最小二乘问题。</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">k</mi></munder><mo stretchy="false">∥</mo><mo>∇</mo><mi mathvariant="bold-italic">x</mi><mo>*</mo><mi mathvariant="bold-italic">k</mi><mo>-</mo><mo>∇</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>δ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">k</mi><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">∥</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">式 (16) 可以通过FFT来高效地求解, 在得到<b><i>k</i></b>后, 把一些负的元素设为0, 并使其归一化, 也就是使所有元素之和为1。</p>
                </div>
                <div class="p1">
                    <p id="134">与目前比较先进的方法相似的是模糊核的预测过程都是采用一种从粗到细的图像金字塔结构。算法2展现了在一层中模糊核预测的主要步骤。</p>
                </div>
                <div class="p1">
                    <p id="135"><b>算法2</b>计算模糊核</p>
                </div>
                <div class="p1">
                    <p id="136"><b>输入</b>:模糊图像。</p>
                </div>
                <div class="p1">
                    <p id="137"><b>输出</b>:模糊核<i>k</i>和中间的潜在图像<b><i>x</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="138">初始化<b><i>k</i></b>//类似金字塔由粗到细的初始化</p>
                </div>
                <div class="p1">
                    <p id="139">while (<i>i</i>≤<i>i</i><sub>max</sub>) </p>
                </div>
                <div class="p1">
                    <p id="140">通过式 (11) 求解<b><i>x</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="141">通过式 (16) 求解<b><i>k</i></b></p>
                </div>
                <div class="p1">
                    <p id="142">end</p>
                </div>
                <h3 id="143" name="143" class="anchor-tag"><b>4 实验结果</b></h3>
                <div class="p1">
                    <p id="144">本文给出与目前比较先进方法的盲去模糊对比实验, 所有实验都是在台式机 (CPU E3-1245 v5, RAM 32.00 GB) 上进行, 并且都是在Matlab R2016a环境下运行的。在所有实验中, 参数设置都是一样的:<i>α</i>=0.005, <i>β</i>=0.0005, <i>γ</i>=0.004, 一些超参数设置为:<i>μ</i><sub>max</sub>=2<sup>3</sup>, <i>φ</i><sub>max</sub>=1e5, <i>ω</i><sub>max</sub>=2<sup>2</sup>。本文采用的数据库是文献<citation id="291" type="reference">[<a class="sup">8</a>]</citation>的数据库 (23幅模糊图像, 里面包括了低照度图像、人脸和自然图像) , 并加入了一些自己的图像, 本文中的模糊核都是使用GiacomoBoracchi等<citation id="292" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的方法随机生成的。</p>
                </div>
                <h4 class="anchor-tag" id="145" name="145"><b>4.1 文本图像</b></h4>
                <div class="p1">
                    <p id="146">把本文方法也应用在了一些文本图像上, 得到了不错的效果, 实验结果如图3所示。从图3中可以看出, 本文方法对于文本图像的去模糊结果与目前比较先进的文本去模糊方法相比还有一定的竞争力。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 文本图像的去模糊效果" src="Detail/GetImg?filename=images/JSJK201908019_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 文本图像的去模糊效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Deblurring of text images</p>

                </div>
                <div class="p1">
                    <p id="148">图3a是原图, 图3b是Pan等<citation id="293" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的方法得到的文本去模糊结果, 图3c图是本文方法得出的结果。</p>
                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>4.2 自然图像</b></h4>
                <div class="p1">
                    <p id="150">本文使用Pan等<citation id="294" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的数据库中的自然图像作为测试数据。由于该数据库中没有提供清晰图像, 本文从图像数据库中下载了若干清晰图像, 用于测试峰值信噪比<i>PSNR</i> (Peak Signal to Noise Ratio) 。结果如表1所示。从表1中可以看出, 本文方法的效果比其他2种先进方法的更好。本文方法的自然图像的平均<i>PSNR</i>要高于其他2种方法的。</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表1 <i>PSNR</i></b><b>的平均值结果表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Average <i>PSNR</i></b></p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td rowspan="2"><br />方法来源</td><td colspan="5"><br /><i>PSNR</i></td></tr><tr><td><br />图1</td><td>图2</td><td>图3</td><td>图4</td><td>平均值</td></tr><tr><td><br />本文</td><td>25.03</td><td>21.74</td><td>20.99</td><td>27.79</td><td>20.81</td></tr><tr><td><br />文献[4]</td><td>23.27</td><td>18.00</td><td>20.61</td><td>26.70</td><td>19.13</td></tr><tr><td><br />文献[8]</td><td>24.65</td><td>21.37</td><td>20.91</td><td>27.37</td><td>20.75</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>4.3 低照度图像</b></h4>
                <div class="p1">
                    <p id="153">低照度的盲去模糊一直都是不好解决的问题, 直到暗通道先验的发现, 才使低照度的图像有了一个很好的处理方式。在文献<citation id="295" type="reference">[<a class="sup">8</a>]</citation>的数据库上把本文方法与其他先进的方法比较, 结果如图4所示。</p>
                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 对低照度图像的去模糊结果" src="Detail/GetImg?filename=images/JSJK201908019_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 对低照度图像的去模糊结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Several advanced deblurring results  under low illumination conditions</p>

                </div>
                <div class="p1">
                    <p id="155">从图4中可以看出, 本文方法相比较于Pan等<citation id="296" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的方法, 对于模糊核的估计更加精确, 并且中间图像的估计也较清楚, 得到的图像也更加清晰。实验结果表明, 本文方法在一些背景不是特别复杂的图像上的表现比目前其他比较先进的方法更好。</p>
                </div>
                <h4 class="anchor-tag" id="156" name="156"><b>4.4 人脸图像</b></h4>
                <div class="p1">
                    <p id="157">本文方法处理人脸图像也有很好的结果, 如图5所示, 本文方法的结果与最先进的方法的处理结果不相上下。我们用自己的数据库并结合文献<citation id="297" type="reference">[<a class="sup">8</a>]</citation>的数据库测试了人脸的复原效果, 结果如表2所示。</p>
                </div>
                <div class="p1">
                    <p id="158">从表2中可以看出, 本文方法的人脸复原效果是要优于文献<citation id="298" type="reference">[<a class="sup">4</a>,<a class="sup">8</a>]</citation>的方法。</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 对人脸图像的去模糊结果" src="Detail/GetImg?filename=images/JSJK201908019_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 对人脸图像的去模糊结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Result of face deblurring the proposed method outperforms</p>

                </div>
                <div class="area_img" id="160">
                    <p class="img_tit"><b>表2 我们的图像数据库做人脸图像去模糊结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Average <i>PSNR</i> of face deblurring on our dataset</b></p>
                    <p class="img_note"></p>
                    <table id="160" border="1"><tr><td rowspan="2"><br />方法来源</td><td colspan="6"><br /><i>PSNR</i></td></tr><tr><td><br />图1</td><td>图2</td><td>图3</td><td>图4</td><td>图5</td><td>平均值</td></tr><tr><td><br />本文</td><td>20.67</td><td>24.22</td><td>21.71</td><td>19.97</td><td>23.81</td><td>18.40</td></tr><tr><td><br />文献[4]</td><td>20.85</td><td>23.57</td><td>19.69</td><td>20.48</td><td>23.55</td><td>18.02</td></tr><tr><td><br />文献[8]</td><td>22.15</td><td>22.29</td><td>21.63</td><td>19.22</td><td>23.84</td><td>18.19</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="161" name="161" class="anchor-tag"><b>5 算法分析</b></h3>
                <h4 class="anchor-tag" id="162" name="162"><b>5.1 强度<i>L</i></b><sub><b>0</b></sub><b>的正则化先验的有效性</b></h4>
                <div class="p1">
                    <p id="163">在文献<citation id="299" type="reference">[<a class="sup">6</a>]</citation>中强度先验被当作图像像素的正则化项, 并且在处理过程中图像是根据阈值从粗到细的方法对像素进行选择, 而阈值是随着粗到细的方法不断减小的。本文直接采用了文献<citation id="300" type="reference">[<a class="sup">6</a>]</citation>中的方法来确定阈值。</p>
                </div>
                <div class="p1">
                    <p id="164">由于本文方法是基于MAP的<citation id="302" type="reference"><link href="224" rel="bibliography" /><link href="230" rel="bibliography" /><link href="260" rel="bibliography" /><link href="262" rel="bibliography" /><link href="264" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">6</a>,<a class="sup">21</a>,<a class="sup">22</a>,<a class="sup">23</a>]</sup></citation>多先验盲去模糊, 类似的方法有很多, 通过文献<citation id="301" type="reference">[<a class="sup">6</a>]</citation>, 了解到强度先验有利于促进显著边缘增强, 并且通过实验证明了这一点。在本文提出的算法1中, 通过式 (13) 解出的<b><i>u</i></b>包含了图像亮度值, 通过式 (14) 解出的<b><i>g</i></b>包含了图像的梯度值。也就是说, 我们解出的<b><i>u</i></b>和<b><i>g</i></b>包含了图像<b><i>x</i></b>的结构, 因此算法1求解出的<b><i>x</i></b>会继承<b><i>u</i></b>和<b><i>g</i></b>的相关性质。与其他的盲去模糊方法相比, 本文方法加入了强度先验和暗通道先验, 在估计潜在图像时, 强度先验提供了一个强边缘信息 (如图6所示) , 并且移除了小的细节, 这样就有利于自然图像的核估计, 避免了噪声对核估计的影响。实验表明, 在一些背景不是特别复杂的图像中, 本文方法的去模糊结果比目前其他比较先进的方法更好。因为本文方法加入了强度先验, 结果就倾向于更干净的边缘, 所以对于一些背景复杂度较低的图像, 本文方法会因为强度先验的影响产生增强的边缘效应。</p>
                </div>
                <div class="p1">
                    <p id="165">如图6b和图6e所示, 本文方法产生了更强的边缘, 而只使用了暗通道的方法, 就没办法产生强的边缘效应。此外, 相比较于自然图像, 人脸图像的背景复杂度就要低得多, 所以对于人脸的图像本文方法对于模糊核的估计是要优于Pan等<citation id="303" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的基于暗通道的方法的。</p>
                </div>
                <div class="area_img" id="166">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 与目前比较先进方法进行的对比实验" src="Detail/GetImg?filename=images/JSJK201908019_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 与目前比较先进方法进行的对比实验  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_166.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Compared with current advanced methods</p>

                </div>
                <h4 class="anchor-tag" id="167" name="167"><b>5.2 暗通道先验的有效性</b></h4>
                <div class="p1">
                    <p id="168">暗通道先验是自然图像特有的特征, 在某个图像的块大小里面存在的最小值是0。这种稀疏性正是我们所需要的。目前也有许多把暗通道先验应用在图像复原上的案例, 这说明了暗通道的稀疏特性是很有用的先验特性。</p>
                </div>
                <div class="p1">
                    <p id="169">本文方法是将暗通道先验和强度先验相结合, Xu等<citation id="304" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的方法是只使用梯度先验, Pan等<citation id="305" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的方法是使用梯度先验和暗通道先验。为了公平地比较, 我们使用相同的环境条件, 相同的数据集, 只对先验做改变进行实验。结果如图7所示。从图7中的白框中可以清晰地看到, 使用了暗通道和梯度先验的Pan等<citation id="306" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的方法比使用了梯度先验的Xu等<citation id="307" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的方法得到了更加清晰的轮廓。这说明在添加了暗通道先验后, 对自然图像的模糊核估计更加精准了。而对于复原潜在图像, 本文方法相比Pan等<citation id="308" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>的方法得到了更加清晰的结果。</p>
                </div>
                <div class="area_img" id="170">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201908019_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 自然图像的去模糊结果" src="Detail/GetImg?filename=images/JSJK201908019_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 自然图像的去模糊结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201908019_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 7 Results of a natural image deblurring</p>

                </div>
                <h4 class="anchor-tag" id="171" name="171"><b>5.3 局限性</b></h4>
                <div class="p1">
                    <p id="172">本文方法虽然有很好的效果, 但是仍然存在一些缺点, 在处理一些背景太复杂的图像时效果不是特别理想, 容易出现伪轮廓, 把一些噪声带进对模糊核的估计从而干扰对模糊核的估计。并且在处理一些不存在暗通道的图像时, 处理效果也不是特别好。所以, 未来的工作将会考虑在模糊图像中怎么去除噪声对模糊核估计的影响。</p>
                </div>
                <h3 id="173" name="173" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="174">本文提出了一个高效结合多种先验 (梯度、暗通道和强度先验) 的基于MAP的去模糊方法, 结合强度先验的显著轮廓特性和暗通道的稀疏特性, 使得到的中间结果更加清晰, 这样就有利于对模糊核的估计, 从而得到清晰的模糊核结果。本文在处理正则化先验时引入了高效的半二次分离算法, 这个方法相较于其他方法更加方便。本文方法在自然图像上相较于其它先进的方法有更好的结果, 在一些特定领域也有不错的竞争力。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="217" type="formula" href="images/JSJK201908019_21700.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">许煜</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="218" type="formula" href="images/JSJK201908019_21800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">刘辉</span>
                                    </div>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="219" type="formula" href="images/JSJK201908019_21900.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">尚振宏</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="220">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 Chen Hua-hua, Bao Zong-pao.Strong edge-oriented blind deblurring algorithm[J].Journal of Image and Graphics, 2017, 22 (8) :1034-1044. (in Chinese) 
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 Fang Shuai, Liu Yuan-dong, Cao Yang, et al.Blur kernel estimation using blurry structure[J].Acta Electronica Sinica, 2017, 45 (5) :1226-1233. (in Chinese) 
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-phase kernel estimation for robust motion deblurring">

                                <b>[3]</b> Xu L, Jia J Y.Two-phase kernel estimation for robust motion deblurring[C]//Proc of the 11th European Conference on Computer Vision, 2010:157-170.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unnatural 10 sparse representation for natural image deblurring">

                                <b>[4]</b> Xu L, Zheng S C, Jia J Y.Unnatural L0 sparse representation for natural image deblurring[C]//Proc of 2013 IEEE COnference on Computer Vision and Pattern Recognition, 2013:1107-1114.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blind deblurring using internal patch recurrence">

                                <b>[5]</b> Michaeli T, Irani M.Blind deblurring using internal patch recurrence[C]//Proc of the 13th European Conference on Computer Vision, 2014:783-798.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deblurring Text Images via LO-Regularized Intensity and Gradient Prior">

                                <b>[6]</b> Pan J S, Hu Z , Su Z X, et al.Deblurring text images via L0-regularized intensity and gradient prior[C]//Proc of 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014:2901-2908.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deblurring Low-Light Images with Light Streaks">

                                <b>[7]</b> Hu Z, Cho S, Wang J, et al.Deblurring low-light images with light streaks[C]//Proc of 2014 IEEE Conference on Computer Vision and Pattern Recognition, 2014:3382-3389.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blind image deblurring using dark channel prior">

                                <b>[8]</b> Pan J S, Sun D Q, Pfister H, et al.Blind image deblurring using dark channel prior[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016:1628-1636.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image deblurring via extreme channels prior">

                                <b>[9]</b> Yan Y Y, Ren W Q, Guo Y F, et al.Image deblurring via extreme channels prior[C]//Proc of 2017 IEEE Conference on Computer Vision and Pattern Recognition, 2017:6978-6986.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Single Convolutional Super-Resolution Network for Multiple Degradations">

                                <b>[10]</b> Zhang K, Zuo W M, Zhang L.Learning a single convolutional super-resolution network for multiple degradations[C]//Proc of the 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:3262-3271.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using very deep residual channel attention networks">

                                <b>[11]</b> Zhang Y L, Li K P, Li K, et al.Image super-resolution using very deep residual channel attention networks[C]//Proc of the 15th European Conference on Computer Vision, 2018:294-310.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond a Gaussian Denoiser:Residual Learning of Deep CNN for Image Denoising">

                                <b>[12]</b> Zhang Kai, Zuo Wang-meng, Chen Yun-jin, et al.Beyond a Gaussian denoiser:Residual learning of deep CNN for image denoising[J].IEEE Transactions on Image Processing, 2017, 26 (7) :3142-3155.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Direct Text Deblurring">

                                <b>[13]</b> Hradis M, Kotera J, Zemcík P, et al.Convolutional neural networks for direct text deblurring[C]//Proc of British Machine Vision Conference, 2015:1-13.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to Deblur">

                                <b>[14]</b> Schuler C J, Hirsch M, Harmeling S, et al.Learning to deblur[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2014, 38 (7) :1439-1451.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Motion Blur Kernel Estimation via Deep Learning">

                                <b>[15]</b> Xu X Y, Pan J S, Zhang Y J, et al.Motion blur kernel estimation via deep learning[J].IEEE Transactions on Image Processing, 2017, 27 (1) :194-205.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deblur GAN:Blind Motion Deblurring Using Conditional Adversarial Networks">

                                <b>[16]</b> Kupyn O, Budzan V, Mykhailych M, et al.DeblurGAN:Blind motion deblurring using conditional adversarial networks[C]//Proc of 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:8183-8192.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Prior for Blind Image Deblurring">

                                <b>[17]</b> Li L R H, Pan J S, Lai W S, et al.Learning a discriminative prior for blind image deblurring[C]//Proc of 2018 IEEE Conference on Computer Vision and Pattern Recognition, 2018:6166-6125.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single Image Haze Removal Using Dark Channel Prior">

                                <b>[18]</b> He K M, Sun J, Tang X O.Single image haze removal using dark channel prior[C]//Proc of 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009:1956-1963.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000000999&amp;v=MTQ1ODVIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMZklKbDRUYnhjPU5pZklZN0s3SHRqTnI0OUZaT3NQQlhVd29CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Xu L, Lu C W, Xu Y, et al.Image smoothing via L0 gradient minimization[J].ACM Transactions on Graphics, 2011, 30 (6) :Article No.174.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling the Performance of Image Restoration from Motion Blur">

                                <b>[20]</b> Boracchi G, Foi A.Modeling the performance of image restoration from motion blur[J].IEEE Transactions on Image Processing, 2012, 21 (8) :3502-3517.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text image deblurring using text-specific properties">

                                <b>[21]</b> Cho H, Wang J, Lee S.Text image deblurring using text-specific properties[C]//Proc of the 12th European Conference on Computer Vision, 2012:524-537.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blind deconvolution using a normalized sparsity measure">

                                <b>[22]</b> Krishnan D, Tay T, Fergus R.Blind deconvolution using a normalized sparsity measure[C]//Proc of 2011 IEEE Conference on Computer Vision and Pattern Recognition, 2011:233-240.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deblurring using regularized locally adaptive Kernel regression">

                                <b>[23]</b> Takeda H, Farsiu S, Milanfar P.Deblurring using regularized locally adaptive kernel regression[J].IEEE Transactions on Image Processing, 2008, 17 (4) :550-563.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201708002&amp;v=MDIyNTZxQnRHRnJDVVJMT2VaZVJtRnk3bFZMck5QeXJmYkxHNEg5Yk1wNDlGWm9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 陈华华, 鲍宗袍.强边缘导向的盲去模糊算法[J].中国图像图形学报2017, 22 (8) :1034-1044.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201705028&amp;v=MzA4MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3bFZMck5JVGZUZTdHNEg5Yk1xbzlIYklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 方帅, 刘远东, 曹洋, 等.基于模糊结构图的模糊核估计[J].电子学报, 2017, 45 (5) :1226-1233.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201908019" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201908019&amp;v=MTg3MTQ3bFZMck5MejdCWmJHNEg5ak1wNDlFYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDY2UXI4RUJreUIrY05aSWZBQVJmWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
