<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358108780000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909019%26RESULT%3d1%26SIGN%3dOxx2iBhIyb1lQl1NzY9%252bySMrX1k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909019&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909019&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909019&amp;v=MTk1NDdlWmVSbUZ5N2tVYjNKTHo3QlpiRzRIOWpNcG85RWJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="&lt;b&gt;2 基于对抗的网络模型&lt;/b&gt; "><b>2 基于对抗的网络模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;2.1 生成对抗网络&lt;/b&gt;"><b>2.1 生成对抗网络</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;2.2 对抗性的领域适应算法&lt;/b&gt;"><b>2.2 对抗性的领域适应算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;3 带有双判别器的对抗性领域适应算法&lt;/b&gt; "><b>3 带有双判别器的对抗性领域适应算法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="&lt;b&gt;4 实验&lt;/b&gt; "><b>4 实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="(1)MNIST:">(1)MNIST:</a></li>
                                                <li><a href="#96" data-title="(2)USPS:">(2)USPS:</a></li>
                                                <li><a href="#98" data-title="(3)SVHN:">(3)SVHN:</a></li>
                                                <li><a href="#100" data-title="(4)Synthetic Numbers:">(4)Synthetic Numbers:</a></li>
                                                <li><a href="#102" data-title="(5)Synthetic Signs:">(5)Synthetic Signs:</a></li>
                                                <li><a href="#104" data-title="(6)GTSRB:">(6)GTSRB:</a></li>
                                                <li><a href="#106" data-title="(7)CIFAR-10:">(7)CIFAR-10:</a></li>
                                                <li><a href="#108" data-title="(8)STL-10:">(8)STL-10:</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="图1 本文算法结构和信息流图(虚线表示参数值固定)">图1 本文算法结构和信息流图(虚线表示参数值固定)</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表1 单通道适应任务下的实验结果&lt;/b&gt;"><b>表1 单通道适应任务下的实验结果</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表2 三通道适应任务下的实验结果&lt;/b&gt;"><b>表2 三通道适应任务下的实验结果</b></a></li>
                                                <li><a href="#120" data-title="图2 本文算法在USPS→MNIST任务上的适应效果">图2 本文算法在USPS→MNIST任务上的适应效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="153">


                                    <a id="bibliography_1" title=" Gretton A,Smola A J,Huang J Y,et al.Covariate shift and local learning by distribution matching[M].Cambridge:MIT Press,2009:131-160." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Covariate shift and local learning by distribution matching">
                                        <b>[1]</b>
                                         Gretton A,Smola A J,Huang J Y,et al.Covariate shift and local learning by distribution matching[M].Cambridge:MIT Press,2009:131-160.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_2" title=" Pan S J,Yang Q.A survey on transfer learning[J].IEEE Transactions on Knowledge &amp;amp; Data Engineering,2010,22(10):1345-1359." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey on Transfer Learning">
                                        <b>[2]</b>
                                         Pan S J,Yang Q.A survey on transfer learning[J].IEEE Transactions on Knowledge &amp;amp; Data Engineering,2010,22(10):1345-1359.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_3" title=" Long Ming-sheng.Transfer learning:Problems and methods[D].Beijing:Tsinghua University,2014.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transfer learning:Problems and methods">
                                        <b>[3]</b>
                                         Long Ming-sheng.Transfer learning:Problems and methods[D].Beijing:Tsinghua University,2014.(in Chinese)
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_4" title=" Goodfellow I J,Pouget A J,Mirza M,et al.Generative adversarial nets [C]//Proc of International Conference on Neural Information Processing Systems,2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[4]</b>
                                         Goodfellow I J,Pouget A J,Mirza M,et al.Generative adversarial nets [C]//Proc of International Conference on Neural Information Processing Systems,2014:2672-2680.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_5" title=" Ganin Y,Lempitsky V.Unsupervised domain adaptation by backpropagation [C]//Proc of the 32nd International Conference on Machine Learning,2015:1180-1189." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Domain Adaptation by Backpropagation">
                                        <b>[5]</b>
                                         Ganin Y,Lempitsky V.Unsupervised domain adaptation by backpropagation [C]//Proc of the 32nd International Conference on Machine Learning,2015:1180-1189.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     Zhao Zeng-shun,Gao Han-xu,Sun Qian,et al.Latest development of the theory framework,derivative model and application of generative adversarial nets[J].Journal of Chinese Computer Systems,2018,39(12):44-48.(in Chinese)</a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_7" title=" Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].Computer Science,arXiv:1511.06434,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised rep-resentation learning with deep convolutional generative ad-versarial networks">
                                        <b>[7]</b>
                                         Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].Computer Science,arXiv:1511.06434,2015.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_8" title=" Nguyen T D,Le T,Vu H,et al.Dual discriminator generative adversarial nets[C]//Proc of the 31st International Conference on Neural Information Processing Systems,2017:2671-2681." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dual discriminator generative adversarial nets">
                                        <b>[8]</b>
                                         Nguyen T D,Le T,Vu H,et al.Dual discriminator generative adversarial nets[C]//Proc of the 31st International Conference on Neural Information Processing Systems,2017:2671-2681.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_9" title=" Arjovsky M,Chintala S,Bottou L.Wasserstein GAN[EB /OL].[2017-01-26].https://arxiv.org /abs/1701.07875.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wasserstein gan">
                                        <b>[9]</b>
                                         Arjovsky M,Chintala S,Bottou L.Wasserstein GAN[EB /OL].[2017-01-26].https://arxiv.org /abs/1701.07875.pdf.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_10" title=" Wang Gong-ming,Qiao Jun-fei,Wang Lei.A generative adversarial network based on energy function[J].Acta Automatica Sinica,2018,44(5):793-803.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805003&amp;v=MjcyNzZxQnRHRnJDVVJMT2VaZVJtRnk3a1ViM0lLQ0xmWWJHNEg5bk1xbzlGWjRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Wang Gong-ming,Qiao Jun-fei,Wang Lei.A generative adversarial network based on energy function[J].Acta Automatica Sinica,2018,44(5):793-803.(in Chinese)
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_11" title=" Ganin Y,Ustinova E,Ajakan H,et al.Domain-adversarial training of neural networks[J].Journal of Machine Learning Research,2017,17(1):2096-2030." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Domain-adversarial training of neural networks">
                                        <b>[11]</b>
                                         Ganin Y,Ustinova E,Ajakan H,et al.Domain-adversarial training of neural networks[J].Journal of Machine Learning Research,2017,17(1):2096-2030.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_12" title=" Liu M Y,Tuzel O.Coupled generative adversarial networks [C]//Proc of the 30th Conference on Neural Information Processing Systems(NIPS 2016),2016:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled generative adversarial networks">
                                        <b>[12]</b>
                                         Liu M Y,Tuzel O.Coupled generative adversarial networks [C]//Proc of the 30th Conference on Neural Information Processing Systems(NIPS 2016),2016:1-9.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_13" title=" Tzeng E,Hoffman J,Saenko K,et al.Adversarial discriminative domain adaptation[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition,2017:2962-2971." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial discriminative domain adaptation">
                                        <b>[13]</b>
                                         Tzeng E,Hoffman J,Saenko K,et al.Adversarial discriminative domain adaptation[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition,2017:2962-2971.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_14" title=" Arjovsky M,Bottou L.Towards principled methods for training generative adversarial networks[J].arXiv:1701.04862,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards principled methods for training generative adversarial networks">
                                        <b>[14]</b>
                                         Arjovsky M,Bottou L.Towards principled methods for training generative adversarial networks[J].arXiv:1701.04862,2017.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_15" title=" Zhuang Fu-zhen,Luo Ping,He Qing,et al.Survey on transfer learning research[J].Journal of Software,2015,26(1):26-39.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201501003&amp;v=MTk5NzdiM0lOeWZUYkxHNEg5VE1ybzlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Zhuang Fu-zhen,Luo Ping,He Qing,et al.Survey on transfer learning research[J].Journal of Software,2015,26(1):26-39.(in Chinese)
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_16" title=" Tzeng E,Hoffman J,Darrell T,et al.Simultaneous deep transfer across domains and tasks[C]//Proc of IEEE International Conference on Computer Vision,2017:4068-4076." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous deep transfer across domains and tasks">
                                        <b>[16]</b>
                                         Tzeng E,Hoffman J,Darrell T,et al.Simultaneous deep transfer across domains and tasks[C]//Proc of IEEE International Conference on Computer Vision,2017:4068-4076.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_17" title=" Ghifary M,Kleijn W B,Zhang M,et al.Deep reconstruction-classification networks for unsupervised domain adaptation [C]//Proc of European Conference on Computer Vision,2016:597-613." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation">
                                        <b>[17]</b>
                                         Ghifary M,Kleijn W B,Zhang M,et al.Deep reconstruction-classification networks for unsupervised domain adaptation [C]//Proc of European Conference on Computer Vision,2016:597-613.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_18" title=" Coates A,Lee H,Ng A Y.An analysis of single-layer networks in unsupervised feature learning [J].Machine Learning Research,2011,15:215-223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An analysis of single-layer networks in unsupervised feature learning">
                                        <b>[18]</b>
                                         Coates A,Lee H,Ng A Y.An analysis of single-layer networks in unsupervised feature learning [J].Machine Learning Research,2011,15:215-223.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_19" title=" Tzeng E,Hoffman J,Zhang N,et al.Deep domain confusion:Maximizing for domain invariance [J].Computer Science,arXiv:1412.3474v1,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Domain Confusion:Maximizing for Domain Invariance">
                                        <b>[19]</b>
                                         Tzeng E,Hoffman J,Zhang N,et al.Deep domain confusion:Maximizing for domain invariance [J].Computer Science,arXiv:1412.3474v1,2014.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_20" title=" Donahue J,Jia Y,Vinyals O,et al.DeCAF:A deep convolutional activation feature for generic visual recognition [C]//Proc of the 31st International Conference on Machine Learning,2014:988-996.附中文参考文献:" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decaf: A deep convolutional activation feature for generic visual recognition">
                                        <b>[20]</b>
                                         Donahue J,Jia Y,Vinyals O,et al.DeCAF:A deep convolutional activation feature for generic visual recognition [C]//Proc of the 31st International Conference on Machine Learning,2014:988-996.附中文参考文献:
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_3" title=" 龙明盛.迁移学习问题与方法研究[D].北京:清华大学,2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1015039180.nh&amp;v=MjM5NTZQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzSVZGMjZHN083RjlERXI1RWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         龙明盛.迁移学习问题与方法研究[D].北京:清华大学,2014.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_6" title=" 赵增顺,高寒旭,孙骞,等.生成对抗网络理论框架、衍生模型与应用最新进展[J].小型微型计算机系统,2018,39(12):44-48." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201812009&amp;v=MjU2OTNZOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzSVBUWGNkckc0SDluTnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         赵增顺,高寒旭,孙骞,等.生成对抗网络理论框架、衍生模型与应用最新进展[J].小型微型计算机系统,2018,39(12):44-48.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     王功明,乔俊飞,王磊.一种能量函数意义下的生成式对抗网络[J].自动化学报,2018,44(5):793-803.</a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     庄福振,罗平,何清,等.迁移学习研究进展[J].软件学报,2015,26(1):26-39.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1656-1661 DOI:10.3969/j.issn.1007-130X.2019.09.018            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>带有双判别器的对抗性领域适应图像分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E6%B5%A9&amp;code=11257868&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%8D%AB%E6%96%8C&amp;code=07528115&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭卫斌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%9C%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0024290&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华东理工大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>生成对抗网络的出现将对抗学习的思想引入了机器学习的不同知识体系,带来了全新的发展。对抗性的领域适应算法利用一个共享特征提取器提取域不变表征,一个判别器进行辨别,双方通过对抗性的迭代更新方式达到最优解。在数据来源上,生成对抗网络和领域适应都有极其类似的2个域。在目标函数上,两者都试图追寻一致性。从理论和逻辑结构出发分析两者的内在相似性,尝试利用已成熟的生成对抗网络体系从更深层次进一步提升领域适应性能。通过类比,提出使用2个判别器解决已有对抗性领域适应算法中存在的“模式崩溃”问题,并使用伪标签进行结构上的完善。最后,在标准领域适应任务上的实验表明了本文算法的可行性和有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%86%E5%9F%9F%E9%80%82%E5%BA%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">领域适应;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">对抗网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    许浩（1996-），男，江苏邳州人，硕士生，研究方向为迁移学习和深度学习。E-mail:xh135345@qq.com,通信地址：200237上海市徐汇区梅陇路130号华东理工大学信息科学与工程学院&lt;image id="150" type="formula" href="images/JSJK201909019_15000.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    郭卫斌（1968-），男，陕西西安人，博士，副教授，CCF会员（08860S），研究方向为高性能计算、软件工程和大数据。E-mail:gweibin@ecust.edu.cn,通信地址：200237上海市徐汇区梅陇路130号华东理工大学信息科学与工程学院&lt;image id="152" type="formula" href="images/JSJK201909019_15200.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-02</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61672227);</span>
                    </p>
            </div>
                    <h1><b>An adversarial domain adaptation image classification algorithm with dual discriminators</b></h1>
                    <h2>
                    <span>XU Hao</span>
                    <span>GUO Wei-bin</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Engineering,East China University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Since the appearance of generative adversarial networks, adversarial learning has been widely used in various branches of machine learning, which drives new developments. In domain adaptation, adversarial domain adaptation methods use a shared feature extractor to extract the domain invariant representation and a discriminator to judge. Both reach optimal solution through the iterative update of adversarial learning. In terms of data sources, generative adversarial networks and domain adaptation have two similar domains. In the aspect of objective functions, both try to pursue consistency. We attempt to further enhance the domain adaption performance from a deeper level by taking advantage of the mature framework of generative adversarial networks and analyzing intrinsic similarities between the two based on the theoretical level and logical structure. By analogy, the model employs two discriminators to solve the mode collapse problem existing in previous adversarial domain adaptation algorithms, and utilzies pseudo labels for structural improvement. Finally, experiments on standard domain adaptation tasks confirm the feasibility and effectiveness of the method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=domain%20adaptation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">domain adaptation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adversarial%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adversarial networks;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    XU Hao,born in 1996,MS candidate, his research interests include transfer learning,and deep learning.Address:School of Information Science and Engineering,East China University of Science and Technology,130Meilong Road,Xuhui District,Shanghai 200237,P.R.China;
                                </span>
                                <span>
                                    GUO Wei-bin,born in 1968,PhD,associate professor,CCF member(08860S),his research interests include high performance computing,software engineering,and big data.Address:School of Information Science and Engineering,East China University of Science and Technology,130Meilong Road,Xuhui District,Shanghai 200237,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2019-01-02</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="52">大数据时代为机器学习的广泛应用创造了条件,得益于大量用于训练的已标注样本,有监督深度学习在计算机视觉、自然语言处理和语音识别等不同的领域取得了巨大的成功。然而,获取到大规模有标签的数据集却不是一件易事。面对日益剧增的数据量,传统的人工标注方法在投入成本方面已不可取,而不同但相关的领域中因为方差偏移(Covariate Shift)<citation id="201" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的存在使已训练模型的迁移效果大打折扣,如何有效地解决目标领域中标注样本稀缺或不存在的问题已成为一项富有挑战的工作。</p>
                </div>
                <div class="p1">
                    <p id="53">迁移学习<citation id="202" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,或称领域适应<citation id="203" type="reference"><link href="157" rel="bibliography" /><link href="193" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">3</a>]</sup></citation>,用于将某个领域中学到的知识或模型迁移到不同但相关的领域中去,作为一个解决上述问题的系统性框架得到了快速发展。相比较传统的浅层领域适应,应用深层神经网络的领域适应算法在共享特征提取和分类精度上表现更为优越,其中以与基于生成对抗网络GAN(Generative Adversarial Nets)<citation id="204" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的对抗思想的结合最为突出。</p>
                </div>
                <div class="p1">
                    <p id="54">本文以无监督领域适应UDA(Unsupervised Domain Adaptation)<citation id="205" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的图像分类为背景,从算法理论和数据处理逻辑出发,比较GAN和领域适应的相似性。基于已有的结合对抗思想的领域适应算法,类比GAN的缺陷和改进,尝试从更深层次利用两者的贴合性进一步优化迁移性能。此外,考虑到两者任务上的差异,引入了伪标签来进行完善,即将最有可能标签作为实际标签来解决目标领域中无标注样本的学习问题。实验采用常用的UDA数据集,在标准迁移任务上的测试达到了预期效果,证实了本文算法的可行性和有效性。</p>
                </div>
                <div class="p1">
                    <p id="55">本文的主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="56">(1)从算法理论和逻辑结构方面深入分析比较生成对抗网络和领域适应的相似性和差异性。</p>
                </div>
                <div class="p1">
                    <p id="57">(2)基于改进的生成对抗网络方法利用相似性提升迁移性能,并使用伪标签进行任务差异上的补充完善。</p>
                </div>
                <div class="p1">
                    <p id="58">(3)实验表明了利用已成熟的生成对抗网络体系框架改进迁移学习的可行性和广阔前景。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag"><b>2 基于对抗的网络模型</b></h3>
                <h4 class="anchor-tag" id="60" name="60"><b>2.1 生成对抗网络</b></h4>
                <div class="p1">
                    <p id="61">GAN受启发于博弈论中的二人零和博弈,生成器<i>G</i>利用服从先验分布<i>p</i><sub><i>z</i></sub>(<i>z</i>)的随机噪声<i>z</i>学习到真实数据<i>x</i>的概率分布<i>p</i><sub>data</sub>(<i>x</i>),判别器<i>D</i>则对<i>G</i>生成的数据和真实数据进行辨别,两者通过最小最大的对抗游戏达到最终的纳什均衡,其目标函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>min</mi><msub><mrow></mrow><mi>G</mi></msub><mspace width="0.25em" /><mi>max</mi><msub><mrow></mrow><mi>D</mi></msub><mspace width="0.25em" /><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">理论的支撑和具备学习高维复杂的真实数据分布的优点使生成对抗网络获得了广泛的关注,在图像生成、图像风格迁移、高清视频帧生成和领域适应等领域蓬勃发展<citation id="206" type="reference"><link href="163" rel="bibliography" /><link href="195" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">6</a>]</sup></citation>,相关改进的模型变体层出不穷。深度卷积生成对抗网络DCGAN(Deep Convolutional Generative Adversarial Networks)<citation id="207" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>采用卷积神经网络从结构上对原始模型训练不稳定等缺点进行改进,取得了极佳的效果。双判别生成对抗网络D2GAN(Dual Discriminator Generative Adversarial Nets)<citation id="208" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>从理论层面提出结合散度KL(Kullback-Leibler)和反KL散度差异的统一目标函数,Wassertein生成对抗网络WGAN(Wasserstein GAN)<citation id="209" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>则使用了Wassertein距离进行度量的算法。文献<citation id="210" type="reference">[<a class="sup">10</a>,<a class="sup">10</a>]</citation>基于重构误差提出一种能量函数意义下的评判指标,提高了GAN的学习效率。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>2.2 对抗性的领域适应算法</b></h4>
                <div class="p1">
                    <p id="65">在领域适应任务中,源领域和目标领域的划分极好地贴合了GAN中对于生成数据分布和真实数据分布的概念,生成器<i>G</i>变成了带有参数<i>θ</i><sub>f</sub>的共享特征提取器,判别器<i>D</i>则通过不断更新参数<i>θ</i><sub>d</sub>来继续充当最佳辨别者的角色,结合训练参数<i>θ</i><sub><i>y</i></sub>的分类问题,应用了对抗思想的领域适应算法神经网络的领域对抗训练DANN(Domain-Adversarial training of Neural Networks)<citation id="211" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>如下所示:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mi>y</mi></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>L</mi></mstyle><msubsup><mrow></mrow><mi>y</mi><mi>i</mi></msubsup><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">)</mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mo stretchy="false">(</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>L</mi></mstyle><msubsup><mrow></mrow><mtext>d</mtext><mi>i</mi></msubsup><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">)</mo><mo>+</mo><mfrac><mn>1</mn><msup><mi>n</mi><mo>′</mo></msup></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup><mi>L</mi></mstyle><msubsup><mrow></mrow><mtext>d</mtext><mi>i</mi></msubsup><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中,<i>n</i>和<i>n</i>′分别代表源领域和目标领域中的样本数,<i>n</i>+<i>n</i>′=<i>N</i>,<i>λ</i>为权重参数,<i>L</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>i</mi></msubsup></mrow></math></mathml>(<i>θ</i><sub>f</sub>,<i>θ</i><sub><i>y</i></sub>)和<i>L</i><sup><i>i</i></sup><sub>d</sub>(<i>θ</i><sub>f</sub>,<i>θ</i><sub>d</sub>)分别为样本<i>i</i>的标签分类损失和领域判别损失。迭代交替更新公式如式(3)和式(4)所示。</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>arg</mi><mspace width="0.25em" /><mi>min</mi><msub><mrow></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></msub><mi>E</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mi>y</mi></msub><mo>,</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>d</mtext></msub><mo>=</mo><mi>arg</mi><mspace width="0.25em" /><mi>max</mi><msub><mrow></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow></msub><mi>E</mi><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mtext>f</mtext></msub><mo>,</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>y</mi></msub><mo>,</mo><mi>θ</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">获得最优解<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></mathml><sub>f</sub>,<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></mathml><sub><i>y</i></sub>,<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></mathml><sub>d</sub>,即最优的适应性能。偶合生成对抗网络CoGAN(Coupled Generative Adversarial Networks)<citation id="212" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>则直接使用2个生成对抗网络,利用共享权重参数试图从源领域和目标领域中学习域不变表征。</p>
                </div>
                <div class="p1">
                    <p id="70">对抗判别域适应算法ADDA(Adversarial Discriminative Domain Adaptation)<citation id="213" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>指出GAN虽具有极佳的可视化生成能力,但因任务上的差异,并不适合处理判别分类问题。文献<citation id="214" type="reference">[<a class="sup">13</a>]</citation>在总结概括先前方法的基础上提出了一个泛化的对抗性领域适应框架,即从是否使用生成模型、是否共享不同领域中特征提取层的参数以及目标损失函数的选择3个方面来考虑。并在分析发现DANN同样存在和原始GAN的梯度消失问题后,提出了最佳的选择方案,将自身归类为仅使用判别模型的算法。即便如此,ADDA在逻辑流程上却更加符合GAN的结构设计,仍可以从生成对抗框架的角度进行类比分析。DANN将基础的领域适应方式,如共享特征提取和迁移已训练模型,分成了3个具有先后顺序的阶段。第1阶段为常用的有监督深度学习,使用源领域中带标签<i>Y</i><sub>s</sub>的样本<i>X</i><sub>s</sub>训练源特征提取器<i>M</i><sub>s</sub>和标签预测器<i>C</i>:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>min</mi><msub><mrow></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>C</mi></mrow></msub><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">(</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo>∼</mo><mo stretchy="false">(</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo></mrow></msub><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup><mn>1</mn></mstyle><msub><mrow></mrow><mrow><mo stretchy="false">[</mo><mi>k</mi><mo>=</mo><mi>y</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">]</mo></mrow></msub><mi>log</mi><mspace width="0.25em" /><mi>C</mi><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中,<i>x</i><sub>s</sub>为<i>X</i><sub>s</sub>中的样本,<i>E</i><sub>(</sub><sub><i>x</i></sub><sub>s,</sub><sub><i>Y</i></sub><sub>s)～(</sub><sub><i>X</i></sub><sub>s,</sub><sub><i>Y</i></sub><sub>s)</sub>表示求联合概率分布的期望。</p>
                </div>
                <div class="p1">
                    <p id="73">第2阶段为共享特征提取的训练过程,类比GAN,随机噪声<i>z</i>变成了目标领域中的数据,生成器<i>G</i>的职能则由目标特征提取器<i>M</i><sub>t</sub>代替,通过不断更新的判别器<i>D</i>去逼近固定的源特征空间,从而提取到域不变表征:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>min</mi><msub><mrow></mrow><mi>D</mi></msub><mspace width="0.25em" /><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>d</mtext><mtext>v</mtext><msub><mrow></mrow><mi>D</mi></msub></mrow></msub><mo stretchy="false">(</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>,</mo><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>-</mo></mtd></mtr><mtr><mtd><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>min</mi><msub><mrow></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></msub><mi>L</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>d</mtext><mtext>v</mtext><msub><mrow></mrow><mi>Μ</mi></msub></mrow></msub><mo stretchy="false">(</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">第3阶段为目标样本的分类,即利用已训练的目标特征提取器<i>M</i><sub>t</sub>提升标签预测器<i>C</i>的迁移效果。</p>
                </div>
                <div class="p1">
                    <p id="76">分阶段进行领域适应,一方面使对抗的思想更直观地显现出来,便于利用已有的理论基础进行分析研究。另一方面,相比前者在单次循环中利用全部样本进行训练的做法,ADDA能够实现对已标注样本的充分利用,保证最佳源分类器的形成。从长期的机器学习经验中可以得出,模型在训练集上的学习度越深,将越有助于提升模型的泛化性能,这一点在迁移学习的任务中也同样适用。</p>
                </div>
                <div class="p1">
                    <p id="77">但是,这样的结构设计在领域适应任务上也存有不足。DANN因梯度反转层的引入将标签预测器和共享特征提取器融合进了单个训练进程中,通过一次迭代全部训练样本,使其模型具备了不断学习到目标领域中特有特征的灵活性,在处理跨域间的分类问题上更有针对性。而ADDA通过分阶段的学习域不变表征来迁移源标签预测器,使其局限于源特征空间的学习,由此失去了灵活性。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>3 带有双判别器的对抗性领域适应算法</b></h3>
                <div class="p1">
                    <p id="79">类比GAN中对于生成器<i>G</i>的目标损失函数的改进:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>min</mi><msub><mrow></mrow><mi>G</mi></msub><mspace width="0.25em" /><mi>L</mi><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mo>-</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">尽管如同式(7)一样能够缓解DANN中存在的导致训练不稳定的梯度消失问题,文献<citation id="215" type="reference">[<a class="sup">14</a>]</citation>仍可以从理论层面证明其存在的不足,即产生了新的模式崩溃问题(Model Collapse)。在领域适应任务中,可以类推为提取目标领域特征多样性的减少。具体来说,就是目标特征提取器更倾向提取出判别器<i>D</i>容易辨别的重复特征,而忽略了有助于分类但不易辨别的稀有特征。</p>
                </div>
                <div class="p1">
                    <p id="82">文献<citation id="216" type="reference">[<a class="sup">8</a>]</citation>从优化GAN的目标函数等价于最小化生成分布和真实分布之间的JSD(Jensen-Shannon)散度入手,通过平衡KL和反KL散度的权重大小来有效避免该问题<citation id="217" type="reference"><link href="181" rel="bibliography" /><link href="199" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">15</a>]</sup></citation>。D2GAN对应的公式如下:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munder><mspace width="0.25em" /><mi>L</mi><mo stretchy="false">(</mo><mi>G</mi><mo>,</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>α</mi><mo>×</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo></mtd></mtr><mtr><mtd><mo>-</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><mo>×</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中,<i>α</i>和<i>β</i>表示权重系数,判别器<i>D</i><sub>1</sub>与判别器<i>D</i><sub>2</sub>的职能相反,分别通过判断数据来自某一方获得高分。可以证明,给定最优的判别器<i>D</i><sup>*</sup><sub>1</sub>和<i>D</i><sup>*</sup><sub>2</sub>,目标函数可以等价为:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>min</mi><msub><mrow></mrow><mi>G</mi></msub><mspace width="0.25em" /><mi>L</mi><mo stretchy="false">(</mo><mi>G</mi><mo>,</mo><mi>D</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mo>,</mo><mi>D</mi><msubsup><mrow></mrow><mn>2</mn><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><mo stretchy="false">(</mo><mi>log</mi><mspace width="0.25em" /><mi>α</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><mo stretchy="false">(</mo><mi>log</mi><mspace width="0.25em" /><mi>β</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mi>D</mi><msub><mrow></mrow><mrow><mtext>Κ</mtext><mtext>L</mtext></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false">∥</mo><mi>p</mi><msub><mrow></mrow><mi>G</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><mi>D</mi><msub><mrow></mrow><mrow><mtext>Κ</mtext><mtext>L</mtext></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><msub><mrow></mrow><mi>G</mi></msub><mo stretchy="false">∥</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">当且仅当<i>p</i><sup>*</sup><sub><i>G</i></sub>=<i>p</i><sub><i>data</i></sub>时,式(10)有最优解,即KL散度和反KL散度的值为0。</p>
                </div>
                <div class="p1">
                    <p id="87">本文基于对GAN和领域适应贴合性的分析,尝试将改进应用到对抗性的领域适应算法ADDA上,式(6)和式(7)变为:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munder><mspace width="0.25em" /><mi>L</mi><mo stretchy="false">(</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>,</mo><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>,</mo><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>,</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mi>α</mi><mo>×</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></msub><mo stretchy="false">[</mo></mtd></mtr><mtr><mtd><mo>-</mo><mi>D</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow></msub><mo stretchy="false">[</mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>β</mi><mo>×</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo>∼</mo><mi>X</mi><msub><mrow></mrow><mtext>t</mtext></msub></mrow></msub><mo stretchy="false">[</mo><mi>log</mi><mspace width="0.25em" /><mi>D</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mtext>t</mtext></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">同时,考虑到提取特征多样性的增加可能会影响已有源样本分类器的迁移效果,以及ADDA结构上的不足,本文创新性地使用了伪标签的技巧来训练针对性的目标样本分类器。为了尽量使样本带有绝对正确的预测标签和减少不确定性,过滤器将依据预测的最高概率值进行伪标签的选择。算法总的框架结构如图1所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909019_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法结构和信息流图(虚线表示参数值固定)" src="Detail/GetImg?filename=images/JSJK201909019_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法结构和信息流图(虚线表示参数值固定)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909019_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Network architecture and 
 information flow of our algorithm 
 (dashed lines indicate fixed network parameters)</p>

                </div>
                <div class="p1">
                    <p id="91">数据的逻辑处理基于ADDA的方式不变,保证了初期最佳源样本分类器的形成。第3阶段将原先固定不变的样本分类器改为可训练,并增加了额外的过滤器对伪标签进行高效利用。此外,针对文献<citation id="218" type="reference">[<a class="sup">16</a>]</citation>中类间关系的研究,本文算法添加了额外的技巧去限制最有可能和其他类别之间概率的差值,以此消除不同类别样本之间的相似性影响。</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag"><b>4 实验</b></h3>
                <div class="p1">
                    <p id="93">为了验证本文算法的可行性和有效性,使用公开的常用数据集和开源代码进行比对测试。按照无监督领域适应的实验要求,目标领域将全部由未标注样本组成。此外,文中若无特别说明,将默认使用整个训练集作为域。参照文献<citation id="219" type="reference">[<a class="sup">13</a>]</citation>中方法对数据集进行处理,以颜色通道的数量为划分依据,将任务分为2组,分别为单通道(GRAY)适应和3通道(RGB)适应,共计6个方向:MNIST→USPS,USPS→MNIST,SVHN→MNIST,Synthetic Signs→GTSRB,Synthetic Numbers→SVHN和CIFAR-10→STL-10。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">(1)MNIST:</h4>
                <div class="p1">
                    <p id="95">作为领域适应测试中最常用的手写数字数据集,本文遵循ADDA算法中USPS和MNIST之间的适应规则,从大小为28×28像素的训练集中随机抽取2 000幅图像。完整的训练集则用于SVHN和MNIST之间的适应任务。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">(2)USPS:</h4>
                <div class="p1">
                    <p id="97">与MNIST类似,从大小为16×16像素的训练集中随机抽取1 800幅图像。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">(3)SVHN:</h4>
                <div class="p1">
                    <p id="99">从真实场景中收集到的大小为32×32像素的数字数据集,虽经过简单的裁剪处理,但部分图像中仍含有多个数字,分类难度较大。训练集中共有73 257幅图像。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">(4)Synthetic Numbers:</h4>
                <div class="p1">
                    <p id="101">为解决使用合成数据训练的模型无法完全推广到真实场景的问题,仿照SVHN创建的人工数据集。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">(5)Synthetic Signs:</h4>
                <div class="p1">
                    <p id="103">虽然目的上与Synthetic Numbers有所重复,但是含有43个不同的交通标志类别,提高了领域适应的难度系数。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">(6)GTSRB:</h4>
                <div class="p1">
                    <p id="105">用于分类问题的大规模真实数据集,具有超过50 000幅大小在15×15～250×250像素的交通标志图像。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">(7)CIFAR-10:</h4>
                <div class="p1">
                    <p id="107">为进行领域适应任务,在文献<citation id="220" type="reference">[<a class="sup">17</a>]</citation>的数据处理基础上进行略微改动,实验中将去除‘frog’类别的样本。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">(8)STL-10<citation id="222" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>:</h4>
                <div class="p1">
                    <p id="109">与CIFAR-10数据集类似,将‘monkey’类别的样本去除,并依照CIFAR-10标签进行重新排序。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>4.1 参数设置</b></h4>
                <div class="p1">
                    <p id="111">本文算法的实现基于已有算法ADDA,为了有效对比,在参数设置上尽量保持不变。考虑到对数运算的影响,2个判别器的输出形式由[0,1]的概率值变为正实数,相应地对判别器的输出选用softplus激活函数进行处理。数据集方面,对于单通道适应,图像大小一律调整为28×28像素并转换为灰度图像。在3通道适应任务中,从Synthetic Signs到GTSRB,图像尺寸统一调整为40×40像素。STL-10依照CIFAR-10将图像大小调整为32×32像素。</p>
                </div>
                <div class="p1">
                    <p id="112">本文的关注点在于能否通过发现GAN和领域适应的相似性,类比利用对原始GAN的改进方法来进一步提升对抗性的领域适应性能,因此实验并没有以找到最高准确度为最终目标。参照D2GAN的实验数据,本文算法固定<i>α</i>的值为0.2,<i>β</i>的取值为{0.01,0.1,0.2}进行评估。过滤器部分使用固定的筛选阈值和差值,分别设置为0.97和0.95。以上并不包括伪标签的使用,为了对比效果,本文选用泛化性较好,参数值都为0.2的<i>α</i>,<i>β</i>另行实验。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>4.2 实验结果</b></h4>
                <div class="p1">
                    <p id="114">从表1和表2可以看出,不使用伪标签训练的本文算法在分类性能上略优于之前的对抗性领域适应算法,证明了尝试类比改进迁移效果的可行性,而且可以观察到随着<i>β</i>的变化,分类准确度相应地有所提高,和D2GAN中对于参数的测试实验结果基本一致,说明通过调节权重参数<i>α</i>,<i>β</i>的值来平衡KL和反KL散度的影响能够有效解决基于GAN的模式崩溃问题,提升领域适应性能。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表1 单通道适应任务下的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Experimental results of single channel adaptation tasks</b></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />Method</td><td>Source<br />Target</td><td>MNIST<br />USPS</td><td>USPS<br />MNIST</td><td>SVHN<br />MNIST</td></tr><tr><td colspan="2"><br />Source only</td><td>0.754</td><td>0.583</td><td>0.610</td></tr><tr><td colspan="2"><br />DANN</td><td>0.771</td><td>0.730</td><td>0.739</td></tr><tr><td colspan="2"><br />DDC<sup>[19]</sup></td><td>0.791</td><td>0.665</td><td>0.681</td></tr><tr><td colspan="2"><br />CoGAN</td><td>0.912</td><td>0.891</td><td>-</td></tr><tr><td colspan="2"><br />ADDA</td><td>0.874</td><td>0.914</td><td>0.785</td></tr><tr><td colspan="2"><br />本文算法(<i>β</i>=0.01)</td><td>0.871</td><td>0.920</td><td>0.793</td></tr><tr><td colspan="2"><br />本文算法(<i>β</i>=0.1)</td><td>0.843</td><td>0.888</td><td>0.784</td></tr><tr><td colspan="2"><br />本文算法(<i>β</i>=0.2)</td><td>0.875</td><td>0.925</td><td>0.799</td></tr><tr><td colspan="2"><br />本文算法(伪标签)</td><td>0.891</td><td>0.930</td><td>0.817</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表2 三通道适应任务下的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Experimental results of three channels adaptation tasks</b></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />Method</td><td>Source<br />Target</td><td>SYN Signs<br />GTSRB</td><td>SYN Numbers<br />SVHN</td><td>CIFAR-10<br />STL-10</td></tr><tr><td colspan="2"><br />Source only</td><td>0.796</td><td>0.793</td><td>0.491</td></tr><tr><td colspan="2"><br />DANN</td><td>0.887</td><td>0.831</td><td>0.525</td></tr><tr><td colspan="2"><br />ADDA</td><td>0.889</td><td>0.852</td><td>0.525</td></tr><tr><td colspan="2"><br />本文算法(<i>β</i>=0.01)</td><td>0.895</td><td>0.843</td><td>0.521</td></tr><tr><td colspan="2"><br />本文算法(<i>β</i>=0.1)</td><td>0.906</td><td>0.844</td><td>0.519</td></tr><tr><td colspan="2"><br />本文算法(<i>β</i>=0.2)</td><td>0.902</td><td>0.843</td><td>0.534</td></tr><tr><td colspan="2"><br />本文算法(伪标签)</td><td>0.914</td><td>0.833</td><td>0.549</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">伪标签的使用则进一步提升了迁移效果,但由于其并不完全真实可靠,在Synthetic Numbers→SVHN的任务中出现了准确率下降的现象,对此可以通过控制过滤器的选择参数和减少带伪标签样本的训练次数进行改善,为保持一般性,本文默认训练次数一律为10次。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>4.3 分析</b></h4>
                <div class="p1">
                    <p id="119">将提取特征通过<i>t</i>分布随机领域嵌入t-SNE(t-Distributed Stochastic Neighbor Embedding)<citation id="221" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>工具进行可视化处理,可以更直观地观察到带有双判别器的对抗性领域适应性能。以USPS→MNIST为例,从每个域随机采样少量样本。如图2所示为未进行领域适应和已进行领域适应的效果对比图,可以发现,源领域和目标领域经领域适应后消除了较大的差异,表明本文算法具有出色的提取域不变表征的能力。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909019_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文算法在USPS→MNIST任务上的适应效果" src="Detail/GetImg?filename=images/JSJK201909019_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文算法在USPS→MNIST任务上的适应效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909019_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Our algorithm adapts USPS to MNIST</p>

                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="122">本文深入探究了生成对抗网络和对抗性的领域适应算法的内在相似性,发现两者在数据的来源域及目标不可分的逻辑处理上具有天然的贴合性。基于原始GAN的不足与改进方法,文中对运用了对抗思想的领域适应算法进行改进,提出了带有双判别器的结构的算法。通过选择不同的参数值,平衡KL和反KL散度的影响来有效解决GAN目标函数中产生的模式崩溃问题,增加了目标领域中提取域不变表征的多样性。考虑到ADDA算法结构上的不足,以及领域适应任务的特殊性,本文创新性地使用伪标签训练针对性的目标样本分类器进行结构上的补充完善,并结合过滤器的作用,有效提升了目标样本的分类准确率。实验部分选用公开的标准领域适应数据集,通过对比其他算法和可视化提取到的共享特征,可以证明本文算法可行且有效,同时也展现了尝试类比利用GAN的改进方法来提升迁移性能的广阔前景。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="153">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Covariate shift and local learning by distribution matching">

                                <b>[1]</b> Gretton A,Smola A J,Huang J Y,et al.Covariate shift and local learning by distribution matching[M].Cambridge:MIT Press,2009:131-160.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey on Transfer Learning">

                                <b>[2]</b> Pan S J,Yang Q.A survey on transfer learning[J].IEEE Transactions on Knowledge &amp; Data Engineering,2010,22(10):1345-1359.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transfer learning:Problems and methods">

                                <b>[3]</b> Long Ming-sheng.Transfer learning:Problems and methods[D].Beijing:Tsinghua University,2014.(in Chinese)
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[4]</b> Goodfellow I J,Pouget A J,Mirza M,et al.Generative adversarial nets [C]//Proc of International Conference on Neural Information Processing Systems,2014:2672-2680.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Domain Adaptation by Backpropagation">

                                <b>[5]</b> Ganin Y,Lempitsky V.Unsupervised domain adaptation by backpropagation [C]//Proc of the 32nd International Conference on Machine Learning,2015:1180-1189.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 Zhao Zeng-shun,Gao Han-xu,Sun Qian,et al.Latest development of the theory framework,derivative model and application of generative adversarial nets[J].Journal of Chinese Computer Systems,2018,39(12):44-48.(in Chinese)
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised rep-resentation learning with deep convolutional generative ad-versarial networks">

                                <b>[7]</b> Radford A,Metz L,Chintala S.Unsupervised representation learning with deep convolutional generative adversarial networks[J].Computer Science,arXiv:1511.06434,2015.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dual discriminator generative adversarial nets">

                                <b>[8]</b> Nguyen T D,Le T,Vu H,et al.Dual discriminator generative adversarial nets[C]//Proc of the 31st International Conference on Neural Information Processing Systems,2017:2671-2681.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wasserstein gan">

                                <b>[9]</b> Arjovsky M,Chintala S,Bottou L.Wasserstein GAN[EB /OL].[2017-01-26].https://arxiv.org /abs/1701.07875.pdf.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805003&amp;v=MTc3ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzSUtDTGZZYkc0SDluTXFvOUZaNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Wang Gong-ming,Qiao Jun-fei,Wang Lei.A generative adversarial network based on energy function[J].Acta Automatica Sinica,2018,44(5):793-803.(in Chinese)
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Domain-adversarial training of neural networks">

                                <b>[11]</b> Ganin Y,Ustinova E,Ajakan H,et al.Domain-adversarial training of neural networks[J].Journal of Machine Learning Research,2017,17(1):2096-2030.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled generative adversarial networks">

                                <b>[12]</b> Liu M Y,Tuzel O.Coupled generative adversarial networks [C]//Proc of the 30th Conference on Neural Information Processing Systems(NIPS 2016),2016:1-9.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial discriminative domain adaptation">

                                <b>[13]</b> Tzeng E,Hoffman J,Saenko K,et al.Adversarial discriminative domain adaptation[C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition,2017:2962-2971.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards principled methods for training generative adversarial networks">

                                <b>[14]</b> Arjovsky M,Bottou L.Towards principled methods for training generative adversarial networks[J].arXiv:1701.04862,2017.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201501003&amp;v=MTY1NzZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzSU55ZlRiTEc0SDlUTXJvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Zhuang Fu-zhen,Luo Ping,He Qing,et al.Survey on transfer learning research[J].Journal of Software,2015,26(1):26-39.(in Chinese)
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous deep transfer across domains and tasks">

                                <b>[16]</b> Tzeng E,Hoffman J,Darrell T,et al.Simultaneous deep transfer across domains and tasks[C]//Proc of IEEE International Conference on Computer Vision,2017:4068-4076.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation">

                                <b>[17]</b> Ghifary M,Kleijn W B,Zhang M,et al.Deep reconstruction-classification networks for unsupervised domain adaptation [C]//Proc of European Conference on Computer Vision,2016:597-613.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An analysis of single-layer networks in unsupervised feature learning">

                                <b>[18]</b> Coates A,Lee H,Ng A Y.An analysis of single-layer networks in unsupervised feature learning [J].Machine Learning Research,2011,15:215-223.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Domain Confusion:Maximizing for Domain Invariance">

                                <b>[19]</b> Tzeng E,Hoffman J,Zhang N,et al.Deep domain confusion:Maximizing for domain invariance [J].Computer Science,arXiv:1412.3474v1,2014.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decaf: A deep convolutional activation feature for generic visual recognition">

                                <b>[20]</b> Donahue J,Jia Y,Vinyals O,et al.DeCAF:A deep convolutional activation feature for generic visual recognition [C]//Proc of the 31st International Conference on Machine Learning,2014:988-996.附中文参考文献:
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1015039180.nh&amp;v=MTM5MDJDVVJMT2VaZVJtRnk3a1ViM0lWRjI2RzdPN0Y5REVyNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 龙明盛.迁移学习问题与方法研究[D].北京:清华大学,2014.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201812009&amp;v=MDU1ODhIOW5Oclk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYjNJUFRYY2RyRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 赵增顺,高寒旭,孙骞,等.生成对抗网络理论框架、衍生模型与应用最新进展[J].小型微型计算机系统,2018,39(12):44-48.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 王功明,乔俊飞,王磊.一种能量函数意义下的生成式对抗网络[J].自动化学报,2018,44(5):793-803.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 庄福振,罗平,何清,等.迁移学习研究进展[J].软件学报,2015,26(1):26-39.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909019" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909019&amp;v=MTk1NDdlWmVSbUZ5N2tVYjNKTHo3QlpiRzRIOWpNcG85RWJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
