<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132358140811250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201909020%26RESULT%3d1%26SIGN%3dkYJJ27ougYJuMTQPUnxgU9xCm3o%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201909020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909020&amp;v=MjU0OTQ3a1ViM05MejdCWmJHNEg5ak1wbzlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;2 相关理论&lt;/b&gt; "><b>2 相关理论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="&lt;b&gt;3 基于YOLO算法的视盘检测方法&lt;/b&gt; "><b>3 基于YOLO算法的视盘检测方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;3.1 网络模型&lt;/b&gt;"><b>3.1 网络模型</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;3.2 视盘检测与定位&lt;/b&gt;"><b>3.2 视盘检测与定位</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;3.3 损失函数&lt;/b&gt;"><b>3.3 损失函数</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;3.4 多尺度检测&lt;/b&gt;"><b>3.4 多尺度检测</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;3.5 检测视盘中心点坐标&lt;/b&gt;"><b>3.5 检测视盘中心点坐标</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.6 数据增强&lt;/b&gt;"><b>3.6 数据增强</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="&lt;b&gt;4 实验分析&lt;/b&gt; "><b>4 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#125" data-title="&lt;b&gt;4.1 数据集&lt;/b&gt;"><b>4.1 数据集</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;4.2 评价指标&lt;/b&gt;"><b>4.2 评价指标</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;4.3 定位结果分析&lt;/b&gt;"><b>4.3 定位结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="图1 网络结构图">图1 网络结构图</a></li>
                                                <li><a href="#94" data-title="图2 视盘检测过程">图2 视盘检测过程</a></li>
                                                <li><a href="#130" data-title="图3 标准视盘中心与模型定位视盘中心距离">图3 标准视盘中心与模型定位视盘中心距离</a></li>
                                                <li><a href="#133" data-title="图4 实验定位结果">图4 实验定位结果</a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表1 本文方法对不同眼底图像数据集视盘定位结果&lt;/b&gt;"><b>表1 本文方法对不同眼底图像数据集视盘定位结果</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;表2 不同方法在DRIVE数据集上视盘定位结果1&lt;/b&gt;"><b>表2 不同方法在DRIVE数据集上视盘定位结果1</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表3 不同方法在MESSIDOR数据集上视盘定位结果&lt;/b&gt;"><b>表3 不同方法在MESSIDOR数据集上视盘定位结果</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表4 不同方法在DRIVE数据集上视盘定位结果2&lt;/b&gt;"><b>表4 不同方法在DRIVE数据集上视盘定位结果2</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="212">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     Zou Bei-ji,Zhang Si-jian,Zhu Cheng-zhang.Automatic localization and segmentation of optic disk in color fundus image[J].Optics and Precision Engineering,2015,23(4):1187-1195.(in Chinese)</a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_2" title=" Sinthanayothin C,Boyce J F,Cook H L,et al.Automated localisation of the optic disc,fovea,and retinal blood vessels from digital colour fundus images[J].British Journal Ophthalmology,1999,83(8):902-910." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated localisation of the optic disc, fovea, and retinal blood vessels from digital colour fundus images">
                                        <b>[2]</b>
                                         Sinthanayothin C,Boyce J F,Cook H L,et al.Automated localisation of the optic disc,fovea,and retinal blood vessels from digital colour fundus images[J].British Journal Ophthalmology,1999,83(8):902-910.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_3" title=" Li Hui-qi,Chutatape O.Automated feature extraction in color retinal images by a model based approach[J].IEEE Transactions on Biomedical Engineering,2004,51(2):246-254." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated Feature Extraction in Color Retinal Images by a Model Based Approach">
                                        <b>[3]</b>
                                         Li Hui-qi,Chutatape O.Automated feature extraction in color retinal images by a model based approach[J].IEEE Transactions on Biomedical Engineering,2004,51(2):246-254.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_4" title=" Lu S,Lim J H.Automatic optic disc detection from retinal images by a line operator[J].IEEE Transactions on Bio-medical Engineering,2011,58(1):88-94." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic Optic Disc Detection From Retinal Images by a Line Operator">
                                        <b>[4]</b>
                                         Lu S,Lim J H.Automatic optic disc detection from retinal images by a line operator[J].IEEE Transactions on Bio-medical Engineering,2011,58(1):88-94.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_5" title=" Park M,Jin J S,Luo S.Locating the optic disc in retinal images[C]//Proc of International Conference on Computer Graphics,Imaging and Visualisation,2006:141-145." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locating the optic disc in retinal images">
                                        <b>[5]</b>
                                         Park M,Jin J S,Luo S.Locating the optic disc in retinal images[C]//Proc of International Conference on Computer Graphics,Imaging and Visualisation,2006:141-145.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_6" title=" Haar F T.Automatic localization of the optic disc in digital colour images of the human retina[D].Utrecht:Utrecht University,2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic localization of the optic disc in digital colour images of the human retina">
                                        <b>[6]</b>
                                         Haar F T.Automatic localization of the optic disc in digital colour images of the human retina[D].Utrecht:Utrecht University,2005.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_7" title=" Daniel W,Jacob S,Cleysonm K,et al.Segmentation of optic disk in color eye fundus images using an adaptive morphological approach [J].Computer in Biology and Medicine,2010,40(2):124-137." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011701757617&amp;v=MTI5NzVtVUxmSUpsOFdhQmM9TmlmT2ZiSzdIdEROcUk5RVkrNElDbjArb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5ag==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Daniel W,Jacob S,Cleysonm K,et al.Segmentation of optic disk in color eye fundus images using an adaptive morphological approach [J].Computer in Biology and Medicine,2010,40(2):124-137.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_8" title=" Foracchia M,Grisan E,Ruggeri A.Detection of optic disc in retinal images by means of a geometrical model of vessel structure[J].IEEE Transactions on Medical Imaging,2004,23(10):1189-1195." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection of optic disc in retinal images by means of a geometrical model of vessel structure">
                                        <b>[8]</b>
                                         Foracchia M,Grisan E,Ruggeri A.Detection of optic disc in retinal images by means of a geometrical model of vessel structure[J].IEEE Transactions on Medical Imaging,2004,23(10):1189-1195.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_9" title=" Hoover A,Goldbaum M.Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels[J].IEEE Transactions on Medical Imaging,2003,22(8):951-958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels">
                                        <b>[9]</b>
                                         Hoover A,Goldbaum M.Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels[J].IEEE Transactions on Medical Imaging,2003,22(8):951-958.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_10" title=" Youssif A R,Ghalwash A Z,Ghoneim A R.Optic disc detection from normalized digital fundus images by means of a vessels’ direction matched filter[J].IEEE Transactions on Medical Imaging,2008,27(1):11-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optic Disc Detection From Normalized Digital Fundus Images by Means of a Vessels&amp;#39; Direction Matched Filter">
                                        <b>[10]</b>
                                         Youssif A R,Ghalwash A Z,Ghoneim A R.Optic disc detection from normalized digital fundus images by means of a vessels’ direction matched filter[J].IEEE Transactions on Medical Imaging,2008,27(1):11-18.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_11" title=" Tobin K W,Chaum E,Govindasamy V P,et al.Detection of an atomic structures in human retinal imagery[J].IEEE Transactions on Medical Imaging,2007,26(12):1729-1739." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection of Anatomic Structures in Human Retinal Imagery">
                                        <b>[11]</b>
                                         Tobin K W,Chaum E,Govindasamy V P,et al.Detection of an atomic structures in human retinal imagery[J].IEEE Transactions on Medical Imaging,2007,26(12):1729-1739.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_12" title=" Narasimhan K,Vijayarekha K,Joginarayana K A,et al.Glaucoma detection from fundus image using OpenCV[J].Research Journal of Applied Sciences Engineering &amp;amp; Technology,2012,4(24):5459-5463." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glaucoma detection from fundus image using OpenCV">
                                        <b>[12]</b>
                                         Narasimhan K,Vijayarekha K,Joginarayana K A,et al.Glaucoma detection from fundus image using OpenCV[J].Research Journal of Applied Sciences Engineering &amp;amp; Technology,2012,4(24):5459-5463.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_13" title=" Mahfouz A E,Fahmy A S.Fast localization of the optic disc using projection of image features[J].IEEE Transactions on Image Processing,2010,19(12):3285-3289." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast localization of the optic disc using projection of image features">
                                        <b>[13]</b>
                                         Mahfouz A E,Fahmy A S.Fast localization of the optic disc using projection of image features[J].IEEE Transactions on Image Processing,2010,19(12):3285-3289.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_14" title=" Yu H,Barriga E S,Agurto C,et al.Fast localization and segmentation of optic disk in retinal images using directional matched filtering and level sets[J].IEEE Transactions on Information Technology in Biomedicine,2012,16(4):644-657." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Localization and Segmentation of Optic Disk in Retinal Images Using Directional Matched Filtering and Level Sets">
                                        <b>[14]</b>
                                         Yu H,Barriga E S,Agurto C,et al.Fast localization and segmentation of optic disk in retinal images using directional matched filtering and level sets[J].IEEE Transactions on Information Technology in Biomedicine,2012,16(4):644-657.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_15" title=" Ramakanth S A,Babu R V.Approximate nearest neighbor field based optic disk detection[J].Computerized Medical Imaging &amp;amp; Graphics,2014,38(1):49-56." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA0B888CCEA0A783C8FF5AF200C6E2A2F&amp;v=MDc4MzR3MkVab1BmWHN4ekdVYm5FbDRPUW5nckJKR2Y4ZVdOTGpwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3JtOXdxMD1OaWZPZmNLNGJObkVwLw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Ramakanth S A,Babu R V.Approximate nearest neighbor field based optic disk detection[J].Computerized Medical Imaging &amp;amp; Graphics,2014,38(1):49-56.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_16" title=" Cheng J,Liu J,Xu Y,et al.Superpixel classification based optic and optic cup segmentation screening [J].IEEE Transactions on Medical Imaging,2013,32(6):1019-1032." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Superpixel classification based optic disc and optic cup segmentation for glaucoma screening">
                                        <b>[16]</b>
                                         Cheng J,Liu J,Xu Y,et al.Superpixel classification based optic and optic cup segmentation screening [J].IEEE Transactions on Medical Imaging,2013,32(6):1019-1032.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_17" title=" Fukushima K.Neural network model for selective attention in visual pattern recognition and associative recall[J].Applied Optics,1987,26(23):4985." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural network model for selective attention in visual pattern recognition and associative recall">
                                        <b>[17]</b>
                                         Fukushima K.Neural network model for selective attention in visual pattern recognition and associative recall[J].Applied Optics,1987,26(23):4985.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_18" title=" Girshick R,Donahue J,Darrell T,et al.Region-based convolutional networks for accurate object detection and segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,38(1):142-158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region-based Convolutional Networks for Accurate Object Detection and Segmentation">
                                        <b>[18]</b>
                                         Girshick R,Donahue J,Darrell T,et al.Region-based convolutional networks for accurate object detection and segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,38(1):142-158.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_19" title=" Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">
                                        <b>[19]</b>
                                         Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:779-788.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_20" title=" Szegedy C,Vanhoucke V,Ioffe S,et al.Rethinking the inception architecture for computer vision[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:2818-2826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">
                                        <b>[20]</b>
                                         Szegedy C,Vanhoucke V,Ioffe S,et al.Rethinking the inception architecture for computer vision[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:2818-2826.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_21" title=" Redmon J,Farhadi A.YOLO9000:Better,faster,stronger[J].arXiv preprint arXiv:1612.08242,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better faster stronger">
                                        <b>[21]</b>
                                         Redmon J,Farhadi A.YOLO9000:Better,faster,stronger[J].arXiv preprint arXiv:1612.08242,2016.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_22" title=" Redmon J,Farhadi A.YOLOv3:An incremental improvement[J].arXiv preprint arXiv:1804.02767,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An incremental improvement">
                                        <b>[22]</b>
                                         Redmon J,Farhadi A.YOLOv3:An incremental improvement[J].arXiv preprint arXiv:1804.02767,2018.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_23" title=" Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of the 32nd International Conference on Machine Learning,2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[23]</b>
                                         Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of the 32nd International Conference on Machine Learning,2015:448-456.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_24" title=" Sinha N,Babu R V.Optic disk localization using L1 minimization[C]//Proc of IEEE International Conference on Image Processing,2013:2829-2832." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optic disk localization using L1 minimization">
                                        <b>[24]</b>
                                         Sinha N,Babu R V.Optic disk localization using L1 minimization[C]//Proc of IEEE International Conference on Image Processing,2013:2829-2832.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_25" title=" Lu S.Accurate and efficient optic disc detection and segmentation by a circular transformation[J].IEEE Transactions on Medical Imaging,2011,30(12):2126-2133." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate and efficient optic disc detection and segmentation by a circular transformation">
                                        <b>[25]</b>
                                         Lu S.Accurate and efficient optic disc detection and segmentation by a circular transformation[J].IEEE Transactions on Medical Imaging,2011,30(12):2126-2133.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                     Zhang Gui-ying,Zhang Xian-jie.Deep learning based on optic disk automatic detection[J].Journal of Guizhou Normal College,2017,33(3):27-32.(in Chinese)</a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_27" title=" Harangi B,Hajdu A.Detection of the optic disc in fundus images by combining probability models[J].Computers in Biology &amp;amp; Medicine,2015,65(C):10-24." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120600576402&amp;v=MTQ3NTU9TmlmT2ZiSzlIOVBNcVk5Rllld0pDSHc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGZJSmw4V2FCYw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         Harangi B,Hajdu A.Detection of the optic disc in fundus images by combining probability models[J].Computers in Biology &amp;amp; Medicine,2015,65(C):10-24.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_28" >
                                        <b>[28]</b>
                                     Zhao Xiao-fang,Lin Tu-sheng,Li Bi.Fast automatic localization of optic in retinal images[J].Journal of South China University of Technology(Natural Science Edition),2011,39(2):71-75.(in Chinese)</a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_29" title=" Mahfouz A E.Fahmy A S.Ultrafast localization of the optic disc using dimensionality reduction of the search space[C]//Proc of the 12th International Conference on Medical Image Computing and Computer Assisted Intervention,2009:985-992.附中文参考文献:" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ultrafast localization of the optic disc using dimensionality reduction of the search space">
                                        <b>[29]</b>
                                         Mahfouz A E.Fahmy A S.Ultrafast localization of the optic disc using dimensionality reduction of the search space[C]//Proc of the 12th International Conference on Medical Image Computing and Computer Assisted Intervention,2009:985-992.附中文参考文献:
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_1" title=" 邹北骥,张思剑,朱承璋.彩色眼底图像视盘自动定位与分割[J].光学精密工程,2015,23(4):1187-1195." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201504035&amp;v=MTk2NzgzTklqWEJZN0c0SDlUTXE0OUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         邹北骥,张思剑,朱承璋.彩色眼底图像视盘自动定位与分割[J].光学精密工程,2015,23(4):1187-1195.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_26" title=" 张贵英,张先杰.基于深度学习的视盘自动检测[J].贵州师范学院学报,2017,39(3):27-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GZJY201703007&amp;v=MDkyNDFqZkJkN0c0SDliTXJJOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeTdrVWIzTkk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         张贵英,张先杰.基于深度学习的视盘自动检测[J].贵州师范学院学报,2017,39(3):27-32.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_28" title=" 赵晓芳,林土胜,李碧.视网膜图像中视盘的快速自动定位方法[J].华南理工大学学报(自然科学版),2011,39(2):71-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNLG201102012&amp;v=MjkxNDBGckNVUkxPZVplUm1GeTdrVWIzTkxTUEhhYkc0SDlETXJZOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         赵晓芳,林土胜,李碧.视网膜图像中视盘的快速自动定位方法[J].华南理工大学学报(自然科学版),2011,39(2):71-75.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(09),1662-1670 DOI:10.3969/j.issn.1007-130X.2019.09.019            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于YOLO算法的眼底图像视盘定位方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E8%8A%B8&amp;code=09140808&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋芸</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E5%A9%B7%E5%A9%B7&amp;code=39136092&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭婷婷</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E5%AE%81&amp;code=39136091&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BE%AF%E9%87%91%E6%B3%89&amp;code=41473097&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">侯金泉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8C%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0012645&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西北师范大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>视盘的各个参数是衡量眼底健康状况和病灶的重要指标,视盘的检测和定位对于观察视盘的形态尤为重要。在以往的视盘定位研究中,主要根据视盘的形状、亮度、眼底血管的走向等特征使用图像处理的方法对眼底图像中视盘进行定位。由于人为因素影响较大,特征提取时间较长,且视盘定位效率低,因此提出一种基于YOLO算法的眼底图像视盘定位方法。利用YOLO算法将眼底图像划分为<i>N</i>×<i>N</i>的格子,每个格子负责检测视盘中心点是否落入该格子中,通过多尺度的方式和残差层融合低级特征对视盘进行定位,得到不同大小的边界框,最后通过非极大抑制的方式筛选出得分最高的边界框。通过在3个公开的眼底图像数据集(DRIVE、DRISHTI-GS1和MESSIDOR)上,对所提出的视盘定位方法进行测试,定位准确率均为100%,实验同时定位出视盘的中心点坐标,与标准中心点的平均欧氏距离分别为22.36 px、2.52 px、21.42 px,验证了该方法的准确性和通用性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E7%9B%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视盘;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLO%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLO算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    蒋芸（1970-），女，浙江绍兴人，博士，教授，研究方向为数据挖掘、粗糙集理论及应用。E-mail:jiangyun@nwnu.edu.cn,通信地址：730070甘肃省兰州市西北师范大学计算机科学与工程学院&lt;image id="207" type="formula" href="images/JSJK201909020_20700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    彭婷婷（1995-），女，甘肃正宁人，硕士，研究方向为数据挖掘。E-mail:994882134@qq.com,通信地址：730070甘肃省兰州市西北师范大学计算机科学与工程学院&lt;image id="209" type="formula" href="images/JSJK201909020_20900.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    谭宁,通信地址：730070甘肃省兰州市西北师范大学计算机科学与工程学院;
                                </span>
                                <span>
                                    侯金泉,通信地址：730070甘肃省兰州市西北师范大学计算机科学与工程学院;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61163036);</span>
                                <span>甘肃省科技计划资助自然科学基金(1606RJZA047);</span>
                                <span>2012年度甘肃省高校基本科研业务费专项资金项目;</span>
                                <span>甘肃省高校研究生导师项目(1201-16);</span>
                                <span>西北师范大学第三期知识与创新工程科研骨干项目(nwnu-kjcxgc-03-67);</span>
                    </p>
            </div>
                    <h1><b>An optic disc positioning method in fundus images based on YOLO</b></h1>
                    <h2>
                    <span>JIANG Yun</span>
                    <span>PENG Ting-ting</span>
                    <span>TAN Ning</span>
                    <span>HOU Jin-quan</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering,Northwest Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The parameters of the optic disc are important indicators for measuring the health status and lesions of the fundus. The detection and localization of the optic disc is especially important for observing the shape of the optic disc. Research on optic disc positioning in the past mainly depends on the shape and brightness of the optic disc, and the direction of the fundus blood vessels, and image-processing methods are used to locate the optic disc in fundus images. Due to the influence of human factors, the feature extraction time is long and the optic disc positioning efficiency is low. We propose a method of locating optic disc of the fundus image based on the YOLO algorithm. The YOLO algorithm is used to divide the fundus image into <i>N</i>×<i>N</i> grids, and each grid is responsible for detecting whether the center point of the disc falls into the grid. The multi-scale method and the residual layer are fused with low-level features to locate the disc, and bounding boxes of different sizes are obtained. The bounding box with the highest score is finally selected through non-maximum suppression. We test the proposed localization method on three open databases of fundus images(DRIVE、DRISHTI-GS1 and MESSIDOR). The positioning accuracy is 100%, and the center point coordinates of the optic disc are located in the experiment. The average Euclidean distances to the center points are 22.36 px, 2.52 px, 21.42 px, respectively, which verifies the accuracy and versatility of the method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optic%20disc&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optic disc;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLO%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLO algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    JIANG Yun,born in 1970,PhD,professor,her research interests include data mining,rough set theory and application.Address:College of Computer Science and Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    PENG Ting-ting,born in 1995,MS,her research interest includes data mining.Address:College of Computer Science and Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    TAN Ning,Address:College of Computer Science and Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                                <span>
                                    HOU Jin-quan,Address:College of Computer Science and Engineering,Northwest Normal University,Lanzhou 730070,Gansu,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-08-03</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="69" name="69" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="70">视盘OD(Optic Disk),全称视神经盘,也叫视神经乳头。它是视网膜血管、神经纤维汇集穿出眼球的部位,是视神经的始端。在医学中,视盘的形状、大小、颜色、面积和生理杯深度等参数是衡量眼底健康状况和病灶的重要指标。在患某些眼病或脑病时,视盘可能发生水肿,因此观察视盘的形态尤为重要。一般情况下,正常人的视盘呈亮黄色椭圆形,明亮而具有光泽,与眼底橘红色形成鲜明的对比,视盘面积约占总眼底图像面积的11%,这是视盘的一个重要特性<citation id="276" type="reference"><link href="212" rel="bibliography" /><link href="270" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">1</a>]</sup></citation>;其次,眼底图像中的血管特性主要表现为视盘区域为主血管交汇区,视盘区域的血管的特性表现为血管粗、密度大;主血管整体走向为抛物线形,最高点汇集于视盘中心,成为视盘的第2个特性。针对以上2个视盘的特性,目前国内外视盘的定位通常有基于亮度、形状和面积特征的视盘定位、基于血管特征的视盘定位和基于多特征融合的视盘定位3类研究方法<citation id="277" type="reference"><link href="212" rel="bibliography" /><link href="270" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">1</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="71">(1)基于亮度、形状和面积的视盘定位方法。在彩色的眼底图像中可以明显观察到视盘与眼底呈不同颜色,灰度值差异较大。Sinthanayothin等<citation id="278" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过识别相邻像素强度变化最大的区域来定位视盘。Li等<citation id="279" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>首先找到1%的亮度最大的像素点,再通过聚类分割出视盘候选区域,然后采用PCA(Principal Component Analysis)分析找到真正的视盘。Lu等<citation id="280" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一种自动视盘检测技术。通过设计线条算子,让线段随亮度的变化而变化,最终捕获视盘独特的圆形亮度结构。Park等<citation id="281" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了一种自动勾画视网膜图像中视盘的方法。基于视盘亮度的属性,使用简单的图像处理算法,其中包括阈值、检测物体的圆度和通过Hough变换的圆检测视盘的位置。Haar<citation id="282" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>则依据视盘的形状特点,构建视盘区域结构模板,通过模板匹配方法找到视盘。Daniel等<citation id="283" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在经典的数学形态学方法上进行了改进,使用一种自适应的形态学算法分割视盘。</p>
                </div>
                <div class="p1">
                    <p id="72">(2)基于血管特征的视盘定位。Foracchia等<citation id="284" type="reference"><link href="226" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>基于对主要视网膜血管的初步检测,提出几何参数模型用于描述源于视盘的所有视网膜血管,通过血管分割的结果定位视盘,但模型复杂度较高、耗时长。Hoover等<citation id="285" type="reference"><link href="228" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>通过计算血管的汇合度,将血管汇合度最大的点确定为视盘。Youssif等<citation id="286" type="reference"><link href="230" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了一种在数字视网膜眼底图像中自动检测OD位置的方法。该方法使用简单的标准二维高斯匹配滤波器对视网膜血管进行分割,然后选出OD中心候选者,最后测量每个OD中心候选者的周围区域的血管方向。所提出的匹配滤波器之间的差异被调整为4种不同的尺寸,并且测量每个OD中心候选者的周围区域的血管方向。最小差异提供OD中心坐标的估计。该方法准确率高但耗时长,每幅图的平均处理时间长达3.5 min。Tobin等<citation id="287" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>则通过分析血管亮度、宽度和方向信息来定位视盘。</p>
                </div>
                <div class="p1">
                    <p id="73">(3)基于多特征融合的视盘定位。Narasimhan等<citation id="288" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出递归应用K-means聚类技术分割视盘,然后通过椭圆化处理,确定视盘的位置。Mahfouz等<citation id="289" type="reference"><link href="236" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>综合考虑视盘亮度和周围血管特征提取图像的特征向量,并采用投影的方法降低特征向量维数,缩短程序运行时间。Yu等<citation id="290" type="reference"><link href="238" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>采用模板匹配的方法定位图像视盘区域,考虑不同图像的分辨率存在差异,该方法使用多种二值图像模板扫描眼底图像,并根据模板区域内像素点的特征,定位视盘区域。相对于基于血管的视盘定位方法,这种方法效率有所提升,其平均处理一幅图像的时间为 4.7 s。Ramakanth等<citation id="291" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了基于最近邻域区域搜索的视盘定位方法,通过图像特征匹配计算候选区域与目标区域之间的相似度,选取相似度大的区域作为视盘定位的结果。Cheng等<citation id="292" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出基于超像素和SVM(Suppot Vector Machine)分类器的方法确定视盘边界。</p>
                </div>
                <div class="p1">
                    <p id="74">综上所述,通过亮度进行视盘定位的方法对于一些灰度值相差较大的眼底图像效果较好,但是一些相对灰度值相差较小的图像对定位效果影响较大。通过血管特征视盘定位的方法充分利用血管的特性( 如血管的走向、血管的结构等信息),可以有效地解决由于病变导致视盘在自身特性不明显的情况下的检测问题。但是,这些方法需要制定较为严格的几何模板,算法的复杂度高,而且模板制定依赖于血管特性的精确分割。为了减少以上问题对视盘定位准确率的影响,本文通过使用YOLO(You Only Look Once)算法对眼底视网膜图像中的视盘进行定位,主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="75">(1)将眼底视网膜图像输入到模型中,通过学习视盘特征与其他区域特征之间的差异对视盘进行识别,在识别的过程中,采用多尺度的思想,将输入模型的图像分割成13×13、26×26、52×52的网格,每一个网格负责检测一个视盘,不同尺寸的网格可以对不同大小的视盘进行检测。</p>
                </div>
                <div class="p1">
                    <p id="76">(2)网络在选择标准框的时候,利用K-means聚类算法,对眼底图像训练集中的视盘边界框进行聚类分析,将所有边界框按大小分成9类,使用这9个中心边界框进行预测,每个尺度使用3个边界框,最终选出得分最高的边界框,其中心点即为视盘的中心点。</p>
                </div>
                <div class="p1">
                    <p id="77">(3)在网络结构中进行特征提取的部分使用残差网络的思想,通过差值的学习,使得模型对权重的改变变得敏感,从而更好地对权重进行调整。模型中使用1×1的卷积核对特征图通道数进行降维,减少模型的参数和计算量,提高视盘定位效率,为研究糖尿病视网膜病变、高血压视网膜病变、视网膜脱离的裂孔定位、眼底肿瘤定位以及视网膜血管性疾病的诊断提供重要方法和依据,也为实现视网膜疾病诊断和预防提供基础工作。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>2 相关理论</b></h3>
                <div class="p1">
                    <p id="79">卷积神经网络CNN(Convolutional Neural Network)是近年发展起来,并引起广泛重视的一种高效识别方法。20世纪60年代,Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性,Fukushima<citation id="293" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在1987年提出的新识别机是卷积神经网络的第1个实现网络,继而提出了CNN。现在,CNN已经成为众多科学领域的研究热点之一,特别是在模式分类领域,由于该网络避免了对图像的复杂前期预处理,可以直接输入原始图像,通过卷积层、池化层和下采样层提取图像特征,最终进行图像分类。</p>
                </div>
                <div class="p1">
                    <p id="80">2014年,Girshick等<citation id="294" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了RCNN(Region-based Convolutional Neural Network)系列。它首次将CNN用于对象检测,RCNN系列能够基于丰富的特征层次结构进行目标精确检测和语义分割,但是由于RCNN网络对候选框的选择和多次重复卷积,大大增加了目标检测的时间和复杂度。2016年,Redmon等<citation id="295" type="reference"><link href="248" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了YOLO算法。YOLO算法只需要对图像进行一次扫描就可以将目标准确定位,是目前最新的目标检测方法。</p>
                </div>
                <div class="p1">
                    <p id="81">YOLO算法的网络模型结构最开始借鉴了GoogleNet分类网络结构<citation id="296" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,但未使用Inception模块,而是使用1×1卷积层(此处1×1卷积层的存在是为了跨通道信息整合)和3×3卷积层简单替代,基于一个端到端的网络结构,完成从原始图像的输入到物体位置和类别的输出,将物体检测作为回归问题求解。YOLO v1相对于之前的物体检测方法有多个优点:(1)检测物体非常快;(2)可以很好地避免背景错误,产生假正例;(3)物体泛化特征较强。同时,它也有一些缺点:(1)物体检测精度低于其他最先进的物体检测系统;(2)容易产生物体的定位错误;(3)对小物体的检测效果不好。</p>
                </div>
                <div class="p1">
                    <p id="82">针对之前的不足,v2在YOLO v1的算法基础上进行了以下改进<citation id="297" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:(1)去掉了dropout层,并在每个卷积层添加了批量归一化。(2)高分辨率分类器,v2将图像输入数据由224×224变为448×448。(3)删除1个池化层,在卷积层之后使用原始框代替全连接层,把输入由448×448转为416×416。(4)使用K-means聚类来确定原始框。(5)选用原始框来预测物体类别。(6)将边界框中心送入激活函数,确保边界框的中心落在栅格中(即坐标落在0～1)。(7)添加1个pass through层,以提高对小物体的检测能力,通过堆叠不同通道中相邻的特征而非空间位置融合高低分辨率的特征。(8)v2网络只用到了卷积层和池化层,因此可以动态调整输入图像的尺寸。通过以上改进,使得v2在检测目标物体速度快的情况下精度更高。</p>
                </div>
                <div class="p1">
                    <p id="83">v3为了使模型的定位准确率更快、更高,提高对小目标的检测能力,YOLO算法又进行了如下改进<citation id="298" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>:(1)多标签分类,将v2预测层中的单标签分类softmax层改进为多标签分类的逻辑回归层。(2)采用多尺度融合的方式做预测。借鉴ResNet网络的原理,在pass through层中将前面一层的特征图层和本层的特征图层进行连接。(3)增加v2中原始框数量,通过K-means聚类的方法从原来的5个原始框增加到9个原始框。(4)删除网络结构中的所有池化层,采用全卷积,增加1×1和3×3卷积层,将19层的v2网络结构增加到53层网络结构,并且引入了残差结构,使得精度提升比较明显。</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag"><b>3 基于YOLO算法的视盘检测方法</b></h3>
                <div class="p1">
                    <p id="85">本文使用的眼底视盘定位模型基于YOLO算法,其中包含54个卷积层和3个上采样层,通过使用带步幅的卷积代替池化层对图片的尺寸进行缩小。模型将所输入的眼底图像数据分成<i>S</i>×<i>S</i>个单元格,每个单元格负责检测是否有视盘“落入”单元格中,利用多次下采样的方法对视盘图像进行特征提取,为了能够多方位地检测到大小不同或位置不同的视盘,模型采用多尺度上采样的方法,分别在图像上的不同尺度进行特征提取和边界框的选择;其次,通过损失函数、边界框得分公式计算出每个候选框的概率,然后使用非极大抑制NMS(Non-Maximum Suppression)的方法设置阈值,设立正负样本,过滤掉得分低的边界框,根据回归分析的方式使用滑动窗口对眼底图像中的视盘进行检测和定位,最终得到眼底图像中视盘的精确位置和中心点位置。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>3.1 网络模型</b></h4>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909020_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络结构图" src="Detail/GetImg?filename=images/JSJK201909020_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909020_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 Network structure</p>

                </div>
                <div class="p1">
                    <p id="88">本文的网络模型如图1所示,该网络结构由多层卷积组成,用于多尺度目标预测。为了使得中心点的位置坐标只有一个,使用<i><b>X</b></i>∈<b>R</b><sup><i>m</i></sup><sup>×</sup><sup><i>n</i></sup><sup>×</sup><sup><i>c</i></sup>(<i>m</i>=<i>n</i>=416,<i>c</i>=3)作为原始图像输入本文的网络模型中,使用多层卷积对图像进行下采样操作,每一个卷积层使用ReLU非线性激活函数,为了防止采样的波动与模型的不稳定,对每个卷积层均使用了批量规范化BN(Batch-Normalization)<citation id="299" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。它的思想是归一化当前层输入,使它们的均值为0,方差为1,类似于归一化网络输入的方法,它的优点在于可以加速收敛,并且加入BN的卷积神经网络模型受权重初始化的影响非常小,具有非常好的稳定性。同时,它还有助于梯度传向更深层的网络,有利于提高卷积的性能和对眼底图像特征的提取。该网络结构中也运用了残差网络的思想,提高了网络结构对权重改变的敏感度,解决了在深度网络结构中梯度消失和梯度爆炸的问题,从而充分地提取眼底图像特征,输出不同尺度的预测和回归结果。</p>
                </div>
                <div class="p1">
                    <p id="89">由于眼底图像中视盘大小和位置不相同,为了更好地定位视盘的位置,本文在不同的特征图上选择不同的原始框大小。在大尺度层预测过程中,本文通过59个卷积层进行特征提取,最终输出一个13×13×18的特征图,即13×13×[3×(4+1+1)]的张量,该特征图是在大尺度选择下对眼底图像507个候选框的预测和回归。同时,大尺度下采样的过程中,添加pass through层,对第57层卷积层的输出进行上采样得到26×26×256的特征图,用该特征图与第43层输出相加得到26×26×768的中尺度特征图,通过卷积、下采样最终输出一个26×26×18的张量,即26×26×[3×(4+1+1)]的张量,该张量是在中尺度特征图选择下对眼底视盘2 028个候选框的预测和回归;在中尺度下采样的过程中,将26×26×256的特征图层进行上采样得到52×52×128的特征图,用该特征图与大尺度卷积过程中的第26层特征图相加得到5×52×384的小尺度初始特征图,通过卷积、下采样最终输出一个52×52×18的张量,即52×52×[3×(4+1+1)]的张量,该张量表示在中尺度特征图上对眼底视盘8 812个候选框预测和回归的坐标及分类。整个过程分为大尺度、中尺度、小尺度3个不同尺度在眼底图像上的预测和回归。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>3.2 视盘检测与定位</b></h4>
                <div class="p1">
                    <p id="91">本文方法对视盘的检测过程如图2所示,首先将眼底图像分成<i>N</i>×<i>N</i>的格子,滑动窗口依次滑过,如果检测到视盘的中心点在网格中,则该网格负责检测视盘。每个格子预测3个边界框,每个边界框的值包括中心点坐标、宽高、置信度5个值,即所预测的边界框的左上角坐标、长、宽(<i>x</i>,<i>y</i>,<i>w</i>,<i>h</i>)和置信度(<i>Confidence</i>),除此之外,每个边界框还要预测出边界框的类别,记为<i>C</i>类。</p>
                </div>
                <div class="p1">
                    <p id="92">其中置信度(<i>Confidence</i>)代表了所预测的边界框中含有视盘的置信度和这个边界框预测的准确度两重信息,其计算公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>n</mi><mi>f</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo>=</mo><mi>Ρ</mi><mi>r</mi><mrow><mo>(</mo><mrow><mi>Ο</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi></mrow><mo>)</mo></mrow><mo>*</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909020_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 视盘检测过程" src="Detail/GetImg?filename=images/JSJK201909020_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 视盘检测过程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909020_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 Optic disc detection process</p>

                </div>
                <div class="p1">
                    <p id="95">其中,<i>Pr</i>(<i>Object</i>)表示视盘是否位于该网格中,如果视盘位于该网格中则<i>Pr</i>(<i>Object</i>)取值为1,否则认为候选框中没有需要检测的视盘,<i>Pr</i>(<i>Object</i>)取值为0。<i>IOU</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup></mrow></math></mathml>(<i>Intersection Over Union</i>)表示预测的边界框和实际的真实框界之间的交叠率,计算公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><mo>∩</mo><mi>a</mi></mstyle><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo stretchy="false">)</mo></mrow><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><mo>∪</mo><mi>a</mi></mstyle><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false">(</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">每个边界框都会有一个对于分类准确置信度的得分,本文用下面的公式计算得分:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>r</mi><mo stretchy="false">(</mo><mi>Ο</mi><mi>D</mi><mo stretchy="false">|</mo><mi>Ο</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mo stretchy="false">)</mo><mo>*</mo><mi>Ρ</mi><mi>r</mi><mo stretchy="false">(</mo><mi>Ο</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mo stretchy="false">)</mo><mo>*</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mo>=</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mi>r</mi><mo stretchy="false">(</mo><mi>Ο</mi><mi>D</mi><mo stretchy="false">)</mo><mo>*</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">其中,Pr(OD|Object)为预测目标为视盘的条件概率,Pr(Object)*IOU<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup></mrow></math></mathml>是每个边界框预测的置信度。两项相乘既包含预测边界框内目标属于视盘的概率,也包含了边界框的准确度信息。通过式(3)得到每个边界框的得分后,使用非极大抑制的方法设置阈值,过滤掉得分低的边界框,得到最终的检测结果。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>3.3 损失函数</b></h4>
                <div class="p1">
                    <p id="101">本文在训练的时候采用了平方和距离误差损失函数进行边界框回归,网络中将输出的<i>S</i>×<i>S</i>×<i>B</i>×(5+<i>C</i>)维向量与真实图像的对应<i>S</i>×<i>S</i>×<i>B</i>×(5+<i>C</i>)维向量的均方和误差作为损失函数优化模型参数。使用平方和距离误差公式的变形,损失函数(如式(4)所示)是由代表预测数据和目标数据之间的坐标误差(如式(5)所示)、<i>IOU</i>误差(如式(6)所示)和分类误差(如式(7)所示)相加得到。</p>
                </div>
                <div class="p1">
                    <p id="102">网络模型的损失函数如下所示:</p>
                </div>
                <div class="p1">
                    <p id="103" class="code-formula">
                        <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munderover><mi>E</mi></mstyle><mo stretchy="false">(</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="104">其中,<i>E</i>(<i>coord</i>)、<i>E</i>(<i>IOU</i>)和<i>E</i>(<i>class</i>)分别代表预测数据与标定数据之间的坐标误差、<i>IOU</i>误差和分类误差。</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">(</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mrow><mtext>E</mtext><mtext>c</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>(</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mrow><mtext>E</mtext><mtext>c</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mrow><mo>[</mo><mrow><mrow><mo>(</mo><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>w</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo>(</mo><mrow><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>h</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中,<i>λ</i><sub>Ec</sub>是坐标误差<i>E</i>(<i>coord</i>)的权重系数。<i>x</i><sub><i>i</i></sub>和<i>y</i><sub><i>i</i></sub>分别表示预测的单元格的边界框中心点的横坐标与纵坐标,<i>w</i><sub><i>i</i></sub>和<i>h</i><sub><i>i</i></sub>分别表示预测的单元格<i>i</i>的边界框的长度和宽度;<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>x</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>、<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>分别表示真实的单元格<i>i</i>的边界框中心点的横坐标与纵坐标,<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>w</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>和<mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>h</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>分别表示真实的单元格的边界框的长度和宽度;<i>I</i><mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow></math></mathml>表示单元格<i>i</i>中第<i>j</i>个边界框是否负责检测视盘对象(其值归一化为0或者1)。</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">(</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mrow><mo>(</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><msub><mrow></mrow><mrow><mtext>E</mtext><mtext>Ι</mtext><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>B</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mrow><mo>(</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">其中,<i>I</i><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow></math></mathml>表示单元格<i>i</i>中第<i>j</i>个边界框负责检测视盘对象;<i>I</i><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow></math></mathml>表示单元格<i>i</i>中第<i>j</i>个边界框不负责检测视盘对象。<i>c</i><sub><i>i</i></sub>表示预测的边界框中单元格<i>i</i>的置信度值;<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>c</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>表示真实的边界框中的单元格<i>i</i>的置信度值。<i>λ</i><sub>Elnoobj</sub>表示预测边界框与真实检测对象区域的重叠面积<i>IOU</i>误差的权重系数。由于绝大部分网格中不包含视盘,导致绝大部分预测边界框的置信度为0,所以同等对待包含视盘和不包含视盘的边界框置信度也是不合理的,若采用相同的权重,则会间接地放大包含有检测对象的单元格的置信度误差,会导致模型不稳定。因此,使用<i>λ</i><sub>Elnoobj</sub>=0.5,以减小传递误差。</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></munderover><mi>Ι</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mi>e</mi><mi>s</mi></mrow></munder><mo stretchy="false">(</mo></mstyle><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo>-</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中,<i>I</i><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>b</mtext><mtext>j</mtext></mrow></msubsup></mrow></math></mathml>表示是否有视盘中心落到第<i>i</i>单元格内。<i>p</i><sub><i>i</i></sub>(<i>c</i>)与<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>p</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>(<i>c</i>)分别表示预测的与真实的滑动窗口中单元格<i>i</i>包含第<i>c</i>类对象的条件概率。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>3.4 多尺度检测</b></h4>
                <div class="p1">
                    <p id="112">为了对不同尺寸的眼底图像进行视盘检测,网络模型在最后进行上采样,将13×13尺度的特征图经过上采样得到26×26的特征图,并与之前下采样得到的26×26的特征图相加得到新的特征图,继续下采样输出预测结果。对52×52的特征图也采取相同的方法,通过这种方式预测3种不同尺度的框(Boxes),将单尺度变为多尺度,并且每个尺度预测3个框,由13×13到26×26到52×52,增加了不同尺度的边界框预测结果,提高了模型的泛化能力和不同尺寸的眼底图像的检测能力。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>3.5 检测视盘中心点坐标</b></h4>
                <div class="p1">
                    <p id="114">在网格预测中,本文通过使用归一化的方式将预测框左上角坐标位置(<i>b</i><sub><i>x</i></sub>,<i>b</i><sub><i>y</i></sub>)、边框的高<i>b</i><sub><i>w</i></sub>和宽<i>b</i><sub><i>h</i></sub>都归一化到 [0,1],以方便运算。</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mrow><mi>t</mi><msub><mrow></mrow><mi>x</mi></msub></mrow><mo>)</mo></mrow><mo>+</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>y</mi></msub><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mrow><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub></mrow><mo>)</mo></mrow><mo>+</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>w</mi></msub><mo>=</mo><mi>p</mi><msub><mrow></mrow><mi>w</mi></msub><mi>e</mi><msup><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></msup></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mi>h</mi></msub><mo>=</mo><mi>p</mi><msub><mrow></mrow><mi>h</mi></msub><mi>e</mi><msup><mrow></mrow><mrow><mi>t</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></msup></mtd></mtr><mtr><mtd><mi>Ρ</mi><mi>r</mi><mrow><mo>(</mo><mrow><mi>Ο</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi></mrow><mo>)</mo></mrow><mo>*</mo><mi>Ι</mi><mi>Ο</mi><mi>U</mi><msubsup><mrow></mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>t</mi><mi>h</mi></mrow></msubsup><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mrow><mi>t</mi><msub><mrow></mrow><mi>o</mi></msub></mrow><mo>)</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">其中,<i>c</i><sub><i>x</i></sub>和<i>c</i><sub><i>y</i></sub>是网格的坐标偏移量,<i>p</i><sub><i>w</i></sub>和<i>p</i><sub><i>h</i></sub>是预设的边界框的边长。最终得到边框右上角的坐标值是(<i>b</i><sub><i>x</i></sub>,<i>b</i><sub><i>y</i></sub>),视盘的中心点坐标为(<i>t</i><sub><i>x</i></sub>,<i>t</i><sub><i>y</i></sub>),通过如下操作定位视盘的边界框:(1)对<i>t</i><sub><i>x</i></sub>和<i>t</i><sub><i>y</i></sub>进行sigmoid激活,并加上对应的偏移量<i>c</i><sub><i>x</i></sub>和<i>c</i><sub><i>y</i></sub>。(2)对<i>t</i><sub><i>h</i></sub>和<i>t</i><sub><i>w</i></sub>进行指数运算,并乘以对应的中心点值。(3)将<i>t</i><sub><i>x</i></sub>,<i>t</i><sub><i>y</i></sub>,<i>t</i><sub><i>w</i></sub>,<i>t</i><sub><i>h</i></sub>乘以对应的步幅。(4)使用sigmoid激活函数对视盘(Objectness)和视盘可信度(Classes Confidence)进行sigmoid激活得到0～1的概率,使用sigmoid激活函数取代softmax激活函数可以防止扩大最大类别概率值而抑制其他类别概率值。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.6 数据增强</b></h4>
                <div class="p1">
                    <p id="118">由于网络模型的训练需要大量的训练样本,本文通过数据增强的方式对训练样本进行扩充从而解决这个问题。数据的增强对于提高网络分类的准确率,防止过拟合和增强网络的鲁棒性至关重要。主要通过如下方式:</p>
                </div>
                <div class="p1">
                    <p id="119">(1)使用原始的图像。</p>
                </div>
                <div class="p1">
                    <p id="120">(2)在原始图像中截取一个区域,所截取区域内包含检测视盘的10%、30%、50%、70%与90%。</p>
                </div>
                <div class="p1">
                    <p id="121">(4)在原始图像中随机截取一个区域,截取的区域的大小为原始图像大小的10%～100%,宽高比例在0.5～2。当标准样本集检测框的中心在采样的区域中时,保留重叠部分。</p>
                </div>
                <div class="p1">
                    <p id="122">在这些采样步骤之后,将每一个采样区域的大小调整到416×416,并且以50%的概率随机进行水平翻转和垂直翻转。</p>
                </div>
                <h3 id="123" name="123" class="anchor-tag"><b>4 实验分析</b></h3>
                <div class="p1">
                    <p id="124">本文的模型以Darknet53为基础使用C++编程实现,运行环境为Ubuntu系统,使用NVIDIA Tesla K80 GPU对模型进行训练,训练时使用Momentum作为优化器进行梯度下降,冲量常数<i>β</i>=0.9,以指数的方式进行学习率衰减,权值衰减系数为0.000 5。另外,Dropout方法以0.5的概率随机将神经元置为零,从而丢弃神经网络中的部分神经元,以减少计算成本,降低节点间耦合性,缓解过拟合问题。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>4.1 数据集</b></h4>
                <div class="p1">
                    <p id="126">本文在3个公共的眼底数据集上进行了实验,包括DRISHTI-GS1、DRIVE和MESSIDOR数据集,其中DRIVE数据集由33幅正常图像和7幅患病图像组成,分成20张训练集和20张测试集,图像大小为768×584,视盘的半径大致为40 像素。DRISHTI-GS1数据集包含31幅正常的眼底图像和70幅患病的眼底图像,其中50 幅为训练图像,51 幅为测试图像,图像的大小为2896×1944,视盘半径大概为210像素。MESSIDOR包含546幅正常的眼底图像和654幅患病的眼底图像,是目前已公开的眼底图像数据集中受到不同拍摄环境影响且形态各异、存在不同程度的病灶、数目最多的数据集。数据集包含有3种分辨率,分别为:2304×1536、1400×960和2240×1488。3种不同大小的图像对应的视盘的半径分别为110、70和100像素,是视盘定位中最具有代表性数据集。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>4.2 评价指标</b></h4>
                <div class="p1">
                    <p id="128">本文通过2种方式对模型进行评估:(1)模型是否能准确地定位出眼底视网膜图像中的视盘,并将视盘框出来,同时边界框的左上角正确地标记该目标为视盘的可信度。(2)模型定位出的视盘中心点与标准的视盘中心点之间的误差,如图3所示,(<i>x</i><sub>1</sub>,<i>y</i><sub>1</sub>)为标准的视盘中心点坐标,(<i>x</i><sub>2</sub>,<i>y</i><sub>2</sub>)为模型标记的视盘中心点。<i>ED</i>为标准视盘中心点与模型标记的视盘中心点的欧几里得距离,理想的<i>ED</i>值为0,计算公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>D</mi><mo>=</mo><msqrt><mrow><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909020_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 标准视盘中心与模型定位视盘中心距离" src="Detail/GetImg?filename=images/JSJK201909020_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 标准视盘中心与模型定位视盘中心距离  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909020_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Distance between standard disc center 
 and model positioning disc center</p>

                </div>
                <h4 class="anchor-tag" id="131" name="131"><b>4.3 定位结果分析</b></h4>
                <div class="p1">
                    <p id="132">将本文方法在上述3个数据集上进行了验证,部分视盘的定位结果如图4所示。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201909020_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 实验定位结果" src="Detail/GetImg?filename=images/JSJK201909020_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 实验定位结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201909020_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Experimental results for positioning</p>

                </div>
                <div class="p1">
                    <p id="134">图4a为DRIVE数据集中眼底视盘区域定位的结果,其中第1幅和第2幅图像中,视盘区域不明显,且第2幅图像中视盘区域存在着变形,模型均能准确地找到视盘区域的中心点位置,并将视盘区域标记出来。图4b展示了对DRISHTI-GS1数据集中部分眼底视网膜图像中的视盘进行标记的结果,由于受到拍摄环境的影响,有的眼底图像比较暗,有的边缘出现高亮,有的由于病变的原因,视盘周围存在亮斑,出现豹纹形态。可以从标注的结果看出,本文训练出的深度网络模型对不同形态、不同程度病变的眼底图像中的视盘定位结果都比较准确,不受背景干扰,泛化性强。图4c和图4d是对MESSIDOR数据集中部分数据进行定位的结果,其中有些图像病灶面积的亮度、面积、颜色等特征和视盘相近,难以区分,部分眼底图像的视盘亮度低,与背景颜色相近,肉眼难以区分,但模型能克服这些困难,对视盘区域进行准确定位。</p>
                </div>
                <div class="p1">
                    <p id="135">此外,本文对上述3个数据集中视盘定位的结果进行了统计,对于3个数据集中的总共1 341幅眼底图像,本文方法可以准确定位所有图像的视盘区域,视盘定位的准确率为 100%,定位一幅图像的平均耗时为0.1 s。从表1中可以看出,本文提出的方法对不同数据集中的眼底图像都适用。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表1 本文方法对不同眼底图像数据集视盘定位结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Positioning results of different fundus image datasets by our method</b></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td><br />数据集</td><td>图像<br />总数量</td><td>正确定<br />位数量</td><td>准确<br />率/%</td><td>时间<br />/s</td><td>中心点<br />距离差/px</td></tr><tr><td>DRISHTI-GS1</td><td>101</td><td>101</td><td>100</td><td>0.098 0</td><td>22.36</td></tr><tr><td><br />DRIVE</td><td>40</td><td>40</td><td>100</td><td>0.109 1</td><td>2.52</td></tr><tr><td><br />MESSIDOR</td><td>1 200</td><td>1 200</td><td>100</td><td>0.100 0</td><td>21.42</td></tr><tr><td><br />总数</td><td>1 341</td><td>1 341</td><td>100</td><td>0.100 0</td><td>15.43</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">本文方法对视盘定位的准确率比较高,原因是本文使用深度卷积神经网络充分地提取了视盘的亮度特征和其周围血管分布的特性,结合多尺度的方法对视盘区域进行定位,该方法能准确定位不同形态,包括存在边缘高亮环和病灶区域的眼底图像视盘区域,能够应对不同尺寸的视盘;其次,本文采用了残差网络,使得模型对权重的改变变得敏感,从而对权重进行更好的调整,使得对视盘区域中心点定位更加准确。此外,考虑到效率在计算机辅助诊断中的重要性,本文通过使用1×1的卷积核和3×3的卷积核对特征图通道数进行转换,首先使用1×1的卷积核减少通道数,再进行3×3的卷积,然后使用1×1的卷积核提升通道数,从而减少参数数量,降低模型计算量,提高模型效率。</p>
                </div>
                <div class="p1">
                    <p id="138">为了进一步验证本文方法的有效性,对比文献<citation id="300" type="reference">[<a class="sup">24</a>]</citation>方法、文献<citation id="301" type="reference">[<a class="sup">25</a>]</citation>方法、文献<citation id="302" type="reference">[<a class="sup">27</a>]</citation>方法与本文方法在DRIVE数据集中的眼底图像视盘定位的准确率和时间,其比较结果如表2所示。从表2可以看出,文献<citation id="303" type="reference">[<a class="sup">24</a>]</citation>方法在DRIVE数据集上的准确率为95.0%,正确定位视盘的数量为38幅,文献<citation id="304" type="reference">[<a class="sup">25</a>]</citation>方法和文献<citation id="305" type="reference">[<a class="sup">26</a>,<a class="sup">26</a>]</citation>方法在DRIVE数据集上的准确率都为97.5%,正确定位视盘的数量为39幅,视盘定位准确率均小于本文在DRIVE数据集上的准确率。同时文献<citation id="306" type="reference">[<a class="sup">24</a>]</citation>方法和文献<citation id="307" type="reference">[<a class="sup">25</a>]</citation>方法中平均每一幅眼底图像视盘定位的时间分别为3.80 s和5.00 s,文献<citation id="308" type="reference">[<a class="sup">26</a>,<a class="sup">26</a>]</citation>使用的深度学习方法,定位的时间比文献<citation id="309" type="reference">[<a class="sup">24</a>]</citation>和文献<citation id="310" type="reference">[<a class="sup">25</a>]</citation>的方法定位时间要短很多,本文通过使用1×1的卷积在保持精度不变的前提下降低了模型计算量,缩短了定位时间。</p>
                </div>
                <div class="area_img" id="139">
                    <p class="img_tit"><b>表2 不同方法在DRIVE数据集上视盘定位结果1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Optic disc positioning results1 of different methods on the DRIVE dataset</b></p>
                    <p class="img_note"></p>
                    <table id="139" border="1"><tr><td><br />方法</td><td>正确定位数量</td><td>准确率/%</td><td>时间/s</td></tr><tr><td><br />文献[25]</td><td>38</td><td>95.0</td><td>3.80</td></tr><tr><td><br />文献[26]</td><td>39</td><td>97.5</td><td>5.00</td></tr><tr><td><br />文献[27]</td><td>39</td><td>97.5</td><td>1.00</td></tr><tr><td><br />本文</td><td>40</td><td>100.0</td><td>0.10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="140">表3对比了文献<citation id="311" type="reference">[<a class="sup">14</a>]</citation>方法、文献<citation id="312" type="reference">[<a class="sup">27</a>]</citation>方法与本文方法在MESSIDOR数据集中的眼底图像视盘定位的准确率和时间。从表3可以看出,文献<citation id="313" type="reference">[<a class="sup">14</a>]</citation>方法在MESSIDOR数据集上的准确率约为99.1%,正确定位视盘的数量为1 189幅,文献<citation id="314" type="reference">[<a class="sup">27</a>]</citation>方法在MESSIDOR数据集上的准确率约为98.3%,正确定位视盘的数量为1 180幅,视盘定位准确率均小于本文方法在MESSIDOR数据集上的准确率。同时,文献<citation id="315" type="reference">[<a class="sup">14</a>]</citation>方法和文献<citation id="316" type="reference">[<a class="sup">27</a>]</citation>方法平均每一幅眼底图像视盘定位的时间分别为4.70 s和0.25 s,定位时间大于本文方法定位的时间,从而体现了本文方法的视盘定位在准确率和时间方面的优越性。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表3 不同方法在MESSIDOR数据集上视盘定位结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Optic disc positioning results of different methods on the MESSIDOR dataset</b></p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td><br />方法</td><td>正确定位数量</td><td>准确率/%</td><td>时间/s</td></tr><tr><td><br />文献[14]</td><td>1 189</td><td>99.1</td><td>4.70</td></tr><tr><td><br />文献[27]</td><td>1 180</td><td>98.3</td><td>0.25</td></tr><tr><td><br />本文</td><td>1 200</td><td>100.0</td><td>0.11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="142">本文方法能够准确地定位出所有眼底图像中的视盘,且能定位出视盘的中心点,在DRIVE数据集中,定位出的视盘中心点与标准的视盘中心点之间的平均欧氏距离为2.52 px,基本与标准的中心点位置相差无误。在表 4中,通过与文献<citation id="317" type="reference">[<a class="sup">14</a>]</citation>方法、文献<citation id="318" type="reference">[<a class="sup">27</a>]</citation>方法和文献<citation id="319" type="reference">[<a class="sup">28</a>,<a class="sup">28</a>]</citation>方法进行比较,可以看出文献<citation id="320" type="reference">[<a class="sup">14</a>]</citation>方法、文献<citation id="321" type="reference">[<a class="sup">27</a>]</citation>方法和文献<citation id="322" type="reference">[<a class="sup">28</a>,<a class="sup">28</a>]</citation>方法中的像素平均误差为17 px、11 px和9 px。本文方法的定位误差小于其他方法的,说明本文方法优于其他方法。</p>
                </div>
                <h3 id="143" name="143" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="144">本文使用YOLO算法对眼底图像中的视盘进行定位,将需要输入的眼底图像分成<i>N</i>×<i>N</i>的网格,通过卷积依次检测网格内是否有需要检测的视盘,为了防止网络层过深导致梯度消失或梯度爆炸,本文使用残差模块对特征图进行差值学习,更好地学习到图像中视盘区域与其他区域间的特征差别,通过使用1×1的卷积核在保证模型效率的情况下降低模型计算量,减少参数数目。通过多尺度的思想实现对大小不同的视盘进行定位,使用K-means对数据集中不同尺寸的边界框进行聚类,选择9个不同尺寸的原始框,实现对不同大小的视盘进行定位。本文在3个公开的眼底图像数据集(DRIVE、DRISHTI-GS1和MESSIDOR)上对所提方法进行了测试,对于数据集中不同尺寸的眼底图像,都能将视盘100%地定位成功,在3个数据集中定位的视盘中心点与标准的视盘中心的平均欧氏距离为15.43 px,对每幅图像定位的平均时间为0.1 s,通过实验结果对比,本文的方法对视盘的定位相比其他方法具有更高的效率和更好的鲁棒性。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表4 不同方法在DRIVE数据集上视盘定位结果2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Optic disc positioning results2 of different methods on the DRIVE dataset</b></p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />方法</td><td>平均误差/px</td><td>时间/s</td></tr><tr><td><br />文献[14]</td><td>17.00</td><td>210.00</td></tr><tr><td><br />文献[27]</td><td>11.00</td><td>0.32</td></tr><tr><td><br />文献[28]</td><td>9.00</td><td>20.00</td></tr><tr><td><br />本文</td><td>2.52</td><td>0.10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="212">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 Zou Bei-ji,Zhang Si-jian,Zhu Cheng-zhang.Automatic localization and segmentation of optic disk in color fundus image[J].Optics and Precision Engineering,2015,23(4):1187-1195.(in Chinese)
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated localisation of the optic disc, fovea, and retinal blood vessels from digital colour fundus images">

                                <b>[2]</b> Sinthanayothin C,Boyce J F,Cook H L,et al.Automated localisation of the optic disc,fovea,and retinal blood vessels from digital colour fundus images[J].British Journal Ophthalmology,1999,83(8):902-910.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated Feature Extraction in Color Retinal Images by a Model Based Approach">

                                <b>[3]</b> Li Hui-qi,Chutatape O.Automated feature extraction in color retinal images by a model based approach[J].IEEE Transactions on Biomedical Engineering,2004,51(2):246-254.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic Optic Disc Detection From Retinal Images by a Line Operator">

                                <b>[4]</b> Lu S,Lim J H.Automatic optic disc detection from retinal images by a line operator[J].IEEE Transactions on Bio-medical Engineering,2011,58(1):88-94.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locating the optic disc in retinal images">

                                <b>[5]</b> Park M,Jin J S,Luo S.Locating the optic disc in retinal images[C]//Proc of International Conference on Computer Graphics,Imaging and Visualisation,2006:141-145.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic localization of the optic disc in digital colour images of the human retina">

                                <b>[6]</b> Haar F T.Automatic localization of the optic disc in digital colour images of the human retina[D].Utrecht:Utrecht University,2005.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011701757617&amp;v=MTUzMTBETnFJOUVZKzRJQ24wK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsOFdhQmM9TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Daniel W,Jacob S,Cleysonm K,et al.Segmentation of optic disk in color eye fundus images using an adaptive morphological approach [J].Computer in Biology and Medicine,2010,40(2):124-137.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection of optic disc in retinal images by means of a geometrical model of vessel structure">

                                <b>[8]</b> Foracchia M,Grisan E,Ruggeri A.Detection of optic disc in retinal images by means of a geometrical model of vessel structure[J].IEEE Transactions on Medical Imaging,2004,23(10):1189-1195.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels">

                                <b>[9]</b> Hoover A,Goldbaum M.Locating the optic nerve in a retinal image using the fuzzy convergence of the blood vessels[J].IEEE Transactions on Medical Imaging,2003,22(8):951-958.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optic Disc Detection From Normalized Digital Fundus Images by Means of a Vessels&amp;#39; Direction Matched Filter">

                                <b>[10]</b> Youssif A R,Ghalwash A Z,Ghoneim A R.Optic disc detection from normalized digital fundus images by means of a vessels’ direction matched filter[J].IEEE Transactions on Medical Imaging,2008,27(1):11-18.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection of Anatomic Structures in Human Retinal Imagery">

                                <b>[11]</b> Tobin K W,Chaum E,Govindasamy V P,et al.Detection of an atomic structures in human retinal imagery[J].IEEE Transactions on Medical Imaging,2007,26(12):1729-1739.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glaucoma detection from fundus image using OpenCV">

                                <b>[12]</b> Narasimhan K,Vijayarekha K,Joginarayana K A,et al.Glaucoma detection from fundus image using OpenCV[J].Research Journal of Applied Sciences Engineering &amp; Technology,2012,4(24):5459-5463.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast localization of the optic disc using projection of image features">

                                <b>[13]</b> Mahfouz A E,Fahmy A S.Fast localization of the optic disc using projection of image features[J].IEEE Transactions on Image Processing,2010,19(12):3285-3289.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Localization and Segmentation of Optic Disk in Retinal Images Using Directional Matched Filtering and Level Sets">

                                <b>[14]</b> Yu H,Barriga E S,Agurto C,et al.Fast localization and segmentation of optic disk in retinal images using directional matched filtering and level sets[J].IEEE Transactions on Information Technology in Biomedicine,2012,16(4):644-657.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA0B888CCEA0A783C8FF5AF200C6E2A2F&amp;v=MzE3NjBmWHN4ekdVYm5FbDRPUW5nckJKR2Y4ZVdOTGpwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkJod3JtOXdxMD1OaWZPZmNLNGJObkVwL3cyRVpvUA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Ramakanth S A,Babu R V.Approximate nearest neighbor field based optic disk detection[J].Computerized Medical Imaging &amp; Graphics,2014,38(1):49-56.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Superpixel classification based optic disc and optic cup segmentation for glaucoma screening">

                                <b>[16]</b> Cheng J,Liu J,Xu Y,et al.Superpixel classification based optic and optic cup segmentation screening [J].IEEE Transactions on Medical Imaging,2013,32(6):1019-1032.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural network model for selective attention in visual pattern recognition and associative recall">

                                <b>[17]</b> Fukushima K.Neural network model for selective attention in visual pattern recognition and associative recall[J].Applied Optics,1987,26(23):4985.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region-based Convolutional Networks for Accurate Object Detection and Segmentation">

                                <b>[18]</b> Girshick R,Donahue J,Darrell T,et al.Region-based convolutional networks for accurate object detection and segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,38(1):142-158.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You Only Look Once:Unified,Real-Time Object Detection">

                                <b>[19]</b> Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:779-788.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rethinking the inception architecture for computer vision">

                                <b>[20]</b> Szegedy C,Vanhoucke V,Ioffe S,et al.Rethinking the inception architecture for computer vision[C]//Proc of 2016 IEEE Conference on Computer Vision and Pattern Recognition,2016:2818-2826.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better faster stronger">

                                <b>[21]</b> Redmon J,Farhadi A.YOLO9000:Better,faster,stronger[J].arXiv preprint arXiv:1612.08242,2016.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An incremental improvement">

                                <b>[22]</b> Redmon J,Farhadi A.YOLOv3:An incremental improvement[J].arXiv preprint arXiv:1804.02767,2018.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[23]</b> Ioffe S,Szegedy C.Batch normalization:Accelerating deep network training by reducing internal covariate shift[C]//Proc of the 32nd International Conference on Machine Learning,2015:448-456.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optic disk localization using L1 minimization">

                                <b>[24]</b> Sinha N,Babu R V.Optic disk localization using L1 minimization[C]//Proc of IEEE International Conference on Image Processing,2013:2829-2832.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate and efficient optic disc detection and segmentation by a circular transformation">

                                <b>[25]</b> Lu S.Accurate and efficient optic disc detection and segmentation by a circular transformation[J].IEEE Transactions on Medical Imaging,2011,30(12):2126-2133.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                 Zhang Gui-ying,Zhang Xian-jie.Deep learning based on optic disk automatic detection[J].Journal of Guizhou Normal College,2017,33(3):27-32.(in Chinese)
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15120600576402&amp;v=MzExNDJjPU5pZk9mYks5SDlQTXFZOUZZZXdKQ0h3N29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxmSUpsOFdhQg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> Harangi B,Hajdu A.Detection of the optic disc in fundus images by combining probability models[J].Computers in Biology &amp; Medicine,2015,65(C):10-24.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_28" >
                                    <b>[28]</b>
                                 Zhao Xiao-fang,Lin Tu-sheng,Li Bi.Fast automatic localization of optic in retinal images[J].Journal of South China University of Technology(Natural Science Edition),2011,39(2):71-75.(in Chinese)
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ultrafast localization of the optic disc using dimensionality reduction of the search space">

                                <b>[29]</b> Mahfouz A E.Fahmy A S.Ultrafast localization of the optic disc using dimensionality reduction of the search space[C]//Proc of the 12th International Conference on Medical Image Computing and Computer Assisted Intervention,2009:985-992.附中文参考文献:
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201504035&amp;v=MTUxMDZPZVplUm1GeTdrVWIzTklqWEJZN0c0SDlUTXE0OUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 邹北骥,张思剑,朱承璋.彩色眼底图像视盘自动定位与分割[J].光学精密工程,2015,23(4):1187-1195.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GZJY201703007&amp;v=MTA1MDJmQmQ3RzRIOWJNckk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5N2tVYjNOSWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> 张贵英,张先杰.基于深度学习的视盘自动检测[J].贵州师范学院学报,2017,39(3):27-32.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HNLG201102012&amp;v=MjY0NjZHNEg5RE1yWTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk3a1ViM05MU1BIYWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> 赵晓芳,林土胜,李碧.视网膜图像中视盘的快速自动定位方法[J].华南理工大学学报(自然科学版),2011,39(2):71-75.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201909020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201909020&amp;v=MjU0OTQ3a1ViM05MejdCWmJHNEg5ak1wbzlIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
