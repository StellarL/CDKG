<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637132348778155000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJK201910018%26RESULT%3d1%26SIGN%3dhtO2AE4MCwEErYTVP%252fCWc%252fdgByE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJK201910018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910018&amp;v=MjQ0OTQ5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnZPTHo3QlpiRzRIOWpOcjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;2 网络模型&lt;/b&gt; "><b>2 网络模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;2.1 经典LeNet-5模型&lt;/b&gt;"><b>2.1 经典LeNet-5模型</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;2.2 改进的LeNet-5网络&lt;/b&gt;"><b>2.2 改进的LeNet-5网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;3 模型训练算法&lt;/b&gt; "><b>3 模型训练算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="(1)卷积层。">(1)卷积层。</a></li>
                                                <li><a href="#86" data-title="(2)池化层。">(2)池化层。</a></li>
                                                <li><a href="#89" data-title="(3)全连接层。">(3)全连接层。</a></li>
                                                <li><a href="#92" data-title="(4)输出层。">(4)输出层。</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="&lt;b&gt;4 实验结果&lt;/b&gt; "><b>4 实验结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#112" data-title="&lt;b&gt;4.1 数据集&lt;/b&gt;"><b>4.1 数据集</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;4.2 MNIST实验结果&lt;/b&gt;"><b>4.2 MNIST实验结果</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;4.3 Fashion-MNIST实验结果&lt;/b&gt;"><b>4.3 Fashion-MNIST实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#130" data-title="&lt;b&gt;5 实验分析&lt;/b&gt; "><b>5 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#131" data-title="&lt;b&gt;5.1 层间相关性传播可视化算法&lt;/b&gt;"><b>5.1 层间相关性传播可视化算法</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;5.2 MNIST结果分析&lt;/b&gt;"><b>5.2 MNIST结果分析</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;5.3 Fashion-MNIST结果分析&lt;/b&gt;"><b>5.3 Fashion-MNIST结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="&lt;b&gt;6 结束语&lt;/b&gt; "><b>6 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="图1 LeNet-5卷积神经网络结构图">图1 LeNet-5卷积神经网络结构图</a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表1 LeNet-5中Layer2与Layer3之间的连接方式&lt;/b&gt;"><b>表1 LeNet-5中Layer2与Layer3之间的连接方式</b></a></li>
                                                <li><a href="#78" data-title="图2 双路卷积神经网络结构图">图2 双路卷积神经网络结构图</a></li>
                                                <li><a href="#178" data-title="图3 Fashion-MNIST数据集类别名称及示例图">图3 Fashion-MNIST数据集类别名称及示例图</a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表2 MNIST分类正确率对比&lt;/b&gt;"><b>表2 MNIST分类正确率对比</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表3 Fashion-MNIST分类正确率对比&lt;/b&gt;"><b>表3 Fashion-MNIST分类正确率对比</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表4 不同类别分类错误统计&lt;/b&gt;"><b>表4 不同类别分类错误统计</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;表5 不同方法在Fashion-MNIST上的正确率对比&lt;/b&gt;"><b>表5 不同方法在Fashion-MNIST上的正确率对比</b></a></li>
                                                <li><a href="#129" data-title="图4 LRP算法描述">图4 LRP算法描述</a></li>
                                                <li><a href="#139" data-title="图5 2种网络在MNIST上的热图">图5 2种网络在MNIST上的热图</a></li>
                                                <li><a href="#140" data-title="图6 2种网络在Fashion-MNIST上的热图">图6 2种网络在Fashion-MNIST上的热图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="315">


                                    <a id="bibliography_1" title=" Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of Advances in Neural Information Processing Systems,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neu-ral Networks">
                                        <b>[1]</b>
                                         Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of Advances in Neural Information Processing Systems,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="317">


                                    <a id="bibliography_2" title=" Szegedy C,Liu W,Jia Y,et al.Going deeper with convolutions[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[2]</b>
                                         Szegedy C,Liu W,Jia Y,et al.Going deeper with convolutions[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:1-9.
                                    </a>
                                </li>
                                <li id="319">


                                    <a id="bibliography_3" title=" Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition[C]//Proc of International Conference on Learning Representation,2015:1-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[3]</b>
                                         Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition[C]//Proc of International Conference on Learning Representation,2015:1-14.
                                    </a>
                                </li>
                                <li id="321">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     He K M,Zhang X Y,Ren S Q,et al.Deep residual learning for image recognition[C]// Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.</a>
                                </li>
                                <li id="323">


                                    <a id="bibliography_5" title=" He K,Zhang X,Ren S,et al.Identity mappings in deep residual networks[C]// Proc of the 14th European Conference on Computer Vision,2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity Mappings in Deep Residual Networks">
                                        <b>[5]</b>
                                         He K,Zhang X,Ren S,et al.Identity mappings in deep residual networks[C]// Proc of the 14th European Conference on Computer Vision,2016:630-645.
                                    </a>
                                </li>
                                <li id="325">


                                    <a id="bibliography_6" title=" Hubel D H,Wiesel T N.Receptive fields,binocular interaction and functional architecture in the cat’s visual cortex[J].Journal of Physiology,1962,160(1):106-154." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Receptive fields, binocular interaction and functional architecture in the cat&amp;#39;s visual cortex">
                                        <b>[6]</b>
                                         Hubel D H,Wiesel T N.Receptive fields,binocular interaction and functional architecture in the cat’s visual cortex[J].Journal of Physiology,1962,160(1):106-154.
                                    </a>
                                </li>
                                <li id="327">


                                    <a id="bibliography_7" title=" Fukushima K,Miyake S,Ito T.Neocognitron:A neural network model for a mechanism of visual pattern recognition[J].IEEE Transactions on Systems,Man &amp;amp; Cybernetics,1982,SMC-13(5):826-834." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neocognitron:A neural network model for a mechanism of visual pattern recognition">
                                        <b>[7]</b>
                                         Fukushima K,Miyake S,Ito T.Neocognitron:A neural network model for a mechanism of visual pattern recognition[J].IEEE Transactions on Systems,Man &amp;amp; Cybernetics,1982,SMC-13(5):826-834.
                                    </a>
                                </li>
                                <li id="329">


                                    <a id="bibliography_8" title=" Rumelhart D E.Learning representations by back-propagating errors[J].Nature,1986,323(6088):399-421." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning representations by back propagating errors">
                                        <b>[8]</b>
                                         Rumelhart D E.Learning representations by back-propagating errors[J].Nature,1986,323(6088):399-421.
                                    </a>
                                </li>
                                <li id="331">


                                    <a id="bibliography_9" title=" Lecun Y,Boser B,Denker J S,et al.Handwritten digit recognition with a back-propagation network[J].Advances in Neural Information Processing Systems,1990,2(2):396-404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Handwritten Digit Recognition with a Back-Propagation Network">
                                        <b>[9]</b>
                                         Lecun Y,Boser B,Denker J S,et al.Handwritten digit recognition with a back-propagation network[J].Advances in Neural Information Processing Systems,1990,2(2):396-404.
                                    </a>
                                </li>
                                <li id="333">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     Lecun Y,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="335">


                                    <a id="bibliography_11" title=" Zhang Hui,Su Hong,Zhang Xue-liang,et al.Convolutional neural network for robust pitch determination[J].Acta Automatica Sinica,2016,42(6):959-964.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606017&amp;v=MTc1MjFDTGZZYkc0SDlmTXFZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS9nVmJ2Qks=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Zhang Hui,Su Hong,Zhang Xue-liang,et al.Convolutional neural network for robust pitch determination[J].Acta Automatica Sinica,2016,42(6):959-964.(in Chinese)
                                    </a>
                                </li>
                                <li id="337">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     Jiang Wen-chao,Liu Hai-bo,Yang Yu-jie,et al.A high similar image recognition and classification algorithm fusing wavelet transform and convolution neural network[J].Computer Engineering &amp;amp; Science,2018,40(9):1646-1652.(in Chinese)</a>
                                </li>
                                <li id="339">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Tao Zhu,Liu Zheng-xi,Xiong Yun-yu,et al.Pedestrian head detection based on deep neural networks[J].Computer Engineering &amp;amp; Science,2018,40(8):1475-1481.(in Chinese)</a>
                                </li>
                                <li id="341">


                                    <a id="bibliography_14" title=" Zhang Ting,Li Yu-jian,Hu Hai-he,et al.A gender classification model based on cross-connected convolutional neural networks[J].Acta Automatica Sinica,2016,42(6):858-865.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606006&amp;v=MDMxODViRzRIOWZNcVk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnZCS0NMZlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Zhang Ting,Li Yu-jian,Hu Hai-he,et al.A gender classification model based on cross-connected convolutional neural networks[J].Acta Automatica Sinica,2016,42(6):858-865.(in Chinese)
                                    </a>
                                </li>
                                <li id="343">


                                    <a id="bibliography_15" title=" Li Yong,Lin Xiao-zhu,Jiang Meng-ying.Facial expression recognition with cross-connect LeNet-5 network[J].Acta Automatica Sinica,2018,44(1):176-182.(in Chinese)" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201801015&amp;v=MjYzODFyQ1VSTE9lWmVSbUZ5L2dWYnZCS0NMZlliRzRIOW5Ncm85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Li Yong,Lin Xiao-zhu,Jiang Meng-ying.Facial expression recognition with cross-connect LeNet-5 network[J].Acta Automatica Sinica,2018,44(1):176-182.(in Chinese)
                                    </a>
                                </li>
                                <li id="345">


                                    <a id="bibliography_16" title=" Tian Zheng-xin.Traffic sign recognition method based on multi scale convolutional neural network[D].Xi’an:Chang’an University,2017.(in Chinese)" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Traffic sign recognition method based on multi scale convolutional neural network">
                                        <b>[16]</b>
                                         Tian Zheng-xin.Traffic sign recognition method based on multi scale convolutional neural network[D].Xi’an:Chang’an University,2017.(in Chinese)
                                    </a>
                                </li>
                                <li id="347">


                                    <a id="bibliography_17" title=" Xu Y,Li Y,Wang Y,et al.Gland instance segmentation using deep multichannel neural networks[J].IEEE Transactions on Biomedical Engineering,2017,64(12):2901-2912." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gland instance segmentation using deep multichannel neural networks">
                                        <b>[17]</b>
                                         Xu Y,Li Y,Wang Y,et al.Gland instance segmentation using deep multichannel neural networks[J].IEEE Transactions on Biomedical Engineering,2017,64(12):2901-2912.
                                    </a>
                                </li>
                                <li id="349">


                                    <a id="bibliography_18" title=" Xiao H,Rasul K,Vollgraf R.Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms[J].arXiv preprint arXiv:1708.07747,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms">
                                        <b>[18]</b>
                                         Xiao H,Rasul K,Vollgraf R.Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms[J].arXiv preprint arXiv:1708.07747,2017.
                                    </a>
                                </li>
                                <li id="351">


                                    <a id="bibliography_19" title=" Canny J.A computational approach to edge detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1986,PAMI-8(6):679-698." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computational approach to edge detection">
                                        <b>[19]</b>
                                         Canny J.A computational approach to edge detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1986,PAMI-8(6):679-698.
                                    </a>
                                </li>
                                <li id="353">


                                    <a id="bibliography_20" title=" Rosenblatt F.The perceptron:A probabilistic model for information storage and organization in the brain[J].Psychological Review,1958,65(6):386-408." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The perceptron: a probabilistic model for information storage and organization in the brain">
                                        <b>[20]</b>
                                         Rosenblatt F.The perceptron:A probabilistic model for information storage and organization in the brain[J].Psychological Review,1958,65(6):386-408.
                                    </a>
                                </li>
                                <li id="355">


                                    <a id="bibliography_21" title=" Quinlan J R.Induction of decision trees[J].Machine Learning,1986,1(1):81-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MTYxNDZvUFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNEbFZMck9KRmM9Tmo3QmFyTzRIdEhOckl4Q2JP&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Quinlan J R.Induction of decision trees[J].Machine Learning,1986,1(1):81-106.
                                    </a>
                                </li>
                                <li id="357">


                                    <a id="bibliography_22" title=" Bottou L.Stochastic gradient descent tricks[M]//Montavon G,Orv G B,M&#252;ller K R.Neural Networks:Tricks of the Trade.Berlin:Springer Berlin Heidelberg,2012:421-436." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic Gradient Descent Tricks">
                                        <b>[22]</b>
                                         Bottou L.Stochastic gradient descent tricks[M]//Montavon G,Orv G B,M&#252;ller K R.Neural Networks:Tricks of the Trade.Berlin:Springer Berlin Heidelberg,2012:421-436.
                                    </a>
                                </li>
                                <li id="359">


                                    <a id="bibliography_23" title=" Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C]//Proc of the 13th European Conference on Computer Vision(ECCV 2014),2014:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[23]</b>
                                         Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C]//Proc of the 13th European Conference on Computer Vision(ECCV 2014),2014:818-833.
                                    </a>
                                </li>
                                <li id="361">


                                    <a id="bibliography_24" title=" Bach S,Binder A,Montavon G,et al.On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation[J].Plos One,2015,10(7):e0130140." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation">
                                        <b>[24]</b>
                                         Bach S,Binder A,Montavon G,et al.On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation[J].Plos One,2015,10(7):e0130140.
                                    </a>
                                </li>
                                <li id="363">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     张晖,苏红,张学良,等.基于卷积神经网络的鲁棒性基音检测方法[J].自动化学报,2016,42(6):959-964.</a>
                                </li>
                                <li id="365">


                                    <a id="bibliography_12" title=" 姜文超,刘海波,杨宇杰,等.一种融合小波变换与卷积神经网络的高相似度图像识别与分类算法[J].计算机工程与科学,2018,40(9):1646-1652." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201809018&amp;v=MTY0ODFyQ1VSTE9lWmVSbUZ5L2dWYnZCTHo3QlpiRzRIOW5NcG85RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         姜文超,刘海波,杨宇杰,等.一种融合小波变换与卷积神经网络的高相似度图像识别与分类算法[J].计算机工程与科学,2018,40(9):1646-1652.
                                    </a>
                                </li>
                                <li id="367">


                                    <a id="bibliography_13" title=" 陶祝,刘正熙,熊运余,等.基于深度神经网络的行人头部检测[J].计算机工程与科学,2018,40(8):1475-1481." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201808021&amp;v=MDc1NjNCWmJHNEg5bk1wNDlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZidkJMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         陶祝,刘正熙,熊运余,等.基于深度神经网络的行人头部检测[J].计算机工程与科学,2018,40(8):1475-1481.
                                    </a>
                                </li>
                                <li id="369">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     张婷,李玉鑑,胡海鹤,等.基于跨连卷积神经网络的性别分类模型[J].自动化学报,2016,42(6):858-865.</a>
                                </li>
                                <li id="371">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     李勇,林小竹,蒋梦莹.基于跨连接LeNet-5网络的面部表情识别[J].自动化学报,2018,44(1):176-182.</a>
                                </li>
                                <li id="373">


                                    <a id="bibliography_16" title=" 田正鑫.基于多尺度卷积神经网络的交通标志识别方法[D].西安:长安大学,2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017869727.nh&amp;v=MTEwNjR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZidkJWRjI2R2J1K0Y5Yk9xSkViUElRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         田正鑫.基于多尺度卷积神经网络的交通标志识别方法[D].西安:长安大学,2017.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJK" target="_blank">计算机工程与科学</a>
                2019,41(10),1837-1845 DOI:10.3969/j.issn.1007-130X.2019.10.017            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于边缘的双路卷积神经网络及其可视化</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%9B%A8%E5%86%B2&amp;code=43026341&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李雨冲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%97%AB%E6%98%AD%E5%B8%86&amp;code=40745265&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闫昭帆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%A5%E5%9B%BD%E8%90%8D&amp;code=40745264&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">严国萍</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%95%BF%E5%AE%89%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0054413&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长安大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提高小尺度复杂图像识别准确率,通过对LeNet-5卷积神经网络并入一个新通道,让其处理与边缘有关的信息。结合两种通道产生的不同特征构造分类器,提出一种基于边缘的双路卷积神经网络,对小尺度复杂数据集进行识别。在包含10类产品数据上分类的结果表明,双路卷积神经网络的识别准确率远高于传统网络。最后通过神经网络可视化算法对双路卷积神经网络进行了可视化分析。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像模式识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E8%B7%AF%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双路卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E5%B0%BA%E5%BA%A6%E5%A4%8D%E6%9D%82%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小尺度复杂图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络可视化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李雨冲（1993-），男，陕西武功人，硕士生，研究方向为计算机视觉和人工智能。E-mail:506424073@qq.com，通信地址:710064陕西省西安市长安大学信息工程学院&lt;image id="173" type="formula" href="images/JSJK201910018_17300.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    闫昭帆（1995-），女，山西运城人，硕士生，研究方向为计算机视觉和人工智能。E-mail:846179149@qq.com，通信地址:710064陕西省西安市长安大学信息工程学院&lt;image id="175" type="formula" href="images/JSJK201910018_17500.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                                <span>
                                    严国萍（1995-），女，甘肃武威人，硕士生，研究方向为计算机视觉和人工智能。E-mail:1823213844@qq.com，通信地址:710064陕西省西安市长安大学信息工程学院&lt;image id="177" type="formula" href="images/JSJK201910018_17700.jpg" display="inline" placement="inline"&gt;&lt;alt&gt;&lt;/alt&gt;&lt;/image&gt;;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>“弘毅长大”研究生科研创新实践项目(2018103,2018109);</span>
                    </p>
            </div>
                    <h1><b>An edge-based 2-channel convolutional neural network and its visualization</b></h1>
                    <h2>
                    <span>LI Yu-chong</span>
                    <span>YAN Zhao-fan</span>
                    <span>YAN Guo-ping</span>
            </h2>
                    <h2>
                    <span>School of Information Engineering,Chang&apos;an University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve the recognition accuracy of small-scale complex images, an edge channel is added into LeNet-5 convolutional neural network to process the edge information. By combining the different features generated by two channels to construct a classifier, a 2-channel convolutional neural network is proposed to identify small-scale complex data sets. Classification results on ten types of product data show that the accuracy of the 2-channel convolutional neural network is much higher than that of the traditional network. Finally, the neural network visualization algorithm is adopted to visualize and analyze the 2-channel convolutional neural network.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20pattern%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image pattern recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=2-channel%20convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">2-channel convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=small-scale%20complex%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">small-scale complex image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network%20visualization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network visualization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                            <b>Author：</b>
                                                        <span>
                                    LI Yu-chong,born in 1993,MS candidate,his research interests include computer vision,and artificial intelligence.Address:School of Information Engineering,Chang’an University,Xi’an 710064,Shaanxi,P.R.China;
                                </span>
                                <span>
                                    YAN Zhao-fan,born in 1995,MS candidate,her research interests include computer vision,and artificial intelligence.Address:School of Information Engineering,Chang’an University,Xi’an 710064,Shaanxi,P.R.China;
                                </span>
                                <span>
                                    YAN Guo-ping,born in 1995,MS candidate,her research interests include computer vision,and artificial intelligence.Address:School of Information Engineering,Chang’an University,Xi’an 710064,Shaanxi,P.R.China;
                                </span>
                    </p>
                                    <p><b>Received：</b> 2018-11-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="64">图像的分类与识别是计算机视觉领域一个重要的研究方向,而图像特征的提取是计算机进行分类与识别的基础,如何简单且高效地提取图像的特征,以便计算机对其进行正确分类,一直是众多学者所研究的重点。早期人们普遍采用人工设计图像特征的方法,典型的有尺度不变特征转换SIFT(Scale Invariant Feature Transform)与梯度方向直方图HOG(Histogram of Gradient),将提取的特征送入支持向量机SVM (Support Vector Machine)等分类器中进行分类。这类方法一般针对所识别图像的类别来具体设计,其设计难度大,泛化能力差,易受到人为因素干扰。</p>
                </div>
                <div class="p1">
                    <p id="65">深度学习的出现较好地解决了人工设计图像特征所存在的问题。LeNet-5卷积神经网络及其在手写体数据集上的成功,引起了人们广泛的关注,网络可以自动提取特征,优化特征学习。但是,由于该网络层数较浅,将其用于处理较复杂图像数据时,根据网络所学习到的特征无法得到理想的分类结果。为此,Krizhevsky等人<citation id="375" type="reference"><link href="315" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>通过加深网络层数提出了AlexNet网络,并在图像数据集ImageNet上取得了很好的效果。在此之后又有学者相继提出了GoogLeNet<citation id="376" type="reference"><link href="317" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、VGG<citation id="377" type="reference"><link href="319" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、残差网络(Residual Network)<citation id="378" type="reference"><link href="321" rel="bibliography" /><link href="323" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>等模型。这些网络深度从十几到上百不等,网络结构针对复杂图像也取得了很好的效果,但这些网络均针对大尺度图像数据集所提出,旨在解决大规模数据图像的识别。而针对64×64以内的小尺度复杂图像,仅通过少量卷积池化操作,特征图的尺寸就会急剧下降,采用上述深度网络无法发挥出其应有的优势,反而增加了网络的计算时间,浪费了资源。</p>
                </div>
                <div class="p1">
                    <p id="66">本文以LeNet-5为基础,设计了一种新的网络结构,使得网络在不增加深度的情况下,实现小尺度复杂数据集的识别。文章结构为:第1节为引言;第2节介绍本文所提出的网络模型结构;第3节介绍模型的训练算法;第4节对本文所提出的方法进行实验;第5节使用网络可视化对实验结果进行分析;第6节总结全文得出结论。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag"><b>2 网络模型</b></h3>
                <h4 class="anchor-tag" id="68" name="68"><b>2.1 经典LeNet-5模型</b></h4>
                <div class="p1">
                    <p id="69">深度学习的起源可追朔到1962年Hubel等人<citation id="379" type="reference"><link href="325" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>对猫视觉皮层的研究。接着,1980年Fukushima等人<citation id="380" type="reference"><link href="327" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>受Hubel等人研究的影响,提出了神经认知机(Neocognitron),1986年Rumelhart<citation id="381" type="reference"><link href="329" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了目前所广泛采用的反向传播BP(Back Propagation)<citation id="382" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>算法,直到1989年,Lecun等人<citation id="383" type="reference"><link href="331" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>应用反向传播算法BP提出了卷积神经网络的雏形,并成功将其应用到手写数字的识别。随后,Lecun等人<citation id="384" type="reference"><link href="333" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>通过进一步完善该模型,在1998年的论文中正式提出了LeNet-5卷积神经网络模型。现今,以该模型为基础的深度学习已广泛应用于语音检测<citation id="385" type="reference"><link href="335" rel="bibliography" /><link href="363" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">11</a>]</sup></citation>、图像分类与识别等领域<citation id="386" type="reference"><link href="337" rel="bibliography" /><link href="339" rel="bibliography" /><link href="365" rel="bibliography" /><link href="367" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>,其结构如图1所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910018_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 LeNet-5卷积神经网络结构图" src="Detail/GetImg?filename=images/JSJK201910018_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 LeNet-5卷积神经网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910018_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 1 LeNet-5 convolutional neural network</p>

                </div>
                <div class="p1">
                    <p id="71">LeNet-5基本结构主要包含输入、卷积conv、池化pool、全连接fc、输出output,其中卷积和池化这2层交替出现。输入32×32像素的图像,经过6个5×5大小的滤波器得到Layer1卷积层的6个28×28大小的特征图。然后对这6个特征图分别进行2×2大小的池化操作,可得到大小为14×14的Layer2池化层特征图。Layer2与Layer3卷积层之间也是进行卷积操作,共有16个大小为5×5的滤波器,所以Layer3有16个大小为10×10的特征图,2层之间的连接主要是由Layer3的滤波器与Layer2特征图部分相连或全部相连得到,具体连接方式见如表1所示,其中“×”代表2层间存在连接。</p>
                </div>
                <div class="p1">
                    <p id="72">Layer4池化层有16个5×5像素的特征图,是由上层网络经过池化后得到的。Layer5依然是卷积层,它的滤波器大小依然为5×5,共有120个,所以Layer4层特征图经过卷积操作后得到120个1×1的特征图。Layer6全连接层有84个神经元,与上层网络全部连接。Output输出层输出网络分类的结果。</p>
                </div>
                <div class="p1">
                    <p id="73">相对于全连接神经网络,卷积神经网络能够通过滤波器得到特征图,从而实现了权值共享,极大减少了网络的参数,提高了网络的训练效率。卷积操作可以使得输入具有平移或旋转不变性。池化操作可以在保留主要特征的同时,提高网络模型的泛化能力,并增加网络的感受野。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>2.2 改进的LeNet-5网络</b></h4>
                <div class="p1">
                    <p id="75">LeNet-5网络在解决小尺度简单图像数据集（如MNIST）上取得了巨大成功，很大程度上推动了卷积神经网络的发展。但是，当其处理同为小尺度但图像内容复杂的数据集时，其效果并不理想。于是在LeNet-5基础上，张婷等人<citation id="387" type="reference"><link href="341" rel="bibliography" /><link href="369" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">14</a>]</sup></citation>提出跨连接的方式，通过结合高低层次特征在小尺度性别分类数据集上取得了较好的效果；李勇等人<citation id="388" type="reference"><link href="343" rel="bibliography" /><link href="371" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">15</a>]</sup></citation>通过跨连接的方式，在32×32的图像表情识别中取得了较高的准确率；田正鑫<citation id="389" type="reference"><link href="345" rel="bibliography" /><link href="373" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">16</a>]</sup></citation>采用多尺度特征的方式，同样在小尺度交通标志图像识别上取得了很好的效果。除此之外，Xu等人<citation id="390" type="reference"><link href="347" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>设计了一种深度多通道模型，该模型通过整合不同通道生成的信息，在图像语义分割上取得了很好的效果。</p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表1 LeNet-5中Layer2与Layer3之间的连接方式</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 1 Connection between Layer2 and Layer3 in LeNet-5 network</b></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td rowspan="2"><br />Layer2<br />卷积核<br />编号</td><td colspan="16"><br />Layer3卷积核编号</td></tr><tr><td><br />1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td></tr><tr><td><br />1</td><td>×</td><td></td><td></td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td>×</td><td></td><td>×</td><td>×</td></tr><tr><td><br />2</td><td>×</td><td>×</td><td></td><td></td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td>×</td><td></td><td>×</td></tr><tr><td><br />3</td><td>×</td><td>×</td><td>×</td><td></td><td></td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td></td><td>×</td><td>×</td><td>×</td></tr><tr><td><br />4</td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td></td><td>×</td><td>×</td></tr><tr><td><br />5</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td>×</td><td></td><td>×</td><td>×</td><td></td><td>×</td></tr><tr><td><br />6</td><td></td><td></td><td></td><td>×</td><td>×</td><td>×</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td>×</td><td></td><td>×</td><td>×</td><td>×</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910018_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双路卷积神经网络结构图" src="Detail/GetImg?filename=images/JSJK201910018_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 双路卷积神经网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910018_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 2 2-channel convolutional neural network</p>

                </div>
                <div class="p1">
                    <p id="79">本文结合多尺度特征融合及多通道的思想,对LeNet-5并入一个新通路。鉴于人在观察事物的时候,首先会关注物体的外形,所以让该通道负责图像边缘特征的处理,希望网络在学习特征时更多地考虑边缘因素。所以,本文提出了一种如图2所示的网络结构。对应于此结构,需要将待测数据集的边缘提取出来,构成一个边缘数据集。此外,从图2中可看出,整个网络分为2部分,下方网络为辅助网络,接收边缘数据集,上方网络为决策网络,接收原始数据集。二者均为经典LeNet-5模型,并在全连接层进行特征融合,组成双路卷积神经网络。这样做是为了通过辅助网络将边缘数据集所学到的与边缘有关的特征送入决策网络,从而使得网络可以在抽象特征中融入一些低层次的特征,辅助分类决策,以期望达到更好的分类效果。</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>3 模型训练算法</b></h3>
                <div class="p1">
                    <p id="81">整个网络的训练由2部分组成,分别为图2中辅助网络与决策网络的前向传播与反向传播,两者训练方法相同,本文以决策网络为例进行介绍。</p>
                </div>
                <div class="p1">
                    <p id="82">在前向传播中,决策网络计算主要分为以下几个部分:</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">(1)卷积层。</h4>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">n</mi><mi mathvariant="bold-italic">v</mi><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">z</mi><mo>)</mo></mrow><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>*</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mi>l</mi></msup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mi>l</mi></msup></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">其中,<i><b>a</b></i><sup><i>l</i></sup><sup>-1</sup>为第<i>l</i>个卷积层的输入,<i><b>w</b></i><sup><i>l</i></sup>为<i>l</i>与<i>l</i>-1层之间卷积核的连接权重,“*”代表卷积运算,<i><b>b</b></i><sup><i>l</i></sup>为该层的偏置项。他们的计算结果经过激活函数<i>σ</i>后,得到该卷积层的输出<i><b>conv</b></i><sup><i>l</i></sup>。在使用中,卷积核为5×5大小的矩阵,步长为1,激活函数为ReLU函数。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">(2)池化层。</h4>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">l</mi><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mi>max</mi><mspace width="0.25em" /><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">z</mi><mo>)</mo></mrow><mo>=</mo><mi>max</mi><mspace width="0.25em" /><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">池化层没有参数需要训练,它的目的是为了提取特征中最重要的特征并减少连接权重;其次该层能够在一定程度上防止过拟合发生,在本文中<i>pool</i>代表最大池化,即只保留池化过滤器中最大的值,池化过滤器大小为2×2,步长为2。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">(3)全连接层。</h4>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">f</mi><mi mathvariant="bold-italic">u</mi><mi mathvariant="bold-italic">l</mi><mi mathvariant="bold-italic">l</mi><msup><mrow></mrow><mi>l</mi></msup><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">z</mi><mo>)</mo></mrow><mo>=</mo><mi>σ</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mi>l</mi></msup><mo>×</mo><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mi>l</mi></msup></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">与卷积层一样,<i><b>a</b></i><sup><i>l</i></sup><sup>-1</sup>为第<i>l</i>个全连接层的输入,<i><b>w</b></i><sup><i>l</i></sup>为连接权重,<i><b>b</b></i><sup><i>l</i></sup>为该层的偏置项,激活函数为ReLU函数。不同的是,“×”代表矩阵乘法操作。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">(4)输出层。</h4>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">y</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">d</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mi>l</mi></msup><mo>×</mo><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mi>l</mi></msup><mo>+</mo><mi mathvariant="bold-italic">b</mi><msup><mrow></mrow><mi>l</mi></msup></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">输出层是将全连接层的结果,送给<i>softmax</i>分类器进行分类,最后得到每种类别的概率值。损失函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">L</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">s</mi><mi mathvariant="bold-italic">s</mi><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi mathvariant="bold-italic">y</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mi>log</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">y</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">d</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">其中,<i><b>ypred</b></i>为网络的预测值,<i><b>y</b></i>为网络的真实值,<i>N</i>为总样本数,<i>C</i>为类别数。</p>
                </div>
                <div class="p1">
                    <p id="97">本文所提出的模型训练算法为反向传播,目标是找到网络各层间的权重,使得损失函数最小。反向传播主要分为2个部分,一个是反馈误差的计算,另一个为权重变化量的计算。其中计算误差也是为了方便后面的权重部分的计算。式(6)为整个网络的反馈误差。</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>7</mn></msup><mo>=</mo><mi mathvariant="bold-italic">y</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi mathvariant="bold-italic">e</mi><mi mathvariant="bold-italic">d</mi><mo>-</mo><mi mathvariant="bold-italic">y</mi></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>6</mn></msup><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>7</mn></msup><mo>×</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mn>7</mn></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>˚</mo><msup><mi>σ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>5</mn></msup><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">δ</mi><msubsup><mrow></mrow><mi>j</mi><mn>6</mn></msubsup><mo>×</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mn>6</mn></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">]</mo><mo>˚</mo><msup><mi>σ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mn>8</mn><mn>4</mn></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>4</mn></msubsup><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">δ</mi><msubsup><mrow></mrow><mi>j</mi><mn>5</mn></msubsup><mo>*</mo><mi>r</mi><mi>o</mi><mi>t</mi><mn>1</mn><mn>8</mn><mn>0</mn><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mn>5</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>˚</mo><msup><mi>σ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mn>1</mn><mn>6</mn><mo>,</mo><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mn>1</mn><mn>2</mn><mn>0</mn></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>3</mn></msup><mo>=</mo><mi>u</mi><mi>p</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>4</mn></msup><mo stretchy="false">)</mo><mo>˚</mo><msup><mi>σ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">δ</mi><msubsup><mrow></mrow><mi>j</mi><mn>3</mn></msubsup><mo>*</mo><mi>r</mi><mi>o</mi><mi>t</mi><mn>1</mn><mn>8</mn><mn>0</mn><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mn>3</mn></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>˚</mo><msup><mi>σ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mn>6</mn><mo>,</mo><mn>1</mn><mo>≤</mo><mi>j</mi><mo>≤</mo><mn>1</mn><mn>6</mn></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>1</mn></msup><mo>=</mo><mi>u</mi><mi>p</mi><mi>s</mi><mi>a</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">δ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>˚</mo><msup><mi>σ</mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">其中,<i>δ</i><sup><i>n</i></sup>为第<i>n</i>层的误差项,<i><b>ypred</b></i>为网络的预测值,<i><b>y</b></i>为真值,<i><b>w</b></i><sup><i>m</i></sup>为<i>m</i>层与<i>m</i>-1层之间的连接权重,“。”表示矩阵对应元素逐元素相乘,<i>σ</i>′为ReLU激活函数的导数,<i><b>z</b></i><sup>(</sup><sup><i>p</i></sup><sup>)</sup>为在激活函数作用前,第<i>p</i>层的输出,<i>rot</i>180()为将括号中的元素旋转180度。<i>upsample</i>为上采样操作,这是因为池化层会降维,所以在反向传播的时候要上采样来进行维度的调整。需要注意的是,在计算卷积层的误差<i>δ</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>4</mn></msubsup></mrow></math></mathml>与<i>δ</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>的时候,由于卷积层可以看作是一个特殊的全连接层,特殊在其2层间的连接并不是全部连接,所以卷积层在计算误差的时候,需要将上层误差矩阵周围补零,使其拉伸到合适的维度,再跟旋转后的矩阵做卷积运算。其次,在计算全连接层的误差<i>δ</i><sup>5</sup>时,由于辅助网络所提供的特征只是提供辅助决策,真正与Layer5层连接的只有Layer6的第1～84个神经元,所以在反向传播的时候只需使用这一部分进行计算。</p>
                </div>
                <div class="area_img" id="100">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJK201910018_10000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="102">式(7)为网络中各层权重与偏置的变化量。<i>δ</i><sup><i>n</i></sup>为第<i>n</i>层的误差项,<i><b>a</b></i><sup><i>m</i></sup>为<i>m</i>层的输出。在最后算完各层权重与偏置项的变化量后,使用当前权重减去学习率乘以对应权重的变化量,来得到新的权重。</p>
                </div>
                <div class="p1">
                    <p id="103">训练算法流程为:</p>
                </div>
                <div class="p1">
                    <p id="104"><b>步骤1</b> 确定网络结构,初始化网络间连接权重与偏置;</p>
                </div>
                <div class="p1">
                    <p id="105"><b>步骤2</b> 对原始数据集采用边缘检测算法提取边缘,并保存为新的数据集;</p>
                </div>
                <div class="p1">
                    <p id="106"><b>步骤3</b> 将步骤2得到的数据集送入辅助网络,原始数据集送入决策网络,分别进行前向传播,并在全连接层进行特征融合;</p>
                </div>
                <div class="p1">
                    <p id="107"><b>步骤4</b> 将2个网络分别进行反向传播,计算误差与权重改变量;</p>
                </div>
                <div class="p1">
                    <p id="108"><b>步骤5</b> 通过梯度下降算法对权重进行更新;</p>
                </div>
                <div class="p1">
                    <p id="109"><b>步骤6</b> 重复步骤3～步骤6,直到达到结束条件,得到最终的网络。</p>
                </div>
                <h3 id="110" name="110" class="anchor-tag"><b>4 实验结果</b></h3>
                <div class="p1">
                    <p id="111">本实验利用Python 2.7实现,硬件平台为联想R720:Intel Core i5-7300HQ,主频2.5 GHz,内存8.00 GB。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>4.1 数据集</b></h4>
                <div class="p1">
                    <p id="113">本文采用2个数据集进行实验。第1个数据集为数字手写体MNIST数据集,第2个数据集为Fashion-MNIST<citation id="391" type="reference"><link href="349" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,该数据集由德国慕尼黑工业大学计算机博士肖涵提出。其数据集包含了10种不同类别的衣物图像共7万幅,其中6万幅为训练图像,1万幅为测试图像,每幅图像大小为28×28,测试集合的每种衣物各1 000幅。该数据集的提出是为了促进深度学习算法在产品搜索中的应用,其次,它可以代替传统的MNIST数据集,更好地区分不同模型的性能。图3为该数据集不同类别的示例。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>4.2 MNIST实验结果</b></h4>
                <div class="p1">
                    <p id="115">为验证本文方法的有效性,首先取2份相同的MNIST训练集,一份采用Canny算子<citation id="392" type="reference"><link href="351" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提取数据集边缘并保存成边缘训练集,另一份数据集不做改动,称之为原始训练集。并将所有数据图像进行填零扩充至32×32像素。实验中,网络的训练过程中损失函数采用交叉熵损失函数,<i>Batchsize</i>=25,迭代次数为10 000次,按照第3节算法流程进行实验。表2为实验结果。由表2可知,相较于LeNet-5,本文方法在MNIST数据集准确率上有小幅度提升。</p>
                </div>
                <div class="area_img" id="178">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910018_17800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Fashion-MNIST数据集类别名称及示例图" src="Detail/GetImg?filename=images/JSJK201910018_17800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Fashion-MNIST数据集类别名称及示例图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910018_17800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 3 Class names and example images in Fashion-MNIST dataset</p>

                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表2 MNIST分类正确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 2 Classification accuracy of different classes on MNIST dataset</b></p>
                    <p class="img_note">%</p>
                    <table id="119" border="1"><tr><td><br />Label</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>整体</td></tr><tr><td><br />LeNet5</td><td>98.78</td><td>99.11</td><td>98.99</td><td>99.43</td><td>98.96</td><td>97.41</td><td>97.81</td><td>98.91</td><td>98.02</td><td>97.75</td><td>98.54</td></tr><tr><td><br />本文方法</td><td>99.18</td><td>99.82</td><td>98.84</td><td>99.11</td><td>99.40</td><td>98.32</td><td>98.75</td><td>99.45</td><td>98.25</td><td>98.51</td><td>99.17</td></tr><tr><td><br />对比</td><td>+0.4</td><td>+0.71</td><td>-0.15</td><td>-0.32</td><td>+0.44</td><td>+0.91</td><td>+0.94</td><td>+0.54</td><td>+0.23</td><td>+0.76</td><td>+0.63</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>4.3 Fashion-MNIST实验结果</b></h4>
                <div class="p1">
                    <p id="122">在图像尺寸保持不变的情况下,采用Fashion-MNIST数据集,改变图像内容复杂度进行实验,实验参数同4.2节。表3为本文所提方法与经典卷积神经网络在Fashion-MNIST数据集上得到的实验结果。为便于表述,本文使用数字表示物体类别,即第0类代表“T-Shirt/Top”,第{4,6}类分别代表“Coat”与“Shirt”类别等。</p>
                </div>
                <div class="p1">
                    <p id="123">由表3可知,本文方法在Fashion-MNIST数据集上的分类准确率较高,而原始LeNet5在Fashion-MNIST上的检测效果并不好。从表3中可以看到,原始网络对第2类的检测正确率只有76.6%,而本文方法将此正确率提高至92.9%,但本文方法对第4类的检测正确率却比原始网络下降了6.6%。</p>
                </div>
                <div class="p1">
                    <p id="124">表4统计了不同类别分类错误的次数。表4中为了突出显示分类错误的数量,将分类正确的值设置为空。从表4可得,本文方法通过引入边缘特征,较好地解决了识别出错的问题,但是可以发现第{0,2,4,6}类之间仍然存在不同程度的误判。</p>
                </div>
                <div class="p1">
                    <p id="125">表5为本文方法与经典分类方法正确率的比较。由表5可以得知,相对于感知机等传统机器学习方法,本文方法的效果更优。</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表3 Fashion-MNIST分类正确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 3 Classification accuracy of different classes on Fashion-MNIST dataset</b></p>
                    <p class="img_note">%</p>
                    <table id="126" border="1"><tr><td><br />Label</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>整体</td></tr><tr><td><br />LeNet5</td><td>82.3</td><td>93.8</td><td>76.6</td><td>86.8</td><td>83.8</td><td>96.6</td><td>62.0</td><td>90.4</td><td>96.7</td><td>91.8</td><td>86.08</td></tr><tr><td><br />本文方法</td><td>94.4</td><td>98.4</td><td>92.9</td><td>94.7</td><td>77.2</td><td>97.2</td><td>71.0</td><td>97.5</td><td>96.0</td><td>96.2</td><td>91.55</td></tr><tr><td><br />对比</td><td>+12.1</td><td>+4.6</td><td>+16.3</td><td>+7.9</td><td>-6.6</td><td>+0.6</td><td>+9.0</td><td>+7.1</td><td>-0.7</td><td>+4.4</td><td>+5.47</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表4 不同类别分类错误统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 4 Classification errors statistics of different classes</b></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br /></td><td>预测为0</td><td>预测为1</td><td>预测为2</td><td>预测为3</td><td>预测为4</td><td>预测为5</td><td>预测为6</td><td>预测为7</td><td>预测为8</td><td>预测为9</td></tr><tr><td><br />真值为0</td><td></td><td>0</td><td>14</td><td>4</td><td>8</td><td>3</td><td>19</td><td>0</td><td>8</td><td>0</td></tr><tr><td><br />真值为1</td><td>2</td><td></td><td>1</td><td>9</td><td>2</td><td>0</td><td>0</td><td>0</td><td>2</td><td>0</td></tr><tr><td><br />真值为2</td><td>14</td><td>1</td><td></td><td>9</td><td>34</td><td>0</td><td>11</td><td>0</td><td>2</td><td>0</td></tr><tr><td><br />真值为3</td><td>11</td><td>0</td><td>13</td><td></td><td>16</td><td>0</td><td>10</td><td>0</td><td>3</td><td>0</td></tr><tr><td><br />真值为4</td><td>2</td><td>3</td><td>153</td><td>12</td><td></td><td>0</td><td>54</td><td>0</td><td>4</td><td>0</td></tr><tr><td><br />真值为5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td></td><td>0</td><td>17</td><td>0</td><td>11</td></tr><tr><td><br />真值为6</td><td>117</td><td>0</td><td>62</td><td>14</td><td>87</td><td>0</td><td></td><td>0</td><td>10</td><td>0</td></tr><tr><td><br />真值为7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>11</td><td>0</td><td></td><td>0</td><td>14</td></tr><tr><td><br />真值为8</td><td>6</td><td>4</td><td>5</td><td>6</td><td>2</td><td>8</td><td>4</td><td>5</td><td></td><td>0</td></tr><tr><td><br />真值为9</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>0</td><td>31</td><td>0</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="128">
                    <p class="img_tit"><b>表5 不同方法在Fashion-MNIST上的正确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"><b>Table 5 Accuracy of different methods on Fashion-MNIST</b></p>
                    <p class="img_note"></p>
                    <table id="128" border="1"><tr><td><br />方法</td><td>模型</td><td>正确率/%</td></tr><tr><td><br />Rosenblatt<sup>[20]</sup></td><td>Perceptron</td><td>78.20</td></tr><tr><td><br />Quinlan<sup>[21]</sup></td><td>DecisionTreeClassifier</td><td>79.80</td></tr><tr><td><br />Bottou<sup>[22]</sup></td><td>SGDClassifier</td><td>81.90</td></tr><tr><td><br />本文方法</td><td>双路LeNet-5</td><td>91.55</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910018_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 LRP算法描述" src="Detail/GetImg?filename=images/JSJK201910018_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 LRP算法描述  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910018_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 4 Description of LRP algorithm</p>

                </div>
                <h3 id="130" name="130" class="anchor-tag"><b>5 实验分析</b></h3>
                <h4 class="anchor-tag" id="131" name="131"><b>5.1 层间相关性传播可视化算法</b></h4>
                <div class="p1">
                    <p id="132">深度学习在图像分类等任务中取得了巨大的成功,但其无法解释网络模型所做出的决策。网络所学习到的特征针对某一数据集的分类效果较好,但是在另一数据集上的分类效果偏差,至今也没有明确的解释。其次,输入数据的哪一部分通过网络后对最终的决策起了作用,并且该作用有多大也无从得知。针对这些问题,Zeiler等人<citation id="393" type="reference"><link href="359" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>通过反卷积来可视化特征,Bach等人<citation id="394" type="reference"><link href="361" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>提出了层间相关性传播LRP(Layer-wise Relevance Propagation)算法。鉴于LRP算法可以将分类结果通过消息传递的方式分解到原始输入的每个像素点,并展现每个像素对于分类结果的影响,进而绘制成该网络的热图。故本文采用该算法对所提出的双路卷积神经网络加以解释。</p>
                </div>
                <div class="p1">
                    <p id="133">LRP算法将分类结果当作初始相关性<i>R</i>,通过式(8)将相关性传播给前一层,依次类推,得到最后分类结果在输入图像上的相关性。</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>←</mo><mi>j</mi></mrow><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi mathvariant="bold-italic">R</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>⋅</mo><mrow><mo>(</mo><mrow><mi>α</mi><mo>⋅</mo><mfrac><mrow><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mo>+</mo></msubsup></mrow><mrow><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mi>j</mi><mo>+</mo></msubsup></mrow></mfrac><mo>+</mo><mi>β</mi><mo>⋅</mo><mfrac><mrow><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mo>-</mo></msubsup></mrow><mrow><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mi>j</mi><mo>-</mo></msubsup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135">其中,<i>j</i>为网络中第<i>l</i>+1层第<i>j</i>个神经元,<i>i</i>为与<i>j</i>连接的<i>l</i>层神经元。<i><b>z</b></i><sub><i>ij</i></sub>为网络前向传播过程中,神经元<i>i</i>经过运算激活后到第<i>j</i>个神经元的结果。其中<i><b>z</b></i><sup>+</sup><sub><i>j</i></sub>=∑<sub><i>i</i></sub><i><b>z</b></i><sup>+</sup><sub><i>ij</i></sub>+<i><b>b</b></i><sup>+</sup><sub><i>j</i></sub>,<i><b>z</b></i><sup>-</sup><sub><i>j</i></sub>=∑<sub><i>i</i></sub><i><b>z</b></i><sup>-</sup><sub><i>ij</i></sub>+<i><b>b</b></i><sup>-</sup><sub><i>j</i></sub>,“+”和“-”代表着<i>z</i><sub><i>ij</i></sub>与偏差<i>b</i><sub><i>ij</i></sub>的正负。<i>α</i>与<i>β</i>是调节因子,<i>α</i>+<i>β</i>=1,其可调节分解式中正负项所占比例。图4为LRP算法的图形描述。图4左侧代表神经元真实相关性,右侧代表经过LRP算法后每个神经元的相关性。颜色越深相关性越高,对分类的结果影响越大。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136"><b>5.2 MNIST结果分析</b></h4>
                <div class="p1">
                    <p id="137">本文通过加入与边缘有关的因素来优化网络特征学习。由表2可知,本文网络结构对MNIST数据集准确率有小幅度提升。为验证该提升是否由边缘因素造成,通过LRP算法对网络进行可视化来寻找原因。</p>
                </div>
                <div class="p1">
                    <p id="138">图5展示了2种网络结构经过可视化后的热图,上面一行为LeNet-5的热图,下面一行为本文网络的热图,从左至右依次为第0类～第9类,其中颜色越深代表其对网络分类的影响越大。该热图根据各类别所有分类正确的平均相关性所绘制。从图5中可以明显看出,针对MNIST数据集,本文方法所得热图有着原始数字的形状,而LeNet-5所得热图并无明显的数字形状。虽然2种网络准确率相差不大,但网络的关注点各有不同,本文方法更加关注图像的边缘及轮廓,而LeNet-5对局部具有较强的关注点。这说明了本文通过对原始LeNet-5加入与边缘有关的因素,成功提升了网络的准确率,更为重要的是优化了网络特征学习,改变了网络的关注点。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910018_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 2种网络在MNIST上的热图" src="Detail/GetImg?filename=images/JSJK201910018_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 2种网络在MNIST上的热图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910018_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 5 Heat maps of two different networks on MNIST</p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJK201910018_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 2种网络在Fashion-MNIST上的热图" src="Detail/GetImg?filename=images/JSJK201910018_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 2种网络在Fashion-MNIST上的热图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJK201910018_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit">Figure 6 Heat maps of two different networks on Fashion-MNIST</p>

                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>5.3 Fashion-MNIST结果分析</b></h4>
                <div class="p1">
                    <p id="142">针对Fashion-MNIST数据集,由表3可知,本文所提出的网络结构整体上较为明显地提升了该数据集的识别准确率。其中{0,2,6}这3类准确率上升最为显著,但第4类有较为明显的下降。从表4可以得知,本文方法对{0,2,4,6}这4类均有一定程度的误判。同5.2节,本文通过LRP算法对网络进行可视化来寻找原因。</p>
                </div>
                <div class="p1">
                    <p id="143">图6为2种网络结构在Fashion-MNIST数据集上的热图,图中各类别分布排列同图5。</p>
                </div>
                <div class="p1">
                    <p id="144">从图6上面一行热图可以看出,LeNet-5对图像的关注度主要位于原始图像的局部像素点。如第{0,2}2类的关注点为右侧像素,第{4,5}2类关注上侧像素。而从图6下面一行(由本文算法所得热图)可发现,网络的关注点已经弱化了局部像素的相关性,转而将关注点放到了轮廓像素中去。如第0类除弱化了右侧局部像素的影响,并且关注点也增加到了对应热图左、上、右3部分。第{4,6}类更为明显,从这2类的热图可看出,其均类似于上衣轮廓,与原始类别吻合。</p>
                </div>
                <div class="p1">
                    <p id="145">此外,本文方法所得到的第{0,2,4,6}类热图大致为一个环,像上衣的轮廓。导致这一结果的主要原因是数据集较为复杂,同时结合上述类的原始图像,可以发现这些类别均属于上衣类,其本身轮廓较为相似,这也导致了本文网络对这些类别容易产生误判。不过这也说明了本文在LeNet5网络结构加入双通路,并让一个通路负责边缘的处理,使得网络学习到的高层特征中加入与边缘有关的特征的做法是成功的,这些边缘特征也起到了预期的效果,即在抽象特征中加入一些简单的初级特征,优化特征学习,在一定程度上将网络对原始图像的关注点引向了图像边缘。</p>
                </div>
                <div class="p1">
                    <p id="146">同时,本文发现LeNet-5分类错误但采用本文方法后分类正确的热图,更加关注边缘处的像素点,本文方法会增强图像边缘处像素对分类结果的影响,从而使得本文方法的准确率较高。</p>
                </div>
                <h3 id="147" name="147" class="anchor-tag"><b>6 结束语</b></h3>
                <div class="p1">
                    <p id="148">深度学习也可以称作特征学习,通过将数据送入深度神经网络来自动学习该类数据的特征。本文在原始LeNet-5的结构基础上设计出一种新结构,将原始数据与其边缘送入网络训练,得到不同层次的特征,并将这些特征结合,使得在不增加单个网络层数的情况下,优化特征学习,提高了网络在小规模复杂数据集上的正确率,也在一定程度上减少了反向传播中的梯度不稳定现象。其次,本文通过LRP可视化算法将LeNet-5及本文网络结构做了对比,实验结果可以发现,将边缘数据集学到的与边缘有关的特征送入网络,使得网络在抽象特征中融入一些低层次的特征,来对分类进行辅助决策,这样的做法是有效的。但是,采用边缘数据仍存在一定缺陷,当数据存在很多轮廓相似的类别时,会使得网络在判定这些类别时出现一定程度的误判,但同时说明了本文加入与边缘有关特征的做法是有效的。</p>
                </div>
                <div class="p1">
                    <p id="149">鉴于边缘数据的缺陷,在今后的研究中希望能够通过优化反卷积重构图像等方法,替代目前所采用的边缘数据集,最终达到更好的识别准确率。其次,通过神经网络的可视化算法对网络结构进行进一步的改进与优化。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="315">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neu-ral Networks">

                                <b>[1]</b> Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//Proc of Advances in Neural Information Processing Systems,2012:1097-1105.
                            </a>
                        </p>
                        <p id="317">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[2]</b> Szegedy C,Liu W,Jia Y,et al.Going deeper with convolutions[C]//Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2015:1-9.
                            </a>
                        </p>
                        <p id="319">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[3]</b> Simonyan K,Zisserman A.Very deep convolutional networks for large-scale image recognition[C]//Proc of International Conference on Learning Representation,2015:1-14.
                            </a>
                        </p>
                        <p id="321">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 He K M,Zhang X Y,Ren S Q,et al.Deep residual learning for image recognition[C]// Proc of the IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.
                            </a>
                        </p>
                        <p id="323">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity Mappings in Deep Residual Networks">

                                <b>[5]</b> He K,Zhang X,Ren S,et al.Identity mappings in deep residual networks[C]// Proc of the 14th European Conference on Computer Vision,2016:630-645.
                            </a>
                        </p>
                        <p id="325">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Receptive fields, binocular interaction and functional architecture in the cat&amp;#39;s visual cortex">

                                <b>[6]</b> Hubel D H,Wiesel T N.Receptive fields,binocular interaction and functional architecture in the cat’s visual cortex[J].Journal of Physiology,1962,160(1):106-154.
                            </a>
                        </p>
                        <p id="327">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neocognitron:A neural network model for a mechanism of visual pattern recognition">

                                <b>[7]</b> Fukushima K,Miyake S,Ito T.Neocognitron:A neural network model for a mechanism of visual pattern recognition[J].IEEE Transactions on Systems,Man &amp; Cybernetics,1982,SMC-13(5):826-834.
                            </a>
                        </p>
                        <p id="329">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning representations by back propagating errors">

                                <b>[8]</b> Rumelhart D E.Learning representations by back-propagating errors[J].Nature,1986,323(6088):399-421.
                            </a>
                        </p>
                        <p id="331">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Handwritten Digit Recognition with a Back-Propagation Network">

                                <b>[9]</b> Lecun Y,Boser B,Denker J S,et al.Handwritten digit recognition with a back-propagation network[J].Advances in Neural Information Processing Systems,1990,2(2):396-404.
                            </a>
                        </p>
                        <p id="333">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 Lecun Y,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="335">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606017&amp;v=MzEwNzVnVmJ2QktDTGZZYkc0SDlmTXFZOUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm1GeS8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Zhang Hui,Su Hong,Zhang Xue-liang,et al.Convolutional neural network for robust pitch determination[J].Acta Automatica Sinica,2016,42(6):959-964.(in Chinese)
                            </a>
                        </p>
                        <p id="337">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 Jiang Wen-chao,Liu Hai-bo,Yang Yu-jie,et al.A high similar image recognition and classification algorithm fusing wavelet transform and convolution neural network[J].Computer Engineering &amp; Science,2018,40(9):1646-1652.(in Chinese)
                            </a>
                        </p>
                        <p id="339">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Tao Zhu,Liu Zheng-xi,Xiong Yun-yu,et al.Pedestrian head detection based on deep neural networks[J].Computer Engineering &amp; Science,2018,40(8):1475-1481.(in Chinese)
                            </a>
                        </p>
                        <p id="341">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606006&amp;v=MTM2Mjk5Zk1xWTlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZidkJLQ0xmWWJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Zhang Ting,Li Yu-jian,Hu Hai-he,et al.A gender classification model based on cross-connected convolutional neural networks[J].Acta Automatica Sinica,2016,42(6):858-865.(in Chinese)
                            </a>
                        </p>
                        <p id="343">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201801015&amp;v=MTA5NjJDVVJMT2VaZVJtRnkvZ1ZidkJLQ0xmWWJHNEg5bk1ybzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Li Yong,Lin Xiao-zhu,Jiang Meng-ying.Facial expression recognition with cross-connect LeNet-5 network[J].Acta Automatica Sinica,2018,44(1):176-182.(in Chinese)
                            </a>
                        </p>
                        <p id="345">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Traffic sign recognition method based on multi scale convolutional neural network">

                                <b>[16]</b> Tian Zheng-xin.Traffic sign recognition method based on multi scale convolutional neural network[D].Xi’an:Chang’an University,2017.(in Chinese)
                            </a>
                        </p>
                        <p id="347">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gland instance segmentation using deep multichannel neural networks">

                                <b>[17]</b> Xu Y,Li Y,Wang Y,et al.Gland instance segmentation using deep multichannel neural networks[J].IEEE Transactions on Biomedical Engineering,2017,64(12):2901-2912.
                            </a>
                        </p>
                        <p id="349">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms">

                                <b>[18]</b> Xiao H,Rasul K,Vollgraf R.Fashion-MNIST:A novel image dataset for benchmarking machine learning algorithms[J].arXiv preprint arXiv:1708.07747,2017.
                            </a>
                        </p>
                        <p id="351">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computational approach to edge detection">

                                <b>[19]</b> Canny J.A computational approach to edge detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1986,PAMI-8(6):679-698.
                            </a>
                        </p>
                        <p id="353">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The perceptron: a probabilistic model for information storage and organization in the brain">

                                <b>[20]</b> Rosenblatt F.The perceptron:A probabilistic model for information storage and organization in the brain[J].Psychological Review,1958,65(6):386-408.
                            </a>
                        </p>
                        <p id="355">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MTQ4NzE1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDRGxWTHJPSkZjPU5qN0Jhck80SHRITnJJeENiT29QWTNr&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Quinlan J R.Induction of decision trees[J].Machine Learning,1986,1(1):81-106.
                            </a>
                        </p>
                        <p id="357">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic Gradient Descent Tricks">

                                <b>[22]</b> Bottou L.Stochastic gradient descent tricks[M]//Montavon G,Orv G B,Müller K R.Neural Networks:Tricks of the Trade.Berlin:Springer Berlin Heidelberg,2012:421-436.
                            </a>
                        </p>
                        <p id="359">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[23]</b> Zeiler M D,Fergus R.Visualizing and understanding convolutional networks[C]//Proc of the 13th European Conference on Computer Vision(ECCV 2014),2014:818-833.
                            </a>
                        </p>
                        <p id="361">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation">

                                <b>[24]</b> Bach S,Binder A,Montavon G,et al.On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation[J].Plos One,2015,10(7):e0130140.
                            </a>
                        </p>
                        <p id="363">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 张晖,苏红,张学良,等.基于卷积神经网络的鲁棒性基音检测方法[J].自动化学报,2016,42(6):959-964.
                            </a>
                        </p>
                        <p id="365">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201809018&amp;v=MTQ4Mjk5bk1wbzlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJtRnkvZ1ZidkJMejdCWmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 姜文超,刘海波,杨宇杰,等.一种融合小波变换与卷积神经网络的高相似度图像识别与分类算法[J].计算机工程与科学,2018,40(9):1646-1652.
                            </a>
                        </p>
                        <p id="367">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201808021&amp;v=MzA4MjdlWmVSbUZ5L2dWYnZCTHo3QlpiRzRIOW5NcDQ5SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 陶祝,刘正熙,熊运余,等.基于深度神经网络的行人头部检测[J].计算机工程与科学,2018,40(8):1475-1481.
                            </a>
                        </p>
                        <p id="369">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 张婷,李玉鑑,胡海鹤,等.基于跨连卷积神经网络的性别分类模型[J].自动化学报,2016,42(6):858-865.
                            </a>
                        </p>
                        <p id="371">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 李勇,林小竹,蒋梦莹.基于跨连接LeNet-5网络的面部表情识别[J].自动化学报,2018,44(1):176-182.
                            </a>
                        </p>
                        <p id="373">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017869727.nh&amp;v=MjgxOTJGeS9nVmJ2QlZGMjZHYnUrRjliT3FKRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm0=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 田正鑫.基于多尺度卷积神经网络的交通标志识别方法[D].西安:长安大学,2017.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJK201910018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201910018&amp;v=MjQ0OTQ5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbUZ5L2dWYnZPTHo3QlpiRzRIOWpOcjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQ3cmEvcXIxT0wrTHhvaVdCNFNyZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
